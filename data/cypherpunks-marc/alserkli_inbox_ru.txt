
@_date: 2009-08-13 09:54:37
@_author: Alexander Klimov 
@_subject: Re: brute force physics Was: cleversafe... 
"Obvious" assumptions are hardest to trace: they assume that the
computation is done by bit flips which flip only constant number of
bits. QM computation is done with the whole state, that is a flip in
an N-qbit state simultaneously flips 2^N numbers describing this
state (if it were simulated by a classical computer).
I hope we agree that an exponential speed-up by QM computation *is*
possible (unless factoring is in P). That is why if we hear that (1) a
"physical limit" says that 2^N bit-flips cannot be done in a
one-light-year computer in a year, and we know that (2) the best
classical M-bit factoring requires 2^N bit-flips, and we know that (3)
this factoring can be done in few minutes on a QM computer, then we
must conclude that such "physical limits" are misleading.
Thanks, it is great reading!
Whether NP is physically computable is an interesting topic, but its
applicability to our subject is quite limited: for cryptography it is
important to know that none of the puzzles created by a user can be
solved in some particular time given reasonable investment in
hardware, and thus the fact that hard instances exist as size tends to
infinity cannot console a practitioner.
There is a set of rules in cryptographic design that are well
known to practitioners and are ignored by kibitzers. In many
cases it is much easier to do a normal design than to explain
why some proposed "improvements" are bad.
Consider a situation where some "smart" person comes to you and
say that for some reason (say, to save space) they want to use
CBC encryption, but keep IV constant; or they want to do CBC
encryption and CBC-MAC with the same key (as a bonus they get
encryption for free). What options you have once you manage to
stop the laughter?  In some cases it is enough to say: "No,
SP800-38a in Appendix C requires IV for CBC to be
unpredictable," but in other cases you also need to show
a plausible-looking attack against the "improvement". In this
situation it is clearly an advantage to have a known weakness of
ignoring rules of sane key management.

@_date: 2009-08-10 08:42:45
@_author: Alexander Klimov 
@_subject: brute force physics Was: cleversafe... 
A problem with this reasoning is that the physical world and the usual
digital computers have exponential simulation gap (it is known at
least in one direction: to simulate N entangled particles on a digital
computer one needs computations exponential in N). This can be
considered as a reason to suspect that physical world is
non-polynomially faster than the usual computers (probably even to an
extent that all NP computations can be simulated).
While it is possible to use physical world to simulate usual computers
in the straightforward way (namely by using voltage levels of a
circuit to represent separate bits), it is not clear that doing
computations in this way is the best way to do computations, for
example, if the meaning of our computations are simulation of the
physical world, then it can be better to use direct
physical-to-physical mapping instead of physical-to-usual followed by
usual-to-physical: analog computers, such as wind tunnels, are still
in use.
I am not aware of any plausible argument why a brute force search in
general (a quintessence of NP class, by the way) or a key search
against any particular algorithm cannot be implemented in a direct way
significantly faster than in the indirect way, that is NP-to-physical
instead of NP-to-usual followed by usual-to-physical. All the fuss
about quantum computing is exactly because people believe that a
different mapping (not thru usual computers) can be more efficient (if
I understand correctly, right now neither the class of algorithms that
can be sped up this way is understood, nor the quantum computers of
practical capacity exist).
I see the situation in the positive way: the recent AES attacks
stress the fact that the key management should be done
correctly, in particular, keys should be derived thru KDF (not
a simple xor) and must be authenticated. With this attack in
hand it is much easier for us now to say why one should not use
K to encrypt messages of one type and K+1 for another type, or
why it is a bad idea to encrypt a key in CTR mode and store the
result without a MAC. I doubt it is possible to find any
professionally designed protocol or standard that becomes weak
due to the recent discovery.
Of course, if AES-256 was used (say for hashing) with input as
the key, then nobody should be surprised that the version that
takes 256 bits at a time is weaker than the version that spends
comparable time for processing only 128 bits.

@_date: 2011-02-21 09:48:22
@_author: Alexander Klimov 
@_subject: [cryptography] Reliably Erasing Data From Flash-Based Solid State 
We empirically evaluate the effectiveness of hard
  drive-oriented techniques and of the SSDs' built-in
  sanitization commands by extracting raw data from the SSD's
  flash chips after applying these techniques and commands. Our
  results lead to three conclusions: First, built-in commands
  are effective, but manufacturers sometimes implement them
  incorrectly. Second, overwriting the entire visible address
  space of an SSD twice is usually, but not always, sufficient
  to sanitize the drive. Third, none of the existing hard
  drive-oriented techniques for individual file sanitization are
  effective on SSDs.

@_date: 2011-03-10 10:30:15
@_author: Alexander Klimov 
@_subject: [cryptography] Reliably Erasing Data From Flash-Based Solid 
Many forensic-worthy files (e.g., images and text documents) are
never actually changed: most files are just downloaded and even
if the user edits and saves a file, in reality the editor each
time creates a new file and remove the previous one. Of course,
databases and swap-files are different.

@_date: 2011-03-10 10:19:06
@_author: Alexander Klimov 
@_subject: [cryptography] Reliably Erasing Data From Flash-Based Solid 
The flash controller (in addition to the address translation) may also
be supposed to do the error correction: detecting that a block is worn
out may be based on how many errors are corrected. Thus it is
plausible that the data may be also re-encoded.

@_date: 2011-03-06 10:29:17
@_author: Alexander Klimov 
@_subject: [cryptography] Reliably Erasing Data From Flash-Based Solid 
It is also harder to rely on SSD as evidence.
  Digital evidence is increasingly relied upon in computer
  forensic examinations and legal proceedings in the modern
  courtroom. The primary storage technology used for digital
  information has remained constant over the last two decades,
  in the form of the magnetic disc.  Consequently,
  investigative, forensic, and judicial procedures are
  well-established for magnetic disc storage devices (Carrier,
  2005). However, a paradigm shift has taken place in technology
  storage and complex, transistor-based devices for primary
  storage are now increasingly common. Most people are aware of
  the transition from portable magnetic floppy discs to portable
  USB transistor flash devices, yet the transition from magnetic
  hard drives to solid-state drives inside modern computers has
  so far attracted very little attention from the research
  community.
  Here we show that it is imprudent and potentially reckless to
  rely on existing evidence collection processes and procedures,
  and we demonstrate that conventional assumptions about the
  behaviour of storage media are no longer valid. In particular,
  we demonstrate that modern storage devices can operate under
  their own volition in the absence of computer instructions.
  Such operations are highly destructive of traditionally
  recoverable data.  This can contaminate evidence; can
  obfuscate and make validation of digital evidence reports
  difficult; can complicate the process of live and dead
  analysis recovery; and can complicate and frustrate the post
  recovery forensic analysis.
  Our experimental findings demonstrate that solid-state drives
  (SSDs) have the capacity to destroy evidence catastrophically
  under their own volition, in the absence of specific
  instructions to do so from a computer.
