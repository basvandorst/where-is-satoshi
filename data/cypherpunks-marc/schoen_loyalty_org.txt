
@_date: 2001-12-08 07:42:26
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Phil Zimmermann on key exchange 
Reviving a thread from last month:
The Board of Directors of EFF met today in San Francisco, and I made a
presentation about this, in the presence of Brad Templeton and others.
One of the conclusions was that EFF's role in implementing something
like this is still not defined clearly enough, and we don't know what
we could most usefully do.
For example, some people like the idea of standardizing protocols
through the IETF; others prefer a completely independent development
of a spec (possibly with advance commitments or at least expressions
of interest from vendors), and then submission to standards bodies
after the technical work is more of a fait accompli.  There is some
disagreement about who exactly should write which code, for what
platform, and what effect it would have for different people (e.g.
famous cryptographers, civil liberties organizations, well-known
scientists or network engineers) to endorse various approaches in
various places.
We want to figure out more about what EFF can best do to make this
happen.  Brad Templeton is planning to write to Phil Zimmermann, and I
plan to write to Phil Karn and some other people.
On the technical side, Brad still prefers his approach to the robot CA.
I argued that the robot CA might be better because it's harder to
launch MITM attacks (you only have _one_ opportunity, at the initial
key verification step, and not each time a new pair of people begin
communicating with one another).  There's also less overhead, because
an interactive verification step happens once per person rather than
once per pair of communicating parties (and keys are only ever sent to
people who can really use them).
Brad was concerned that the robot CA is a single point of failure and
an easy target for attacks (DOS, subpoena, physical intrusions); it
_does_ hold some secret and trusted information (its own private
signing key) and also has a uniquely valuable key which can be
compromised -- an event which would tend to undermine the entire
scheme.  He added that both schemes are equally secure against passive
wiretapping, and the scheme he outlined can survive even if the
organizations which originally supported it go away.
Brad's revision of the PGP threat model is fairly radical; he says
that it's reasonable to accept that e-mail messages will be
compromised if an attacker performs an active MITM attack at the right
time, or if your computer is physically compromised or seized, or
even, in some configurations, possibly if your ISP's mail server is
physically compromised or seized (which was the hardest for me to
I want to bifurcate the issue and ask everyone here:
(1) What's the best design for an "informal key exchange" scheme in
which active MITM attacks may be permitted, but privacy against
passive wiretapping (as well as trivial impersonation attacks) is
maintained?  How can this be implemented with the smaller amount of
user interface, while maintaining the largest amount of compatibility
in both directions with existing e-mail privacy systems for
sophisticated users?
(2) What's the best way to get such a system designed and deployed to
the general public?  How can an organization like EFF best help
accomplish this?  Whose help do we need?

@_date: 2002-08-06 19:11:39
@_author: Seth David Schoen 
@_subject: Re: Privacy-enhancing uses for TCPA 
I would just like to point out that the view that "the protection of
privacy [is] the same technical problem as the protection of
copyright" is Microsoft's and not mine.  I don't agree that these
problems are the same.
An old WinHEC presentation by Microsoft's Peter Biddle says that
computer security, copyright enforcement, and privacy are the same
problem.  I've argued with Peter about that claim before, and I'm
going to keep arguing about it.
For one thing, facts are not copyrightable -- copyright law in the
U.S. has an "idea/expression dichotomy", which, while it might be
ultimately incoherent, suggests that copyright is not violated when
factual information is reproduced or retransmitted without permission.
So, for example, giving a detailed summary of the plot of a novel or
a movie -- even revealing what happens in the ending! -- is not an
infringement of copyright.  It's also not something a DRM system can
But privacy is frequently violated when "mere" facts are redistributed.
It often doesn't matter that no bits, bytes, words, or sentences were
copied verbatim.  In some cases (sexual orientation, medical history,
criminal history, religious or political belief, substance abuse), the
actual informational content of a "privacy-sensitive" assertion is
extremely tiny, and would probably not be enough to be "copyrightable
subject matter".  Sentences like "X is gay", "Y has had an abortion",
"Z has AIDS", etc., are not even copyrightable, but their dissemination
in certain contexts will have tremendous privacy implications.
"Technical enforcement of policies for the use of a file within a
computer system" is a pretty poor proxy for privacy.
This is not to say that trusted computing systems don't have interesting
advantages (and disadvantages) for privacy.

@_date: 2002-08-06 19:11:39
@_author: Seth David Schoen 
@_subject: Re: Privacy-enhancing uses for TCPA 
I would just like to point out that the view that "the protection of
privacy [is] the same technical problem as the protection of
copyright" is Microsoft's and not mine.  I don't agree that these
problems are the same.
An old WinHEC presentation by Microsoft's Peter Biddle says that
computer security, copyright enforcement, and privacy are the same
problem.  I've argued with Peter about that claim before, and I'm
going to keep arguing about it.
For one thing, facts are not copyrightable -- copyright law in the
U.S. has an "idea/expression dichotomy", which, while it might be
ultimately incoherent, suggests that copyright is not violated when
factual information is reproduced or retransmitted without permission.
So, for example, giving a detailed summary of the plot of a novel or
a movie -- even revealing what happens in the ending! -- is not an
infringement of copyright.  It's also not something a DRM system can
But privacy is frequently violated when "mere" facts are redistributed.
It often doesn't matter that no bits, bytes, words, or sentences were
copied verbatim.  In some cases (sexual orientation, medical history,
criminal history, religious or political belief, substance abuse), the
actual informational content of a "privacy-sensitive" assertion is
extremely tiny, and would probably not be enough to be "copyrightable
subject matter".  Sentences like "X is gay", "Y has had an abortion",
"Z has AIDS", etc., are not even copyrightable, but their dissemination
in certain contexts will have tremendous privacy implications.
"Technical enforcement of policies for the use of a file within a
computer system" is a pretty poor proxy for privacy.
This is not to say that trusted computing systems don't have interesting
advantages (and disadvantages) for privacy.

@_date: 2002-08-09 04:15:33
@_author: Seth David Schoen 
@_subject: Re: dangers of TCPA/palladium 
I heard a suggestion that Microsoft could develop (for this purpose)
a provably-correct minimal compiler which always produced identical
output for any given input.  If you believe the proof of correctness,
then you can trust the compiler; the compiler, in turn, should produce
precisely the same nub when you run it on Microsoft's source code as
it did when Microsoft ran it on Microsoft's source code (and you can
check the nub's hash, just as the SCP can).
I don't know for sure whether Microsoft is going to do this, or is
even capable of doing this.  It would be a cool idea.  It also isn't
sufficient to address all questions about deliberate malfeasance.  Back
in the Clipper days, one question about Clipper's security was "how do
we know the Clipper spec is secure?" (and the answer actually turned
out to be "it's not").  But a different question was "how do we know
that this tamper-resistant chip produced by Mykotronix even implements
the Clipper spec correctly?".
The corresponding questions in Palladium are "how do we know that the
Palladium specs (and Microsoft's nub implementation) are secure?" and
"how do we know that this tamper-resistant chip produced by a
Microsoft contractor even implements the Palladium specs correctly?".
In that sense, TCPA or Palladium can _reduce_ the size of the hardware
trust problem (you only have to trust a small number of components,
such as the SCP), and nearly eliminate the software trust problem, but
you still don't have an independent means of verifying that the logic
in the tamper-resistant chip performs according to its specifications.
(In fact, publishing the plans for the chip would hardly help there.)
This is a sobering thought, and it's consistent with ordinary security
practice, where security engineers try to _reduce_ the number of
trusted system components.  They do not assume that they can eliminate
trusted components entirely.  In fact, any demonstration of the
effectiveness of a security system must make some assumptions,
explicit or implicit.  As in other reasoning, when the assumptions are
undermined, the demonstration may go astray.
The chip fabricator can still -- for example -- find a covert channel
within a protocol supported by the chip, and use that covert channel
to leak your keys, or to leak your serial number, or to accept secret,
undocumented commands.
This problem is actually not any _worse_ in Palladium than it is in
existing hardware.  I am typing this in an ssh window on a Mac laptop.
I can read the MacSSH source code (my client) and the OpenSSH source
code (the server listening at the other end), and I can read specs for
most of the software and most of the parts which make up this laptop,
but I can't independently verify that they actually implement the
specs, the whole specs, and nothing but the specs.
As Ken Thompson pointed out in "Reflections on Trusting Trust", the
opportunities for introducing backdoors in hardware or software run
deep, and can conceivably survive multiple generations, as though they
were viruses capable of causing Lamarckian mutations which cause the
cells of future generations to produce fresh virus copies.  Even if I
have a Motorola databook for the CPU in this iBook, I won't know
whether the microcode inside that CPU is compliant with the spec, or
whether it might contain back doors which can be used against me
somehow.  It's technically conceivable that the CPU microcode on this
machine understands MacOS, ssh, vt100, and vi, and is programmed to
detect BWA HA HA! arguments about trusted computing and invisibly
insert errors into them.  I would never know.
This problem exists with or without Palladium.  Palladium would
provide a new place where a particular vendor could put
security-critical (trusted) logic without direct end-user
accountability.  But there are already several such places in the
PC.  I don't think that trust-bootstrapping problem can ever be
overcome, although maybe it's possible to chip away at it.  There is
a much larger conversation about trusted computing in general, which
we ought to be having:
What would make you want to enter sensitive information into a
complicated device, built by people you don't know, which you can't
take apart under a microscope?
That device doesn't have to be a computer.

@_date: 2003-10-13 06:44:16
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] LOCAL Mountain View, California, 
Sorry that didn't happen.  And I still haven't fixed TCPA.
Intel has posted its Policy Statement on LaGrande Technology:
LaGrande is in the interstices between TCG and NGSCB.  TCG has not
specified a secure I/O path or "curtained memory" as required by
NGSCB.  LaGrande does, so it effectively provides the complete
hardware support NGSCB would need.  (AMD has a similar project called
SEM, which I know very little about other than that it is supposed to
do similar things and at least one of the people working on it is
exceptionally honest.)
Anyway, Intel wants your comments on the LT policy.  The thing that
jumps out at me (as the author of "Trusted Computing: Promise and
Risk") is that Intel thinks that opt-out or opt-in can solve the
problems of attestation.  This is the official view of a lot of
trusted computing proponents.  The defects of this view are difficult
to describe and are complicated by the fact that some trusted
computing critics don't believe that LT (or TCG or NGSCB) will
actually provide an opt-out.  (I do believe this.)
The root of the difficulty is that, in the nature of attestation, you
can be _punished_ for opting out (beyond the scope of simply not
enjoying particular features to which what you opted out of is
technically necessary).  For example, if you have a feature with
privacy implications like What's Related in browsers, you can opt of
using What's Related and the only penalty will be that you won't see
what's related to the sites you're looking at.  Or if you don't like
Microsoft's software updates, you can opt out of those and the only
penalty will be that your software won't be patched.  (This is
actually a somewhat thorny issue since no other sources of patches to
Microsoft software have so far arisen.)
But in most other cases with which we're familiar, opting out has a
relatively narrow effect, and there is fairly little leverage to
punish you for having done so.  At least, that's true of opt-out
features in the context of technology choices; it might not be true in
some off-line situations.
In the nature of attestation and its effect on interoperability,
though, opting out of attestation might be ruinous for your hopes of
communicating with others.  If they can be induced to use proprietary
protocols or file formats, opting out may lead to a permanent
inability to exchange data with them.  Opting in, by the same token,
could lead to a permanent loss of software choice (and the effective
inability to reverse engineer or repair your software) at least during
the particular periods of time when you want to communicate with other
people or manipulate what they sent you.
Opt-in can't undo the harmful network effects attestation will produce
for competition and for all computer owners.
Anyway, that's what I plan to tell Intel, in somewhat more detail,
sometime before December 31.
And remember:
   [T]rusted computing systems fundamentally alter trust relationships.
   Legitimate concerns about trusted computing are not limited to one
   area, such as consumer privacy or copyright issues.

@_date: 2003-12-12 23:36:17
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Monday 15 Dec: first all-Open Source 
That's correct, absent other data migration or recovery plans
supported by software.
I was told that it doesn't relate directly.  But you could imagine
having devices as part of Secure Audio Path that have certificates
that can be verified by software running under NGSCB.  They can
encrypt their audio output so that it can only be read by those
devices, although there is no way to prove that the device itself is
on a local platform as oppose to a remote platform.  So one relevant
attack might be creating a program that streams audio output from a
DRM client to a sound card on a remote system instead of a sound card
on a local system.  You apparently still get only one audio output
stream per client -- you would just get to decide which computer it
plays on.
That sounds like an interesting area for more research.
Did Microsoft say anywhere that it was going to include document
revocation in Office?

@_date: 2003-12-12 07:32:31
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Monday 15 Dec: first all-Open Source 
I don't think the TCG TPM is the really fast processor you're looking
for; from all accounts I've heard, it's really rather slow.
I don't think "spy on the operating system" is quite the right term.
It might be better to say "prove that the relevant parts of the
operating system, if any, are completely unmodified".  There is no
intrinsic value judgment and there is also no attempt to identify
particular _ways_ of modifying the operating system as "bad".  At the
same time, you can also try to reduce the privilege of the operating
system to affect certain things, so that it may not matter to certain
secure operations whether the operating system is compromised or not.
There is an ingenious thing called a platform configuration register
(PCR) that contains hashes of running code.  The hardware is designed
in a such a way that, if the PCRs are used at all, at least the first
PCR will accurately reflect the code first loaded and to which control
was first transferred.
Because of the way the hardware is set up, if you load different code
(a different or modified nexus, for example), the PCR values are
guaranteed to be different.
To put this a different way, the hardware has (minimal, but
security-relevant) knowledge of what software is running.  If
different software is running -- for whatever reason -- the PCR values
will be different.
The encryption and decryption keys for the seal and unseal operation
are derived by hashing the PCRs, and the PCRs are derived by hashing
running code.  There is supposed to be no way to get arbitrary values
into PCRs (which is actually an oversimplification, but you can
pretend it's true) and so you can choose not to use them at all, but
you can't choose to load values into them that precisely correspond to
the values they would have been loaded with if you had booted an
operating environment other than the operating environment you
actually did boot.
Therefore, the _availability_ of a valid encryption or decryption key
inside the TPM -- to make an unseal operation work properly -- depends
on what software is really running.  You can have many different
operating environments installed on a single PC -- differing by a
little or a while -- and in principle they cannot unseal one another's
sealed data at all, because each one has its own family of PCR values
that results when it's booted.
There is no central CA necessarily implied by NGSCB.  Loading these
applets should require some kind of user decision and should not be
automatic based on presentation of a certificate.
The trusted computing applets are not supposed to have direct access
to any hardware (except that they can take input directly from the
keyboard when no other process is doing so, and they can write output
directly to the video framebuffer).  That is the NGSCB model.  So they
can't themselves directly "poke around your machine and your network";
they would need to have an ordinary (unencrypted) user-space agent
that does that.  In priciple, it should be possible to detect and
interfere with the operation of that agent by standard anti-virus or
IDS techniques.  If there is some reason that the user-space agent
can't be disrupted or detected -- which I don't imagine is the case --
then you have a severe problem.  But note that the trusted computing
applet can't authenticate the identity of the user-space agent code.
This part seems very possible.
Since the worm applet needs to use regular operating system services
to access the network, and since your computer is connected to a
network you can observe, you should be able to see the IP addresses of
the peers -- you can just run netstat, or you can run a sniffer, or a
firewall that monitors connection requests.  You can tell who the
peers are.  Of course, the worm can conceivably conceal where the
_ultimate_ destination is, if the worm network is a store-and-forward
kind of network.  In fact, the attacker who wrote the worm can join
the worm network in a way that looks like just another infected peer

@_date: 2003-12-12 07:11:47
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Monday 15 Dec: first all-Open Source 
... you might observe that they attained the first of the three
regulatory objectives described in the Content Protection Status
Report last month.
(If you have the ability to read PowerPoint documents, take a look
especially at the MPAA presentations at ARDG, like the introduction to
the analog reconversion problem and the MPAA reference model.)

@_date: 2003-12-11 20:45:17
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Monday 15 Dec: first all-Open Source 
You don't have to have a trusted computing machine to be harmed by
other people's abuses of trusted computing, because the most important
harms are from lost interoperability.  If people are precluded from
accessing some service unless they can prove they're using a
particular program, they may be worse off _whether or not they are,
and whether or not they can prove it_ compared to the status quo in
which nobody can prove it.
Having and using non-TC hardware _WILL NOT PROTECT YOU_ from being
punished by various entities for not using TC hardware.

@_date: 2004-01-02 20:47:25
@_author: Seth David Schoen 
@_subject: Re: [camram-spam] Re: Microsoft publicly announces Penny Black  PoW postage  project 
It seems like one risk for hashcash is that, when mailing lists are
whitelisted, a spammer can then use the lists to amplify spam (which I
think is what Richard Clayton was suggesting above).  For instance,
you generated a single hashcash stamp for cryptography of
the same value as the stamp you generated for richard
That stamp would hypothetically induce metzdowd.com to send your
message to _all_ of the cryptography subscribers, all of whom have
hypothetically whitelisted the list.  That means that, if your message
were spam, you delivered it to the whole subscriber base at very low
Or does hashcash only help moderated mailing lists (where it "pays"
the moderator for her time)?  My current impression is that it will
benefit individual e-mail recipients but not help subscribers to large
unmoderated mailing lists.

@_date: 2004-01-02 20:47:25
@_author: Seth David Schoen 
@_subject: Re: [camram-spam] Re: Microsoft publicly announces Penny Black 
It seems like one risk for hashcash is that, when mailing lists are
whitelisted, a spammer can then use the lists to amplify spam (which I
think is what Richard Clayton was suggesting above).  For instance,
you generated a single hashcash stamp for cryptography of
the same value as the stamp you generated for richard
That stamp would hypothetically induce metzdowd.com to send your
message to _all_ of the cryptography subscribers, all of whom have
hypothetically whitelisted the list.  That means that, if your message
were spam, you delivered it to the whole subscriber base at very low
Or does hashcash only help moderated mailing lists (where it "pays"
the moderator for her time)?  My current impression is that it will
benefit individual e-mail recipients but not help subscribers to large
unmoderated mailing lists.

@_date: 2012-05-11 23:34:21
@_author: Seth David Schoen 
@_subject: Re: [cryptography] Bitcoin-mining Botnets observed in the wild? (was: Re: Bitcoin in endgame 
It's wrong by three orders of magnitude because 1 TH/sec = 1000000 MH/sec.

@_date: 2013-01-03 01:25:15
@_author: Seth David Schoen 
@_subject: Re: Gmail and SSL 
You can see the current list of cert pins and HSTS preloads in the Chromium
source tree at

@_date: 2013-09-08 03:00:04
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Surveillance 
Do you think you could help distributors help other users achieve the
same result?  That's awesome.

@_date: 2013-09-07 16:18:42
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Surveillance 
No, but I think it's safer at several levels to have a meaningful way to
check that the code that you're running is the same as some
widely-published source code.
I actually think Raymond's Law is being proven wrong, at least for some
interpretations -- we don't seem to have the social structures or
incentives for very much free software source code to be proactively
reviewed by anyone other than its author, at least when the bugs aren't
obvious to end users in their daily use.  However, taking binaries from
someone without a way to know how they made them is an _extra_ level of

@_date: 2013-09-07 16:03:35
@_author: Seth David Schoen 
@_subject: Re: [linux-elitists] Surveillance 
I presume Don means that many Gentoo users are building most of their
binaries from scratch, while users of other distributions are accepting
binaries that their distributors compiled (and currently those
distributors don't have a simple way to prove that the binaries
correspond to the sources).
I think Debian has acknowledged that they have a real security risk here
and they're working on fixing it.  My understanding is that today they
still allow _individual package maintainers_ to ship (signed) binaries
directly to users based on the developer's claim that they built a
particular binary from particular source code.  (Note that the developer
might claim in good faith that they did so, but their laptop might be
compromised!)  But I think Debian is moving quickly to change this.
Perhaps Gentoo's design implies trusting fewer people or devices in this
respect right now.
