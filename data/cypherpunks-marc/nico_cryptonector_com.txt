
@_date: 2011-06-14 21:58:29
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
In a pinch crypto will not protect you.
Let's say you have an unbreakable code.  But there's still traffic
analysis, and even with onion routing and such, you don't know if your
peers are ratting you out, or if the state has infiltrated the onion
to a sufficient degree to find out a bit of what you're up to.  And so
on.  Push comes to shove, the truth is that power still comes from the
barrel of a gun.
Do the same thought experiment regarding cryptographic coins if you
like.  The state could easily make it so insignificant amounts of
business gets transacted in a cryptographic coin that the state cannot
subvert or control.
We use crypto to protect ourselves from non-state actors and
insufficiently motivated, _civilized_ state actors.  That's it.  And
even regarding non-state actors, you still depend on the civil law
system administered by the state.  If there's no state then you have
to arm yourself to the teeth and be willing to fight.
Uncivilized state actors will not give a damn about your crypto.  They
will torture you, your friends, your family.  They may enslave you.
Ultimately they may kill you.  The trick regarding uncivilized state
actors is to stay under their radar, or to take advantage of such
somewhat civilized subset of their behaviors as might remain, and
ultimately to get rid of the uncivilized state actors (by emigration,
revolution, or just waiting until you get lucky and they civilize on
their own).
Civilized state actors granting you due process of law might also not
give a damn about your crypto (see, e.g., the UK, where you can be
compelled, under criminal penalties, to produce cryptographic keys).
They won't torture you, etcetera, but they might deprive you of your
So: don't live in an uncivilized state (either civilize it or
emigrate), and then refrain from doing the sorts of things that might
cause even a civilized state to come down you like a ton of bricks
(i.e., refrain from committing crimes).  For added safety use crypto.

@_date: 2011-06-14 15:44:47
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
That's the question?  No.  The question is: how can you not be embarrassed?
Utter nonsense.  Crypto will NOT protect you from the state.
You use racial epithets and racist theories based on generalization
from anecdotes, and won't address that when challenged.  How can you
expect to be taken seriously?

@_date: 2011-06-14 06:03:36
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
Thanks for noting that and for going further.  Indeed, I'm rather
embarrassed to have been so shy in my own response.
Hear hear!

@_date: 2011-06-14 04:04:58
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
The idea that race is correlated with crime may be un-PC, but it might
also be correct.  Of course, an incorrect theory of race/crime
correlation might be offensive to people who do not subscribe to PC if
the theory were ill-informed, such as by being based on anecdotal
evidence only.  But the main thing is that the use of derogatory
racial or ethnic terms is neither PC nor polite.  I don't insist on PC
and wouldn't think much of anyone who did, but I do insist on a basic
degree of politeness (I've may have been annoyed to the point of
rudeness once or twice on a public mailing list, but never would I
utter an insult, much less a racial epithet).

@_date: 2011-06-13 16:06:33
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
You just proved the point: the market was distorted, with private
actors acting _within_ the distorted market parameters.  Thus people
who needed to make low-risk investments did make what _seemed_ like
low-risk investments (after all, real estate had been a low-risk
investment for decades in the U.S.), but actually were not just
high-risk, but bound to fail.
You can blame the derivative sinners (pun not intended) all you like,
but there's an original sin here.  Everyone else was either fooled
into sinning, peer-pressured into it, or outright forced, and though
there surely were some who understood what was happening and sought to
profit from it, you can hardly blame them either -- we all do
something of the sort (if you see inflation coming and manage your
money accordingly, are you ripping off all those who can't or don't
know to do anything about inflation? and if so, are you a terrible
person for it?).
The whole tranching thing was almost brilliant, and would have worked
out fine (securitized mortgages from the 80s seem to have done fine,
no?) if there had been no bubble (but in a bubble the securitization
helped it along), and if all the issues in tracking the underlying
loans (and thus pricing the securities) had been worked out correctly.
Yeah, well, we need a sub-list for OT discussions.  At Sun we used to
have lists with sub-lists named the same + a "-extra" suffix, where
people who wanted to participate in these sorts of long, flame
war-ish, OT discussions could.

@_date: 2011-06-13 15:29:56
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
I agree with much else of what you wrote in your post, but I take
issue with two things here.  First, there were plenty of middle class
(and better off) people who used their ever-increasing home values as
an ATM card.  The stories I've seen are hair-raising.  Second, we
don't need to use derogatory terms here.  There's a difference between
being polite and being PC, such as that PC involves choice of ideas,
not just choice of words.
I particularly agree that CRA and Frannie primarily set in motion the
market dynamic that led to either the bubble itself or its ultimate
size, or both.  There's straightforward evidence: total up the amount
in securities sold by Frannie and the amount they were left holding
and the amount pumped in by the feds, and you're up to on the order of
$1 trillion, which is a large portion of the losses in the bubble's
popping.  Of course, the losses have been larger than $1 trillion
(because the $1 trillion provided the necessary momentum to get the
bubble inflating and stay inflating, but much more liquidity flowed
from elsewhere so as to not miss out on the "opportunity"), so one has
to consider whether Frannie was along for the ride in a bubble that
wasn't their fault, or whether Frannie caused the bubble.  But any
time you have a government taking such enormously distorting actions
it's difficult to argue that they couldn't have been the cause of the
crash -- one has to be suspicious of artificial market distortions.
Exactly.  Conformity was enforced, and it wasn't all that hard.
"Look, you have to make bad loans.  Yes, we know that's insane, but
here's the deal: you package them up and sell them immediately, and if
you can't find a buyer in the market, well, that's what Frannie's here
for, they are the buyer of last resort.  So go on, make bad loans
without fear!"  The rest follows.  Though the bargain was never that
explicit.  Liar loans?  Pfeh.  Of course you'll have liar loans in
such an environment, but it hardly gets the government off the hook
that their actions had the consequences that people did predict (well,
I seem to remember arguments in 1992 and 1999 about the wisdom of
these policies -- it's almost certain that there were cassandras).
It wasn't just acquisitions.  If you needed capital for anything and
your returns were low relative to the rest of the game-playing
industry, then you lost out.  In order to stay competitive when
looking for capital you had to stay competitive in terms of returns,
which could only happen (while the bubble was inflating) if you
participated in the bubble.  This shows that the market distortion
needed to get and keep the bubble going didn't have to be all that
large because there was a leveraging effect in the industry.

@_date: 2011-06-13 05:39:15
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
Who said anything about poor people?  Just because CRA and Frannie
were sold partly as programs that benefit the poor doesn't mean that
was either the actual intention, or the effect.  The people who pushed
liar loans did it in large part because there were large incentives to
doing that, which were mostly a result of not playing with their own
nor their bosses' money, but with funny money, since no matter what
loans they made there was always a ready buyer in Frannie, backed with
a ready lender of last resort in the Fed.  Yes, people who made funny
loans committed sins, but they wouldn't have been able to (not in the
same scale anyways) without the larger, more original sin committed by
the politicians (who did it in the name of the poor, or whatever).  IF
Congress had created an above board $100 billion/year housing subsidy
for the poor, that wouldn't have led to a bubble, and it would have
helped the poor much more than the actual, hidden $100 billion/year
housing subsidy did.  If you care about the poor you won't defend CRA
and Frannie blindly.

@_date: 2011-06-13 04:57:58
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
Perhaps it's because I earn some of my living from a financial
institution (but I doubt it, since I held this opinion back when I
didn't), but I don't think it's fair to blame private financial
institutions for the ill-effects of an ill-advised government plan to
subsidize housing ownership by individuals.  Without Frannie, CRA, or
anything of the sort I don't think we'd have seen the degree of
financialization of housing that we saw, meaning that we wouldn't have
seen the home mortgage credit growth that drove the housing bubble,
thus neither the bubble nor the crash.  (Well, bubbles can happen
without the help of the government, so let's say that the likelihood
of such an immense bubble would have been pretty low without Frannie
and CRA).
Now, financial institutions clearly played a role, but mostly it was a
fee-taking role (since they mostly passed mortgages through to
Frannie), and it was a role they had to play (see CRA).  Some played a
role in the securitization of lousy mortgages, but I'm not sure that
they understood the systemic risk -- the securities' buyers certainly
didn't, even though most were also financial institutions with
sophisticated people in charge, so it's not too much of a stretch to
think that this was all really just a necessary consequence of CRA and
Frannie.  That's my theory anyways.

@_date: 2011-06-13 04:44:56
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
Yes, exactly.  If you want stable money then you need either a natural
resource with naturally limited growth (e.g., gold) (and some other
features), or an artificially limited resource (e.g., paper money).
The latter is always susceptible to politically caused inflation.  Any
cryptographic coin based on proof of work is a disaster because of
Moore's law and because the "work" is wasted energy -- and energy,
you'll note, is a very precious resource.  Ergo, a cryptographic coin
needs to be based on trusted issuers (or something else I've not seen
before nor thought of), and that means, effectively, a fiat currency.
Crypto is NOT a solution to political problems; never has been, never
will be.
Here's a thought experiment: if the present value of all actual,
tangible property, things, capital (production and service capacity),
as well as less tangible things such as people, and institutions
-altogether, basically, a nation's patrimony- were far, far exceeded
by nominal value stored in money in banks and mattresses, would that
money really be worth all that much?  I suspect that the answer is
"no".  I suspect that the most valuable feature of money is not to
store value in the long-term, but to lubricate commerce, that is, to
enable transactions on a basis better than barter.
It's always possible to get some people to do things that are not in
their interest.  See various cults.  (Note: I'm not saying bitcoin is
a cult.)

@_date: 2011-06-13 03:54:57
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
Doesn't help.  The trials would be political trials, and it's all
politics, which in its most naked form is "who has the guns," and next
most is "who has the votes."  Truth is not dispositive in politics,

@_date: 2011-06-13 03:51:23
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
Clearly.  The nice thing about the U.S. was, and still might turn out
to be once the current statist episode is over, that its political
system didn't stray too far in the direction that so many democracies
have tended to in the 20th century.
Clearly, but at least as long as you have property rights and there's
no capital controls to speak of you can work around those gyrations
(really, long periods of low, but higher than gold inflation, followed
by short but sharp credit contractions, with occasional bouts of
hyperinflation, but with the degree of inflation being a political
problem, thus manageable in countries with institutions and political
traditions that value low inflation).
You might note that in the long run all fiat currencies will be
debased, but you might also note that in the long run all gold
standards tend to get abandoned.  The problem is democracy, which you
might remember is the worst form of government _but for all the
Let's not exaggerate, total power looks a bit different than what you
see here, so far.  Granted, it's getting there, but there's hints that
the ship of state might yet right itself, and if not, well, there's
not a lot of options (obcrypto: and crypto won't get you any real
The real issue in Europe is that if Greece (and Portugal, and Ireland,
...) default then most of the banking system in the rest of Europe
will be insolvent, which means that either they are allowed to fail,
and people are allowed to lose part or even most of their deposits, or
their debt (deposits) will have to be nationalized and monetized, all
in a credit contraction environment (deflation).  Either way real
wealth has been frittered away, destroyed, and no one wants to be the
one to tell the public that they are poorer, thus the game is to make
the process by which the loss of wealth becomes apparent take much
longer, which only delays real recovery, thus making things worse.

@_date: 2011-06-11 22:53:32
@_author: Nico Williams 
@_subject: [cryptography] Digital cash in the news... 
+1.  A fiat currency with no capital controls and reasonably free
trade is probably the best currency system yet.  Details do matter
though.  It helps when the issuer doesn't inflate, for example.
Still, the U.S. dollar has been that sort of currency since the 70s,
and it's worked out rather well.  (Which isn't to say it will
continue, but if it doesn't, it won't be due to any flaws in this
currency system.)
A simple digital coin would be one with double spend detection, and
blind signatures for anonymity.  Double spend detection is a problem,
because it requires online infrastructure, which then becomes a
super-critical part of the economy, but I'm not sure how we can avoid
it.  The proof of computation idea is a total waste of precious energy
(check the news, energy shortages are likely to be a common problem in
Japan and Europe as a result of Fukushima, and probably elsewhere
Good point.  It's all in what's in that basket, and the rate of
transactions (i.e., whether people use this thing).

@_date: 2011-09-23 15:09:47
@_author: Nico Williams 
@_subject: [cryptography] Nirvana 
That'd be nice, but since the financial system ultimately has to make
payments work between any pair of peers, this doesn't work -- Visa (or
whoever) cannot attest to the trustworthiness of everyone.  Also, what
if we had real cryptographic money, with anonymity?  In other words:
the payments system cannot be the trusted third party for everything.
The financial industry could be/have a trusted third party for
authenticating its members to their customers (and prospective

@_date: 2011-09-22 22:33:57
@_author: Nico Williams 
@_subject: [cryptography] Nirvana 
It could vary.
For low-security applications, like blog comments, yes, leap-of-faith will do.
For a medium-security application, like shopping (where systems like
credit card fraud protection render the risk to the user low),
security bootstrapped from leap-of-faith + trust-building or trusted
third parties will probably do.
For high-security applications (like banking) you'll generally want to
bootstrap security via something else, either an off-line interaction,
or a trusted third party that can authenticate relatively few peers to
you (and thus is probably more trustworthy w.r.t. verification of your
peer's credentials).

@_date: 2011-12-09 23:28:52
@_author: Nico Williams 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
I really would like the Android model to be elaborated on a fair bit.
Users should be able to deny apps privileges that they request.
Users should be able to label data with simple labels for additional
isolation (think of it as multiple instances of apps).
How does this relate to crypto?  Right, not at all, except through the
use of digital signatures strictly for continuity and pseudonymous
identification (public key == ID).
cryptography mailing list

@_date: 2012-03-27 17:17:31
@_author: Nico Williams 
@_subject: Re: [cryptography] Key escrow 2012 
Well, the context was specifically the U.S. government wanting key
escrow.  That's not feasible because the national security
establishment will win any fight over this with law enforcement. The
U.S. govt is not a monolythic entity...
As for corporate networks, yes, and often we already have this in the
form of MITM TLS boxes, with users having to install trust anchors for
them.  And, really, for e-mail security needs to be between domains,
not between users, which is roughly equivalent to saying that users
should have no privacy vis-a-vis their mail servers.  But that's
another topic :)
cryptography mailing list

@_date: 2012-03-26 04:54:13
@_author: Nico Williams 
@_subject: Re: [cryptography] Key escrow 2012 
No, probably parts of it: the ones that don't have to think of the big
picture.  The U.S. government is not monolythic.  The NSA has shown a
number of times that they are interested in strong civilian
cryptography for reasons of... national security.  In a battle between
law enforcement and national security the latter has to win.
Key escrow == gigantic SPOF.  Even if you split the escrow across
several agencies and don't use a single master key, it's still
concentrating systemic failure potential into too few points.  To
build a single point of catastrophic failure into one's economic
infrastructure is one of the biggest strategic blunders I can imagine
(obviously there's worse, such as simply surrendering when one clearly
has the upper hand, say).  Back in the early 90s this probably wasn't
as clear as it is today.
Most users already pay heavy battery/performance taxes in the form of
uninstallable adware built into their devices.  The vendors might be
the ones to object then since they might have to stop shipping such
software.  But ultimately this argument depends on how heavy a burden
the users end up feeling.
For my money the winning argument is the strategic idiocy/insanity of
unnecessary SPOFs.  Who wants to ever even think of saying to the
POTUS "Mr. President, we have a mole, they've stolen the codes for our
civilian networks and they've shut them down from the people's shear
fear of financial and other losses. It will take months to re-key
everything and in the meantime we'll lose X% of GDP. The stock and
bond markets have crashed."  As time passes X will tend to increase in
the event of such a catastrophe.  The higher that percentage the more
crippling the attack, with derivatives losses becoming overwhelming at
small values of X.  It could get worse: "Mr. President, we can't even
re-key without changing all these hardware dongles that are
manufactured by the enemy, who's now not selling them to us."
If the point of key escrow is to make law enforcement easier then
there are much simpler non-cryptographic solutions -- not ones to your
taste or mine perhaps, but certainly ones that don't involve strategic
I'm with you: key escrow is necessarily dead letter, at least for the
time being and the foreseeable future.
cryptography mailing list

@_date: 2013-04-07 06:38:14
@_author: Nico Williams 
@_subject: Re: [cryptography] ICIJ's project - comment on cryptography & tools 
Well, it's like a pendulum.  As China and others make use of "cyber"
warfare to fight wars by proxy the comsec folks will regain the upper
hand at NSA.  Or so we should hope.  We can be secure in our comms and
have a hard time eavesdropping on anyone or we can be insecure in our
comms and have a hard time eavesdropping on anyone other than our own.
 It's pretty obvious, no?  we need strong civilian crypto.
On the flip side, no amount of crypto can get one past certain
fundamental issues in security.  How do you know your peer is who you
think it is?  Crypto can't truly answer that, much less the question
of whether they are doing as you wish.
Oh, well, we don't need to resort to conspiracy theories to answer
_that_.  We've built a house of cards, not so much on the Internet as
on the web (but not only!).  Web application security is complete
mess.  And anyways, we build on foundations, but the foundations
(operating systems) we built on are now enormous and therefore full of
vulnerabilities.  We're human -fallible-, and our systems reflect this
-our failures-.
cryptography mailing list

@_date: 2013-04-06 04:27:46
@_author: Nico Williams 
@_subject: Re: [cryptography] ICIJ's project - comment on cryptography & tools 
It'd be nice (for good guys certainly) to be able to open-code
everything that one needs, or otherwise review all of the source code
to the object code that one needs.  In practice you cannot do this.
It's ETOOMUCH.
In the worst case scenario for the LEA there's still traffic analysis
and warrants/court orders/rubber hoses that they can resort to.
Crypto only helps the good guys w.r.t. bad guys and other governments
(and then only sometimes); crypto is just a polite way of saying "try
harder, get a warrant" to the LEA with jurisdiction over you (or your
devices).  For LEA my guess is that the biggest problem isn't how to
get at evidence, but how to know who the bad guys are: in a sea of
traffic it's hard to tell when you don't even know what's needles and
what's hay, which must be why LEA tend to have such a dislike for good
guy crypto.  We hope the NSA types haven't forgotten that good guys
need crypto, whether LEA like it or not.
cryptography mailing list

@_date: 2013-04-04 21:39:31
@_author: Nico Williams 
@_subject: Re: [cryptography] ICIJ's project - comment on cryptography & tools 
But note that this doesn't mean that iMessage can't be MITMed or
otherwise be made susceptible (if it isn't already) to MITM attacks or
plain traffic analysis.
iMessage relies on Apple as a trusted third-party.  Therefore Apple
can MITM its users.  The best case scenario is that the iMessage
clients can add jey pinning to force the TTP to either never MITM or
always MITM any pair of peers.  But since the TTP also distributes the
client software...
Online we have lots of security problems that are difficult to
resolve, from physical security of devices (there's not enough) to the
lack and general difficulty/impossibility of reliably open-coding or
reviewing everything that one has to trust (mostly software, and some
firmware too).
Basically, this is complaint by the DEA is disinformation or
misinformation (or both!).  If the former case we might even be
staring at the start of a new crypto wars period.
cryptography mailing list

@_date: 2013-08-16 19:32:12
@_author: Nico Williams 
@_subject: Re: [cryptography] LeastAuthority.com announces PRISM-proof storage service 
That's fair, and true-enough, although you never know.  pwning
everyone is a very costly operation: you can only do it once for each
pwn, and the political risks and costs are high enough to put the
entire concept at risk.  But we've seen actors take some breathtaking
risks in recent years (e.g., Flame)...
That's fair, and a point that I should learn to make in general.  We
saw China back down from banning github -- that's a big clue that
sufficiently popular services have leverage against foreign
governments, and possibly local ones too.
cryptography mailing list

@_date: 2013-08-13 20:16:33
@_author: Nico Williams 
@_subject: Re: [cryptography] LeastAuthority.com announces PRISM-proof storage service 
This is the only way in which crypto helps against the PRISMs: when
legitimate business interests come to depend enough on services that
can neither easily be compromised by the PRISM nor easily be shut off
because of the large dependence on those services.  That's really more
a political effect than a technological one, though facilitated by
Nothing really gets anyone past the enormous supply of zero-day vulns
in their complete stacks.  In the end I assume there's no
technological PRISM workarounds.
cryptography mailing list

@_date: 2013-08-13 20:10:36
@_author: Nico Williams 
@_subject: Re: [cryptography] LeastAuthority.com announces PRISM-proof storage service 
A few points:
 - if only you access your own files then there's much less interest
for a government in your files: they might contain evidence of crimes
and conspiracies, but you can always be compelled to produce those
 - if you share files then traffic analysis will reveal much about
what you're up to, and there may be much interest in getting at your
files' contents.
 - commercial operators who give you software to run can compromise
(or allow governments to compromise) you even if they are not
technically an end-point[*] for your end-to-end protocols.
 - it's really not easy to defeat the PRISMs.  the problem is
*political* more than technological.
 - i'm not trying to detract from Tahoe-LAFS -- it's a spectacular
idea, I wish it well, and I generally endorse filesystems of this
[*]  In Tahoe-LAFS, ZFS, and any other similar filesystems, there is
only one end-point: the client(s); the server, in particular, is NOT
an end-point.
cryptography mailing list

@_date: 2013-08-22 16:40:32
@_author: Nico Williams 
@_subject: Re: [cryptography] urandom vs random 
Applications have no business specifying such things.  Users/sysadmins
do have a need to be able to configure HW RNG sources, but not in a
weighed manner like that.
Say you have N>1 HW RNG sources of differing quality (and output
rate!).  Why bother taking different numbers of bits from each to form
an input to the SRNG pool?  Just take whatever each source has
available and feed it all in.  If your SRNG is any good this is good
What I'd like is for the HW RNG source configutation to be made very
clear to users: at boot time, at login time, when source availability
changes, and at critical secret or private key generation times.  That
last is difficult without changing implementations of all sorts of
My suggestion is /dev/urandomN where N is one of 128, 192, or 256, and
represents the minimum entropy estimate of HW RNG inputs to date to
at read(2) time, then block, else never block and just keep stretching
that entropy and accepting new entropy as necessary.
cryptography mailing list

@_date: 2013-08-17 01:24:10
@_author: Nico Williams 
@_subject: Re: [cryptography] urandom vs random 
It might be useful to think of what a good API would be.  I've thought
before that the Unix everything-as-a-file philosophy makes for lame
entropy APIs, and yet it's what we have to work with...
I'd like something like /dev/urandom128 -> min. 128 bits of real
entropy in the pool.
I'd also wish open(2) of AF_LOCAL socket names were the same as a
connect(2) on the same thing, and to block like named pipe opens do
(why on Earth is this not so?  what could possibly break if it were
so?  considering that named pipe opens block... one would think
"nothing could break").  Then we could have each open of /dev/prngN
result in a PRNG octet stream seeded by N bits of real entropy.
(I saw a blog post recently about using AF_LOCAL sockets as PID files.
 Making open(2) of them == connect(2) to them would make that an
awesome idea.)
cryptography mailing list

@_date: 2013-08-17 21:20:45
@_author: Nico Williams 
@_subject: Re: [cryptography] Reply to Zooko (in Markdown) 
A more interesting approach would be to use a variety of independently
sourced disassemblers to compare builds and check that object code
differences from one build to the next can be accounted for by
corresponding changes to the source code or build systems.  This is
not really tractable when you change compilers or their settings, but
at least you can get a pretty good idea as you develop of what object
code is being produced.  This is terribly time-consuming, but you can
automate the comparison process and archive results for post-mortems
as a deterrent.  You'd have to do this on multiple machines handled by
different people, and so on...
It's not too farfetched, see (Solaris release engineering used to use this tool, and I imagine that
they still do).
No one can.  We're in luck w.r.t. the Thompson attack: it needs care
and feeding, as it will rot if not kept up to date.  Any effort to
make it clever enough to keep up with a changing code base is likely
to lead to the attack being revealed.  Any effort to maintain it risks
detection too.  Any effort to use it risks detection.  And today a
Thompson attack would have to hide from a multiplicity of disassemlers
(possibly run on uncompromised systems), decompilers, and, of course,
tracing and debugging tools that may work at layers that the generated
exploit cannot do anything about (e.g., DTrace) without the bugged
compiler having been used to build pretty much all of those tools.
That is, I wouldn't worry too much about the Thompson attack.
Yes, it's turtles all the way down.  You stop worrying about far
enough turtles because you have no choice (and hopefully they are too
"far" to really affect your world).
Indeed, the vulnerabilities from the plethora of bugs we
unintentionally create, overwhelm (or should, in any reasonable
analysis) any concerns about turtles below the one immediately holding
up the Earth.
cryptography mailing list

@_date: 2013-09-07 07:50:23
@_author: Nico Williams 
@_subject: Re: [cryptography] Compositing Ciphers? 
We have a purely (now mostly) all-symmetric key protocol: Needham-Schroeder

@_date: 2013-09-07 01:24:44
@_author: Nico Williams 
@_subject: Re: [cryptography] Compositing Ciphers? 
The list of things to get right is long.  The hardest is getting the
implementation right -- don't do all that work just to succumb to a
remotely exploitable buffer overflow.  Next up is physical security.
Then key management.  Then all the crypto stuff (ciphers, modes, MACs,
hash functions, ...).  Then the RNG....  That's assuming off-the-shelf
crypto algorithms.
And then there's your trusted insiders/counterparties.  They are your
biggest risk of all, or possibly second biggest, after plain old
buffer overflows and similar.
cryptography mailing list

@_date: 2013-09-07 00:58:03
@_author: Nico Williams 
@_subject: Re: [cryptography] Compositing Ciphers? 
My own very subjective opinion is that assuming all of: constant time
implementations, an appropriate cipher mode, proper {key management,
RNG, local end-point security}, then AES is perfectly safe.
Of course, that's a lot of assumptions!  You'll almost certainly fail
at the local end-point security part.  Long before your choice of
ciphers is attacked your systems/protocols will have succumbed to
other, cheaper attacks -- assuming they are targeted at all.
If you have the hardware for it, that's fine.  I wouldn't bother
composing ciphers in any given layer.
Well, yes, it's been studied.  Look for papers on 3DES, for example.
Make sure not to make mistakes that leave you susceptible to
meet-in-the-middle type attacks.  But, really, first make sure that
you've covered the other bases, the ones that are going to be your
achilles' heel if you don't, such that your adversaries have no choice
but to attack the crypto.  THEN concern yourself with improving the
IMO.  Also, IANAC.
cryptography mailing list

@_date: 2013-10-19 00:15:15
@_author: Nico Williams 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Didn't I say that in exactly the next sentence?!
You went to all the trouble of quoting this one sentence separately from
the preceding and following sentences, then corrected me with the
contents of the very following sentence...  Why?

@_date: 2013-10-18 23:08:34
@_author: Nico Williams 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
The problem is that many apps expect /dev/urandom never to block.  This
is a severe problem if such an app is invoked early in boot and blocks
the rest of the bootup procedure.  But, then again, that would be a
serious bug, therefore blocking until seeded would be very useful
behavior: it would allow one to find such bugs.
Now, once seeded, /dev/urandom should not block again (apps that use
should get periodically reseeded.

@_date: 2013-10-31 06:57:10
@_author: Nico Williams 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
I've been wishing it were so for many years.  Certification can be
everything to a vendor, and when the standards and labs require a bad
result, the vendor provides it.  Management demands it, the engineers
that don't like it move on to other projects, and the ones that don't
know better do what they're told.  It's just too difficult to argue with
anyone about the requirements that must be met to get a certification.
Or at least that's the perception.  That perception is wrong: I've
actually succeeded in getting NIST to change a proposed requirement in
the past (or so I was told after the fact, verbally; my emailed comments
actually went unanswered at the time).  But not that long ago colleagues
running a product past certification were emphatic that changing the
requirements was just too difficult/expensive/not worthwhile.
Standards can be a source of systemic risk.  This is true even in the
absence of conspiracies: just because of inertia.  Standards can also be
a source of stability and can prevent serious mistakes -- also because
of inertia.  Lack of standards is worse.
It's a reasonable explanation.  For 2004.  It's still believable in 2013
because most of us don't deal in FIPS-anything, so we can (and do) shrug
it all off.  But it's also a bit embarrassing.  As someone here recently
said (Peter Gutmann, IIRC), we thought "oh, you were serious about
that?!".  But, yes, standards with certification lab enforcement are
serious, therefore we should not shrug them off.  AND, in order to do
that, the standards bodies in question have to be accessible.

@_date: 2013-11-04 22:51:25
@_author: Nico Williams 
@_subject: Re: [Cryptography] /dev/random is not robust 
Not at all.  Encrypted swap should be added after local filesystems are
mounted and *RNG state seeded.  (And, by implication, it should be
possible to run without swap.)
Break up and re-arrange bring-up as needed to get things right.
Also, *RNG state seeding could be done at multiple points in time.
Where there's RDRAND-like HW entropy sources then use those immediately
upon startup, else use a seed file from the RAMfs miniroot, then re-seed
when / (and /var) is (are) mounted, then re-seed when device drivers for
other HW entropy sources are loaded, then re-seed using jitter data,
then re-seed using interrupt data...
Aside: can the bootloader do anything to provide the kernel with entropy?
Also, ASLR should use /dev/urandom, and perhaps init and friends should
re-exec themselves after proper seeding.
A bigger problem on RDRAND-less tick-less systems is power management:
waking up to sample HW entropy is not an option.  This might lead to
higher latency for /dev/random on such systems.  But I think this is
manageable, besides /dev/*random should work like a periodically
re-seeded SRNG, so there should be no need for /dev/random to block once
properly seeded.

@_date: 2013-11-08 21:31:03
@_author: Nico Williams 
@_subject: Re: [Cryptography] randomness +- entropy 
Good.  We could argue about how slowly entropy gets consumed as outputs
of a properly seeded PRNG/SRNG get produced, which I think should be...
...incredibly slowly, at least by 2^-n for every unit of output where n
is the number of bits of entropy estimated in the RNG's state.  So if
you have 256 bits of entropy to start with you'll never in a million
years (figure of speech) end up with less than 180 or so bits of entropy
left.  Add to that periodic TRNG inputs to provide fast recovery from
state compromises and what more could we want?
It only needs to have high entropy, unpredictable state.  Given a
mixer/extractor design that does not cause entropy to go down much at
all just because output bits are extracted... it doesn't matter if
(an SRNG, basically) as opposed to just a TRNG.  With proper crypto no
harm could come from that, nor could anyone distinguish one from the
Given RNGs whose states' entropy is difficult to consume there should be
no real difference between /dev/random and /dev/urandom once they've
been properly initialized, especially if they are both designed to be to
recover quickly from one-time state compromises.  Or, if there should be
a difference, a) what is it, b) why should I care, c) how could I tell?
If indeed there is no need for there to be a difference between the two
[once properly initialized] then why do we have the two interfaces?  And
also, (2a) would follow naturally.
The key here is "once properly initialized".
 - /dev/random should block whenever it hasn't had fresh entropy mixed
   in in the past N seconds or if it has not yet been properly seeded.
   I.e., guarantee strong and robust outputs.
 - /dev/urandom should block if it has not yet been properly seeded,
   then it should never block again.
   I.e., guarantee strong outputs.
 - Both should get fresh entropy mixed in frequently (at least as
   frequently as outputs are demanded).
 - Nothing in the boot sequence should need entropy before /dev/urandom
   has been seeded properly.  Anything that does should be modified not
   to.
   For example, ASLR should not require strong RNG outputs until
   /dev/urandom has been seeded, and then init(1M) and any other
   long-running processes should restart.  (Solaris' init(1M) can be
   restarted, so this is not farfetched.)  Seeding of the RNG should be
   done as early as possible in the boot sequence.  Sources of entropy
   should include:
    - RDRAND or similar
    - jitter (requires hi-res CPU cycle counters and timers)
    - async event timing (interrupts)
    - a seed in the RAMfs boot image
    - a seed in /var saved at last boot
    - a seed in /var saved at last shutdown
    - datetime, CPUID, ...
    - network entropy servers
   in roughly that order.  Only jitter entropy should be trusted for
   estimating initial RNG state entropy, but some of each of the others
   must be required.
Once seeded, and if frequently reseeded then it's the same as
contract where each open file descriptor represents an instance of a
seeded-one-time-only PRNG, but I don't think anyone needs this, and
anyways, most apps open, read, close /dev/urandom rather than keep it
Not really.  The two pseudo-devices have to be reasonable
high-bandwidth, and [once properly seeded] secure under fairly broad
threat models.  So starting from that premise the goal is a
fast-but-secure mixer/extractor with TRNG inputs fed to the mixer as
often as possible (though not on a timer, for power mamangement reasons,
so much as on demand as read()s are done).
If entropy isn't consumed linearly (but way sub-linearly) with demand
then I don't see this trade-off.  You just agreed with my statement that
that is a desirable property.
No, we should demand 128 bits of entropy before anything useful can be
done with /dev/urandom's output.  The only question is: whence the
entropy -- we always end up at this very first square.
Right!  We're not extracting entropy.  We're extracting outputs from an

@_date: 2013-11-08 01:46:05
@_author: Nico Williams 
@_subject: Re: [Cryptography] randomness +- entropy 
That's a strawman.  I never said that's a definition of a PRNG (good,
bad, whatever).
I also never said anything about provability.
I was only arguing that consuming n bits of PRNG output != lowering the
PRNG's "entropy" by n bits.
Think DTrace.  The debugger might only be permitted to read.

@_date: 2013-11-07 22:06:30
@_author: Nico Williams 
@_subject: Re: [Cryptography] randomness +- entropy 
That's *not* supposed to be the case.  That is, a good PRNG does not
allow an attacker that observes some of the PRNG's output to use it to
guess future outputs.  Obviously the security of a PRNG will decrease as
the attacker observes more and more outputs, but, given a random and
unpredictable (high-entropy) initial state of N bits, the PRNG's
resistance to such attacks will be reduced by one bit (N--) for each bit
observed by the attacker!
A PRNG with n bits of high-entropy state should provide as-good-as-
brute-force protection.  Allowing the attacker to observe one output of
the PRNG should reduce the attacker's work factor by 2^-n, not by a
factor of 2!
The PRNG needs an estimate of security relative to attackers that get to
observe some (many, most, all) of the PRNG's outputs.  This estimate is
not the same thing as an estimate of the PRNG's state's entropy.  This
estimate of strength should go down very slowly as outputs are produced,
and it should go up sharply when new trusted seeds are consumed.
An estimate of entropy is useful for protection against generating keys
from a not-yet-(or unsafely)-seeded PRNG.  Once the PRNG is seeded with
with enough entropy, the PRNG's state's entropy estimate should be
largely irrelevant because the more interesting question becomes: how
resistant is the PRNG is to guessing by attackers that get to observe
its outputs.
I.e., ssh-keygen might want to demand /dev/urandom outputs with N>256 bits
of entropy and 2^256 brute-force-equivalent cryptanalysis resistance.
Periodic re-seeding with high-quality seeds is necessary for robustness
reasons: to recover quickly from state compromises (e.g., a sysadmin
with root access using a kernel debugger to get at the PRNG's state and
using this to escalate privilege once the debugging session is over).
Ideally /dev/urandom gets re-seeded on every read using HW RNG outputs.
Then occasional state compromises become almost a non-issue; sustained
compromise == complete compromise.

@_date: 2014-01-03 20:28:28
@_author: Nico Williams 
@_subject: Re: [cryptography] pie in sky suites - long lived public key pairs for persistent identity 
But clearly you must not be.
If you want to assume quantum cryptanalysis then you should only use
ECDH when you can protect the public keys with something like NTRU
(that is, if you must exchange public keys over an insecure network at
all) that we think is impervious to quantum cryptanalysis.  Once you
have that then IMO the DJB curves look pretty good.  Once you have
session keys you can use AES in any reasonable AEAD mode (by generic
composition with HMAC, with SHA-3, GCM, whatever) if you like (and I
would, provided the implementation is constant-time).
Why do you need working keys?  Mostly for session management reasons
(traffic analysis alert!).  If you can avoid the need for
distinguishing between long-term and working keys and you can
physically distribute public ECDH keys and then keep them secret then
you don't even need NTRU.
cryptography mailing list

@_date: 2014-03-25 02:52:58
@_author: Nico Williams 
@_subject: Re: [cryptography] Compromised Sys Admin Hunters and Tor 
[The context was sysadmins, who generally wield a lot of power.]
Anecdotal, yes.  I'm not sure if I'm at liberty to discuss any of the
events of which I have close knowledge, though one of them was in the
news at the time (that is, I'm not sure if I'm at liberty to discuss
the details).  In the largest incident I've close knowledge of a
laid-off sysadmin left a time bomb in thousands of servers that caused
significant downtime for the business' customers.
And then there's Mr. Snowden...
...and the long line of insiders who spied against their nations,
versus the number of outsiders who made it through whatever
technological barriers were in their way.
Even if you limit yourself to the Internet era, the most famously
damaging attacks I can think of were all insider attacks.  Many were
not "attacks" in the sense of "security attacks" like buffer
overflows, say, but rather in the sense of actions that went beyond
legitimate access and badly damaged a business (Nick Leeson, anyone?).
It stands to reason that insiders who have vast and/or intimate
knowledge, and legitimate access to a business' resources, have a lot
of power to cause damage.  By definition they have more capacity to
cause immediate damage than outsiders.  Whether insiders are the
biggest threat in the sense of probability is, of course, not easy to
predict and largely irrelevant: they are the first threat to protect
I'm not sure that empiricism has any place in this very particular
matter; without the insiders on your side, you stand no chance against
outsiders.  So I'm not sure what you're asking for...  Even if there
was little data as to actual attacks by insiders, that would not mean
that insiders are not a danger, and even if individual insider risk
were empirically far lower than outsider risk, that would not mean
that the total damage an insider could cause is far less than that
which outsiders can cause.
Which isn't to say that outsiders must not be protected against.  Of
course security in depth is critical -- and the right approach.
cryptography mailing list

@_date: 2014-03-22 03:28:43
@_author: Nico Williams 
@_subject: Re: [cryptography] Compromised Sys Admin Hunters and Tor 
Insiders are always your biggest threat.
Er, so?  The NSA could just... read the public docs and source
anyways.  I'd personally love to be able to sit down with NSA
cryptonerds and chat -- if they talked at all I'd learn something.  As
long as there was no coercion anyways.
cryptography mailing list

@_date: 2014-04-08 14:48:02
@_author: Nico Williams 
@_subject: Re: [cryptography] The Heartbleed Bug is a serious vulnerability in OpenSSL 
Thanks for this analysis.
Sadly, a variable-sized heartbeat payload was probably necessary, at
least for the DTLS case: for PMTU discovery.
Once more, a lack of an IDL, standard encoding, and tools, has hurt us.
Hand-coded parsers/encoders are disasters waiting to happen.
The TLS ad-hoc message syntax and encoding are not even adhered to
consistently in all the extensions, so I'm not sure that we could fix
this problem now (in TLS 1.3, say).  There was a thread on the TLS WG
list about this a while back...  Fixing this in 1.3 wouldn't fix the
implementations.  Making tooling available wouldn't either: it's very
difficult to retrofit an IDL compiler into a codebase with hand-coded
coders -- it's so difficult that it may be easier to build codebase-
specific IDL compilers.  Plus waiting for tooling would delay other
important enhancements.

@_date: 2015-01-12 17:59:14
@_author: Nico Williams 
@_subject: Re: [Cryptography] open hardware as a defence against state-level attacks 
Perhaps someday we'll see something like small, cheap, portable 3D
printers, but for ICs.  Such printers would make it easier to implement
countermeasures.  Of course a printer could be compromised and look for
patterns in which to insert backdoors, but that might well prove easy to
