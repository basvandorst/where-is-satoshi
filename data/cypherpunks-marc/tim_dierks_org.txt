
@_date: 2002-08-12 19:28:15
@_author: Tim Dierks 
@_subject: Re: Palladium: technical limits and implications 
The addition of an additional security ring with a secured, protected memory space does not, in my opinion, change the fact that such a ring cannot accurately determine that a particular request is consistant with any definable security policy. I do not think it is technologically feasible for ring -1 to determine, upon receiving a request, that the request was generated by trusted software operating in accordance with the intent of whomever signed it.
Specifically, let's presume that a Palladium-enabled application is being used for DRM; a secure & trusted application is asking its secure key manager to decrypt a content encryption key so it can access properly licensed code. The OS is valid & signed and the application is valid & signed. How can ring -1 distinguish a valid request from one which has been forged by rogue code which used a bug in the OS or any other trusted entity (the application, drivers, etc.)?
I think it's reasonable to presume that desktop operating systems which are under the control of end-users cannot be protected against privilege escalation attacks. All it takes is one sound card with a bug in a particular version of the driver to allow any attacker to go out and buy that card & install that driver and use the combination to execute code or access data beyond his privileges.
In the presence of successful privilege escalation attacks, an attacker can get access to any information which can be exposed to any privilige level he can escalate to. The attacker may not be able to access raw keys & other information directly managed by the TOR or the key manager, but those keys aren't really interesting anyway: all the interesting content & transactions will live in regular applications at lower security levels.
The only way I can see to prevent this is for the OS to never transfer control to any software which isn't signed, trusted and intact. The problem with this is that it's economically infeasible: it implies the death of small developers and open source, and that's a higher price than the market is willing to bear.
  - Tim
PS - I'm looking for a job in or near New York City. See my resume at

@_date: 2002-08-12 20:32:05
@_author: Tim Dierks 
@_subject: Re: trade-offs of secure programming with Palladium (Re: 
I agree; I think the system as you describe it could work and would be secure, if correctly executed. However, I think it is infeasible to generally implement commercially viable software, especially in the consumer market, that will be secure under this model. Either the functionality will be too restricted to be accepted by the market, or there will be a set of software flaws that allow the system to be penetrated.
The challenge is to put all of the functionality which has access to content inside of a secure perimeter, while keeping the perimeter secure from any data leakage or privilege escalation. The perimeter must be very secure and well-understood from a security standpoint; for example, it seems implausible to me that any substantial portion of the Win32 API could be used from within the perimeter; thus, all user interface aspects of the application must be run through a complete security analysis with the presumption that everything outside of the perimeter is compromised and cannot be trusted. This includes all APIs & data.
I think we all know how difficult it is, even for security professionals, to produce correct systems that enforce any non-trivial set of security permissions. This is true even when the items to be protected and the software functionality are very simple and straightforward (such as key management systems). I think it entirely implausible that software developed by multimedia software engineers, managing large quantities of data in a multi-operation, multi-vendor environment, will be able to deliver a secure environment.
This is even more true when the attacker (the consumer) has control over the hardware & software environment. If a security bug is found & patched, the end user has no direct incentive to upgrade their installation; in fact, the most concerning end users (e.g., pirates) have every incentive to seek out and maintain installations with security faults. While a content or transaction server could refuse to conduct transactions with a user who has not upgraded their software, such a requirement can only increase the friction of commerce, a price that vendors & consumers might be quite unwilling to pay.
I'm sure that the whole system is secure in theory, but I believe that it cannot be securely implemented in practice and that the implied constraints on use & usability will be unpalatable to consumers and vendors.
  - Tim
PS - I'm looking for a job in or near New York City. See my resume at

@_date: 2003-06-08 22:03:29
@_author: Tim Dierks 
@_subject: Re: An attack on paypal 
I don't think it's https that's broken, since https wasn't intended to solve the customer authentication / authorization problem (you could try to use SSL's client certificates for that, but no one ever intended client certificate authentication to be a generalized transaction problem).
When I responded to this before, I thought you were talking about the server auth problem, not the password problem. I continue to feel that the server authentication problem is a very hard problem to solve, since there's few hints to the browser as to what the user's intent is.
The password problem does need to be solved, but complaining that HTTPS or SSL doesn't solve it isn't any more relevant than complaining that it's not solved by HTML, HTTP, and/or browser or server implementations, since any and all of these are needed in producing a new solution which can function with real businesses and real users. Let's face it, passwords are so deeply ingrained into people's lives that nothing which is more complex in any way than passwords is going to have broad acceptance, and any consumer-driven company is going to consider "easy" to be more important that "secure".
Right now, my best idea for solving this problem is to:
  - Standardize an HTML input method for  which does an SPEKE (or similar) mutual authentication.
  - Get browser makers to design better ways to communicate to users that UI elements can be trusted. For example, a proposal I saw recently which would have the OS decorate the borders of "trusted" windows with facts or images that an attacker wouldn't be able to predict: the name of your dog, or whatever. (Sorry, can't locate a link right now, but I'd appreciate one.)
  - Combine the two to allow sites to provide a user-trustable UI to enter a password which cannot be sucked down.
  - Evangelize to users that this is better and that they should be suspicious of any situation where they used such interface once, but now it's gone.
I agree that the overall architecture is broken; the problem is that it's broken in more ways than can just be fixed with any change to TLS/SSL or HTTPS.
  - Tim

@_date: 2004-08-18 18:01:20
@_author: "Tim Dierks" 
@_subject: Re: MD5 collisions? 
How likely is it that this attack could be extended to creating
common-prefix collisions? This is equivalent to specifying two IVs for
the two hashes. Looking at X.509 ASN.1, it seems like it would be
pretty easy to construct a certificate request such that, if you could
predict the certificate serial number you were to be issued, you could
create a hash collision in an X.509 certificate:
You would construct two strings, each the same length, an even
multiple of 512 bits long, which contained the prefix of the
certificate you were going to be issued and the prefix of the
certificate you would like to claim to have been issued (P1 =
authentic prefix, P2 = forged prefix). The trailing part of each
prefix would be the first few bits of the value of n in the RSA key.
These first few bits can be different in P1 and P2, and randomly
selected, as long as they begin with a 1. Call the first few bits of n
in P1 "p1", and the first few bits of n in P2 "p2".
You would take IV1 = internal state of MD5 after hashing P1 and IV2 =
internal state of MD5 after hashing P2. You can tweak the first few
bits of n if you want IV1 and IV2 to have a particular relation, as
long as it's not infeasible to find that relation. Then find two
1024-bit 2-block messages, M1, and M2, such that they create a
collision. Define v1 = p1 concatentated with M1, and similarly for v2.
Calculate D = abs(v1-v2). Factor D, finding a 1024-bit prime that
evenly divides D ("i"). If you can't easily factor D, or it doesn't
yield a 1024-bit prime factor, find another collision pair. Now
construct the last bits of n (call them t), which will be used in both
requests, such that n1 = v1 concatentated with k and n2 = v2
concatenated with k are both equal to i times j1 and j2 (each 1024-bit
primes). I think it may be possible to acclerate it, but even if you
just have to calculate collisions until it's true, you should be able
to find a collision D and a value k that gives primes i, j1, and j2
once in every ~30 million collisions. (I'm taking a stab in the dark
that values of D that have a single 1024-bit factor have 1/300
density, but this is demonstrably worst-case.) If you can find such a
collision with 5 minutes of CPU time, the attack will take, at worst,
275 CPU-years. This is university-feasible.
Of course, I don't have any idea how hard it is to extend the known
attack with common IVs to one in which the IVs are not common.
I'd recommend that anyone operating a CA choose serial numbers which are
long & unpredictable; if you want some structure, append a random 128 bit
value to your structure.
- Tim
