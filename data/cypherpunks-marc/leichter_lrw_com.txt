
@_date: 2009-02-23 19:03:01
@_author: Jerry Leichter 
@_subject: Re: Shamir secret sharing and information theoretic security 
I've never seen any work done in this direction.  When you consider  exact values, FP arithmetic is very messy and has almost no nice  mathematical properties.  (It's nice in a model where all you care  about is relative error - which is actually a rather unnatural  model!)  As a result, I think it's unlikely you can come up with any  general theory here.  But you can probably come up with examples  showing that there's a problem.  It's usually easiest to work with a  simpler form of FP math - e.g., assume 4 decimal digits and a 1-digit  decimal exponent.  Consider just quadratics, which we can write as
p(x) = (x - r1)*(x - r2).  If r1*r2 overflows in a particular FP  system, you can't write down the value of the constant coefficient -  hence, you can't write down the value p(0).  Yet p(1) and p(2) might  have values you *can* write down.  I'm not sure how you leverage this  to produce a bias, but it certainly shows that FP arithmetic just  plain doesn't have the right properties to support the reasoning  behind Shamir secret sharing....
                                                         -- Jerry

@_date: 2009-02-22 19:37:13
@_author: Jerry Leichter 
@_subject: Re: Shamir secret sharing and information theoretic security 
It is.  Knowing some of the coefficients, or some constraints on some  of the coefficients, is just dual to knowing some of the points.   Neither affects the security of the system, because the coefficients  *aren't secrets* any more than the values of f() at particular points  are.  They are *shares* of secrets, and the security claim is that  without enough shares, you have no information about the remaining  The argument for information-theoretic security is straightforward:   An n'th degree polynomial is uniquely specified if you know its value  at n+1 points - or, dually, if you know n+1 coefficients.  On the  other hand, *any* set of n+1 points (equivalently, any set of n+1  coefficients) corresponds to a polynomial.  Taking a simple approach  where the secret is the value of the polynomial at 0, given v_1,  v_2, ..., v_n and *any* value v, there is a (unique) polynomial of  degree at most n with p(0) = v and p(i) = v_i for i from 1 to n.   Dually, the value p(0) is exactly the constant term in the  polynomial.  Given any fixed set of values c_1, c_2, ..., c_n, and any  other value c there is obviously a polynomial p(x) = Sum_{0 to n}(c_i  x^i), where c_0 = c, and indeed p(0) = c.
Or ... in terms of your problem:  Even if I give you, not just a pair  of linear equations in c1, c2, and S, but the actual values c1 and c2  - the constant term (the secret) can still be anything at all.
The description above is nominally for polynomials over the reals.  It  works equally for polynomials over any field - like the integers mod  some prime, for example.  For a finite field, there is an obvious  interpretation of probability (the uniform probability distribution),  and given that, "no information" can be interpreted in terms of the  difference between your a priori and a posterio estimates of the  probability that p(0) takes on any particular value, the values of  p(1), ..., p(n) (and that differences is exactly 0).  Because there  can be no uniform probability distribution over all the reals, you  can't state things in quite the same way, and "information theoretic  security" is a bit of a vague notion.  Then again, no one does  computations over the reals.  FP values - say, IEEE single precision -  aren't a field and there are undoubtedly big biases if you try to use  Shamir's technique there.  (It should work over infinite-precision                                                           -- Jerry

@_date: 2009-07-26 12:10:13
@_author: Jerry Leichter 
@_subject: Re: cleversafe says: 3 Reasons Why Encryption is Overrated 
It seems to me there's a much simpler critique.  The Cleversafe  approach - which is not without its nice points - solves the "key  management problem" in exactly the same way that some version of  Windows solved the "frequent General Protection Fault crashes" problem  (by eliminating the error message).
The "key management problem" comes down to:  I have encrypted data  stored somewhere (where we assume attackers can access it, but not  make use of it without the key).  To make that data meaningful, I need  to be able to locate the key appropriate to that data.  What's a key?   It's some private information.  In Cleversafe's approach, I have data  stored in pieces all over the place.  To get at it, I need to know  where the pieces of some data are.  That information has to be secret,  since anyone who has access to it can do the same computation and  recover the data just as I can.
Alternatively, I can rely not on the secrecy of that information, but  on the discretion of those who hold the pieces.  OK, but I could have  done that with a simpler technique:  Encrypt the data conventionally,  then split the key among the trusted holders.  That's a tiny, and more  to the point, *fixed* overhead beyond the size of the data, which will  always beat the cleverest Reed-Solomon or erasure coding.  (It also  has - if I use an appropriate mode - such nice features as random  access to small parts of the data without the need to decrypt the  whole thing first.)
Granted, Cleversafe has other nice features.  But other than changing  "the key management problem" to "the secret information needed to get  at the data, which won't be used as a crypto key" problem, I don't see  how they've actually *solved* anything.
Further:  If I'm only encrypting stuff for myself, there's little  reason to use multiple keys.  The key management problem becomes  interesting when there is different encrypted data with different  access rights for different groups of users.  It's beyond me how  Cleversafe's approach makes this easier - or harder.
                                                         -- Jerry

@_date: 2009-08-10 01:48:45
@_author: Jerry Leichter 
@_subject: Re: cleversafe says: 3 Reasons Why Encryption is Overrated 
Since people do keep bringing up Moore's Law in an attempt to justify  larger keys our systems "stronger than cryptography," it's worth  keeping in mind that we are approaching fairly deep physical limits.   I wrote about this on this list quite a while back.  If current  physical theories are even approximately correct, there are limits to  how many "bit flips" (which would encompass all possible binary  operations) can occur in a fixed volume of space-time.  You can turn  this into a limit based solely on time through the finite speed of  light:  A computation that starts at some point and runs for n years  can't involve a volume of space more than n light years in radius.   (This is grossly optimistic - if you want the results to come back to  the point where you entered the problem, the limit is n/2 light years,  which has 1/8 the spacial volume).  I made a very approximate guess at  how many bit-flips you could get in a time-space volume of a 100 light- year sphere; the answer came out somewhere between 2^128 and 2^256,  though much closer to the former.  So physical limits prevent you from  doing a brute force scan - in fact, you can't even enumerate all  possible keys - in 100 years for key lengths somewhere not much more  than 128 bits.
It's rather remarkable that such fundamental limits on computation  exist at all, but physics over the last 100 years - and especially  over the last couple of decades - has increasingly shown us that the  world is neither continuous nor infinite; it has solid finite limits  on almost everything.  Even more remarkable is that we've pretty much  reached some of those limits.  For any recently designed cryptosystem,  brute force is simply out of the question, and will remains so forever  (unless we are very much mistaken about physics).  Moore's Law as a  justification for using "something more" makes no sense.
As you point out, the story for advances in cryptographic theory is  much more complex and impossible to predict.  That cryptographic  advances would render the "safer" AES-256 at risk while AES-128  remains secure (for now) is something no one could have predicted,  though in retrospect some of the concerns about the key scheduling may  have been right.  All the protocols and standards out there calling  for AES-256 - it's obviously "better" than AES-128 because after all  256 is *twice as large* as 128! - were just a bunch of nonsense.  And,  perhaps, dangerous nonsense.
                                                         -- Jerry

@_date: 2009-08-11 23:47:54
@_author: Jerry Leichter 
@_subject: Re: brute force physics Was: cleversafe... 
When the first results about exponential speedup of factoring came  out, people assumed that this was true in general.  But it isn't.  In  particular, simple search, where you have only an equality test so you  can't build a hash table or some kind of ordered structure - is O(N)  on a traditional computer - and O(sqrt(N)) on a quantum computer.  I'm  not sure what the current knowledge about what a quantum machine can  do for NP computations, but there's no "probably" here.
The physical arguments to which I was referring say *nothing* about  how the computation is done.  It can be a QM computation as well.
In any case, the simple search result above applies directly to brute  force:  For that problem, you only get a polynomial speedup anyway.
That's a ... bizarre point of view.  :-)  Should freedom from related- key attacks be part of the definition of a "secure" encryption  algorithm?  We should decide that on some rational basis, not on  whether, with care, we can avoid such attacks.  Clearly, a system that  *is* secure against such attacks is more robust.  Do we know how to  build such a thing?  What's the cost of doing so?  But to say it's an  *advantage* to have a weakness seems like some kind of odd moral  argument:  If you're hurt by this it's because you *deserve* to be.
                                                         -- Jerry

@_date: 2009-08-12 01:27:09
@_author: Jerry Leichter 
@_subject: Re: Ultimate limits to computation 
It must be the summer weather or something.  I've received a whole  bunch of messages - mainly privately - that say either "Here's another  result that has a higher upper bound on computation" or "Here's a  design for a machine that exceeds your bound".  Both ignore (a) how  bounds work:  That fact that you have a weaker bound doesn't mean I  don't have a stronger one; (b) that impossibility results can exist in  physics, not just in mathematics.  True, the nature of such results  are a bit different, since all our current physical theories might  turn out to be wrong.  But, hey, maybe our understanding of  computation or even mathematics has some fundamental flaw, too.
The estimate on the limits to brute-force search are mine, based on a  *very* rough estimate that draws on the results in the following  paper:  (I haven't actually read the paper; my analysis was based on an  article I can't find that discussed the implications of this one.)
The basic summary of the author's result is:  "[T]he total number of  bits and number of operations for the universe does not exceed  O(10^123)."  I guessed about how this value scales (as the cube of the  time - one factor for time, two for the size of the light sphere you  can reach in that time; not 3 because the information content of space  goes up as the area, *not* the volume - a very weird but by now  standard result).
Now, my scaling technique may be completely flawed, or my computations  may be wrong.  Or the paper could be wrong.  (I wouldn't bet on it.)   Or the physics may be wrong.  (I *really* wouldn't bet on that.)  But  the fact that there are other bounds around that are not as tight, or  that one can describe a machine that would do better if there were a  way to realize it, aren't evidence for any of these.  Bounds can be  improved, and a description isn't a working machine.
In fact, the whole point of the article that I *did* read is that this  result should make use re-examine the whole notion of a "possible"  computation.  It's easy to describe a computation that would take more  than 10^123 steps.  Ackerman's function exceeds that for pretty small  input values.  We've traditionally said that a computation is  "possible" if we can describe it fully.  But if it couldn't be  realized by the entire universe - is that really a *useful* notion of                                                           -- Jerry

@_date: 2009-09-09 02:08:59
@_author: Jerry Leichter 
@_subject: Re: "Fed's RFIDiocy pwnd at DefCon" 
Remember:  Before it's actually happened, any discussion is just  reckless speculation, rumor-mongering, or worse.
After it's actually happened, it's either (a) not a real issue; (b) a  major new attack that could not have been foreseen but that will be  dealt with immediately by top people.  Top people.
                                                         -- Jerry

@_date: 2009-09-02 02:55:31
@_author: Jerry Leichter 
@_subject: "Fed's RFIDiocy pwnd at DefCon" 
"NSA spooks gather for a colleagues retirement party at a bar. What  they  dont know is that an RFID scanner is picking them out - and a  wireless Bluetoothwebcam is taking their picture.
Could that really happen? It already did.
(The Feds got a taste of the real world risks of RFID passports and  IDs at DefCon, the annual hacker conference. According to Wired  . . . federal agents at the conference got a scare on Friday when they  were told they might have been caught in the sights of an RFID reader.
The reader, connected to a web camera, sniffed data from RFID-enabled  ID cards and other documents carried by attendees in pockets and  backpacks as they passed a table where the equipment was stationed in  full view...."
                                                         -- Jerry

@_date: 2009-11-09 03:11:57
@_author: Jerry Leichter 
@_subject: Re: hedging our bets -- in case SHA-256 turns out to be insecure 
I'd worry about using this construction if H1's input block and output  size were the same, since one might be able to leverage some kind of  extension attack.  That's not a problem for SHA256 or SHA512, but it's  something to keep in mind if this is supposed to be a general  construction, given that all likely hash functions will be constructed  by some kind of iteration over fixed-size blocks.
Rather than simply concatenating H1(x) and H2(x), you might do better  to interlace them.  Even alternating bytes - cheap enough that you'd  never notice - should break up any structure that designs of practical  hash functions might exhibit.  (As a matter of theory, a vulnerability  of alternate bytes is as likely as a vulnerability of leading bytes;  but given the way we actually build hash functions, as a practical  matter the latter seems much more likely.)
                                                         -- Jerry

@_date: 2010-10-08 00:14:41
@_author: Jerry Leichter 
@_subject: Re: English 19-year-old jailed for refusal to disclose decryption key 
You're thinking too much about the technology.
The court demands a company turn over its books.  The company denies it
keeps any books.  Sure - massive fines, possible jail sentences for the
Alternatively, the company turns over fake books.  There is evidence  that the
books are fake - they show the company only did 2000 transactions last  but somehow the company paid a staff of 200 to take phone calls last  Or the books don't show any payments for things that we see sitting in  warehouse.  Or maybe there are just purely statistical anomalies:  The
variation in income from week to week is way out of the range shown by
other businesses.  Or there's just someone who swears that these are not
the books he's seen in the past.  Same outcome for the company.
Maybe the high-tech cheats let you get away with stuff; maybe they  Then again, maybe the fake paper books let you get away with stuff, and
maybe they don't.  Technology lets you play some games more easily,
but it's not magic pixie dust that immunizes you from reality.

@_date: 2010-10-07 16:05:09
@_author: Jerry Leichter 
@_subject: Re: English 19-year-old jailed for refusal to disclose decryption key 
Sure. And the technology used would have no effect on the standard used in court:  Is there sufficient convincing evidence that there's data there to decrypt (e.g., you used the system in the last day to send a message based on the kind of information sought)?  If so, decrypt or go to jail.  "Beyond a reasonable doubt" isn't the standard for everything, and even of it were, it's as understood by a judge or jury, not a logician.                                          -- Jerry

@_date: 2013-08-28 03:39:51
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
And yet DHT's have completely failed at doing this.
Redundancy and validation of updates are issues separable from the implementation of the map and, in particular, from routing.  DHT's try to combine all four and, as we've seen, fail.
Just because it's possible to actually store the contents of a DHT in a single big database doesn't mean you'd actually want to do it that way.  I'm suggesting that you start with the idealization of a single, secure database, then make the modifications needed to actually attain the necessary properties in the face of high distributed QPS, random failures, and a variety of attacks.
Why in the world would you want to put the information for even a million users on such a server.  This would be a server that exists to provide services to at most a few 10's of people - probably fewer.  How many users will they, personally, ever contact it their collective lifetimes?  This is an ideal application for local caching of relevant information from the global database stored "somewhere else".  It might well, transparently, also contain mapping information that its own users received "out of band" and want to use - but have no reason to share globally.
Again, why would individuals want to store that much data?
The DHT model says that millions of Raspberry Pi's and thumb drives together implement this immense database.  But since a DHT, by design, scatters the data around the network at random, *my* thumb drive is full of information that I will never need - all the information *I* need is out there, somewhere - where, based on the research we've been discussing, I have no secure way to get at it.  Why would I buy into such a design?  Doesn't it make much more sense for me to store the information relevant to me?
It's not as if this isn't a design we have that we know works:  DNS.  Yes, DNS, even the "secure" versions, have security issues.  But then so do DHT's, so they are hardly an improvement.  And many of DNS's problems have to do with the assumption of a single hierarchy with, as a result, a small number of "extremely trusted" nodes up at the top.  That's a problem that can be attacked.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-08-28 01:13:59
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Implementations, attacks on DHTs, Mix Nets? 
I wonder if much of the work on secure DHT's and such is based on bad assumptions.  A DHT is just a key/value mapping.  There are two reasons to want to distribute such a thing:  To deal with high, distributed load; and because it's too large to store on any one node.  I contend that the second has become a non-problem.  The DHT uses I've seen involve at most a couple of billion small key/value pairs; most involve a few million at most.  Even at the high end, what's today a fairly small, moderately powered system can handle this much data with no problems.  The limitations are on QPS.  However, there are plenty of mundane techniques to deal with that, including replication, deterministic sharding, and caching.  They are all much simpler than DHT's and are hence less likely to have the subtle security problems that DHT's do.
Fundamentally, we're asking DHT's to solve three problems at once:  Distribute a map; be robust in the face of node failure; do it all securely.  Better to use good solutions to the individual problems and combine them than to try to find a way to do all at once.
I worked on data structures somewhat like DHT's back in the late 1970's (to implement the Linda distributed programming language on LAN's and hypercubes and similar networks).  Neat idea at the time, and it was fun to see it come back as a neat idea on a much larger scale years later; but perhaps its time is (again) passing.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-08-30 11:13:47
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] The Case for Formal Verification 
Many years back, I did some work with Naftaly Minsky at Rutgers on his idea of "law-governed systems".  The idea was that you have an operational system that is wrapped within a "law enforcement" system, such that all operations relevant to the "law" have to go through the enforcer.  Then you write "the law" to specify certain global properties that the implementation must always exhibit, and leave the implementation to do what it likes, knowing that the enforcer will force it to remain within the law.
You can look at this in various ways in modern terms:  As a generalized security kernel, or as the reification of the attack surface of the system.
Minsky's interests were more on the software engineering side, and he and a couple of grad students eventually put together a law-governed software development environment, which could control such things as how modules were allowed to be coupled.  (The work we did together was on an attempt to add a notion of obligations to the law, so that you could not just forbid certain actions, but also require others - e.g., if you receive message M, you must within t seconds send a response; otherwise the law enforcer will send one for you.  I'm not sure where that went after I left Rutgers.)
While we thought this kind of thing would be useful for specifying and proving security properties, we never looked at formal proofs.  (The law of the system was specified in Prolog.  We stuck to a simple subset of the language, which could probably have been handled easily by a prover.  Still, hardly transparent to most programmers!)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-08-30 11:17:08
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
So the latest Snowden data contains hints that the NSA (a) spends a great deal of money on cracking encrypted Internet traffic; (b) recently made some kind of a cryptanalytic "breakthrough".  What are we to make of this?  (Obviously, this will all be wild speculation unless Snowden leaks more specific information - which wouldn't fit his style, at least as demonstrated so far.)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-08 13:03:05
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
OK, let's look at this another way.  The broader argument being made here breaks down into three propositions:
1.  If you have a way to "spike" a block cipher based on embedding a secret in it, you can a way to create something with the formal properties of a public key cryptosystem - i.e., there is a function E(P) which anyone can compute on any plaintext P, but given E(P), only you can invert to recover P.
2.  Something with the formal properties of a public key cryptosystem can be used as a *practical* public key cryptosystem.
3.  A practical public-key cryptosystem is much more valuable than a way to embed a secret in a block cipher, so if anyone came up with the latter, they would certainly use it to create the former, as it's been "the holy grail" of cryptography for many years to come up with a public key system that didn't depend on complex mathematics with uncertain properties.
If we assume these three propositions, and looks around us and observe the lack of the appropriate kinds of public key systems, we can certainly conclude that no one knows how to embed a secret in a block cipher.
Proposition 1, which is all you specifically address, is certainly true.  I claim that Propositions 2 and 3 are clearly false.
In fact, Proposition 3 isn't even vaguely mathematical - it's some kind of statement about the values that cryptographers assign to different kinds of primitives and to publication.  It's quite true that if anyone in the academic world were to come up with a way to create a practical public key cryptosystem without a dependence on factoring or DLP, they would publish to much acclaim.  (Of course, there *are* a couple of such systems known - they were published years ago - but no one uses them for various reasons.  So "acclaim" ... well, maybe.)  Then again, an academic cryptographer who discovered a way to hide a secret in a block cipher would certainly publish - it would be really significant work.  So we never needed this whole chain of propositions to begin with:  It's self-evidently true that no one in the public community knows how to embed a secret in a block cipher.
But ... since we're talking *values*, what are NSA's values?  Would *they* have any reason to publish if they found a way to embed a secret in a block cipher? Hell, no!  Why would they want to give away such valuable knowledge?  Would they produce a private-key system based on their breakthrough?  Maybe, for internal use.  How would we ever know?
But let's talk mathematics, not psychology and politics.  You've given a description of a kind of back door that *would* produce a practical public key system.  But I've elsewhere pointed out that there are all kinds of back doors.  Suppose that my back door reduces the effective key size of AES to 40 bits.  Even 20+ years ago, NSA was willing to export 40-bit crypto; presumably they were willing to do the brute-force computation to break it.  Today, it would be a piece of cake.  But would a public-key system that requires around 2^40 operations to encrypt be *practical*?  Even today, I doubt it.  And if you're willing to do 2^40 operations, are you willing to do 2^56?  With specialized hardware, that, too, has been easy for years.  NSA can certainly have that specialized hardware for code breaking - will you buy it for encryption?
In fact, this is an example I was going to give:  In a world in which differential crypto isn't known, it *is* a secret that's a back door.  Before DC was published, people seriously proposed strengthening DES by using a 448-bit (I think that's the number) key - just toss the round key computation mechanism and provide all the keying for all the rounds.  If that had been widely used, NSA would have been able to break it use DC.
Of course we know about DC.  But the only reason RSA is safe is that we don't know how to factor quickly!  (Actually, even that's not quite true - after all these years, as far as I know, we *still* haven't managed to show that RSA is as hard as factoring - only the obvious fact that it's no harder.  It would be an incredible and unexpected result to separate the problems, but it *could* happen.)  It happens that in the case of RSA, we can point to *a particular easy to state problem* that, if solved, would break the system.  Things are much more nebulous for block ciphers, but that shouldn't be surprising:  They don't have simple, clean mathematical structures.  (In fact, when Rijndael was first proposed, there was some concern that it was "too clean" - that its relatively simple structure would provide traction for mathematical attacks.  It doesn't seem to have worked out that way.)
There are, in fact, even closer analogues between potential RSA weaknesses and DC weaknesses.  What DC tells us is that certain S boxes lead to weak ciphers, even if the general structure is otherwise sound.  Well ... over the years, we've learned that certain choices of primes lead to weak RSA, so we've restricted the choices we make to the "good" primes.  This is a much more pronounced effect with discrete log-based algorithms - and yet more so with elliptic curve algorithms.  Anyone who knows about such weaknesses before they are known to the public, and is in a position to influence how primes or curves are chosen, can embed a back door - a back door that will be closed as soon as the potential weakness becomes known.  How is this different in the public-key and block cipher cases?
In the case of elliptic curve algorithms, we *know* that in some cases its possible to embed a secret, but we don't know of any way to tell if a secret *actually was embedded*.  DC happens to have the property that once you know about it, you can check if your S-boxes are bad - but where's the proof that a different attack will necessarily have this property?  If such an attack were to emerge against AES, *mathematically*, we'd have to say, well, maybe someone put a back door into AES - we really don't know one way or another, just as we don't know, one way or the other, about the NSA-suggested elliptic curves and points.  In either case, we should probably come up with something new.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-07 15:39:28
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Actually, if you look at the papers cited, *they* are themselves informal.  The fundamental thing they are lacking is a definition of what would constitute a "master key".  Let's see if we can formalize this a bit:
We're given a block cipher E(K,P) and a corresponding decryption algorithm D(K,C).  The system has a master key M such that D(E(K,P),M) == P.  This is what a "master key" does in a traditional lock-and-key system, so unless we see some other definition, it's what we have to start with.  Is there such a system?  Sure, trivially.  Given any block cipher E'/D', I simply define E(K,P) = E'(M,K) || E'(K,P).  (I can optimize the extra length by leaking one randomly chosen bit of E'(M,K) per block.  It won't take long for the whole key to be transmitted.)  OK, where's the public key system?
So maybe there isn't *one* master key, but let's go to the extreme and say there is one unique master per user key, based on some secret information S.  That is:  Given K, there is a function F(S,K) which produces a *different* key K', with the property that D(K,C) == D(K',C).  Or maybe, as in public key systems, you start with S and some random bits and produce a matched pair K and K'  But how is this a "master key" system?  If I wasn't "present at the birth" of the K that produced the cyphertext I have in hand ... to get K' now, I need K (first form) or S and the random bits (second form), which also gives me K directly.  So what have I gained?
I can construct a system of the first form trivially:  Just use an n-bit key but ignore the first bit completely.  There are now two keys, one with a leading 0, one with a leading 1.  Constructing a system of the second form shouldn't be hard, though I haven't done it.  In either case, it's uninteresting - my "master key" is as hard to get at as the original key.
I'm not sure exactly where to go next.  Let's try to modify some constraints.  Eliminate directly hiding the key in the output by requiring that E(K,.) be a bijection.  There can't possibly be a single master key M, since if there were, what could D(M,E(M,0...0)) be?  It must be E(K,0...0) for any possible K, so E(K,0...0) must be constant - and in fact E must be constant.  Not very interesting.  In fact, a counting argument shows that there must be as many M's as there are K's.  It looks as we're back to the two-fold mapping on keys situation.  But as before ... how could this work?
In fact, it *could* work.  Suppose I use a modified form of E() which ignores all but the first 40 bits of K - but I don't know that E is doing this.  I can use any (say, 128-bit) key I like, and to someone not in on the secret, a brute force attack is impossible.  But someone who knows the secret simply sets all but the first 40 bits to 0 and has an easy attack.
*Modified forms (which hid what was happening to some degree) of such things were actually done in the days of export controls!*  IBM patented and sold such a thing under the name CDMF (  I worked on adding cryptography to a product back in those days, and we had to come up with a way to be able to export our stuff.  I talked to IBM about licensing CDMF, but they wanted an absurd amount of money.  (What you were actually paying for wasn't the algorithm so much as that NSA had already approved products using it for export.)  We didn't want to pay, and I designed my own algorithm to do the same thing.  It was a silly problem to have to solve, but I was rather proud of the solution - I could probably find my spec if anyone cares.  It was also never implemented, first because this was right around the time the crypto export controls got loosened; a
 nd second because we ended up deciding we didn't need crypto anyway.  We came back and did it very differently much later.  My two fun memories from the experience:  (a) Receiving a FAX from NSA - I still have it somewhere; (b) being told at one point that we might need someone with crypto clearance to work on this stuff with NSA, and one of my co-workers chiming in with "Well, I used to have it.  Unfortunately it was from the KGB."
Anyway ... yes, I can implement such a thing - but there's still no public key system here.
So ... would *you* like to take a stab at pinning down a definition relative to which the theorem you rely on makes sense?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-07 03:22:02
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I'm sorry, but this is just nonsense.  You're starting with informal, rough definitions and claiming a mathematical theorem.
I said all this before.  A back door doesn't have to be fast.  It doesn't have to be implementable using amounts of memory that are practical for a fielded system.  It may require all kinds of expensive pre-computation to be useful at all.  It just has to allow practical attacks.  A back door that reduced the effective key size of AES to 40 bits would amount to an effective break of AES, but would be "a public key system" only in some very technical and uninteresting sense.
And none of this is relevant to whether one could have a system with many weak keys.  Some kind of structure in the round computation structure would be an obvious place to look.
In fact, now that I think of it, here's a rough example of such a system:  Take any secure round-based block cipher and decide that you're not going to use a round computation at all - you'll let the user specify the full expanded per-round key.  (People proposed doing this with DES as a way of getting beyond the 56-bit key size.  It didn't work - DES is inherently good for no more than 56 bits, more or less.  In general doing this with any decent block cipher won't make it any stronger.  But that's not the point of the example.)  There are now many weak keys - all kinds of repetitive structures allow for slide attacks, for example.  Every bad way of designing a round computation corresponds to a set of weak full keys.  On the other hand, for a certain subset of the keys - those that could have been produced by the original (good) round computation - it's *exactly* the original cipher, with *exactly* the original security guarantees.  If I carefully use only keys from that se
 t, I've lost nothing (other than wasted space for a key longer than it needs to be).
So now I have a block cipher that has two sets of keys.  One set makes it as secure as the original cipher; one set makes it easy to break - my back door.  Have I just invented a new public key system?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 13:23:29
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Following up on my own posting:
So here's a thought experiment for a particular approach:  Imagine that it's the case that half of all possible AES keys are actually "pseudo-weak", in the sense that if you use one of them, some NSA cryptanalytic technique can recover the rest of your key with "acceptable (to NSA)" effort.  Their attack fails for the other half of all possible keys.  Further, imagine that NSA has a recognizer for pseudo-weak keys.  Then their next step is simple:  Get the crypto industry to use AES with good, randomizing key generation techniques.  Make sure that there is more than one approved key generation technique, ideally even a way for new techniques to be added in later versions of the standard, so that approved implementations have to allow for a choice, leading them to separate key generation from key usage.  For the stuff *they* use, add another choice, which starts with one of the others and simply rejects pseudo-weak keys (or modifies them in some way to produce strong keys.)  T
 hen:
- Half of all messages the world sends are open to attack by NSA until the COTS producers learn of the attack and modify their fielded systems;
- All messages NSA is responsible for are secure, even if the attack becomes known to other cryptanalytic services.
I would think NSA would be very happy with such a state of affairs.  (If they could arrange it that 255/256 keys are pseudo-weak - well, so much the better.)
Is such an attack against AES *plausible*?  I'd have to say no.  But if you were on the stand as an expert witness and were asked under cross-examination "Is this *possible*?", I contend the only answer you could give is "I suppose so" (with tone and body language trying to signal to the jury that you're being forced to give an answer that's true but you don't in your gut believe it).
Could an encryption algorithm be explicitly designed to have properties like this?  I don't know of any, but it seems possible.  I've long suspected that NSA might want this kind of property for some of its own systems:  In some cases, it completely controls key generation and distribution, so can make sure the system as fielded only uses "good" keys.  If the algorithm leaks without the key generation tricks leaking, it's not just useless to whoever grabs onto it - it's positively hazardous.  The gun that always blows up when the bad guy tries to shoot it....
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 11:42:51
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Argh!  And this is why I dislike using "symmetric" and "asymmetric" to describe cryptosystems:  In English, the distinction is way too brittle.  Just a one-letter difference - and in including or not the letter physically right next to the "s".
                                                        -- Jerry :-)
The cryptography mailing list

@_date: 2013-09-06 11:28:41
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I don't think we're really in disagreement here.  Much of what you say later in the message is that the way we are using symmetric-key systems (CA's and such), and the way browsers work, are fundamentally wrong, and need to be changed.  And that's really the point:  The system we have is all of a piece, and incremental changes, sadly, can only go so far.  We need to re-think things from the ground up.  And I'll stand by my contention that we need to re-examine things we think we know, based on analyses done 30 years ago.  Good theorems are forever, but design choices apply those theorems to real-world circumstances.  So much has changed, both on the technical front and on non-technical fronts, that the basis for those design choices has fundamentally changed.
Getting major changes fielded in the Internet is extremely difficult - see IPv6.  If it can be done at all, it will take years.  But the alternative of continuing on the path we're on seems less desirable every day.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 03:24:54
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
They want to buy COTS because it's much cheap, and COTS is based on standards.  So they have two contradictory constraints:  They want the stuff they buy secure, but they want to be able to break in to exactly the same stuff when anyone else buys it.  The time-honored way to do that is to embed some secret in the design of the system.  NSA, knowing the secret, can break in; no one else can.  There have been claims in this direction since NSA changed the S-boxes in DES.  For DES, we now know that was to protect against differential cryptanalysis.  No one's ever shown a really convincing case of such an embedded secret hack being done ... but now if you claim it can't happen, you have to explain how the goal in NSA's budget could be carried out in a way consistent with the two constraints.  Damned if I know....
I'm not sure exactly what you're trying to say.  Yes, Miller and Koblitz are the inventors of publicly known ECC, and a number of people (Diffie, Hellman, Merkle, Rivest, Shamir, Adelman) are the inventors of publicly known public-key cryptography.  But in fact we now know that Ellis, Cocks, and Williamson at GCHQ anticipated their public key cryptography work by several years - but in secret.
I think the odds are extremely high that NSA was looking at cryptography based on algebraic curves well before Miller and Koblitz.  Exactly what they had developed, there's no way to know.  But of course if you want to do good cryptography, you also have to do cryptanalysis.  So, yes, it's quite possible that NSA was breaking ECC a decade before its (public) invention.  :-)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 03:02:14
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Perhaps it's time to move away from public-key entirely!  We have a classic paper - Needham and Schroeder, maybe? - showing that private key can do anything public key can; it's just more complicated and less efficient.
Not only are the techniques brittle and increasingly under suspicion, but in
practice almost all of our public key crypto inherently relies on CA's - a structure that's just *full* of well-known problems and vulnerabilities.  Public key *seems to* distribute the risk - you "just get the other guy's public key" and you can then communicate with him safely.  But in practice it *centralizes* risks:  In CA's, in single magic numbers that if revealed allow complete compromise for all connections to a host (and we now suspect they *are* being revealed.)
We need to re-think everything about how we do cryptography.  Many decisions were made based on hardware limitations of 20 and more years ago.  "More efficient" claims from the 1980's often mean nothing today.  Many decisions assumed trust models (like CA's) that we know are completely unrealistic.  Mobile is very different from the server-to-server and dumb-client-to-server models that were all anyone thought about the time.  (Just look at SSL:  It has the inherent assumption that the server *must* be authenticated, but the client ... well, that's optional and rarely done.)  None of the work then anticipated the kinds of attacks that are practical today.
I pointed out in another message that today, mobile endpoints potentially have access to excellent sources of randomness, while servers have great difficulty getting good random numbers.  This is the kind of fundamental change that needs to inform new designs.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 02:31:50
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
The actual documents - some of which the Times published with few redactions - are worthy of a close look, as they contain information beyond what the reporters decided to put into the main story.  For example, at  the following goal appears for FY 2013 appears:  "Complete enabling for [redacted] encryption chips used in Virtual Public Network and Web encryption devices".  The Times adds the following note:  "Large Internet companies use dedicated hardware to scramble traffic before it is sent. In 2013, the agency planned to be able to decode traffic that was encoded by one of these two encryption chips, either by working with the manufacturers of the chips to insert back doors or by exploiting a security flaw in the chips' design."  It's never been clear whether these kinds of notes are just guesses by the reporters, come from their own sources, or com
 e from Snowden himself.  The Washington Post got burned on one they wrote.  But in this case, it's hard to come up with an alternative explanation.
Another interesting goal:  "Shape worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS."  Elsewhere, "enabling access" and "exploiting systems of interest" and "inserting vulnerabilities".  These are all side-channel attacks.  I see no other reference to "cryptanalysis", so I would take this statement at face value:  NSA has techniques for doing cryptanalysis on certain algorithms/protocols out there, but not all, and they would like to steer public cryptography into whatever areas they have attacks against.  This makes any NSA recommendation *extremely* suspect.  As far as I can see, the bit push NSA is making these days is toward ECC with some particular curves.  Makes you wonder.  (I know for a fact that NSA has been interested in this area of mathematics for a *very* long time:  A mathematician I knew working in the area of algebraic curves (of which elliptic curves are an example) was re
 cruited by - and went to - NSA in about 1975.  I heard indirectly from him after he was at NSA, where he apparently joined an active community of people with related interests.  This is a decade before the first public suggestion that elliptic curves might be useful in cryptography.  (But maybe NSA was just doing a public service, advancing the mathematics of algebraic curves.)
NSA has two separate roles:  Protect American communications, and break into the communications of adversaries.  Just this one example shows that either (a) the latter part of the mission has come to dominate the former; or (b) the current definition of an adversary has become so broad as to include pretty much everyone.
Now, the NSA will say:  Only *we* can make use of these back doors.  But given the ease with which Snowden got access to so much information ... why should we believe they can keep such secrets?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 00:30:40
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
This has bothered me from the beginning.  Even the first leaks involved material that you would expect to only be available to highly trusted people *well up in the organization* - they were slides selling capabilities to managers and unlikely to be shown to typical employees, cleared or not.  My immediate impression was that we were looking at some disgruntled higher-up.
The fact that these are coming from a sysadmin - who would never have reason to get legitimate access to pretty much *any* of the material leaked so far - is a confirmation of a complete breakdown of NSA's internal controls.  They seem to know how to do cryptography and cryptanalysis and all that stuff - but basic security and separation of privileges and internal monitoring ... that seems to be something they are just missing.
Manning got to see all kinds of material that wasn't directly related to his job because the operational stuff was *deliberately* opened up in an attempt to get better analysis.  While he obviously wasn't supposed to leak the stuff, he was authorized to look at it.  I doubt the same could be said of Snowden.  Hell, when I had a data center manager working for me, we all understood that just because root access *let* you look at everyone's files, you were not *authorized* to do so without permission.
One of the things that must be keeping the NSA guys up night after night is:  If Snowden could get away with this much without detection, who's to say what the Chinese or the Russians or who knows who else have managed to get?  Have they "spiked the spikers", grabbing the best stuff the NSA manages to find?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 00:07:20
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
[This drifts from the thread topic; feel free to attach a different subject line to it]
Random number generators make for a very interesting target.  Getting decent amounts of entropy on conventional machines is very difficult.  Servers have almost no random variation in their environments; desktops somewhat more; modern laptops, yet more.  Virtualization - now extremely common on the server side - makes things even harder.  But even laptops don't have much.  So we're left trying to distill "enough" randomness for security - a process that's error-prone and difficult to check.
So ... along comes Intel with a nice offer:  Built-in randomness on their latest chips.  Directly accessible to virtual machines, solving the very difficult problems they pose.  The techniques used to generate that randomness are published.  But ... how could anyone outside a few chip designers at Intel possibly check that the algorithm wasn't, in some way, spiked?  For that matter, how could anyone really even check that the outputs of the hardware Get Random Value instruction were really generated by the published algorithm?
Randomness is particularly tricky because there's really no way to test for a spiked random number generator (unless it's badly spiked, of course).  Hell, every encryption algorithm is judged by its ability to generate streams of bits that are indistinguishable from random bits (unless you know the key).
Now, absolutely, this is speculation.  I know of no reason to believe that the NSA, or anyone else, has influenced the way Intel generates randomness; or that there is anything at all wrong with Intel's implementation.  But if you're looking for places an organization like the NSA would really love to insert itself - well, it's hard to pick a better one.
Interestingly, though, there's good news here as well.  While it's hard to get at sources of entropy in things like servers, we're all carrying computers with excellent sources of entropy in our pockets.  Smartphones have access to a great deal of environmental data - accelerometers, one or two cameras, one or two microphones, GPS, WiFi, and cell signal information (metadata, data, signal strength) - more every day.  This provides a wealth of entropy, and it's hard to see how anyone could successfully bias more than a small fraction of it.  Mix these together properly and you should be able to get extremely high quality random numbers.  Normally, we assume code on the server side is "better" and should take the major role in such tasks as providing randomness.  Given what we know now about the ability of certain agencies to influence what runs on servers, *in general*, we need to move trust away from them.  The case is particularly strong in the case of randomness.
Of course, there's a whole other layer of issue introduced by the heavily managed nature of phone software.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-29 13:01:37
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] RSA equivalent key length/strength 
The actual algorithms are classified, and about all that's leaked about them, as far as I can determine in a quick search, is the names of some of them, and general properties of a subset of those - e.g., according to Wikipedia, BATON is a block cipher with a key length of 320 bits (160 of them checksum bits - I'd guess that this is an overt way for NSA to control who can use stolen equipment, as it will presumably refuse to operate at all with an invalid key).  It looks as if much of this kind of information comes from public descriptions of equipment sold to the government that implements these algorithms, though a bit of the information (in particular, the name BATON and its key and block sizes) has made it into published standards via algorithm specifiers.  cryptome has a few leaked documents as well - again, one showing BATON mentioned in Congressional testimony about Clipper.
Cryptographic challenge:  If you have a sealed, tamper-proof box that implements, say, BATON, you can easily have it refuse to work if the key presented doesn't checksum correctly.  In fact, you'd likely have it destroy itself if presented with too many invalid keys.  NSA has always been really big about using such sealed modules for their own algorithms.  (The FIPS specs were clearly drafted by people who think in these terms.  If you're looking at them while trying to get software certified, many of the provisions look very peculiar.  OK, no one expects your software to be potted in epoxy ("opaque in the ultraviolet" - or was it infrared?); but they do expect various kinds of isolation that just affect the blocks on a picture of your software's implementation; they have no meaningful effect on security, which unlike hardware can't enforce any boundaries between the blocks.)
Anyway, this approach obviously depends on the ability of the hardware to resist attacks.  Can one design an algorithm which is inherently secure against such attacks?  For example, can one design an algorithm that's strong when used with valid keys but either outright fails (e.g., produces indexes into something like S-boxes that are out of range) or is easily invertible if used with invalid keys (e.g., has a key schedule that with invalid keys produces all 0's after a certain small number of rounds)?  You'd need something akin to asymmetric cryptography to prevent anyone from reverse-engineering the checksum algorithm from the encryption algorithm, but I know of no fundamental reason why that couldn't be done.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-02 21:44:57
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
Except that that's not what happened in this case.
Someone took an old, valid Microsoft license - which should never have been issued, and which was blocked on Vista and Windows 7.  They worked around the block using a technique that required the ability to produce MD5 collisions, which allowed them to spoof Windows Update.  All the details are at A cryptographic approach for producing chosen-prefix collisions in MD5 was presented at CCC in 2008, with a cost estimate of about $20K on a 2008 Amazon EC2 cluster - the authors showed a POC using a cluster of PS3's.  Open source code to implement the attack was published in 2009.
However, the form of the collision apparently didn't match the published code, nor, more fundamentally, the theoretical work that made it possible.  Someone has a *different*, so far nowhere-published attack.  The comment that this required "world-class cryptanalysis" came from the developer of the published chosen-prefix attack, Marc Stevens.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-02 19:09:31
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
a)  The very reference you give says that to be equivalent to 128 bits symmetric, you'd need a 3072 bit RSA key - but they require a 2048 bit key.  And the same reference says that to be equivalent to 256 bits symmetric, you need a 521 bit ECC key - and yet they recommend 384 bits.  So, no, even by that page, they are not recommending "equivalent" key sizes - and in fact the page says just that.
b)  Those comparisons long ago became essentially meaningless.  On the symmetric size, it's using brute force attack strengths.  But no one is going to brute force a 128-bit key with any known or suggested technology, and brute force attacks against 256-bit keys are way beyond what physics says is even remotely possible.  (I posted on this a long time back:  Any theory even vaguely consistent with what we know about quantum mechanics places a limit on the number of elementary bit flips in a finite volume of space-time.  If you want an answer in 100 years, your computer is at most a sphere in space-time 100 light-years cubed by 100 years in diameter - and that's a gross overestimate.  My quick calculation showed that the quantum limit for that sphere is not far above 128 bits.)
In any real terms, *if you're talking brute force*, 128 bits and 256 bits - and a million bits, if you want to go nuts about it - are indistinguishable.
For the other columns, they don't say where the difficulty estimate comes from. (You could get a meaningless estimate by requiring that the number of primes of the size quoted be equivalent to the number of symmetric keys, but I'm assuming they're being more intelligent about the estimate than that, as a brute force attack on primes makes no sense at all.  What makes more sense - and what they are presumably using - is the number of operations needed by the best known algorithm.  But now we're at point of comparing impossible attacks against 128- and 256-bit symmetric keys with impossible attacks against 3072- or 15360-bit RSA keys - a waste of time.  The relevant point is that attacks against RSA keys have been getting better faster than predicted, while the best publicly known attacks against AES have barely moved the needle from simple brute force.
Given *currently publicly known algorithms*, a 2048 bit RSA key is still secure.  (The same page shows that as equivalent to a 112-bit symmetric key, which is not only beyond any reasonable-term brute force attack, but longer than the keys used - according to some reports, anyway - on some Suite A algorithms.)
And here we actually agree.  Note that I didn't say there was any evidence that NSA was ahead of the public state of the art - even given the public state of the art and the rate that it's advancing, using Z/p as a field is rapidly fading as a realistic alternative.  NSA, looking forward, would be making the recommendation to move to elliptic curves whether or not they could do better than the public at large.  So we can't read much into that aspect of it.  However, note (a) that if NSA does have a theoretical breakthrough, factoring is probably more likely than AES - we know they've hired many people in related fields over many years, and even in public the state of the art has been advancing; (b) most of the Internet is way behind recommendations that are now out there for everyone.  Google recently switched to 2048 bit keys; hardly any other sites have done so, and some older software even has trouble talking to Google as a result.
Just what is it you claim we *know*?
On the asymmetric side, NSA recommends technology that Internet sites *could* have, but in practice mainly don't and for the most part won't for quite some time.  Can NSA break RSA or DH or DSA as most sites on the Internet are using it today?  Damned if I know - but either way is consistent with what we know.
On the symmetric side, I've already agreed that NSA's approval indicated that the considered AES secure 10 years ago, but if they've since learned otherwise but think they are and will remain the only ones with a viable attack for a while, they would be unlikely to admit it by changing their recommendation now.  This isn't evidence that AES is insecure or that NSA has an attack on it, simply a reason why the recommendation is not much in the way of evidence *for* its strength any more either.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-02 11:21:25
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
e secret community *does* have capabilities (to conduct a collision attacks=
) beyond those known to the public - capabilities sufficient to produce fak=
e Windows updates.
For some version of "know".  From "Microsoft released an emergency Windows update on Sunday after revealing t=
hat one of its trusted digital signatures was being abused to certify the v=
alidity of the Flame malware that has infected computers in Iran and other =
Middle Eastern Countries.
The compromise exploited weaknesses in Terminal Server, a service many ente=
rprises use to provide remote access to end-user computers. By targeting an=
 undisclosed encryption algorithm Microsoft used to issue licenses for the =
service, attackers were able to create rogue intermediate certificate autho=
rities that contained the imprimatur of Microsoft's own root authority cert=
ificate=97an extremely sensitive cryptographic seal. Rogue intermediate cer=
tificate authorities that contained the stamp were then able to trick admin=
istrators and end users into trusting various Flame components by falsely c=
ertifying they were produced by Microsoft....
Based on the language in Microsoft's blog posts, it's impossible to rule ou=
t the possibility that at least one of the certificates revoked in the upda=
te was ... created using [previously reported] MD5 weaknesses [which allowe=
d collision attacks]. Indeed, two of the underlying credentials used MD5, w=
hile the third used the more advanced SHA-1 algorithm. In a Frequently Aske=
d Questions section of Microsoft Security Advisory (2718704), Microsoft's s=
ecurity team also said: "During our investigation, a third Certificate Auth=
ority has been found to have issued certificates with weak ciphers." The ad=
visory didn't elaborate."
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-02 04:06:21
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
I'll make just a couple of comments:
- Given the huge amount of material classified these days, SECRET doesn't seem to be a very high level any more, whatever its official definition.  TOP SECRET still means a great deal though.  But the really important stuff is compartmented (SCI), and Suite B is not approved for it - it has to be protected by unpublished Suite A algorithms.
- To let's look at what they want for TOP SECRET.  First off, RSA - accepted for a transition period for SECRET, and then only with 2048 bit moduli, which until the last year or so were almost unknown in commercial settings - is completely out for TOP SECRET.  So clearly they're faith in RSA is gone.  (Same for DH and DSA.)  It looks as if they are betting that factoring and discrete logs over the integers aren't as hard as people had thought.
The whole business of AES-128 vs. AES-256 has been interesting from day one.  Too many recommendations for using it are just based on some silly idea that bigger numbers are better - 128 bits is already way beyond brute force attacks. The two use the same transforms and the same key schedule.  The only clear advantage AES-256 has is 4 extra rounds - any attack against the basic algorithm would almost certainly apply to both.  On the other hand, many possible cracks might require significantly heavier computation for AES-256, even if the same fundamental attack works.  One wonders....
NSA also wants SHA-384 - which is interesting given recent concerns about attacks on SHA-1 (which so far don't seem to extend to SHA-384).
I don't want to get into deep conspiracy and disinformation campaign theories.  My read of the situation is that at the time NSA gave its approval to this particular combination of ciphers, it believed they were secure.  They seem to be having some doubts about RSA, DSA, and DH, though that could be, or could be justified as, ECC being as strong with much smaller, more practical, key lengths.
Now, imagine that NSA really did find a way in to AES.  If they were to suddenly withdraw approval for its use by the government, they would be revealing their abilities.  A classic conundrum:  How do you make use of the fruits of your cryptanalytic efforts without revealing that you've made progress?  England accepted bombing raids on major cities to keep their crack of Enigma secret.  So the continuation of such support tells us little.  What will be interesting to see is how long the support continues.  With work under way to replace SHA, a new version of the NSA recommendations will eventually have to be produced.  Will it, for example, begin a phase-out of AES-128 for SECRET communications in favor of requiring AES-256 there as well?  (Since there's no call so far to develop a cipher to replace AES, it would be difficult for NSA to recommend something else.)
It's indeed "a wilderness of mirrors", and we can only guess.  But I'm very wary of using NSA's approval of a cipher as strong evidence, as the overall situation is complex and has so many tradeoffs.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-01 20:33:56
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
We know they *say in public* that it's acceptable.  But do we know what they *actually use*?
Same problem.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-01 11:11:06
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
Well, sure.  But ... I find it hard to be quite so confident.
In practical terms, the vast majority of encrypted data in the world, whether in motion or at rest, is protected by one of two algorithms:  RSA and AES.  In some cases, RSA is used to encrypt AES keys, so an RSA break amounts to a bypass of AES.  If you want to consider signatures and authentication, you come back to RSA again, and add SHA-1.
This is not to say there aren't other techniques out there, or that new ones aren't being developed.  But to NSA it's clearly a game of numbers - and any kind of wedge into either of just two algorithms would expose huge amounts of traffic to interception.
Meanwhile, on the authentication side, Stuxnet provided evidence that the secret community *does* have capabilities (to conduct a collision attacks) beyond those known to the public - capabilities sufficient to produce fake Windows updates.  And recent evidence elsewhere (e.g., using a bug in the version of Firefox in the Tor Browser Bundle) has shown an interest and ability to actively attack systems.  (Of course, being able to decrypt information without an active attack is always the ideal, as it leaves no traces.)
I keep seeing statements that "modern cryptographic algorithms are secure, don't worry" - but if you step back a bit, it's really hard to justify such statements.  We *know*, in a sense, that RSA is *not* secure:  Advances in factoring have come faster than expected, so recommended key sizes have also been increasing faster than expected.  Most of the world's sites will always be well behind the recommended sizes.  Yes, we have alternatives like ECC, but they don't help the large number of sites that don't use them.
Meanwhile, just what evidence do we really have that AES is secure?  It's survived all known attacks.  Good to know - but consider that until the publication of differential cryptanalysis, the public state of knowledge contained essentially *no* generic attacks newer than the WW II era attacks on Enigma.  DC, and to a lesser degree linear cryptanalysis not long after, rendered every existing block cipher (other than DES, which was designed with secret knowledge of DC) obsolete in one stroke.  There's been incremental progress since, but no breakthrough of a similar magnitude - in public.  Is there really anything we know about AES that precludes the possibility of such a breakthrough?
There's a fundamental question one should ask in designing a system:  Do you want to protect against targeted attacks, or do you want to protect against broad "fishing" attacks?
If the former, the general view is that if an organization with the resources of the NSA wants to get in, they will - generally by various kinds of bypass mechanisms.
Of the latter, the cryptographic monoculture *that the best practices insist on* - use standard protocols, algorithms and codes; don't try to invent or even implement your own crypto; design according to Kirchoff's principle that only the key is secret - are exactly the *wrong* advice:  You're allowing the attacker to amortize his attacks on you with attacks on everyone else.
If I were really concerned about my conversations with a small group of others being intercepted as part of dragnet operations, I'd design my own small variations on existing protocols.  Mix pre-shared secrets into a DH exchange to pick keys.  Use simple steganography to hide a signal in anything being signed - if something shows up signed without that signal, I'll know (a) it's not valid; (b) someone has broken in.  Modify AES in some way - e.g., insert an XOR with a separate key between two rounds.  A directed attack would eventually break all this, but generic attacks would fail.  (You could argue that the failure of generic attacks would cause my connections to stand out and thus draw attention.  This is, perhaps, true - it depends on the success rate of the generic attacks, and on how many others are playing the same games I am.  There's no free lunch.)
It's interesting that what what little evidence we have about NSA procedures - from the design of Clipper to Suite B - hints that they deploy multiple cryptosystems tuned to particular needs.  They don't seem to believe in a monoculture - at least for themselves.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-01 02:01:25
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
I don't understand this claim.  Shor's work opened up a really hot new area that both CS people and physicists (and others as well) have rapidly jumped into.  There's been a huge amount of publication on quantum computing and, more generally, the field of quantum information.  No one - at least publicly - claims to know how to build a non-toy quantum computer here (the D-wave machine, if it's really doing quantum computation, is a special kind of machine and couldn't run Shor's algorithm, for example).  But there are many reported advances on the physics.  Simultaneously, there's quite a bit of published work on the algorithmic/complexity side as well.
A look at  will readily confirm this.  If you want to dig deeper, there's Scott Aaronson's blog at                                                         -- Jerry
The cryptography mailing list

@_date: 2013-09-11 22:02:33
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Why prefer symmetric crypto over public key crypto? 
This is not *necessarily* safe.  In another thread, we discussed whether choosing the IV for CBC mode by encrypting 0 with the session key was sufficient to meet the randomness requirements.  It turns out it does not.  I won't repeat the link to Rogoway's paper on the subject, where he shows that using this technique is strictly weaker than using a true random IV.
That doesn't mean the way it's done in Ed25519 is unsafe, just that you cannot generically assume that computing a random value from existing private information is safe.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-08 18:56:36
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Why prefer symmetric crypto over public key crypto? 
Jonathan Katz found the paper I was thinking of -                                                         -- Jerry
The cryptography mailing list

@_date: 2013-09-08 17:08:40
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Why prefer symmetric crypto over public key crypto? 
Spoke too quickly - that paper is something else entirely.  I still can't locate the one I was thinking of.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-08 17:06:19
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Why prefer symmetric crypto over public key crypto? 
It's even better than you make out.  If Eve does manage to get hold of the Alice's current keys, and uses them to communicate with Bob, *after the communication, Bob will have updated his keys - but Alice will not have*.  The next time they communicate, they'll know they've been compromised.  That is, this is tamper-evident cryptography.
There was a proposal out there based on something very much like this to create tamper-evident signatures.  I forget the details - it was a couple of years ago - but the idea was that every time you sign something, you modify your key in some random way, resulting in signatures that are still verifiably yours, but also contain the new random modification.  Beyond that, I don't recall how it worked - it was quite clever... ah, here it is:                                                          -- Jerry
The cryptography mailing list

@_date: 2013-09-08 11:32:14
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Why prefer symmetric crypto over public key crypto? 
Indeed, that was exactly what I had in mind when I suggested we might want to do without private key cryptography on another stream.
Not every problem needs to be solved on Internet scale.  In designing and building cryptographic systems simplicity of design, limitation to purpose, and humility are usually more important the universality.  Most of the email conversations I have are with people I've corresponded with in the past, or somehow related to people I've corresponded with in the past.  In the first case, I already have their keys - the only really meaningful notion of "the right key" is key continuity (combined with implied verification if we also have other channels of communication - if someone manages to slip me a bogus key for someone who I talk to every day, I'm going to figure that out very quickly.)  In the second case - e.g., an email address from a From field in a message on this list - the best I can possibly hope for initially is that I can be certain I'm corresponding with whoever sent that message to the list.  There's no way I can bind that to a particular person in the real world wit
 hout something more.
Universal schemes, when (not if - there's no a single widely fielded system that hasn't been found to have serious bugs over its operation lifetime, and I don't expect to see one in *my* lifetime) they fail, lead to universal attacks.  I need some kind of universal scheme for setting up secure connections to buy something from a vendor I never used before, but frankly the NSA doesn't need to break into anything to get that information - the vendor, my bank, my CC company, credit agencies are call collecting and selling it anyway.
The other thing to keep in mind - and I've come back to this point repeatedly - is that the world we are now designing for is very different from the world of the mid- to late-1990's when the current schemes were designed.  Disk is so large and so cheap that any constraint in the old designs that was based on a statement like "doing this would require the user to keep n^2 keys pairs, which is too much" just doesn't make any sense any more - certainly not for individuals, not even for small organizations:  If n is determined by the number of correspondents you have, then squaring it still gives you a small number relative to current disk sizes.  Beyond that, everyone today (or in the near future) can be assumed to carry with them computing power that rivals or exceeds the fastest machines available back in the day - and to have an always-on network connection whose speed rivals that of *backbone* links back then.
Yes, there are real issues about how much you can trust that computer you carry around with you - but after the recent revelations, is the situation all that different for the servers you talk to, the routers in the network between you, the crypto accelerators many of the services use - hell, every piece of hardware and software.  For most people, that will always be the situation:  They will not be in a position to check their hardware, much less build their own stuff from the ground up.  In this situation, about all you can do is try to present attackers with as many *different* targets as possible, so that they need to split their efforts.  It's guerrilla warfare instead of a massed army.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-06 20:25:12
@_author: Jerry Leichter 
@_subject: [Cryptography] Bruce Schneier has gotten seriously spooked 
A response he wrote as part of a discussion at Q: "Could the NSA be intercepting downloads of open-source encryption software and silently replacing these with their own versions?"
A: (Schneier) Yes, I believe so.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-10 10:42:12
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Techniques for malevolent crypto hardware 
Good observations, but I think you're being too pessimistic.  All the examples you give *could* be tested - but not by "ignorant black box testing" - testing that ignores not just what's inside the box, but the actual requirements on what the box is supposed to produce.  A non-seeded PRNG, and even one seeded with a very small amount of entropy, will be caught by a test that runs multiple instances of the PRNG from the system starting state and ensures that the ensemble of first outputs (and, for good measure, the first *couple* of outputs) has the right statistics.  Similarly, a test that inserts the same password into multiple instances of the same system in the same state can check that the hashed versions have the right statistics.  No, these can't catch deliberate attack code which produces random-looking values that the attacker can predict - no test can.  But it will catch a broad class of common errors.
The others aren't cryptographic issues and require different approaches.
The fact that there are bad testing practices - and that those bad practices are used all too often - doesn't mean there aren't good practices, and that they could not be applied.  Where the testing is bad because of ignorance of what is actually important and how to test for it, learning from the failures of the past is the way forward - which was exactly the point of "PRMG failures" classification.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-09 03:56:16
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Techniques for malevolent crypto hardware 
Which brings into the light the question:  Just *why* have so many random number generators proved to be so weak.  If we knew the past trouble spots, we could try to avoid them, or at least pay special care to them during reviews, in the future.
I'm going entirely off of memory here and a better, more data-driven approach, might be worth doing, but I can think of three broad classes of root causes of past breaks:
1.  The designers just plain didn't understand the problem and used some obvious - and, in retrospect, obviously wrong - technique.  (For example, they didn't understand the concept of entropy and simply fed a low-entropy source into a whitener of some kind - often MD5 or SHA-1.  The result can *look* impressively random, but is cryptographically worthless.)
2.  The entropy available from the sources used was much less, at least in some circumstances (e.g., at startup) than the designers assumed.
3.  The code used in good random sources can look "strange" to programmers not familiar with it, and may even look buggy.  Sometimes good generators get ruined by later programmers who "clean up the code".
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-29 13:13:41
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] RSA recommends against use of its own products. 
It's standard.  :-)
We've been through two rounds of standard data interchange representations:
1.  Network connections are slow, memory is limited and expensive, we can't afford any extra overhead.  Hence DER.
2.  Network connections are fast, memory is cheap, we don't have to worry about them - toss in every last feature anyone could possibly want.  Hence XML.
Starting from opposite extremes, committees of standards experts managed to produce results that are too complex and too difficult for anyone to get right - and which in cryptographic contexts manage to share the same problem of multiple representations that make signing such a joy.
BTW, the *idea* behind DER isn't inherently bad - but the way it ended up is another story.  For a comparison, look at the encodings Knuth came up with in the TeX world.  Both dvi and pk files are extremely compact binary representations - but correct encoders and decoders for them are plentiful.  (And it's not as if the Internet world hasn't come up with complex, difficult encodings when the need arose - see IDNA.)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-25 18:12:25
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] RSA recommends against use of its own products. 
In favor off ... who?
We already know that GCHQ is at least as heavily into this monitoring business as NSA, so British providers are out.  The French have been playing the "oh, we're shocked, shocked that there's spying going on" game - but they have a long history of their own.  It's been reported for many years that all Air France seats are bugged by the French security services and the information recorded has been used to help French economic interests.  And even if you don't think a particular security service has been going after in-country suppliers, recall decades of US spiking of the Swiss Crypto AG machines.
It's a really, really difficult problem.  For deterministic algorithms, in principle, you can sandbox the implementation (both physically and in software) and compare inputs and outputs to a specification.  That leaves you to worry about (a) holes in the specification itself; (b) physical leakage of extra information (Tempest-like).  Both of these can be dealt with and you can gain any degree of assurance you consider necessary, at least in principle.  Sure, someone can spike your hardware - but if it only does what the spec says it's supposed to do, what does that gain them?  (Storing some of your secrets within the sandboxed system does them no good if they can't get the information out.  Of course, physical security is essential, or your attacker will just walk the system, with all its contained information, out the door!)
For probabilistic algorithms - choosing a random number is, of course, the simplest example - it's much, much harder.  You're pretty much forced to rely on some mathematics and other analysis - testing can't help you much.
There are really no absolutes; you really have to think about who you want to protect yourself from and how much you are willing to spend, because there's no limit on how much you *could* do.  Build your own foundry?  Create your own circuit synthesis code?  You very quickly get yourself into a domain where only a handful of companies or countries can even begin to go.
My take on this:  I don't much worry about attacks against general-purpose hardware.  The difficulty of modifying a processor so that you can tell when it's implementing a cipher and then do something useful about it seems insurmountable.  The exception is when the hardware actually gets into the crypto game - e.g., the Intel AES extensions and the random number generator.  If you're going to use these, you need to do so in a way that's secure even if those features are spiked - e.g., use the random number generator only as one of a couple of sources.
Still, *much* more worrisome are the badly implemented, insecure extensions to allow remote control of the hardware, which are being discussed in a separate thread here.  These are really scary - there's no protection against an attacker who can send a magic packet to your network interface and execute code with full privileges.
Code, at least for symmetric cryptography primitives and modes, is simple enough that you can find it all over the place.  Realistically, the worst attacks against implementations these days are timing attacks.  Bernstein's ciphers have the advantage of being inherently secure against these, showing that this is possible (even if you don't necessarily trust his particular constructions).
Denker's ideas about how to get random numbers whose safety is based on physical principles are great.  You do have to be careful of the hardware and software you use, but since the hardware is designed for entirely different purposes (A/D sound converters) it's unlikely anyone has, or really could, spike them all.
It's the asymmetric algorithms and implementations that seem to be the most vulnerable.  They are complex and difficult to get right, much less to get both efficient *and* right, and protocols that use them generally need to be probabilistic - so "black box testing" isn't feasible.  At the same time, they have rich mathematical structures in which we know things can be hidden.  (In the symmetric case, the algorithms are generally have little mathematical structure, and we *assume* nothing can be hidden in there - but who can really say with absolute confidence.)  I had a long debate here earlier on this subject, and my own conclusions remain:  Use symmetric crypto as little as you possibly can.  (What would be really, really nice is something like DH key exchange without all the mathematical structure.)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-24 16:01:58
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] RSA recommends against use of its own products. 
The conclusion it leads to is that *if used in the default mode*, it's (well, it *may be*) unsafe.  We know no more today about the quality of the implementation than we did yesterday.  (In fact, while I consider it a weak argument ... if NSA had managed to sneak something into the code making it insecure, they wouldn't have needed to make a *visible* change - changing the default.  So perhaps we have better reason to believe the rest of the code is OK today than we did yesterday.)
a)  How would knowing this change the actions you take today?
b)  You've posed two alternatives as if they were the only ones.  At the time this default was chosen (2005 or thereabouts), it was *not* a "mistake".  Dual EC DRBG was in a just-published NIST standard.  ECC was "hot" as the best of the new stuff - with endorsements not just from NSA but from academic researchers.  Dual EC DRBG came with a self-test suite, so could guard itself against a variety of attacks and other problems.  Really, the only mark against it *at the time* was that it was slower than the other methods - but we've learned that trading speed for security is not a good way to go, so that was not dispositive.
Since we know (or at least very strongly suspect) that the addition of Dual EC DRBG to the NIST standards was influenced by NSA, the question of whether RSA was also influenced is meaningless:  If NSA had not gotten it into the standard, RSA would probably not have implemented it.  If you're asking whether NSA directly influenced RSA to make it the default - I doubt it.  They had plenty of indirect ways to accomplish the same ends (by influencing the terms of government purchases to make that a requirement or a strong suggestion) without leaving a trail behind.
And?  It's cool for discussion, but has absolutely nothing to do with whether (a) BSAFE is, indeed, safe if you use the current default (we assume not, at least against NSA); (b) BSAFE is safe if you *change* the default (most will likely assume so); (c) users of BSAFE or BSAFE-based products should make sure the default is not used in products they build or use (if they're worried about NSA, sure) (d) implementors and users of other crypto libraries should change what they are doing (avoid Dual EC DRBG - but we already knew that).
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-09-22 13:43:21
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] RSA recommends against use of its own products. 
Wow.  You took as holy writ on a technical matter a pronouncement of the general press.  And not just of the general press, but of RT, a Russian publication with a clear pro-Russian anti-American bias.  (Not that they don't deliver useful information - I've read them in the past, along with Al Jazeera and, on the flip side, The Debka Files.  They are all valid and useful members of the press, but their points of view, AKA biases, are hardly a secret.)
The original article in Wired is still a bit sensationalistic, but at least it gets the facts right.
BSAFE is a group of related libraries of crypto primitives that RSA has sold for many years.  They generally implement everything in the relevant standards, and sometimes go beyond that and include stuff that seems to be widely accepted and used.  Naturally, they also use the same libraries in some packaged products they sell.  As far as I know, the BSAFE implementations have been reasonably well thought of over the years.  In my experience, they are conservatively written - they won't be the fastest possible implementations, but they'll hold their own, and they probably are relatively bug-free.  A big sales advantage is that they come with FIPS certifications.  For whatever you think those are actually worth, they are required for all kinds of purposes, especially if you sell products to the government.
I remember looking at BSAFE for use in a product I worked on many years ago.  We ended up deciding it was too expensive and used a combination of open source code and some stuff we wrote ourselves.  The company was eventually acquired by EMC (which also owns RSA), and I suspect our code was eventually replaced with BSAFE code.
Since Dual EC DRBG was in the NIST standards, BSAFE provided an implementation - along with five other random number generators.  But they made Dual EC DRBG the default, for reasons they haven't really explained beyond "in 2004 [when these implementations were first done] elliptic curve algorithms were becoming the rage and were considered to have advantages over other algorithms...."
I'd guess that no one remembers now, six or more years later, exactly how Dual EC DRBG came to be the default.  We now suspect that a certain government agency probably worked to tip the scales.  Whether it was directly through some contacts at RSA, or indirectly - big government client says they want to buy the product but it must be "safe by default", and "Dual EC DRBG is what our security guys say is safe" - who knows; but the effect is the same.  (If there are any other default choices in BSAFE, they would be worth a close look.  Influencing choices at this level would have huge leverage for a non-existent agency....)
Anyway, RSA has *not* recommended that people stop using BSAFE.  They've recommended that they stop using *Dual DC DRBG*, and instead select one of the other algorithms.  For their own embedded products, RSA will have to do the work.  Existing customers most likely will have to change their source code and ship a new release - few are likely to make the RNG externally controllable.  Presumably RSA will also issue new versions of the BSAFE products in which the default is different.  But it'll take years to clean all this stuff up.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-20 14:22:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
Perry re-started this mailing list in response to the Snowden revelations about the NSA's attacks on Internet cryptography.  He raised the questions of whether we could make a Prism-proof Internet.
That's a big problem, and we've been debating small pieces of it ever since.  I'd like to suggest a smaller problem, just as a kind of rallying point.
This list has certainly attracted NSA interest.  Whether by subject, by keyword matching, or because so many of the participants here are clearly "adversaries" of those in the NSA responsible for gaining access to all that crunchy good stuff out there, there's no way anyone here could avoid scrutiny.
So ... imagine we don't like that.  How could this list be constituted in a "secure" way?  The quotes are on "secure" because even the definition of the word isn't clear.  Realistically, there's no way to avoid an NSA "plant" joining an open group, so perhaps there's little point in encrypting the messages.  Anonymous/pseudonymous posting?  Signed messages?  (A few members post them; hardly any of us do.)  Does that just make messages even more traceable/linkable?
We think we know what it means to have secure 1-1 email.  Adding a couple of additional participants seems as if it leaves the nature of the problem unchanged, but in fact you very quickly get into trust issues:  In a 1-1 conversation, I can decide whether to trust my correspondent.  In any multi-party conversation, it's likely that at least one of the participants doesn't know *all* the others, so must make an indirect trust decision:  I trust him because the others here seem to trust him.  For a mailing list, this problem explodes - while in addition there are all kinds of issue with how exactly to set up key exchanges.  A moderated group like this could be looked at as a star configuration:  In a way, each participant really only communicates with the moderator.  But that seems to miss the point - despite the moderation, this group *feels* like a free-flowing conversation among a group of people.
So what would a reasonable security model for the Cryptography list look like?  Is it inherently just an open discussion?  Or could we come up with something else?  If we can do more, what kind of software would be needed to make it as free-flowing and easy to participate in and manage as the current list?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-20 14:22:16
@_author: Jerry Leichter 
@_subject: [Cryptography] Mail Lists In the Post-Snowden Era 
Perry re-started this mailing list in response to the Snowden revelations about the NSA's attacks on Internet cryptography.  He raised the questions of whether we could make a Prism-proof Internet.
That's a big problem, and we've been debating small pieces of it ever since.  I'd like to suggest a smaller problem, just as a kind of rallying point.
This list has certainly attracted NSA interest.  Whether by subject, by keyword matching, or because so many of the participants here are clearly "adversaries" of those in the NSA responsible for gaining access to all that crunchy good stuff out there, there's no way anyone here could avoid scrutiny.
So ... imagine we don't like that.  How could this list be constituted in a "secure" way?  The quotes are on "secure" because even the definition of the word isn't clear.  Realistically, there's no way to avoid an NSA "plant" joining an open group, so perhaps there's little point in encrypting the messages.  Anonymous/pseudonymous posting?  Signed messages?  (A few members post them; hardly any of us do.)  Does that just make messages even more traceable/linkable?
We think we know what it means to have secure 1-1 email.  Adding a couple of additional participants seems as if it leaves the nature of the problem unchanged, but in fact you very quickly get into trust issues:  In a 1-1 conversation, I can decide whether to trust my correspondent.  In any multi-party conversation, it's likely that at least one of the participants doesn't know *all* the others, so must make an indirect trust decision:  I trust him because the others here seem to trust him.  For a mailing list, this problem explodes - while in addition there are all kinds of issue with how exactly to set up key exchanges.  A moderated group like this could be looked at as a star configuration:  In a way, each participant really only communicates with the moderator.  But that seems to miss the point - despite the moderation, this group *feels* like a free-flowing conversation among a group of people.
So what would a reasonable security model for the Cryptography list look like?  Is it inherently just an open discussion?  Or could we come up with something else?  If we can do more, what kind of software would be needed to make it as free-flowing and easy to participate in and manage as the current list?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-16 21:10:00
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] /dev/random is not robust 
Can we separate the issue of actual issues in the Linux RNG from the responses to this paper?
It's been a long time since I worked on an RNG, for a product that ran on systems that provided nothing like /dev/random; our Linux version mixed in /dev/random, and our Windows system mixed in whatever Windows calls its random number provider.  I have no doubt that the Linux RNG is immensely better than the hack we were able to cobble together - not being in the kernel severely restricted our entropy sources.  From what I've seen of the Linux RNG, it does a pretty good job.  I'm not about to suggest any code changes, and frankly nothing in this paper suggests any either.  Following its guidelines would produce an entirely new RNG, which absolutely *no one should trust* until it's been kicked around in the community for a while.
My problem is *entirely* with some of the intemperate responses I've seen out there, which basically say "Oh, that's just some academic nonsense, pay no heed".  I've seen nothing of that sort from you, Theodore Ts'o, personally; while you clearly have an emotional attachment to the work you've done - don't we all? - your responses are mainly technical in nature, pointing out places where the paper misunderstood what the current implementation does.  However, you do let a bit of an attitude come through in remarks about "not caring about publishing papers".  As I said, there is good and bad academic work, and not even all the good academic work is actually useful.  As an example of good and useful academic work, I usually point to Phil Rogoway's papers.  While the papers themselves are highly technical and get into a great deal of detail, what comes out at the end are simple designs that can be, and have been, implemented.  If you implement one of Rogoway's designs, you know t
 here's a formal proof behind it.  You don't need to actually understand the proof - others have reviewed it and it's almost certainly solid.  Merge sort will run in O(n log n) for you whether you understand the theory or not.
I see the paper as valuable for proposing strong security definitions for "PRNG's with input", showing that neither Barak and Halevi's algorithm nor the Linux RNG's algorithm attain those definitions, but suggesting an algorithm that does.  The answer "well, yes, the Linux generator fails if its entropy sources are bad in a particular way, but we have entropy sources that aren't" misses the point.  At one time, not so very long ago, no one knew how to build a cipher secure against a known-plaintext attack.  Today, that's assumed.  A defense of a modern cipher as "well, we won't let anyone see the plaintext" isn't good enough.  (Even worse is the claim that "you can only see the state of the PRNG from root, and then there are other attacks".  This isn't even true - a Linux system frozen into a VM can't prevent anyone from reading that state if they want it hard enough.)
Just as they extended Barak and Halevi's definitions, others may well come along and "tune" theirs.  That doesn't make theirs, or Barak and Halevi's, work "wrong"; it just makes them incomplete.  Others will also eventually come along and produce better algorithms that attain the various definitions that the community comes to agree on.
I'm not sure how the whole business of entropy estimation feeds into this.  There are others who've criticized it as just guesswork.  Frankly, they have a point.  John Denker's work on Turbid provides a much more principled approach to the problem.  Still, the Linux kernel has to work with what it has.
Whether new, better, strong approaches will come along in the future is impossible to tell.  If they do, I would hope those engineering Linux and other systems will have the humility to admit that the direction they've been going was, as it turns out, incorrect, and that they should start again from scratch.  That's how science and engineering are supposed to work.
Random numbers are way too important to allow the kinds of dismissive responses we've been seeing.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-24 20:14:48
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Anything you can connect to, by any means, that can transfer random bits to you.
The point of my comment is to counter the usual claim that getting anything through the network isn't helpful because your attacker may be watching everything go by on the network.  In fact, in many situations, it's extremely difficult to watch everything going by, and even if you as the attacker can, the node trying to grab data of the network may only grab some of it and it may be difficult to tell *what* it grabbed.
I'll be the first to admit that all if this is Denker's "squish":  You don't know how to predict it, but you can't prove it's unpredictable either.  I'd much rather have a vetted generator based on shot noise or something of that sort.  But if I *don't* have that, I have to do *something*.  Starting with the assumption of an omniscient attacker who has unfailing access to everything I can see or do leads to no solutions at all.  But it's also unrealistic in virtually all situations.  Attackers have some variation of a wiretap channel:  They see what you see but with more random errors, or perhaps just with *randomly different* errors.  If you can estimate the magnitude of the difference between your view and theirs, you can leverage it to move your state exponentially further away from their model of it, eventually leaving them with no useful information.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-24 16:55:00
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
As long as you're at it, ask a whole bunch of hosts, close and far, for 256 random bits from their own generators.  If even a single one of the response slips by an attacker, he's lost.
This is a process you can repeat periodically - and certainly at each boot - except that after the first time, you can use secure connections, with the best security you are able to set up with each particular host.  An attacker then would have to be able to not just see all the responses but also decrypt them.
There are no absolute assurances here, but you're inverting the usual attacker/defender relationship.  Usually, an attacker only has to find one hole, while a defender has to close all of them.  Here, the attacker has to grab *every* piece of the incoming information, without fail and repeatedly, while the defender doesn't really care whether all its sources respond with valid random data, or for that matter respond at all.  (In fact, an interesting approach in some situations might be to ask a bunch of sources to *flood* you with responses, knowing that your hardware will unpredictably drop some of them.  An attacker will have trouble knowing which ones made it through and which one's didn't; using "extra" information is just as bad as missing some information you *did* use.  I would want to know something about more about the nature of the transmission medium and the receiver before using this, though, as the way a given piece of hardware responds to overload *might* be pre
 dictable.)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-22 01:34:22
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
No, you're missing the point.
Mathematical cryptanalysis is about breaking highly specific idealized models using a particular set of techniques - usually formalized as something along the lines of "a probabilistic Turing machine receiving X, Y and Z on its input tape and running for no more than T seconds has a probability of less then epsilon of producing a result of a particular kind."
Real-world cryptanalysis includes the possibility that someone planted a camera in the ceiling above the desk where you do the paper-and-pencil computation of your perfect OTP cipher.
Mathematical cryptanalysis gives you an upper bound on the difficulty of the problem.  There's not much point in using a system if the upper bound you get is small relative to the value of what you're protecting.  But once it becomes large enough, measuring the mathematical difficulty becomes much less useful, as the vulnerabilities are elsewhere.
A true one-time-pad is completely secure against mathematical cryptanalysis, but that fact tells you almost nothing of interest about the security of a real-world system that uses a one-time-pad.  Remember how we got here:  From the initial claim that any system can be attacked given enough resources, to the counterclaim that a one-time-pad could *not* be attacked, to my counter-counter claim, just repeated here, that any real-world implementation *can* be attacked, given sufficient resources.  It's not a claim about the particular weaknesses of particular realizations of one-time-pads; it's a claim that *some* weakness will always be present.  The best you can do is make the cost of attacking that weakness exceed the value of whatever you're protecting.  *That* is the real-world analogue of the "perfect (mathematical) security" of a one-time-pad.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-21 19:35:53
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
...and this is the difference between real-world cryptanalysis and mathematical cryptanalysis.
Mathematical cryptanalysis, given infinite time and resources, can do nothing about OTP.
Real-world cryptanalysis, given sufficient, very finite, time and resources, can break into secure facilities and computers, bribe or blackmail people, tap into various kinds of leakage channels ... and break OTP.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-23 10:14:53
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] programable computers inside our computers (was: Hasty PRISM proofing considered 
Actually, there is a difference:  Palladium had remote attestation built in - it was a selling point.  People concentrated on that as the "bad" part, thought the rest could actually be useful.  The reference designs let you do whatever you wanted with your own device - you have full access to the trusted elements, could sign your own boot loader if you wanted.  Of course, someone providing DRM'ed material could refuse to talk to your system if it didn't attest to running "acceptable" code.
The new technologies don't build remote attestation in, so avoid the whole debate.  And the base technologies are neutral on the issue of whether you can write your own trusted code.  It's the specific implementations that block you from changing the keys, the bootloader, any of the code running in the secure element, etc.
The net effect is similar.  Nothing keeps a system builder from including remote attestation, but because of the nature of the devices, who is doing the controlling (the cell service providers), and the much higher level of integration of the components (making it harder to pull pieces out of the controlled environment) it really doesn't much matter:  If you're successfully talking to the cell network at all, they assume you have "approved" hardware. (Should people start building their own cell hardware from the ground up - certainly possible if you don't care about how practical the device is as a *cell phone*, but extremely difficult if you want something practical - they could always add remote attestation, or some simplified variant that's good enough for the cell provider's purposes, later.)
Palladium was subject to political attack because it was open about what it could do for DRM suppliers.  The new technologies are harder to attack this way because the responsibility is diffused, and the good and the bad are very thoroughly mixed together.  The availability of secure modes in the hardware can be explained as necessary to allow for safe operation in an unsafe world, and in and of themselves harmless - just a safer extension of user space/kernel space isolation.  The system builders build things to keep the systems safe from malware, a known and growing problem.  The network providers want to protect their networks.  Everyone sees the need for heavy protection - including from the device owner - of internal "wallets".
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-23 10:14:53
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] programable computers inside our computers (was: Hasty PRISM proofing considered 
Actually, there is a difference:  Palladium had remote attestation built in - it was a selling point.  People concentrated on that as the "bad" part, thought the rest could actually be useful.  The reference designs let you do whatever you wanted with your own device - you have full access to the trusted elements, could sign your own boot loader if you wanted.  Of course, someone providing DRM'ed material could refuse to talk to your system if it didn't attest to running "acceptable" code.
The new technologies don't build remote attestation in, so avoid the whole debate.  And the base technologies are neutral on the issue of whether you can write your own trusted code.  It's the specific implementations that block you from changing the keys, the bootloader, any of the code running in the secure element, etc.
The net effect is similar.  Nothing keeps a system builder from including remote attestation, but because of the nature of the devices, who is doing the controlling (the cell service providers), and the much higher level of integration of the components (making it harder to pull pieces out of the controlled environment) it really doesn't much matter:  If you're successfully talking to the cell network at all, they assume you have "approved" hardware. (Should people start building their own cell hardware from the ground up - certainly possible if you don't care about how practical the device is as a *cell phone*, but extremely difficult if you want something practical - they could always add remote attestation, or some simplified variant that's good enough for the cell provider's purposes, later.)
Palladium was subject to political attack because it was open about what it could do for DRM suppliers.  The new technologies are harder to attack this way because the responsibility is diffused, and the good and the bad are very thoroughly mixed together.  The availability of secure modes in the hardware can be explained as necessary to allow for safe operation in an unsafe world, and in and of themselves harmless - just a safer extension of user space/kernel space isolation.  The system builders build things to keep the systems safe from malware, a known and growing problem.  The network providers want to protect their networks.  Everyone sees the need for heavy protection - including from the device owner - of internal "wallets".
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-31 18:18:40
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
Jeez, don't people recognize smileys any more?
                                                        -- Jerry :-)
The cryptography mailing list

@_date: 2013-10-31 10:49:16
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
Ah, so like FIPS, Linux only accepts "real" entropy from "authenticated" sources.  :-)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-30 21:00:33
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
Are you so sure?
Consider a Linux-style RNG.  Suppose I know that all the existing sources produce k bits/second of randomness.  If I draw k bits/second of data out of it, after a while, it has no "spare" randomness inside - it's giving me exactly what it has.  If I draw j >> k bits/second out of it, it quickly "runs out".  It may block, effectively rate-limiting me; or it may stretch what it had.
Now suppose I inject j >> k bits of my own, controlled data, declaring that it represents j bits of entropy - all the while continuing to draw j bits out.  The generator now has plenty of entropy - or thinks it does - so never blocks.  But eventually it must be the case that I'm getting way more bits out than the real entropy going in.  If I can't predict the bits I'm getting out, it can only be because of the lingering entropy from the other sources.  (If j is much larger than k, then most of the bits I get out are computed without any bits other than my own going in.)
But this is an odd state of affairs.  If the assumption is that the results remain unpredictable, no matter how much larger j is than k, then why should the generator *ever* block because it's output more bits than it got in?  After all, that situation is effectively indistinguishable from it having gotten all 0 bits at some very high rate.
So:  For extra sources to always be harmless, it must be the case that the bits are unpredictable *even if no new entropy arrives*.  All that matters, in effect, is that the internal state be unknown and unpredictable *once*.  BBS has this property, as (on different assumptions) do crypto-based PRNG's like Yarrow.  But this has a performance cost, and I'm not sure that a Linux-style generator does.  If you have it ... why would you need to allow additional (allegedly random) sources?
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-10-30 18:09:20
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
Maybe.  Or maybe we just see a misapplied reasonable principle that any input that could affect sensitive data must be authenticated.
"Never attribute to malice what can be explained by incompetence."  One of the really bad things about the NSA's apparent attempts to subvert crypto is that it leads you to question this assertion.  We just have no way of knowing.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-05 15:22:22
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
So you think it's all about *you*?  If a mechanism doesn't protect *you*, *right now*, it's not a security mechanism?  Some security mechanisms aim to protect the population at large, not any particular individual at a particular point in time.
The point of checking URL's to see if they are "bad" is to protect those who look at them *after* they are found to be bad.  Properly checking URL's takes significant time.  You wouldn't want the check inserted in real-time into the Skype message stream.
I have never seen a URL in a Skype message removed or in any way marked as invalid; but then I'm not a heavy Skype user.  But really, a bad URL in a Skype message is not in and of itself a problem - it's only a problem when someone goes to that site in a browser.  I don't use IE and don't know where it gets a list of "dangerous" URL's from - but I would be surprised if it doesn't.  (Chrome, Firefox, and Safari all use a list that Google maintains.)
Not everything is the NSA.  It's clear they want to operate in the dark, not being noticed.  It seems highly unlikely they would start hitting random web sites a short time after they got mentioned in a Skype message.  If they were doing this themselves, I'd expect them to be patient, making it very difficult to correlate on random hit on the site with any particular action.
On the other hand, this is a fine source of URL's to feed into a malware detector.
Could the NSA piggy-back on Microsoft's data capture?  Sure.  But they have so many ways to get hold of URL's, I'm not sure why they they would bother.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-03 03:16:25
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
If an evil URL appears in a Skype message, the intent is almost certainly to get someone to go to that URL.  That's where the potential danger is.  Of course, for the recipient of the original URL, it's likely way too late.  This is a way to build up a database of URL's to warn people against - the *hope* is that large numbers of others will be protected.
I have no data one way or another whether this pays off, and I'm neutral on whether it's appropriate.  Scanning of email for malware of various sorts is nothing new and is generally thought to be a good idea.  I know of no argument why scanning of chats should be any different.  If it makes clear that the emails and chats are actually accessible to your provider ... well, knowing about it doesn't make it worse that having it happen without you knowing about it.  (Unless you believe in the NSA's idea that if you don't know your privacy has been invaded, it hasn't been.)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-02 19:49:39
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
That's not the NSA - it's Microsoft.  This pattern has been reported befor; it's the result of Microsoft searching for "evil" URL's (those that have drive-by malware, mainly, though I suppose they look for other stuff, too).
See  for one discussion.
(That's not to say NSA doesn't *also* do this, though with Microsoft already doing it, they would get more bang for the buck by just getting hold of the Microsoft databases.)
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-13 20:33:02
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
Why?  It's not like you're going to need to generate a new key; nor do disk encryption modes typically require an IV or any other associated random value.  (They are typically block by block, so there's no room to store anything beyond the encrypted block itself.)
Beyond that ... what data could *need* encryption?  By hypothesis, everything the system does is predictable to an opponent - otherwise we'd have our random seed.  What's the point in encrypting data an opponent can already predict?
It depends on what you wrote there - and on whether you're concerned that an attacker could have gotten his hands on it.  If you're not concerned about an attack who could see the physical disk, the raw data stored in an encrypted file system is a pretty good seed.  If you think someone go get access to the physical disk, but that they wouldn't have the key, then you can store all the state you want, securely per hypothesis, within one ore more encrypted file systems.
There really should not be.  Of course, people make dumb decisions all the time, and then find it easy to convince themselves that what they did is *right* and must be supported.    In that direction madness lies.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-12 21:24:40
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
This is why the continuous reseeding model is so insidious, and so difficult to understand.
For a CSPRNG with k bits of internal state, I would say there are two possible modes:
1.  Uninitialized.  The k bits have not yet been set to values chosen from a large enough set of possibilities.  (In general, we assume they ultimately get chosen uniformly at random from the set of all possible k-bit strings.  If you really want to consider the general case, the internal state might not be a k-bit string - it could be a value mod some large N, for example.  But this doesn't change anything fundamental.)  Once state has been set of k "unpredictable" bits, we transition to mode 2.
2.  Initialized, safe.  Those k bits have been set, and we assume an attacker has no knowledge of their values.
In mode 1, it's *impossible* to deliver useful outputs.  Any games that say things like "oh, I have k/2 bits in my internal state properly initialized so I'll only deliver k/4 bits of output and that should be safe" are just making things up as they go along.  Maybe one can design a system in which this is safe, but I haven't seen one yet.  A proper design *must not* deliver any outputs when it's in mode 1.
When a properly designed generator is in mode 2, it can deliver some reasonable fraction of 2^k output bits before its internal state is *computationally* no longer safe.  *You can't use an entropy measurement here!*  Entropy is concerned with absolute security:  Can an opponent with unbounded computation power determine the state.  It is almost certainly the case that once you've output k generated bits, the internal state is uniquely determined.  An unbounded Turing machine can simply enumerate all 2^k possible starting state values and see which produces the k outputs actually seen.  It will almost always find that there is exactly one starting state.  (If there are a couple, a few more outputs will certainly determine the starting state.  This is just a counting argument.)  From the point of view of entropy, every bit of output reduces the internal entropy by one bit.  What a CSPRNG guarantees (modulo assumptions about the crypto primitives used) is that no *polynomially  bounded* (probabilistic) TM can gain any significant advantage over guessing the next bit, or working backwards to previous states.  If the primitives used are exactly the ones you're going to use elsewhere in the system, then you have exactly as much reason to trust them there as you do to trust them within the CSPRNG.
After some fraction of 2^k outputs, a CSPRNG *must* move back to mode 1 and somehow get another k bits of "unknowable" state - regardless of any possible compromise.  In the event of a possible compromise of the state, the generator must also transition back to mode 1.
But ... how can we know if a compromise was possible?  Continuously reseeding generators just start with the assumption that a compromise of the internal state *could* occur at any time.  If compromises an occur as fast as the CSPRNG can produce outputs, then it's not contributing anything:  It will have to fully re-seed between every two successive output bits, so it's just burning through the random bits used to seed it k bits at a time.  Better to simply output the true random bits as they arrive!  The obvious *safe* way to do this is to wait for k good random bits to accumulate, then replace the entire state of the CSPRNG, completely eliminating any previous knowledge an adversary might have had.  What's dangerous - and you've illustrated the problem - is to add a few bits of "good randomness" into a compromised state.  I don't see any way to avoid this problem.  (In the paper about the security of the Linux generator that Theodore Tso forwarded a link to earlier, there's
  a result showing that its pool combination and stirring is a "mixing function", in the sense that the result has at least much entropy as either of the inputs.  But that doesn't help in the case of a state compromise, since in that case the entropy of the internal state is 0.)
How often should we reseed?  There's no certain way to know without knowing how a state compromise might occur.  You'd like to reseed at an interval short enough that a compromise during that interval is "unlikely enough".
If you assume that there is no correlation between how fast bits are drawn from the CSPRNG and how likely a compromise is at any point in time, there are simple relationships that let you say something like:  Assuming can generate r true random bits/second, the chance of a state compromise is one every c seconds, and the total demand on the CSPRNG is d bits/second, and I've drawn n bits, the chance that m of them were compromised is no more than p.  If I can't increase r, I can make p lower by limiting d (by blocking if the demand rate is too high).
This suggests a very simple design:  There's a CSPRNG with k bits of internal state that's the source of random numbers for virtually all purposes.  There's a source of "true random" bits.  The CSPRNG blocks until it receives (either from the "true random" generator or from a previously-saved and assumed to be uncompromised) k seed bits; then it delivers those bits freely, with a possible rate cap to limit d as suggested above.  In parallel, we continue to gather bits from the true random generator; when we have k of them, we completely replace all k seed bits.
As far as I can see, this is (a) as safe as possible within constraints you can manipulate as you like (k, d, r, p); (b) simple, hence "obviously correct".  It's perfectly OK to allow access to the "true random number generator" so long as you reserve the assumed r bits/second for the CSPRNG, which almost everyone should be using.
What advantages do other approaches offer?  ("Not blocking" in favor of returning values that may not be safe is not a good option.  Except for initial startup, you can manipulate what you think is safe well enough, rather than just going by a hope and a prayer.  There's simply nothing you can to at startup if you don't have sufficient true random bits to get started, and you're not doing anyone a favor by trying.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-11 22:56:31
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
Every DSL modem I've ever seen can display noise information in multiple frequency bands.  (It's always gathering this information to adjust its use of the link.)  Firmware would have access to it, and it's likely a very good source to use for driving an RNG.  (Yes, you'd want to first invest some effort in determining exactly how this data is sampled, whether it's exported anywhere - the adjustment is done by the two ends of the link together, but I know nothing about what specific information is exchanged between them, etc.)
I know nothing about cable modems, but most likely they have access to similar kinds of information:  Sending data across long, uncontrolled spans of wire will generally require some sort of adaptation to the characteristics of that wire.
All that said ... I have yet to see a DSL or cable modem that *needs* a secure source of random numbers.  They live at L2 and below and don't encrypt or decrypt anything.  OK, they usually have http interfaces for management - which should really be https and they should come pre-configured with a certificate, as they come with a unique password.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-08 01:11:57
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
This is a very weak and inappropriate definition of a PRNG.  The BBS generator *provably* can reveal many more bits than its seed without allowing an attacker to gain any significant advantage in computing either past or future outputs.  (The actual security statement is complicated to state.  Since BBS is way too slow to be practical, the details aren't all that interesting anyway.)  The proof, as is the case for all proofs we have in cryptography, is based on the difficulty of quadratic residue computation, which in turn is equivalent to factoring.
A practical generator that takes the one-way hash of its internal state using something like SHA doesn't have any strong proofs associated with it, but in practice is as "unpredictable" as the cryptographic algorithms it's use with even if way more bits of output than were in the initial seed.  (You still don't release the intact internal state, of course, as then anyone can compute the next value.)
This is why I don't like "entropy talk".  If you want to discuss *information theoretic security*, then, yes, you should (very carefully, see my comments earlier about the "entropy cost" of a three-way choice) count bits.  But no algorithm you're going to feed these values into comes with information theoretic security claims, just computational hardness claims.  (Well, OK, if you want to use the output to generate a one-time-pad, you want information theoretic unpredictability.  But that's not a particularly interesting case.)
A *good* PRNG releasing one bit should allow a "reasonable" (polynomially-bounded) attacker way less than that - essentially nothing.  The security claims for AES-128 state that with an unknown key K, the stream of values produced by E(K,0), E(K,1), ... - i.e., the values used in counter mode - are indistinguishable from a random sequence even as you approach 2^64 *blocks* of output.  Wouldn't you want your PRNG to have a similar security property?  And if you have such a property, what would be the point of a *stronger* requirement when generating an AES key?  (There *is* a point; see below.)
Granted, a cryptographically strong one-way or encryption function in your PRNG may be too slow.  But unless you have hardware and associated software to generate random bits you trust at a very high rate, even a couple of invocations of a good PRNG is going to generate strong values faster than you can pull them from your environment.
This is an entirely different class of attacks, and absolutely, to defend against "state exposure" attacks, you need a way to get some new, independently unpredictable, state.  Of course, the kernel debugger attack is a funny one:  You're assuming an attacker who can read all of your state, but can't modify things (like, say, the constants used to decide how many bits of entropy various collectors contribute, even if you want to assume that the code is somehow protected from modification).  I think a more interesting state exposure attack is against a VM image.  The image could be signed to prevent any modification, but still expose its internal state.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-07 19:08:40
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
I answered this separately:  Adding writable storage will most likely *reduce* security.
The smartphone and the embedded system are *very* different.  Any phone has a microphone and a radio.  Unfortunately, the radios are usually sealed off so you couldn't use them as a source of radio noise; but the microphones are wide open.  And there are plenty of other environmental sensors - location, movement, orientation, ambient lighting level.  Not to mention a display you can use to ask the user to enter stuff on the keypad.  Smartphones are *easy* - though historically some of them have done a crappy job, even with the rich sources they have available.
Embedded systems are among the hardest.  People want routers and switches and similar hardware to need zero configuration, and yet some of them play essential cryptographic roles and really need good sources of randomness.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-07 04:16:42
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
I don't know how many such systems are out there, but if there are such, they are likely old or very cheap embedded systems that it'll be tough to get software updates onto, and impossible to get new hardware onto.  Declaring them "unsound" may not make the go away.
In fact, though, I can think of one simple example:  A CD Linux image used precisely to conduct operations we want to keep secure.  For example, there's a suggestion that small businesses use exactly such a thing to do their on-line banking, as their usual systems are way too vulnerable to various kinds of malware (and small businesses have been subject to attacks that bankrupted them).  The CD itself can't carry a seed, as it will be re-used repeatedly.  It has to come up quickly, and on pretty much any hardware, to be useful.  You could probably get something like Turbid in there - but there are plenty of CD's around already that have little if anything.
Engineering, like politics, is often the art of the possible - and this is exactly a situation where we need to look for solutions that make the situation as much better as we can.  A request for a random seed on the LAN - whether through a DHCP extension, or in some other way - would at least protect against attackers not in a position to watch the local LAN.  Not ideal, but compared to what may be there now - nothing - a step forward.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-05 17:38:20
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
However *you* may be using the word, its use in most discussions *surrounding cryptography* is not particularly technical.
An ahistorical view.  When Shannon introduced the term, it was by analogy.  In the 1930's and '40's, no one thought about physics in information terms.  Hell, no one thought about "information" as something formalizable.  That was Shannon's genius.  But even he didn't realize that his notion of information entropy would prove to have deep and profound connections to physical entropy.  That wasn't really recognized until 20 or more years later.
Agreed.  But you're the one who started on what I think is a fool's errand of trying to get the word used "correctly" in the random number generator community.
Here's a simple example - which I alluded to earlier - that demonstrates why this is much more difficult than it seems.  Suppose I have a source of true random bits.
a)  I want to draw a number uniformly at random from the set {0, 1}.  "Entropy" tells me that 1 bit from the source is enough.
b)  I want to draw a number uniformly at random from the set {0, 1, 2, 3}.  Same argument - two bits "of entropy" are exactly what we need.
c)  I want to draw a number uniformly at random from the set {0, 1, 2}.  Clearly "I need less entropy" than in case (b) - I'm *delivering* less entropy.  But ... you can't actually do this with two random bits.  About the best you can do is:  Take two bits of entropy.  If the result is 00, 01, or 10, you're done; otherwise draw another two bits.  Repeat until done.  The expectation value for the number of bits required is *at least* (3/4)*2 + (1/4)*4 = 2.5 bits; really, it's more.  (There are other correct ways - and many, many *incorrect* ways - to make this "random" choice, but none of them can use fewer bits than this.)
Naive arguments about entropy will lead you astray.  Naive entropy estimates will lead you astray.  If you want to do the math, do the math - don't guess.
I know perfectly well how to do computations with entropy.  That's exactly why I'm disturbed by the way the term gets misused.
I would put it all differently.  The two are barely comparable.
- A TRNG draws from a random distribution, a concept that is mathematically an axiom.  We believe (with very good reason) that a TRNG provides an appropriate model for the operation of some physical processes; and, flipping this around, we believe we can realize, in the physical world, something with properties akin to the mathematical abstraction by building an appropriate physical system.
- A (cryptographically strong) PRNG is a product of computational complexity theory.  It is *defined* in terms of the complexity of particular models of computation.  The "connection" between them shows up entirely in the way the PRNG and the attack models for it are defined:  The attacker is assumed to receive as input the outputs of the PRNG, but not its internal state.  If the internal state were somehow "sneaked into" the computation, the whole thing would break down.  Mathematically, we can simply say "the only information available to the machine is what is on its input tape", but in a physical realization, the internal state has to come from *somewhere*, and that "somewhere" must not leak any information to the (physical) attacker or the pretty mathematical model fails.  The only way of providing such internal state, in general, is with a TRNG.
It depends on who you count as "in the world".  If you're talking about physicists and information theorists and communications engineers, I'll agree with you.  If you're talking about designers of nifty new "random number" sources, complete with "entropy estimates" "conservatively" pulled out of a hat; or those who talk about "how many bits of entropy" the RNG must gather before we attempt to choose a value from {0, 1, 2} - well, not so much.
I don't know what "other cases" you're comparing to.
Probability theorists and statisticians generally talk about "randomness", not entropy - but surprisingly little, since that's in their axioms; rather, they talk about "drawing from a random distribution".  Sure, you can think of a random distribution as something with a source entropy, and you do that in information theory all the time, but for many other purposes - e.g., if you want to compute the moments or think about correlations - that's probably not the best representation to use.
"Unpredictability" is a nice informal catch-all for what we'd like to get, but if that's all you have, you're not ready to specify or build a system.
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-05 12:18:58
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
Well, yes, and in its technical definitions - either in thermodynamics where it arose or information theory where it was imported because of a similarity in formalisms - it plays virtually no role in cryptographic discussion.  In cryptography, especially when discussing random number generators, the word has a loosy-goosy meaning that's somehow tied to lack of predictability - but in an unspecified way.  When people make it a regular habit to "guess" the entropy of various processes and then go on to build systems based on guesses, you know the word has lost any formal meaning it ever had.
While the distinctions you draw are valid and important, I'm afraid "entropy" no longer has the capability to distinguish them.
I don't know what this means.
I don't know what *this* means either.  I drew a distinction in an earlier post - a distinction you can find in many papers on cryptographic primitives - between random values (unpredictable to the attackers being considered) and nonces (never repeated in a given cryptographic context, but there are no assumptions about unpredictability).  Where a random n-bit value is specified, I can think of no paper that does not assume what we call "n bits of entropy" - though a better way to say it is "a value chosen uniformly at random from the set of n-bit strings".  Sometimes the value is supposed to be chosen uniformly at random from some other set - e.g., from Z/p, i.e., between 0 and p-1.  Trying to state this in terms of entropy is a losing game - and in fact it isn't actually trivial, given a random *bit* source, to produce such a distribution.  People have gotten it wrong in the past.  (The obvious technique of choosing a k with 2^k > p, choosing a "random" k-bit value, and the
 n reducing it mod p, if described in terms of "entropy", looks fine - I have k bits of entropy, which is more than enough to cover the choice I need to make.  But the output is biased.)
"Unicity distance" is a nice but different concept - and one with little bearing on cryptography today.  (If I draw a cipher E at random from some a collection of functions - e.g., by choosing a key - then the unicity distance is the unicity distance is just the number of pairs (x, E(x)) that specify E uniquely within the set.  I suppose you can stretch this to talk about how many samples from the "random" generator are needed to specify *it* uniquely - i.e., be able to determine all past and future states - but I've seen the term used that way.)  A broader - hence more useful - classical term (which may have been introduced by Shannon) is "equivocation".  I don't recall the formal definition, but it attempts to capture the uncertainty an attacker has about the next plaintext, given all the information he already has.  If I know a message was encrypted using a one-time pad, my uncertainty about the first character is not absolute - it's much more likely to be "t" than "z".  T
 o provide information-theoretic security, a one-time-pad must contain "enough randomness" to keep my a priori and a postiori equivocation equal.  This *is* something that can be given in terms of the entropies of the message and one-time-pad sources, but the deracinated notion of "entropy" one sees in most discussions is way too weak to say anything useful here.)
A BBS generator is "indistinguishable" from a true random number generator.  What's missing from that statement - and from the distinction you're drawing above - is a specification of the attack model.
The BBS generator has inherent limitations that are given in its proof of correctness:  The attacker has polynomially bounded resources (in fact, "attacker" literally means a polynomially-bounded probabilistic TM), which in turn implies that it only has access to a polynomially-bounded number of outputs.  These are generally fine.  The proof doesn't bother to state (though it's implicit) the obvious:  That the attacker doesn't have access to the internal state of the generator.  This isn't an "entropy" assumption - the internal state must, to the attacker, appear to have been chosen uniformly at random from all possible internal states, and is not available to the attacker.  If you want to use "entropy-talk", all the entropy in a BBS generator is there, at the start, in the choice of the initial state.  And yet there's a profound difference between the output of a BBS generator and the output of, say, a linear congruential PRNG starting with exactly the same state.  One is pr
 edictable given a few successive samples; the other is secure given any polynomially-bounded number of them.  "Entropy" simply cannot capture this distinction.  And it's in fact exactly the distinction - in the transition from Shannon's classic notions of information-based security to modern computability-based ones - that make it possible to say anything useful at all about encryption algorithms other than one-time-pads.
While we have no proofs like those for BBS for PRNG's built on practical cryptographic primitives, we generally assume that they have similar properties.  (More correctly, we can prove some properties given assumptions about others.  But those are the same assumptions we make about the actual encryption and hashing and signature algorithms we use.  If they fail, the whole system falls down regardless of the source of "random" values.)
To summarize:  The distinction between cryptographic PRNG's and "true" RNG's has to do with the attack model.  The attacker considered in the former is polynomially bounded (OK) and doesn't receive as input some special piece that we label "internal state" (this one can be violated).  The attacker against a "true" RNG is ... what?  Formalizing that is equivalent to formalizing randomness - which no one has managed to do.  (In fact, modern developments of probability theory don't even try - random distributions are simply among the axioms.  Even more, if you believe John Conway, the "Free Will" Theorem he and he and Simon Kochen proved show that "quantum unpredictability" is *stronger* than randomness!)  In practical cryptography, this translates into "whatever I can imagine".  In my other thread on plausible attacks, I tried to focus on limiting the attacker to, well, plausible operations in a particular real-world setting.  If you *don't* do that, you can make no progress at
  all.  A hardware generator - even Turbid - is vulnerable if I include as plausible an attacker who's infiltrated every supplier of electronic components in the world and has slipped a small transmitter into everything built, including every diode, resistor - hell, maybe every piece of wire!
I have no problem with designing strong, practical random number generators.  I'd love to see them deployed more widely.  But clever designs of low-level primitives, as important as they are, are not a substitute for *system* designs; in fact, they can blind us to the necessary system properties.  If a cryptographic primitive needs some unpredictable input, we need to go further and ask (a) how much? (b) unpredictable under what attack model?  Only then can we begin to answer the *system* design question of "what's a suitable generator for such inputs?"  If we can, for a reasonable cost (measured in any appropriate way), get our hands on a generator whose attack model assumes way more attacker resources than attacks on multiple uses of those values - great, let's make use of it.
Too much discussion of "random number generators" is the equivalent of "I'm not sure AES is strong enough, so I'll do a ROT-13 encoding first - it can't hurt".  And it can't - until you run into Tony Hoare's comment to the effect that "You can make a system so simple that you can see at a glance that it's correct, or so complex that you can't see at a glance that it's *not* correct."
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-11-05 12:18:58
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] randomness +- entropy 
Well, yes, and in its technical definitions - either in thermodynamics where it arose or information theory where it was imported because of a similarity in formalisms - it plays virtually no role in cryptographic discussion.  In cryptography, especially when discussing random number generators, the word has a loosy-goosy meaning that's somehow tied to lack of predictability - but in an unspecified way.  When people make it a regular habit to "guess" the entropy of various processes and then go on to build systems based on guesses, you know the word has lost any formal meaning it ever had.
While the distinctions you draw are valid and important, I'm afraid "entropy" no longer has the capability to distinguish them.
I don't know what this means.
I don't know what *this* means either.  I drew a distinction in an earlier post - a distinction you can find in many papers on cryptographic primitives - between random values (unpredictable to the attackers being considered) and nonces (never repeated in a given cryptographic context, but there are no assumptions about unpredictability).  Where a random n-bit value is specified, I can think of no paper that does not assume what we call "n bits of entropy" - though a better way to say it is "a value chosen uniformly at random from the set of n-bit strings".  Sometimes the value is supposed to be chosen uniformly at random from some other set - e.g., from Z/p, i.e., between 0 and p-1.  Trying to state this in terms of entropy is a losing game - and in fact it isn't actually trivial, given a random *bit* source, to produce such a distribution.  People have gotten it wrong in the past.  (The obvious technique of choosing a k with 2^k > p, choosing a "random" k-bit value, and the
 n reducing it mod p, if described in terms of "entropy", looks fine - I have k bits of entropy, which is more than enough to cover the choice I need to make.  But the output is biased.)
"Unicity distance" is a nice but different concept - and one with little bearing on cryptography today.  (If I draw a cipher E at random from some a collection of functions - e.g., by choosing a key - then the unicity distance is the unicity distance is just the number of pairs (x, E(x)) that specify E uniquely within the set.  I suppose you can stretch this to talk about how many samples from the "random" generator are needed to specify *it* uniquely - i.e., be able to determine all past and future states - but I've seen the term used that way.)  A broader - hence more useful - classical term (which may have been introduced by Shannon) is "equivocation".  I don't recall the formal definition, but it attempts to capture the uncertainty an attacker has about the next plaintext, given all the information he already has.  If I know a message was encrypted using a one-time pad, my uncertainty about the first character is not absolute - it's much more likely to be "t" than "z".  T
 o provide information-theoretic security, a one-time-pad must contain "enough randomness" to keep my a priori and a postiori equivocation equal.  This *is* something that can be given in terms of the entropies of the message and one-time-pad sources, but the deracinated notion of "entropy" one sees in most discussions is way too weak to say anything useful here.)
A BBS generator is "indistinguishable" from a true random number generator.  What's missing from that statement - and from the distinction you're drawing above - is a specification of the attack model.
The BBS generator has inherent limitations that are given in its proof of correctness:  The attacker has polynomially bounded resources (in fact, "attacker" literally means a polynomially-bounded probabilistic TM), which in turn implies that it only has access to a polynomially-bounded number of outputs.  These are generally fine.  The proof doesn't bother to state (though it's implicit) the obvious:  That the attacker doesn't have access to the internal state of the generator.  This isn't an "entropy" assumption - the internal state must, to the attacker, appear to have been chosen uniformly at random from all possible internal states, and is not available to the attacker.  If you want to use "entropy-talk", all the entropy in a BBS generator is there, at the start, in the choice of the initial state.  And yet there's a profound difference between the output of a BBS generator and the output of, say, a linear congruential PRNG starting with exactly the same state.  One is pr
 edictable given a few successive samples; the other is secure given any polynomially-bounded number of them.  "Entropy" simply cannot capture this distinction.  And it's in fact exactly the distinction - in the transition from Shannon's classic notions of information-based security to modern computability-based ones - that make it possible to say anything useful at all about encryption algorithms other than one-time-pads.
While we have no proofs like those for BBS for PRNG's built on practical cryptographic primitives, we generally assume that they have similar properties.  (More correctly, we can prove some properties given assumptions about others.  But those are the same assumptions we make about the actual encryption and hashing and signature algorithms we use.  If they fail, the whole system falls down regardless of the source of "random" values.)
To summarize:  The distinction between cryptographic PRNG's and "true" RNG's has to do with the attack model.  The attacker considered in the former is polynomially bounded (OK) and doesn't receive as input some special piece that we label "internal state" (this one can be violated).  The attacker against a "true" RNG is ... what?  Formalizing that is equivalent to formalizing randomness - which no one has managed to do.  (In fact, modern developments of probability theory don't even try - random distributions are simply among the axioms.  Even more, if you believe John Conway, the "Free Will" Theorem he and he and Simon Kochen proved show that "quantum unpredictability" is *stronger* than randomness!)  In practical cryptography, this translates into "whatever I can imagine".  In my other thread on plausible attacks, I tried to focus on limiting the attacker to, well, plausible operations in a particular real-world setting.  If you *don't* do that, you can make no progress at
  all.  A hardware generator - even Turbid - is vulnerable if I include as plausible an attacker who's infiltrated every supplier of electronic components in the world and has slipped a small transmitter into everything built, including every diode, resistor - hell, maybe every piece of wire!
I have no problem with designing strong, practical random number generators.  I'd love to see them deployed more widely.  But clever designs of low-level primitives, as important as they are, are not a substitute for *system* designs; in fact, they can blind us to the necessary system properties.  If a cryptographic primitive needs some unpredictable input, we need to go further and ask (a) how much? (b) unpredictable under what attack model?  Only then can we begin to answer the *system* design question of "what's a suitable generator for such inputs?"  If we can, for a reasonable cost (measured in any appropriate way), get our hands on a generator whose attack model assumes way more attacker resources than attacks on multiple uses of those values - great, let's make use of it.
Too much discussion of "random number generators" is the equivalent of "I'm not sure AES is strong enough, so I'll do a ROT-13 encoding first - it can't hurt".  And it can't - until you run into Tony Hoare's comment to the effect that "You can make a system so simple that you can see at a glance that it's correct, or so complex that you can't see at a glance that it's *not* correct."
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-12-21 17:45:21
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
[About problems getting signed/encrypted mail to work in Mail.app, concluding:]
Apparently, there's one step you have to take that's mentioned nowhere in the documentation:  After you put your new key in the keychain, you have to restart Mail.app.  I normally let it run indefinitely, and didn't even consider this possibility.  Then yesterday I restarted it for some reason I no longer remember.  Suddenly, the option to send signed or encrypted mail ... just appeared!
                                                        -- Jerry
The cryptography mailing list

@_date: 2013-12-12 21:28:10
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
(Issues with using S/Mime - or PGP - with Outlook.)
Out of curiosity, I decide to see how this worked in Mail.app, which has built-in support for S/Mime.  Incoming support for signed messages is easy:  When a signed message arrives, the Mac automatically downloads the necessary certificate, installs it in your Keychain, and adds an indication to the mail.  I've never received an encrypted S/Mime message, so I don't know the flows in that case.
As for sending signed mail, it's easy to find an Apple article -  - telling you how to send a signed or encrypted message.  It has a link to an article telling you how to install a certificate in your keychain.
Unfortunately, it gives you no hint about how to actually *get* such a certificate.  Most users would probably get stuck at this point.
For those willing to do a bit of work, a quick Google search for "get mail signing certificate" led me to Comodo, where it was fairly straightforward to create a certificate.  After confirmation, you end up at a page that tells you it's trying to download and install your certificate.  But it just sits there - I don't know if the "and install" part can work on a Mac at all, or whether it only works because I disable "open safe files automatically".  But eventually I figured out that it had downloaded a small .p7c file.  I tried all the recommended ways to add it to Keychain.  From the GUI, nothing seemed to happen.  Using the command line "certtool" utility, I was able to get an error message claiming that the file had "Bad PEM formatting" and an abort.  Except, as I found out much later ... I had, somewhere along the way, already added the certificate.  (A discussion on the Comodo website shows that others have had the same problem for months; no solution was given.)
Since the Comodo certificate seemed not to work, I went back to my search and found CACert.  I again created and download a certificate; this one seemed to install just fine.
Unfortunately, though, Mail doesn't see the certificates.  I tried repeatedly to sign this message - including along the way marking my CACert certificate, and the CACert public CA certificate, as trusted from Email (as the Mac considers it an unknown certificate authority otherwise).  (The Mac already trusts Comodo.) The option to sign just fails to appear.  Of course, this being a Mac, when you use the GUI you get no error messages.  (There's nothing in the system logs either.)  "It just works" or "It just doesn't work" - nothing in between.
Summary:  On the surface, the Mac provides easy-to-use support.  But when you actually try to enable it, it fails in a way that is certainly beyond the ability of most users to fix - and even being quite knowledgable about this stuff, after 1/2 an hour or so of trying, I gave up.
I suspect the Mac implementation works very well when someone sets up certs for end users; just using them is easy.  But for the ordinary user trying to get going on his own, the problems are probably insurmountable.
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-02-18 15:53:04
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Are Tor hidden services really hidden? 
Going off on a side track, but are you seeing any effect from SafePlug (  This is a little box made by the guys who build PogoPlug, which lets you make your disks available over the Internet.  SafePlug acts as a proxy that sends your browser request through Tor.  If can also be configured to offer itself as a full Tor node.  Hard to beat at $49.
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-07-27 13:58:17
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] hard to trust all those root CAs 
My interpretation - the lawyer's comment was brief and without detail.
You'll often see complaints that "this law is inconsistent with the contracts I've made with my customers" or "this law makes it impossible for me to make money" or "it's impossible for my business to comply both with law A and law B".  When you get comments like this, it's usually with the implication "...therefor the law is invalid".  But in fact the *correct* implication is "...therefor I need to find another business to run."
That's not to say the law might not be invalid!  But there would have to be *other reasons* beyond "it puts me out of business".  After all, street corner heroin dealers - and clear fraudsters - could make the same arguments.
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-07-26 22:48:44
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] hard to trust all those root CAs 
The comment I heard on this from a very experienced, very tech-savvy lawyer was:  I would have loved to take this case back when I was a prosecutor (making it quite clear that he was sure he would win).
For an idea of the direction the law has gone on this general class of things:  Destruction of evidence has long been illegal and severely punished. At one time, it was assumed (more or less) that you were at risk once you were told you (well, a criminal act) were under investigation and reasonably knew that what you were destroying was evidence. Then the standard became broader - you needed to know little in the way of details. Today, we've got prosecutors claiming that if you knew you were destroying evidence of a criminal act, they can go after you. The courts are more or less agreeing.
The prosecutor's view will be:  *You* deliberately put yourself in a situation where to obey the law requires you to keep publishing. It's not the law's problem. You get to live with the consequences of your decisions.                                           -- Jerry
The cryptography mailing list

@_date: 2014-07-19 21:38:35
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] hard to trust all those root CAs 
This is an "it depends" situation.
"Legitimate" (I'll come back to the quotes later) packet inspection is done by companies or other large organizations that provide both the computers (or other devices) and the network connectivity to people they employ.  They act as their own CA, setting up the computers they own to trust a cert that they deploy to the packet inspection device.  This is "legitimate" in the sense that the computers, the network, the CA and the packet inspection device are all owned by the same party, which trusts itself and its own certificate.  SSL is, from the owner's point of view, doing exactly what it's supposed to do.
Of course, from the point of view of those *using* the computers and other equipment, this may not look quite so legitimate.  In theory, those computers and networks are only supposed to be used for business-related activity - so for the owner to look at the messages is perfectly fine.  But we all know that this is a fiction:  Everyone uses employer-provided computers for personal stuff.  And sometimes the lines get very hard to draw anyway - consider someone dealing with their company-provided health insurance provider.  The issue of "legitimacy" here is then not about cryptography, but about legalities and appropriate social policy and expectations.  (Which for me come down on the "not legitimate" side almost all the time, though I see this as an issue that smart phones and 4G Internet connections have the potential to eliminate.)
There *have* been instances where these packet inspection devices have been given certs from "trusted" CA's.  Where these have become public knowledge, they've been universally condemned as "illegitimate" and withdrawn.  How many that *aren't* public knowledge is anyone's guess, of course.  I very strongly doubt the number is 0.
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-08-04 02:28:47
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] You can't trust any of your hardware 
In iPhone is not a "USB device".  It's a device that has a USB port.  The same goes for an PC or laptop with a USB port - which is pretty much any PC or laptop built in the last 5, maybe 10, years.
I think it's pretty clear that when people think of a USB device, it's a device that exists solely be be used through its USB port.  Memory sticks are by far the most common such devices, but you can find others as well.  I'd say there's a pretty clear line at the point where the device implements - as far as the user is concerned - only and exactly one of the standard USB profiles.  While these devices *also* tend to implement firmware update mechanisms - in fact, there's even a standard profile for that - this is never documented for end users and hardly anyone has been aware that that additional interface is present.  That's what makes this attack surprising.
An iPhone uses its USB port for ancillary functions - charging, syncing, software updates.  The updates primarily - perhaps exclusively, in the life of the iPhone; it's hard to tell - apply to firmware that has little to do with running the USB port.
The Bose headphone you mention is primarily used through Bluetooth.  A quick look through the documentation indicates that the overwhelming use for the USB port is to charge the device.  Yes, it can also be used to update the device - but again presumably the intended primary updates are to the parts of the firmware that, well, run the headset - not the USB port.
Your printer is an interesting case, and I don't have an answer.
There are few sharp lines here, but there is a very broad, very heavily populated, set of "USB devices" that we commonly look at as having fixed functions based on code that will never be changed.  USB memory sticks are extremely cheap and produced in the hundreds of millions.  No one thinks of them as active devices.  And yet ... they are.  They contain significant processing power running non-trivial code - and that code can be replaced.  That's the big message here.  Yes, obvious in retrospect - but how much have *you* thought about defenses against legitimate memory sticks from major manufactures that have had their standard firmware replaced with attack code?
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-10-26 22:54:54
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Best internet crypto clock: hmmmmm... 
Quite a few years ago, I argued that it should be possible to identify laser printers by small variations in toner placement.  The argument the other way was that manufacturing tolerances would make this impossible.  Nothing new here.
Manufacturing tolerances are reduced down to the point where they produce artifacts relevant for the use at hand.  For a laser printer, that means visual effects noticeable to the human eye at the closest distance a paper page is likely to be held in normal usage.  For a camera, it means visual effects noticed at the largest print sizes viewed at their appropriate ranges.
All of these things have fundamental limits set by human sensory capabilities.  Most of our digital technologies are near those limits - most obviously in high-resolution LCD displays (what Apple calls "Retina" displays).  Sure, under some circumstances, some well-trained observers can easily spot the remaining variations.  But it's getting harder every day, and soon only the "golden ears" (and their analogues in different spheres) will even claim to be able to tell, and they'll consistently fail careful tests.
Once you get to that point, there's no reason to go further in controlling manufacturing processes and such.  (Oh, some will for advertising points, but it's a very expensive business to wring out minor variations, so few will try.) And yet it's easy to *measure* details to orders of magnitude finer than you can *control* them.  We may yet be in the period where images are getting more controlled fast enough to annoy the authenticators - but that period will end soon.
The cryptography mailing list

@_date: 2014-10-23 11:27:51
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Best internet crypto clock 
Somehow we went from using the AC hum as a forensic mechanism to using it as a clock.
The use of the 60 (or 50) Hz baseline power frequency to produce accurate electric clocks goes way back.  In fact, this was a usage specifically supported by the power companies:  While all the generators in a system need to be synchronized, there's no need for them to maintain long-range stability or remain centered at 60Hz.  But cheap, accurate, synchronized electric clocks were an early selling point, so the systems were built to actually stay close to the nominal center frequency, and were deliberately manipulated to long-term average stability.  To do this, the systems themselves need an accurate time scale to refer to - and most likely they rely on NIST.  So you'd be getting the NIST timebase, with noise.  On a human scale, over reasonable human periods of time, the errors are nil.  On a scale appropriate to today's computers, the story would be very different.  Deliberately manipulating the frequency across a whole grid for long enough to matter to humans would be extremely di
 fficult and would get noticed:  We now have other, accurate time providers to compare our old electric clocks to.  Of course, if you're manipulating the environment, you can plug the device, not into the wall, but into your own frequency generator and make it see whatever you like.
The article Jonathan Thornburg linked to ( describes the *forensic* mechanism.  It's based on looking at the short-time-scale variations in frequency around the nominal center point.  These are caused by variations in load, variations in supply (generators coming on and off line), lightning strikes, surges due to solar weather, and the interaction of these effects with the synchronizers that are put into the system exactly to keep those variations under control.  They are *not* local, but are constant within a single electric grid - synchronization across a grid is exactly the point!  Grids are very large.  England is covered by just one grid.  The continental US is covered by something like three, if I remember correctly.  (It may be a bit more, but we're still talking a handful.)  These variations are unpredictable in detail, but easily recorded anywhere on the grid.  Recording them *deliberately* as an absolute measure of time
  is an interesting idea - essentially aid the use the forensic technique.  In principle, one could probably replay past variations in a highly controlled setting to make it look as if a recording was made at some point in the past, but sounds rather hard to accomplish.  How successful one might be in replacing a recorded signal with a different one without leaving detectable artifacts is impossible to say without actual testing.
BTW, there's an interesting contrast here between a "tick generator" - which gives you an accurate repeatable way to step a clock from some fixed point - and an "absolute time reference", which lets you map from something (like a record of hum frequency variation) to absolute date and time.  We generally think of "clocks" as tick generators that we start off at some external date and time; absolute time references are relatively infrequent in day to day use.  (They are omnipresent in analyses of records of the past - e.g., looking at the stratum in which a fossil is found as a way of dating it.)  There's a Youtube video out there of a "clock" that measures elapsed time directly by looking at a couple of simple measurements that change as a potato rots....
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-10-05 12:06:38
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Best internet crypto clock 
I'm not sure what it is you would want to check.  The protocol for each beacon would be, at each time T, to send the triple , where T is the current time, Tp < T is the value that T had in the last emitted triple, and R is the random value.  The triple is signed using the beacon's signature.  Any collection of such value can be merged to create a new triple.  The Tp/T values are re-computed relative to the new triple.  A careful "combining beacon" will forward the triples that went into its computation.  (There are some obvious requirements for how the various input Tp/T value can be combined to produce the output Tp/T values.)
A signed triple from a beacon is self-identifying - there is no need for anyone who doesn't intend to use it as part of an assertion to store it, and there's no need for a history.  Chaining the values together makes it harder for a beacon to go back and "revise history", though how much that adds isn't clear - anyone who *uses* a beacon value will present a signed triple, and if the beacon ever lies about a past value that someone has used, it will get caught.  (If it wishes to lie about past values that no one used ... why should we care?)
(The reason for including Tp is that it makes the question "what was the triple emitted by a given beacon that had the smallest value T >= t?" unambiguously answerable.)
Someone here a couple of months back discussed an actual, real-world attempt to compute a value this way.  It failed.  (I searched around for it but was unable to find it....)  The numbers get corrected and changed after the fact.  The changes may be trivial, and they may be infrequent, but they are frequent enough to make the process fail.
While this wasn't part of the previous posting, I think the lesson to be learned is that public sources like this *make no claim that what they've published will never change*.  There's no reason why they should - such a claim isn't relevant to the reason the sources exist.  Errors get corrected, stuff gets reformatted to match some new standard.  The nominal semantics of what's in the database is supposed to never change, but that's based on human understanding, not something you could readily create a hash from.  And, in fact ... even *that's* probably not true.  Documents get screwed up in production - someone leaves out a paragraph, or includes some material in both old a new forms by mistake, or does something else that doesn't get noticed until later.  Then the document gets fixed and the database updated.  *Maybe* the new one gets a "Revised" marker.  Almost certainly, however, the old document gets deleted:  Saving it doesn't add to - and likely detracts from - the value of t
 he database.
Ask yourself:  To whom would this be valuable?  Would the value exceed the cost of maintaining such a thing?  You cite the SEC as an example of a potential user, but there is, as far as I can tell, nothing in any SEC regulation that would require such a thing.  It would supposedly be for protection against someone producing a faked version of a document from the past.  While such issues aren't common, they do occur - consider Paul Ceglia's claim that he owns half of Facebook.  We already have plenty of ways of investigating the validity of such a claim.  Unless you *require* that all documents be added to this database, anyone creating a fake will simply say "Oh, we didn't think it was important at the time so we didn't send it in."
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-10-04 19:50:01
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Best internet crypto clock 
There are two issues here:  The clock, and the original problem of establishing that some event occurred no later than a given time.
The first isn't hard to solve, in the traditional way of producing trustworthy random number generators:  Simply have NIST, the NSA, the EFF, the Russian and Chinese governments - whoever is willing - implement beacons.  To produce a beacon you trust, choose any subset, combine the "random" numbers, and sign the result in the usual way.  The subset and the method of combination are all public and committed to; all the inputs are public.  Since the individual beacons can only be corrupted by entirely stopping them, or by producing predictable (to the attacker) values, unless someone corrupts *all* the sources, the combination is unpredictable.
The question of replicating the "picture of the kidnapped person" scenario, however, seems impossible.  Consider what it claims to deliver:  Anyone looking at the photo, at any time after it was made, can be sure that the person in the photo was actually alive when the photo was taken, and the photo could not have been taken earlier than the date on the newspaper.  Well, maybe that was more or less true back in the days of black-and-white photography; but there would not be the slightest difficulty in faking such a photograph today using Photoshop or similar software.  You then are reduced to the battle of the photo experts - the ones who produce better and better fakes vs. the ones doing better and better detection of fakes.
The fundamental thing you're trying to prove is that some *event* - the taking of the photograph - took place after some time T.  This isn't the kind of thing we deal with in cryptography, where the usual starting point is "some string of bits" B.  Proving that "some string of bits" could not have been produced before T seems difficult.  In fact, if you pose the problem as "combine B with some other string of bits S(T), such that the result proves that B was not known before T", the problem is clearly insoluble.
(Before you go, oh, but you can commit a hash of B to the blockchain at time T - that solves the *inverse* problem:  It proves that you knew B *no later than* T.)
If you instead go back to trying to solve the original problem, you can pose it a different way:  I want to "apply" my victim to S(T) to produce an output that (a) only the victim could have produced; (b) could only be produced with the knowledge of S(T).  For example, suppose that voice-printing were an infallible way of identifying a speaker.  Then we could use a recording of the victim reading S(T) aloud.  (Of course, "infallible" has to include the ability to detect splices and other ways of modifying or combining recordings made earlier to produce the "proof of life".)  Having him write it out with pen and paper would work about as well.
If there were a way to produce a (digital) signature based on "something you are" - assuming that this becomes unavailable after death - then the victim's signature of S(T) would serve this purpose.  Some of the work on biometrics might eventually get us there, though it seems doubtful.
I'm not even sure how to pose a general version of this problem.  There are some special cases that work and might be useful.  Extending the signature example, suppose we have a tamper-proof signing box.  Using it to sign S(T) is proof of possession of the box at some time after T.  Perhaps this could provide some kind of proof of receipt.
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-11-08 11:10:30
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] $750k Fine for exporting crypto 
I'm troubled comparison.  Nixon et al targeted political foes - an extension of a long history of nasty political tricks techniques engaged in by hard-ball politicians since ... forever.  Illegal, pushing beyond the boundaries - but a fight limited to inner circles.
What we are seeing today is unprecedented in American history:  Wholesale monitoring of entire populations, "just in case" the information might be "needed" later.  Saying "beware, someone evil like Nixon could use this stuff" *misses the point*:  It's bad *even if never abused*.  Its mere *existence* is abuse, no matter who controls it.  If the system were under the control of a saintly administration consisting of nothing but good actors, and there were a magic button that would be pressed just before they handed over the reigns to someone not so saintly that magically erased all the stored information and destroyed the information-gathering systems ... it would *still* be wrong.
                                                        -- Jerry
The cryptography mailing list

@_date: 2014-12-01 05:00:28
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Toxic Combination 
The weaknesses of current PKI system are well documented.  There are exampl=
es governments getting false credentials by having effective control over C=
A's.  But I'm not aware of any attack in which criminal organizations got a=
 fake EV certificate (it would have to be an EV cert to turn the address ba=
r green).  The *potential* vulnerabilities are real and there are a number =
of ideas out there (cert pinning, certificate transparency, others) that wo=
uld make them much hard to pull off.  Nevertheless, at the moment, this doe=
s not appear to be a vulnerability that's widely exploited.
No clue what this is supposed to mean.  Checking certificate fingerprints w=
ould only be useful if you had some independent way of know what those fing=
erprints were supposed to be.  What would that independent way be?  This so=
unds like a poor man's approach to certificate pinning - which is automated=
, not based on human matching.
I'm unaware of any broad attacks based on stealing passwords this way.  Why=
 bother?  If you can get the user to trust a bogus site, you just act as a =
MITM of the conversation.  No problem with slight (or major) differences in=
 how the site looks - it *is* the real site.  Send whatever commands you wa=
Again, this is a *potential* vulnerability, but it's not one that scammers =
are exploiting in any big way.  It's so much easier to steal the password f=
ile from the site itself.  It may not even be hashed - and if hashed, is of=
ten hashed using bad techniques, make brute force attacks easy.  And then p=
eople re-use passwords on other sites.
Security is not an absolute; it's about managing risk.  The *actual* risk o=
f an attack by "scammers" based on getting a fake certificate and successfu=
lly impersonating a site in order to get passwords appears to be very small=
; other risks are *much* greater.  (Again, if you're talking about attacks =
by intelligence agencies and similar institutions, the story is different -=
 but in many ways, making it a much harder problem.)  There are a number of=
 mechanisms out there that ameliorate entire classes of related risks, incl=
uding these.
                                                        -- Jerry
The cryptography mailing list

@_date: 2015-02-01 00:36:36
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] How the CIA Made Google 
Now *that's* setting a pretty low bar, based on everything we know about the security of Federal agencies.  As for "most" ... one would have to believe that of all Federal agencies, the one doing the best job at protecting its digital assets would be the NSA.  And yet ... Snowden and, most likely, at least one other leaker as well.
Having worked at Google at one time ... yes, they are *extremely* careful about maintaining the security of their data.  I can't talk about some of the things I know they were using at the time - and (a) I certainly didn't know about all of them; (b) given the revelations of the last couple of years, I'm sure they've added much more.
It is important to keep in mind that security implements a particular policy.  It's strong to the degree that it prevents violations of the policy.  You may not like the way Google targets ads based on what you say in gMail, say, but providing some access to the contents of gMail by the advertising systems is part of the policy.  Similarly, providing access to government agencies under certain conditions is also part of the policy - perhaps not by Google's choice, but because it's a constraint that they have to work under as a US (and EU, and various other jurisdictions) corporation.  Just because you would prefer that Google have some different policies doesn't make it a security issue when they implement theirs and violate the policies you would prefer.
What Google would consider a violation of its security policies concerning gMail, for example, is that those not specifically permitted access gain it.  Google was extremely upset by some Chinese hacking into its systems to get at gMail, and by NSA's listening in on inter-data-center links for the same purpose.  In both cases, they responded by strengthening the appropriate security layers - e.g., encrypting all those links.  Every security system has its failures.  What you need to look at is how frequent are they, and what's the response.
Google's certainly not the only company with this kind of attitude.  Apple seems to have "gotten religion" under Tim Cook.  (Security doesn't seem to be something that much interested Steve Jobs.  It's not that he was against it - he just focused on other things.)  Microsoft is harder to read.  On the one hand, they've put a huge effort into moving away from the insecure coding practices that plagued them - and every user of their software - for many years.  On the other, they arguably took Skype and "de-secured" it.  Facebook ... let's not go there.  Twitter seems to take things seriously.  Most smaller, rapidly-growing consumer Web companies - not so good, anything that would slow down mega-growth gets ignored.
Once you move beyond the consumer-facing companies, operations are typically very opaque to outsiders, so it's hard to comment.
                                                        -- Jerry
The cryptography mailing list

@_date: 2015-02-17 22:21:54
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Equation Group Multiple Malware Program, NSA Implicated 
Snowden may be wrong.  He may have been deliberately lead astray.  He may be a plant.
It's not that I think any of these is true, but presumably Snowden believes *the encryption he was given to use while at NSA* works.  But that's not the encryption any of us use, so it doesn't help.  His statement that encryption, *in general*, works is odd because some of the stuff he released implies that there may be breaks.  It's all ambiguous - you could interpret the same material to say that the encryption itself is fine but the eavesdroppers have all kinds of way to get hold of keys.  But in the end ... does it really matter?  If we knew that AES absolutely, positively could not be broken by NSA - but we had *no idea* what key distribution or generation methods had been broken - we would be absolutely nowhere.
But they also use simple XOR (see the slide set).  This is a situation where unbreakability isn't necessary.  They're more interested in hiding what they are doing - once someone *notices* an exfiltration or command stream, it doesn't matter all that much whether they can read exactly what it says; the game is up.  Even fairly weak encryption would be adequate to prevent simple recognition of such streams based on content.
The FBI complains all the time.  The NSA isn't about to share their advanced techniques with the FBI; they don't want them exposed, which, "parallel construction" or not, will inevitably leak out if used in support of criminal cases.
And I still don't have an strong belief that the NSA can break AES or any other particular magic thing.  But my belief that they *can't* is much weaker today than it was the day before yesterday.
Unfair to whom?  Besides, they've as much as admitted (by the actions they've taken since) that they've come to see their pre-Snowden precautions as insufficient.
Depends on what you count, I suppose.  The military services have large numbers of people doing "cyber warfare", most of whom are doing defense.  Their jobs are not all that different from those of security managers (not developers) at commercial organizations - though since this is the military, the procedures are much more standardized, as are the systems being managed, and there are many more people there on a day by day basis.
If you mean people doing security development - defense or attack; or even those training to do attack with tools the bigwigs develop for them ... it's anyone's guess.  My gut says closer to 10,000 than 100,000, and that would be including a large number of people with basic training on how to attack low-level, ill-defended targets.
Remember how the Navy Seals who took out OBL grabbed all the computers and USB sticks and such?  They clearly had training on what to look for and how to seize it in a way that didn't destroy the data.  And there are undoubtedly rooms full of people who work on extracting data from such seized devices.  They are part of the "cyber warfare" community, by any reasonable definition of the term.
                                                        -- Jerry
The cryptography mailing list

@_date: 2015-02-17 15:56:56
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] Equation Group Multiple Malware Program, NSA Implicated 
What evidence is there for this?
Again, do you have any evidence?
It's not that I have evidence the other way.  We just don't know.  What concerns me is that most of the arguments are "faith-based" - the kind of arguments that support "open always wins":  No matter how big/smart you are, there are more smart people who *don't* work for you than who *do*, and in the long run the larger number of people, openly communicating and sharing, will win.  And yet Apple sold more phones in the US last quarter than all Android makers combined - the first time they've been in the lead.  It's not even clear how to compare the number of smart cryptographers inside and outside of NSA - and NSA has more funding and years of experience they keep to themselves.  This is exactly how organizations win over smart individuals:  They build a database of expertise over many years, and they are patient and can keep at it indefinitely.
Why would they push for new stuff out in the open world?  They *should* be pushing for it, because they *should* be putting more emphasis on defense of non-NSA systems. But what we've seen confirmed repeatedly over the last couple of years is that they have concentrated on offense - and against everything that *isn't* an NSA system.  (To the point where they've apparently even neglected defense of their own internal systems:  What Snowden did was certainly something they *thought* they had a defense against.)
Actually, in that case, I think there's a simpler explanation:  Their models were really the only ones out there, because they'd been dealing with the problem for many years.  Industry hadn't - its needs for security models were, until the pervasive computerization of information, much simpler and in little need of formalization.
There's precedent for this.  When large-scale industrial organizations came into being - a fairly recent development; Engels, Marx's friend, owned what was then one of largest factories in England, employing a few hundred people - they had to figure out how manage themselves.  They copied the only form of organizational structure for large numbers of people that then existed:  Militaries, which followed a style going back to Roman times.  Think about the traditional factory:  Large numbers of "workers" out on the floor; a much smaller number of ex-workers promoted to line management; and then a hierarchy of "professional managers" - with specialized training; almost never promoted from among the line workers - above them.  It's not coincidence that this looks exactly like the traditional army, with its privates, non-coms, and a professional officer corps.  New models for large corporations only started to arise in the late 1960's, with the development of so-called "knowledge organiza
 tions".  (The military has had to back-port some of these innovations as it, too, has become more knowledge/expertise based.)
They apparently haven't even tried, on the defense side - and I agree that we're probably out ahead because of this.  But they're certainly working hard on the offense side....
Maybe.  It's really impossible to say.  Two days ago, I would probably have agreed with you.  Now ... I'm not so sure.
*But attacking these security systems is exactly what they appear to be experts at!*
                                                        -- Jerry
The cryptography mailing list

@_date: 2015-02-25 01:53:21
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] trojans in the firmware 
But in fact you can't design or manufacture *everything*.  Do you need control of your chips all the way back to mining the sand?
I'm pushing this beyond the logical limits, but there is a deep, underlying question:  Just what *do* you have to control?  Is there a theory we can apply here?  There seem to be some "obvious" lines.  If I build a computer out of (logical) MSI-level chips - simple logic gates JK-flipflops - it seems that even someone who could arbitrarily modify their behavior would have a tough time breaking my security.  Disk drives today, with tons of microcode embedded in the on-board controller, are clearly vulnerable.  How about a 1970's era drive, which left the CPU to handle all the complicated stuff?
But ... is that even true?  The attacker is not constrained to operate at the same level of abstraction as I am.  What I think is a JK-flipflop might be transmitting every set/reset to an attacker.  Can he do something with that?
Somehow, we're looking for something analogous to the theory of oblivious computation.  That's usually set up in a fairly high-powered framework - you assume a CPU similar to what we are used to today - say, an AWS instance - then require that the CPU instructions executed are not dependent on the secret data.  Is that enough?  Is there a meaningful way for memory to be oblivious?  There's a sense in which the answer is easy:  Storage of encrypted data, without the key.  But is that enough, as a component, to build a useful system?
The original problem in practical computing was how to build a *reliable* system out of *unreliable* components.  The modern, emerging problem is:  Can we build a secure system out of actively insecure components?
More questions than answers here.
Some interesting speculations about hiding stuff at ever lower levels of the abstraction hierarchy - and a great read - in David Brin's book Existence.
                                                        -- Jerry
The cryptography mailing list

@_date: 2015-02-20 11:19:55
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] trojans in the firmware 
Good luck with that.  SSD performance and even proper operation is still somewhat of a black art; much of the value of the device comes from the proprietary algorithms that control it, which are build knowing details of the design.  Samsung, like other SSD makers, has every reason to keep that stuff secret.  The market advantage of increments in speed and other features is significant; the market to people who want to program it themselves is essentially non-existent.
It depends on the implementation and what kind of attacker you're considering.  There have been implementations in the past which use simply match a password stored in the device - encrypted with AES so that the advertising claims aren't outright lies - against a password entered at boot; the data itself was left unencrypted.  But there's plenty of power in a device like this to essentially build FDE right into the SSD.  That's probably proof against any attack against a stolen/seized SSD.  (Of course, Samsung may have deliberately, or through incompetence, provided a back door - we'd never know.  But most attackers wouldn't know either.  I'm sure North Korea would *assume* that the South Korean intelligence services have access, whether it's true or not.)
Low-enough level attacks against the boot sequence could intercept and leak the password.  The OS typically would come in way too late to see the password - but of course if you take it over, you have full access to the device.
In summary:  Assuming a decent implementation and no back doors available to the attackers of interest to you, this has exactly the strengths and weaknesses of FDE, with no overhead in the host.  Not really security theatre, but given modern hardware, perhaps not much of an advantage either.  You could go for defense in depth by using FDE on top of what the device provides.
                                                        -- Jerry
The cryptography mailing list

@_date: 2015-02-20 11:19:55
@_author: Jerry Leichter 
@_subject: Re: [Cryptography] trojans in the firmware 
Good luck with that.  SSD performance and even proper operation is still somewhat of a black art; much of the value of the device comes from the proprietary algorithms that control it, which are build knowing details of the design.  Samsung, like other SSD makers, has every reason to keep that stuff secret.  The market advantage of increments in speed and other features is significant; the market to people who want to program it themselves is essentially non-existent.
It depends on the implementation and what kind of attacker you're considering.  There have been implementations in the past which use simply match a password stored in the device - encrypted with AES so that the advertising claims aren't outright lies - against a password entered at boot; the data itself was left unencrypted.  But there's plenty of power in a device like this to essentially build FDE right into the SSD.  That's probably proof against any attack against a stolen/seized SSD.  (Of course, Samsung may have deliberately, or through incompetence, provided a back door - we'd never know.  But most attackers wouldn't know either.  I'm sure North Korea would *assume* that the South Korean intelligence services have access, whether it's true or not.)
Low-enough level attacks against the boot sequence could intercept and leak the password.  The OS typically would come in way too late to see the password - but of course if you take it over, you have full access to the device.
In summary:  Assuming a decent implementation and no back doors available to the attackers of interest to you, this has exactly the strengths and weaknesses of FDE, with no overhead in the host.  Not really security theatre, but given modern hardware, perhaps not much of an advantage either.  You could go for defense in depth by using FDE on top of what the device provides.
                                                        -- Jerry
freebsd-security mailing list
To unsubscribe, send any mail to "freebsd-security-unsubscribe

@_date: 2015-03-23 10:53:56
@_author: Jerry Leichter 
@_subject: [Cryptography] "Most Americans Don't Mind Being on Candid Camera" 
Maybe the most important job for those of us concerned about security is convincing our fellow citizens that there's something to be concerned about.
Depressing article:
                                                        -- Jerry
The cryptography mailing list
