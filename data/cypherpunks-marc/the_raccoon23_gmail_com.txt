
@_date: 2012-03-10 05:55:18
@_author: The23rd Raccoon 
@_subject: Re: [tor-talk] Tor and HTTPS graphic 
I meant blinded to the severity.
Only if you have enough data to encode a time signature into. One cell
is not very much data. You'll see why this matters in a few
Your requirements don't seem to match the goal of revisiting tagging,
so I bent them slightly. Your requirements seem instead to invite
revisiting correlation entirely. But that's OK. I want to deal with
tagging, too, so I'll deal with it first. It's along the way, as they
As we'll see, tagging allows a type of amplification attack that can
be *simulated* with a timing attack, but I'll argue it is simulated
poorly. I have not yet provided full Bayesian analysis of the bounds
of the accuracy of simulation, but I have written the dominating
components, and I'll finish it if you like.
If you want me to analyze active timing attacks using similar Bayesian
analysis, that might be a taller order. I'd need to scavenge the local
dumpster archives for a while to collect a representative sample of
attacks and pour over how to interpret their (very likely
misrepresented or at least embellished) results. If you could select
your favorites, it might speed things along.
Either way, just let me know.
Intuitively, tagging attacks create a "half-duplex global" adversary
in places where there was no adversary before, because the
non-colluding entrances and exits of the network start working for
you. You get to automatically boost your attack resource utilization
by causing any uncorrelated activity you see to immediately fail, so
you don't even have to worry about it. This effect is by virtue of the
tag being destructive to the circuit if the cell is not untagged, and
also being destructive when a cell is "untagged" on a non-tagged
In other words: in the EFFs graphic, tagging attacks create a second
translucent NSA dude everywhere in the world *for free*. This
translucent NSA dude is effectively closing circuits that the real NSA
dude didn't want to go to there in the first place. He makes sure that
your circuits only go through another NSA dude.
So to answer your question: because of this "half-duplex global"
property, the tagging attack actually does not require you to have to
worry about a true global adversary to see it is worse than
correlation (active or passive).
Any amount of resources (global or local) that you devote to tagging
automatically get amplified for free by the global translucent NSA
How well you are able to correlate afterword requires a secondary
attack. Depending upon the nature of the tagging vulnerability you
find, you might be able to encode an arbitrary bitstring to uniquely
identify the user, eliminating the need for any subsequent
correlation. In fact, I'm pretty sure this is possible.
I'm going to bend the rules again and instead try to convince you that
an attacker who tags can observe more compromised traffic than an
active timing attacker who attempts to simulate his attack, making
tagging qualify as an amplification attack in a separate class
To simulate the same amplification attack with correlation (active or
passive), you have to correlate every circuit at your first NSA dude
to every other circuit at your second NSA dude, and kill the circuits
that don't have a match on both sides.
You also have the added challenge of doing the initial correlation
with few enough cells to kill the circuit before any streams are
attached (so users don't notice). The need for early detection rules
out virtually all of the benefits of active timing attacks for this
step, which require quite a lot of data to encode their fingerprints
(especially when making them provably effective or practically
Therefore, we are back to analysis dominated by passive correlation
for the circuit killing step (the crux of the simulation).
In order to kill the circuits that don't match, NSAdude1 has to ask
NSAdude2 out of band if NSAdude2 has seen a match for each circuit
that NSAdude1 sees, and viceversa. The probability P(M|C) of the NSA
dudes seeing a true match given their correlater predicted one trends
down in proportion to P(M) = (c/n)^2 * (1/M)^2, similar to my Example
3 but with an extra 1/M factor in there, since we're talking about a
fully correlating adversary.
Note that that M doesn't change (from 5000 in my examples) just
because you see less streams locally. Your probability of seeing a
match is pretty low compared to all the other things you see. (This
piece will also be key for any later analysis of active timing
attacks, which still will be dominated by 1/M^2).
Therefore, even if (or perhaps *especially if*) you don't devote
global-scale resources to the attack, you're going to be crushed by
the base rate.
To complete the simulation, at the circuit killing stage your choice
is either over-estimate and take the union of the first pass
mismatches and waste resources, or only pick the intersection of 1:1
matches on the first pass and kill off quite a few actual matches.
Therefore, you lose either resource amplification or the omnipresent
"half-duplex" translucent NSA dude that the tagger gets for free, and
depending on implementation choice you might even end up doing worse
than the active attack by itself without attempting the
circuit-closing amplification. The exact amount of tradeoff depends on
how global vs how local you are, and if you choose to be lenient or
aggressive in your uncorrelated killing.
I conclude that the superiority of true tagging over simulated tagging
clearly makes true tagging qualify as a resource amplification attack,
which is indeed considered a different class of attack than
correlation alone.
Would you like a Bayesian proof with some real numbers, or do you
concede we should move on to active timing attacks?
Is this a trick question? Dude, you realize I'm a Raccoon, right?...
Nothing is robust to intersection attacks. If you add up enough pieces
of info over time, you deanonymize someone. The game's all about
collecting enough bits from wherever you can (or about scattering
those bits to the wind, if you're on the other side of the line).
You (and others in this thread) misunderstand me. I'm not saying that
correlation never works, or that all three of my examples are safe
places to be if you want anonymity from the tor network as it is
currently deployed and used.
I'm merely saying that sweeping all types of end to end attacks under
the rug blinds you to the very real effect that adding more concurrent
users to the network has on correlation, and the difference is in fact
substantial enough to alter at least some aspects of the threat model
to take user base size and activity into account before evaluating
tor-talk mailing list

@_date: 2012-03-08 07:52:57
@_author: The23rd Raccoon 
@_subject: Re: [tor-talk] Tor and HTTPS graphic 
Yes. Murdoch's work was quite informative, one of the more palatable
dumpster morsels I've happened across.
If you draw a line straight down figure 5(a) of [1] at 10k packets,
you actually can see the effect of the base rate fallacy right there.
As his concurrent flow count increases, the P(M|C) (which he calls
P(correct target)) rate drops rather quickly. I bet if you got the
actual P(C|M) values and adjusted the units appropriately, you'd find
a 1/M^2 in there.
George Danezis claimed in [2] that the best-match decision process of
modern classifiers eliminates the quadratic 1/M^2 drop-off, but I
don't believe that to be the case. I think that experimentally you'll
find that your best-match classifier performs worse when you throw
more items at it, just as Murdoch did. This effect is also seen in
authorship classification work. The more authors you try to correlate,
the worse off your rankings are. In fact, the last time I checked,
state of the art text classification currently breaks down at around
just 100 authors, using a best-match classifier.
[1]. tor-talk mailing list

@_date: 2012-03-08 06:41:25
@_author: The23rd Raccoon 
@_subject: Re: [tor-talk] Tor and HTTPS graphic 
For passive correlation attacks, I have not seen any in
dumpster-accessible research literature.
For active attacks, there are varying classes that can achieve 0
error. In general, 0-error success depends upon how much information
you are able to encode into the stream, how quickly you are able to do
it, and how reliably you are able to extract it.
In fact, I think the research community's insistence that passive
correlation can always succeed has blinded the tor devs to a very
serious type of active attack that actually will: the crypo-tagging
The crypto-tagging attack performs an operation on a cell at the entry
to the network that will cause an error upon exit of the network,
*unless* a party at the exit of the network is able to undo it. It
ensures a node will only carry compromised traffic.
In 2009, the devs dismissed a version of the crypto-tagging attack
presented by Xinwen Fu as being equivalent to correlation back when
the "One Cell is Enough to Break Tor's Anonymity" attack came out[1].
They dismissed Fu's comments about false positives by quoting
researchers claiming that a false positive rate of 0.0006 "is just a
nonissue". But if you do the math in my Example 1, a 0.0006 false
positive rate is more than enough to prevent dragnet analysis of a
heavily used network.
In [1], the devs offered to work towards fixing the issue if someone
could show that it was indeed worse than passive correlation. I
believe I have done so. Is there anything that can be done? I'm not
sure at the moment. Probably a conversation for another thread.
[1]. tor-talk mailing list

@_date: 2012-03-07 04:55:54
@_author: The23rd Raccoon 
@_subject: Re: [tor-talk] Tor and HTTPS graphic 
Now bear in mind that I'm just a Raccoon, but some time ago I scrawled
a proof out that showed that the correlation accuracy of a "dragnet
GPA" goes down in proportion to the square of the number of concurrent
users using an anonymization service:
The belief that you can test a correlation system independent of a
population size is called the Base Rate Fallacy, and I believe much of
the PETS timing attack literature suffers from it. In that post I
demonstrated the effect the Fallacy has on dragnet correlation. I also
gave some example calculations for how accuracy changes from different
points of network surveillance with respect to population size and
correlation accuracy.
With end-to-end encryption and proper Tor cell size choice, the NSAs
odds of watching everyone all the time (Example 1 in my post) and
getting the correlation right are low and do clearly drop as more
people use Tor.
Therefore, I think the most accurate representation would be to put a
question mark next to the data link between the two NSA dudes in your
graphic, because they aren't exactly sharing perfectly; they are
consulting each other, correlating observed traffic patterns with some
error rate, and rolling the dice. A question mark captures this well.
Putting "Capabilities Uncertain" underneath the question mark or as a
footnote might be even better, if we already have newspaper articles
citing the graphic as proof Tor is broken...
P.S. To the list administrators, it looks like the new archives have
truncated my proof at the new archive:
tor-talk mailing list
