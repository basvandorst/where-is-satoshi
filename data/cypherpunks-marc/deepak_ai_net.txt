
@_date: 2002-03-26 04:05:21
@_author: "Deepak Jain" 
@_subject: RE: 1024-bit RSA keys in danger of compromise (fwd) 
That is a falicy. Moore's law is most certainly not accelerating -- in
1965-1990 Moore's law stated that the number of transistors per square
inch on integrated circuits (and therefore, the speed) doubles every 2
years. The pace has since slowed down a bit, but appears to be holding
steady at doubling every 18 months (1995-present).
However, this trend cannot continue forever. In 1997, Moore predicted we
would reach the physical limits on transistor miniaturization somewhere
around 2017. Whatever the actual date, we will need a break-through in
computing to continue to obtain performance increases over time past this
If we are just limiting our analysis to computing power and their physical
size limitations, there are plenty of such breakthroughs on the horizon:
Like Molecular Transistors:
This is WAY off topic for NANOG. I'm done with this publicly.
Deepak Jain

@_date: 2002-03-26 03:25:21
@_author: "Deepak Jain" 
@_subject: RE: 1024-bit RSA keys in danger of compromise (fwd) 
$2B isn't an insurmountable barrier. It is well within most intelligence
agencies' budgets, and that price will only get lower.
Agreed. Imagine what intelligence agencies could gain by turning your most
valuable employees for secrets.
Passive attacks are, by definition, undetectable. Active attacks are not;
some are simply more detectable than others.
I disagree about passive attacks, but I won't go into all of the reasons
here. Passive attacks, by my definition, only imply that they do not
interrupt the flow they are observing. [interrupt, at least at a macroscopic
level]. For an example of passive monitoring that can be detected, look at
the example of how one would sniff live fiber in the field [without splicing
or introducing electronics]. Or for a more common place example, think of an
induction coil next to an electrical wire. Its a passive attack, but is
_definitely_ detectable.
Remember that there is no international law preventing a country's
intelligence agency from committing industrial espionage for its own
companies (and in fact this is common practice).
Sure, no argument.
Also, remember that the US Military has considered, and may very well be
using, IPsec in the field to coordinate military maneuvers.
I think you're really missing the main point with that $2 billion figure.
The "big surprise" is that we might be able to put a price-point on
factoring 1024 bit keys -- previously, they were thought to be "secure
I guess this is an assumption we don't all share. You know what they say
about assumptions.
A machine that costs $2 billion today, according to Moore's law, will cost
about $200,000 20 years from now. Not counting inflation. That will be
well within many people's budgets.
Also agreed. Anyone who thinks the shelf life of their keys is 20 years, or
the information captured today is valuable for more than a couple of years,
then they are making generous assumptions too.
If its a big surprise that any key of any arbitrary length can be cracked in
finite time and in finite resources, I think people haven't been thinking
about the information presented in the security books out there. Most of the
estimates that say anything is "unbreakable" don't recognize that Moore's
law is real, and accelerating...
Deepak Jain

@_date: 2002-03-26 01:31:18
@_author: "Deepak Jain" 
@_subject: RE: 1024-bit RSA keys in danger of compromise (fwd) 
Exactly. Why think $2B is some insurmountable barrier when there are far
cheaper ways of  getting what you want. Most computer people think of
security only in terms of computers. Bribing a few night security guards is
far cheaper than even cryptanalysis and will give any sufficiently
interested party access to the machines signing the keys.
At present, if you have the sophistication to break an "interesting" key,
you could have the sophistication to not be detected MITM. The difference
between inserting/replacing a valid flow, and simply listening [unless the
attacker is stupid] isn't that big a difference from a detection [of the
attack] point of view.
Again, I am assuming things about the attacker that makes them scary. If the
attacker is a little kiddie using his home broadband connection, he is not
necessarily going to be able to use that information for anything
particularly harmful.
Yes, but the trust architecture out there today are far more vulnerable
[IMO] than the underlying key-encryption. Again, while key negotiation is
interesting and important, RSA/DSA/etc are only used in that stage, and
generally the underlying connection [for performance reasons] moves with a
significantly less bulky encryption algorithm. Blowfish, IDEA and a few
others come to mind.
It is far more trivial to capture and compromise an instream algorithm than
worrying about the key at the get go is [unless you are trying to
permanently compromise a victim, at which point the CA is an easier target
anyway]. This is especially the case when you allow for dedicated hardware.
I have always been of the opinion that all of this internet-widely-available
encryption is primarily to make customers feel safe and save credit card
companies some liability. There wasn't enough thought put into it at all
levels to make it more safe/secure than that.
No one is going to spend millions of dollars to get at most the same
millions of dollars of back in credit card fraud [good money after bad].
Anyone who is relying on these commercial architectures to secure gov't
secrets or secrets worthy of an intelligence outfit's attention is a moron
[for numerous reasons]. If all you are doing is trying to secure machines
against script kiddies, starting huge public debates and initiatives and the
like seems like overkill to me. [investment is greater than reward]. YMMV.
Deepak Jain
-----Original Message-----
Len Sassaman
Sent: Monday, March 25, 2002 8:14 PM
Well, that's not really the case. Breaking a 384 bit key is trivial.
Breaking a 1024 bit key is probably not possible without a multi-billion
dollar budget. 2048 bit keys are still in no danger of being broken any
time soon unless further advances are made in factoring.
But I see the point you are making, which is that targeting the CA lets
you attack all of the browsers that trust keys signed by that CA, rather
than specifically targeting that one site. However, MITM attacks are
active attacks, and run the risk of being detected by the the victim. If
you break the key a site is using for encryption, you can read the traffic
without fear of detection.
Other comments on this issue, which I covered in my DEFCON 9 presentation:
it would probably be a lot easier to compromise a CA's root key by means
of network or physical attack, rather than through cryptanalysis. It also
doesn't have to be Verisign you target -- there are over a hundred trusted
root certification authorities in IE, some of them issued to companies
that have gone bankrupt, or sold their root as part of their assets.
Remember, if you're attempting a MITM attack in TLS, you're really
exploiting poor design of the trust-management features of the client,
which is a whole can-o-worms in and of itself.

@_date: 2002-03-26 00:41:58
@_author: "Deepak Jain" 
@_subject: RE: 1024-bit RSA keys in danger of compromise (fwd) 
Since you are mentioning Verisign here, and CA authorities in general, has
anyone considered that factoring the CA authority's key is far simpler than
breaking the underlying key [no matter how large?]. Based on the
implementation, the CA's key cannot be changed often or easily. Key
revocations are not automatic or even respected, and the CA's key, once
compromised, can sign any other key you'd like for a beautiful
man-in-the-middle attack.
The man-in-the-middle is the only attack these keys are designed to thwart,
because if you can't access the physical bits, you don't have anything to
decipher anyway. The beautiful thing about compromising the CA's key is that
its not easily traceable.
Deepak Jain
-----Original Message-----
Len Sassaman
Sent: Monday, March 25, 2002 6:32 PM
I discussed this in detail with Lucky before he posted it. I'll give a
summary of how this affects the readers of NANOG here -- feel free to
forward if you like.
Prior to Bernstein's discovery the row-reduction step in factorization
could be made massively parallelizable, we believed that 1024 bit keys
would remain unfactorable essentially forever. Now, 1024 bit RSA keys look
to be factorable either presently, or in the very near future once Moore's
law is taken into account. However, at a price tag of $2 billion for a
specialized machine, we have a few years before anyone outside of the
intelligence community attempts this.
What is most concerning to me is a few discoveries that were made while
looking into the problem of widespread use of 1024 bit keys:
First: Verisign appears to have no minimum requirements for the key sizes
it will sign. I have discussed at length Verisign's active contributions
to the hindrance of security on the Internet in the past (see the
archives of my presentation at DEFCON 9), but I somehow missed this gem. A
few months ago, in fact, Verisign issued a 384 bit certificate. (You could
factor this on your desk top machine in days.) 512 bit keys are also
fairly commonly signed by Verisign. (Ugh.)
Question for people who know: Does Verisign allow you to submit CSRs for
2048 to 4096 bit certificates?
Second: As far as I can tell, OpenSSH (and I assume the commercial
versions of SSH as well) offer no mechanism for enforcing the size of
users' keys when public key authentication is turned on. This means that
users could be placing (factorable) 512 bit keys in their
~/.ssh/authorized_keys files, which is in effect worse than using weak
passwords (as an attacker would leave no false login attempts for you to
detect in your logs).
I've mailed Theo de Raadt asking if OpenSSH has an undocumented mechanism
for specifying minimum permitted key size that I don't know about. If
there is one, I'll certainly post a follow-up.
Lucky also mentions S/MIME, which has so many flaws I'm not going to
address it; PGP, which places the risks squarely on the key-holder and
doesn't prevent the use of 2048 bit keys (which should be safe even taking
Bernstein's findings into account), so I'm not to concerned with that; and
IPsec, which sadly isn't in widespread use.
So, my main concerns are TLS, (which is damaged due to poor engineering on
the part of Netscape and Microsoft, and uncouth policy issues on the part
of Versign) and SSH, which may suffer from an easily correctable
engineering flaw. Note that the biggest concerns don't have to do
specifically with 1024 bit keys, but rather, small key sizes in general.
