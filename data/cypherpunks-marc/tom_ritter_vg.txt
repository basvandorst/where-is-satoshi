
@_date: 2011-11-15 13:19:18
@_author: Tom Ritter 
@_subject: Re: [p2p-hackers] Verifying Claims of Full-Disk Encryption in 
Hash: SHA1
Misdirected a reply to Eugen instead of the list a week ago.  I don't think this
will correctly reply, because I wasn't subscribed to this list at the time.
I used whatever documents I could find to get as much information
about the drive as possible.  That was the marketing material (which
obviously didn't help much), and the FIPS-140 document (which did have
some technical information).  If I could use the Common Criteria or
Protection Profile document, I'd love to - but I'm not sure how to get
those or go about requesting them (besides just calling and asking.)
I may be naive, having never dealt with FIPS validation, but I kind of
hoped/assumed that things that were insecure wouldn't be approved.
I'm using insecure casually, basically meaning "If I steal your
laptop, can I recover your data for under a couple thousand dollars?'
If that is possible, and within the reach of a hobbyist (or organized
crime, minor government, etc) - I would expect it not to be approved.
And if it was approved, I'd expect the approval to be in error.
Maybe I'm wrong about the approval process - I've never been involved
with it.  I'm just approaching it from the perspective of 'Should I
trust this?' and using the FIPS-140 approval to gain a little intel
and make a good starting point for a hard drive to start with.
- -tom

@_date: 2011-12-05 20:34:03
@_author: Tom Ritter 
@_subject: Re: [cryptography] really sub-CAs for MitM deep packet 
Interesting for me is probably around 65%.  You can judge for
yourself:  I like it, but to be honest, it does get spouts of super-high volume
(like the last week while I was traveling).  I consume it via RSS from
gmane which keeps my inbox cleaner.

@_date: 2013-03-08 18:05:30
@_author: Tom Ritter 
@_subject: Re: Summary of where we are right now 
I don't agree that the NRL funded Tor for this purpose, but I do agree
that our tools today (Tor, mixmaster/mixminion, PGP mail, RedPhone,
TextSecure, OTR, etc) are easily distinguishable in traffic streams,
and that this is a problem.  Just as Riseup collects a bunch of people
who care a lot about privacy onto one mailserver - people using these
tools are likely to be interesting.
Skype, Facebook, Gmail - for all their problems, they are ubiquitous,
and don't draw attention.
A friend I talked with recently told me he thought it was easy to set
up an anonymity system that worked great for you and your friends, and
near impossible to build one that worked well for everyone else.  Once
it got popular or you became a target of investigation, people would
put the effort into detecting it.  Otherwise, it would continue along,
looking like another TLS/SSH/Skype/whatever that just a little bit
odd...  Tor faces this problem immensely.
I don't see us as having won, I see us as now knowing how to fight.
We know the devices they will use to easily detect our traffic, and in
most cases we can get access to them.  We must make our protocols
indistinguishable on the wire. We know the ubiquitous services and
protocols that we must work within or disguise ourselves as.
We know (some of? most of?) the statistical attacks adversaries of the
future can conduct - we must make them as difficult and expensive as
possible for them to achieve.
We know how woefully inadequate the user interfaces and requirements
of the first generation of tools were, and we know where we must go:
to browsers, smartphones, tablets, and consumer operating systems.
We have a much better idea of how normal people will react to our
tools, and thus how much effort we must make to make them usable, and
push for ubiquity.
We know what requirements are unreasonable of us to make upon people,
and that we must design systems where those requirements are worked
around, dulled, or the single 'sharp edge' of the system.

@_date: 2013-08-12 20:23:51
@_author: Tom Ritter 
@_subject: Re: Information theoretically secure communication networks 
As Lance said, this is pretty close to what alt.anonymous.messages
evolved into in the 90s and early 00's.
I gave a talk two weeks ago looking at 10 years of messages there and
finding user errors, weak passwords, user-segmenting settings, and
traffic patterns.  Details are over here:

@_date: 2013-08-28 00:57:35
@_author: Tom Ritter 
@_subject: Re: Metadata anonymization through time delayed email messaging. 
I don't know - if I'm performing physical or network surveillance of a
target, and I see a Mix message leave - that tells me something very
definite about the timing.  Obviously you wouldn't want to store the
message in plaintext, but if you encrypted it to the first hop, along
with the address, and a time to send (and tried your hardest to lie
about the timestamps on the filesystem); you can increase the
difficulty of learning something definite.  And I think that holds
even if the attacker does a physical intrusion and looks at the
filesystem. (It reminds me Rivest's FlipIp game - the attacker is
allowed to do a physical intrusion and read the filesystem, but
everyone learns that they have and thus distrusts that node.)  Of
course it only holds if there are multiple possible senders, delaying
an email from my home when I live alone doesn't help me.  But if there
are multiple possible senders, it feels like tacking on a
lesser-quality mix node at the beginning.
Another argument to it's utility is there is no easy way to disguise
the fact that you are sending a mix message.  Right now the only ways
I can think of hiding that fact would be to use mix bridges (some
entry remailer node that isn't published, akin to Tor's bridges) with
a protocol that looks as identical to SSL in a webbrowser as you can;
or to send them out over Tor.
I think the user-configurable time is the idea behind Alpha Mixing,
although I hope it's implemented better than in Type 1 Remailers.
Absolutely.  And on the sender end, I can't think of good ways to
obfuscate large messages.  The splitting technique of Mixmaster has
always felt like a bit of a hack (no offense), because someone doing
end to end correlation should be able to link those fairly easily.
For receiving large files, I think a client/server architecture where
you can choose to delete the message on the server, or download chunk
by laborious chunk over time would be advantageous[0].
[0] This might, might, even be an argument of added complexity by
splitting files, before compressing and encrypting them, so you can
download chunks 1-4 and (potentially) get a portion of the file in a
readable albeit incomplete format.

@_date: 2013-09-19 19:39:53
@_author: Tom Ritter 
@_subject: Re: Linus Torvalds admits he was asked to insert a backdoor into GNU/Linux 
Is there any indication he took the question seriously and wasn't just
making a joke?  This is a lot to conclude from a single sentence.

@_date: 2013-10-14 21:30:09
@_author: Tom Ritter 
@_subject: Re: An Interview with Simon Persson of CounterMail 
"You can delete the private key from our server (but we recommend this
only for advanced users, your private key is always encrypted on our
server anyway"
This sounds pretty similar to Lavabit. The server stores your emails
encrypted, but they're decrypted for you when you login, using your
password as the key to decrypt your private key.  The difference (I
think, I never used Lavabit) is that you can retrieve the private key
from Countermail and then ask them to delete it.  It would be even
nicer if they let you upload your public key so they never see the
private key.  You'd still have to trust them not to copy plaintext as
it's coming in, which depending on how you think about it might be
equivalent to them having a private key to your mail in the first
In all these 'secure email' providers, they all have the same problem:
they see incoming plaintext, and could be compelled to store it/record
it. It's not their fault, they do the best they can, it's just how
email works.

@_date: 2013-10-01 02:08:35
@_author: Tom Ritter 
@_subject: Re: Surveillance 
I would say you are incorrect. The UK and the US cooperate very, very
closely. Likewise, the Echelon/Five Eyes program is a publicly
documented SIGINT sharing program

@_date: 2013-10-22 11:35:26
@_author: Tom Ritter 
@_subject: Re: [cryptography] funding Tor development 
Part of any organization's advice, given by a lawyer, is both "What,
in the courts (and my) interpretation of the law, is legal", and ALSO
"What I feel comfortable defending you for" and "Ways your life can be
made miserable through things like frivolous court cases that are
reasonable enough to go to a judge."
It's entirely possible that: Lawyer A loves bitcoin, reads all they
can about it, and would go to court (or more likely, private meetings
with the IRS) in a heartbeat citing case law and making a spirited
defense.  Lawyer B; however, is kind of curious about bitcoin, but
doesn't have much free time to read more about it, or look up case
precedence, because s/he's very busy - perhaps overworking themselves
by giving cheap rates to a bunch of nonprofits. Lawyer B isn't
necessarily opposed to bitcoin, but actually making a defense of it
would require a ton of research on their part, which would incur a
large cost for the organization.
The decision to not accept bitcoin doesn't always hinge on "I think
this is legal vs illegal", but a spectrum of "How much of a pain in
the butt is it going to be for me if someone _does_ take issue with
it, given my current financial situation, legal representation, and
business relationships."
cryptography mailing list

@_date: 2013-10-22 16:29:33
@_author: Tom Ritter 
@_subject: Re: [Cryptography] programable computers inside our computers (was: Hasty PRISM proofing considered 
And to add another, there was a presentation on ARM TrustZone, the OS
inside your CPU, that's seems so designed for backdoors that ARM
actually gives tips for running TrustZone invisible to the normal OS.
These are increasingly worrying me as well.  The Secure Element on
Android can at least (if you root and edit the .xml file) be queried
to learn identifiers of what is installed there, if not directly
interact with them.
The cryptography mailing list

@_date: 2013-11-15 16:02:21
@_author: Tom Ritter 
@_subject: Re: [Cryptography] programable computers inside our computers (was: Hasty PRISM proofing considered 
Reviving an old thread because I particularly like this statement and
agree with it at the moment.
Also, I believe TPM 2.0 includes remote attestation. Clearly this
could be abused, and probably will be, but I'm also interested in
applicability in scenarios where the queryier and attestor are in
cooperation. I'd love to query cryptocat's servers and verify they are
running a particular system build without modification. This might
even be able to provide more improved warrant canary type approaches.
Similarly, in the corporate sector (which includes field agent
activists) verifying that a user's laptop is running the bios and
kernel you expect. This can all raise the bar for attackers.
The cryptography mailing list

@_date: 2013-12-14 03:31:21
@_author: Tom Ritter 
@_subject: Re: Joke 
I doubt it - abuse through Tor is a legitimate problem.  Wikipedia blocks
editing from Tor for the same reason.
There are ideas for solving this though, and it would be cool to see more
ideas, and more fleshing out of them.  Mike Hearn has talked about having
people make a bitcoin deposit for an account, and after so much time of
legitimate use, the deposit is refunded.  Before that, if it's used for
abuse, the deposit is kept by the service.

@_date: 2013-12-24 19:16:18
@_author: Tom Ritter 
@_subject: Re: [Cfrg] Requesting removal of CFRG co-chair 
I did not interpret it as a reference to conspiracy, rather a note of the
lack of opportunity to raise objections, ask for volunteers, or
nominations. I'm not familiar with the actual appointment in question, it's
likely Kevin was an active participant that, aside from his employer, made
sense. But, if there was no opportunity to raise concern/conflict of
interest, this lack of opportunity to provide input is worth noting. Not
that it means conspiracy, just that it makes this conversation all the more
relevant (as there is no 'you could have replied to this thread' argument).

@_date: 2013-12-15 19:23:09
@_author: Tom Ritter 
@_subject: Re: Gmail's receiving mostly authenticated email 
I saw that article too, and thought it was interesting, but I noticed
something odd in their statistics:
91.4% of ***NON-SPAM*** emails sent to Gmail users come from
authenticated senders, which helps Gmail filter billions of
impersonating email messages a year from entering our users inboxes.
More specifically, the 91.4% of the authenticated ***NON-SPAM***
emails sent to Gmail users come from senders that have adopted one or
more of the following email authentication standards: DKIM (DomainKey
Identified Email) or SPF (Sender Policy Framework).
""" (emphasis mine)
So first Google runs their pretty-good-but-not-perfect spam filtering,
then they look at what they're categorized as non-spam to generate
those statistics.  The ham (not spam) emails that are miscategorized
are much more likely to be omitting SPF/DKIM, so there's a bit of
selection bias occurring.
Also, for what it's worth, SPF isn't related to crypto at all, and is
ridiculously easy to set up for 'normal' domain admins.  (That is,
domain admins with a couple well-known SMTP servers, and not some
crazy distributed architecture.)  There's a great calculator online
for it here: There's some tricky questions people may not know the answer to, but
omitting answers will only create a more _permissive_ policy, rather
than run the risk of borking your email.

@_date: 2013-12-14 20:23:50
@_author: Tom Ritter 
@_subject: Re: BlueHat v13 crypto talks - request for leaks ;) 
This is different from the normal 'repeated/non-random k leads to private
key', is it not?  Is there a paper/reference I can read more about this

@_date: 2013-12-14 10:55:13
@_author: Tom Ritter 
@_subject: Re: BlueHat v13 crypto talks - request for leaks ;) 
I can answer for Cryptopocalype. :)  I had a follow-up blog post after
Black Hat, but the crux is looking for the next crypto black swan. Joux's
work in optimizing the function field sieve for fields of a small
characteristic has been a significance improvement kind of out of left
field. If he or anyone else made improvements to the FFS for fields of a
large  characteristic or the GNFS - we would be in a bad way. The security
margin on the ECDLP is greater than DL or factoring and while we've got the
algorithms, the implementations are sometimes missing and the ability to
pivot, in software update mechanisms, in CAs, everywhere - is completely
missing. ECC has other attributes that make it attractive too, so let's get
the plumbing ready, so we can support a quick pivot away from RSA and over
to ECC if we have to.
I copied Justin rather than (poorly) summarize his work.
(Just landed, sent from the baggage claim, excuse brevity)

@_date: 2013-12-22 18:14:36
@_author: Tom Ritter 
@_subject: Re: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: Jim Bidzous of RSA) 
I'm confused, but maybe missing something?  The article says:
The stakes rose when more technology companies adopted RSA's methods
and Internet use began to soar. The Clinton administration embraced
the Clipper Chip, envisioned as a mandatory component in phones and
computers to enable officials to overcome encryption with a warrant.
RSA led a fierce public campaign against the effort, distributing
posters with a foundering sailing ship and the words "Sink Clipper!"
A key argument against the chip was that overseas buyers would shun
U.S. technology products if they were ready-made for spying. Some
companies say that is just what has happened in the wake of the
Snowden disclosures.
The White House abandoned the Clipper Chip and instead relied on
export controls to prevent the best cryptography from crossing U.S.
borders. RSA once again rallied the industry, and it set up an
Australian division that could ship what it wanted.
"We became the tip of the spear, so to speak, in this fight against
government efforts," Bidzos recalled in an oral history.
RSA, meanwhile, was changing. Bidzos stepped down as CEO in 1999 to
concentrate on VeriSign, a security certificate company that had been
spun out of RSA. The elite lab Bidzos had founded in Silicon Valley
moved east to Massachusetts, and many top engineers left the company,
several former employees said.
It seems like Bidzous was out of RSA long before DUAL EC PRNG was even
proposed, and was in fact campaigning and strategizing against RSA
while he was there.  Where are references to other accusations or

@_date: 2014-01-23 05:47:48
@_author: Tom Ritter 
@_subject: Re: and not a single Tor hacker was surprised... 
> About
this. Is there a way to serve 2 (or more) certificates for a given HTTPS
that do
There are a lot of things like this, but the big question is: how does the
user indicate to you which cert they want?
If it was via pubca.x.com or privca.x.com - that's easy just put the
different certs in the different sites.
But otherwise, you have to rely on quirks.
TLS allows you to send different certs to different users, but this is
based off the handshake and is for algorithm agility - not cert chaining.
EG I send ECDSA signed certs if I know you can handle them, and RSA if not.
You can also send two leaf certs, two cert chains, a cert and garbage, a
cert and a stego message - whatever. This is the closest to what you want,
but this is undefined behavior. Browsers may build a valid chain off the
public CA, and monkeysphere off the private* and it works perfect... Or the
browser may pop an invalid cert warning. It's undefined behavior. You'll
have to test, see what happens, and hope chrome doesn't break when it
updates every week.
* I realize monkey sphere doesn't use a private CA, just using it as an

@_date: 2014-02-14 16:50:01
@_author: Tom Ritter 
@_subject: Re: [Cryptography] Are Tor hidden services really hidden? 
Agreed, wholly.
I'll nit and say we only have 5K nodes, including 1K exits and 2K
guards but it's possible.  I proposed (and intended to do before I got
sidetracked) that we just go through the exit probability percents and
tick off which nodes we believe are run by trustworthy people, and
just see what percentage we get to.  I think it will be less than 50%,
but greater than 25%.
The cryptography mailing list

@_date: 2014-02-14 15:50:42
@_author: Tom Ritter 
@_subject: Re: [Cryptography] Are Tor hidden services really hidden? 
The RP is chosen by the client, so the attacker doesn't need to
control those. When the HS contacts the RP, it's via a Tor circuit, so
the RP doesn't learn the HS's actual IP, only the exit IP.  This
doesn't get you any closer to finding it though.
The attacker needs to be come the entry point for a HS to perform a
traffic confirmation attack. By sending lots of data to the HS from a
client, the entry point can correlate that traffic being delivered to
the connection, even if it can't read it.
To protect against this attack, Tor uses Entry Guards:
 These aim to 'stick' a
client (or HS) to a set of entry nodes.  If you, the attacker is in
that set - you're good to go. But if you're not, it's much more
difficult for you to get _into_ that set.
The cryptography mailing list

@_date: 2014-02-13 23:24:14
@_author: Tom Ritter 
@_subject: Re: [Cryptography] Are Tor hidden services really hidden? 
Hiding a server is of course much harder than hiding a client.  But
clients can also be servers - Facebook chat, for example, turns anyone
into a server that can be contact with variable length messages at the
attacker's leisure.
Lots of people assume this, but it doesn't seem to bear out well.
Besides the NSA docs that expose their lack of interest in doing so,
visit here:  While there's a large 'unknown' percentage - most of these large
bubbles are people that the Tor Project is in close contact with and
the community knows personally.
An entry node knows who is talking, but not to whom. A middle node
knows no IP addresses. An exit node knows the recipient IP but not the
origin.  So I'm not sure what you mean by seeing IPs, but they are
unable to see sender and receiver IPs unless they operate both the
start and end node. This is a tagging attack (active) or a traffic
confirmation attack (passive).  It's difficult to achieve, as Tor uses
entry guards to lower the probability of achieving the entry node.
It depends on your model. If you're saying a Globally Passive
Adversary can de-anonymize low latency connections - and thus the dark
net can't exist: I would agree with you.  If you're saying "Tor Hidden
Services can never provide a level of protection against automatic
wide-scale de-anonymization attacks by a government TLA" - I'll
disagree and start diving into specifics with you.
The cryptography mailing list

@_date: 2014-02-08 04:02:18
@_author: Tom Ritter 
@_subject: Re: FB's Conceal secure-storage API 
It's not like preventing root from getting the key is some attribute
they omitted by accident or incompetence - it's a significant design
change that changes the way the application would work.
It seems like everyone criticizing Facebook is angry that they're not
compromising their design principals for added security.  They have
very clear priorities: We are _going_ to benchmark and make sure any
code we add does not increase UI latency beyond an unacceptable limit.
 We are _going_ to cache some large MB of data on the phone, because
it makes the app faster. We are _not_ going to take up more space than
we need. We are _going_ to support old phones that have an SD Card,
and if that's where we cache the data, then so be it. We are _not_
going to require the user to enter a password or PIN on app startup.
We are _not_ going to require the phone to be online to used the
cached data.
With requirements like those, what you get is exactly this library. It
adds some small level of security against a very specific attack: data
stored on the SD Card and accessible to other programs. (It may even
be a way to get the security they need to permit themselves to store
cached data on the SD Card, which is a desirable situation because it
makes the app faster.)
If you relax some of those requirements, you can add security
features. Relax the latency or minimal storage requirement and you can
create an encrypted container, and hide metadata like filenames,
sizes, and times (like IOCipher). Relax the password requirement, and
you can have the user enter a password on app startup and prevent root
from getting the key unless it's in memory or entered.  Relax the
latency and offline requirement, and you can have the server send down
a key to decrypt the data.
Facebook is starting with the User Experience and adding as much
security as it allows.

@_date: 2014-06-29 23:11:31
@_author: Tom Ritter 
@_subject: Re: [liberationtech] Nsa-observer: organising nsa leaks by attack vector 
It is up for me.  The site itself is open source
( and the data ex
exportable (

@_date: 2014-06-04 12:50:14
@_author: Tom Ritter 
@_subject: Re: "a skilled backdoor-writer can defeat skilled auditors"? 
On 4 June 2014 01:54, Stephan Neuhaus Perhaps this is getting too far into nits and wording, but I audit software
for my day job (iSEC Partners).  I'm not speaking for my employer. But,
with very few exceptions (we have a compliance arm for example), one does
not 'Pass' or 'Fail' one of our audits.  (Perhaps they might be better
termed as 'security assessments' then, like we call them internally, but
we're speaking in common english, and people tend to use them synonymously.)
Our customers are (mostly) on board with that too.  They never ask us if
they 'passed' or failed' - I'm certain some of them look at a report where
we failed to 'steal the crown jewels' as a successful audit - but the
expectation we set with them, and they sign on with, is not one of
'Pass/Fail'. And engagements where they want a statement saying they're
secure, we turn down - we're not in the business of rubber stamps*.
Our goal is to review software, identify bugs, and provide recommendations
to fix that issue and prevent it from occurring again. AND, in addition to
the specific bugs, provide general recommendations for the team to make
their application and environment more secure - provide defense in depth.
Maybe I didn't find a bug that let me do X, but if there's a layer of
defense you can put in that would stop someone who did, and you're missing
that layer, I would recommend it.
Examples: I audited an application that had no Mass Assignment bugs - but
no defenses against it either. Blacklists preventing XSS instead of
whitelist approaches, and like Andy said, homebrew C-code parsing JSON. We
'flag'-ed all of that, and told them they should rewrite, rearchitect, or
add layered defenses - even if we couldn't find bugs or bypasses.
So the notion of 'Passing' or 'Failing' an audit is pretty foreign to me.
 Perhaps people mean a different type of work (compliance?) than the one I
* The closest we get is one where we say 'We tested X as of [Date] for Y
amount of time for the following classes of vulnerabilities, reported them,
retested them Z months later, and confirmed they were fixed.'  As we do
this very rarely, very selectively, for clients we've dealt with before.

@_date: 2014-07-07 12:52:59
@_author: Tom Ritter 
@_subject: Re: US enhanced airport security checks target electronics 
The text in the email is satire/commentary and not actual reporting.

@_date: 2014-08-20 16:10:35
@_author: Tom Ritter 
@_subject: Re: New end to end encrypted IM/VOIP web app focused on ease of use 
This is cool!  I love the combined distribution of providing a hosted
version, and encouraging people to host it themselves.
I looked into the code to understand more about how it works.  Is it
fair to say that you use WebRTC with SRTP for the transport
encryption, and then a homebaked AES-GCM-based protocol with RSA
public keys to do the encrypted chat/actions/invites, and also to
distribute/authenticate the WebRTC fingerprints?

@_date: 2014-11-05 16:15:48
@_author: Tom Ritter 
@_subject: Re: [tor-talk] Platform diversity in Tor network [was: OpenBSD doc/TUNING] 
I tried installing OpenBSD once... it was tough, heh.
Coming from a Windows background, I like the idea of running more
nodes on (up-to-date, maintained) Windows servers.
I'll also throw out the obvious that if we're talking about diversity
for the purposes of security, the network-accessible parts of tor rely
on OpenSSL, which would probably be difficult to swap out, but might
be worth it as an experiment.  Even if it's to LibreSSL.  Maybe the
zlib library also, but that one's had a lot fewer problems than

@_date: 2014-11-19 14:59:44
@_author: Tom Ritter 
@_subject: Re: [Cryptography] STARTTLS, was IAB Statement on Internet Confidentiality 
Or we tend to skip over threads with a low fact:opinion ratio ;)
But then why didn't Cricket do what Comcast does, and just block it,
instead of doing this super-sketchy 'Let's just remove the crypto and
inspect the user's data' approach?  Or, what I think is a fairly
reasonable tactic that some ISPs do on consumer home ISPs, and block
ports but let you opt-out in your user account.  (I had an ISP that
blocked 80 and 25, and two checkboxes to immediately undo it.)
The cryptography mailing list

@_date: 2014-11-23 02:24:19
@_author: Tom Ritter 
@_subject: Re: Microsoft Root Certificate Bundle, where? 
I don't know.
But I know some copy of it can be accessed here:
I don't know how it's generated, how complete it is, or how up to date
it is.  Depending on your needs to may be sufficient, or may be

@_date: 2015-02-04 01:52:52
@_author: Tom Ritter 
@_subject: Re: What the fark is "TFC" 
TCB is usually Trusted Computing Base.
Some searching indicates TFC may be Traffic Flow Confidentiality.  (Or
less likely, TinFoil Chat, which appears to be some random chat app
plugin for encrypted messaging.)

@_date: 2015-03-24 11:51:44
@_author: Tom Ritter 
@_subject: Re: Firefox 36+ listens on UDP:1900 
This is a close-to-but-not-exact recounting. His disclosure of his
employer was required by state law, and was neither a statement of
support by the company nor his attempt to make it so.

@_date: 2015-05-28 12:28:13
@_author: Tom Ritter 
@_subject: Re: Firefox will scan your browsing history to suggest advertiser sites 
4) The browser fetches all available suggested tiles based on country
and language from Onyx without using cookies or other user tracking
5) User interactions, such as clicks, pins and blocks, are examples of
data that may be measured and processed. View Mozillaâs Privacy Policy
or our Data Privacy Principles for more information.
6) Onyx submits the interaction data to Disco, a restricted access
database for largescale analysis.
7) Disco aggregates all Firefox tiles interactions, anonymizing
personally identifiable data before sending to Redshift for reporting.
8) Charts and reports are pulled from Redshift using Zenko, a Content
Services reporting tool, for analysis by Mozilla.
9) Mozilla sends this report to the partner shortly after the campaign ends.
""" [0]
How do you determine user interests?
For Suggested Tiles, we know whether users are interested in your
market category by matching a list of defined URLs (domains, or
subdomains) with their most frequently and recently visited URLs in
Firefox. In this way, we are able to preserve usersâ anonymity while
providing a high level of confidence about their interest in different
site categories.
What input do I have over the interest categories?
We work with all our Suggested Tiles partners to define the most
effective interest categories. Partners may provide suggestions for
what URLs should be include. Mozillaâs Content Services Team will
actually define those categories.
""" [1]
I'm most curious about what 'User Interactions' are reported.  Clicks,
pins, and blocks all reveal which tile a user saw, and therefore
something about their browsing history. But they're also pretty
fundamental to advertising.  I'm more worried about Firefox reporting
"Views" or "Mouseovers" or other things that are not clear,
user-initiated actions.
[0] [1]

@_date: 2015-07-11 19:19:59
@_author: Tom Ritter 
@_subject: Re: progression of technologies 
Yes! That's the case I was obliquely referring to. Sorry, I kind of
glazed over that part of your argument in the article.
I guess where we quibble is I'm skeptical that the general public (as
defined by the courts?) will (ever?) adopt the types of tools you
refer to (uniquely identifying individuals based on electromagnetics,
tracking tire pressure sensors.)  I don't think the 'general public'
has adopted thermal imagers.  These will make their way into
industry... (advertisers tracking WiFi probes in malls obviously).
So my wonder now is if industry adopting a technology is sufficient
for the courts to qualify as 'general public'. But this, at best, only
affects exotic technology.  We're already fighting this battle.
Automated license plate readers have never (?) been challenged
(successfully?). They are an extension of "a police officer just
watching a highway" which is legal.  And the courts like extensions of
things that are already done - see bulk collection of metadata!
You're right - collection of this data by personals or corporations,
and selling it, is indeed the right battleground. I'm don't think the
answer is correlation, but the collection, as you say in the last

@_date: 2015-07-05 22:44:21
@_author: Tom Ritter 
@_subject: Re: progression of technologies 
I'm far from certain, but I think what you have wrong is the notion
that wavelength doesn't matter. I think the courts have decided it
does: Specifically, "most of the general public lacks the expertise to
intercept and decode payload data transmitted over a Wi-Fi network."
Therefore the notion that you can point whatever sort of 'camera' you
want at people to capture them isn't accurate.  (The other relevant
case is that the police do need a warrant to point infrared cameras at
people's houses.)
