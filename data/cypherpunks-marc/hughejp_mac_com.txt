
@_date: 2004-08-17 20:54:21
@_author: james hughes 
@_subject: Re: CRYPTO2004 Rump Session Presentations, was Re: A collision in MD5' 
I have 2 items of note for this list.
1. The web site is updated with program and the times.
2. I was typing fast, and mistyped my title. I am General Chair this year, not 2002 as was stated.

@_date: 2004-08-17 06:37:03
@_author: james hughes 
@_subject: CRYPTO2004 Rump Session Presentations, was Re: A collision in MD5' 
This is Jim Hughes, General Chair of CRYPTO2002. There are three significant Rump session papers on hash collisions that will be presented, including an update on this one (and about 40 other short papers on other aspects of cryptography). As the session firms up, more information it will be posted at
Barring technical or other difficulties, if you want to hear this from the horses mouth, the CRYPTO2004 Rump Session will be webcast at 7pm pacific Tuesday Aug 17 for as long as it takes. You may join us virtually using the following links (depending on the readers).
Internet Explorer
Microsoft media server
The players (for MS and Mac) are available from
I assume MS clients will be able to cope. I know that my MacOSX machine with Windows Media Player can use the mms: link. I welcome feedback from anyone using other readers on other platforms like Linux.
The server is currently up and running and is broadcasting a dark, empty, and silent hall. This should be more interesting after sunup Tuesday Santa Barbara time. You may expect sound near to the start This is our the conferences first webcast, and I hope that it works for you. If there are problems, I will apologize in advance.

@_date: 2009-02-24 18:53:31
@_author: james hughes 
@_subject: Re: SHA-3 Round 1: Buffer Overflows 
Two aspects of this conversation.
1) This algorithm is designed to be parallelized. This is a  significant feat. C is a language where parallelization is possible,  but fraught with peril. We have to look past the buffer overflow to  the motivation of the complexity.
2) This algorithm -can- be implemented with a small footprint -if-  parallelization is not intended. If this algorithm could not be  minimized then this would be a significant issue, but this is not the  I would love this algorithm to be implemented in an implicitly  parallel language like Fortress.

@_date: 2009-07-26 04:11:02
@_author: james hughes 
@_subject: Re: cleversafe says: 3 Reasons Why Encryption is Overrated 
....and probably patent pending regardless of there being significant  amounts of prior art for their work. One reference is POTSHARDS:  Secure Long-Term Storage Without Encryption by Storer, Greenan, and  Miller at  Maybe  they did include this in their application. I certainly do not know.  They seem to have one patent
and 7 pending.
The trick is cute, but I argue largely irrelevant. Follows is a  response to this web page that can probably be broadened to be a  criticism of any system that claims security and also claims that key  management of some sort is not a necessary evil.
I agree with many of your points. I would like to make a few of my own.
1) If you are already paying the large penalty to Reed Solomon the  encrypted data, the cost of your secret sharing scheme is a small  additional cost to bear, agreed. Using the hash to prove you have  all the pieces is cute and does turn Reed Solomon into an AONT. I will  argue that if you were to do a Blakley key split of a random key, and  append each portion to each portion of the encrypted file you would  get similar performance results. I will give you that your scheme is  simpler to describe.
2) In my opinion, key management is more about process than  cryptography. The whole premise of Shamir and Blakley is that each  share is independently managed. In your case, they are not. All of the  pieces are managed by the same people, possibly in the same data  center, etc. Because of this, some could argue that the encryption has  little value, not because it is bad crypto, but because the shares are  not independently controlled. I agree that if someone steals one  piece, they have nothing. They will argue, that if someone can steal  one piece, it is feasible to steal the rest.
3) Unless broken drives are degaussed, if they are discarded, they can  be considered lost. Because of this, there will be drive loss all the  time (3% per year according to several papers). As long as all N  pieces are not on the same media, you can actually lose the media, no  problem. This can be expanded to a loss of a server, raid controllers,  NAS box, etc. without problem as long as there is only N-1 pieces, no  problem. What if you lose N chunks (drives, systems, etc.) over time,  are you sure you have not lost control of someones data? Have you  tracked what was on each and every lost drive? What is your process  when you do a technology refresh and retire a complete configuration?  If media destruction is still necessary, will resulting operational  process really any easier or safer than if the data were just split?
4) What do you do if you believe your system has been compromised by a  hacker? Could they have read N pieces? Could they have erased the logs?
5) I also suggest that there is other prior art out there for this  kind of storage system. I suggest the paper POTSHARDS: Secure Long- Term Storage Without Encryption by Storer, Greenan, and Miller at    because it covers the same space, and has a good set of references  to other systems.
My final comment is that you raised the bar, yes. I will argue that  you did not make the case that key management is not needed. Secrets  are still needed to get past the residual problems described in these  comments. Keys are small secrets that can be secured at lower cost  that securing the entire bulk of the data. Your system requires the  bulk of the data to to be protected, and thus in the long run, does  not offer operational efficiency that simple bulk encryption with a  traditional key management provides.

@_date: 2009-08-06 23:08:16
@_author: james hughes 
@_subject: Re: cleversafe says: 3 Reasons Why Encryption is Overrated 
Until you reach the threshold, you do not have the information to  attack. It becomes information theoretic secure.
They are correct, if you lose a "slice, or two, or three" that's fine,  but once you have the threshold number, then you have it all. This  means that you must still defend the site from attackers, protect your  media from loss, ensure your admins are trusted. As such, you have  accomplished nothing to make the management of the data easier.
Assume your threshold is 5. You lost 5 disks... Whose information was  lost? Anyone? Do you know? What if the 5 drives were lost over 5  years, what then? CleverSafe can not provide any security guarantees  unless these questions can be answered. Without answers, CleverSafe is  neither Clever nor Safe.

@_date: 2009-11-18 15:33:58
@_author: james hughes 
@_subject: Re: hedging our bets -- in case SHA-256 turns out to be insecure 
I guess I need a slight correction... I missed a 'not'. Putting multiple insecure algorithms together does NOT guarantee a secure one.

@_date: 2009-11-12 14:32:12
@_author: james hughes 
@_subject: Re: hedging our bets -- in case SHA-256 turns out to be insecure 
I agree.
The logic of a "unknown flaw" being fixed flies in the face of prudent cryptanalysis. If you don't know the flaw, how can do you know you can or have fixed it. What if there is an unknown flaw in the fix? Wrap that again? Turtles all the way down. Putting multiple insecure algorithms together does guarantee a secure one.
The only solution that works is a new hash algorithm that is secure against this (and all other) vulnerabilities. It may include SHA 256 as a primitive, but a true fix is fundamentally a new hash algorithm. This process is being worked on by a large number of smart people. I can guarantee you that this kind of construction has been looked at. It is my opinion that putting a bandaid around SHA 256 "just in case" is not cryptanalysis, it's marketing.
P.S. once Sha-3 comes out, your bandaid will look silly.
