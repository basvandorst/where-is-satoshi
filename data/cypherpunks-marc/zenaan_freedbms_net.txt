
@_date: 2018-07-02 01:11:13
@_author: Zenaan Harkness 
@_subject: Re: Webmail? 
Only a -true- Scotsman would self sign and issue his own SSL certs :)
I believe it's degrees of snake oil...
It's actually a bit of an open question, but the real question is
"what level of security do you actually need or want?" â¦ of course.
Consider e.g.:
 - how to conduct an MITM?
   - get a copy of private key, issue additional cert
   - get private key, issue intermediate cert
   - get intermediate cert key, issue other intermediate certs
 - is an MITM of concern to you?
   - I do/don't care if the govt has open access to my certs,
     and can/can't MITM my users
   - I do/don't care if Random J Cracker can MITM my telecommuting
     co-workers
Most of us by now will have read at least one article about the CIA
and NSA's cornucopia of network cracking tools, bugs, physical
network bugs and "rogue" hosts hosted at every host hoster worth his
hosting salt - google, amazon, your local ISP, etc - oh, and "by
consent, possibly with $ inducement and implied threat of legal
sanction for failure to 'co-operate' with said inducement".
But again, what level of security do you need?
 â If you want minimum pain web servers accessible to the public,
   providing "basic trust level", then LetsEncrypt is likely your
   best approach.
 â If you don't have "general public access" as a requirement, and
   you are willing to double check each device accessing your local
   network to ensure they've each pre-loaded your internal-only web
   server cert, then your own PKI and self signed certs may give you
   higher security (barring catastrophic mistakes on your part), and
   certainly more control.
   In this scenario, if you don't need control over the private keys
   to your electronic kingdom - by all means, use a commercial or
   free provider, and LetsEncrypt is perhaps about as good as it
   gets.
   For those after a higher level of certainty and control however,
   there is no substitute to offline master key generation and
   storage, private PKI and total in-house control over all
   sub-keys/sub-certs, issuances, revocations and per-device key
   signature double checking.
If you want anything even resembling certainty that is...
Here's a true fist hand anecdote:
 A few years ago, I was setting up a VM and some basic web services
 for a client with a top tier 'reputable' provider - one of the
 largest in Australia, at least at the time.
 When connecting via ssh for the first time, the usual "This is a new
 host, here's the sig, do you confirm this is the correct signature"
 type message came up.
 So I check the time and it's well after peak hour (about 11pm), and
 quickly dial their 24-hour tech support line, asking for someone to
 please check the host signature (this was virtual hosting, just
 before VMs took off), so as to preclude any superficial MITM
 swankery.
 The tech says to me "Um, I'm not sure about that, I've never been
 asked that question before," and promptly bumps it up to a deep tech
 PoC (person of competency), who double checks the host SSH sig hash
 from their internal network, confirms it's correct, then he too says
 "you know, as far as I'm aware, in over 15 years, this is the first
 time, ever, that any client has ever asked us to confirm their end
 to end encryption/ signature!" I confirmed that he had in fact
 worked at the company since it began.
And most important - if you intend to configure your own PKI for
"increased security" purposes, note that you really need to make sure
you use certificate pinning for your own org's certs, or disable the
other CAs "by default trusted" by your client OS browsers! - The easy
way is use a Firefox profile for your company/ internal websites/
VPN/ telecommuters and make sure only your CA is the trusted root in
that profile! (and make sure that your internal web sites can only be
accessed from that profile) :
 Are self-signed certificates actually more secure than CA
 signed certificates now?
 See also:
4 fatal problems with PKI
SSL certificate revocation and how it is broken in practice
When are self-signed certificates acceptable for businesses?
Good luck :)

@_date: 2018-07-01 02:27:33
@_author: Zenaan Harkness 
@_subject: Re: Webmail? 
For sieve, make sure you batch-process your emails where
possible. E.g. for bastion end user workstation setup, getmail or
mpop to download in a batch, have it dump the incoming mails to a tmp
file, then run sieve on that file - I find mail dl excrutiatingly
slow without one of those batch downloaders, and sieve must compile
all the rules - no point doing that for each email, just for the
batch. Good luck,

@_date: 2018-09-14 01:16:56
@_author: Zenaan Harkness 
@_subject: root "login" xterm to increase security? 
Anyone know if it's possible to get xterm (or xfce4-terminal or any
other terminal for that matter) to be a "native/ clean login
terminal", to increase security when running root commands?
 To: CypherPunks

@_date: 2020-07-06 10:49:35
@_author: Zenaan Harkness 
@_subject: debmirror: apt update performed "unsandboxed"? ~=> file path not readable 
This was a question, but after some digging, answered itself (see near bottom), via a short recursive path analysis script showing that one path component of the path hierarchy failed to have world-readable perms (a dir in the middle), so in case it's useful for some:
Local debmirror mirror, InRelease is out of date so setting Acquire::Check-Valid-Until=false but getting "unsandboxed" notice/warning:
# apt update -o Acquire::Check-Valid-Until=false
------->> 20200706 <<-------
Get:1 file:/public/debian/sid sid InRelease [146 kB]
Ign:2 file:/public/debian/sid sid/main amd64 Packages  Err:3 file:/public/debian/sid sid/main Translation-en    File not found - /public/debian/sid/dists/sid/main/i18n/Translation-en (2: No such file or directory)
Get:4 file:/public/debian/sid sid/contrib amd64 Packages [70.1 kB]
Reading package lists... Done        N: Download is performed unsandboxed as root as file '/public/debian/sid/dists/sid/InRelease' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)
E: Failed to fetch file:/public/debian/sid/dists/sid/main/i18n/Translation-en  File not found - /public/debian/sid/dists/sid/main/i18n/Translation-en (2: No such file or directory)
E: Some index files failed to download. They have been ignored, or old ones used instead.
Now when checking that file which is purpotedly causing the "unsandboxed" 'download', we get this:
# ll /public/debian/sid/dists/sid/InRelease
------->> 20200706 <<-------
93K -rw-r--r-- 1 zenan zenan 143K 20200627 16:32.03 /public/debian/sid/dists/sid/InRelease
Clearly that file is readable by all users.. hmm.
So let's analyze the full path:
$ zfile /public/debian/sid/dists/sid/InRelease
------->> 20200706 <<-------
---- Analyzing "/public/debian/sid/dists/sid/InRelease"
type: /home/zenan/bin/zfile: line 9: type: /public/debian/sid/dists/sid/InRelease: not found
f: /public/debian/sid/dists/sid/InRelease
Drwxr-xr-x root  root  /
drwxr-xr-x root  root  public
lrwxrwxrwx root  root  debian -> /Library/Lpools/zen/p1-setups_misc/repos/debian
Drwxr-xr-x root  root    /
drwxr-xr-x root  zenan   Library
drwxr-xr-x root  root    Lpools
drwxr-x--- zenan zenan   zen
Drwxr-xr-x zenan zenan   p1-setups_misc
Drwxr-xr-x zenan zenan   repos
drwxrwxr-x zenan zenan   debian
lrwxrwxrwx root  root  sid -> d00
lrwxrwxrwx zenan zenan   d00 -> d00-sid+tst+src-64
drwxr-xr-x zenan zenan     d00-sid+tst+src-64
drwxrwxr-x zenan zenan dists
drwxrwxr-x zenan zenan sid
-rw-r--r-- zenan zenan InRelease
-rw-r--r-- 1 zenan zenan 146310 Jun 27 16:32 /Library/Lpools/zen/p1-setups_misc/repos/debian/d00-sid+tst+src-64/dists/sid/InRelease
text/plain; charset=us-ascii
{namei|readlink|/usr/bin/file} -f {file}...
And we notice that /public/debian is a symlink and further down, this suspicious dir:
drwxr-x--- zenan zenan   zen
Culprit identified!  A quick chmod a+rx /Library/Lpools/zen and the show is back on the road.
And the swanky recursive path analyzer (bash script):

@_date: 2020-07-08 14:02:39
@_author: Zenaan Harkness 
@_subject: Re: minimizing buffer size (i.e. page cache) for bulk copy/rsync -- Re: Swappiness in Buster 
I also know nothing about swappiness.
But this soft limit tool sounds exceedingly powerful ()
Hmm. Excuse me ... where was I?
Oh yes, powERRR!
What I imagine, and surely hope is that this softlimit program works as
advertised.  You see, the copy program (normally aliased as `cp`) when
copying say 900 Gigabytes, does not, or certainly should not, consume
more than say 1 Megabyte of memory as a read buffer.  Of course _some_
buffer is needed, since otherwise each read call reads only one byte,
which would result in a ridiculous number of kernel calls and the
performance that doing so entails.  But half a MiB being written to the
destination, whilst another half MiB is being read from the source
"should" be more than enough, since, roughly speaking, those buffers can
simply swap over when they're both ready for another round.
Since if it's not written in a totally insane way, cp (and to a lesser
extent rsync) should work like this already, what we really need to
limit is that pig with the strange "Linux" name (I think it's called
fake lipschtick on an even-toed ungulate family Suidae, I mean kernel).
You see Linux in its eternal and fruitless desire to make user's happy,
gladly tells cp's read side that reading is finished, and also tells its
write side that writing is finished, and so cp races as fast as it
possibly can even if, and especially if, one of the drives is a
different speed than the other (almost always the case) - and the bigger
the difference, the quicker RAM is filled up, and Firefox's tabs evicted
from memory.
What has been happening since the dawn of time is that this kernel
caches _every_ disk page read by a program such as cp (I know, I know,
this is previously unheard of and amazing information I'm leaking) into
something Linux calls the 'page cache' (totally bizzare thing to call an
in memory cache of disk pages, but hey, what do I know...).
And Linux, ever generous with other peoples' resources, hands out your
RAM, ballooning the page cache as though the world will end if it does
not do this to the greatest extent possible "because you might refer to
one of those pages in the near future" and of course you do, when you
write it to the destination disk, at which point, like a pregnant Sow on
meth, Linux almost faints in excitement as its prediction that you would
use that page again, comes true.
And so now Linux is ecstaticly hyping the utility of all these "read
once, write once" pages in the vain hope that evicting all your firefox
Tab's cached memory pages is The Right Thing To Do â¢Â©Â®.  (And Firefox,
unlike cp and rsync, is helpful enough to tag most of its RAM pages with
"Totally ephemeral dude, feel free to dump to disk or even completely
destroy at any time").
Firefox is super helpful like that :(
And, cp and rsync are evidently nowhere as helpful as Firefox is to
Linux's desire to please the user, and so your desktop experience goes
straight down the drain.
This has been happening since I discovered Linux - 20 years ago, the
symptom was the entire desktop stuttering, the X cursor jumping with a
latency of 10 seconds, if you were lucky.
Alas, every one of the 4000+ kernel developers works for Google or
Amazon ECS, and throughput is the only thing the datacenter needs - that
and the ability to compile Linux kernels.

@_date: 2020-07-08 07:51:54
@_author: Zenaan Harkness 
@_subject: minimizing buffer size (i.e. page cache) for bulk copy/rsync -- Re: Swappiness in Buster 
Your thread brings to mind an "$AGE_OF_LINUX_KERNEL + 1 year" years old
Linux kernel bona fydee, bulk copy bug bear:
Doing a bulk copy e.g. using cp or rsync, to copy GiBs far greater than
available RAM from one disk to another, results in ...
    Total Annihilation of Desktop Latency
This happens to this day even on the latest Linux kernels on Sid such as
5.7.0-1-amd64 to pick an entirely random example... and has been
literally bugging me since the beginnings of my Debian journey ~20 years
Note that in this extremely humble user's experience, even
    ionice --class Idle ...
is not even able to quell the Memory Beast's all consuming hunger games
for all possible RAM and then some, in it's insatiable appetite for all
the page cache in...
     The  Entire  Universe
What needs to happen â¢Â©Â®
What is needed is an option somewhere, strictly abided by, where "For
the following command" only say 1MiB of buffer and Page Cache, in TOTAL,
is able to be used by that command ever, until its completion.
In this way my 16GiBs of RAM will __finally__ be left to its rightful
owners Firefox, mutt, and my XOrg's cursor, since to me as an extremely
humble desktop user, these are the only things in the world that I ever
care about, and I esPECIALLY want to NOT care that a 1,700 GiB movie
disk backup takes an extra 5 or 10 % longer than the NINE hours it
already takes across its USB 2 connection!
Seriously, is there a way to stop bulk copies from eternally flushing my
$Desktop's cached pages down the drain minute after blessful minute,
hour after gratitude filled hour?
The heavens will truly be pleased ..

@_date: 2020-07-18 05:18:58
@_author: Zenaan Harkness 
@_subject: Re: tmpfs is not a ramdisk (was: delimiters with more than one character? ...) 
PC snow-nerf: "You don't need encrytion if you've got nothing to hide, and unless you're visiting friends during the lock down which you must not do because it's like totally illegal, you've got nothing to hide."
Alpha nagger: "I don't have to be doing anything wrong, to want my privacy!"
(Hush now, you're only s'posed to talk about human rights, not actually live them .. right?)
If you're current RAM+swap is say 8GiB+8GiB, one alternative is to consider upgrading RAM to 16GiB and ditching swap entirely.  That could save your lazy butt from having to learn how to encrypt swap - and just so you know I do in fact want you to feel good about your choice, it will also mean a faster overall user experience ;D

@_date: 2020-07-18 05:18:58
@_author: Zenaan Harkness 
@_subject: Re: tmpfs is not a ramdisk (was: delimiters with more than one character? ...) 
PC snow-nerf: "You don't need encrytion if you've got nothing to hide, and unless you're visiting friends during the lock down which you must not do because it's like totally illegal, you've got nothing to hide."
Alpha nagger: "I don't have to be doing anything wrong, to want my privacy!"
(Hush now, you're only s'posed to talk about human rights, not actually live them .. right?)
If you're current RAM+swap is say 8GiB+8GiB, one alternative is to consider upgrading RAM to 16GiB and ditching swap entirely.  That could save your lazy butt from having to learn how to encrypt swap - and just so you know I do in fact want you to feel good about your choice, it will also mean a faster overall user experience ;D
