
@_date: 2002-04-22 08:50:32
@_author: Nicko van Someren 
@_subject: Re: objectivity and factoring analysis 
You'd need a large abacus and a vary large stack of paper...
If you had read the Bernstein proposal in detail you would
understand that (among other things) it details the conceptual
design of a machine for computing kernels in a large, sparse
matrix.  The design talks of the number of functional units and
the nature of the communication between these units.  What I
set out to do was look at how complex those units are and how
hard it would be to connect them and to place a cost on this.
I'm sorry, but I don't think at infinite speed.  I started
this process after lunch and the panel session started at
1:30pm.  I did say that this was an impromptu session.
See my comments above.  The costing was based on transistor count
and engineering costs.  The design suggested in the Bernstein
proposal does not have a simple size/time trade-off since the size
of the system is proscribed by the algorithm.
I used the number 10^9 for the factor base size (compared to about
6*10^6 for the break of the 512 bit challenge) and 10^11 for the
weight of the matrix (compared to about 4*10^8 for RSA512).  Again
these were guesses and they certainly could be out by an order of
OK, here are the numbers I used.  Again I preface this all with
it being order of magnitude estimates, not engineering results.
It's based on a proposal, not a results paper, and there are
doubtless numerous engineering details that will make the whole
thing more interesting.
The matrix reduction cells are pretty simple and my guess was
that we could build the cells plus inter-cell communication
in about 1000 transistors.  I felt that, for a first order guess,
we could ignore the transistors in the edge drivers since for a
chip with N cells there are only order N^(1/2) edge drivers.
Thus I guessed 10^14 transistors which might fit onto about 10^7
chips which in volume (if you own the fabrication facility) cost
about $10 each, or about $10^8 for the chips.  Based on past work
in estimating the cost of large systems I then multiplied this
by three or four to get a build cost.
As far at the speed goes, this machine can compute a dot product
in about 10^6 cycles.  Initially I thought that the board to
board communication would be slow and we might only have a 1MHz
clock for the long haul communication, but I messed up the total
time and got that out as a 1 second matrix reduction.  In fact to
compute a kernel takes about 10^11 times longer.  Fortunately it
turns out that you can drive from board to board probably at a
few GHz or better (using GMII type interfaces from back planes
of network switches).  If we can get this up to 10GHz (we do have
lots to spend on R&D here) we should be able to find a kernel in
somewhere around 10^7 seconds, which is 16 weeks or 4 months.
There are, of course, a number of other engineering issues here.
One would want to try to work out how to build this machine to
be tolerant of hardware faults since getting 10^7 chips to all
run faultlessly for months at a time is tough to say the least.
Secondly, we are going to need some neat power reduction
techniques in these chips to dissipate the huge power that it
needs, which will likely run to 10^8 watts or so so the facility
will look a bit like a steel foundry.
Lastly, I want to reiterate that these are just estimates.  I
give them here because you ask.  I don't expect them to be used
for the design of any real machines; much more research is
needed before that.  I do however think that they are rather
more accurate than my first estimates.
I hope this helps.

@_date: 2002-04-21 18:37:32
@_author: Nicko van Someren 
@_subject: Re: objectivity and factoring analysis 
I'm inclined to agree with you there
Since you mention it, I will make a comment for the purpose of
clearing up a number of misunderstandings about what I said
and the context in which the comments were made.
At the Financial Cryptography 2002 conference a small and
impromptu panel was convened to discuss the Bernstein
proposal.  Since I'm in the business of building hardware I
was asked to comment on the cost of building some of the
hardware described therein.  I limited myself to comments
about the design for the engine that could be used to take
the results of the sieve process and compute values leading
to a pair of roots, and furthermore prefaced and qualified
my comments with strong statements about any numbers being
very rough back of an envelope calculations.  The estimate
of the cost of construction I gave was "some hundreds of
millions of dollars", a figure by which I still stand.
I was then asked how fast this machine would run and I tried
to do the calculation on the spot without a copy of the
proposal to hand, and came up with a figure on the order
of a second based on very conservative hardware design.
This figure is *wildly* erroneous as a result of both not
having the paper to hand and also not even having an
envelope on the back of which I could scratch notes.  The
number was based on a miscalculation of the number of
clock ticks the circuit would need by a factor of 10^11,
which is a vast error that is only slightly moderated by
the fact that on further analysis I concluded that the
hardware could be made to operate on a clock that ran
between 10^3 and 10^4 times faster since the inter-circuit
communication did not need to be as slow as I had originally
thought.  Thus I think that with care a matrix reduction
machine of the sort described could be built to run in a few
weeks or months for 1024 bit keys.
Despite all the qualifying of these statements I felt then,
and still feel now, that if you have keys that you think
rich governments might genuinely be interested in then you
should use ones that are longer than 1024 bits.  If you have
personal information that you want to keep secret from rich
governments for many years to come then you should probably
use longer keys anyway.  After all we can expect on past form
that the security agencies are some years ahead of the
academic state of the art in this field anyway.  On the
other hand if you are moving general commercial data around
on an everyday basis I don't think that there is much wrong
with 1024 bits keys and I would not, and have not, suggested
that there is anything insecure about such key lengths for,
say, electronic banking or e-commerce using SSL.
I have to say that Adam's suggestion that there was some
sort of ulterior motive for my comments is both disingenuous
and somewhat insulting.  In the context of short and impromptu
discussion on a topic which people felt was a live one, I made
a back of the envelope calculation in which I used the "time"
figure as the "cost" figure and came up with totally the wrong
number.  I wasn't expecting that this was going to then be
used as the basis for anything other than maybe an excuse for
looking into the problem a little more deeply.  The critical
problem here seems to be that Lucky then quoted this number on
a mailing list before I'd had a chance to look more closely.
I don't think that there was any conflict of interest involved
at all given both the nature of the discussion and the fact
that these days cryptographic acceleration is pretty peripheral
to the nature of my business.
I agree entirely.  The field of factoring is still moving forward
and with luck Bernstien's proposal will lead to him getting funding,
which will lead to new advances, and this my yet necessitate longer
keys in the future.  The biggest cause for concern with regards to
factoring at the moment is that 17% of the SSL servers on the net
are still using 512 bit keys and in countries like France (which have
in the past had domestic usage controls) the figure is more like 40%.
My research student last winter showed that 512 bit keys can be
factored in a matter of weeks using only the hardware found in a
busy 70 person office.  People tend not to change over existing
keys for longer ones as time goes by and now that 512 bit keys are
low hanging fruit people's inertia and resistance to altering working
server configurations is in danger of becoming a real problem.
I think your suggestion of bias is itself misplaced.  Most of
the comments on this that I have seen have been pretty measured.
Even if your cynicism is well founded in some cases I think that
the real problem here has been people being too swift to quote
and too slow to check with the source.

@_date: 2002-04-24 08:42:58
@_author: Nicko van Someren 
@_subject: Re: Any info on this maybe improved matrix algebra for GNFS? 
there are some neat tricks in this implementation.  Ben Handley
from the University of Otago in New Zealand worked for us last
winter and spent some time on a fast implementation of the
block Lanczos algorithm with the speed critical sections done
in Itanium machine code.  This was one step towards the goal of
showing that 512 bit keys can be factored without having to use
a supercomputer but can in fact be factored using just what we
had in the office.  The technology is little different from the
stuff done to solve the last puzzle in The Codebook other than
the fact that few offices have quad-processor Compaq Alpha
servers but an increasingly large number have Win2K servers
with Itaniums in them.
and code as part of his university dissertation.  I don't know
if he reads this list so I will forward this message to him in
case he wants to add more.
