
@_date: 2002-06-26 03:57:09
@_author: Jon Callas 
@_subject: Re: Ross's TCPA paper 
I think it even goes further than that.
I was giving one of my DMCA-vs-Security talks while l'affaire Sklyarov was
roiling, and noted that while that was going on, the US was being testy with
China over alleged espionage by US nationals while in China. At a high
level, each of infringement and espionage can be described as:
Alice gives Bob some information. Bob is careless with it, disclosing it to
someone that Alice would rather not see it. Alice has a non-linear response.
You can call it infringement or you can call it espionage, but at the bottom
of it, Alice believes that a private communication has been inappropriately
disclosed. She thinks her privacy has been compromised and she's stomping
angry about it.
At the risk of creating a derivative work, you say pr-eye-vacy, I say
pr-ih-vacy. Infringement, espionage, let's call the whole thing off.
    Jon

@_date: 2002-08-02 03:47:05
@_author: Jon Callas 
@_subject: Re: Challenge to David Wagner on TCPA 
Is this a tacit way to suggest that the only people who need anonymity or
pseudonymity are those with something to hide?
    Jon

@_date: 2002-08-02 03:47:05
@_author: Jon Callas 
@_subject: Re: Challenge to David Wagner on TCPA 
Is this a tacit way to suggest that the only people who need anonymity or
pseudonymity are those with something to hide?
    Jon

@_date: 2011-09-24 01:34:39
@_author: Jon Callas 
@_subject: [cryptography] Nirvana 
Much better to counter with TPM-protected malware.

@_date: 2011-12-18 21:04:59
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
Which is precisely what can't be done, in the general case.
It's really, really, doable in the singular case. If the channel signs the code (which is what Apple does on the App Store), then sure, Alice is your auntie. But when developer D has code they sign *themselves* with a cert given from signatory S, and delivered to marketplace M, you end up with some sort of DSM-defined insanity. There's no responsibility anywhere. The worst, though, is to go to the signer and say, "This is another fine mess you've gotten me into, Stanley."
cryptography mailing list

@_date: 2011-12-11 23:12:12
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
Yes, doing it properly is the key and I'll assert that Apple is doing a pretty good approximation of it. They are doing more or less what I described -- good coding enforcement backed up with digital signatures. There are plenty of people squawking about it. I know developers who've thrown up their hands and there is plenty of grumpiness I've heard. Some of it reasonable grumpiness, too.
But the end result for the users is that malware rate is close to zero. The system is by no means perfect, and has side-effects. But the times when something slipped through the net are so few that they're notable still. (And some of the malware has been kinda charming, like the flashlight app that had a hidden SOCKS proxy that let people use it for tethering.) More importantly, the system does not throw things at the users that they're incapable of handling, like the Android way of just informing you what capabilities an app needs. People can and do just hand devices to their kids and let them use them with no ill effects.
cryptography mailing list

@_date: 2011-12-10 20:13:46
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
Sure, and that's why the assurance system and the signatures have to be tied together and the incentives have to be aligned. In a software market where the app store itself is doing the validation, doing the enforcement, signing the code, and taking the responsibility for both delivering the software and backfilling the inevitable errors, you'll see the *system* lower malware. But even in that, it's the system that's doing it, not digital signatures. The signatures are merely the wax seals. The quality system has to be built to create and deliver quality. That is the sine qua non of this whole thing.
I think we agree that trying to build quality by giving certificates to developers is a fantasy at best.
cryptography mailing list

@_date: 2011-12-10 17:44:16
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
aged it =
ches of =
nside =
aling =
s and =
run out =
igures =
rs would demonize the practice as having to get a license to code.
ryone but
 via
 know
Well said, Steve, and that's largely my point. Any code signing system that=
 wants to survive the scrutiny of people like us has to essentially hand ou=
t certs for free. (And there are regimes that in fact do that, and work wel=
Therefore, you must assume that any given cert might have been stolen or si=
mply issued to a bad person. The sketch I gave previously about how code-si=
gning is useful even with zero trust on the signing certs. You detect malwa=
re by signatures, hashes, and keys, and *then* by content scanning, and bui=
ld up whitelists and blacklists. It isn't perfect, but it works better than=
 mere content scanning alone.
Any code-signing system that assigns worth to the developers based upon cer=
tificate issuance is effectively agreeing with those who think that evil-do=
ing can be stopped by ID cards and an attendant ban on anonymity/pseudonymi=
There is, however, a way to surpass the loosie-goosy signature system. If t=
he software marketplace itself were to issue signatures, then you'd have a =
way to get an improvement. You'd really want to back up the mere digital si=
gnature with an assurance system. If the marketplace enforced code reviews,=
 and backed up the code reviews with a capability-based OS so that they cou=
ld enforce some practices (for example, you could keep a Disgruntled Birds =
game from turning on the microphone and camera with capabilities and a code=
 review), then you would expect such a software marketplace to have a drama=
tically lower rate of malware. =
They'd have that lower malware rate because in that system, the digital sig=
nature is an assurance mark on top of an actual assurance system. The debat=
e that we're having boils down to wringing our hands over how to make an as=
surance mark that assures quality with no underlying assurance evaluation a=
nd enforcement.
If someone actually built such combination of OS and marketplace, it would =
work for the users very well, but developers would squawk about it. Properl=
y done, it could drop malware rates to close to nil.
But anyway, you're absolutely right, and that's really my point. A code sig=
ning system that operates on its own has to assume that certs get lost, sto=
len, or handed out to bad (or misguided) people, and have that as part of i=
ts threat model.
cryptography mailing list

@_date: 2011-12-09 21:01:05
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
Maybe this is syntactically true, or even code-wise true, but this sounds crazed.
OpenPGP has the same problem, since all users are CAs, and revocation has to come from a cert itself (or a delegated revoker).
If you have a certificate issue a revocation for itself, there is an obvious, correct interpretation. That interpretation is what Michael Heyman said, and what OpenPGP does. That certificate is revoked and any subordinate certificates are also implicitly revoked. It's also like making a CRL for everything you issued.
If a software implementation did any of the other things, like crash, it's pretty obviously a bug. If a developer defended crashing or accepting any relevant certs on the grounds of it not being a well-formed first order logic, we'd yell at that developer.
cryptography mailing list

@_date: 2011-12-09 20:46:18
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
If it were hard to get signing certs, then we as a community of developers would demonize the practice as having to get a license to code.
cryptography mailing list

@_date: 2011-12-07 22:55:09
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
I understand what you're saying, but I don't agree.
CAs have always punted liability. At one point, SSL certs came with a huge disclaimer in them in ASCII disclaiming all liability. Any CA that accepts liability is daft. I mean -- why would you do that? Every software license in the world has a liability statement in it that essentially says they don't even guarantee that the software contains either ones or zeroes. Why would certificates be any different?
I don't think it really exists, not the way it gets thrown around as a term. Liability is a just a bogeyman -- don't go into the woods alone at night, because the liability will get you!
cryptography mailing list

@_date: 2011-12-07 22:45:55
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
Marsh, you've hit on a few good points.
The main one is that one of the original purposes of digital signatures is to make it possible to sign a contract between parties that are not physically present. That actually works quite well. But there's been mission creep into absurdity and that happened nearly immediately in the development of digital signatures.
Nonreputiation is one of these. I think that the very idea of nonrepudiation goes back to Leibniz, who thought we could get rid of judges and solve disputes with, "Gentlemen, let us calculate!" That isn't going to happen, and we only have to wave towards Messrs. Russell, Whitehead, Goedel, and Turing (Hi, guys!) and move on.
Nonrepudiation is a somewhat daft belief. Let me give a gedankenexperiment. Suppose Alice phones up Bob and says, "Hey, Bob, I just noticed that you have a digital nature from me. Well, ummm, I didn't do it. I have no idea how that could have happened, but it wasn't me." Nonrepudiation is the belief that the probability that Alice is telling the truth is less than 2^{-128}, assuming a 3K RSA key or 256-bit ECDSA key either with SHA-256. Moreover, if that signature was made with an ECDSA-521 bit key and SHA-512, then the probability she's telling the truth goes down to 2^{-256}.
I don't know about you, but I think that the chance that Alice was hacked is greater than 1 in 2^128. In fact, I'm willing to believe that the probability that somehow space aliens, or Alice has an unknown evil twin, or some mad scientist has invented a cloning ray is greater than one in 2^128. Ironically, as the key size goes up, then Alice gets even better excuses. If we used a 1k-bit ECDSA key and a 1024-bit hash, then new reasonable excuses for Alice suggest themselves, like that perhaps she *considered* signing but didn't in this universe, but in a nearby universe (under the many-worlds interpretation of quantum mechanics, which all the cool kids believe in this week) she did, and that signature from a nearby universe somehow leaked over. This absurd-excuse paradox means that if you *really* believe in non-repudiation, you need not only to avoid keys that are too small, but too large.
Now, in the real world, Alice might repudiate the signature, but pay Bob anyway. Or Bob might just accept Alice's excuse because there are reasonable chances something odd happened (like Alice got hacked). Or Bob might take Alice to court, where a judge or jury would access a constellation of things including the reasonableness of the contract, Alice and Bob's individual reputations, and also some defaults (a five-dollar charge might be presumed to be disputable, and a million-dollar property purchase assumed to not be disputable).
We got to this problem through some reasonable and unreasonable natural human things. We inherently distrust new technologies. There was a time when you couldn't fax a legal document. Then we got used to it. Today, most places will accept an emailed PDF of a scan of a document, but not all. There are a few amusing situations where you take a scan, print it, then fax the paper and it's a legal document, but not that PDF itself, either digitally signed or not.
Nonrepudiation is really an argument that this math combined with some rituals make bits as good as a fax.
Intent is another good point. Contract law and practice has intent wired through it all over the place. Trust is also a huge can of worms, as well as possibly not even being definable.
If we step back, though, this is similar to the code-signing discussion in that there's *mechanism* of PKI and *policy* of PKI. Not only do we conflate the two, but we have a tendency to criticize mechanism because of policy, and vice versa.
That conflation of mechanism and policy is a huge problem, and made worse by those who want to make it a bigger problem, by wanting to encode policy into mechanism. Yeah, yeah, they can never be completely separate, but admitting they aren't the same thing would be a great start.
cryptography mailing list

@_date: 2011-12-07 20:10:25
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
By policy, I mean that you decide what it's supposed to mean, which is what you get to at the end of this. But in the rest of paragraph reflects that I am a systems developer and if I am trying to debug why something that is revoked is (or isn't) working when it shouldn't (or should), then I have to create things that are error conditions.
I also once worked on a secure microprocessor, and there were many ways to permanently kill it. I know I wouldn't buy such a system.
I think that's the central problem we're dealing with. There is scads of mechanism and little policy.
I also don't think we're going to agree on what policy should be, except within limited contexts.
cryptography mailing list

@_date: 2011-12-07 17:34:29
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
I want to say that the answer is "mu" because you can't actually revoke a certificate. That's not satisfying, though.
I think it is a policy question. If I were making a software development system that used certificates with both expiration dates and revocation, I would check both revocation and expiry. I might consider it either a warning or an error, or have it be an error that could be overridden. After all, how can you test that the revocation system on the back end works unless you can generate revoked software?
On a consumer-level system, I might refuse to install or run revoked software; that seems completely reasonable. Refusing to install or run expired software is problematic -- the thought of creating a system that refuses to work after a certain date is pretty creepy, and the workaround is to set the clock back. But really, it's a policy question that needs to be answer by the creators of the system, not the crypto/PKI people. We can easily create mechanism, but it's impossible to create one-size-fits-all policy.
cryptography mailing list

@_date: 2011-12-07 16:31:23
@_author: Jon Callas 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
There are many things about code signing that I don't think I understand.
I think that code-signing is a good thing, and that all things being equal, code-signing is a good thing, and that code should be signed.
However, there seems to strange, mystical beliefs about it.
As an example, there's the notion that if you have signed code and you revoke the signing key (whatever revoke means, and whatever a key is) then the software will automagically stop working, as if there's some sort of quantum entanglement between the bits of the code and the bits of the key, and invalidating the key therefore invalidates the code.
This seems to me to be daft -- I don't see how this *could* work in a general case against an attacker who doesn't want that code to stop working (and that attacker could be either a malware writer or the owner of the computer). I can see plenty of special cases where it works, but it is fundamentally not reliable and a security system that wants to stop malware or whatever by revoking keys is even less reliable because we now have three or four parties (malware writer, machine owner, certifier, anti-virus maker).
It also seems to me that discussions on this list hit this situation from two strange directions. One is the general sneering at the daft belief. The other is continuing to discuss it. I don't care who is using it (even effectively); we're all smart enough to know both that DRM cannot work, and yet there are users of it that are happy with it. Whatever.
Slightly tangential to this is a discussion of expiration of signing keys. In reality, they don't expire. Unless you you make a device that can be permanently broken by setting the clock forward (which is certainly possible, merely not desirable), then expiry can be hacked around. The rough edge of what happens to code that expires while it is executing generalizes out to a set of other problems that just show that in fact, you can't really expire a code signing key any more than you can revoke it -- that is to say there are many edge conditions in which it works and many of these are useful to some people and some circumstances, but in the general case, it doesn't and cannot work.
But that doesn't mean that code signing is a bad thing. On the contrary, code signing is very useful because you can use the key, the signature, or the hash as a way to detect malware and form a blacklist, as well as detect software that should be whitelisted.
Simply stated, an anti-malware scanner can detect (and remove) a specific piece of malware by the simple technique of comparing its signature to a blacklist. It can compare a single object's hash to a list of hashes and that only requires the scanner to hash the code object; this catches the simple case of malware that is merely re-signed with a new key. It also permits it to do more complex operations than a simple hash (like hashing pieces, or hash at different times) to identify a piece of malware. It can also use the key to detect whole classes of malware (or good-ware).
Code signing is good because it gives the anti-malware people a set of tools that augment what they have with some easy, fast, effective ways to categorize software as known goods or known bads. But that's it -- you don't get the spooky action at a distance aspects that some people think you can do with revocation. You get something close, if you feed the blacklist/whitelist information to whatever the code-scanner is. Nonetheless, this answers how you deal with signed malware (once it's known to be malware, you stop it via signature), or bogus 512-bit signing keys (just declare anything signed by such to either be treated as malware or as unsigned).
So am I missing something? I feel like I'm confused about this discussion because *of* *course* you can't revoke a key and have that magically transmit to software. Perhaps some people believe that daft notion and have built systems that assume that this is true. So what? Maybe it works for them. The places where it doesn't work aren't even interesting. Perhaps observing when this daft notion meets the real world is helpful as an object lesson. Perhaps it works for *them* but not *us*.
But really, I think that code signing is a great thing, it's just being done wrong because some people seem to think that spooky action at a distance works with bits.
cryptography mailing list

@_date: 2012-02-26 19:33:46
@_author: Jon Callas 
@_subject: Re: [cryptography] US Appeals Court upholds right not to decrypt a drive 
You're assuming that they operate with the same security model that you do.
Your security model presupposes US law, to start with. I can see that in the glib comment asking if I'd ever heard of "innocent until proven guilty" -- which is a US principle. It is one that I not only have heard of, but think is is pretty darn good idea, too!
Nonetheless, it does not exist everywhere in the world, and I said this was not the US. In fact the very reason I said it wasn't the US was because I wanted to point out that objections to the story based upon US law are irrelevant. Moreover, innocent until proven guilty is interpreted differently depending on what sort of case there is. The term *proven* is context-dependent. There are different ways they prove, different burdens of proof. "Beyond reasonable doubt" and "clear and convincing evidence" are two used in criminal cases in the US. "Preponderance of evidence" is usually used in civil cases.
None of these are "plausible deniability." As I said before, this is a term of spycraft and statecraft. Usually it's used to describe how a powerful entity like a nation state can defend itself against attacks by less-powerful entities. There are forms of torture that are popular because they leave no marks on the victim and therefore give the state plausible deniability. Bureaucracies also use this technique to spread blame or leave the blame with some other person. In a number of cases involving spectacularly failed companies, the CEO has tried to stick someone else with the blame through plausible denial. Or perhaps the family and associates of a fraudster use a form of plausible denial to avoid conviction or trial. (I am not saying that using plausible means you're guilty -- it only means you don't have a better defense.) It works sometimes and doesn't work others. It didn't work for Bernie Ebbers, for example. Plausible denial combined with a lack of evidence works really well, but it's not a legal principle at all.
Most people who use the term "plausible denial," particularly us crypto people, would be better served to say "reasonable doubt." It's a better marketing term at the very least.
But anyway, back to deniable encryption and what is a language-theoretic issue.
If your security model includes technical issues and policy issues, but your attacker has different policies, then your security might fail for language-theoretic reasons.
To a border control person (and that's who I was talking about), Truecrypt is the same thing as a suitcase with a false bottom. Technically, we'd say that it is a container that (assuming it works correctly) *might* have a secret compartment and that one that does have secret compartment is information-theoretcially indistinguishable from one that has a secret compartment. But if you read the previous sentence to a border control person, they might hear, "...it is a container ... that ... has a secret compartment." The difference is policy, not technical. If their security model includes the policy that there's no reason to have a suitcase with a false bottom except to put something in it, then how you make a denial becomes everything.
If your denial is "don't be ridiculous, I *know* you guys can spot hidden volumes and that's why I'd never use one -- I use it because I'm cheap" then you're doing well. If your denial is, "you can't prove there's a hidden volume there" then you're not doing so well.
My point is that there are security models out there that know about hidden volumes and have their own defenses against them. I used the word "defenses" intentionally. They are border control people. Their model considers a hidden volume to be an attack, not a defense. They have developed their own defenses against smuggling that take hidden volumes into account.
Absolutely. This is a good thing, too. Please don't think that I am defending what they do. I think people should have full legal protections at border crossings. I think that the erosion of fourth and fifth amendment rights is deplorable. But again, you're assuming US law, and you're ignoring administrative actions. They can seize your laptop and there's not a lot you can do. It's a loss, you'll never get it back, particularly if it's not your country. They can flag you for a full search on every crossing, too. Ironically, one of the best defenses you'd have (particularly as a US citizen coming into the US) would be to say something like, "Of course there's a hidden volume there. I'll tell you what's in it -- there's the personal records for my customers, and if I open that up for you, I have to notify every one of them under forty-some state laws. There's millions of them there, and I can't afford to do that. That's why I'm using Truecrypt -- I have to have that data with me, and I don't trust other countries from stealing my data. If you have to keep my laptop until my lawyer calls, it will suck, but I understand. But can you cut me some slack this time? I wrote a bunch of emails and memos while I was on the plane and they aren't backed up."
Note that this is also a form of plausible denial, but the plausible denial has nothing to do with the crypto.
cryptography mailing list

@_date: 2012-02-26 17:35:55
@_author: Jon Callas 
@_subject: Re: [cryptography] US Appeals Court upholds right not to decrypt a drive 
Only from your point of view. From their point of view, the user is the one with wrong assumptions.
Remember what I said -- they're law enforcement and border control. In their world, Truecrypt is the same thing as a suitcase with a hidden compartment. When someone crosses a border (or they get to perform a search), hidden compartments aren't exempt. They get to search them. Also to them, Truecrypt is a suitcase that advertises a hidden compartment, and that's pretty useless, in their world.
Or just put something in it that you can show. cryptography mailing list

@_date: 2012-02-25 07:50:39
@_author: Jon Callas 
@_subject: Re: [cryptography] US Appeals Court upholds right not to decrypt a drive 
There is no such thing as plausible deniability in a legal context.
Plausible deniability is a term that comes from conspiracy theorists (and like many things contains a kernel of truth) to describe a political technique where everyone knows what happened but the people who did it just assert that it can't be proven, along with a wink and a nudge.
But to get to the specifics here, I've spoken to law enforcement and border control people in a country that is not the US, who told me that yeah, they know all about TrueCrypt and their assumption is that *everyone* who has TrueCrypt has a hidden volume and if they find TrueCrypt they just get straight to getting the second password. They said, "We know about that trick, and we're not stupid."
I asked them about the case where someone has TrueCrypt but doesn't have a hidden volume, what would happen to someone doesn't have one? Their response was, "Why would you do a dumb thing like that? The whole point of TrueCrypt is to have a hidden volume, and I suppose if you don't have one, you'll be sitting in a room by yourself for a long time. We're not *stupid*."
cryptography mailing list

@_date: 2012-03-29 22:38:53
@_author: Jon Callas 
@_subject: Re: [cryptography] Key escrow 2012 
Hash: SHA1
I'd have to disagree with you on much of that.
The US Government never required key escrow for international commerce. Encrypted data was never restricted, what was restricted was the export of software etc. If you were of a mind where you thought that the only way to get cryptographic software was from the US, then you'd think this might be something like effective. In reality, the idea was absurd from the get-go because encrypted data was never restricted.
The people who wanted to push key escrow never had a good way to explain to anyone why they'd want it. They never had a good carrot, either, for it. At one point, they tried to sugar-coat it by offering fast-tracks on export for it, but Commerce granted export easily. Furthermore, Commerce's own rules progressed so fast with so many exemptions that it was all obviated before it could be developed.
Amusingly, I ended up having TIS's RecoverKey under my bailiwick because Network Associates bought PGPi and then TIS. The revenues from it were so small that I don't think they even covered marketing material like that shirt you had. In a very real sense, it didn't exist as anything more than a proof-of-concept that proved the concept was silly.
Also, there wasn't a PGP system. The PGP "additional decryption key" is really what we'd call a "data leak prevention" hook today, but that term didn't exist then. Certainly, lots of cypherpunks called it that at the time, but the government types who were talking up the concept blasted it as merely a way to mock (using that very word) the concept.
cryptography mailing list

@_date: 2012-05-25 18:19:33
@_author: Jon Callas 
@_subject: Re: [cryptography] can the German government read PGP and ssh traffic? 
Hash: SHA1
My money would be on a combination of traffic analysis and targeted malware. We know that the Germans have been pioneering using targeted malware against Skype. Once you've done that, you can pick apart anything else. Just a simple matter of coding.
cryptography mailing list

@_date: 2013-02-09 07:06:55
@_author: Jon Callas 
@_subject: Re: [cryptography] "Meet the groundbreaking new encryption app set to revolutionize privacy..." 
Hash: SHA1
I am separating this from my previous as I went into a rant.
As we were designing Silent Text, we talked to a lot of people about what they needed. I don't remember who told me this anecdote, but this person went over to a colleague's office after they'd been texting to just talk. They walked into the colleagues office and noticed their phone open with a conversation plainly visible with someone else. A third party who was their mutual colleague was texting about that meeting.
In short: Alice goes to Bob's office for a meeting and sees texts from Charlie about that meeting, including comments about Alice.
There wasn't anything untoward about the texting. No insults about Alice or anything, but there was an obvious privacy loss here. What if it *had* been included an intemperate comment about our Alice? Alice said nothing about it to Bob, but I got an earful. That earful included the opinion that the threat of accidental disclosure of messages within a group of people is greater than either the messages "being plucked out of the air" or seizure and forensic groveling over the device. Alice's opinion was that when people have a secure communications channel, they loosen up and say things that are more dramatic than they would be otherwise. It's not that they're more honest, they're less honest. They're exaggerated to the point of hyperbolic at times. Alice said that she knew that she'd texted some things to Bob that she really wouldn't want the person she'd said them about to see them. They were said quickly, in frustration, and so on. It's not that they'd be taken out of context, it's  that they'd be taken *in* context.
It's interesting underlying the story, Alice suddenly saw Bob not as an ally in snark, but a threat -- the sort of person who leaves their phone unlocked on their desk. Bob, of course, would say something like that if the texts had been potentially offensive, he'd have locked his phone. This explanation would thus convince Alice that Bob is *really* not to be trusted with snark.
This is incredibly perceptive, that the greatest security threat is not the threat from outside, it's the threat from inside. It is exactly Douglas Adams's point about the babelfish that by removing barriers to communication, it created more and bloodier wars than anything else.
That's where "Burn Notice" came from. It's a safety net so that when Charlie texts Bob, "I'm tired of Alice always..." it goes away.
What I find amusing is the reaction to it all around. There's a huge manic-depressive, bimodal reaction. Lots of people get ahold of this and they're like girls who've gotten ahold of makeup for the first time. ZOMG! You mean my eyelids can be PURPLE and SPARKLY? This is the same thing that happens when people discover font libraries or text-to-speech systems. For a couple of days that someone gets the new app, there's nothing but text messages that are self-destructing, purple, sparkly eyelids with font-laden Tourette's Syndrome with the Mission Impossible theme song playing in the background. (Note, if you are using Silent Text, you can't actually make the text purple, nor sparkly, nor change fonts. You need to put all of that in a PDF or an animated GIF -- and you will. This is a metaphor, not a requirements document.)
The next thing that happens is that they are so impressed with some particularly inspired bit self-desctructing childishness that they take a screen shot. As they gaze at the screen shot, or sometimes just as they take the screen shot, light dawns. Oh. You mean.... Oh. Then the depressive phase kicks in.
Back in the dark ages, PGP had the "For Your Eyes Only" feature. This is pretty much the ancestor of Burn Notice. Simultaneously useful and worthless. It's useful because it signals to your partner that this is not only secret but sensitive and does something to stop accidental disclosure. It is utterly ineffective against a hostile partner for many of the same reasons. We did all sorts of silly things with FYEO that included an anti-TEMPEST/Van Eck font, and other things. Silent Text actually has an FYEO feature that isn't exposed, thank heavens.
I mention all of that because once you're in the depressive phase, it's easy to go down the same rathole we did with FYEO. I spent time researching if you can prevent screen shots on iOS (you can't). I did this while telling people that it was dumb because I can take a picture of my iPhone with my iPad. I held up my phone to video chat and said, "Here, see this? This is what you can do!"
Sanity prevailed, but I think that fifteen years of FYEO helped a lot. When you stare into self-destructing messages, trying to figure out how make them really go away flawlessly, they stare back. You will end up trying to figure out how to do a destructive two-phase commit, what class libraries need to be patched so those that non-mutable strings inherit from mutable strings (not the other way around), all while a nagging voice whispers in the back of your head about how brave freedom fighters are gonna die because of this.
After the depressive phase comes the patronizing, retributive phase in which it's clear that letting people delete potentially embarrassing messages is bad, because it's imperfect. Imperfect security is worse than plaintext. People have to learn self-control. Cue the Kalil Gibran quotes. People can't just say any old thing on a secure chat program because that leads to purple eyeshadow and thus inevitably to brave freedom fighters having their phones seized at borders, and then people will die -- all because we let them delete their incriminating messages. This phase makes so little sense that it's hard for me even to mock it. But the gist of that objection really is that it's bad to let people delete sensitive things because that will cause seizure of sensitive things. Otherwise sane people have said this to me, and they don't seem to see how funny they are.
Nonetheless, there's two things that happen. On the one hand, there are people who think this cute, simple feature is the second coming of sliced bread. The other hand is the people who insist it must be impossible (because they've over-thought it) or evil (because security shouldn't be fun, let alone purple). There is a small point to the dour, greyfaced side of this, I admit. You cannot solve human problems with technology. Technology often just shuffles around the brilliance that humans have at shooting themselves in the foot. I'm well aware of Laotse's snarky comment that the invention of locks created burglary, and I often agree with him. But I think there has to be fun with security. We talk a lot about how security has to be usable, but I think fun is up there, too. If it's fun, people will use it. They make their mistakes cheaply, and in a reasonably safe environment. Most of all, they'll actually use it. That's been the challenge of the last couple decades, getting people to use it. People use things that they play with. I think thus that play is part of security, too. What's "groundbreaking" in what we're doing is that we're having fun and encouraging others to do so, too.
cryptography mailing list

@_date: 2013-02-08 19:26:23
@_author: Jon Callas 
@_subject: Re: [cryptography] "Meet the groundbreaking new encryption app set to revolutionize privacy..." 
Hash: SHA1
Thanks for your comments, Ian. I think they're spot on.
At the time that the so-called Arab Spring was going on, I was invited to a confab where there were a bunch of activists and it's always interesting to talk to people who are on the ground. One of the things that struck me was their commentary on how we can help them.
A thing that struck me was one person who said, "Don't patronize us. We know what we're doing, we're the ones risking our lives." Actually, I lied. That person said, "don't fucking patronize us" so as to make the point stronger. One example this person gave was that they talked to people providing some social meet-up service and they wanted that service to use SSL. They got a lecture how SSL was flawed and that's why they weren't doing it. In my opinion, this was just an excuse -- they didn't want to do SSL for whatever reason (very likely just the cost and annoyance of the certs), and the imperfection was an excuse. The activists saw it as being patronizing and were very, very angry. They had people using this service, and it would be safer with SSL. Period.
This resonates with me because of a number of my own peeves. I have called this the "the security cliff" at times. The gist is that it's a long way from no security to the top -- what we'd all agree on as adequate security. The cliff is the attitude that you can't stop in the middle. If you're not going to go all the way to the top, then you might as well not bother. So people don't bother.
This effect is also the same thing as the best being the enemy of the good, and so on. We're all guilty of it. It's one of my major peeves about security, and I sometimes fall into the trap of effectively arguing against security because something isn't perfect. Every one of us has at one time said that some imperfect security is worse than nothing because it might lull people into thinking it's perfect -- or something like that. It's a great rhetorical flourish when one is arguing against some bit of snake oil or cargo-cult security. Those things really exist and we have to argue against them. However, this is precisely being patronizing to the people who really use them to protect themselves.
Note how post-Diginotar, no one is arguing any more for SSL Everywhere. Nothing helps the surveillance state more than blunting security everywhere.
cryptography mailing list

@_date: 2013-02-07 07:18:33
@_author: Jon Callas 
@_subject: Re: [cryptography] "Meet the groundbreaking new encryption app set to revolutionize privacy..." 
Hash: SHA1
No offense is taken. You don't even need a pen test. I'll tell you how it w=
There's no magic there. Every message that we send has metadata on it that =
is a timeout. The timer starts when you get the message. So if I send you a=
 seven minute timeout while you're on an airplane, the seven minutes starts=
 when you receive the message.
And you are correct, the iOS app model doesn't allow background tasks, so i=
f you switch away from the app for an hour, the delete doesn't happen until=
 you switch back to the app. Until Apple lets us do something in the backgr=
ound, we're stuck with that limitation. It's that simple. We hope to do bet=
ter on Android. And if someone from Apple happens to be listening in, we'd =
love to be able to schedule some deletions.
Deleting the things, however, is trivial. This is a place that iOS shines. =
Every file is encrypted with a unique key and if you delete the file, it is=
 cryptographically erased. You're correct in that flash *is* notoriously di=
fficult to wipe unencrypted secrets. Fortunately for us, all the flash on i=
OS is encrypted and the crypto management is easy to use.
cryptography mailing list

@_date: 2013-04-04 18:43:12
@_author: Jon Callas 
@_subject: Re: [cryptography] ICIJ's project - comment on cryptography & tools 
Hash: SHA1
re its team-based project work:
represents one of the biggest cross-border investigative partnerships in jo=
urnalism history. Unique digital systems supported private document and inf=
ormation sharing, as well as collaborative research. These included a messa=
ge center hosted in Europe and a U.S.-based secure online search system.  T=
eam members also used a secure, private online bulletin board system to sha=
re stories and tips."
ch as PGP (=93Pretty Good Privacy=94) were abandoned because of complexity =
and unreliability that slowed down information sharing. Studies have shown =
that police and government agents =96 and even terrorists =96 also struggle=
 to use secure e-mail systems effectively.  Other complex cryptographic sys=
tems popular with computer hackers were not considered for the same reasons=
.  While many team members had sophisticated computer knowledge and could u=
se such tools well, many more did not."
This is great. It just drives home that usability is all.
cryptography mailing list

@_date: 2013-07-23 18:39:52
@_author: Jon Callas 
@_subject: Re: Python Random Number Generator for OTP 
Actually, you want to whiten it before output, not before input. Whitening before input is a problem, because you can't run an estimator on the input -- because it's been whitened.
If you want to know the unbiased entropy of a source, you want the raw inputs. If you don't care about the unbiased entropy, then you don't.

@_date: 2013-08-29 23:04:35
@_author: Jon Callas 
@_subject: Re: [cryptography] Reply to Zooko (in Markdown) 
Hash: SHA1
I've spoken to my own lawyers and gotten their opinions. My comments on things reflect my knowledge.
You are, of course, entitled to your own opinion, but I disagree. I say the answer is no -- or perhaps more fully, no more than yours is.
Thank you.
cryptography mailing list

@_date: 2013-08-17 22:35:07
@_author: Jon Callas 
@_subject: Re: [cryptography] Reply to Zooko (in Markdown) 
Hash: SHA1
I've had the privilege on several occasions to talk to people who really do this stuff. A couple of things really stuck with me:
* "Don't patronize us. We know what we're doing, we know what we're up against." The guy who told me this had his brother murdered horribly. His tradecraft was basic and elegant.
* Simple, usable countermeasures are best because they have to be used by the sort of person who decided yesterday that they're not going to take it any more. They're newly-minted heroes who a threat to themselves and others if they screw up what they're doing. We asked them what they'd like most and the answer was SSL on websites. This was after Diginotar and we'd been talking about advanced threats, so we were a bit taken aback. They explained that the biggest problems are people putting stuff on websites as well as mistakes like making calendar entries for times and places of meetings. That put a fine point on the admonition not to patronize them. Heck, the adversaries don't have to crack anything sophisticated when they can just sniff CalDAV.
cryptography mailing list

@_date: 2013-08-17 06:04:38
@_author: Jon Callas 
@_subject: [cryptography] Reply to Zooko (in Markdown) 
Hash: SHA1
Also at # Reply to Zooko
(My friend and colleague, [Zooko Wilcox-O'Hearn]( wrote an open letter to me and Phil [on his blog at LeastAuthority.com]( Despite this appearing on Silent Circle's blog, I am speaking mostly for myself, only slightly for Silent Circle, and not at all for Phil.)
Thank you for writing and your kind words. Thank you even more for being a customer. We're a startup and without customers, we'll be out of business. I think that everyone who believes in privacy should support with their pocketbook every privacy-friendly service they can afford to. It means a lot to me that you're voting with your pocketbook for my service.
Congratulations on your new release of [LeastAuthority's S4]( and [Tahoe-LAFS]( Just as you are a fan of my work, I am an admirer of your work on Tahoe-LAFS and consider it one of the best security innovations on the planet.
I understand your concerns, and share them. One of the highest priority tasks that we're working on is to get our source releases better organized so that they can effectively be built from [what we have on GitHub]( It's suboptimal now. Getting the source releases is harder than one might think. We're a startup and are pulled in many directions. We're overworked and understaffed. Even in the old days at PGP, producing effective source releases took years of effort to get down pat. It often took us four to six weeks to get the sources out even when delivering one or two releases per year.
The world of app development makes this harder. We're trying to streamline our processes so that we can get a release out about every six weeks. We're not there, either.
However, even when we have source code to be an automated part of our software releases, I'm afraid you're going to be disappointed about how verifiable they are. It's very hard, even with controlled releases, to get an exact byte-for-byte recompile of an app. Some compilers make this impossible because they randomize the branch prediction and other parts of code generation. Even when the compiler isn't making it literally impossible, without an exact copy of the exact tool chain with the same linkers, libraries, and system, the code won't be byte-for-byte the same. Worst of all, smart development shops use the *oldest* possible tool chain, not the newest one because tool sets are designed for forwards-compatibility (apps built with old tools run on the newest OS) rather than backwards-compatibility (apps built with the new tools run on older OSes). Code reliability almost requires using tool chains that are trailing-edge.
The problems run even deeper than the raw practicality. Twenty-nine years ago this month, in the August 1984 issue of "Communications of the ACM" (Vol. 27, No. 8) Ken Thompson's famous Turing Award lecture, "Reflections on Trusting Trust" was published. You can find a facsimile of the magazine article at  and a text-searchable copy on Thompson's own site, .
For those unfamiliar with the Turing Award, it is the most prestigious award a computer scientist can win, sometimes called the "Nobel Prize" of computing. The site for the award is at .
In Thompson's lecture, he describes a hack that he and Dennis Ritchie did in a version of UNIX in which they created a backdoor to UNIX login that allowed them to get access to any UNIX system. They also created a self-replicating program that would compile their backdoor into new versions of UNIX portably. Quite possibly, their hack existed in the wild until UNIX was recoded from the ground up with BSD and GCC.
In his summation, Thompson says:
    The moral is obvious. You can't trust code that you did not totally
    create yourself. (Especially code from companies that employ people
    like me.) No amount of source-level verification or scrutiny will
    protect you from using untrusted code. In demonstrating the
    possibility of this kind of attack, I picked on the C compiler. I
    could have picked on any program-handling program such as an
    assembler, a loader, or even hardware microcode. As the level of
    program gets lower, these bugs will be harder and harder to detect.
    A well installed microcode bug will be almost impossible to detect.
Thompson's words reach out across three decades of computer science, and yet they echo Descartes from three centuries prior to Thompson. In Descartes's 1641 "Meditations," he proposes the thought experiment of an "evil demon" who deceives us by simulating the universe, our senses, and perhaps even mathematics itself. In his meditation, Descartes decides that the one thing that he knows is that he, himself, exists, and the evil demon cannot deceive him about his own existence. This is where the famous saying, "*I think, therefore I am*" (*Cogito ergo sum* in Latin) comes from.
(There are useful Descartes links at:  and  and .)
When discussing thorny security problems, I often avoid security ratholes by pointing out Descartes by way of Futurama and saying, "I can't prove I'm not a head in a jar, but it's a useful assumption that I'm not." Descartes's conundrum even finds its way into modern physics. It is presently a debatable, yet legitimate theory that our entire universe is a software simulation of a universe . Martin Savage of University of Washington  has an interesting paper from last November on ArXiV .
You can find an amusing video at   in which Savage even opines that our descendants are simulating us to understand where they came from. I suppose this means we should be nice to our kids because they might have root.
Savage tries to devise an experiment to show that you're actually in a simulation, and as a mathematical logician I think he's ignoring things like math. The problem is isomorphic to writing code that can detect it's on a virtual machine. If the virtual machine isn't trying to evade, then it's certainly possible (if not probable -- after all, the simulators might want us to figure out that we're in a simulation). Unless, of course, they don't, in which case we're back not only to Descartes, but Godel's two Incompleteness Theorems and their cousin, The Halting Problem.
While I'm at it, I highly, highly recommend Scott Aaronson's new book, "Quantum Computing Since Democritus"  which I believe is so important a book that I bought the Dead Tree Edition of it. ([Jenny Lawson]( has already autographed my Kindle.)
Popping the stack back to security, the bottom line is that you're asking for something very, very hard and asking for a solution to an old philosophical problem as well as suggesting I should prove Godel wrong. I'm flattered by the confidence in my abilities, but I believe you're asking for the impossible. Or perhaps I'm programmed to think that.
This limitation doesn't apply to just *my* code. It applies to *your* code, and it applies to all of us. (Tahoe's architecture makes it amazingly resilient, but it's not immune.) It isn't just mind-blowing philosophy mixed up with Ken Thompson's Greatest Hack.
Whenever we run an app, we're trusting it. We're also trusting the operating system that it runs on, the random number generator, the entropy sources, and so on. You're trusting the CPU and its microcode. You're trusting the bootloader, be it EFI or whatever as well as [SMM]( on Intel processors -- which could have completely undetectable code running, doing things that are scarily like Descartes's evil demon. The platform-level threats are so broad that I could bore people for another paragraph or two just enumerating them.
You're perhaps trusting really daft things like [modders who slow down entropy gathering]( and [outright bugs]( Ironically, the attack vector you suggest (a hacked application) is one of the harder ways for an attacker to feed you bad code. On mobile devices, apps are digitally signed and delivered by app stores. Those app stores have a vetting process that makes *targeted* code delivery hard. Yes, someone could hack us, hack Google or Apple, or all of us, but it's very, very hard to deliver bad code to a *specific* person through this vector, and even harder if you want to do it undetectably.
In contrast, targeted malware is easy to deploy. Exploits are sold openly in exploit markets, and can be bundled up in targeted advertising. Moreover, this *has* happened, and is known to be a mechanism that's been used by the FBI, German Federal Police, the Countries Starting With the Letter 'I' (as a friend puts it), and everyone's favorite The People's Liberation Army. During Arab Spring, a now-defunct government just procured some Javascript malware and dropped it in some browsers to send them passwords on non-SSL sites.
Thus, I think that while your concern does remind me to polish up my source code deployment, if we assume an attacker like a state actor that targets people and systems, there are smarter ways for them to act.
I spend a lot of time thinking, "*If I were them, what would I do?*" If you think about what's possible, you spend too much time on low-probability events. Give yourself that thought experiment. Ask yourself what you'd do if you were the PLA, or NSA, or a country starting with an 'I.' Give yourself a budget in several orders of magnitude. A grand, ten grand, a hundred grand, a million bucks. What would you do to hack yourself? What would you do to hack your users without hacking you? That's what I think about.
Over the years, I've become a radical on usability. I believe that usability is all. It's easy to forget it now, but PGP was a triumph because you didn't have to be a cryptographer, you only had to be a techie. We progressed PGP so that you could be non-technical and get by, and then we created PGP Universal which was designed to allow complete ease of use with a trusted staff. That trusted staff was the fly in the ointment of Silent Mail and the crux of why we shut it down -- we created it because of usability concerns and killed it because of security concerns. Things that were okay ideas in May 2013 were suddenly not good ideas in August. I'm sure you've noted when using our service our belief in usability. Without usability that is similar to the non-secure equivalent, we are nothing because the users will just not be secure.
I also stress Silent Circle is a *service*, not an app. This is hard to remember and even we are not as good at it as we need to be. The service is there to provide its users with a secure analogue of the phone and texting apps they're used to. The difference is that instead of having utterly no security, they have a very high degree of it.
Moreover, our design is such to minimize the trust you need to place in us. Our network includes ourselves as a threat, which is unusual. You're one of the very few other people who do something similar. We have technology and policy that makes an attack on *us* to be unattractive to the adversary. You will soon see some improvements to the service that improve our resistance to traffic analysis.
The flip side of that, however, is that it means that the device is the most attractive attack point. We can't help but trust the OS (from RNG to sandbox), bootloader, hardware, etc.
Improvements in our transparently (like code releases) compete with tight resources for improvements in the service and apps. My decisions in deploying those resources reflect my bias that I'd rather have an A grade in the service with a B grade in code releases than an A in code releases and a B service. Yes, it makes it harder for you and others, but I have to look at myself in the mirror and my emphasis is on service quality first, reporting just after that. Over time, we'll get better. We've not yet been running for a year. Continuous improvement works.
I'm going to sum up with the subtitle of the ACM article of Ken Thompson's speech. It's not on his site, but it is on the facsimile article:
    To what extent should one trust a statement that a program is free
    of Trojan horses? Perhaps it is more important to trust the people
    who wrote the software.
Thank you very much for your trust in us, the people. Earning and deserving your trust is something we do every day.
cryptography mailing list

@_date: 2013-08-31 07:13:28
@_author: Jon Callas 
@_subject: Re: Who bought off Zimmermann? 
I consider delivering a zero-day to be a form of cryptanalysis. I believe that they do, too. I've been harping on that for some time.
I recognize that I have a tendency to be glib in one sentence and then rigorous in another and that's a character flaw. It's glib to say both "the crypto works" and "zero days are cryptanalysis" in many respects.
When I say, "the crypto works" I mean the basic structures. We know how to build block ciphers. We figured out hash functions a few years ago. We understand integer-based public-key cryptography well enough that it gives us the creeps. We kinda sorta understand ECC, but not as well as we think we do. I think our understanding of ECC is like our understanding of hash functions in 2003. Meow.
The protocols mostly work, except when they don't. The software is crap. It's been nearly fifteen years since Drew Gross enlightened me by saying, "I love crypto; it tells me what part of the system not to bother attacking."
Look at it anthropicly. We know the crypto works because the adversary says they're looking at metadata. To phrase that differently, they're looking at metadata because the crypto works! Look at things like Fishbowl, even. It's easy to get dazzled by the fact that Fishbowl is double encryption to miss that it's really double *implementations*.
The crypto works. The software is crap.
Think like the adversary. Put yourself in their shoes. What's cheaper, buying a 'sploit or cracking a cipher? Once you start buying 'sploits, why not build your own team to do them yourself, and cut out the middleman? Every other part of the tech world has seen disintermediation, what makes you think this is different.
On the other end of things, there's traffic analysis. We have seen -- stuff -- from them over the last decade. Papers on social graph analysis, pattern analysis. Emphasis on malware, validation, and so on. Here's another analogy. Imagine that you're looking at a huge, fantastically complex marching band. You're trying to figure out who all is doing what to what parts of the music and it's horribly complex. And then accidentally one day, you lose the audio feed and then realize that it's *easier* to tell what the band is doing when the sound is off.
Aphasiacs are (so I am told) good at telling truth from lies because they look at the face rather than listen to the voice. They analyze the metadata, because they can't hear the data and it works *better*.
Traffic analysis is what you do if your feed from the marching band loses its audio. It's what you do if you're aphasiac -- which is *exactly* what happens when the crypto works, by the way.
Thus with a large budget, you do both. With one hand, you crack the crypto by cracking the software. When it works it works. When it doesn't, it doesn't. Stop stressing. With the other hand, you revel in the glory of silence. In silence you can think. You watch the band, you watch square dance. You just watch who is pairing with whom, where the lines cross and the beats are. Sometimes you can even guess the tune by watching the dance (which is also cryptanalysis).
And all of that is why the problem in email isn't the crypto, it's SMTP.

@_date: 2013-08-30 23:12:41
@_author: Jon Callas 
@_subject: Re: Who bought off Zimmermann? 
That's precisely what we mean.
The crypto is the easy part. The hard part is the traffic analysis, of which the worst part is the Received headers. Everyone should look at their own headers -- especially people on this list and at least comprehend that your email geotracks you forever, as it's all in the Mailman archive.
There are plenty of other leaks like Message-ID, Mime-Version, X-Mailer, the actual separators in MIME part breaks, and so on. It's absolutely correct that some combination of VPNs, Tor, remailers of whatever stripe, and so on can help with this, but we're all lazy and we don't do it all the time.
What we're learning from Snowden is that they're doing traffic analysis -- analyzing movements, social graphs, and so on and so forth. The irony here is that this tells us that the crypto works. That's where I've been thinking for quite some time.
Imagine that you're a SIGINT group trying to deal with the inevitability of crypto that works being deployed everywhere. What do you do? You just be patient and start filling in scatter plots of traffic analysis.
The problem isn't the crypto, it's SMTP.

@_date: 2013-09-11 15:17:28
@_author: Jon Callas 
@_subject: Re: hardware RNG 
Yes. If you took noise off of a diode or even a resister and just threw it into Yarrow, you'd have a very nice thing.
The biggest problem with building good random number generators is that it's harder than you think on first glance and easier than you think on third glance.

@_date: 2013-09-01 03:15:25
@_author: Jon Callas 
@_subject: Re: why not disable external mail, keep intenal mail (Re: Who bought off Zimmermann?) 
I believe that when one is on a team, the more senior one is on the team, the more one has the responsibility to discuss the *team* decision even when one's opinion was different. Actually, *especially* when one's personal decision was different. Every decision has reasons for and reasons against. One's job as a senior team member is to talk about the way one came to the decision for, and not about the reasons against.
I just had a short conversation with Mike Janke about this issue and this discussion, and with his leave I'm going to go against my normal beliefs.
Silent Circle is Mike's vision. He did physical security in a variety of countries and saw that people who are expats from anywhere in anywhere else have a lot of issues they have to face that are all secure communications. Moreover, these people are told "no" all the time (don't use Skype, don't use Gmail, don't trust SMS, don't use cell phones, landlines) and never "yes." The initial vision of Silent Circle was to give those people a "yes." There are (were) three pillars of that vision to give people yesses -- voice/video/etc., texting etc., and email etc.
When I wrote that the email was "something of a quandary," that means that Mike was always for it and I was always against it. I see the other side of it and believe that something that's email-like is essential. We have an architecture for how we're going to grow texting into "messaging" and that will be email-like with true end-to-end security for internal mail. It is a ways off. There are lots of things to work on, from user experience to syncing across devices -- each with real security.
In the meantime, what do the users do? We did a lot of talking to end users, and what they want and need is more than just internal email. They need it to be connected to the Internet. Part of the use case includes that someone wants to send a subscriber a PDF of an insurance form, rental agreement, or so on that the subscriber needs to print out, sign, scan, and send back. A number of them said that what they really wanted as much as anything was an email system run by people who give a damn about security as much as the crypto itself. Whatever that means.
Mike was (and is) a happy customer of one of the existing secure email systems for years, understood its limitations and thought that a useful system could be made out of a conventional email infrastructure augmented by PGP Universal. I was on the other side. PGP Universal is designed for a different use case, a different threat model, blah, blah, blah. You've heard me say it, so I won't repeat it.
When I rationally looked at the facts of the situation, Silent Mail's proposed security was *different* than other secure email systems, but similar. If someone uses it "securely" then it's very good, and when they use it "conveniently" it isn't worse than any of the other convenience-minded secure email systems. Moreover, and getting to the real brass tacks here, Mike's the boss. It's his dream and his money funding it. As an interim system to have, it isn't that bad.
Additionally, one of my bugaboos about security is something I call "security arrogance." Security arrogance is when the security person tells the users what their threat model should be. It's closely related to another thing I talked about a decade ago that I called "the security cliff" -- you start with no security and to get to security, you have to climb a cliff rather than ascend a ramp in that you can't stop halfway up. I believe that one of the ways we security people shoot our clients in the foot is to focus on the ways that security is imperfect and thus argue that less-than-perfect security is worse than no security.
Okay, fine. Hoist by my own petard. Silent Mail, ho!
I'll also add that other team members were of course, spread all over the essential quandary here from thinking it was wonderful to being conflicted to thinking that Silent Mail was worse than nothing.
Development-wise, we had some plans to improve Silent Mail -- specifically, one of the tasks was to make a network widget that would scrape offending headers out of SMTP. However, note that we're a startup. Life is not a zero-sum game, but development is. Every iota of effort that's spent propping up SMTP is an iota that's not going to making its replacement. This ended up being a different sort of quandary. The people who accepted Silent Mail warts and all (or shock, horror liked it) like the idea of the new "messaging" system even better. Thus, propping up SMTP didn't really have any champions, and it's not like we have people sitting around doing nothing. We all considered Silent Mail to be a stop-gap.
Let me fast-forward up to the day before we shut Silent Mail down. One of the major requests that we had was to split the suite of products up. We were working on precisely that. (And it should go live next week.) In fact, we were *discussing* a breakup of the suite even before Silent Mail went live, and we noted that it became a legacy product after being up for about a week.
As there was more and more news about state-sponsored espionage (China, Countries Starting With The Letter 'I', etc.), we got more "business" customers and they were as a rule not interested in secure email that was not under the direct control of their own IT. Post-Snowden, the people who thought, "It's good enough" became fewer. The proportion of users who were using Silent Mail was about 5% of the total.
Every account has a page where you set up your devices, and there's a link to click to set up Silent Mail. Only people who clicked that link got set up, and the 5% number is the people who set it up, so that's obviously an upper bound of people using it.
We had been discussing shutting it down -- that 5% figure is either an argument for why it just isn't succeeding as a product, or an argument why the people who are using it understand it and its limitations. It was a discussion item for our September BoD meeting. My plan was to suggest we stop taking new orders and subscription renewals as part of the suite break-up, and then just let it fade away. I was, in fact, lobbying hard for that. I believe I would have prevailed at the board meeting, but of course I'd think that.
Your suggestion about making it be internal-only was something I'd be willing to compromise on. However, remember that much of the whole *point* of Silent Mail is that it's a well-run Internet Email System.
Now let's get to the day we shut it down. I had been at the VoIP conference, ClueCon, in Chicago. As luck would have it, I finished up early and failed to get standby on an early flight home. Others of us were scattered with other travel. One of my major thoughts was what if there's paperwork on its way, and that paperwork doesn't know I'm in an airport lounge? When I finally got Mike on the phone, he said, "You did the right thing. I'm glad you're my partner." Interestingly, the guys who work for me told me after that they had decided that they would delete things themselves if things went on for another couple hours.
I know this has been long, so let me sum up answers to your questions:
* Silent Mail was always a debate between perfect and good enough. It was even a debate over what it means to be good enough.
* The people who thought it was good enough don't think like you and me, and I think their point of view has it's own validity.
* The people who wanted it wanted it to be an Internet Email System above all. Even in the design of the new thing, it has to be connected to the Internet so that someone on the Internet can send you an email. Pulling back to being internal-only would not meet the goals of the people who wanted it.
* We're a startup. We only have so many resources, and no one was the champion of making Silent Mail better. The people who thought it was good enough didn't see the point in making it better, and the people who thought it wasn't good enough didn't see the point either.
I hope this helps explain.

@_date: 2013-09-07 04:31:33
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
Actually, I'm doing the opposite. I'm starting with a theorem and arguing informally from there. And yes, I know it's informal. I'm starting with the math and then arguing as an engineer -- it doesn't make sense do go the way you're suggesting when there are other paths that are known to be worth doing. Oh, and we know that the NSA is doing.
I don't see that it is nonsense to argue that if I were an evil genius, I'd go here for these reasons and then note that that is actually the direction that we know the NSA is going.
I'm saying that the math and the facts on the ground point thataway.
The cryptography mailing list

@_date: 2013-09-07 00:58:33
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
I'd be happy to give a different answer, like -- almost certainly not.
We know as a mathematical theorem that a block cipher with a back door *is* a public-key system. It is a very, very, very valuable thing, and suggests other mathematical secrets about hitherto unknown ways to make fast, secure public key systems. To me, it's like getting a cheap supply of gold and then deciding you'll make bullets out of it instead of lead. To riff on that analogy, it feels like you're suggesting that they would shoot themselves in the foot because they know that the bullet fragments will hurt their opponent.
That's why I say almost certainly not. It suggests irrationality beyond my personal ken. It's something I classify colloquially as "too stupid to live."
My assumptions about the NSA are that they're smart, clever, and practical. Conjectures about their behavior that deviate from any of those axes ring false to the degree that they deviate from that.
My conjectures start with assuming they're at least as smart as me, and I start with "what would I do if I were them?" I think they're smart enough not to attack the strong points of the system, but the weak points. I think they're smart enough to prefer operating in stealth.
Yeah, yeah, sure, if with those resources I stumbled into a fundamental mathematical advantage, I'd use it. But I would use it to maximize my gain, not to be gratuitously sneaky.
The math we know about block ciphers suggests (not proves, suggests) that a back door in a cipher is impractical, because it would imply the holy grail of public key systems -- fast, secure, public key crypto. It suggests secure trapdoor functions that can be made out of very simple components.
If I found one, it would be great, but I'd devote my resources to places where I technology is on my side. Those include network security and software security, along with traffic analysis.
If I wanted to devote research resources, I'd be looking closely at language-theoretic security. I'd be paying close attention to the fantastic things that have come out of there.
The stuff that Bangert, Bratus, Shapiro, and Smith did on turning an MMU into a Turing machine is where I'd devote research, as well as their related work on "weird machines."
I apologize for repeating myself, but I'd fight the next war, not the last one.
The cryptography mailing list

@_date: 2013-09-07 00:21:18
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
This is why I try to say "public key" and "symmetric key" whenever possible.
The cryptography mailing list

@_date: 2013-09-06 04:33:31
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
What am I trying to say?
I'm being a bit of a smartass. I'm sorry, it's a character flaw, but it's one that amuses me. I'll be blunt, instead.
There is a lot of discussion here -- not really so much from you but in general --  that in my opinion is fighting the last war. Sometimes that last war is the crypto wars of the 1990s, but sometimes it's WWII. Yeah, yeah, if you don't remember history you'll repeat it, but we need to look through the windshield, not the rear view mirror.
My smartassedness was saying that by looking at the past, gawrsh, maybe we're seeing a time machine!
The present war is not the previous one. This one is not about crypto. It involves crypto, but it's not *about* it. The bright young things of 1975 who went to work for the NSA wrote theorems and got lifetime employment. The bright young things of 2010 write shellcode and are BAH contractors.
There are two major trends that are happening. One is that they're hitting the network, not the crypto. Look at Dave Aitel's career, not your mathematician friend. Aitel is one of the ones that got away, and what he talks about is what we're seeing that they are doing. If you have to listen to one of the old school mathematicians, listen to Shamir -- they go around crypto. (And actually, we need to look not at Aitel as he left in 2002, but the bright young thing who left last year, but I think I'm making my point.)
The other major trend is that outsourcing, contracting and other things ruined the social contract between them and the people who work there. (This reflects the other other problem which is that the social contract between them and us seems to be void.) Nonetheless, Aitel and others left and are leaving because no longer do they tap you on the shoulder in college and then there's the mutual backscratching of a lifelong career. Now a contractor knows that when the contract is over, they're out of a job. And when the contractor sees malfeasance that goes all the way up to the Commander-in-Chief, they look at what their employment agreement said, as well as the laws that apply to them.
If you're in that environment and you see malfeasance, you go to your superior and it's a felony not to. If your superior is part of the malfeasance, you go to your superior's superior. If it goes all the way up to the CiC, then some sharp, principled kid who is just a contract sysadmin just might put a lot of files on a laptop and decide they have to go to We The People, who are, after all, the ultimate superior.
The cryptography mailing list

@_date: 2013-09-06 03:54:47
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
Not really. The Needham-Schroeder you're thinking of is the essence of Kerberos, and while Kerberos is a very nice thing, it's hardly a replacement for public key.
If you use a Needham-Schroeder/Kerberos style system with symmetric key systems, you end up with all of the trust problems, but on steroids.
(And by the way, please say "symmetric key" as opposed to "public key" -- if you say "private key" then someone will inevitably get confused and think you mean the private half of a public key pair and there will be tears.)
I have to disagree. You don't need a CA. There's a very long rant I could make here, and I'll try to keep it a summary.
Much of the system we have is built needing CAs, but it was only built that way. A long time ago, the certificate structure we're still vestigially using had as one of its goals a way to keep the riff-raff from using crypto. I remember when I got my first PEM certificate, I had to send my blinking passport off to MITRE for two weeks so they could let me encrypt the crapola that was sitting on my disk unencrypted. It was harder to get a cert than it was to get a visa to Saudi Arabia! So much of what we would have encrypted we just printed on paper and put in a file cabinet. Excuse me, I'm starting on that rant I said I wouldn't do.
The major problem one has with public key is knowing that the public key of the endpoint you want to talk to us actually the right public key. Trusted Introducers of any sort are one way to solve the problem. CAs are merely an industrialized form of Trusted Introducer and not ipso facto bad. The way that "Web PKI" (as it's now being called) is using Trusted Introducers is suboptimal, but ironically, we are on the inflection point of a real honest-to-whomever fix to them in the form of Certificate Transparency. That suggests even another discussion, one that I promised Ben I'd get to eventually.
The major problem with the certificate system is actually the browsers, in my opinion, because they actively discourage using certificates in any other way. If browsers, for example, allowed you to use a private cert with a user experience that was ultimately SSH-like (also called TOFU for Trust On First Use) as opposed to putting big blood-red danger warnings up, it would work out better for everyone including the CAs.
But anyway, there are other solutions. They range from some variant of Direct Trust being TOFU or even using a Kerberos-like system to hand you a key, or what we do in ZRTP, or lots of other things. The bottom line is that if you want to send someone a message securely and you have never talked to them before, you have no other way to deal with it than public key systems. (Or you can re-define the problem. Suppose I want to send Glenn Greenwald a message and his Kerberos controller gives me an AES key, I merely have to trust the controller. If we say that trusting him is the same as trusting the controller, then yeah, sure, it works. That's a suitable redefinition in which the KDC is isomorphic to a CA. But if we allow public key, then I could get Mr. Greenwald's public key from an intermediary who is not necessarily an authority, or even self-publish keys. It's done with PGP all the time.)
I concur that the way that browsers and web servers us SSL is suboptimal. This doesn't mean that a solution is impossible, it only means we have a widely-distributed suboptimal system.
I have to disagree on that one, too. The servers that have problems indeed have problems. Servers do not implicitly have problems. The solutions are not evenly distributed, but they're not architectural. Heck, a Raspberry Pi has a hardware RNG.
The cryptography mailing list

@_date: 2013-09-06 02:55:04
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
Yes, but. The reason we are using those curves is because they want them for products they buy. I think it might even go deeper than that. ECC was invented in the civilian world by Victor Miller and Neal Koblitz (independently) in 1985, so they've been planning for breaking it even a decade before its invention. I definitely believe (b). However, I also think that they aren't a monolith, and we know that each part of the mission is the adversary of the other. I don't believe that the IA people would do a bad job to support SIGINT. Once you start down that path, it's easy to get to madness, or perhaps merely evidence that they have time travel.
I'll add that they have a third mission -- run the government's classified computer network, and that *that* mission is the one that Snowden worked for.
The cryptography mailing list

@_date: 2013-09-06 02:19:12
@_author: Jon Callas 
@_subject: Re: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
I don't disagree by any means, but I've been through brittleness with both discrete log and RSA, and it seems like only a month ago that people were screeching to get off RSA over to ECC to avert the "cryptocalypse." And that the ostensible reason was that there are new discrete log attacks -- which was just from Mars and I thought that that proved the people didn't know what they were talking about. Oh, wait, it *was* only a month ago! Silly me.
"Crypto experts issue a call to arms to avert the cryptopocalypse"
Discrete log has brittleness. RSA has brittleness. ECC is discrete log over a finite field that's hard to understand. It all sucks.
And now we're back to the hymnal you and I have been singing from. It ain't the crypto, it's the software.
The cryptography mailing list

@_date: 2013-09-09 13:08:06
@_author: Jon Callas 
@_subject: Re: [cryptography] [liberationtech] Random number generation being influenced - rumors 
Hash: SHA1
I have to disagree with you. Lots of us have told Intel that we really need to see the raw bits, and lots of us have gotten informal feedback that we'll see that in a future chip.
In the meantime, don't use it if you don't like it!
Better, however, would be to continue using whatever software RNG you're using, and reseed it with whatever you're doing now and throw an RDRAND reading in. It won't hurt anything no matter how badly it's broken and helps against any number of things. Heck, I've done that with TPM RNGs that I knew were of limited quality.
Once Intel better documents the RNG and we have ways to look at the entropy source, then we might use it more. Until then, it's somewhere between a toy and a curiosity.
cryptography mailing list

@_date: 2013-09-03 16:28:29
@_author: Jon Callas 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
Hash: SHA1
My understanding is that of the NIST curves, P-256 and P-384 are unencumbered and that P-521 was dropped from Suite B because of IP concerns along with MQV. I don't pretend to speak with authority on any of it. The niggling things often don't make sense. I'm just saying what my understanding is.
The cryptography mailing list

@_date: 2013-09-03 04:49:29
@_author: Jon Callas 
@_subject: Re: [Cryptography] NSA and cryptanalysis 
Hash: SHA1
The real issue is that the P-521 curve has IP against it, so if you want to use freely usable curves, you're stuck with P-256 and P-384 until some more patents expire. That's more of it than 192 bit security. We can hold our noses and use P-384 and AES-256 for a while.
The cryptography mailing list

@_date: 2013-09-07 06:50:26
@_author: Jon Callas 
@_subject: Re: [Cryptography] Why prefer symmetric crypto over public key crypto? 
Hash: SHA1
Point taken.
Bruce made a quip, and I offered an explanation about why that quip might make sense. I have also, in debate with Jerry, opined that public-key cryptography is a powerful thing that can't be replaced with symmetric-key cryptography. That's something that I firmly believe. At its most fundamental, public-key crypto allows one to encrypt something to someone whom one does not have a prior security relationship with. That is powerful beyond words.
If you want to be an investigative reporter and want to say, "If you need to talk to me privately, use K" -- you can't do it with symmetric crypto; you have to use public-key. If you are a software developer and want to say say, "If you find a bug in my system and want to tell me, use K" -- you can't do it with symmetric crypto.
Heck, if you want to leave someone a voicemail securely you've never talked to, you need public key crypto.
That doesn't make Bruce's quip wrong, it just makes it part of the whole story.
The cryptography mailing list

@_date: 2013-09-11 15:19:02
@_author: Jon Callas 
@_subject: Re: [guardian-dev] pgp, nsa, rsa 
LTC is my preferred place to start with a crypto library. It's just brilliant in design.

@_date: 2013-10-03 21:31:12
@_author: Jon Callas 
@_subject: Re: [cryptography] the spell is broken 
Hash: SHA1
You might call it "security theatre," but I call it (among other things) "protest." I have also called it "trust," "conscience," and other things including "emotional." I'm willing to call it "marketing" in the sense that marketing often means non-technical. I disagree with "security theatre" because in my opinion security theatre is *empty* or *mere* trust-building, but I don't fault you for being upset. I don't blame you for venting in my direction, either. I will, however, repeat that I believe this is something gentlepersons can disagree on. A decision that's right for me might not be right for you and vice-versa.
Since the AES competition, NIST has been taking a world-wide role in crypto standards leadership. Overall, it's been a good thing, but one could have one's disagreements with a number of things (and I do), but it's been a good *standards* process.
A good standard, however, is not necessarily the *best*, it's merely agreed upon. A standard that is everyone's second choice is better than a standard that is anyone's first choice. I don't think there are any problems with AES, but I think Twofish is a better choice. During the AES competition, the OpenPGP community as a whole, and I and my PGP colleagues put Twofish into OpenPGP *independently* of the then-unselected AES. It was thus our vote for it. When Phil, Alan, and I were putting ZRTP together, we put in Twofish as an option (RFC 6189, section 5.1.3). Thus in my opinion, if you know my long-standing opinions on ciphers, this shouldn't be a surprise. I think Twofish is a better algorithm than Rijndael.
ZRTP also has in it an option for using Skein's one-pass MAC instead of HMAC-SHA1. Why? Because we think it's more secure in addition to being a lot faster, which is important in an isochronous protocol. Silent Phone already has Twofish in it, and is already using Skein-MAC.
In Silent Text, we went far more to the "one true ciphersuite" philosophy. I think that Iang's writings on that are brilliant. As a cryptographer, I agree, but as an engineer, I want options. I view those options as a form of preparedness. One True Suite works until that suite is no longer true, and then you're left hanging.
To be fair, there are few options in ZRTP -- it's only AES or Twofish and SHA1-HMAC or Skein-MAC, so the selection matrix is small when compared to OpenPGP. We have One True Elliptic Curve -- P-384, and options for AES-CCM in either 128 or 256 bits and paired with SHA-256 or SHA-512 as hash and HMAC as appropriate. There's a third option, AES-256 paired with Skein/Skein-MAC, which I don't think is in the code, merely defined as a cipher suite. I can't remember. So we have to add Twofish there, but it's in Silent Phone now.
Now let me go back to my comment about standards. Standards are not about what's *best*, they're about what's *agreed*, and part of what's agreed on is that they're good enough. When one is part of a standards regime, one sublimates one's personal opinions to the collective good of the standard. That collective good of the standard is also "security theatre" in the sense that one uses it because it's the thing uses to be part of the community.
I think Twofish is better than AES. I believe that Skein is better than SHA-2. I also believe in the value of standards.
The problem one faces with the BULLRUN documents gives a decision tree. The first question is whether you think they're credible. If you don't think BULLRUN is credible, then there's an easy conclusion -- stay the course. If you think it is credible, then the next decision is whether you think that the NIST standards are flawed, either intentionally or unintentionally; in short, was BULLRUN *successful*. If you think they're flawed, it's easy; you move away from them.
The hard decision is the one that comes next -- I can state it dramatically as "Do you stand with the NSA or not?" which is an obnoxious way to put it, as there are few of us who would say, "Yes, I stand with the NSA." You can phrase less dramatically it as standing with NIST, or even less dramatically as standing with "the standard." You can even state it as whether you believe BULLRUN was successful, or lots of other ways.
Moreover, it's not all-or-nothing. Bernstein and Lange have been arguing that the NIST curves are flawed since before Snowden. Lots of people have been advocating moving to curve 25519. I want a 384-or-better curve because my One True Curve has been P-384.
If I'm going to move away from the NIST/NSA curve (which seems wise), what about everything else? Conveniently, I happen to have alternates for AES and SHA-2 in my back pocket, where they've been *alternates* in my crypto going back years. They're even in part of the software, sublimated to the goodness of the standard. The work is merely pulling them to the forefront and tying a bow around it.
And absolutely, this is an emotional response. It's protest. Intellectually, I believe that AES and SHA2 are not compromised. Emotionally, I am angry and I want to distance myself from even the suggestion that I am standing with the NSA. As Coderman and Iang put it, I want to *signal* my fury. I am so pissed off about this stuff that I don't *care* about baby and bathwater, wheat and chaff, or whatever else. I also want to signal reassurance to the people who use my system that yes, I actually give a damn about this issue.
I am fortunate enough to have a completely good cipher and completely good hash function in my back pocket. So I'm going to use them. If it turns out that there's a good explanation, that BULLRUN is wrong, it's just software. Your situation is different, as is everyone else's. I admire your cool head, but I have to stand over there. I apologize for angering you, but I'm not sorry.
If I'm wrong, I'll have to eat my words. I would rather eat my words in this direction -- moving away -- than the other direction -- standing pat.
cryptography mailing list

@_date: 2013-10-02 22:23:11
@_author: Jon Callas 
@_subject: Re: [cryptography] the spell is broken 
Hash: SHA1
Thank you very much for that assessment.
I'm not implying at all that AES or SHA-2 are broken. If P-384 is broken, I believe the root cause is more that it's old than it was backdoored. But it doesn't matter what I think. This is a trust issue.
A friend of mine offered this analogy -- what if it was leaked that the government replaced all of a vaccine with salt water because some nasty jihadis get vaccinated. This is serious and pretty horrifying.
If you're a responsible doctor, and source your vaccines from the same place, even if you test them yourself you're stuck proving a negative and in a place where stating the negative can look like you're part of the conspiracy.
I see this as a way out of the madness. Yes, it's "marketing" if by marketing you mean non-technical. By pushing this out, we're letting people who believe there's a problem have a reasonable alternative. If we, the crypto community, decide that the P-384+AES+SHA2 cipher suite is just fine, we can walk the decision back. It's just a software change.
Let me also add that I wouldn't fault anyone for deciding differently. We, the crypto community, need to work together with security and respecting each other's decisions even if we make different decisions and do different things. I respect the alternate decision, to stay the course.
cryptography mailing list

@_date: 2013-10-18 16:34:12
@_author: Jon Callas 
@_subject: Re: [Cryptography] /dev/random has issues 
Hash: SHA1
I say yes, this is a fine place, myself. How /dev/random, etc. work is at the core of lots of cryptography, from key generation to the appropriateness of whitening functions to real-world engineering, and at the end of the day, cryptography is an engineering discipline.
The cryptography mailing list

@_date: 2013-10-18 17:11:15
@_author: Jon Callas 
@_subject: Re: Curious RNG stalemate [was: use of cpunks] 
Because people think that over-the-top is necessary.
Perhaps more to the point, people start gilding the lily, and then worrying about how pure the gold is on the lily, and then deciding that the gilt on the lily needs to be mono-atomic and to form a single crystal.
Even more to the point, they start thinking in their heads that they will be criticized for not having a single-crystal structure on the gilt on their lily, and give up. After that, they criticize other people who grow lilies because -- heck, anyone can do that, and years ago, they gave up on lilies because of how hard it is to get mono-crystalline gilt. Go look it up in the cypherpunks archives, for pete's sake. Nicholas Bourbaki discussed it to death there back in '92.
Building a good RNG is both simpler than you think and harder. You need:
* An unguessability source. It doesn't have to be as good as you think it does. If it's crap, you just need more. It just has to be unguessable. The deterministic process going on on my LAN might be good enough. It might not. What matters is the work factor of guessing.
Here's an example of a source I have seen that is plenty good enough, but not what most people think:
- Take an array of unsigned ints; 16 or 32 in length is fine.
- On every interrupt, read the "cpu clock." Lots of CPUs have something good enough. Tick counters, high-speed uptime, etc. It almost doesn't matter.
- XOR that into the current array element. - Rotate that element by an odd number of bits. Mathematically, all that's required is that you pick an odd number that's relatively prime with the word size so you get maximum mixing over time intra-element. Most people can pick a suitable odd number.
- Increment/wrap your element pointer.
Poof, you're done. Incidentally, this is *also* a quantum process, it's just quantized fatter than other quantum processes. That's why you want to do it on every interrupt. When I first saw this, my jaw dropped at its elegance.
* A pool and distiller. Hash functions are great here, as are other things. The thing to remember is only that you might get a megabyte of input that has only a single bit of unguessability in it, and you need to cope. That's why hash functions are great. Entropy estimation is highly desirable, but not necessary. This is a long discussion. It's possible to build one with no estimator that works well, and one with an estimator that works poorly.
* An output function. Ciphers and hash functions are your friends. The SP 800-90 DRBGs are all designed by committee, but work (with the obvious exception). Far more important is to have your output function stir back into the pool. Something as simple as feeding back the length of the request is fine. Even better is to feed in something like a cpu tick counter. If you do something mildly reasonable here in stir-back, then you can completely forget about entropy depletion. This is another long discussion, and that's why I'm simply asserting it. There are gentlepersons who disagree with me. That's it. Yeah, the devil's in the details, and my sketch is kinda like saying oh, all you have to do to land on the moon is get some rockets and life support. But this is a lot easier than landing on the moon, despite many people thinking it's harder.
And yes, yes, there are other other considerations like rebooting, suspend, hibernate, restoring VMs, and of course initial boot.

@_date: 2013-10-17 21:39:01
@_author: Jon Callas 
@_subject: Re: Curious RNG stalemate [was: use of cpunks] 
Be aware in all of this of the Heisenberg-Schdinger Credulity Effect. That effect is that the word "quantum" sucks people's brains out, and otherwise sensible people suffer from impaired reasoning.
It is certainly true that radioactivity is a random effect, and is quantum in nature. That does not mean that in order for a random sampling to be quantum, it must be based on radioactivity; there are other quantum sources of randomness. Noisy diodes, resister noise, CCD noise, etc. are all quantum. If you want to get picky, *all* physical effects are quantum, even ones that aren't usefully random. There is nothing magic about one physical source or other that makes it more suited for crypto. Thinking that a hardware source should be radioactive is affirming the consequence, as well.
Not does it mean that a radioactive (or other) source is suitable for cryptography without some sort of conditioning. Hardware sources are often biased in distribution, or have other numeric flaws that can be fixed with a whitening function.
In short, radioactivity is neither necessary nor sufficient for cryptographic use. If you want to use a source for crypto, you want to run it through a system like /dev/random or at the very least a DRBG to give clean outputs.
Furthermore, what we really want in crypto is what I call "unguessability." This is both weaker than true randomness and stronger. It's stronger in that the numbers have to remain secret. A completely random process that everyone knows is completely unsuitable for crypto, but a weakly entropic input can be jiggered into suitability.
To sum up -- don't get wrapped around the axle about radioactivity. It's not the only random process in the universe, and you have to do a lot of work once you have it. The sort of work that you need to do is precisely what a well-done OSRNG does.

@_date: 2013-12-16 23:28:56
@_author: Jon Callas 
@_subject: Re: Gmail's receiving mostly authenticated email 
Zero. DKIM requires at least a 1024-bit key. Whatever you might want to say about those is a different discussion.
SPF is non-cryptographic authentication.

@_date: 2013-12-13 19:19:05
@_author: Jon Callas 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
Hash: SHA1
Please don't think I am trying to discourage you. You're doing something fantastic here. I'm only trying to give some hints based on my own successes and failures.
The major reason email security has failed is that crypto is easy, user experience is hard. The developments have focused on the crypto, and only then on the UX. Even the best ones fall down on the most important parts of UX, the initial experience.
Every place I have succeeded, it's because we started with the UX and made the crypto work. The places where we let the crypto trump the UX, we failed.
Snowdonia is giving a spur to lots of people to finally get off their asses and do something. However, if they think to themselves, "Well, the NSA isn't after *me*..." then we're back where we were.
The cryptography mailing list

@_date: 2013-12-13 18:27:18
@_author: Jon Callas 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
Hash: SHA1
The stats on my server show that about 1.5% to 2% are encrypted/signed messages. Of that, about 85% are decryption/verify events. My results are obviously going to be more than the population at-large because I'm running an encryption server, but anecdotally, almost all of those are OpenPGP or S/MIME signature verifications.
The cryptography mailing list

@_date: 2013-12-13 00:57:26
@_author: Jon Callas 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
Hash: SHA1
Cool. Laudable goal. Even achievable. We did it at PGP Corporation. I'll add that despite the name "PGP Universal" it did S/MIME, too, and images a public key into both OpenPGP keys and S/MIME certificates.
There are lots of things you can do here. For example, the advantage of S/MIME is that certs are in the messages. You can sniff them out of incoming messages and cache them. So, for example, suppose Alice sends an S/MIME message to Bob. If Charlie then sends Alice a message, you can use that cached cert to get transparent encryption for Charlie.
You can use the convention we did of keys.* to be a domain name for a key/cert server, as well. Our SMTP proxy would go ask the recipient domain for relevant certs and use them. My 2003 "Self-assembling PKI" paper gives the basic rundown of many, many techniques. They work amazingly well.
The cryptography mailing list

@_date: 2013-12-12 23:29:11
@_author: Jon Callas 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
Hash: SHA1
I don't think my answer is lots different than it's been for the last decade.
First of all, people have to actually care. There are lots of systems that are extremely easy to use if people actually care.
For example, there's Enigmail for Thunderbird. But there's also the built-in S/MIME support in many popular systems. I can tell you from experience that the S/MIME support in OS X and iOS effin' rocks. I hear it's good on other platforms, too. The easiest email encryption I've every used is S/MIME on OS X and iOS, modulo a whole bunch of things -- like the out-of-the-box experience, which sucks. (I could mention other issues, but that's a digression.)
However, the whole model is one that weeds out the people who don't actually care. It's not something where someone just checks a box and turns on secure email, you have to prove you're *worthy* of it. For S/MIME, that means getting a cert, for example, which is surprisingly hard. It means renewing certs, which sometimes is amazingly harder. (At one time when I was doing S/MIME things, a certain commercial CA actually would not work if you tried to buy a new cert before your old one expired. Shame on me for wanting to renew. I solved this by buying from a different CA.) It means the way that the software actively discourages self-signed certs or private PKIs. For OpenPGP, this includes the difficulty of generating your keys, figuring out there to publish them, etc. Many of the community keyservers have no way to *delete* keys. I keep getting things sent to me encrypted to keys that I retired in the last millennium.
The bottom line is that the infrastructure for secure email makes it hard, because that's apparently good for you or something. There are plenty of things that make it really simple -- oh, like my aforementioned PGP Universal, excuse me, Symantec Encryption Server. I use that myself and it passes the "My Seventy-Nine Year Old Mother Can Use It" test because in fact, my seventy-nine year-old mother *does* use it. If it weren't for that, I wouldn't do secure email because, well, it's basically so hard that I can't be bothered, either.
The real underlying issue is that the people who are *creating* email security don't care enough to make it easy. The idea seems to be that only those who care enough to jump through the hoops deserve security.
The cryptography mailing list

@_date: 2013-12-12 01:27:07
@_author: Jon Callas 
@_subject: Re: [Cryptography] Size of the PGP userbase? 
Hash: SHA1
You should at the very least look at keyserver.pgp.com, which has very good numbers, because you have to re-verify your key there every six months or it gets deleted.
As Tamzen said, you're going to miss lots of keyservers that are domain-specific keyservers for all the people using PGP Universal (now known as Symantec Ecryption Server, because when you think of email encryption, you think of Symantec). You'll find those because they're (typically) using the hostname "keys.domain.tld" and have an LDAP server. I know there are several million users of that around the net.
The cryptography mailing list

@_date: 2014-01-31 21:13:10
@_author: Jon Callas 
@_subject: Re: [Cryptography] The crypto behind the blackphone 
We *are* using some of the Guardian Project's software. Also software that we're building for Blackphone will be available for other people to use on their own ROMs. And heck, you can go to Github, get the Silent Circle apps and put them on your own device. We're finally to the point that we've QA'ed people who aren't us building them and using them. (And if you can't, it's a bug.)
Let me answer your question with a question.
What's the difference between going to a restaurant as opposed to going to the grocery store and buying a bunch of ingredients and making the same meal? There are groups devoted to making food the way the Child or Keller might. You can't have a meal by Child because she's gone, but you could make a Keller meal as well as Keller's people can. Why go to the restaurant?
Now to comment on that line of both our questions, we all have a set time in this existence and some people might like to write their own compilers so they can write their own software, just as some people grow their own food so they can make their own meals. But some people don't want to do that, and every single one of us trades off the things we want to do against things we're happy to pay other people to do. No offense taken. I may be a smartass, but I like tough questions. If/when they do, I'd love to see it. I don't have time to make an open, secure baseband, but want to include one. The world needs one. Maybe we can arrange some sort of trade.

@_date: 2014-01-31 21:13:10
@_author: Jon Callas 
@_subject: Re: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
We *are* using some of the Guardian Project's software. Also software that we're building for Blackphone will be available for other people to use on their own ROMs. And heck, you can go to Github, get the Silent Circle apps and put them on your own device. We're finally to the point that we've QA'ed people who aren't us building them and using them. (And if you can't, it's a bug.)
Let me answer your question with a question.
What's the difference between going to a restaurant as opposed to going to the grocery store and buying a bunch of ingredients and making the same meal? There are groups devoted to making food the way the Child or Keller might. You can't have a meal by Child because she's gone, but you could make a Keller meal as well as Keller's people can. Why go to the restaurant?
Now to comment on that line of both our questions, we all have a set time in this existence and some people might like to write their own compilers so they can write their own software, just as some people grow their own food so they can make their own meals. But some people don't want to do that, and every single one of us trades off the things we want to do against things we're happy to pay other people to do. No offense taken. I may be a smartass, but I like tough questions. If/when they do, I'd love to see it. I don't have time to make an open, secure baseband, but want to include one. The world needs one. Maybe we can arrange some sort of trade.
The cryptography mailing list

@_date: 2014-01-27 21:28:09
@_author: Jon Callas 
@_subject: Re: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
 nobody is using them today. Some of us find that undesirable and try to do
ides users with everything they need to ensure privacy and control of their=
 communications, along with all the other high-end Smartphone features they=
 have come to expect.=94
o exchange secure messages, that would be easy. Solving the "real problem"
You cut to *precisely* one of my points. Usability is all. We have to have =
things that people will use.
Crypto matters to this in the same way that concrete relates to building st=
rong buildings. It's often a good idea, not the whole solution, and nothing=
 to do with whether people want to live in it.
The cryptography mailing list

@_date: 2014-01-27 21:10:18
@_author: Jon Callas 
@_subject: Re: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
Thank you, very much for the vote of confidence.
But still, I've never said "safe." I've said security-enhanced, and lots of things like that, which are all true, but I'd never say safe -- because I know about the dangers of compromised hardware.
Reporters have to make a living, too, and many of them have written hyperbolic headlines. Well, okay, usually, it's the *editor* who puts the hyperbolic headline on the well-written story.
My truest personal goal for Blackphone is read an Android hardening guide sometime in the future that will give a list of the things you should do to lock down your Android phone, and at the end it will say, "Or you could just buy a Blackphone." I want it to come out of the box the way that serious people like us on this list would want it.
It will also have a set of software and services that people like us would like to have, which is part of the hardening, in my opinion.
It would be very nice to achieve that goal with a V1.0 product, but it would be hubris to suggest that that's going to happen.
(Also -- should there be some ad, web thing, or other communications from Blackphone that says something like it's "safe," mail me off list. I'll get it fixed. That's just another form of software and software comes with bugs, especially in its early days.)
The cryptography mailing list

@_date: 2014-01-27 16:20:31
@_author: Jon Callas 
@_subject: Re: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
I can answer, but I have no idea what your question means. Sorry. No, it's not proprietary, but I have no clue what you mean by "real problem" or the apparent opposition to -- something.
The cryptography mailing list

@_date: 2014-11-12 22:07:36
@_author: Jon Callas 
@_subject: Re: [Cryptography] "DarkHotel" APT routinely breaking RSA512 
One can factor RSA 512 with less than earthly resources. One friend of mine back in 2009 was factoring RSA 512 with a single tower machine in about two weeks. He upgraded the machine in 2011 and could do it in about ten days.

@_date: 2015-02-20 22:36:55
@_author: Jon Callas 
@_subject: Re: [Cryptography] trojans in the firmware 
NAND memory runs faster when the hamming weight of the data is approximately even between zeroes and ones. You can speed up NAND flash by running the data through a suitable whitening function.
AES is a great whitening function. If you then go to the extra effort to do key management, you have security. It's a simple matter of architecture and programming. :)
freebsd-security mailing list
To unsubscribe, send any mail to "freebsd-security-unsubscribe

@_date: 2015-02-20 22:36:55
@_author: Jon Callas 
@_subject: Re: [Cryptography] trojans in the firmware 
NAND memory runs faster when the hamming weight of the data is approximately even between zeroes and ones. You can speed up NAND flash by running the data through a suitable whitening function.
AES is a great whitening function. If you then go to the extra effort to do key management, you have security. It's a simple matter of architecture and programming. :)

@_date: 2015-02-20 22:36:55
@_author: Jon Callas 
@_subject: Re: [Cryptography] trojans in the firmware 
Hash: SHA256
NAND memory runs faster when the hamming weight of the data is approximately even between zeroes and ones. You can speed up NAND flash by running the data through a suitable whitening function.
AES is a great whitening function. If you then go to the extra effort to do key management, you have security. It's a simple matter of architecture and programming. :)
The cryptography mailing list

@_date: 2015-05-28 05:01:13
@_author: Jon Callas 
@_subject: Re: Apple At-Rest Encryption 
You should turn it on. The battery effect on the CPU is negligible; its using AES-NI in the processor and thats running at less than one clock per byte. But if youre on a computer that has flash  like any of the Air/Retina machines  the write time and power requirements of NAND flash are much better when you use a whitening function, of which AES makes a great one.
But in any event, its all going to be not worth worrying about in the costs. You might even benefit. You are also gaining in the security end. We can certainly debate whatever the operational security benefits are from encrypting your disk, but the real benefit comes from when you inevitably decommission that machine and storage. You are vastly, vastly better off with encrypted storage then, and better off for having encrypted it all along.
