
@_date: 2013-09-09 05:18:18
@_author: David Johnston 
@_subject: Re: [cryptography] urandom vs random 
I like the Markov-Renye test described in the current SP800-90B draft. It handles biased, correlated, non stationary data conservatively and gives you a min-entropy estimate.
cryptography mailing list

@_date: 2013-09-09 05:15:54
@_author: David Johnston 
@_subject: Re: [cryptography] urandom vs random 
I annoyed a cryptographer by calling it the cascade construction RNG in Intel documentation for RdRand.
It appears to be sticking in some places.
cryptography mailing list

@_date: 2013-09-09 04:57:43
@_author: David Johnston 
@_subject: Re: [cryptography] urandom vs random 
I've argued in private (and now here) that a large entropy pool is a natural response to entropy famine and uneven supply, just like a large grain depot guards against food shortages and uneven supply.
If you've got lots of good quality random data available, you don't need a large state. You can just stir lots on raw data into a small state and the small state will become fully entropic. The natural size for the state shrinks to the block size of the crypto function being used for entropy extraction. Once the value is formed and fully entropic, you spit it out and start again.
This is one of the things that drove the design decisions in the RdRand DRNG. With 2.5Gbps of 95% entropic data, there is no value in stirring the data into a huge pool (E.G. like Linux) so that you can live off that pool for a long time, even though the user isn't wiggling the mouse or whatever. There will be more random data along in under 300ps, so prepare another 256 bits from a few thousand raw bits and reseed.
A consequence of Linux having a big pool is that the stirring algorithm is expensive because it has to operate over a many bits. So an LFSR is used because it's cheaper than a hash or MAC. An LFSR may be a good entropy extractor, but I don't know of any math that shows that to be the case. We do have that math for hashes, CBC-MACs and various GF arithmetic methods.
When I count my raw data in bits per second, rather than gigabits per second, I am of course going to use them efficiently and mix up a large pot of state, so I can get maximum utility. With the RdRand DRNG, the bus is the limiting factor, not the supply or the pool size.
cryptography mailing list

@_date: 2013-09-07 19:08:20
@_author: David Johnston 
@_subject: Re: [cryptography] Random number generation influenced, HW RNG 
It's SP800-90A, B & C (but B & C are draft).
cryptography mailing list

@_date: 2013-09-07 17:48:02
@_author: David Johnston 
@_subject: Re: [cryptography] Random number generation influenced, HW RNG 
It interesting to consider the possibilities of corruption and deception that may exist in product design. It's a lot more alarming when it's your own design that is being accused of having been backdoored. Claiming the NSA colluded with intel to backdoor RdRand is also to accuse me personally of having colluded with the NSA in producing a subverted design. I did not.
A quick googling revealed many such instances of statements to this effect, strewn across the internet, based on inferences from the Snowden leaks and resulting Guardian and NYT articles.
I personally know it not to be true and from my perspective, the effort we went to improve computer security by making secure random numbers available and ubiquitous in a low attack-surface model is now being undermined by speculation that would lead people to use less available, less secure RNGs. This I expect would serve the needs of the NSA well.
cryptography mailing list

@_date: 2013-09-09 15:11:16
@_author: David Johnston 
@_subject: Re: [cryptography] [liberationtech] Random number generation being influenced - rumors 
It's not 'very hard'. Just suppress long strings a bit.
Just because the entropy source is real doesn't mean it's feeding the So this is both wrong and moot.
cryptography mailing list

@_date: 2013-09-09 04:26:24
@_author: David Johnston 
@_subject: Re: [cryptography] [liberationtech] Random number generation being influenced - rumors 
I answered this once before many months ago, the last time you asked.
There is no 'whitener'. It is a CBC-MAC based entropy extractor, as per the spec in the current SP800-90B draft. You can call it a whitener, but that would risk confusing it with things like the Von Neumann or Yuval Peres whiteners, which are a different class of algorithm with different The reasons for it are:
 Maintaining a strong security boundary. We don't want an attacker to be able to infer the seed values by exposing them to all sorts of classes of attack by letting the values get into the system state accessible by the microprocessor SW.
 FIPS compliance. Which is more or less  restated. It wants stuff to happen within a well defined boundary.
 Robust engineering. Our goal is to make the lack-of-entropy problem go away on intel based products. Reseeding the DRBG 2 million times a second is a good way of making it hard to infer the state of the DRBG. This is one of several stages of mitigation design, intended to make the DRBG robust even if a problem should arise with any one of the stages. You can't do that in software. In general, once attackers have got a foot in the door of a software system, it is game over.
 Software solutions have been a demonstrable failure. At least this hardware solution remains robust.
 A non-goal is making James A Donald satisfied. Sorry, but no solution compatible with security and manufacturing realities is going to satisfy the demands you have made.
cryptography mailing list

@_date: 2013-10-17 20:56:31
@_author: unknown_name 
@_subject: Re: [Cryptography] /dev/random is not robust 
Yes. So they don't release low entropy numbers.
But hardware should provide a firehose throughput entropy source from
power on so the RNG is always initialized to full entropy and won't block
regardless of the load on the random number service.
The cryptography mailing list

@_date: 2013-10-17 20:36:11
@_author: unknown_name 
@_subject: Re: [Cryptography] /dev/random is not robust 
Don't confuse the SP800-90 requirements for continuous testing and rest of
the necessary chip testing that all chips need. FIPS 140-2 imposes limits
on what passes through a FIPS boundary. This means that it is way easier
to do your chip testing using logic BIST contained within the FIPS
boundary than it is to try and pull full chip test through FIPS. You also
need the SP800-90 continuous tests implemented within the FIPS boundary.
You need to test your chips. FIPS forces you to use BIST.
You need to monitor your sources. SP800-90 forces you to use a particular
type of continuous monitoring.
These are quite separate things in type and style, but you have to do both.
In a FIPS compliant system with a software SP800-90 DRBG, I assume that
the FIPS boundary almost never coincides with the boundary of the SP800-90
implementation, so the rules and driving issues are quite different.
This has almost nothing to do with the Linux kernel random service which
does it's own thing very differently to NISTy RNGs.
The cryptography mailing list

@_date: 2013-10-31 21:43:54
@_author: which can come from anywhere 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
And what are we supposed to do with 4.9.2 of FIPS 140-2? That reduces the
output entropy to considerably less than (1-epsilon) | epsilon < 1/2^64,
as required by SP800-90A.
If every there was a clause that I though was inserted to weaken an RNG
spec, that would be it.
Please re-open FIPS 140-2 for comment.
"If each call to a RNG produces blocks of n bits (where n > 15), the first
n-bit block generated after power-up, initialization, or reset shall not
be used, but shall be saved for comparison with the next n-bit block to be
generated. Each subsequent generation of an n-bit block shall be compared
with the previously generated block. The test shall fail if any two
compared n-bit blocks are equal.
The cryptography mailing list

@_date: 2013-10-31 20:01:24
@_author: which can come from anywhere 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
It's not a misunderstanding. It's right there in section 4 of FIPS 140-2.
FIPS 140-2 and SP800-90 are fundamentally incompatible.  It leads directly
to demands from the certification houses that cannot be met. Similarly I
can't get CTR_DRBG vectors out of a test lab that don't include additional
entropy. They read it as mandatory due to ambiguous text in the spec. The
situation for certifying a coincident boundary FIPS 140-2, SP800-90
circuit is thoroughly messed up in ways that compromise security.
I've detailed the details in my comments against SP800-90A, which I'll be
sending off in the next couple of days.
I'm not persuaded by the angst about malicious input. The text in SP800-90
around defining the 'consuming application' is just fine. The consuming
application can give the nonces, personalization strings and additional
entropy necessary to secure its own instance. If you trust the other guy
to supply your personalization string for you, good luck with that.
But it's FIPS 140-2 that prevents an unauthenticated consuming application
putting in the material necessary to get the random numbers necessary to
perform a secure authentication. A chicken and egg problem if ever there
was one.
The cryptography mailing list

@_date: 2013-10-30 04:09:57
@_author: David Johnston 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
This is a multi-part message in MIME format.
 boundary="------------090200030706010402050308"
This is a multi-part message in MIME format.

@_date: 2013-10-28 21:28:11
@_author: SP800-90x & FIPS 140-2 
@_subject: Re: [Cryptography] [RNG] /dev/random initialisation 
But the specifications (SP800-90x & FIPS 140-2) make it spectacularly hard
to mix in multiple sources in a compliant way. SP800-90 gives a way to mix
in "additional entropy" and "personalization strings", but FIPS 140-2
states that all sources must be authenticated. All configuring entities
must be authenticated. Try authenticating hardware on one end of chip
against hardware at the other end of the chip. It is the mother of all
chicken and egg problems.
The 'per instance' nature of the additional entropy data provided during
the SP800-90A instantiation algorithm is entirely incompatible with
hardware implementations that have fixed state.
The solution is to not use or permit any of these extraneous inputs and
don't permit instantiation other than at reset time, then you don't need
to go about 'authenticating' the caller, whatever that means in the
context of one circuit on a chip talking to another circuit on a chip.
Just so you can take it in for FIPS certification.
SP800-90A defines a reseed as a callable function and defines the caller
as being the thing that provides the fresh entropy. FIPS 40-2 requires
that the caller be authenticated. That's what I call a mess.
If NIST want people to make compliant paranoia RNGs, that accept all
sources, authenticated or not, and mix them, then that is what the specs
should say, but they do not. All sources have to have been designed in
ahead of time. All entities it interacts with have to have been
provisioned with authentication credentials.
So the 'mix everything in' RNGs give one model of security (Only 1 of N
sources need be good). The NIST RNGs give another (We designed the one
true source to be good). But there is no overlap. One cannot be both a
NISTesque RNG and a 'mix everything in' RNG.
I have heard this complaint from multiple people on multiple projects -
paraphrasing: "We had to make our RNG less secure to get it through FIPS -
We had to take out the additional mix in sources".
The cryptography mailing list

@_date: 2013-11-08 21:01:58
@_author: David Johnston 
@_subject: Re: [Cryptography] randomness +- entropy 
This is a multi-part message in MIME format.
 boundary="------------000403010809040407090108"
This is a multi-part message in MIME format.

@_date: 2014-07-03 03:17:36
@_author: David Johnston 
@_subject: Re: [cryptography] Stealthy Dopant-Level Hardware Trojans 
This is a multi-part message in MIME format.
 boundary="------------050404080307060400040501"
This is a multi-part message in MIME format.

@_date: 2014-10-23 17:43:49
@_author: unknown_name 
@_subject: Re: [Cryptography] Best internet crypto clock 
Remote sources require external inputs.
Transistors have plenty of thermal noise in their gates. It's local, well
understood and can be modeled for min-entropy analysis over all
environmental conditions and attack scenarios.
You just need know how to get at it in a robust way. We published two
different circuits that do this. The one for RdRand that's been repeated
in numerous papers (like this:
 and
the other is this one:
There are many gigabits/s of data you can get out of a transistor with a
high entropy distribution. You can be reasonably confident that the noise
in the transistor gate is the aggregate signal from many quantum events in
the particles out of which it is constructed.
On silicon, the circuits are small. You may be able to do something
similar with discrete components, albeit at lower speed.
The cryptography mailing list

@_date: 2014-12-18 23:29:06
@_author: unknown_name 
@_subject: Re: [Cryptography] Fwd: 78716A 
'Overly broad' is not the limiting factor.
I made an FOIA request that they tell me where I've traveled (I don't have
records, but I know the government does and they want me to fill in where
I've been on a form).
It couldn't be less broad.
I'm still waiting for a response 6 months later.
The cryptography mailing list

@_date: 2015-02-18 06:53:47
@_author: David Johnston 
@_subject: Re: [Cryptography] trojans in the firmware 
I've designed many embedded computers using directly memory mapped flash devices. It was how it was always done before people started trying to make them look like hard disks at the interface.
There is a direct analogue with hard disks of old, that the OS directly controlled. The OS knew where the tracks and sectors were and where the head was and could therefore do things like head trajectory optimization. When disks got smart and started trying to look like a perfect array of sectors while hiding the physical layout, the opportunity for the OS to do the right thing was removed. So we ended up with ludicrous things like cache memory in the disk drive, rather than close to the CPU, because the CPU doesn't have the information necessary to manage the hardware.
So the management software in flash disks is a lot more complex than the incremental write and leveling algorithms that were commonplace with directly mapped flash. Not because it needs to be, but because it is trying to present a model at the interface that is very different to the underlying medium.
You can (or you certainly could in the past) get PCI cards with memory mapped flash on them, if you aren't in a situation to make you own circuit boards.
Flash files systems were commonplace. They are still there in the Linux source code and I assume they are used in many products.
The cryptography mailing list
