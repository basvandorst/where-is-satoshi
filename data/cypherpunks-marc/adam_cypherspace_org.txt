
@_date: 2001-09-20 23:56:29
@_author: Adam Back 
@_subject: Re: a libertarian approach to airport security: suggestions 
Well I guess a little rational thought might not hurt as the motivator
of policy rather than public perception, and hidden agendas to
increase state power and outlawing unrelated technologies they'd like
to outlaw anyway (eg privacy and encryption software to increase the
scope of open source signals intelligence.)
Identification doesn't appear to be that relevant to hardening
I saw some inanely stupid arguments on news over the last week about
policies and procedures along the lines of 'if our policy was
different this wouldn't have happend', the perpetrators knew the
policies in effect and planned their operation around them.  The world
can spend billions re-jigging policies and procedures, but few of them
even seem like speed bumps, and most I've seen proposed look like they
would have no effect at even complicating further attacks.  If one presumes the motives of the politicians are sincere they are
idiots -- as chess players go they aren't even able to think one move
ahead -- aren't able to ask the question of themselves: "what would
the opponent do if this candidate policy were in place?"
It's pretty impossible to harden a country with such low population
density and number of fat targets.  The attacker will simply attack
the target with the best trade-off of political value, destructive
value and ease of attack.  You can't pre-empt this stuff.
Also as well as the obvious fact that people of arabic descent are
rather a broad profile to single out for extra scrutiny, it seems on
reading more of the background of the region that there are
fundamentalist muslims in Afghanistan from different ethnic
backgrounds: Egyptians, Chinese, Africans, and perhaps even the odd
Russian.  It's pretty much a "doh" that scrutinizing arabic looking
people, will lead the would-be attacker to select suicide attackers
with different ethnic appearance.
A much better approach to aggresively and honestly persue would be to
attempt to improve relations with the minorities who are feeling
persecuted directly by previous US military and political actions and
by indirect actions in sponsoring, funding, training: Iraq, Iran,
Saudi Taleban, Israel etc, etc at various points in the past and
It seems that world stability is more likely to be achieved by
diplomacy than by engaging in tit-for-tat escalations of violence.
Guerilla tactics make it impossibly expensive to harden a country
against such attacks.
Whereas one might normally attribute actions with apparently the
opposite effect of the claimed intent to stupidity ("never attribute
to malice that which can be explained by stupidity"), I suspect that
the real reason for the disparity is that the stated intent is not the
real intent.  The real intents are probably economic, and the actions
planned inline with economic analysis and forcasts, though not the
interests of world stability.  This can't be explained openly to the
public as they would reject the strategies.

@_date: 2001-09-13 18:04:41
@_author: Adam Back 
@_subject: escalation not the answer (Re: where do we go from here (and where should we have gone)) 
This has little to do with US middle east policies.
I understand people are emotional at this point, but to be living
under the impression that the US has no part in the escalation is
ignorant of history.  The fundamentalists behind these terrorist
attacks are justifying their actions on the basis of US sponsorship of
Israel which in their view is terrorising Palestinians, and various US
interentionist military acts -- missile attacks, assassination
attempts, blanket bombings etc.  They may or may not have other
agendas, but these historic grievances are what allow them to gain
It seems likely that we will see an escalation:
- Israel and Palestine conflict, Israel funded by US, both sides have
  greivances in their escalation of violence
- previous WTC bombing, and US reactions to that
if both sides continue to react with increasing violence where does it
I'm not saying there is an easy answer, but escalation seems unlikely
to help long term political stability.  Unfocused escalation "no
distinction between perpetrators and harborers" it would seem will be
likely to create more victims, who have had family members killed who
were bystanders and previously neutral or antagonistic to the
perpetrators who will then be fodder for future supporters of that
currently small minority of fundamentalist islam calling for jihad on
the US.
Pre-emptive strikes if any should be focused on military targets only
not for the purpose of revenge but for the purpose of reducing the
chance of further attacks.  If a strike would in fact increase chance
of further attacks and contribute to further escalation, I think it
would be a bad idea, and instead attempts should be made to suppress
the current cycle of escalations.

@_date: 2001-10-15 15:14:49
@_author: Adam Back 
@_subject: Re: Schneier on Stego, Dead Drops, bin Laden 
Bruce writes about uses of steganography as digital dead drops.
But he also claims that there are no business uses for steganography.
I don't think this claim is valid.
There are business scenarios where traffic analysis can leak
information about potential mergers, investment analysis activity and
so on.
Steganography is just a valid mechanism to hide traffic as cover
traffic.  Stego in fact offers marginally higher security against
traffic analysis because it will not be evident that the two parties
exchanged information, nor even had the opportunity to.  The
opportunity to have communicated would be evident if they were using
just cover traffic.
Apart from business uses there are uses for civil rights workers, and
generally members of the public who choose to retain association
I don't think we should be giving the press and government ammunition
in their arguments to ban various forms of crypto, especially for
forms of communication which may help civil rights workers, and which
infringe on the tools available to the individual to partially regain
his privacy be that confidentiality or of association.

@_date: 2001-10-13 13:57:25
@_author: Adam Back 
@_subject: Re: RIAA Safeweb Proxy ID 
A carnivore box at Safewb would work also for tying browsing habits to
users.  Think Safeweb with their CIA ties would balk at installing
one?  Do they already have one installed?

@_date: 2001-10-19 23:41:27
@_author: Adam Back 
@_subject: Re: What info does Zero Knowledge collect on users of Freedom 
Let me try give some more details behind this.  The idea was to create
separate modules that can be separately shipped and sold.  Freedom 3.0
"privacy & security tools" is the first of those.  It has a subset of
the functions in freedom 2.2 (cookie management etc), but some of
those functions have new features or have been improved.
Declan's interivew with Austin and Ian's comments on slashdot and here
cover the aspect of the story that the current freedom network which
was the support infrastructure behind the anonymous browsing and mail
part of Freedom 2.2 was decomissioned.
Other consumer software related plans are not public.

@_date: 2001-11-28 23:23:18
@_author: Adam Back 
@_subject: zks freedom websecure trial 
I noticed some discussion of the SafeWeb cancellation of free
services here.
ZKS announced yesterday freedom websecure, which is an anonymous web
browsing system with more robust redirection and script blocking than
systems that rely on html re-writing.  There is a free trial offered
for a couple of months.
Unfortunately it only works as shipped with IE on windows in this
(The local browser plugin is the point where srcipting, java etc are
disabled, and traffic is encrypted (with SSL) and directed to a

@_date: 2001-11-28 21:47:22
@_author: Adam Back 
@_subject: Re: pipenet padding 
There is some discussion of pipenet and freedom attacks in:
"Traffic Analysis Attacks and Trade-Offs in Anonymity Providing Systems"
Adam Back, Ulf Moeller, and Anton Stiglic

@_date: 2001-12-01 01:24:58
@_author: Adam Back 
@_subject: Re: Moving beyond "Reputation"--the Market View of Reality 
I think you'd want to have a regular public key change, where the user
creates a new public key, signs it with their old key.  This way key
change does not suggest possible ownership change.
Then to effect a nym transfer, the new public key is instead chosen by
the new owner, and the old owner signs it.
This still leaves the problem of the old nym revealing a previously
unpublished identity revocation, or simply some signed statements
which are damaging to the nyms reputation.  Some ideas on this:
- introduce a time-stamping service for certification signatures only
  (explicitly not for documents)
- re-define valid certificate signatures to be signatures made on
  public key and identity pairs within the validity period of a key
- publish signature private keys after expiry.
Now anyone can create nym revocation with old keys, and so no
reputational damage can be done by the old owner with nym revocations.
The time-stamper will not assist in signing arbitrary documents, and
so the old nym owner can not use it to prove a document was signed
during the validity period of the key.  (Were it signed after the
validity period it would anyway not be considered a valid signature).
Where general document signing time-stamping is used, to prevent
post-sale nym reputation suicide, the new owner could demand all past
signed messages and verify them against the merkle hash tree master
hash maintained by the time-stamper, and vet the messages as not
damaging to the nyms reputation.
For private encrypted messages the nym would not like to share, even
after nym sale, a non-transferable signature scheme could be used,
with the time-stamper having a separate hash-tree for such signatures.
The new owner would be somewhat assured that the old owner could not
have any signatures that he can both prove the time of authorship of
and transfer.  The availability of third party time-stamping services, and other
simple methods of dating documents (pre-published hash, 3rd party
vouching for time of authorship) limits the assurances provided by the
above approaches.
Another avenue might be designated verifier signatures where the
signature sheme necessarily requires collaboration of a verifier, and
the verifier will take instructions from the current owner.
Or better where only the current owner in collaboration with the
designated verifier can assist a user in verifying a signature.  (eg
using pro-active security to re-split the key on nym sale so the old
owner isn't able to collaborate in verification).
In this way the new owner gets to vet which signatures the public can

@_date: 2002-02-28 13:32:29
@_author: Adam Back 
@_subject: InfoSpace buys ecash technologies assetss 
Followers of the saga of digicash may be interested to hear the latest
in the train of events: InfoSpace (INSP) bough the patents from ecash
technologies which had previously bought the patents from digicash
after it filed for chapter 11 bankruptcy protection.
I didn't see any mention of anonymity of financial privacy in the
press release.

@_date: 2002-02-15 00:33:05
@_author: Adam Back 
@_subject: Re: Preventing double spending 
There are two general approaches to ecash protocols, online and
offline.  This applies to Chaum's protocol which has:
- an online version where the payee has to deposit the coin at the
  time he receives it to guarantee it hasn't been double spent.
- an offline version where the payee deposits some time later, but is
  assured that if the coin is double spent, the bank will learn the
  identity of the person who withdrew the coin.  It's too late to
  prevent the double spending as the payees are not online with each
  other.  The withdrawl protocol embeds the identity of the withdrawer
  in the coin such that if it is spent twice two equations are
  revealed which can be solved to compute the withdrawers identity.
That could be a problem for example with software bug resulting in
accidental double spending.  The bank wouldn't be able to distinguish
necessarily between that and intentional double spending.
Well the DigiCash technology, and I presume still Ecash Technologies
which bought the patents and has/had some description of a product
suite on it's web page is the same, were both using the online
In fact the basic offline Chaum protocol is quite inefficient as it
uses cut-and-choose, though see [1] for a more efficient variant.
Tying the withdrawal (and deposit) to a bank account was an arbitrary
policy / business decision not related to the protocols.
So it would be quite possible with online Chaum e-cash to have
accountless operation.  Similarly I quite agree with your comments on
financial privacy of not wanting necessarily to reveal the volume of
withdrawals and deposits, and it is also quite possible to have
accountless deposit (you just get fresh anonymous cash for your
deposited cash).
So the low tech approach to your question is to use the online
protocols which removes the need for identity in the withdrawal
You could make the coin contain a serial number which is the hash of a
random number.  If you have a coin you were given you will know the
preimage, the bank will not.  If they claim it's spent but can't show
you the preimage, they're lying.
Probably you could do that with Brands' ecash protocol, it's much more
flexible than Chaum's protocols.
One approach that might work is to construct a trap-door in the
unlinkability of the coins.  Then when a coin double spent the
trap-door key is revealed and you could link the remaining coins.  The
tricky part is you would have to convince the bank that the coins you
submitted had this trap-door property with a zero-knowledge proof.
Brands' coins can have multiple attributes which can have quite
arbitrary values.  You show the bank the attributes during withdrawal,
but the resulting secret-key certificate is blinded and so unlinkable.
When a coin is shown normally (spent) a user selected subset of the
attributes can be revealed.When a coin is double spent all of the
attributes are revealed.
Also the offline version of Brands' ecash has an efficient withdrawal
protocol compared to Chaum's cut and choose.
There are a number of interesting degrees of flexibility with Brands'
protocols.  If you're interested to explore do I'd recommend reading
[2] which I find more accessible than his thesis book.
Anyone interested in applications of ecash or credentials and has some
reasonable understanding of how Chaum's ecash and credentials work
would do well to read that paper to gain an insight into the kinds of
things that are possible with Brands private credentials.
Disclaimer: personal opinions only of course
[1] Niels Ferguson "Single Term Off-Line Coins"
    [2] "A technical overview paper titled 'A Technical Introduction to
    Private Credentials' has been submitted to the International
    Journal on Information Security, on invitation by its editor, Moti
    Yung. This 34-page paper assumes that the reader has an
    understanding of basic mathematical concepts in public-key
    cryptography, in particular as related to the Discrete Logarithm
    problem in groups of prime order."

@_date: 2002-02-18 11:58:23
@_author: Adam Back 
@_subject: p2p and asymmetric bandwidth (Re: Fear and Futility at CodeCon) 
I think the asymmetric up/down speed is not as much a problem for
peer2peer as anonymous fears.  Morpheus has demonstrated that the
approach of having a single request served by multiple servers works
well.  A cable modem users download speed can be merrily supplied by
dozens of even dialup, or other cable modem users thin pipe uploads
Morpheus seems to be able to tunnell through even to corporate
firewalls with the approach (I presume) that the firewalled /
unreachable host maintains a connection to the super-node, and when
someone wants to connect to it and can't they connect to the
super-node and the super-node tells the unreachable node over the
already open connection to connect back to the connecting machine.
Of course this can't work (without moving data via the super-node)
between two unreachable machines, but the balance seems to be
sufficiently in favor of reacable machines that I don't see it
currently presents a problem.

@_date: 2002-03-27 14:10:05
@_author: Adam Back 
@_subject: mixmaster upgrades? (Re: 1024-bit RSA keys in danger of compromise) 
I think it wouldn't hurt to use 2048 bit RSA keys for anything that
supports them.  I've been using 2048 bit RSA keys with PGP since 1995
based on the assumption even given uncertainty about the future of
factoring that double the key size can't hurt, and didn't make any
significant difference to message processing time.
Mixmaster is an example of an application which could benefit from
larger key sizes, given the presumed long-term assurances one would
like about it's anonymity.  There was some discussion a while ago
about a candidate mixmaster version 3 protocol:
I made some comments at the time about a way to reduce the space
overhead of using RSA:
by reusing some of the space inside the RSA encrypted message to
transport part of the chained encrypted message as well as the
symmetric keys.  I think this would allow 2048 bit keys without
increasing the already 50% overhead of key-exchange to message with
mixmaster.  (10k for each).
The other thing mixmaster really needs is forward secrecy, ideally
end-to-end forward secrecy, but hop-by-hop forward secrecy would be a
start.  Lack of forward-secrecy leaves remailer operators open to a
fair risk of subpoena attack if someone went to the trouble of having
an ISP record the incoming messages.
The other current weak point is DSA signature key sizes maxing out at
1024 bits due to the SHA1 hash output size.  I presume that in due
course NIST will make an extended DSA to go with the extended SHA1
(SHA-256, SHA-384 and SHA-512).
But signatures key strengths aren't so important for forward secrecy
as encryption key strengths; you only have to be convinced that
current adversaries can't forge them given the current signature size
you're using.  If at some point in the future after you've upgraded
your key sets to larger signature keys, it's not as significant if
someone can go back and forge old small key signatures.

@_date: 2002-03-26 14:29:19
@_author: Adam Back 
@_subject: ciphersaber-2 human memorable test vectors 
A while ago I wrote some code to search for human readable test
vectors for Arnold Reinhold's ciphersaber-2
(  Ciphersaber-2 is designed to be simple enough to be implemented from
memory, to avoid the risk of being caught with crypto software on your
computer for use in regimes which have outlawed encryption.  But the
only available test vectors are a big string of hex digits which
probably the implementor will find difficult to remember, and it is
quite easy to make implementation mistakes implementing ciphers -- and
the risks of accidentally using a incorrectly implemented and weak
variant of ciphersaber-2 are significant.  It would be useful
therefore for the stated purpose of ciphersaber-2 to have human
readable and memorable test vectors.
The software for exploring for human readable test vector phrases and
information on using it is here:
I have not myself spent much time looking for satisfyingly witty,
topical and memorable phrases. I'll appoint Arnold as the judge and
let you the reader see what you can come up with using the
software. The winning test-vector phrases will go here. Perhaps Arnold
will make some honorary 2nd level Cipher Knighthood and certificate
for producing the coolest phrase which is also a ciphersaber-2 test
By way of example the following is a ciphertext plaintext pair:
% csvec -c2 db 3 4 3 5 3 spook.lines
selected 155 words of length [3-5]
k="AMME",iv="spy fraud ": bce DES
which is interpreted as:
ciphertext = "spy fraud DES"
plaintext = "bce"
key = "AMME"
(the iv is prepended to the ciphertext)
and can be checked as:
% echo -n "spy fraud DES" | cs2.pl -d -k="AMME"
and so on.

@_date: 2002-03-24 00:40:05
@_author: Adam Back 
@_subject: the mail2news hashcash experiment (Re: the anti-s**mmer and anti-flooder arms-races) 
The only web description about the mail2news experiment seems to be
this one:
there was/is a fair bit of discussion about it in
news:///alt.privacy.anon-server but it's horribly flooded so quite
hard to find with a news reader; also a lot of the discussion is
political about the claimed economic unfairness against poorer posters
of imposing a cost function related to the speed and hence cost of a
persons computer; probably you'd do better off with groups.google.com
and a search term of "alt.privacy.anon-server hashcash" like this:
google rocks.

@_date: 2002-03-23 12:48:18
@_author: Adam Back 
@_subject: merkle authentication trees (Re: Resources discussing secure time (nonce) in a distributed environme 
I think Merkle authentication trees allow you to do this, if you don't
care about specific time, but just about the ordering of events.  Most
of the time-stamping services are based on this, where they publish a
daily master hash somewhere.
I can't seem to find an online copy of the Merkle paper as it's quite
old.  The patent has expired.
A Merkle Authentication Tree is basically a tree of hashes to allow
the efficient verification of a particular leaf in the tree with
log(n) queries. Imagine a binary tree representing a set of events
with names as the leaf nodes. The parent of a leaf node is the hash of
the pair of events below it. The parent of that node with it's
neighbor is the hash of it and the neighbor hash, and so on up to the
root which is the master hash of all the hash of hashes of events down
to the leaves.  The time-stamping service application just publishes
on a daily basis the master hash.  Each days master hash should also
include the previous days.  Now even if the time-stamping service were
corrupt it can not after the fact create a stamp in a previous days
epoch without being caught.

@_date: 2002-03-22 18:48:43
@_author: Adam Back 
@_subject: cp-moderated archive 
I filled in the (semi?-)automated online archive for
cypherpunks-moderated at:
I presume in due course it will start archiving at:
It seems to be already archiving (separately and multiply) each of the
other cypherpunks nodes, plus cryptography and a number of other

@_date: 2002-03-21 17:47:05
@_author: Adam Back 
@_subject: Re: moderated CDR node 
Well I just subscribed to cypherpunks-moderated and
unsubbed from the node I was previously on.
We'll see how this works -- no more Choate one-liners and fall out has
got to be an improvement, and the filtering policy sounds reasonable
to me.
I'd encourage anyone else to try it also, so that the moderators
efforts are put to good use.

@_date: 2002-03-19 01:30:13
@_author: Adam Back 
@_subject: (old note contd.) lotus-notes NSA key as PGP key 
I was looking for a file in my collection of archived stuff recently
and came across my attempts to reverse engineer the NSA's RSA public
key out of lotus notes.  I think I never did publicly post the RSA key
that I found.
So here it is as a PGP key, the name associated with this key in Lotus
Notes visible under the debugger was:
where O is X.500 naming for Organization, and CN for Common Name (the
key owners name).
The PGP key is:
Type Bits/KeyID    Date       User ID
pub   760/13629D8D 1998/10/25 Director, NSA -----BEGIN PGP PUBLIC KEY BLOCK-----
Version: 2.6.3i
-----END PGP PUBLIC KEY BLOCK-----
It's a 760 bit RSA key with a public exponent of 3.
I found it a little odd that it was 760 bits rather than 768 bits, but
I think I got the endianness and encoding right as the number is not
trivially factorizable (I left a computer running pollard-rho for a
few weeks at the time and didn't come up with anything).  One possible
explanation for 760 bits rather than 768 bits is the 768 bit 32 bit
aligned area of memory ended with with a 0 byte, and ASN.1 encoding
for big integers is to include a leading 0 if the most significant bit
of the number is otherwise a 1 (to prevent it being considered a
negative number).  I know it's not prime as it fails primality checks,
but I think it's fairly unlikely is that a randomly chosen number (if
there is a mistake in the reverse engineering or interpretation of the
encoding) would be both composite and that hard to factor.
More details about the key at:
(A quick google shows that this was probably originally reported
around Sep 99.)
I wonder how many copies of export versions of lotus notes and
similarly us export weakened products are still being used unknowingly
by users.

@_date: 2002-03-17 23:03:52
@_author: Adam Back 
@_subject: known-plaintext and file systems (Re: microsoft - "convergent 
I think convergent-encryption would only be resistant to partial
known-plaintext, if there were more than 128 bits of data to guess.
It would be broken if you know the whole plaintext or almost all as
then you know the key by definition, or could guess the remainder of
the file to get the key.
Problem is as I commented in later followup to my own mail, quite
often the plaintext of the file is not at all secret, and so the
system can be brute forced by trying hashes of all known public files.
Even for user created files, for short files it may be the case that
there is less than 128 bits of entropy in the file.
The use of random IVs and chaining modes (CFB, CBC) is normally used
to avoid known plaintext header problems.  However doing encryption in
filesystems has a number of extra complexities.  Sometimes the drivers
can't cope with message expansion, or don't want to for efficiency or
reliability reasons.  ECB avoids expansion, but is bad generally as
the attacker can build up a code-book and recognize equal cipher-block
sized chunks of files throught-out the file system.  But you can
generally use or derive IVs per chunk of file that must be encrypted
and decrypted and use CBC.
Another fun risk for file system encryption is that journaling and
on-the-fly defragmentation tends to leave scattered around the disk
copies of old ciphertext, and so modes with output feedback (CFB, OFB)
have the remaining risk that you expose the xor of two plaintexts ala
re-using keys with RC4 if the IV does not change as the data is moved
by the journaling (OFB is worst and really is equal to the re-used IV
picture, CFB you'd "only" get the xor of the first differing
ciphertext block pair's corresponding plaintexts.)
EFS (the Encrypted File System in Microsoft's NTFS) tech documentation
[1] [2] seems to say that it is encrypting and decrypting on a
per-block basis with a FEK (File Encryption Key).  It also states that
the FEK is randomly generated.
I take this to be relatively clear indication that they are not in
fact using "Convergent Encryption" in EFS as I speculated might be the
case earlier.  I presume therefore they do indeed just decide to live
with the side-effect that SIS [3] presumably won't coalesce files
across files not currently accessible to the OS.
They use public key crypto to encrypt the FEKs so they can share files
across multiple users and implement a recovery mechanism for the
administrator account.
Looks like the default config tries quite strongly to ensure the admin
account can by default recover user accounts files, though if I
understand you can disable the recovery feature.
[1] [2] [3] [1] and [2] are word docs, I put pdf convered versions here for people
that don't have word:

@_date: 2002-03-17 21:02:22
@_author: Adam Back 
@_subject: does NTFS encryption use "convergent encryption?" (Re: 
Just a sneaking suspicion, but I wonder if this "convergent
encryption" is part of win2k's file encryption feature.
I note in:
they have "single instance storage" feature in win2k NTFS file system,
clearly for the encryption and this feature to work across files
encrypted by multiple users they'd have to use something like
"convergent encryption", or just live with the fact that when files
are encrypted SIS won't work.
(win2k is multi user, files are encrypted with different keys across
different users, so not all encrypted files will be accessible to the
OS at any given time).
That'd be a fine mess.  Guess what files might be stored in encrypted
form and you can verify, just by trying.  Insufficient entropy in a
short file and you can guess all possibilities and try them.  Better
hope you wouldn't have any encrypted warez that aren't licensed (which
will have well known hashes.)  If they were using convergent
encryption it would be possible to quickly and efficiently sweep a
disk for encrypted copies of software.
Adam

@_date: 2002-03-14 02:36:57
@_author: Adam Back 
@_subject: hashcash-0.14: new hashcash format 
I made a number of improvements to the hashcash software to make it
into a more robust and better documented unix tool, including man page.
In doing this I changed the date format to be the simpler and more
human readable YYMMDD rather than 5 digit days since begining of unix
So the tokens now look like:
and the tool is more picky about returning error codes, it will only
return success if you really check every aspect (the resource string,
the number of bits of collision, and whether the token is double
See the man page
for all new usage and database.
(It can also support an extended date format based on UTCTIME: of
YY[MM[DD[hh[mm[ss]]]]]'Z' where stuff in [] is optional for tokens
with long or short validity periods, but just creates the human YYMMDD
format by default).
I made a lot of changes, so if anyone finds any bugs or portability
issues let me know.  The double spend database is not backwards-
compatible as I re-wrote it pretty much from scratch and changed the

@_date: 2002-03-10 09:51:34
@_author: Adam Back 
@_subject: Re: anti-flooding for mail2news gateways (used via mixmaster) 
Generally the collision string should be the resource name, so this
should not be an issue.  So for a mail2news gateway, the natural
resource names would be either the mail2news gateway address, or the
(perhaps better), the newsgroup name.
The reason newsgroup name may be better is the mixmaster user has no
reason or necessarily even easy way to find out what mail2news gateway
a mixmaster exit node is using.
You may be referring to the "fluffybunny" collision string in the
trial, but anyway the string must be standardized pre-published and
the user can not be expect to connect to the resource to find out what
the string is as the way to talk to it is either store-and-forward
email, or even more store-and-forward mixmaster delivery.

@_date: 2002-03-02 23:22:20
@_author: Adam Back 
@_subject: Re: Viability of Anonymous Reputations ? (Barrings Bank, Enron, 
Is a person credentials would help make the e-bay seller's fraud
tactic harder.  (I only read briefly about the case, but I think he
made use of lots of personas to talk up his own reliability and
merchandize quality; if this is not what he did the hypothetical
stands anyway).
To perpetrate his type of fraud the user would then have to bribe
other people to sell him their is-a-person credential.
Also, though I thought e-bay attempt to do this, you would think that
lots of votes from new users with no reputation should not count for
as much as a few votes from other people with existing reputations.

@_date: 2002-03-09 14:29:08
@_author: Adam Back 
@_subject: anti-flooding for mail2news gateways (used via mixmaster) 
(There has been some discussion of controlling floods on USENET
through mail2news gateways on remailer-operators list recently -- take
a look for example at alt.anon.privacy-server).
So this is indeed a problem.
The other proposal I saw recently here was adapative charging --
charge nothing unless flood is detected, then increase postage
requirement dynamically until the flood is squelched when the flooder
is slowed down to a trickle.
This has a couple of problems -- firstly the sender has no direct
connection to the resource which is setting the price, so it is
inconvenient to find out what value to put on the token.  Anyway by
the time the token arrives perhaps the price has increased and so the
mail bounces.
Related to anonymity: anonymous users don't want to direct http
connections or such to find out what the current price is as that will
tend to identify them as remailer users, as well as tending to
correlate their true identify with their anonymous posts due to timing
correlations between the two events.
Some other ideas:
What about is-a-person credentials with some non-trivial purchase
cost.  So a new nym would go to a web page do some proof of being
human (type in a number contained in a gif), maybe do some proof of
work (hashcash), and do some mild proof of uniqueness and anti-theft
of credential (mail the credential to the email address given).
If the same email-address is used twice, the user will be refused
another credential.
The user can then use the credential pseudonymously without being
identified.  If the user exceeds some pre-defined volume limit on the
resource, the resource revokes the pseudonym.
This has more of the desireed properties: there is some sign-up
over-head for all users, which adds some inconvenience for regular
users, but at least it is only one-off for them.  For flooders on the
other hand they can only send some sane limit per day of messages per
nym; and the overhead of creating a whole stream of nyms to make a big
flood is sufficiently inconvenient to make it quite tedious, though of
course not impossible for some truly dedicated person who wants to
spend all day typing numbers contained in images, minting 24hours
worth of hashcash on a normal machine etc.
If you wanted to get fancy you might be able to arrange that if the
nym sent more than a certain volume of messages in a time period his
email address would be revealed.
Thoughts on this?
(The anonymous is-a-person credentials could be built with Chaum's
credentials, or more flexibly with Brands' credentials, perhaps
Wagner's blind MAC based e-cash scheme.)

@_date: 2002-03-17 20:36:37
@_author: Adam Back 
@_subject: microsoft - "convergent encryption" - heh 
Just looking around at peer to peer file sharing sites, and came
across this research project page at microsoft, and in their faq they
describe "convergent encryption".
Heh.  Thought you might all find it amusing to observe what is wrong
with this picture:

@_date: 2002-03-20 21:36:58
@_author: Adam Back 
@_subject: disconnect choate list from other lists? 
Given that Jim Choate has a different view of events and the purpose
of the list to it seems just about everyone else, why don't we just
disconnect his lists from the other lists, then he and perhpas mattd
and a few other noisy types can go inhabit Jim's list and Jim will
surely be content to keep posting to the Choate version of the
cypherpunks list.
Then we will just let the market decide.
Between the [CDR] prefix and the insessant one liners and the changing
email addresses so he keeps popping up out of your filters, and that
he creates such a nuisance that people discussing the nuisance he's
caused also cause the topic to inevitably spill out of kill files, I
think it would be a net positive, and presumably amenable to Jim also
as he claims some kind of proprietorship over his node.

@_date: 2002-03-21 00:03:02
@_author: Adam Back 
@_subject: Re: Books, Ideas, the List, and Getting Back to Basics 
I think coderpunks has died -- John Gilmore had ISP problems.  Perhaps
if we could motivate some kind of distributed (and optionally
subscribed-to) filtering as I described in previous article to Igor,
cypherpunks might again be the preferred applied crypto strong
cypherpunk relevance discussion forum.
Choate is so annoying that you've got to wonder if he's on someone's
payroll as a disrupter.
I think there are a number of interesting aspects to distributed
storage.  As well as largely cypherpunks irrelevant aspects such as
off-site storage, scaling to cope with flash-crowds, persistent
site-independent URLs (so called URIs); there is scope for censor
resistance, publisher and reader anonymity, and possibly anonymous
micropayments to pay for bandwidth leading towards a world where web
content delivery service could replace the web, and established
content distribution networks such as TV, radio, news.  The barrier to
entry would be lowered, and the mundane massively biased pap that
passes for news on the networks for example could be replaced by real
reporting of all sides of the issue with smaller news companies.
Also if the infrastructure were there, the space could be a
replacement as well as for the web, for distribution and archive for
USENET, personal email boxes living in the net, movie archives, etc.
I think if you look at codecon, a number of people who are currently
jobless are using it as an opportunity to work on their favourite
crypto project that they had been meaning to do for a long time but
too busy work for boring PKI vendors and the like devote any time to.
On the issue of nothing to show for it, I don't really want to comment
negatively on peoples choices because I don't know their situations,
unavoidable costs of living etc but IT salaries have been pretty high,
and someone living it up and not saving anything for the future has
been taking a risk, though it's a little late now for anyone to revise
their strategy, it's something people might want to think about in
general.  It seems to me that a lot of people have a suprisingly small
cash buffer compared to salary, and essentially no long term strategy.
Even their PKIs (from S/MIME set of applications) are unsuitable for
basic individual centric communication -- centrall subvertable by
design; designed for central from the ground up by PKI vendors
influence on the standards groups.

@_date: 2002-03-22 17:07:31
@_author: Adam Back 
@_subject: Re: distributed filtering with server-side NoCeM's 
I suppose the advantage of using NoCeM's and onspool NoCeM processing
over a moderated list is that it allows multiple moderators.  The
moderators task is then to choose NoCeM issuers from the pool of
people issuing them.
An parallel group with no onspool processing might also be nice for
people who have NNTP news clients able to process the NoCeM's locally.
Then people have the choice.  Would be kind of nice.  As another
poster noted you can use NNTP off-line ok with some clients.
But anway the cypherpunks-moderated seems to be working out
OK so far.
btw. I sent an email inviting people from the now dead coderpunks list
(list server is out of action) to join cypherpunks-moderated
(or cypherpunks at their choice).

@_date: 2002-03-21 02:26:13
@_author: Adam Back 
@_subject: Re: distributed filtering with server-side NoCeM's (Re: 
For one reason or another people don't seem to use kill-files much, so
the overall effect is that noise chases out the signal.
It seems to me a useful low controversy filtering (which people don't
have to subscribe to anyway) would be just a straight policy: news
articles with no hand-written summary are deleted (plus of course junk
mail is deleted.)

@_date: 2002-03-20 23:42:04
@_author: Adam Back 
@_subject: distributed filtering with server-side NoCeM's (Re: disconnect 
Yes perhaps.
There is a technology, and in theory it should work, and was designed
for distributed ratings: NoCeMs.
Anyone explored NoCeM's?  I don't think everyone wants to upgrade to
NoCeM aware clients, but if there were a list listening to the NoCeM's
from people that were interested to do a kind of distributed filtering
for quality, then that distributed filtered list might be a nicer
alternative.  The filtered list owner would do the job of choosing
which NoCeM raters to honor the ratings of, and everything below some
threshold would be trashed.  There would still be unfiltered lists.
I hear there is an onspool NoCeM implementaiton for newsgroups, so
perhaps there is an equivalent for mailing lists.  Or perhaps one
could achieve it anyway with the clunky approach of gating the list to
a local newsgroup, running onspool NoCeM, and then gating it back
periodically (so the NoCeM's could accumulate and take effect) to a
mailing list.  Or giving access to a newsgroup -- that's pretty
convenient for permanantely connected people, though marginally less
so for dialup.

@_date: 2002-03-22 18:44:03
@_author: Adam Back 
@_subject: design considerations for distributed storage networks 
Here's something I wrote up the other night with my thoughts about the
differences between peer-to-peer networks vs the more ambitious
storage surface type propsals and the design criteria which one might
entertain designing against.
Suggestions for more criteria welcome.  How do the current raft of systems like bittorrent, mnet/mojonation,
freenet, and the others presented at codecon rack up against criteria
such as these?  Plus how do the non privacy and censor-resistant
focussed, but censor resistant to some extent just by sheer volume and
popularity like gnutella, morpheus/kazza/fasttrack, edonkey, imesh
btw I've noticed while looking around at storage-surface web pages
recently while writing the above that it would seem that some are
showing signs of gearing up for commercial backing.
eg.  -- I'm pretty sure that used to look
more research oriented and it's now looking quite corporate.  Also the
interest from commercial vendors like micrsoft who has their own
farsite project:

@_date: 2002-03-23 13:50:53
@_author: Adam Back 
@_subject: Peer-to-Peer File Sharing and Copyright 
To follow-up on Tim's comments about the safety to be had from
publihsing p2p software anonymously, and the risks of not doing so,
this is an interesting analysis of the topic by Berkeley Centre for
Law & Technology lawyer Fred von Lohmann, hosted by EFF.
IAAL: Peer-to-Peer File Sharing and Copyright Law after Napster
The discussion of contributory and vicarious copyright arguments is an
eye-opener (the full paper goes into more detail).  My conclusion
after reading this (well before also actually, but it re-enforced the
view) is that the safest and simplest thing to do is to just publish
such software anonymously.
Ian Clarke of freenet has a lot to say about copyright vs freedom of
He's right there is a conflict between anonymous censor-resistant
publication systems and as I've said a number of times over the years,
I figure sooner or later DMCA, WIPO et al are going to run into
remailers, and p2p systems designed for publisher anonymity; and then
we're likely to see a head on battle ala USG against strong crypto,
but with the battle between freedom-of-expression and anonymous
publication systems and copyright.  Ultimately there isn't room for
both strong anonymity and strong copyright enforcement, it's another
binary choice.  Already we're seeing battles between copyright
enforcers where the strong-copyright lobby has the staggering hubris
to attempt to outlaw general purpose computers without copyright
enforcement hardware builtin.
If I recall there was a brief skirmish between a remailer operator and
SPA (software publishers association -- the bit police for software
bits) when someone anonymously constructed some "designer abuse" and
then themselves reported it to the SPA in an attempt to shut-down the

@_date: 2002-03-23 14:42:23
@_author: Adam Back 
@_subject: future uses for storage surfaces 
I think a couple of comments recently stem from a missing reference --
some comments were in effect follow-ups to the short comment about
current interesting topics for discussion I included in my invite to
coderpunks list posters to join cypherpunks-moderated; which did not
get posted here.
Here is that part of the invite (the rest was subscription
instructions, and explanation of what happened to coderpunks list
server etc).
I think the social implications of technologies don't really take hold
until the technology moves from proof of concept to wide deployment
and use.  Viz napster, kazza/morpheus, gnutella.  Many of these
applications were in theory possible before; indeed I remember maybe
10 years ago the FSP file trading scene.  The advantage of FSP was
that it was almost stealth technology -- it could be set to consume
only fixed bandwidth and so people would have unbeknownst to them a
big FSP server with a limited resource limit.  Or also IRC bots, and
direct file trading via IRC; file trading in hidden FTP directories.
I just saw Steve Shear's post (copied below) on the dcsb list where he
mentions USENET movie trading in VCD format in alt.binaries.vcd.  I
didnt' try any out, but it took my newsreader a fair while to download
and thread the subject lines, and there certainly seem to mostly
binary attachments.  Though this approach lacks the convenience of
kazza et al, it's interesting to see the plethora of channels by which
this is happening.  Censoring alt groups is also pretty hard, so I
guess it marks another nail in the coffin of the control the
DMCA/MPAA/RIAA/WIPO/SPA bit-police imagine they had, and think they
stand a chance to regain.

@_date: 2002-03-23 21:23:55
@_author: Adam Back 
@_subject: anonymously published software examples (Re: Peer-to-Peer File Sharing and Copyright) 
Historically there were some relatively significant packages
anonymously published: eg Pr0duct Cyphers' magic-money, Henry Hastur's
PGP Stealth; possibly there are others which have not especially tried
to draw attention to the fact that they were strongly anonymously
published.  The bit-police would then find that out later if and when
they tried to trace the author, and when the trail is years cold, the
remailers used and their keys long gone.  Shades of the scientologists
bumping into a long dead nymserver account probably pointing at a
newsgroup after obtaining an email address from penet.fi.
Though perhaps less fun the latter kind of low profile anonymity seems
like an even better idea; it's not like you're going to notice the

@_date: 2002-03-24 00:28:32
@_author: Adam Back 
@_subject: Re: signal to noise proposal 
Apart from my recent comments about NoCeM's and on onspool NoCeM
reader, another perhaps simpler idea would be to do it all with simple
CGI stuff and a web archive.  I'm sure this has been discussed before
in the past, but I don't recall anyone actually trying it out:
subscribers would choose how long their messages should be held until
being delivered; and which moderators they want to accept negative
votes for.  Then moderators would read cypherpunks on the web page and
select tick-boxes of messages they thought were junk.
I suspect the weak point would be how many people would read via web,
and bother to vote on articles and so how many moderators you would
expect.  Without someone keeping track of useful
moderator-configuration ratings ("I'll have whatever set of moderators
person X uses" -- to avoid having to keep up to date with currently
active moderators.) it might be a little inconvenient.  Selecting all
moderators obviously wouldn't work -- we've got enough loons that
there would be people trying to moderate all messages.
Are there people who already read cpunks regularly via the web?
(Reading email and mailing-lists via the web always seemed clunky to
me, even on broadband, but there are apparently vast numbers of people
who use only web-email by preference, and to this group presumably a
web archive is preferable to subscribing to a list and reading it's
contents via their web-email account page.)

@_date: 2002-03-24 00:17:56
@_author: Adam Back 
@_subject: the anti-s**mmer and anti-flooder arms-races 
I'm finding the open-relay black-list is starting to cause more
problems than it solves -- the reliability of email is suffering at
the hands of over-zealous and dictatorial black-listers.  I had in the
last month to effect two changes to such things to avoid problems
people reported to me about my address being unreachable to them.
It's bad because it's a form of censorship almost, or certainly ends
up damaging the reliability of email in ways that are sometimes hard
to fight your way out of -- it seems that some of the people operating
these black-hole lists seem to have the sys-admin BOFH mentality that
they are going to _punish_ people who inadvertently had open-relays or
configuration problems by delaying the removal of the relay by some
function of time related to how long it took them to fix their relay
problem.  It also leads to court cases which is another bad thing
(lawyers and laws shouldn't be drug into internet message routing).
John Gilmore apparently is paying a programmer to work on mail
software with better filtering.  I'm not sure what the feature-set is
but John's argument seemed to be that smarter local filtering is the
answer (to junk mail) -- rather than open-relay black-list systems --
toad.com got into an argument with his ISP verio over open relay at
toad.com.  (I think the verio argument is why the coderpunks list
server died).
Gilmore wrote a long article about this on politech:
        (I finally managed to get _off_ politech, now that Declan has a web
interace for unsubscribing, I'd been filing it to /dev/null for a
couple of years when it seemed quite impossible to get off that list.
I saw Gilmore's article on dcsb list as Robert Hettinga forwarded it
In a related vein someone posted on the camram list recently
( -- mailing list about deploying hashcash as a
junk mail counter-measure) this:
which sounds like a more plausible next step in the arms-race against
commercial junk mail.
Then there's the CAMRAM experiments at:
where people are trying to figure out ways to overcome the deployment
problems other issues with hashcash as applied to suppressing junk
mail.  Lapo Luchini's nice java hashcash generator at:
is a big step towards making it practical (though still pretty
inconvenient if you ask me) to impose hashcash on senders.  Lapo says
the just-in-time compilers make his custom optimised java SHA1 code
run at 50% speed of native C code, which is suprisingly fast, and
makes this web page a more practical addition.
Hashcash is at:
I made recently a number of functionality and portability improvements
to the hashcash code and some windows binaries as there are some
experiments under-way in with Mike Shinn and Alex de Joode I think:
experimenting with hashcash postage for mail2news gateways to try
throttle flooding in the a.p.a-s news group, and I guess longer term
in newsgroups reachable from the m2n gateway in general.

@_date: 2002-03-24 01:01:14
@_author: Adam Back 
@_subject: gnu emacs MOTIVATION doc (Re: anonymously published software examples) 
Not coincidentally there is a file called MOTIVATION included with the
emacs package (try "locate MOTIVATION" or your unix system) which
talks about these issues.  It rang quite true for me.  I've found
programmer performance is strongly related to interest -- I figure a
strongly motivated programmer can produce times over more creative
work and can deliver more quickly than someone slogging through
intrinsically boring trudge work; financial incentives don't make
something interesting in itself.  The attention span, motivation depth
and concentration level of a disinterested person is I suspect still
typically lower than an interested person even given relatively
significant purely financial rewards.
full text here:
Indeed perl is another good example -- a huge contribution by Larry
Wall and many other contributors and the primary motivation doesn't
appear to be financial reward.
I suppose another general example would be the system of peer reviewed
journals -- the reward being related to recognition of achievements
from peers and furtherance of a field of research (though in some
cases publication volume is tied to financial rewards and tenure; but
this is itself a pressure somewhat tending to pull down the quality of
published work in my view).

@_date: 2002-03-28 02:18:39
@_author: Adam Back 
@_subject: gnutella's problems (Re: network topology) 
I think you're right and that's how it works, which is why it scales
badly as the search messages flooded seven hops deep into the randomly
organized network even with duplicate suprresion based on query-id (or
whatever they use), end up consuming some significant proportion of
the network bandwidth.
The other problem apparently is that for a low bandwidth node (eg
dialup) the searches can saturate your link, so you can hardly do
anything but receive and answer search queries.
Apparently there are some hacks to reduce this problem, but Gnutella's
other big problem is that there are lots of independent clients so
some of the problems come from interoperability problems, bugs etc.
And gnutella is not able to resume a transfer that dies part way
through which is very bad for download reliability.  FastTrack/Kazza
(but no longer Morpheus since the Kazza / Morpheus fall-out) on the
other hand can resume, and in fact do multiple simultaneous downloads
from multiple nodes having the same content so that it gets the
content both much faster and much more reliably.  Also helps cope with
different link speeds as a group of slow nodes or asymmetric bandwidth
nodes (like cable with fast down but limited up) can satisfy the
download of cable and other broadband users.
There's a nice write-up about the gnutella's problem's on openp2p.com
Contrary to what article [2] claims FastTrack/Kazza really does blow
Gnutella away, the supernode concept with high performance nodes
elected to be search hubs makes all the difference.  Gnutella last I
tried it was barely functional for downloads, ~95% of downloads
failed, and searches were much slower.
[1] Gnutella: Alive, Well and Changing Fast, by Kelly Truelove
[2] Gnutella Blown Away? Not Exactly, by Serguei Osokine

@_date: 2002-03-30 15:25:19
@_author: Adam Back 
@_subject: node discover & searching (Re: Celsius 451 -the melting point 
I never really found discovering a currently active node on the
network a problem even with original gnutella client.
google seemed to reliably give you a host cache file with at least
some remaining online nodes.
As an aside, it seems that google (and other search engines) are vying
for first place as replacement for URLs as first line content
location.  I looked recently at the web logs on my web pages over the
last year or so and I think > 50% of queries came from search engines
(from referrer field).  The search terms (also in the referrer field
with google) mostly looked purposeful ("hashcash adam", "perl-rsa"
etc, though there were a few ill-thought out or name-clash search
terms "making hash" who probably didn't find what they were looking
for ;-)
This seems like a success factor for a would be storage-surface with
ambitions to replace and subsume the web -- any such system has to
have a really good google competitive search mechanism, or have a way
for it's content to be indexed by google.

@_date: 2002-03-31 17:08:51
@_author: Adam Back 
@_subject: on the state of PGP compatibility (2nd try) 
[This is actually slightly more accurate and even worse than my first
mail which bounced to some of the lists as I had a typo, _and_
separately encountered a mail hub outage at cyberpass.net -- apologies
to those who get duplicates].
So I was trying to decrypt this stored mail sent to me by a GPG user,
and lo pgp6.x failed to decrypt it.
So I try an older gpg I had installed, and it fails because it doesn't
support RSA or IDEA, and this GPG user it seems installed the RSA and
IDEA patches.
So I go fetch GPG from  but it still doesn't contain
IDEA by default due to the anti-patent religion thing gone-to-far over
at GNU land, so I try to download the idea.c plugin -- except they
seem to have removed it (accidentally? hmm -- I fire off a mildly
brusque email to webmaster at gnupg.org -- and hit google.com but all
the first few hits are pointing at the gnupg.org faq with labyrinth of
links ending eventually in the same dud link.)
Either way _even_ if idea.c were still where they claimed it would be
it seems intentionally very well hidden, which I think is
unconscionable for a security product: to _intentionally_ frustrate
users attempts to interoperate with secure previous versions and
alternate implementations.
So then I try pgp5.x but the binary is using dynamic libraries that
are no longer on my shiny new redhat7.1 installation, so I try to
compile it but it no longer compiles.  Tinker briefly fixing up
things, but the errors are multiple, and I haven't got time for this.
So my last hope is pgp2.x, but some buggy pgp variant has left my
pgp2.x key ring empty, and you can't use it directly on pgp6.x
keyrings as it will barf on the new key formats.  So then I ponder
exporting my private key out of pgp6.5.8 which isn't going to do it
compatibly without some serious thought -- openPGP's salted key
stretching maybe being used -- do I want to export it without a
password (not really), so figure out how do I turn the salted key
stretching off, will pgp6.5.8 even let you export private keys, or is
it easier to just extract it with a binary editor?  Fortunately I
finally find a pgp2.x keyring secring.bak file (thanks PRZ), and move
it back and lo pgp2 can't decrypt it because of some unsupported
packetry.  I take a look at it with Mark Shoulsen's pgpacket and it
seems that _even_ pgpacket thinks there is some unsupported packets at
the end, so dig out another packet analysis program -- pgpdump by Kazu
Yamamoto and it doesn't seem to realise it's ascii armored or is
expecting to find an external program to de-armor which is missing --
I don't care, so use emacs and mmencode -u to produce a binary
version, and then it plays.  And there lies the problem: gpg encrypted
it with IDEA using the new openPGP streaming options to encode the
message in chunks despite it being encrypted with idea (presumably the
sender forget to invoke --rfc1991 not realising my potential future
predicament).  Thus sprach Kazu's analyzer:
New: Symmetrically Encrypted Data Packet(tag 9)(512 bytes) partial start
New:	(201 bytes) partial end
So, for now, give up.  I guess it's cheaper to just send the original
author an email ask him if he remembers that idea he sent me 4 months
ago and have him send me it in clear text to be sure!
What a nightmare!  Try that sequence on a novice user and they give up
before they get past the first GPG faq with rant about algorithm
We've really got to do something about the compatibility problems.

@_date: 2002-03-31 17:08:51
@_author: Adam Back 
@_subject: on the state of PGP compatibility (2nd try) 
[This is actually slightly more accurate and even worse than my first
mail which bounced to some of the lists as I had a typo, _and_
separately encountered a mail hub outage at cyberpass.net -- apologies
to those who get duplicates].
So I was trying to decrypt this stored mail sent to me by a GPG user,
and lo pgp6.x failed to decrypt it.
So I try an older gpg I had installed, and it fails because it doesn't
support RSA or IDEA, and this GPG user it seems installed the RSA and
IDEA patches.
So I go fetch GPG from  but it still doesn't contain
IDEA by default due to the anti-patent religion thing gone-to-far over
at GNU land, so I try to download the idea.c plugin -- except they
seem to have removed it (accidentally? hmm -- I fire off a mildly
brusque email to webmaster at gnupg.org -- and hit google.com but all
the first few hits are pointing at the gnupg.org faq with labyrinth of
links ending eventually in the same dud link.)
Either way _even_ if idea.c were still where they claimed it would be
it seems intentionally very well hidden, which I think is
unconscionable for a security product: to _intentionally_ frustrate
users attempts to interoperate with secure previous versions and
alternate implementations.
So then I try pgp5.x but the binary is using dynamic libraries that
are no longer on my shiny new redhat7.1 installation, so I try to
compile it but it no longer compiles.  Tinker briefly fixing up
things, but the errors are multiple, and I haven't got time for this.
So my last hope is pgp2.x, but some buggy pgp variant has left my
pgp2.x key ring empty, and you can't use it directly on pgp6.x
keyrings as it will barf on the new key formats.  So then I ponder
exporting my private key out of pgp6.5.8 which isn't going to do it
compatibly without some serious thought -- openPGP's salted key
stretching maybe being used -- do I want to export it without a
password (not really), so figure out how do I turn the salted key
stretching off, will pgp6.5.8 even let you export private keys, or is
it easier to just extract it with a binary editor?  Fortunately I
finally find a pgp2.x keyring secring.bak file (thanks PRZ), and move
it back and lo pgp2 can't decrypt it because of some unsupported
packetry.  I take a look at it with Mark Shoulsen's pgpacket and it
seems that _even_ pgpacket thinks there is some unsupported packets at
the end, so dig out another packet analysis program -- pgpdump by Kazu
Yamamoto and it doesn't seem to realise it's ascii armored or is
expecting to find an external program to de-armor which is missing --
I don't care, so use emacs and mmencode -u to produce a binary
version, and then it plays.  And there lies the problem: gpg encrypted
it with IDEA using the new openPGP streaming options to encode the
message in chunks despite it being encrypted with idea (presumably the
sender forget to invoke --rfc1991 not realising my potential future
predicament).  Thus sprach Kazu's analyzer:
New: Symmetrically Encrypted Data Packet(tag 9)(512 bytes) partial start
New:	(201 bytes) partial end
So, for now, give up.  I guess it's cheaper to just send the original
author an email ask him if he remembers that idea he sent me 4 months
ago and have him send me it in clear text to be sure!
What a nightmare!  Try that sequence on a novice user and they give up
before they get past the first GPG faq with rant about algorithm
We've really got to do something about the compatibility problems.

@_date: 2002-04-29 18:24:59
@_author: Adam Back 
@_subject: attack on rfc3211 mode (Re: disk encryption modes) 
I can see that, but the security of CBC MAC relies on the secrecy of
the ciphertexts leading up to the last block.  In the case of the mode
you describe in RFC3211, the ciphertexts are not revealed directly but
they are protected under a mode which has the same splicing attack.
The splicing attack on "CBC MAC with leading ciphertext" works through
CBC encryption, here's how that works:
Consider plaintext P1,P2,P4,P5, first pass ciphertext A1,...,A5 and
second pass ciphertext B1,...,B5:
If we swap the first and second blocks of ciphertext (B1,B2) like this:
B1'=B2, B2'=B1, B3'=B3, B4'=B4, B5'=B5
and then try decrypting as usual with the two pass mode, first decrypt
B5 using B4 as IV to get A5:
IV = A5 = D(B5)+B4 = A5
so the IV is the same.
Then decrypt B1' to B4' to get A1' to A4':
So the CBC mode has recovered by A4, then decyrpt A1',...,A5' using IV
of 0 as usual to get P1',...P5':
and you can see we have effected a partial and targetted garbling of
the plaintext.
I would have thought this would be considered a 'break' of a
non-malleable cipher mode as discussed for disk encryption where each
bit of plaintext depends on each bit of ciphertext as would be the
case with a secure cipher matching Mercy's design goals (a block
cipher used in ECB mode with a different key per block).
With a disk mode, unlike with RFC3211 pasword based encryption for CMS
there is no place to store the structure inside the plaintext which
may to some extent defend against this attack.

@_date: 2002-04-28 17:02:24
@_author: Adam Back 
@_subject: "news" is irrelevant -- write code not laws (Re: Cypherpunks Europe) 
I guess there are a fair number of people from Europe on the list.  I
think there are a number of UK readers, plus others Tim mentioned.
(I'm from the UK, but living in Canada right now).  There is a UK
crypto list, but it's full of news and legal stuff so relatively
But the reason at least from my side that I don't post news is I
eschew news of the banal kind such as our resident idiot Jim Choate
streams dozens of on a daily basis (I kill-filed him, plus the
moderator of the moderated version squashes most obviously idiotic
output).  I intentionally watch almost no TV, including TV "news", or
other traditional news.
There are perhaps 20-30 news items worthy of comment per year and
discussion usually happens here so using traditional media news won't
achieve anything apart from wasting your time consuming typically
heavily biased, technically confused journalists produce cute sound
bites and generally mindlessly regurgitating the party line.  I find
these days I have such negative views of the bias in the traditional
news that it makes me cringe and turn it off.  (Irrelevant detour, but
every time "shrub" (aka "small Bush" -- US president) is broadcast his
inarticulate stuttering and inane grin, just causes me to hit the
off-switch, the guy seems like a complete moron -- Blair is smug, also
with his cheshire cat grin, but at least he is somewhat articulate and
can come across intelligently -- shrub is a PR disaster.)
So the interesting technical challenges from a "cypherpunks write
code" point of view are already abundabtly clear without more news.
You can pretty much rely on the maxim that politicians and the media
will achieve the worst legal system for personal liberties in
cyberspace, so our job is to build cypherspace where their ill-thought
out laws particularly on speech, content, copyright etc largely don't
So I'd sooner for example spend time discussing how to design censor
resistance, publisher and reader anonymity into a large scale
file-sharing or next gen distributed web publishing replacement than
for example the on-going burble along the lines "gee look what stupid
laws the politicians and media are thinking of introducing now".  We
already know they continue to make stupid laws, our task is to use
technology to make their stupid laws as irrelevant as possible.
Combing over the details of the political systems stupidity never
seemed like a constructive use of time to me.  Yes, there are 'Net
lobbying groups, but I'm not sure they ultimately achieve anything
apart from at best burning resources just acting as a stupidity-brake,
and typically worse being sucked into the deals and favors for trade
lobbying and bribing-fest.

@_date: 2002-04-27 17:28:24
@_author: Adam Back 
@_subject: Re: disk encryption modes (Re: RE: Two ideas for random number generation) 
Yes I gathered, but this what I was referring to when I said not
possible.  The OSes have 512Kbytes ingrained into them.  I think you'd
have a hard time changing it.  If you _could_ change that magic
number, that'd be a big win and make the security easy: just pick a
new CPRNG generated IV everytime you encrypt a block.  (CPRNG based on
SHA1 or RC4 is pretty fast, or less cryptographic could be
sufficient depending on threat model).
Security / space trade off with no performance hit (other than needing
to write 7% or 14% more data depending on size of IV) is probably more
desirable than having to doubly encrypt the block and take a 2x cpu
overhead hit.  However as I mentioned I don't think it's practical /
possible due to OS design.
Well with the sector level encryption, the encryption is below the
defragmentation so file chunks get decrypted and re-encrypted as
they're defragmented.
With the file system level stuff the offset is likley logical (file
offset etc) rather than absolute so you don't mind if the physical
address changes.  (eg. loopback in a file, or file system APIs on
Yes, that's what I was referring to by "already had some problems".

@_date: 2002-04-01 00:34:35
@_author: Adam Back 
@_subject: what is GPG's #1 objective: security or anti-patent stance ( Re: on the state of PGP compatibility ( 
I've trimmed the Cc line a bit as this is now focussing more on GPG
and not adding any thing new technically for the excluded set.
I don't see how this is a useful distinction.  They are self-evidently
not close enough for practical purposes as evidenced by the fragmented
user base and ongoing problems you experience if you try using PGP.
Back in the days of pgp2.x I used to receive and send a fair
proportion of mail encrypted with pgp; these days it is a much lower
proportion, and a rather high proportion of those fail.  It's not like
I'm using old software or failing to try what is reasonable to get
messages to work.  Even with my fairly complete collection of PGP
versions you saw the results.  Imagine how much worse it will be
between people who do not upgrade frequently or take such defensive
strategies.  So you say upgrade already.  However as I think I have
demonstrated, I follow this strategy myself and as you can see it
doesn't work either.
I can't speak of PGP 7's behavior in this discussion as it is not
available for the operating system I primarily use (linux) as far as I
am aware.
I would characterise the situation as intentionally frustrating
attempts to use IDEA.  The whole point of the little exercise of
stripping out the idea.c, making it a separate dynamically loadable
module, tucking it away in a FAQ where you are pointed to lectures
about how software and algorithm patents are bad is _specifically, and
explicitly_ to discourage use of patented algorithms (and in this case
of the idea.c implementation) and to encourage people to do lobby
about the patent madness.
Campaigning against patent madness is a good cause in itself but not
when it gets in the way of privacy to the point that people are
sending messages in plaintext.  After all what is GPG's primary
purpose: is it an email security software package focussing on
security, or a platform for promulgating political views.  I view the
exclusion of idea.c from GPG as basically a security bug of higher
severity than for example PGP2.x's manipulable fingerprint, or
pgp5.something's (before it got fixed) unsigned ADK bug packet, or the
potential buffer overflow in ZLIB.  This bug is worse because it
reproducibly and frequently causes people to send mail in clear text.
The other bugs are by comparison less dangerous, yet they (the two
more recent ones) were fixed by NAI, and GPG and other PGP software
maintainers with rushed over-night hot fixes.
I would suggest this bug would be best fixed in GPG by:
a) including IDEA as a default option in GPG -- the ASCOM patent
license is really very liberal for non-commercial use, and b) if that goes against the GNU philosophy to the extent that it is
worth causing hinderance to hundreds of thousands of users who
practically are _going_ to want it they could at least make it a
configuration file option and ship it as other crypto libraries such
as openSSL do.  (I'm not sure how it hurts the anti-patent stance to
do this -- gnupg.org is after all _already_ distributing idea.c, just

@_date: 2002-04-01 00:34:35
@_author: Adam Back 
@_subject: what is GPG's #1 objective: security or anti-patent stance ( Re: on the state of PGP compatibility ( 
I've trimmed the Cc line a bit as this is now focussing more on GPG
and not adding any thing new technically for the excluded set.
I don't see how this is a useful distinction.  They are self-evidently
not close enough for practical purposes as evidenced by the fragmented
user base and ongoing problems you experience if you try using PGP.
Back in the days of pgp2.x I used to receive and send a fair
proportion of mail encrypted with pgp; these days it is a much lower
proportion, and a rather high proportion of those fail.  It's not like
I'm using old software or failing to try what is reasonable to get
messages to work.  Even with my fairly complete collection of PGP
versions you saw the results.  Imagine how much worse it will be
between people who do not upgrade frequently or take such defensive
strategies.  So you say upgrade already.  However as I think I have
demonstrated, I follow this strategy myself and as you can see it
doesn't work either.
I can't speak of PGP 7's behavior in this discussion as it is not
available for the operating system I primarily use (linux) as far as I
am aware.
I would characterise the situation as intentionally frustrating
attempts to use IDEA.  The whole point of the little exercise of
stripping out the idea.c, making it a separate dynamically loadable
module, tucking it away in a FAQ where you are pointed to lectures
about how software and algorithm patents are bad is _specifically, and
explicitly_ to discourage use of patented algorithms (and in this case
of the idea.c implementation) and to encourage people to do lobby
about the patent madness.
Campaigning against patent madness is a good cause in itself but not
when it gets in the way of privacy to the point that people are
sending messages in plaintext.  After all what is GPG's primary
purpose: is it an email security software package focussing on
security, or a platform for promulgating political views.  I view the
exclusion of idea.c from GPG as basically a security bug of higher
severity than for example PGP2.x's manipulable fingerprint, or
pgp5.something's (before it got fixed) unsigned ADK bug packet, or the
potential buffer overflow in ZLIB.  This bug is worse because it
reproducibly and frequently causes people to send mail in clear text.
The other bugs are by comparison less dangerous, yet they (the two
more recent ones) were fixed by NAI, and GPG and other PGP software
maintainers with rushed over-night hot fixes.
I would suggest this bug would be best fixed in GPG by:
a) including IDEA as a default option in GPG -- the ASCOM patent
license is really very liberal for non-commercial use, and b) if that goes against the GNU philosophy to the extent that it is
worth causing hinderance to hundreds of thousands of users who
practically are _going_ to want it they could at least make it a
configuration file option and ship it as other crypto libraries such
as openSSL do.  (I'm not sure how it hurts the anti-patent stance to
do this -- gnupg.org is after all _already_ distributing idea.c, just

@_date: 2002-04-10 22:12:04
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line 
The issue the smart-card setting addresses is that people don't, or
anyway shouldn't place great trust in closed systems that they, or
someone with the technical background necessary can not examine.  A
smart card is such a closed system.  The framework allows the use of
smartcards to resist fraud while not making it necessary to for the
users to trust the smart-card with their privacy.  Privacy is
controlled by the more auditable host computer.

@_date: 2002-04-10 22:08:47
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
Is there anything specific PKILAB have said about Brands certs?
I'm not sure what their claim could be, from what I can see the Brands
credentials provide or can equally be used with all of the common PKI
models (RAs, CAs, OCSP, short-lived certs, revocation lists) plus a
bunch of other options (blind refresh, update, privacy, etc) which are
not possible with X.509 identity and attribute certificate PKIX stuff.
btw I did a google search for PKILAB and Brands to see if I could find
anything along the lines you mention and look what it said:
Mar 2001 "Welcome Stefan Brands to PKILabs Advisory Board"

@_date: 2002-04-10 11:55:02
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
You don't need the minter's secret key to identify the double-spender.
Anyone who happens to see two coin transcripts answering different
challenges with the same coin private key can recover all the
attributes of the coin, including the identity attribute.
This is described on p23 of [1].
[1] "A Technical Overview of Digital Credentials", Stefan Brands, to appear International Journal on Information Security

@_date: 2002-04-10 01:59:40
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line 
The regular consumer will rely on a third party to examine the source
to see that they securely and correctly implement the protocols to
assure privacy.
In the smart card setting with Brands protocols there is a host
computer (eg pda, laptop, mobile-phone main processor, desktop) and a
tamper-resistant smart-card which computes part of the coin transfer
and prevents double-spending (to the limit of it's tamper-resistance).
You can't verify what the smart-card is doing so easily, however the
computation by the host computer assures that the smart-card even if
it is intentionally hostile to your privacy can not help the bank
trace your payments as everything it says is blinded by the host
computers calcluations which are more verifiable.
It may seem convoluted, but by comparison assurance of security of
algorithms used with credit-cards over SSL, or even the authentication
framework used by card swipe credit cards also would appear
complicated to many.  All that matters at the consumer level is that
it demonstrably works, the people running the system are confident
enough in it to deploy it, fraud is low, and that consumers gain trust
in it through whatever means.
For acceptance of privacy features similar issues will hold.  Do the
privacy advocates, analysts, and experts agree that the system
provides privacy.

@_date: 2002-04-10 01:44:05
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
OK so an additional feature you're arguing for is to hide the very
fact that one uses ecash, kind of like steganography, hiding ones
participation.  In this sense it is true that off-line requires one
expose one's participation in the payment system to the bank (aside
from the somewhat weak arguments about escrow accounts and is-a-person
credentials which may not be a very convincing deterent given
difficulty of limiting double-spending).
It would be technically possible to have a more user-trusted
Registration Authority verify identity and have the bank not learn
identity until coins are double spent, but I'm not sure this is very
comforting for users and it's not clear banks would like this either
as they usually want to control their own risks.
How meaningful participation-hiding is depends on the size of the
anonymity set.  If the number of users of anonymous cash is small then
hiding one's participation may be a feature some users place value in.
If there were lots of users however the benefits of
participation-hiding is more limited, and would tend to be outweighed
by the additional privacy risk for payees to make online deposits
(payers will also likely be payees some of the time).
Another option would be for people who wanted participation-hiding to
privately contract to use someone's identity, for example a money
changers.  The penalty clause for violating the contract would be
negotiated between those parties.  In event that the coin is double
spent and the identity-loaner is identified and approached by the
bank, the identity-loaner would show the private contract and identify
the true culprit.  Or follow some other outcome specified by the
contract.  Of course this is also inferior to simply not revealing
ones identity for the application of participation-hiding.
Anyway participation-hiding against the bank can also be offered by an
off-line transferable system.  Well more strictly I suppose you would
call this a hybrid of the two where users chose what kinds of coins
they want to get.  Off-line, off-line transferable and online coins
can all mix and be exchanged in the same payment network allowing
users individually to choose between the features they want, including
participation-hiding, payer anonymity, payee anonymity and trade-offs
between immediate fraud-prevention and options for higher latency more
anonymous connections and deferred deposit, spending and refresh.
In a hybrid system, online coins would be recognizable as online and
so payees would know the coin required online verification with the
bank to avoid risk of fraud-tracing free double-spending.
This hybrid system I think allows all features and advantages
previously discussed for payee privacy: particularly it allows both
higher latency and hence easier to anonymize communications, and also
facilitates participation hiding as an alternative for those that
value that privacy feature more.
Note that the differing possible privacy desires of the payer and
payee are not always ideally met.  For example online gives the payee
good participation-hiding anonymity as he can buy money from a
money-changer or other user (hopefully without having to trust them to
not also double-spend his coin before he can use it).  However the
online coins are less anonymous for the payee as they have to be
cashed online.  So there would be some negotiation between payer and
payee for the type of payment.
Conceivably a payer who insisted on participation-hiding online coins
and a payee who insisted on transferable off-line coins so he could
robustly hide identity and volume from the bank may have some trouble
Actually I think most of the delay is because I post in the morning
before work (I'm on EST, it's just I get up early so I can log some
hours before work) or in the evening after work.  Professionalism
dictates I don't spend much time reading personal email at work.  (I
work at ZKS, and as readers who followed recent anonymous rumors will
have noticed Brands is no longer at ZKS, so discussing ecash stuff is
no longer even tangentially relevant for me to spend work time on.)
Cc's are of course most welcome, but in addition if I'm actively
following an interesting thread (such as this one), and the flow of
posts runs dry while I have time, such as at the weekend for example,
I check the threaded posts on the web archive.

@_date: 2002-04-10 00:30:59
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
I suppose the bank would have to hold onto the funds until the coins
issued using that account as guarantee expired.  Normally coins anyway
expire to allow the bank to put a cap on the size of it's double-spend
database.  This makes ecash inconvenient for long-term off-line value
storage, even if they are denominated in for example Intel stock or
something.  Software would automate the process of exchanging old
epoch coins for current epoch coins -- for example Pr0duct Cipher's
magic money works like this.
One approach I think might be interesting for value storage is to
offer longer validity periods for high value tokens.  Small change can
expire more quickly, as you're typically spending that so it's less of
an inconvenience.
Aside from the problem with limit you identify, I think generally the
precedent is already set by the non-electronic world: to engage in
transactions which typically require reputation and identity for
contract violation enforcement anonymously, you have to pony up cash
up-front.  (eg. my secured credit card example which I can only
presume is because they worry that an untrusted foreigner will run up
the card and leave).
No that is prevented.  The user (the person who most recently received
the cash which now looks like [A_v-coin B_0-coin ... F_0-coin] where
v-coin is the original coin with value withdrawn unlinkably from the
bank by Alice, then Alice spends to Bob by binding to a 0-value coin
with Bob's identity in it, and Bob spends to Charlie... until Fred
gets the coin, and Fred decides to exchange it for a fresh coin from
the Bank, only Fred would prefer not to show this transaction in his
account, and doesn't want to identify himself to the bank again
either.  So when Fred wants to exchange this coin for a fresh coin he first has
to convince the bank that he knows that coins private key by answering
a challenge.  Then the bank uses the refreshing protocol to issue a
new coin which shares attributes with the F_0-coin it is being
exchanged for.  Fred's identity is in the new coin, however the bank
doesn't learn Fred's identity, though it is just assured that it is
the same identity as that encoded in the F_0-coin.  But if this is all we did it would not be useful because the new coin
would also have a 0-value as it's undisclosed attributes are cloned
from F_0-coin.  So in combination the bank must use the updating
protocol to change the value from 0 to the value v from the A_v-coin.
Note also the refreshing protocol could be used to get a batch of
fresh 0-value coins so the user would not need to identify himself
beyond whatever identity were leaked by his communication link to the
A correction on something I said earlier about Chaum double-blinding:
This is innacurate, it is actually a simultaneous withdrawal and
spend, followed by an arbitrarily later spend by the payee as the
payee knows the payer does not see the coin due to the extra blinding.
It's still less convenient of course because the payer and payee have
to be simultaneously and interatively online, where as normally with
online Chaum, and with off-line and off-line transferable protocols
the payer may for example send a payment via email.  Off-line and
off-line transferable also allow more anonymity for payer where the
payee also wishes to be anonymous.

@_date: 2002-04-09 13:30:56
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
Not quite inherently, there are other things you could do.  (This has
been discussed before I think in [1] at least from reference in the
thesis).  You could if you wished, rather than putting identity in the
coin, put an anonymous escrow account number in the coin.  Users who
preferred to be anonymous at withdrawal would put a deposit which is
held in escrow like a good behavior bond.  If they double spend they
are not identified but their escrow account is frozen.  The account
could optionally be based on is-a-person credentials as a further
inconvenience for double-spenders to have an account frozen, though is
a-person-credentials implies strong identification to a Registration
Authority.  The actual withdrawal could then be made from the
anonymous account hiding identity from the bank.  However similar
effect can be achieved with accountless operation, which brings us to
your next comment...
(btw There are some real world analogies to escrow accounts, though this
one has nothing to do with the anonymity aspect.  Upon moving to
Canada, not being a Canadian citizen, I found that I could only get a
credit card by providing a deposit of 2x the value of the "credit"
limit which is held in an escrow account.
Another example would be having to give a deposit to get mobile phone
for people with poor credit ratings.  Also in Europe pay as you go,
cash only mobile phone usage is popular due to credit elegibility
reasons also I think.  You can plunk down a 10 pound note and walk out
with a mobile phone with air time on it, you can buy more air time
With Brands off-line coins you _can_ anonymously exchange off-line
coins at the bank if you choose to set it up that way.
Technically how this works is using an attribute hiding refreshing
protocol which issues a new fresh coin with the same attributes
(identity, denomination) as the previous spent coin while revealing
only some negotiated sub-set of the attributes of the old coin (in
this case denomination), so the new coin is unlinkable for the bank
and yet the bank is assured that it will contain the same identity
that was certified originally so the bank will be able to recover the
identity if it is later double spent.  There is a description of this
protocol in section 5 of [3].  This works for off-line coins.  For
transferable off-line coins you need additionally to update the
0-value last holder coin to match the value of the coin being
exchanged, using the updating protocol (see section 5.2.1 in [2], or
probably [1] may have some discussion).
The cops would not be tracking down a double-spending user for you
(the user who was left with a double-spent coin), they would be
tracking down the double-spending user for the bank of Timbuktu who
now owes the bank money.  The bank would expect the local cops to
track down someone who attempted to defraud them.
Agree, this is a limitation of the anonymous escrow account approach.
Also, much of this would be better limited with a smart-card setting
as the barrier to double-spending is much higher, and security is also
much higher (against rogue software on OSes with weak security).
In the case where a bank does not anyway directly provide accountless
operation (exchanging old coins for fresh coins without requiring the
association of the exchange with an account) a money changer is simply
another user or merchant who fulfils the same function -- exchanging
old coins for fresh coins, presumably in this case for some
transaction charge.
A second spend would allows the person to prove that it was their
coin.  But simple text comparison already allows them to recognize it
was their coin.
However if the bank offers accountless exchange for example, it's not
clear what colluding with the bank achieves for an isolated user, they
won't by doing so be able to directly identify anyone.
If anything transferable off-line cash in this sense offers more payee
anonymity, not less than the standard online Chaum protocol as
implemented by digicash for example.  (Recall it was a designed
feature of that system that a payer could collude with the bank to
identify the person they spent the money with in case they felt they a
victim of fraud by the merchant).  In this case the person who would
be somewhat idenitifed (only in as much as he is anyway identified by
his connection to the bank with accountless operation) is likely to be
someone entirely unrelated to your spending as the coin would most
probably have changed hands a number of times.  (There is the double
blind Chaum variant, but it is even less convenient as both the payer
and payee have to be online at what becomes a simultaneous withdrawl,
spend and deposit time.)
It may still be interesting to prevent chain linkability without
collusion from the bank by individuals or groups of colluding users.
Note as I described above accountless is possible with transferable
off-line (and with off-line) coins also (depending on the scheme -- it
is with Brands, but I don't know of anyway to do that with Ferguson's
single term off-line coin variant of Chaum's off-line protocol).
This seemed to be the only feature you claimed that suggested online
only coins offered a feature with anonymity advantage not available
with off-line or off-line transferable coins.
So I still maintain off-line and off-line transferable give you
_everything_ you get from online coins as a user choice, as the user
can still use them as online coins if they choose, plus as I argued in
my previous message they give you a number of extra features and extra
[1] "An Efficient Off-line Electronic Cash System Based on the
Representation Problem, Stefan Brands, CWI tech report CS-R9323
[2] "Rethinking public key infrastructures and digital certificates -
building in privacy", Stefan Brands, PhD Thesis, MIT press
[3] "A Technical Introduction to Digital Credentials", Stefan Brands,
to appear Journal of Information Security,

@_date: 2002-04-09 11:11:36
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
You can't outright counterfeit technically as the recipient of each
coin checks that it's correctly formed, and authenticated by the bank,
and that the chain of spends are all bound together.  By doing this
the user is assured that either the coin will not be double-spent, or
the bank will identify the double spender when the coin is deposited.
You might reasonably expect the bank to deal with double-spending
itself and give the depositor fresh money regardless of double spent
If you use the normal approach of putting the identity in the coin,
you can't double-spend anonymously.
Building up technology trust is harder yes.  But that I guess is
largely marketing and reputation.  Most people probably don't
understand the security mechanisms in place with credt-cards either
(PIN offset on card etc.), or even more the more secure smart-card
based credit cards used in some parts of the world.

@_date: 2002-04-09 02:10:17
@_author: Adam Back 
@_subject: Re: all about transferable off-line ecash (Re: Brands off-line tech) 
Anonymous gives some comments on some deficiencies in the properties
of the transferable ecash schemes to date:
While it is true that the coins are by unavoidably linkable, the
linkability will only leak information where a user happens to see the
same coin twice as it gets re-spent, as he can recognize this.  As the
chain length is also visible he knows how many hands it has gone
through since he spent it.  However he has no way to identify the
intermediate payers except the last payer.
The amount of identifying information the immediate payer discloses is
up to that payer, though some identification may be relatively hard to
avoid if there is no anonymous communication link used.
So in general the shorter the intermediate chain the more revealing
about the first and last payer in the intermediate chain the
observation is.  The more people who collude, the more chance their is
that the colluding group can find samples of respent coins and so
identify or gain information about the transactions of a target payer
or payee.
The transaction information leakage from the linkability may be fairly
limited in practice -- for example by comparison how much transaction
leakage would you expect to get as an individual or small group of
coluding individuals if you write down the serial number on a bank
note and wait until you see it again -- or even if a bank were to
perform the same experiment, and they are far more likely to see it
again due to volume.  The issue will tend to be worse in small payment
Clearly it's not ideal, and it is useful to think about things you
could do to improve the situation:
- One thing that could be done to obscure this is to add a few extra
random spending hops (say 0-2) which the user can do himself by
spending to himself, though this comes at some extra space overhead.
The recipient won't be able to distinguish self-spends from
third-party spends.
- Another defense would be to use third party money-changer to
exchange coins for different coins.  Basically to shuffle coins around
a bit so that receiving a coin from someone with a short enough chain
length between current and recognised spend to normally leak some
information will no longer gain useful information.
Ideas for more robustly fixing it:
- Perhaps there is a way to encrypt the original chain with the bank's
public key with a randomizable encryption algorithm such as Elgamal
and yet retain sufficient proofs that the encrypted chain contains
coin transcripts which would identify the appropriate part if the coin
were double spent, and such that people handling the coin are assured
of it's issue value.
Also here are some comments on the conclusions:
Online actions are harder to perform anonymously, therefore added
flexibility to behave more off-line is good for anonymity.  Off-line
and transferable off-line coins add several new features which are
useful to an anonymous user:
- ability to transfer rather than deposit, so better hiding payee
identity from bank for payers that want this (there are good uses for
payee privacy as well as payer privacy)
- accountless operation is better for privacy than forcing payments to
be deposited and withdrawn as it also gives a user privacy of
transaction volume; however accountless operation where you have to
connect to the bank in real time (online protocol) makes it more
difficult to remain anonymous due to the need for interactive
low-latency communication
- a money changer is much easier and more realistic to operate with
off-line transferability -- it's basically impossible for the bank to
detect with off-line transferability.  With online coins a money
changer would stand out exchanging a lot of coins through it's account
(with forced-account option), plus even with accountless online
exchange of fresh coins at the bank it's harder for the money changer
to hide it's identity due to it's necessarily high bandwidth,
low-latency interactive communication link with the bank (precisely
the kind of anonymity which is hardest to achieve).
- also you don't have to trust the money changer with off-line
transferability -- he does not see blinding factors (though of course
there is the double blind protocol variant with online chaum, this
double blinding is not possible with other protocols, eg Brands I
think).  Conveniently the money changer also has an incentive to not
cheat if the bank is hostile to this kind of operation -- or he will
be identified, so users can be fairly confident in the unlikelihood of
the money-changer double spending.
- it becomes more plausible to have a hidden bank as the communication
links to it could be quite strongly protected with high latency more
anonymous communication links such as mixmaster, and usenet message
pools, while still allowing interactive payments and re-spending.
So in summary I think transferable off-line gives you a large number
of advantages.  The features are entirely selectable by the different
users.  People who want immediate clearing and to avoid all risk of
being left with a double spent coin and having to go to the bank and
wait for the bank to extract the funds from the double spender can
still achieve that as it as online clearing is a sub-protocol.  People
who want to trade-off some of that guarantee for the kinds of
applications and features discussed above can do so where they could
not with a purely online payment scheme.
Off-line coins just offer an extra optional feature for the user, any
user who chooses can instead use them as online coins.  So I would
argue off-line coins are better than online coins.
Tranferable off-line coins allow all kinds of cool anonymity features
as described above, I also argued above that the linkability
deficiency can somewhat defended against.
And transferable off-line coins add yet more flexibility, while again
not preventing online clearing for those that prefer it.  While some
of the features have the linkability artifact, those features are
optional and the user has free choice to select methods to avoid
entirely or defend against linkability by any of the available methods
respectively fetching fresh online coins, using money-changers to do
the same more off-line, and self re-spending to add confusion.  Hence
transferable off-line coins are already superior to both
non-transferable off-line coins and online coins due to the selection
of choice of new features and trade-offs offered to the users.  All we
need now is a way to more robustly defeat linkability.

@_date: 2002-04-07 21:43:37
@_author: Adam Back 
@_subject: all about transferable off-line ecash (Re: Brands off-line tech) 
A short while ago I wrote this comment on the dbs list describing a
transferable off-line ecash idea I'd been thinking about with
While looking for a reference to something else to do with ecash I
found that a somewhat general way to convert an off-line ecash scheme
into a transferable off-line ecash scheme has already been proposed:
See Section 8 of "Easy come - easy go divisible cash", by Agnes Chan,
Yair Frankel and Yiannis Tsiounis [1] references [vA90] H van
Antwerpen "Electronic cash", Master's thesis, CWI 1990 as providing a
general way to make coins transferable from a shop (who now acts as a
payer) to another payee.
They say of van Antwerpen's approach:
"8 Extensions and open problems Transferability: There is a general
method with which a coin can be transferred from the shop (who now
acts as a payer) to another payee, proposed in [vA90]. The method
preserves the anonymity of the shop and is applicable to all anonymous
off-line e-cash schemes. The coins grow upon each transfer, but
[CP93a] showed that this is inevitable, and the approach is
asymptotically optimal. Intuitively, the shop obtains a "blank"
(zero-valued) blind coin from the bank, and includes it in the hash of
the random challenge to the user (for exact payments divisible "blank"
coins can be obtained). Then the shop can transfer the payment by
"spending" the blank coin with a payee. Note that the blank coin is
"bound" to the original payment (since it is included in the random
challenges used for that payment), while the shop cannot over-spend,
or it is identified. The shop only needs to contact the bank (in an
off-line manner) in order to obtain "blank" coins; finding algorithms
to withdraw multiple (unlinkable) "blank" coins faster than performing
multiple withdrawals in parallel is a problem pending further
So I'm presuming from this description that the trick is, you just
bind a 0-valued fresh coin to the spent coin.  If the 0-valued coin
with spent-coin is double spent it will be the double-spender who is
caught.  The coins here also grow as there will be the original coin,
plus a chain of appended 0-valued fresh-coins taking ownership at each
(My experiments were somewhat related in that I had got to the stage
of a coin and receipt which grew once with each spend, but had not
thought of directly using a 0-value fresh-coin, and instead was
experimenting with more complex requirement of having the payer and
payee run a protocol to change the embedded off-line fraud information
(eg the identity) as the coin changed hands).
Also they note as I figured that it is unavoidable that the coins must
grow for this type of scheme and reference a proof about this in
[CP93a].  But I would think that is anyway obvious for simple storage
Also in trying to find an electronic copy of [vA90], I see that Brands
[2] notes that [vA90] also applies to his ecash scheme.  Getting
interesting.  (Divisibility of Chan et al is nice but it still has
linkability across the divided coins, and I'm not sure how much
analysis Chan's scheme has compared to Brands).
However I still can't find [vA90].  Hans van Antwerpen seems to have
disappeared off the 'Net.
So at a high-level this means you can have off-line ecash with
fraud-tracing the identifies the double-spender, and the participants
do not need to involve the bank they can just transfer coins
peer-to-peer for a fairly arbitrary number of hops.  (The only
restriction being that the coin increases in size by one coin's size
at each spend).
I think depending on the scheme involved (this would be the case for
Brands) you could potentially distribute the double spend database.
Ie there is nothing to stop the users or 3rd party services having
double spend databases to improve the odds of catching double spenders
before the coin gets back to the bank.  This also means that a bank
could realistically be more off-line yet, not needing high
availability, or even itself being pseudonymous (which tends to mean
less online -- eg behind a nymserver account) provided there was some
receiver anonymous way to transfer resources of whatever kind the
ecash represents to it.
Potentially the operation of running the double-spend database could
be separated from the coin minting part which would also occasionally
exchange fresh-coins for multiply exchanged coins which were getting
unworkably large.
Techniques like merkle authentication trees as used in time-stamp
servers where a server can commit to a values publication without
seeing the value and without being able to later retract that
statement could perhaps be used to deter or prevent conflicts of
interest between the nodes participating in the distributed double
spending database.
[1] "Easy come - easy go divisible cash"
 [2] An efficient off-line electronic cash system based on the
respresentation problem, tech report CS-R9323, 1993 [va90] "Electronic Cash", Hans van Antwerpen, Master's Thesis
[CP93a] "Transferred cash grows in size", D Chaum and T Pedersen
Eurocrypt 92

@_date: 2002-04-09 23:47:16
@_author: Adam Back 
@_subject: pre-paid/pay-as-you go cell phone service (Re: all about transferable off-line ecash) 
But from what I saw it was around 4x more expensive.  A SIM with a
years contract (all paid up front) is pretty easy to obtain for 10 -
50 pounds depending on number of free minutes included.
Yes other things being equal I would find the anonymity aspects of
buying SIM without contract etc quite cool if there was not such a
price disparity.

@_date: 2002-04-11 23:43:40
@_author: Adam Back 
@_subject: Re: overcoming ecash deployment problems (Re: all about transferable off-line ecash) 
To be more concrete: there are already apparently e-gold backed credit
cards.  So why not Everquest virtual platinum backed credit cards for
spending your Everquest acquired wealth directly in the real world.  I
would have thought Sony could have a lot of fun and gain some
interesting press from going in this direction -- blurring the line
between the VR and the real world -- rather than trying pretty much
ineffectivley and hopelessly to stop people trading virtual platinum.

@_date: 2002-04-11 23:38:48
@_author: Adam Back 
@_subject: Re: overcoming ecash deployment problems (Re: all about transferable off-line ecash) 
This is all evidence that there is lots of demand for anonymous
payment systems, the problems are in deployment and interfacing to the
existing payment systems for in and out exchange to make it
sufficiently convenient.  (An convenient exchange in and out could
bypass the lack of merchant acceptance -- your payment to a merchant
accepting only credit cards would be just a conversion to a one-use
credit card number funded for the purpose of the transaction with
anonymous ecash.)
Well online porn industry would surely be a killer app, natural desire
for privacy, higher than normal skepticism about giving credit cards
to merchants.  I'm not aware of any attempts to capitalize on this
market.  If I recall even there were claims that MTB tried to
discourage this kind of application by closing selected merchant
Online gambling might be another plausible application.
Well actually to elaborate what I meant is we now (as opposed to 5 or
more years ago) have a large scale new payment related component to
play with which has interesting properties for ecash.
I was suggesting that the ecash mint operator exchange ecash directly
for Everquest currency (virtual "platinum pieces").  The Everquest VR
is a place in cyberspace, and there are people who make their living
by trading and selling virtual artifacts acquired in the game.  The VR
world is of quite spectacular scale.  To give a few factoids about the
scale virtual worlds from some recent articles I read "Norrath's per
capita income is roughly between Russia and Bulgaria. Or put another
way, Norrath is the 77th richest country in the world."
"Revenues from online gaming were already $208m in 2000 and some
estimates see that figure rising to $1.7bn by 2004. Sony's monthly
revenue from EverQuest alone is estimated to be $3.6m. In the far
east, virtual worlds earn more than that. The most successful online
role-playing game in the world is South Korea's Lineage, which has 2.5
million subscribers and is played in one of every eight households."
(both from:
So there is already a fairly liquid market in virtual platinum pieces,
despite Sony's complaints and attempts to stop this.  Given the
expected growth and richness of VRs, it becomes more plausible that
online gambling and online porn itself could take place in these
virtual worlds with their ready made currency systems which have
already overcome the chicken-and-egg problems to some extent.
If they grew large enough their acceptance, or an ecash system backed
in them, might spill over into the real world and allow purchase of
services on the web, or even physical goods.
Yes.  Magic Money suffered that problem.  My thoughs were that
Everquest et al may allow one to leap-frog the boot-strapping problem.

@_date: 2002-04-22 23:37:50
@_author: Adam Back 
@_subject: Re: objectivity and factoring analysis 
For people who aren't following as closely I think it would be useful
to remind that this is an estimate of the building cost and running
time of one aspect of computation in the overal proposal.  (You give a
rough running-time estimate which some might misunderstand).
The _overall_ running time of the algorithm and whether it is even any
faster or more economical than existing algorithms remains _unknown_
due to the asymptotic result which the experiment is intended to
If the hardware were to be built it might for example turn out that
the asymptotic result may only start to offer speedups at far larger
key sizes and if this were the case, depending on the results it could
be that the approach turns out to offer no practical speed-ups for the
for-seeable future.
Or it might turn out that it does offer some incremental improvement,
and even that key sizes should be increased.
But right now no one knows.
btw. As disclaimed in the original post no insult was intended --
merely more accurate information given the somewhat wild speculations.

@_date: 2002-04-19 13:51:59
@_author: Adam Back 
@_subject: objectivity and factoring analysis 
I'd just like to make a few comments about the apparently unnoticed or
unstated conflicts of interest and bias in the analysis surrounding
Bernstein's proposal.
The following is not intended to trample on anyone's ego -- but I
think deserves saying.
- I'm not sure any of the respondents so far except Bernstein have
truly understood the math -- there are probably few who do, factoring
being such a narrow research area.
- Dan Bernstein stated that it is not easy to estimate the constants
involved to know whether the asymptotic result affects currently used
key sizes; he stated that the conclusion should be considered unknown
until experimental evidence is gained.
- Nicko van Someren -- the person credited with originally making the
exaggerated, or at least highly worst case interpretation at the FC02
panel -- has a conflict interest -- hardware accelerator gear that
ncipher sell will be more markedly needed if people switch to 2048 or
larger keys.  Nicko has made no public comments in the resulting
- Ian Goldberg also on the panel quickly distanced himself from van
Someren's claim, as Lucky's earlier mail could have been read to imply
Goldberg had also agreed with van Someren's claim.
- RSA's FAQ down playing the result seems relatively balanced though
they have an incentive to downplay the potential of Bernstein's
approach.  They have a history of producing biased FAQs: for example
previously the ECC FAQ where they compared ECC unfavorably to RSA.
The FAQ was removed after they licensed tech from certicom and
included ECC in BSAFE.
- Bob Silverman, former RSA factoring expert, observes on sci.crypt,
- Bruce Schneier's somewhat downplaying comments, as far as I know
Bruce isn't an expert on factoring and he doesn't credit anyone who is
in his report.  Bruce's comments lately seem to have lost much of
their earlier objectivity -- many of his security newsletters lately
seem to contain healthy doses of adverts for counterpane's managed
security offering, and calls for lobbying and laws requiring companies
to use such products for insurance eligibility.
- Lucky on the other hand suggested a practical security engineering
approach to start to plan for possibility of migrating to larger key
sizes.  Already one SSH implementation added a configuration option to
select a minimum key size accepted by servers as a result.  This seems
like a positive outcome.  Generally the suggestion to move to 2048 bit
keys seems like a good idea to me.  Somewhat like MD5 -> SHA1, MD5
isn't broken for most applications but it is potentially tainted by a
partial result.  Similarly I would concur with Lucky that it's prudent
security engineering to use 2048 bit keys in new systems.
Historically for example PGP has had similar migrations from minimum
listed key sizes for casual use from 512 -> 768 -> 1024 over the
years.  The progression to 2048 is probably not a bad idea given
current entry level computer speeds and possibility of Bernstein's
approach yeilding an improvement in factoring.
The mocking tone of recent posts about Lucky's call seems quite
misplaced given the checkered bias and questionable authority of the
above conflicting claims we've seen quoted.

@_date: 2002-04-27 01:31:25
@_author: Adam Back 
@_subject: Re: disk encryption modes (Re: RE: Two ideas for random number generation) 
Right, it sounds like the same approach I alluded to, except I didn't
use a salt -- I just used a fast pseudon random number generator to
make the IV less structured than using the block number directly.
I did some experiments with a used disk and found that if you use the
block number directly for the IV, with CBC mode the block number and
plaintext difference cancel to result in the same input text to the
block cipher, resulting in the same ciphertext in a fair proportion of
cases (don't have the figures handy, but clearly this
not-insignificant number of collisions represents a leakage about the
plaintext on the disk).
The with aforementioned fast pseudo-random number generator I got no
collisions on the disk sample size (10Gig disk almost full of windows
application software and data).
I figure that's good empirical evidence of the soundness of the
approach, however another glitch may be if you consider that the
attacker can work partly from the inside -- eg influencing the
plaintext choice, as well as having read-only access to the ciphertext

@_date: 2002-04-26 22:56:39
@_author: Adam Back 
@_subject: disk encryption modes (Re: RE: Two ideas for random number generation) 
The weakness is not catastrophic, but depending on your threat model
the attacker may see the ciphertexts from multiple versions of the
plaintext in the edit, save cycle.
This could happen for example the attacker has read access to your
disk, or if the attacker gained temporary physical access to the
machine but didn't have enough resources to install software trojan,
or if good disk checksumming is in place, didn't have enough resources
to install hardware trojan.
Performance is often at a premium in disk driver software --
everything moving to-and-from the disk goes through these drivers.
Encrypt could be slow, encrypt for IV is probably overkill.  IV
doesn't have to be unique, just different, or relatively random
depending on the mode.
The performance hit for computing IV depends on the driver type.
Where the driver is encrypting disk block at a time, then say 512KB
divided (standard smallest disk block size) into AES block sized
chunks 16 bytes each is 32 encrypts per IV geenration.  So if IV
generation is done with a block encrypt itself that'll slow the system
down by 3.125% right there.
If the driver is higher level using file-system APIs etc it may have
to encrypt 1 cipher block size at a time each with a different IV, use
encrypt to derive IVs in this scenario, and it'll be a 100% slowdown
(encryption will take twice as long).
That's typically not practical, not possible, or anyway very
undesirable for performance (two disk hits instead of one),
reliability (write one without the other and you lose data).
CBC isn't ideal as described above.  Output feedback modes like OFB
and CTR are even worse as you can't reuse the IV or the attacker who
is able to see previous disk image gets XOR of two plaintext versions.
You could encrypt twice (CBC in each direction or something), but that
will again slow you down by a factor of 2.
Note in the file system level scenario an additional problem is file
system journaling, and on-the-fly disk defragmentation -- this can
result in the file system intentionally leaving copies of previous or
the same plaintexts encrypted with the same key and logical position
within a file.
So it's "easy" if performance is not an issue.
Another approach was Paul Crowley's Mercy cipher which has a 4Kbit
block size (= 512KB = sector sized).  But it's a new cipher and I
think already had some problems, though performance is much better
than eg AES with double CBC, and it means you can use ECB mode per
block and key derived with a key-derivation function salted by the
block-number (the cipher includes such a concept directly in it's
key-schedule), or CBC mode with an IV derived from the block number
and only one block, so you don't get the low-tide mark of edits you
get with CBC.
But Mercy as a set of design criteria is very interesting for this

@_date: 2002-04-28 15:02:46
@_author: Adam Back 
@_subject: Re: disk encryption modes 
I saw something similar also discussed which was:
which seems essentiall the same.
So if you mean the approach in 1311 you referenced below:
are you sure it's not vulnerable to splicing attacks (swapping
ciphertext blocks around to get a partial plaintext change which
recovers after a block or two)?  CBC mode has this property, and this
mode seems more like CBC in CBC than a CBC-MACed CBC-encrypted message

@_date: 2002-05-23 18:10:01
@_author: Adam Back 
@_subject: why OpenPGP is preferable to S/MIME (Re: NAI pulls out the DMCA 
Certificate authorities also can forge certificates and issue
certificates in fake names if asked by government agencies.  S/MIME is
too much under central control by design to be a sensible choice for
general individual use.
The central control is doubtless primarily motivated by the hopes of
turning a profit selling certificates to allow people to exchange
secure email etc.
OpenPGP's WoT provides a superset of S/MIME's hierarchically
controlled answer to identification and trust -- you can still have
CAs with OpenPGP, plus you can cross check and peer-to-peer certify
people you wish to interact with and so not need to trust some
untrustworthy and generally incompetent organisation.  (Verisign for
example issued someone a microsoft code signing cert).

@_date: 2002-05-14 20:00:42
@_author: Adam Back 
@_subject: convenience and advantages of cash (Re: Eyes on the Prize...not 
You can apparently get Canadian $1,000 notes too, not that I've ever
seen one.  That would be worth almost exactly the same as 1000 swiss
If you get a bundle of 50 GBP notes from a bank in the UK they put
them in a little sealed bag containing 10 notes (500 pounds).  That
note collection is convenient for counting etc for larger items also.
Largest thing I bought cash was 2,000 GBP for a 2nd hand car some
years ago.  I did toy with trying to buy a house with paper cash to
see if it could be done, but I didn't bother in the end -- but I think
that all that would have happened is the seller's lawyer would go to
the bank and pay it in to make sure it's good.
I've also moved more than 2,000 GBP that between bank accounts and
investment accounts in the past -- withdraw from current account
10,000 GBP, walk across the street and pay into another institutions
investment account and the money is instantly available to write a
check, and accrues interest from that day, rather than 3 days later.
The bank charges 20 GBP or more to do the same day transfer
electronically (CHAPs), where as the "no fee" option is BACs and takes
3 working days and they keep the interest on your money while it's

@_date: 2002-05-09 17:11:58
@_author: Adam Back 
@_subject: Re: UK e-money legal, sort-of 
Do you know is that minimum or maximum of those two figures?  ie if
you have 2% of capital you issue is that enough or does it have to be
larger of those.  GBP 600K (USD 900K) is still a lot of money for a
small scale operation.  If it were the former it might be more
plausible that someone might set something up as a hobby operation.
The tricky part as ever will be putting money into the system if it's
anonymous ecash, to limit fraud.  Interfacing anonymous to
non-anonymous transaction systems is a problem.  The convenient
non-anonymous transactions systems (credit cards, debit cards)
typically are quite vulnerable to fraud and have weak security
What does the redeemable within five days mean -- that this is the
maximum processing time for in-transfers or for out-transfers?

@_date: 2002-05-23 20:58:48
@_author: Adam Back 
@_subject: Re: why OpenPGP is preferable to S/MIME (Re: NAI pulls out the 
This won't achieve the desired effect because it will just destroy the
S/MIME trust mechanism.  S/MIME is based on the assumption that all
CAs are trustworthy.  Anyone can forge any identity for clients with
that key installed.  S/MIME isn't really compatible with the web of
trust because because of the two tier trust system -- all CAs are
assumed trustworthy and all users are not able to sign anything.  By
issuing a key and revealing it's private key, you elevate a rogue user
to being a CA and then the system would be broken.
I think you'd have to do it in reverse to stand a chance if you
literally published the private key -- they're never going to add the
public key for a known compromised private key.  Also it costs lots of
money, and takes some time to take effect.

@_date: 2002-05-25 04:13:36
@_author: Adam Back 
@_subject: Re: S/MIME and web of trust (was Re: NAI pulls out the DMCA 
The S/MIME aware MUAs do not ignore the trust delegation bit.
Therefore you can not usefully sign other certs with a user grade
certificate from verisign et al.  If you make your own CA key (with
the trust delegation bit set) and self-sign it, S/MIME aware MUAs will
also flag signatures made with it as invalid signatures because your
self-signed "CA" key is not signed by a CA in the default trusted CA
key database.
While it is true that you can extend X.509v3 I don't see how useful it
would be to add a WoT extension until it got widely deployed.
Recipient MUAs will at best ignore your extensions, and worse will
fail on them until support for such an extension is deployed.  I view
the chances of such an extension getting deployed as close to nil.
The S/MIME MUA / PKI library / CA cartel has a financial incentive to
not deploy it -- as they view it as competition to the CAs business.

@_date: 2002-06-06 16:12:42
@_author: Adam Back 
@_subject: Re: overcoming ecash deployment problems (Re: all about transferable off-line ecash) 
I think you are assuming things about rational economic behavior when
a money system is subject to high deflation.
Consider during periods of high inflation people don't like holding
money, as it devalues too fast.  They will hold interest bearing
deposits instead.
During periods of high deflation, they will hold cash if it is the
most attractive "investment".  The result will be shortage of cash,
for people who actually want to use it to make purchases because
investors will buy all of it.
Perhaps there are some government monetary systems in history which
had this problem.  For example gold with sudden shortage of gold
supply, or similar.

@_date: 2002-06-12 14:39:16
@_author: Adam Back 
@_subject: Re: What's with all the spam?... 
is quite close to that.
The moderation policy is:
- obvious spam gets dropped.
- one-line pointers to news articles will tend to get dropped.
- news articles posted in full without comment will tend to get dropped.
- content will tend to get passed, even if it's off-the-wall
  (eg. Xenix Chainsaw Massacre). - submissions from high signal posters will tend to get passed.
- the list is read-only. submissions should go to cypherpunks
  or another CDR node.
- cypherpunks will remain unfiltered and unmoderated.
To subscribe, send the text "subscribe cypherpunks-moderated" to
it's archived here:

@_date: 2002-06-26 19:37:12
@_author: Adam Back 
@_subject: Re: Ross's TCPA paper 
Hear, hear!  First post on this long thread that got it right.
Not sure what the rest of the usually clueful posters were thinking!
DRM systems are the enemy of privacy.  Think about it... strong DRM
requires enforcement as DRM is not strongly possible (all bit streams
can be re-encoded from one digital form (CD->MP3, DVD->DIVX),
encrypted content streams out to the monitor / speakers subjected to
scrutiny by hardware hackers to get digital content, or A->D
reconverted back to digital in high fidelity.
So I agree with Bear, and re-iterate the prediction I make
periodically that the ultimate conclusion of the direction DRM laws
being persued by the media cartels will be to attempt to get
legislation directly attacking privacy.
This is because strong privacy (cryptographically protected privacy)
allows people to exchange bit-strings with limited chance of being
identified.  As the arms race between the media cartels and DRM
cohorts continues, file sharing will start to offer privacy as a form
of protection for end-users (eg. freenet has some privacy related
features, serveral others involve encryption already).
There is lots of technical difference.  When was the last time you saw
your doctor use cryptlopes, watermarks etc to remind himself of his
obligations of privacy.
The point is that with privacy there is an explicit or implied
agreement between the parties about the handling of information.  The
agreement can not be technically *enforced* to any stringent degree.
However privacy policy aware applications can help the company avoid
unintentionally breaching it's own agreed policy.  Clearly if the
company is hostile they can write the information down off the screen
at absolute minimum.  Information fidelity is hardly a criteria with
private information such as health care records, so watermarks, copy
protect marks and the rest of the DRM schtick are hardly likely to
Privacy applications can be successful to the in helping companies
avoid accidental privacy policy breaches.  But DRM can not succeed
because they are inherently insecure.  You give the data and the keys
to millions of people some large proportion of whom are hostile to the
controls the keys are supposedly restricting.  Given the volume of
people, and lack of social stigma attached to wide-spread flouting of
copy protection restrictions, there are ample supply of people to
break any scheme hardware or software that has been developed so far,
and is likely to be developed or is constructible.
I think content providors can still make lots of money where the
convenience, and /or enhanced fidelity of obtaining bought copies
means that people would rather do that than obtain content on the net.
But I don't think DRM is significantly helping them and that they ware
wasting their money on it.  All current DRM systems aren't even a
speed bump on the way to unauthorised Net re-distribution of content.
Where the media cartels are being somewhat effective, and where we're
already starting to see evidence of the prediction I mentioned above
about DRM leading to a clash with privacy is in the area of
criminalization of reverse engineering, with Skylarov case, Ed
Felten's case etc.  Already a number of interesting breaks of DRM
systems are starting to be released anonymously.  As things heat up we
may start to see incentives for the users of file-sharing for
unauthorised re-distribution to also _use_ the software anonymsouly.
Really I think copyright protections as being exploited by media
cartels need to be substantially modified to reduce or remove the
existing protections rather than further restrictions and powers
awareded to the media cartels.

@_date: 2002-06-26 19:37:12
@_author: Adam Back 
@_subject: Re: Ross's TCPA paper 
Hear, hear!  First post on this long thread that got it right.
Not sure what the rest of the usually clueful posters were thinking!
DRM systems are the enemy of privacy.  Think about it... strong DRM
requires enforcement as DRM is not strongly possible (all bit streams
can be re-encoded from one digital form (CD->MP3, DVD->DIVX),
encrypted content streams out to the monitor / speakers subjected to
scrutiny by hardware hackers to get digital content, or A->D
reconverted back to digital in high fidelity.
So I agree with Bear, and re-iterate the prediction I make
periodically that the ultimate conclusion of the direction DRM laws
being persued by the media cartels will be to attempt to get
legislation directly attacking privacy.
This is because strong privacy (cryptographically protected privacy)
allows people to exchange bit-strings with limited chance of being
identified.  As the arms race between the media cartels and DRM
cohorts continues, file sharing will start to offer privacy as a form
of protection for end-users (eg. freenet has some privacy related
features, serveral others involve encryption already).
There is lots of technical difference.  When was the last time you saw
your doctor use cryptlopes, watermarks etc to remind himself of his
obligations of privacy.
The point is that with privacy there is an explicit or implied
agreement between the parties about the handling of information.  The
agreement can not be technically *enforced* to any stringent degree.
However privacy policy aware applications can help the company avoid
unintentionally breaching it's own agreed policy.  Clearly if the
company is hostile they can write the information down off the screen
at absolute minimum.  Information fidelity is hardly a criteria with
private information such as health care records, so watermarks, copy
protect marks and the rest of the DRM schtick are hardly likely to
Privacy applications can be successful to the in helping companies
avoid accidental privacy policy breaches.  But DRM can not succeed
because they are inherently insecure.  You give the data and the keys
to millions of people some large proportion of whom are hostile to the
controls the keys are supposedly restricting.  Given the volume of
people, and lack of social stigma attached to wide-spread flouting of
copy protection restrictions, there are ample supply of people to
break any scheme hardware or software that has been developed so far,
and is likely to be developed or is constructible.
I think content providors can still make lots of money where the
convenience, and /or enhanced fidelity of obtaining bought copies
means that people would rather do that than obtain content on the net.
But I don't think DRM is significantly helping them and that they ware
wasting their money on it.  All current DRM systems aren't even a
speed bump on the way to unauthorised Net re-distribution of content.
Where the media cartels are being somewhat effective, and where we're
already starting to see evidence of the prediction I mentioned above
about DRM leading to a clash with privacy is in the area of
criminalization of reverse engineering, with Skylarov case, Ed
Felten's case etc.  Already a number of interesting breaks of DRM
systems are starting to be released anonymously.  As things heat up we
may start to see incentives for the users of file-sharing for
unauthorised re-distribution to also _use_ the software anonymsouly.
Really I think copyright protections as being exploited by media
cartels need to be substantially modified to reduce or remove the
existing protections rather than further restrictions and powers
awareded to the media cartels.

@_date: 2002-06-26 22:03:08
@_author: Adam Back 
@_subject: DRMs vs internet privacy (Re: Ross's TCPA paper) 
I don't mean that you would necessarily have to correlate your viewing
habits with your TrueName for DRM systems.  Though that is mostly
(exclusively?) the case for current deployed (or at least implemented
with a view of attempting commercial deployment) copy-mark
(fingerprint) systems, there are a number of approaches which have
been suggested, or could be used to have viewing privacy.
Brands credentials are one example of a technology that allows
trap-door privacy (privacy until you reveal more copies than you are
allowed to -- eg more than once for ecash).  Conceivably this could be
used with a somewhat online, or in combination with a tamper-resistant
observer chip in lieu of online copy-protection system to limit
someone for example to a limited number of viewings.
Another is the "public key fingerprinting" (public key copy-marking)
schemes by Birgit Pfitzmann and others.  This addresses the issue of
proof, such that the user of the marked-object and the verifier (eg a
court) of a claim of unauthorised copying can be assured that the
copy-marker did not frame the user.
Perhaps schemes which combine both aspects (viewer privacy and
avoidance of need to trust at face value claims of the copy-marker)
can be built and deployed.
(With the caveat that though they can be built, they are largely
irrelevant as they will no doubt also be easily removable, and anyway
do not prevent the copying of the marked object under the real or
feigned claim of theft from the user whose identity is marked in the
But anyway, my predictions about the impending collision between
privacy and the DRM and copy protection legislation power-grabs stems
from the relationship of privacy to the later redistrubtion
observation that:
1) clearly copy protection doesn't and can't a-priori prevent copying
and conversion into non-DRM formats (eg into MP3, DIVX)
2) once 1) happens, the media cartels have an interest to track
general file trading on the internet;
3) _but_ strong encryption and cryptographically enforced privacy mean
that the media cartels will ultimately be unsuccessful in this
4) _therefore_ they will try to outlaw privacy and impose escrow
identity and internet passports etc. and try to get cryptographically
assured privacy outlawed.  (Similar to the previous escrow on
encryption for media cartel interests instead of signals intelligence
special interests; but the media cartels are also a powerful
Also I note an slip in my earlier post [of Bear's post]:
Ross Anderson's comments were also right on the money (as always).

@_date: 2002-06-26 22:03:08
@_author: Adam Back 
@_subject: DRMs vs internet privacy (Re: Ross's TCPA paper) 
I don't mean that you would necessarily have to correlate your viewing
habits with your TrueName for DRM systems.  Though that is mostly
(exclusively?) the case for current deployed (or at least implemented
with a view of attempting commercial deployment) copy-mark
(fingerprint) systems, there are a number of approaches which have
been suggested, or could be used to have viewing privacy.
Brands credentials are one example of a technology that allows
trap-door privacy (privacy until you reveal more copies than you are
allowed to -- eg more than once for ecash).  Conceivably this could be
used with a somewhat online, or in combination with a tamper-resistant
observer chip in lieu of online copy-protection system to limit
someone for example to a limited number of viewings.
Another is the "public key fingerprinting" (public key copy-marking)
schemes by Birgit Pfitzmann and others.  This addresses the issue of
proof, such that the user of the marked-object and the verifier (eg a
court) of a claim of unauthorised copying can be assured that the
copy-marker did not frame the user.
Perhaps schemes which combine both aspects (viewer privacy and
avoidance of need to trust at face value claims of the copy-marker)
can be built and deployed.
(With the caveat that though they can be built, they are largely
irrelevant as they will no doubt also be easily removable, and anyway
do not prevent the copying of the marked object under the real or
feigned claim of theft from the user whose identity is marked in the
But anyway, my predictions about the impending collision between
privacy and the DRM and copy protection legislation power-grabs stems
from the relationship of privacy to the later redistrubtion
observation that:
1) clearly copy protection doesn't and can't a-priori prevent copying
and conversion into non-DRM formats (eg into MP3, DIVX)
2) once 1) happens, the media cartels have an interest to track
general file trading on the internet;
3) _but_ strong encryption and cryptographically enforced privacy mean
that the media cartels will ultimately be unsuccessful in this
4) _therefore_ they will try to outlaw privacy and impose escrow
identity and internet passports etc. and try to get cryptographically
assured privacy outlawed.  (Similar to the previous escrow on
encryption for media cartel interests instead of signals intelligence
special interests; but the media cartels are also a powerful
Also I note an slip in my earlier post [of Bear's post]:
Ross Anderson's comments were also right on the money (as always).

@_date: 2002-07-31 20:34:35
@_author: Adam Back 
@_subject: document popularity estimation / amortizable hashcash (Re: 
I proposed a construct which could be used for this application:
called "amortizable hashcash".
The application I had in mind was also file sharing.  (This was
sometime in Mar 2000).  I described this problem as the "disitrbuted
document popularity estimation" problem.  The other aspect of the
problem is you have to distribute the popularity estimate and make it
accessible, so I think you want it to be workably compact (you don't
want to ship around 1 million hash collisions on the document hash).
Amortizable hashcash addresses this problem.
There is also some discussion of it here:

@_date: 2002-07-05 03:54:52
@_author: Adam Back 
@_subject: copyright restrictions are coercive and immoral (Re: Piracy is wrong) 
I agree with the Anonymous posters analysis.
I would further elaborate with regard to current copyright related
- parties are free to enter into NDA or complex distribution and use
contracts surrounding exchange of content or information generally as
anonymous describes, and this is good and non-coercive
- but that private contract places no burden on other parties if that
agreement is broken and the content distributed anyway.  This is
exactly analogous to the trade secret scenario where once the trade
secret is out, it's tough luck for the previous trade secret owner --
clearly it's no longer a secret.
- where I find current copyright laws at odds with a coercion free
society is in placing restrictions on people who did not agree to any
NDA contract.  ie. There are laws which forbid copying or use of
information by people who never entered into any agreement with the
copyright holder, but obtained their copy from a third party.
- in a free society (one without a force monopoly central government)
I don't think copyright would exist -- voluntary agreements -- NDAs of
the form anonymous describes -- would be the only type of contract.
- the only form of generally sanctioned force would be in response to
violence initiated upon oneself.
- if the media cartels chose to hire their own thugs to threaten
violence to people who did not follow the cartels ideas about binding
people to default contracts they did not voluntarily enter into, that
would be quite analogous to the current situation where the media
cartels are lobbying government to increase the level of the threats
of violence, and make more onerous the terms of the non-voluntary
contracts.  (Also in a free society individuals would be able to employ the
services of security firms protection services to defend themselves
from the media cartels thugs, as the media cartels would not have the
benefit of a force monopoly they have the lobbying power to bribe to
obtain enforcement subsidies).

@_date: 2002-07-03 22:33:01
@_author: Adam Back 
@_subject: personal freedom vs copyright (Re: Hayek was right. Twice.) 
There's been some recent discussion of ethics and markets relating to
copyright prompted by the Orwellian sounding overtones of the latest
Microsoft powergrab.
Seems about time to replay my periodic reminder that copyright is not
a black-and-white moral issue, it is merely a societal convention
which given public appetites for file sharing, and extreme difficulty
of preventing the public continuing apace (kazaa has some millions of
users online, with 2 peta-bytes of shared files and growing), it seems
to me that the natural evolution of laws etc would be for the laws
surrounding copyright be revoked as out-dated and no longer
applicable in an era of digital copying.  Without this adjustment
reality and content distribution laws are getting increasingly
out-of-synch, which is going to lead to some probable very undesirable
side effects in more laws further tilting the playing field in the
favor of the big media cartels, and starting to lead to very draconian
and Orwellian systems enforced under force of law.
Copyright is effectively a massive corporate welfare program to the
benefit of the media cartels at this point.  It's a business model
protection racket with the government providing the thugs at no
expense to the business.  No wonder the businesses that benfit from
this want to lobby to maintain this free enforcement corporate welfare
handout.  They get the financial benefits, and don't care about the
negative societal implications, such as described in Stallmann's
prescient essay on the long term implications of the coming brawl.
I don't see that the media cartels -- the main short-term benefactors
and lobbyists of the current and rapidly expanding copyright laws have
any moral right to have these conventions and corporate welfare
continue.  If society just said no, which it would appear of the
internet population they largely are, I think it likely we'd still
have movies, music etc., and that artists would continue to make money
and businesses associated with managing artists works would also make
money; the landscape might look a little different but so what.  Also,
even if one type of business model or content was no longer
economically supported, I can't see how that's a loss, or a bad thing

@_date: 2002-07-02 19:53:30
@_author: Adam Back 
@_subject: [OT] why was private gold ownership made illegal in the US? 
Just curious, but what was the rationale under which private posession
of gold was made illegal in the US?  It boggles the mind...

@_date: 2002-07-08 21:20:38
@_author: Adam Back 
@_subject: movie distribution post copyright (Re: Artists) 
But right now copies of recent release movies (post screen release,
but pre DVD/VHS relase) are not generally available in high quality
format, suitable for projecting.
So one way that the movie distribution industry could plausibly
continue to make money would be rather than the movie theatre being
subject to copyright laws forbidding them from copying and further
distributing, they would be under a private contract not to do that.
Actually I'm not sure what they're doing now -- it would seem likely
that both private contract and copyright are used -- the movie
distributors may easily want to impose more restrictions than those
directly imposed by default copyright.
Post copyright, with private contract only, the movie theatre would
have an interest to comply with the contract due to the penalties
agreed to in the contract, which might include fines, escrowed monies,
or no access to further releases.
The movie industry has so far been succesful from what I've seen in
preventing DVD quality copies being distributed prior to DVD release.
Publicly distributed copies of pre-DVD release movies are "Screeners"
obtained with a CAM corder in the theatre.  Early releases
(unauthorised distribution shortly before general public release) come
from journalists or their guests making screeners from the pre-release
screenings offered to journalists.
The advent of digital projection which doesn't have much deployment at
theatres yet may alter this equation as perhaps it would then become
easier for an insider (a theatre projectionist for example) to convert
the content into MPEG4/DIVX format and retain good quality.

@_date: 2002-07-12 19:13:33
@_author: Adam Back 
@_subject: Re: Rant: The U.S. facing the largest financial collapse ever 
Tim describes how US national debt may be as high as US$200k /
household.  Now some interesting question related questions are:
- who is that debt owed to?
- what proportion of current year US tax revenues go to service that
some of the debt may not be being serviced (no interest paid and just
left to increase -- eg pensions etc, but this just makes the problem
worse as the future debt will grow faster with no interest paid).
Some completely back of the envelope calculation: if the average US
household has an annual income of US$50k, and the interest rate on the
US national debt is 5%, that interest payments represent 20% of the
average US households gross income.  But isn't 20% fairly close to
what the average household's direct tax rate?
How close is the US to reaching a standstill where 100% of collectible
tax revenue goes to fund debt service, and all current spending comes
from increased future debt?

@_date: 2002-07-23 19:24:26
@_author: Adam Back 
@_subject: Re: Tunneling through hostile proxy 
This isn't just the default behavior; it's the only defined behavior
While it's _possible_ to do this, I've never heard of a server hosted
application that advertises that it's doing this.  I would think it
would be quite hard to get a CA to issue you a certificate if this is
what you intended to do with it (act as a general MITM on SSL
connections you proxy).
There have been applications which do this locally eg. a no longer
shipped product called SafePassage by c2.net, and achilles a SSL
debugger both of which are local proxies and both of which ask the
user to install a certificate allowing this when they are installed.
The installed certificate is self-signed however, and not issued by a
CA, as it is only valid for that user machine anyway, the user won't
want to buy a cert to authenticate information to their own machine,
it would be less secure to do so, and the user won't want to pay for
this certificate.
Is there any software actually doing this?  (I know wild card certs
are available, but would think a wild card cert on .com would be a
very dangerous thing for a CA to issue, and you'd hope browsers would
be smart enough to reject such certs).
This is what SafePassage et al do.

@_date: 2002-08-21 13:43:41
@_author: Adam Back 
@_subject: Re: alternate dos pgp client? 
I put together a list of openpgp related software at:
this includes library only code, and add on software.
Not sure about your questions about key versions, but I forwarded it
to Ulf Moeller and Len Sassaman (current maintainer of mix3).
From what I've seen mix3 (pgptest app) is the closest to providing a
command line.  There was also Tom Zerucha's reference openPGP code,
which is command line but it's alpha level code I think and no longer

@_date: 2002-08-12 18:30:00
@_author: Adam Back 
@_subject: Re: Palladium: technical limits and implications 
Peter Biddle, Brian LaMacchia or other Microsoft employees could
short-cut this guessing game at any point by coughing up some details.
Feel free guys... enciphering minds want to know how it works.
(Tim Dierks: read the earlier posts about ring -1 to find the answer
to your question about feasibility in the case of Palladium; in the
case of TCPA your conclusions are right I think).
I thought we went over this before?  My hypothesis is: I presumed
there would be a stub TOR loaded bvy the hardware.  The hardware would
allow you to load a new TOR (presumably somewhat like loading a new
BIOS -- the TOR and hardware has local trusted path to some IO
I don't know what leads you to this conclusion.
How would the OS or user mode apps communicate with trusted agents
with this model?  The TOR I think would be the mediator of these
communications (and of potential communications between trusted
agents).  Before loading a real TOR, the stub TOR would not implement
talking to trusted agents.
I think this is also more symmstric and therefore more likely.  The
trusted agent space is the same as supervisor mode that the OS runs
in.  It's like virtualization in OS360: there are now multiple "OSes"
operating under a micro-kernel (the TOR in ring -1): the real OS and
the multiple trusted agents.  The TOR is supposed to be special
purpose, simple and small enough to be audited as secure and stand a
chance of being so.
The trusted agents are the secure parts of applications (dealing with
sealing, remote attestation, DRM, authenticated path to DRM
implementing graphics cards, monitors, sound cards etc; that kind of
thing).  Trusted agents should also be small, simple special purpose
to avoid them also suffering from remote compromise.  There's limited
point putting a trusted agent in a code compartment if it becomes a
full blown complex application like MS word, because then the trusted
agent would be nearly as likely to be remotely exploited as normal
trusted-agents will also need to use OS services, the way you have it
they can't.
I don't think it's a big problem to replace a stub TOR with a given
TOR sometime after OS boot.  It's analogous to modifying kernel code
with a kernel module, only a special purpose micro-kernel in ring -1
instead of ring 0.  No big deal.
In TCPA which does not have a ring -1, this is all the TPM does
(compute metrics on the OS, and then have the OS compute metrics on
While Trusted Agent space is separate and better protected as there
are fewer lines of code that a remote exploit has to be found in to
compromise one of them, I hardly think Palladium would discard the
existing windows driver signing, code signing scheme.  It also seems
likely therefore that even though it offers lower assurance the code
signing would be extended to include metrics and attestation for the
OS, drivers and even applications.
I take this to mean that as stated somewhere in the available docs the
OS can not observe or even know how many trusted agents are running.
So he's stating that they've made OS design decisions such that the OS
could not refuse to run some code on the basis that a given Trusted
Agent is running.
This functionality however could be implemented if so desired in the

@_date: 2002-08-10 04:37:30
@_author: Adam Back 
@_subject: p2p DoS resistance and network stability (Re: Thanks, Lucky, 
The point that a number of people made is that what is said in the
article is not workable: clearly you can't ultimately exclude chosen
clients on open computers due to reverse-engineering.
(With TCPA/Palladium remote attestation you probably could so exclude
competing clients, but this wasn't what was being talked about).
The client exclusion plan is also particularly unworkable for gnutella
because some of the clients are open-source, and the protocol is (now
since original reverse engineering from nullsoft client) also open.
With closed-source implementations there is some obfuscation barrier
that can be made: Kazaa/Morpheus did succeed in frustrating competing
clients due to it's closed protocols and unpublished encryption
algorithm.  At one point an open source group reverse-engineered the
encryption algorithm, and from there the contained kazaa protocols,
and built an interoperable open-source client giFT
 but then FastTrack promptly changed the
unpublished encryption algorithm to another one and then used remote
code upgrade ability to "upgrade" all of the clients.
Now the open-source group could counter-strike if they had
particularly felt motivated to.  For example they could (1)
reverse-engineer the new unpublished encryption algorithm, and (2) the
remote code upgrade, and then (3) do their own forced upgrade to an
open encryption algorithm and (4) disable further forced upgrades.
(giFT instead after the "ugrade" attack from FastTrack decided to
implement their own open protocol "openFT" instead and compete.  It
also includes a general bridge between different file-sharing
networks, in a somewhat gaim like way, if you are familiar with
I grant you that making simultaneously DoS resistant, scalable and
anonymous peer-to-peer networks is a Hard Problem.  Even removing the
anonymous part it's still a Hard Problem.
Note both Freenet and Mojo try to tackle the harder of those two
problems and have aspects of publisher and reader anonymity, so that
they are doing less well than Kazaa, gnutella and others is partly
because they are more ambitious and tackling a harder problem.  Also
the anonymity aspect possibly makes abuse more likely -- ie the
attacker is provided as part of the system tools to obscure his own
identity in attacking the system.  DoSers of Kazaa or gnutella would
likely be more easily identified which is some deterrence.
I also agree that the TCPA/Palladium attested closed world computing
model could likely more simply address some of these problems.
(Lucky slide critique in another post).

@_date: 2002-08-08 05:34:15
@_author: Adam Back 
@_subject: Re: Palladium: hardware layering model 
No I think the above diagram is closer than what you propose.  Peter
also pointed us at Seth Schoen's blog [1] which is a write up of a
briefing Microsoft gave to EFF.  It contains the statement:
Looks consistent with my picture to me.
Your other objection:
I think would just be covered by the details of how the machine
switches from this picture:
to the one above.
For example imagine a default stub nub/TOR that leaves the new MMU
features wide open.  (Supervisor mode can access everything, no
Trusted Agent code compartments running).  The would be some API to
allow the supervisor mode code to load a TOR and switch the TOR code
to ring-0 while leaving the OS running in supervisor mode.
Or alternatively and with equivalent effect: with the boot state, the
OS runs in full ring-0 mode, but just isn't written to make use of any
of the extra ring-0 features.  When it switches to loading a nub/TOR
the OS is relagated to supervisor mode, some MMU permission bits are
juggled around and the TOR occupies ring-0, and the TOR is just an OS
micro-kernel which happens to be written to use the new hardware
features (code compartmentalization, new MMU features, sealing etc)
Clarification on this:
Not what I meant.  Say that you have some code that looks like this:
if ( ! remote_attest( /* ... */ ) ) { exit 0; }
then the remote attest is not doing anything apart from acting as a
remote dongle, so all I have to do to virtualize this code, or break
the licensing scheme based on the remote dongle is nop out the remote
attest verification, then the code can be run as a user application
rather than a trusted agent application and so can be run in a
debugger, have it's state examined etc.
If on the other hand the code says:
download_sealed_content( /* ... */ );
key = remote_attest_and_key_negotiate( /* ... */ );
decrypt_sealed_content( key, /* ... */ );
then nopping out the remote_attest will have a deleterious effect on
the applications function, and so virtualizing it with the remote
attests nopped out will not be useful in bypassing it's policies.

@_date: 2002-08-07 20:38:55
@_author: Adam Back 
@_subject: Palladium: hardware layering model (Re: Palladium: technical 
some definitions:
hw layer -- SCP which I think provides crypto key store, crypto
 	    co-processor for sealing, remote attesation
ring 0 -- new layer which controls memory management unit and secured
code compartments
supervisor mode -- normal supervisor mode, which can now only read
user space, but not trusted agents running in code compartments
user mode -- legacy user level apps under complete control of
supervisor mode
and some ASCII art:
each layer below can decide policy and information disclosure through
APIs to the layer above.  The implications of which are:
- the SCP can implement sealing with data separation against ring-0
(ring-0 can't bypass sealing data separation)
- ring-0 can read all superviser, user, and trusted agent space, but
- ring-0 and MMU can compartmentalize trusted agents so they can't
tamper with each other, and
- ring-0 and MMU can exclude supervisor mode from trusted agent space
and ring-0 space; supervisor mode is itself just another
compartmentalized trusted-agent level space.  Therefore ring-0 can
restrict what supervisor mode (where the normal OS is located) can do.
whereas the normal protected CPU architecture is just:
- from these assumptions it appears an OS could be implemented so that
all OS calls pass through ring-0 APIs and mediation to get to
supervisor mode OS.  In this case the OS could observe system calls
the trusted agent makes, but not in general read, debug, modify
virtualize or modify trusted-agent code.  The non-virtualization
presumes encrypted trusted-agent code, which Peter said is not done,
so this can't be how it works.
I would be interested to hear what model takes for Palladium mapping
the interactions and restrictions between Trusted Agents, user space,
OS kernel, TOR to the hardware.  We need this kind of detail to reason
about limits of the Palladium and make distinctions between what is
possible with Palladium implementation choices vs what other types of
OSes could be built from the hardware features.
One idea I think would be interest is as follows:
- the TOR (which lives in ring-0) _could_ be used together with the OS
to force all trusted-agent in-flows and out-flows (network traffic) to
go through code under supervisor mode control.  I don't think this is likely in the current design; but this change
would be an improvement: - it would at least allow user audit and control of in-flows and
- the user could block suspicious phone-home information out-flows,
- the user could read out-flows and demand un-encrypted documented
formats, or if encrypted, encrypted with keys the supervisor mode gets
copies of.
- similarly in-flow control is interesting, because with no in-flows a
trusted agent could be more liberally allowed to make out-flows (if it
has no input knowledge, and is in a code compartment, and the user
gave it no sensitive it doesn't know anything to leak.)
(Even with encrypted code, or public code which could not otherwise be
audited actively in the sense of debugging it's actual operation to
see what it does in practice in your machine given your data and
circumstances rather than looking at static code and third party
certifications to try to deduce that.  Not all apps may be unencrypted
(a TOR and SCP could clearly be built to support this feature).
So on anonymous comments about OS control:
I'm not sure if anonymous is just generalising when he says the app
can't in any circumstances know anything if the OS is hostile, but I
think it could potentially know things if the OS is hostile.  As I
described with the control and layer I think the palladium hardware
uses.  It seems possible to build some of separations and exclude the
OS from certain types of application.  It depends what you include in
the OS; if the OS includes the TOR, then no.  But it was stated that
the TOR is somewhat independent from the OS.  You could mix and match
and use an MS Palladium TOR with linux potentially (though perhaps not
in practice, it would have to be designed to allow it).  It also
depends on how the OS, trusted agents and supervisor mode is mapped to
the hardware.
Peter seemed to claim these kinds of assurance.  Sealing doesn't
prevent application virtualization, it just prevents the sealed data
being shared between non-virtualized instances of the apps and
virtualized instances.
So I was wondering how Peter could simultaneously claim that
encryption was not used and that "SW can known that it is running on a
given platform."
Remote attestation, which is not itself general -- just a remote
dongle thing -- if not tied to remote dongle controlled sealing which
is necessary for the main application function could be nopped out.
So in the general case it seems that remote attestation is also
effectively virtualizable, modifiable and debuggable by first nopping
out remote attestation checks.  (This is not strictly virtualizable as
the remote dongle call nopping modification makes it no longer the
same application, but as I said unless this is necessary for the
application it doesn't otherwise change it's behavior, so it's
effectively virtualizable).

@_date: 2002-08-07 16:28:04
@_author: Adam Back 
@_subject: (fwd) Re: more TCPA stuff (Re: "trust me" pseudonyms in TCPA)] 
Another Peter Biddle reply to the TCPA/Palladium thread on
"Adam Back" Sent: Monday, August 05, 2002 2:26 PM
The Pd SCP isn't extensible or programable. I wouldn't say that it is
"general purpose" either, but I am not sure what you mean by this. It is
soldered to your motherboard. It provides a limited (smaller than a TPM)
feature set. Pd does not create a a centralised point belonging to
Microsoft. There are no root certs from MS except those to certify our own
nub and SW, and these are SW certs. How others do this for their SW is up to
them. I expect that we will want to get third party certification for our Pd
software as well as certing it ourselves. HW is assumed to be certified by
whomever built it, based on whatever criteria they want to use for whatever
the solution and cost dictate, and they too can get third-party certs as
they see fit.
It is entirely possible to run Pd and get it's benefits without telling MS
Inc. anything about your machine. For Pd to work you have to tell the MS TOR
(unless you are using a different TOR) about your machine, and so we have to
prove to everyone that telling the TOR something is very different from
telling MS Inc. something. Pd doesn't phone home on it's own.
There isn't centralized control in Pd. Users are in control. It is up to
whomever cares about the trust on a given system to decide if they trust it,
and this obviously must start with the user.

@_date: 2002-08-05 21:26:28
@_author: Adam Back 
@_subject: more TCPA stuff (Re: "trust me" pseudonyms in TCPA) 
Note there is one key that is endorsed, so per machine there is one
key, singular.
On the other interpretation of your question: do we trust that the
manufacturer didn't take a copy of the key while certifying it?
Good quesion.  The scenario is analogous to the pre-generated private key on a smart
card.  Do you trust what the hardware vendor did with it?  Did they
generate the private key it off chip and keep a copy?  Did they
generate the private key on chip but export it at the time of
certifying the public key?
Except in this case the smart card is attached to your motherboard,
mediates control of the platform and is called the "TPM" Trusted
Platform Module.
While there are approaches to having third party audits of the
process, publishing the source code, etc; it's still typically not a
very transparent affair as it's in tamper resistant hardware, plus
vulnerable to plausibly deniable snafus, and undetectable backdooring
even if it is generated on TPM.
Effectively I think the best succinct description of the platforms
motivation and function is that:
"TCPA/Palladium is an extensible, general purpose programmable dongle
soldered to your mother board with centralised points belonging to
It seems to me there is both strong possibility for it becoming a
focus for future government attempts at policy malware and legislated
technology implementation, and a focus RIAA/MPAA/WIPO polices imposing
futher expansionist and monopoly propping legislation and legislated
technology implementation to enforce the worst excesses of DMCA.
The technology components are very interesting.  The implications of
what can be done with sealing, secure boot-strapping and remote
attestation are a departure from what people were thinking was
possible with general purpose computing.  As anonymous points out it
makes possible all kinds of applications and changes the nature of
what can be cryptographically assured.
With current non-TCPA platforms the limit of what can be
cryptographically assured is for example what can be encrypted with
password, or other cryptographic mechanism.
Cryptographic assurance is also known as "data separation" -- the
concept that the crytography is able to completely cover the
applications policy restrictions without leaving "trusted" software
components necessary to enforce policies too complex to implement with
encryption / data separation.
With TCPA you can build general purpose policy code which does not
exhibit cryptographic assurance, and yet due to the TCPA platform
assures similar levels of security assurance.  That's a huge change in
world view in the domain of security applications.
In slightly more detail, you can either build applications rooted in
the remote attestation, sealing and secure boot-strapping functions I
described in an earlier post.  Or you can add your own custom policy
and even applications inside a hardware assured code compartment which
the user can not access or tamper with.
One aspect of the implications is the implementation and security
possibilities it lends to DRM applications.  Personally I don't find
this aspect a good thing because I think current copyright law has
reached a state of being a net negative for society and freedom, and
that it's time to rescind them and start-over.
I think we should try analyse as William Arbaugh suggested in [7] what
is desirable, what is safe to implement, and ways to change the
platform to remove the negative aspects.
From my current understanding, the worst problem is the centralised
control of this platform.  If it were completely open, and possible to
fix it's potential dangers, it would bring about a new framework of
secured computing and could be a net positive.  In it's current form
with centralised control and other problems it could be a big net
[7] "The TCPA; What's wrong; What's right and what to do about",
William Arbaugh, 20 Jul 2002

@_date: 2002-08-03 04:26:12
@_author: Adam Back 
@_subject: info-theoretic model of anonymity 
Just read this paper published in PET02 "Towards an Information
Theoretic Metric for Anonymity" [1]:
or	
it uses a Shannon like entropy model for the anonymity provided by a
system uses this model to analyse the effect of different parameters
one can tune with mixmaster (POOLSIZE, RATE, in mixmaster.conf).
The "anonymity entropy" measurement can be interpreted as how many
bits of information the attacker needs to identify a user and is
computed from probabilities.
Would be interesting to try estimate the entropy provided by the
current mixmaster network.  A number of nodes publish their parameter
choices, and traffic volume over time (in hourly increments).
  author = "Andrei Serjantov and George Danezis",
  title = "Towards an Information Theoretic Metric for Anonymity",
  booktitle = "Proceedings of the Workshop on Privacy Enhancing Technologies",
  year = "2002",
  note = "Also available as \url{

@_date: 2002-08-02 01:28:17
@_author: Adam Back 
@_subject: Re: document popularity estimation / amortizable hashcash (Re: 
This paper is quite interesting and proposes another method of
metering content [1]:
It's proposed in the context of web site traffic metering to determine
site traffic rates (for advertising payment or other applications).
It relies on a trusted auditor, which could become a central failure
point so is perhaps less attractive for file sharing, but perhaps that
could be fixed.
Another problem is that it presumes a communication pattern where the
auditor sends a secret to each user, the user makes a cheap
computation involving the secret to send with each request, and then
the respective server collects all of the requests it gets and does
some computation to arrive at a compact proof that it received some
number k of requests.  (The server also receives a secret, but this is
not problematic, it it anyway wants to participate).
On the plus side their approach is not probabilistic -- it gives an
accurate measurement of traffic, it is also not vulnerable to server
traffic inflation, and is somewhat resistant to multiple client and
server collusion.  (Though of course any scheme is generically
vulnerable to server traffic inflation -- servers can _act_ as
multiple clients and simply generate the claimed traffic themselves,
or contract other parties to do this for them.)
    author = "Moni Naor and Benny Pinkas",
    title = "Secure and Efficient Metering",
    journal = "Lecture Notes in Computer Science",
    volume = "1403",
    pages = "576--??",
    year = "1998",
    note = "Also available as \url{

@_date: 2002-08-07 06:20:16
@_author: Adam Back 
@_subject: Re: dangers of TCPA/palladium 
Thanks for the clarifications of the differences between TCPA and
Palladium.  The lack of Palladium docs and fact that TCPA docs
describe OS level features led to the inference that Palladium unless
otherwise stated did what TCPA proposed.
* Documentation
I'm feeling frustrated in being unable to properly analyse Palladium
due to lack of documentation.  Surely microsoft must have _some_
internal documentation that could be released.  Two second hand blog
articles by third parties doesn't quite cut it!
The documents claim privacy advocacy group consultation?  Could the
information shared with these groups be published?
It's quite difficult to reason about implications and limits of a
novel new architecture with incomplete information -- you need
grounded facts down to the technical details to work out what the
implications are from first principles and compare and evaluate the
proponents claims.
* privacy CA
Insufficient data to comment on the degree of openness provided by
palladium for 1 (allow owner to load trusted root cert), and 2 (allow
TPM to be completely disabled).
For 3, the TCPA version of the "privacy CA" is broken (implemented
using "trust me" by a server the user does not and should not have to
trust).  Does Palladium do something different?
later in same message you said:
It sounds as if Palladium suffers from the same broken privacy problem
as TCPA then.  Saying you have a choice about whether to use a service
doesn't alter the fact that you are linkable and identifiable to some
extent -- the extent depending on the exact permutation of attribute,
identity and endoresment certificates you have used.
This vulnerability is unnecessary.  You can certifying things without
having to trust anyone.
* architecture functionality
main features of Palladium together with a hardware collection called
the SCP (= ?) can do the following things:
1. no secure-bootstrapping -- unlike TCPA this is not implemented 2. software-attestation -- Palladium uses SCP able to hash perhaps
   the TOR, Trusted Agents, and application software, and then uses
   TCPA-like endorsed hardware keys in the SCP to remotely attest to
   these hashes.  3. hardware assisted compartmentalization -- CPU can run another layer
   of privileged software with ability to prevent supervisor mode
   (and user mode) reading chosen user mode process memory areas.  The
   ubermode code is called the TOR.  The supervisor mode can install
   any TOR into the ubermode, but the TOR can be remotely attested(?)
4. sealing -- TCPA-style sealing
on 2, softeware-attestation from information so far, it's unclear what
is hashed and what is attested.
on 3, hardware assisted compartmentalization
- Presumably Palladium enabled applications would refuse to run
  unless a Palladium/Microsoft certified TOR is running?
  - Limits the meaningfulness of claims of openness in loading your
    own TOR.  More "you can turn x off but nothing will work if you
    do".
   - or perhaps it is up to the individual Palladium application
     which TOR it trusts?
     - but wouldn't this lead to a break:
       - If a microsoft written Palladium enabled application would
         talk to any TOR, user can load a TOR which is under user
         control to by-pass the compartmentalized memory
         restrictions, regaining root.  But if he can do this, he
         can break Palladium enforced DRM.
also about hardware assisted compartmentalization, earlier I said:
you responded:
If I understand the architecture makes it possible to write a TOR
which supports encrypted applications encrypted for the machines key
which is stored in the SCP.
I thought this scheme would be in the current design because as far as
I can see this would in fact be necessary for strong copy protection
for software (software licensed only for a given machine), which I
presumed microsoft has an interest in.
It would be a bit like a "sealed" application.
* on claim "palladium doesn't prevent anything":
Well as you can still run the same software that is a tautology.
The correct question is: can Palladium enabled software prevent you
from doing things you could do with non-Palladium enabled software.
Palladium is in fact designed explicitly to prevent the user doing
The are whole classes of things Palladium enabled software using
Trusted Agents, a default TOR and SCP features can prevent the user
from doing:
- it prevents the owner modifying application code running on his
  machine which uses the remote-attestation functions to talk to
  remote servers
- it could be used to robustly prevent the user auditing what
  information flowing into and out of his machine (the user can't
  obtain the keys negotiated by the SCP and a remote site, and can't
  grab the keys from the application because it's a trusted agent
  running in a TOR mediated code compartment.)
- it could be used to make file formats which are impossible for third
  parties to be compatible with (Ross Anderson came up with this
  example in [4])
- it could be used to securely hide undocumented APIs
- it could be used to securely implement software copy-protection
  using encrypt for endorsed SCP stored machine keys
- it could be used to prevent reverse-engineering applications
- it could be used for DRM to enforce play-once, to revoke fair use
  rights or any other arbitrary policies (on formats only available to
  Palladium enabled applications)
- it could be used to implement key-escrow of SCP stored keys to put
  government or corporate backdoors in sealed data, and comms with
  remote servers encrypted using SCP negotiated keys; wasn't there a
  statement somewhere that CAPI uses would more immediately get
  benefit from Palladium SCP functions?  Not sure how the mix of
  non-palladium using CAPI applications and palladium enabled SCP
  and/or CAP using applications work out for the key-escrow
  implementing TOR scenario.
now as I mentioned in an earlier post you could claim, oh that's ok
because the user has choice, he can still boot with his own custom
TOR.  In the short term that argument would work.  In the long term we
run the risk that:
a) many users will be so baffled by technology they won't know when
they are at risk and when they are not.
b) new non-backwards compatible file formats and feature creep
together with potential "palladium only format" enforcement could
start to make it very inconvenient to use non-standard TORs or non
palladium applications.
On top of that there are next gen issues, and on-going legal issues.
For example what happens when this scenario plays out:
1. recent release digital content is published early (same time as
cinema for the right price say).
2. hardware hackers do a break-once run anywhere by ripping all the
content on their hacked machine and start distributing it on kazaa
(2.5 Peta-bytes of ripped content and growing weekly)
3. RIAA/MPAA goes back and lobbys for further controls, DMCA
4. new legal, DMCA and RIAA/MPAA, and competition from Sony pressures
are placed on microsoft
difficult to see that far out what is going to happen, but blase
presumptions that it's unrealistic to expect key-escrow, or certified
TOR only and to assume that Lucky Green's Document Revocation Lists
don't get rolled out with DMCA style laws against interfering with --
that ignores a lot of history.
Also mentioned in previous post: just because it's law doesn't mean it
should be enforced.  People are currently afforded the ability to
ignore the masses of extreme and ridiculous IP law as individuals.
When a large chunk of it gets implemented into DRM, and narcware the
once free-wheeling internet information exchanges will become
marginalized, or underground only affairs -- the world will become
stifled by their own computers acting as the policeman inside -- your
own computer deputized by the US government, DMCA et al.
I think you must have misinterpreted what I said: I wasn't talking
about Palladium APIs.
I was talking about the fact that microsoft has historically used
undocumented APIs as a business tactic, and I presume this is still an
ongoing strategy, and the Palladium hardware architecture could be
used to build a TOR which forced competitors attempting to discover
undocumented APIs in order to fairly compete to resort to hardware
hacking.  Unless and until we get the software copyright analog of
DMCA reverse-engineering restrictions which makes that illegal.
I'm not sure what you mean by "de-centralized trust".  Perhaps that
the TOR is publicly audited?  Perhaps that there are multiple vendors
endorsing SCP implementations?
I think Palladium clearly has possibilities to magnify centralized
It is in microsoft's economic interests, and historically their
modus-operandi to aggresively try to create and exploit control points
to extract monopolistic rents, and/or suppress competition.  And of
course other companies also have used the same strategies, but the
current limits placed on such practices by the ability to reverse
engineer for compatibility may come to be eroded by TCPA/Palladium.
It's nothing personal, it's just that intent is open to evolutionary
change, legislative attack, pressures outside of your personal, or
microsoft's control -- economic and legal incentives will arise which
make the platform deviate from current stated intent.  2002 stated
intent is zip guarantee.
Someone sent me this hugely appropriate quote in email relating to
this point:
except here we are not talking about legislation directly but a
hardware platform with the tools to create different control points
and the legal and business pressures that act upon the use and misuse
of that platform and those created control points.
I can only think that your definitions of user control probably are in
terms of abstract "security" rather than a distrusting viewpoint that
wants to be able to audit and modify application behavior.  I presume you mean "freedom" in the sense of freedom from the ills
that it is claimed Palladium enabled applications could improve.
I think of freedom as the ability for self-determination, to
completely control and audit all aspects of software running on my
system.  Without these freedoms applications and vendors can conspire
against the machine owner.  I suspect the majority of people feel
With current information it seems that the Palladium platform is
damaging to freedom and indivdual liberty, and a future risk to free
society.  I am of course completely open to being proven wrong, and
look forward to seeing more detailed Palladium specs so that this can
be tested, and to see the community given the opportunity to point out
potential more open and less control point prone modifications.
[4] "Security in Open versus Closed Systems (The Dance of Boltzmann,
Coase and Moore)", Ross Anderson,
(Sections 4 and 5 only, rest is unrelated)

@_date: 2002-08-07 06:20:16
@_author: Adam Back 
@_subject: Re: dangers of TCPA/palladium 
Thanks for the clarifications of the differences between TCPA and
Palladium.  The lack of Palladium docs and fact that TCPA docs
describe OS level features led to the inference that Palladium unless
otherwise stated did what TCPA proposed.
* Documentation
I'm feeling frustrated in being unable to properly analyse Palladium
due to lack of documentation.  Surely microsoft must have _some_
internal documentation that could be released.  Two second hand blog
articles by third parties doesn't quite cut it!
The documents claim privacy advocacy group consultation?  Could the
information shared with these groups be published?
It's quite difficult to reason about implications and limits of a
novel new architecture with incomplete information -- you need
grounded facts down to the technical details to work out what the
implications are from first principles and compare and evaluate the
proponents claims.
* privacy CA
Insufficient data to comment on the degree of openness provided by
palladium for 1 (allow owner to load trusted root cert), and 2 (allow
TPM to be completely disabled).
For 3, the TCPA version of the "privacy CA" is broken (implemented
using "trust me" by a server the user does not and should not have to
trust).  Does Palladium do something different?
later in same message you said:
It sounds as if Palladium suffers from the same broken privacy problem
as TCPA then.  Saying you have a choice about whether to use a service
doesn't alter the fact that you are linkable and identifiable to some
extent -- the extent depending on the exact permutation of attribute,
identity and endoresment certificates you have used.
This vulnerability is unnecessary.  You can certifying things without
having to trust anyone.
* architecture functionality
From the limited information available my understanding is that the
main features of Palladium together with a hardware collection called
the SCP (= ?) can do the following things:
1. no secure-bootstrapping -- unlike TCPA this is not implemented 2. software-attestation -- Palladium uses SCP able to hash perhaps
   the TOR, Trusted Agents, and application software, and then uses
   TCPA-like endorsed hardware keys in the SCP to remotely attest to
   these hashes.  3. hardware assisted compartmentalization -- CPU can run another layer
   of privileged software with ability to prevent supervisor mode
   (and user mode) reading chosen user mode process memory areas.  The
   ubermode code is called the TOR.  The supervisor mode can install
   any TOR into the ubermode, but the TOR can be remotely attested(?)
4. sealing -- TCPA-style sealing
on 2, softeware-attestation from information so far, it's unclear what
is hashed and what is attested.
on 3, hardware assisted compartmentalization
- Presumably Palladium enabled applications would refuse to run
  unless a Palladium/Microsoft certified TOR is running?
  - Limits the meaningfulness of claims of openness in loading your
    own TOR.  More "you can turn x off but nothing will work if you
    do".
   - or perhaps it is up to the individual Palladium application
     which TOR it trusts?
     - but wouldn't this lead to a break:
       - If a microsoft written Palladium enabled application would
         talk to any TOR, user can load a TOR which is under user
         control to by-pass the compartmentalized memory
         restrictions, regaining root.  But if he can do this, he
         can break Palladium enforced DRM.
also about hardware assisted compartmentalization, earlier I said:
you responded:
If I understand the architecture makes it possible to write a TOR
which supports encrypted applications encrypted for the machines key
which is stored in the SCP.
I thought this scheme would be in the current design because as far as
I can see this would in fact be necessary for strong copy protection
for software (software licensed only for a given machine), which I
presumed microsoft has an interest in.
It would be a bit like a "sealed" application.
* on claim "palladium doesn't prevent anything":
Well as you can still run the same software that is a tautology.
The correct question is: can Palladium enabled software prevent you
from doing things you could do with non-Palladium enabled software.
Palladium is in fact designed explicitly to prevent the user doing
The are whole classes of things Palladium enabled software using
Trusted Agents, a default TOR and SCP features can prevent the user
from doing:
- it prevents the owner modifying application code running on his
  machine which uses the remote-attestation functions to talk to
  remote servers
- it could be used to robustly prevent the user auditing what
  information flowing into and out of his machine (the user can't
  obtain the keys negotiated by the SCP and a remote site, and can't
  grab the keys from the application because it's a trusted agent
  running in a TOR mediated code compartment.)
- it could be used to make file formats which are impossible for third
  parties to be compatible with (Ross Anderson came up with this
  example in [4])
- it could be used to securely hide undocumented APIs
- it could be used to securely implement software copy-protection
  using encrypt for endorsed SCP stored machine keys
- it could be used to prevent reverse-engineering applications
- it could be used for DRM to enforce play-once, to revoke fair use
  rights or any other arbitrary policies (on formats only available to
  Palladium enabled applications)
- it could be used to implement key-escrow of SCP stored keys to put
  government or corporate backdoors in sealed data, and comms with
  remote servers encrypted using SCP negotiated keys; wasn't there a
  statement somewhere that CAPI uses would more immediately get
  benefit from Palladium SCP functions?  Not sure how the mix of
  non-palladium using CAPI applications and palladium enabled SCP
  and/or CAP using applications work out for the key-escrow
  implementing TOR scenario.
now as I mentioned in an earlier post you could claim, oh that's ok
because the user has choice, he can still boot with his own custom
TOR.  In the short term that argument would work.  In the long term we
run the risk that:
a) many users will be so baffled by technology they won't know when
they are at risk and when they are not.
b) new non-backwards compatible file formats and feature creep
together with potential "palladium only format" enforcement could
start to make it very inconvenient to use non-standard TORs or non
palladium applications.
On top of that there are next gen issues, and on-going legal issues.
For example what happens when this scenario plays out:
1. recent release digital content is published early (same time as
cinema for the right price say).
2. hardware hackers do a break-once run anywhere by ripping all the
content on their hacked machine and start distributing it on kazaa
(2.5 Peta-bytes of ripped content and growing weekly)
3. RIAA/MPAA goes back and lobbys for further controls, DMCA
4. new legal, DMCA and RIAA/MPAA, and competition from Sony pressures
are placed on microsoft
difficult to see that far out what is going to happen, but blase
presumptions that it's unrealistic to expect key-escrow, or certified
TOR only and to assume that Lucky Green's Document Revocation Lists
don't get rolled out with DMCA style laws against interfering with --
that ignores a lot of history.
Also mentioned in previous post: just because it's law doesn't mean it
should be enforced.  People are currently afforded the ability to
ignore the masses of extreme and ridiculous IP law as individuals.
When a large chunk of it gets implemented into DRM, and narcware the
once free-wheeling internet information exchanges will become
marginalized, or underground only affairs -- the world will become
stifled by their own computers acting as the policeman inside -- your
own computer deputized by the US government, DMCA et al.
I think you must have misinterpreted what I said: I wasn't talking
about Palladium APIs.
I was talking about the fact that microsoft has historically used
undocumented APIs as a business tactic, and I presume this is still an
ongoing strategy, and the Palladium hardware architecture could be
used to build a TOR which forced competitors attempting to discover
undocumented APIs in order to fairly compete to resort to hardware
hacking.  Unless and until we get the software copyright analog of
DMCA reverse-engineering restrictions which makes that illegal.
I'm not sure what you mean by "de-centralized trust".  Perhaps that
the TOR is publicly audited?  Perhaps that there are multiple vendors
endorsing SCP implementations?
I think Palladium clearly has possibilities to magnify centralized
It is in microsoft's economic interests, and historically their
modus-operandi to aggresively try to create and exploit control points
to extract monopolistic rents, and/or suppress competition.  And of
course other companies also have used the same strategies, but the
current limits placed on such practices by the ability to reverse
engineer for compatibility may come to be eroded by TCPA/Palladium.
It's nothing personal, it's just that intent is open to evolutionary
change, legislative attack, pressures outside of your personal, or
microsoft's control -- economic and legal incentives will arise which
make the platform deviate from current stated intent.  2002 stated
intent is zip guarantee.
Someone sent me this hugely appropriate quote in email relating to
this point:
except here we are not talking about legislation directly but a
hardware platform with the tools to create different control points
and the legal and business pressures that act upon the use and misuse
of that platform and those created control points.
I can only think that your definitions of user control probably are in
terms of abstract "security" rather than a distrusting viewpoint that
wants to be able to audit and modify application behavior.  I presume you mean "freedom" in the sense of freedom from the ills
that it is claimed Palladium enabled applications could improve.
I think of freedom as the ability for self-determination, to
completely control and audit all aspects of software running on my
system.  Without these freedoms applications and vendors can conspire
against the machine owner.  I suspect the majority of people feel
With current information it seems that the Palladium platform is
damaging to freedom and indivdual liberty, and a future risk to free
society.  I am of course completely open to being proven wrong, and
look forward to seeing more detailed Palladium specs so that this can
be tested, and to see the community given the opportunity to point out
potential more open and less control point prone modifications.
[4] "Security in Open versus Closed Systems (The Dance of Boltzmann,
Coase and Moore)", Ross Anderson,
(Sections 4 and 5 only, rest is unrelated)

@_date: 2002-08-05 20:46:43
@_author: Adam Back 
@_subject: Re: dangers of TCPA/palladium 
Some comments on selected parts of anonymous post:
1) about claimed "complexity" of cryptographically assured privacy,
rather than the current "trust me" privacy via the privacy CA "TTP":
To address privacy with for example Brands digital credentials, the
underlying cryptography may be harder to understand, or at least less
familiar, but I don't think using a toolkit based on Brands digital
credentials would be significantly harder than using an identity or
attribute based PKI toolkit.  Similar for Chaum's credentials or other
approach.  Also I notice you imply patents are a problem.  However, the TCPA
itself has patents and will of course charge for the hardware.
Patents it doesn't seem would present a problem for this application,
where there is non-zero reproduction cost hardware involved.
2) about the "root key" / potential for malicious remote control claim
that I make (and Ross Anderson I think also makes):
The "root key" to your computing environment is the private key of the
CA that signs the software updates.  You'll recall that in the secure
boot-strapping process if you choose to boot in the TCPA mode, if
there are deviations or updates these are fetched and are only
accepted if certified by the layer owner.  (I presume different layers
would have updates and certification managed by different vendors
Eg. hardware vendor / TPM vendor for firmware, OS manufactturer for
OS, application manufacturer for application software etc, and that
the secure bootstrap process would accordingly transfer control to the
respective next layers certification keys in case of need for software
The closer to the hardware a software update is the more pervasive the
control a malicious update could exert.  For example there are
apparently plans for TPM mediated direct path to input devices
(esp. keyboard), a malicious update close enough to the hardware could
subvert this protection.
and more on the "root key" problem:
The root key is not the endorsement master keys -- that one just
allows the TPM vendor to extract rent from the hardware manufacturers

@_date: 2002-08-05 20:46:43
@_author: Adam Back 
@_subject: Re: dangers of TCPA/palladium 
Some comments on selected parts of anonymous post:
1) about claimed "complexity" of cryptographically assured privacy,
rather than the current "trust me" privacy via the privacy CA "TTP":
To address privacy with for example Brands digital credentials, the
underlying cryptography may be harder to understand, or at least less
familiar, but I don't think using a toolkit based on Brands digital
credentials would be significantly harder than using an identity or
attribute based PKI toolkit.  Similar for Chaum's credentials or other
approach.  Also I notice you imply patents are a problem.  However, the TCPA
itself has patents and will of course charge for the hardware.
Patents it doesn't seem would present a problem for this application,
where there is non-zero reproduction cost hardware involved.
2) about the "root key" / potential for malicious remote control claim
that I make (and Ross Anderson I think also makes):
The "root key" to your computing environment is the private key of the
CA that signs the software updates.  You'll recall that in the secure
boot-strapping process if you choose to boot in the TCPA mode, if
there are deviations or updates these are fetched and are only
accepted if certified by the layer owner.  (I presume different layers
would have updates and certification managed by different vendors
Eg. hardware vendor / TPM vendor for firmware, OS manufactturer for
OS, application manufacturer for application software etc, and that
the secure bootstrap process would accordingly transfer control to the
respective next layers certification keys in case of need for software
The closer to the hardware a software update is the more pervasive the
control a malicious update could exert.  For example there are
apparently plans for TPM mediated direct path to input devices
(esp. keyboard), a malicious update close enough to the hardware could
subvert this protection.
and more on the "root key" problem:
The root key is not the endorsement master keys -- that one just
allows the TPM vendor to extract rent from the hardware manufacturers

@_date: 2002-08-05 05:00:31
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Like anonymous, I've been reading some of the palladium and TCPA docs.
I think some of the current disagreements and not very strongly
technology grounded responses to anonymous are due to the lack of any
concise and informative papers describing TCPA and palladium.
Not everyone has the energy to reverse engineer a detailed 300-odd
pages of TCPA spec [1] back into high-level design considerations; the
more manageably short business level TCPA FAQs [2], [3] are too
heavily PR spun and biased to extract much useful information from.
So so far I've read Ross Anderson's initial expose of the problem [4];
plus Ross's FAQ [5].  (And more, reading list continues below...).
The relationship between TCPA, and Palladium is:
- TCPA is the hardware and firmware (Compaq, Intel, IBM, HP, and
Microsoft, plus 135+ other companies)
- Palladium is a proposed OS feature-set based on the TCPA hardware
The main 4 features proposed in the TCPA/palladium scheme are:
1. secure bootstrap -- checksums of BIOS, firmware, privileged OS code
are used to ensure the machine knows whether it is running certified
software or not.  This is rooted in hardware, so you can't by pass it
by using virtualization, only by hardware hacking (*).
2. software attestation -- the hardware supports attesting to a third
party whether a call comes from a certified software component as
assured by the hardware described in feature 1. 3. hardware assisted compartmentalization -- CPU can run privileged
software, and RAM can contain information that you can not examine,
and can not modify.  (Optionally the software source can be published,
but that is not necessary, and if it's not you won't be able to
reverse-engineer it as it can be encrypted for the CPU).
4. sealing -- applications can store data that can only be read by
that application.  This works based on more hardware -- the software
state checksums developed in feature 1 are used by hardware to
generate encryption keys.  The hardware will refuse to generate the
key unless the same software state is running.
One good paper to understand the secure bootstrap is an academic paper
"A Secure and Reliable Bootstrap architecture" [6].
It's interesting to see that one of the author's of [6] has said that
TCPA as curently formed is a bad thing and is trying to influence TCPA
to make it more open, to exhibit stronger privacy properties read his
comments at [7].
There are a lot of potential negative implications of this technology,
it represents a major shift in the balance of power comparable in
magnitude to the clipper chip:
1. Potentially cedes control of the platform -- while the palladium
docs talk about being able to boot the hardware with TCPA turned off,
there exists possibility that with minor configuration change the
hardware / firmware ensemble that forms palladium/TCPA could be
configured to allow only certified OSes to boot, period.  It's
intereseting to note, if I read correctly, that the X-box (based on
celeron processor and TCPA / TCPA-like features) does employ this
feature.  See for example: [8].  The documents talk about there being no barrier to certifying TCPA
aware extensions to open-source OSes.  However I'm having trouble
figuring out how this would work.  Perhaps IBM with it's linux support
would build a TCPA extension for linux.  Think about it -- the
extension runs in privileged mode, and presumably won't be certified
unless it passes some audit enforcing TCPA policies.  (Such as keeping
the owner of the machine from reading sealed documents, or reading the
contents of DRM policy controlled documents without meeting the
requirements for the DRM policy.)
2. DSS over-again -- a big aspect of the DSS reverse-engineering was
to allow DVDs to be played in software on linux.  The TCPA platform
seems to have the primary goal of making a framework within which it
is possible to build extensions to implement hardware tamper resistant
DRM.  (The DRM implementation would run in a hardware assisted code
compartment as described in feature 3 above).  So now where does that
put open source platforms?  Will they be able to read such DRM
protected content?  It seems likely that in the longer term the DRM
platform will include video cards without access to video memory,
perhaps encryption of the video signal out to the monitor, and of
audio out to the speakers.  (There are other existing schemes to do
these things which dovetail into the likely TCPA DRM framework.)  With the secure boot strap described in feature 1, the video card and
so on are also part of the boot strap process, so the DRM system would
have ready support from the platform for robustly refusing to play
except on certain types of hardware.  Similarly the application
software which plays these DRM policy protected files and talks to the
DRM policy module in the hardware assisted code compartment will
itself be an application which uses the security boot-strapping
features.  So it won't be possible to write an application on for
example linux to play these files without an audit and license etc
from various content, DRM and OS cartels.  This will lead to exactly
the kind of thing Richard Stallman talked about in his prescient paper
on the coming platform and right to develop competing software control
wars [9].
3. Privacy support is broken -- the "privacy" features while clearly
attempts to defuse a re-run at the pentium serial number debacle, have
not really fixed it's problems.  You have to trust the "Trusted Third
Party" privacy CA not to track you and not to collude with other CAs
and software vendors.  There are known solutions to this particular
sub-problem, for example Stefan Brands digital credentials [10], which
can be used to build a cryptographically assured privacy preserving
PKI avoiding the linking problems arising from identity based and
attribute certificates.
4. Strong enforcement for DMCA DRM excesses -- the types of DRM system
which the platform enables stand a fair chance of providing high
levels of enforcement for things which though strictly legally
mandated (copyright licensing restrictions, limited number of plays of
CDs / DVDs other disadvantageous schemes; inflexible and usurious
software licensing), if enforced strictly would have deleterious
effects on society and freedom.  Copyright violation is widely
practiced to a greater or less extent by just about all individuals.
It is widely viewed as acceptable behavior.  These social realities
and personal freedoms are not taken into account or represented in the
lobbying schemes which lead to the media cartels obtaining legal
support for the erosion of users rights and expansionist power grabs
in DMCA, WIPO etc.
Some of these issues might be not so bad except for the track records,
and obvious monopolistic tendencies and economic pressures on the
entities who will have the root keys to the worlds computers.  There
will be no effect choice or competition due to existing near
monopolies, or cartelisation in the hardware, operating system, and
content distribution conglomerates.
5. Strong enforcement for the software renting model -- the types of
software licensing policy enforcement that can be built with the
platform will also start to strongly enable the software and object
rental ideas.  Again potentially these models have some merit except
that they will be sabotaged by API lock out, where the root key owners
will be able to charge monopoly rents for access to APIs.
6. Audits and certification become vastly more prevalent.  Having had
some involvement with software certification (FIPS 140-1 / CC) I can
attest that this can be expensive exercises.  It is unlikely that the
open source community will be able to get software certified due to
cost (the software is free, there is no business entity to claim
ownership of the certification rights, and so no way to recuperate the
costs).  While certification where competition is able to function is
a good thing, providing users with a transparency and needed
assurance, the danger with tying audits to TCPA is that it will be
another barrier to entry for small businesses, and for open source
7. Untrusted, unauditable software will be able to run without
scrutiny inside the hardware assisted code compartments.  Some of the
documentation talks about open sourcing some aspects.  While this may
come to pass, but that sounded like the TOR (Trusted Operating Root);
other extension modules also running in unauditable compartments will
not be so published.
8. Gives away root control of your machine -- providing potentially
universal remote control of users machines to any government agencies
with access to the TCPA certification master keys, or policies
allowing them to demand certifications on hostile code on demand.
Central authorities are likely to be the only, or the default
controllers of the firmware/software upgrade mechanism which comes as
part of the secure bootstrap feature.
9. Provides a dangerously tempting target for government power-grabs

@_date: 2002-08-05 05:00:31
@_author: Adam Back 
@_subject: dangers of TCPA/palladium 
Like anonymous, I've been reading some of the palladium and TCPA docs.
I think some of the current disagreements and not very strongly
technology grounded responses to anonymous are due to the lack of any
concise and informative papers describing TCPA and palladium.
Not everyone has the energy to reverse engineer a detailed 300-odd
pages of TCPA spec [1] back into high-level design considerations; the
more manageably short business level TCPA FAQs [2], [3] are too
heavily PR spun and biased to extract much useful information from.
So so far I've read Ross Anderson's initial expose of the problem [4];
plus Ross's FAQ [5].  (And more, reading list continues below...).
The relationship between TCPA, and Palladium is:
- TCPA is the hardware and firmware (Compaq, Intel, IBM, HP, and
Microsoft, plus 135+ other companies)
- Palladium is a proposed OS feature-set based on the TCPA hardware
The main 4 features proposed in the TCPA/palladium scheme are:
1. secure bootstrap -- checksums of BIOS, firmware, privileged OS code
are used to ensure the machine knows whether it is running certified
software or not.  This is rooted in hardware, so you can't by pass it
by using virtualization, only by hardware hacking (*).
2. software attestation -- the hardware supports attesting to a third
party whether a call comes from a certified software component as
assured by the hardware described in feature 1. 3. hardware assisted compartmentalization -- CPU can run privileged
software, and RAM can contain information that you can not examine,
and can not modify.  (Optionally the software source can be published,
but that is not necessary, and if it's not you won't be able to
reverse-engineer it as it can be encrypted for the CPU).
4. sealing -- applications can store data that can only be read by
that application.  This works based on more hardware -- the software
state checksums developed in feature 1 are used by hardware to
generate encryption keys.  The hardware will refuse to generate the
key unless the same software state is running.
One good paper to understand the secure bootstrap is an academic paper
"A Secure and Reliable Bootstrap architecture" [6].
It's interesting to see that one of the author's of [6] has said that
TCPA as curently formed is a bad thing and is trying to influence TCPA
to make it more open, to exhibit stronger privacy properties read his
comments at [7].
There are a lot of potential negative implications of this technology,
it represents a major shift in the balance of power comparable in
magnitude to the clipper chip:
1. Potentially cedes control of the platform -- while the palladium
docs talk about being able to boot the hardware with TCPA turned off,
there exists possibility that with minor configuration change the
hardware / firmware ensemble that forms palladium/TCPA could be
configured to allow only certified OSes to boot, period.  It's
intereseting to note, if I read correctly, that the X-box (based on
celeron processor and TCPA / TCPA-like features) does employ this
feature.  See for example: [8].  The documents talk about there being no barrier to certifying TCPA
aware extensions to open-source OSes.  However I'm having trouble
figuring out how this would work.  Perhaps IBM with it's linux support
would build a TCPA extension for linux.  Think about it -- the
extension runs in privileged mode, and presumably won't be certified
unless it passes some audit enforcing TCPA policies.  (Such as keeping
the owner of the machine from reading sealed documents, or reading the
contents of DRM policy controlled documents without meeting the
requirements for the DRM policy.)
2. DSS over-again -- a big aspect of the DSS reverse-engineering was
to allow DVDs to be played in software on linux.  The TCPA platform
seems to have the primary goal of making a framework within which it
is possible to build extensions to implement hardware tamper resistant
DRM.  (The DRM implementation would run in a hardware assisted code
compartment as described in feature 3 above).  So now where does that
put open source platforms?  Will they be able to read such DRM
protected content?  It seems likely that in the longer term the DRM
platform will include video cards without access to video memory,
perhaps encryption of the video signal out to the monitor, and of
audio out to the speakers.  (There are other existing schemes to do
these things which dovetail into the likely TCPA DRM framework.)  With the secure boot strap described in feature 1, the video card and
so on are also part of the boot strap process, so the DRM system would
have ready support from the platform for robustly refusing to play
except on certain types of hardware.  Similarly the application
software which plays these DRM policy protected files and talks to the
DRM policy module in the hardware assisted code compartment will
itself be an application which uses the security boot-strapping
features.  So it won't be possible to write an application on for
example linux to play these files without an audit and license etc
from various content, DRM and OS cartels.  This will lead to exactly
the kind of thing Richard Stallman talked about in his prescient paper
on the coming platform and right to develop competing software control
wars [9].
3. Privacy support is broken -- the "privacy" features while clearly
attempts to defuse a re-run at the pentium serial number debacle, have
not really fixed it's problems.  You have to trust the "Trusted Third
Party" privacy CA not to track you and not to collude with other CAs
and software vendors.  There are known solutions to this particular
sub-problem, for example Stefan Brands digital credentials [10], which
can be used to build a cryptographically assured privacy preserving
PKI avoiding the linking problems arising from identity based and
attribute certificates.
4. Strong enforcement for DMCA DRM excesses -- the types of DRM system
which the platform enables stand a fair chance of providing high
levels of enforcement for things which though strictly legally
mandated (copyright licensing restrictions, limited number of plays of
CDs / DVDs other disadvantageous schemes; inflexible and usurious
software licensing), if enforced strictly would have deleterious
effects on society and freedom.  Copyright violation is widely
practiced to a greater or less extent by just about all individuals.
It is widely viewed as acceptable behavior.  These social realities
and personal freedoms are not taken into account or represented in the
lobbying schemes which lead to the media cartels obtaining legal
support for the erosion of users rights and expansionist power grabs
in DMCA, WIPO etc.
Some of these issues might be not so bad except for the track records,
and obvious monopolistic tendencies and economic pressures on the
entities who will have the root keys to the worlds computers.  There
will be no effect choice or competition due to existing near
monopolies, or cartelisation in the hardware, operating system, and
content distribution conglomerates.
5. Strong enforcement for the software renting model -- the types of
software licensing policy enforcement that can be built with the
platform will also start to strongly enable the software and object
rental ideas.  Again potentially these models have some merit except
that they will be sabotaged by API lock out, where the root key owners
will be able to charge monopoly rents for access to APIs.
6. Audits and certification become vastly more prevalent.  Having had
some involvement with software certification (FIPS 140-1 / CC) I can
attest that this can be expensive exercises.  It is unlikely that the
open source community will be able to get software certified due to
cost (the software is free, there is no business entity to claim
ownership of the certification rights, and so no way to recuperate the
costs).  While certification where competition is able to function is
a good thing, providing users with a transparency and needed
assurance, the danger with tying audits to TCPA is that it will be
another barrier to entry for small businesses, and for open source
7. Untrusted, unauditable software will be able to run without
scrutiny inside the hardware assisted code compartments.  Some of the
documentation talks about open sourcing some aspects.  While this may
come to pass, but that sounded like the TOR (Trusted Operating Root);
other extension modules also running in unauditable compartments will
not be so published.
8. Gives away root control of your machine -- providing potentially
universal remote control of users machines to any government agencies
with access to the TCPA certification master keys, or policies
allowing them to demand certifications on hostile code on demand.
Central authorities are likely to be the only, or the default
controllers of the firmware/software upgrade mechanism which comes as
part of the secure bootstrap feature.
9. Provides a dangerously tempting target for government power-grabs

@_date: 2002-08-05 05:48:01
@_author: Adam Back 
@_subject: "trust me" pseudonyms in TCPA (Re: Other uses of TCPA) 
I haven't read the TCPA detailed spec yet (next on TCPA/Palladium list
of reading material), but this bit I can infer I think:
The corresponding public key is certified by the secure hardware
manufacturer, I think.
Then they have this privacy CA which accepts requests signed by the
platform's signature key, and gives in return a certified pseudonym of
the users choice.  They claim this gives privacy, which it only does
if you trusted the "privacy CA" -- the privacy CA can link all of your
anonymous and pseudonymous credentials.  (Anonymous may want to
straighten out the different keys names -- I think there are some
encryption, some signature, some sealing keys derived from other
secret keys and the checksum of the application and OS / firmware
Brands digital credentials could be used to fix this sub-problem I
They put in the privacy CA thing as a defense against the PR problems
Intel had with the pentium serial number.  The FAQs at
 talk about this arguing how this is better than
pentium serial number at avoiding linkability.
The documentation problem I find is there isn't much documentation
available which is technical except for the 330 page spec which drops
right down to implementation details in RFC standards style.

@_date: 2002-08-06 23:57:43
@_author: Adam Back 
@_subject: Re: USENIX Security TCPA/Palladium Panel Wednesday 
Anonymous: clearly Lucky and Ross have been talking about two aspects
of the TCPA and Palladium platforms:
1) the implications of platform APIs planned for first phase
implementation based on the new platform hardware support;
2) the implications of the fact that the owner of the machine is
locked out from the new ring-0;
For 2) one obviously has to go beyond discussing the implications of
the APIs discussed in the documents, so the discussion has included
other APIs that could be built securely with their security rooted in
the new third-party controlled ring-0.
In my initial two messages looking at implications I did try to
clearly distinguish between documented planned APIs and new APIs that
become possible to build with third-party controlled ring-0s.  Other areas where analysis is naturally deviating from the aspects
covered by the available documentation (such as it is) are:
- discussion of likelihood that a given potential API will be built
- looking at history of involved parties:
  - Intel: pentium serial number
  - Microsoft: litany of anti-competetive and unethical business
    practices,   - governments: history of trying to push key-escrow, censorship,
    thought-crime and technologies and laws attempting to enforce
    these infringements of personal freedom
  - RIAA/MPAA: history of lobbying for legislation such as DMCA,
    eroding consumer rights
  - industry/government collaboration: Key Recovery Alliance
    ( which shows an interesting intersection of
    big-companies who are currently and historically were signed on to
    assist the government in deploying key-escrow
- suspicion that the TCPA/Microsoft are putting their own spin and
practicing standard PR techniques: like selective disclosure,
misleading statements, disclaiming planned applications and hence not
taking everything at face value.  TCPA/Microsoft have economic
pressures to spin TCPA/Palladium positively. - analysis is greatly hampered by the lack of definitive, concise,
clearly organized technical documentation.  Some of the main
informative documents even microsoft is pointing at are like personal
blog entries and copies of personal email exchanges.
a number of your responses have been of the form "hey that's not a
fair argument, what section number in the TCPA/Palladium documents
gives the specification for that API".
I suspect some arguing about the dangers of TCPA/palladium feel no
particular obligation to point out this distinction the fact that an
API is not planned in phase 1, or not publicly announced yet offers
absolutely no safe-guard against it's later deployment.

@_date: 2002-08-06 23:57:43
@_author: Adam Back 
@_subject: Re: USENIX Security TCPA/Palladium Panel Wednesday 
Anonymous: clearly Lucky and Ross have been talking about two aspects
of the TCPA and Palladium platforms:
1) the implications of platform APIs planned for first phase
implementation based on the new platform hardware support;
2) the implications of the fact that the owner of the machine is
locked out from the new ring-0;
For 2) one obviously has to go beyond discussing the implications of
the APIs discussed in the documents, so the discussion has included
other APIs that could be built securely with their security rooted in
the new third-party controlled ring-0.
In my initial two messages looking at implications I did try to
clearly distinguish between documented planned APIs and new APIs that
become possible to build with third-party controlled ring-0s.  Other areas where analysis is naturally deviating from the aspects
covered by the available documentation (such as it is) are:
- discussion of likelihood that a given potential API will be built
- looking at history of involved parties:
  - Intel: pentium serial number
  - Microsoft: litany of anti-competetive and unethical business
    practices,   - governments: history of trying to push key-escrow, censorship,
    thought-crime and technologies and laws attempting to enforce
    these infringements of personal freedom
  - RIAA/MPAA: history of lobbying for legislation such as DMCA,
    eroding consumer rights
  - industry/government collaboration: Key Recovery Alliance
    ( which shows an interesting intersection of
    big-companies who are currently and historically were signed on to
    assist the government in deploying key-escrow
- suspicion that the TCPA/Microsoft are putting their own spin and
practicing standard PR techniques: like selective disclosure,
misleading statements, disclaiming planned applications and hence not
taking everything at face value.  TCPA/Microsoft have economic
pressures to spin TCPA/Palladium positively. - analysis is greatly hampered by the lack of definitive, concise,
clearly organized technical documentation.  Some of the main
informative documents even microsoft is pointing at are like personal
blog entries and copies of personal email exchanges.
a number of your responses have been of the form "hey that's not a
fair argument, what section number in the TCPA/Palladium documents
gives the specification for that API".
I suspect some arguing about the dangers of TCPA/palladium feel no
particular obligation to point out this distinction the fact that an
API is not planned in phase 1, or not publicly announced yet offers
absolutely no safe-guard against it's later deployment.

@_date: 2002-08-06 03:46:37
@_author: Adam Back 
@_subject: (fwd) Re: dangers of TCPA/palladium 
Response from Peter Biddle on cryptography list.  (I think he is a
microsoft tech manager involved with palladium from a quick google).
Sent: Sunday, August 04, 2002 10:00 PM
Like anonymous and Adam, I have also been reading lots on Palladium lately.
I have also been working on Pd since 1997.
I agree, and from my perspective this is a problem. We have a great deal of
information we need to get out there.
We have done technical reviews of Palladium, as shown by Seth Schoen's notes
(a), which I think talk directly about many of the things discussed in this
thread. I suggest anyone who wants to start to understand Pd read these
You don't cite the MS whitepaper. This is not a technical paper but it does
set precedent and declare intent. See (b).
The suggestions for TCPA responses that William Arbaugh raises seem quite
good (c). 1 and 2 are already true for Pd, I believe that 3 is true but I
would need to talk with him about what he means here to confirm it, 4 is
covered in Eric Norlin's blog (d), and 5 is something we should do.
The current TPM (version 1.1) doesn't have the primitives which we need to
support Palladium, and the privacy model is different. We are working within
TCPA to get the instruction set aligned so that Palladium and TCPA could use
future silicon for attestation, sealing, and authentication, but as things
stand today the approaches to the two of them are different enough so that
TCPA 1.1 can't support Pd.
Pd is an OS feature set based on new hardware. Pd requires changes to the
CPU, chipset and/or memory controller, graphics and USB, as well as new
silicon (we call an SCP or SSP), . Microsoft currently has no announced
plans to support TCPA directly, and as things stand today there is no SW or
HW compatibility between the two.
This is not how Palladium works. Palladium loads a small piece of code
called the TOR after the OS has booted and is running (this could be days
later). Pd treats the BIOS, firmware, and privileged Windows OS code as
untrusted. Pd doesn't care if the SW is certified or not - that is a
question left to users.
In Palladium, SW can actually know that it is running on a given platform
and not being lied to by software. In 1, you say that SW virtualization
doesn't work, and that is part of the design. (Pd can always be lied to by
HW - we move the problem to HW, but we can't make it go away completely).
As SW is capable of knowing its own state, it can attest this state to
others - users, services, other apps, etc. It can't lie when it uses Pd to
say what it is. It's up to third parties (again, the user of the machine, or
an app, or service) to decide if it likes the answer and trusts the
application. Disclosure of the apps identity is up to the user and no one
Note that in Pd no one but the user can find out the totality of what SW is
running except for the nub (aka TOR, or trusted operating root) and any
required trusted services. So a service could say "I will only communicate
with this app" and it will know that the app is what it says it is and
hasn't been perverted. The service cannot say "I won't communicate with this
app if this other app is running" because it has no way of knowing for sure
if the other app isn't running.
Confusion. The memory isn't encrypted, nor are the apps nor the TOR when
they are on the hard drive. Encrypting the apps wouldn't make them more
secure, so they aren't encrypted. The CPU uses HW protections to wall new
running programs from the rest of the system and from each other. No one but
the app itself, named third parties, and the TOR can see into this apps
space. In fact, no one (should the app desire) can even know that the app is
running at all except the TOR, and the TOR won't report this information to
anyone without the apps permission. You can know this to be true because the
TOR will be made available for review and thus you can read the source and
decide for yourself if it behaves this way.
Correct enough for this thread; it is actually the TOR that will manage the
keys for the apps, as this makes the concept of migration and data roaming
far more manageable. (Yes, we have thought about this.)
Comparing xBox and Pd isn't particularly fruitful - they are different
problems and thus very different solutions. (Also note that xBox doesn't use
the PID or any other unique HW key.)
Palladium mostly doesn't care about the BIOS and considers it to be an
untrusted system component. In Pd the BIOS can load any OS it wants, just
like today, and in Pd the OS can load any TOR specified by the user. The MS
TOR will run any app, as specified by the user. The security model doesn't
depend on some apps being prevented from running.
I believe that there isn't a single thing you can do with your PC today
which is prevented on a Palladium PC. I am open to being challenged on this,
so please let me know what you think you won't be able to do on a Pd PC that
you can do today.
I think you mean CSS, not DSS.
I don't want people snooping my passwords from the keyboard buffer, nor my
account info from the frame buffer, and HW protections in those HW areas
prevent that.
Palladium doesn't boot strap the OS. Pd loads a secure piece of SW, called
the TOR, which runs in a secure space and loads other apps that want
security. Anyone can load an app into this environment and get the full
protections Pd offers. MS doesn't require that you show them the SW first -
you wanna run, you get to run - provided the user wants you to run. If a
user doesn't like the looks of your app, then you (the developer) have a
problem with that user.
The privacy model in Pd is different from TCPA. I could go on for a long
time about it, but the key difference is that the public key is only
revealed to named third parties which a user trusts. You are right in
thinking that you need to trust them, but you don't have to show anyone your
key if you don't trust them, so you (the user) are always in control of
Pd is not about user authentication - it is about machine and SW
authentication. User auth can be better solved on a Pd platform than on a PC
today, but it isn't required. Pd doesn't need to know who you are to work.
I don't know where to begin on this one. It deserves a long, well thought
out response, and I don't have the time to do it at the moment. I will
follow up on this. Let me state that I think that much of the energy around
DRM and HW is misplaced, and that Pd is designed to enable seamless
distribution of encrypted information, not to disable distribution of clear
text information.
MS will not have the root keys to the world's computers. The TOR won't have
access to the private keys either. No one but the HW does. The TOR isn't
"MS" per se - it is a piece of SW written by users but vetted and examined
by hopefully thousands of parties and found to do nothing other than manage
the local security model upon which Pd depends. You can read it and know it
doesn't do anything but effectively manage keys and applications. And if you
don't trust it, you won't run it.
If you don't trust the TOR, you don't trust Palladium. Trust is the *only*
feature we are attempting to achieve, so every decision we make will be made
with trust and security in mind.
I am confused as to how this would work in Pd. Anyone can write apps to the
Pd API. Zero restrictions. (API's are full of restrictions - by their nature
they limit things to a protocol, and potentially HW, both of which have
understood limitations; I am dodging this concept in saying there are no
This is a problem anyone who wants to compete in the security and trust
space will need to overcome. I don't think that it is particularly new or
different in a world with Pd. Writing a TOR is going to be really hard and
will require processes and methods that are alien to many SW developers. One
example (of many) is that we are generating our header files from specs. You
don't change the header file, you change the spec and then gen the header.
This process is required for the highest degrees of predictability, and
those are cornerstones for the highest degree of trust. Unpredictable things
are hard to trust.
Everything in the TCB (Trusted Computing Base) for Pd will be made available
for review to anyone who wants to review it;  this includes software which
the MS TOR mandates must be loaded.
This doesn't happen in Pd. There is no secure boot strap feature in Pd. The
BIOS boots up the PC the same way it does today. Root control is held by the
owner of the machine. There is no certification master key in Pd.
One of the beauties of Pd is that if there is any SW backdoor, you will know
about it. HW robustness will be something for manufacturers to work out. For
most systems, I think that extensive HW tamper resistance will be a waste of
time, but for some (e.g. highly secure govt systems) it will be a necessity
and one that works well in Pd.
I know that we aren't using undocumented API's and that we will strive for
the highest degree of interoperability and user control possible. Pd
represents massive de-centralization of trust, not the centralization of it.
I think that time is going to have to tell on this one. I know that this
isn't true. You think that it is. I doubt that my saying it isn't true is
going to change your mind; I know that the technology won't do much of what
you are saying it does do, but I also know that some of these things boil
down to suspicion around intent, and only time will show if my intent is
aligned with my stated goals.
Pd does not give root control of your machine to someone else. It puts it
into your hands, to do with as you so desire, including hacking away at it
to your hearts content.
I think that Pd represents an enhancement to personal freedoms and user
control over their machines. I hope that over time I will be able to explain
Pd sufficiently well so that you have all the facts you need to understand
how and why I say this.
(a) Seth Schoens Blog (b) MS Paper
(c) William Arbaugh on TCPA
(d) Eric Norlin's blog

@_date: 2002-08-12 18:30:00
@_author: Adam Back 
@_subject: Re: Palladium: technical limits and implications 
Peter Biddle, Brian LaMacchia or other Microsoft employees could
short-cut this guessing game at any point by coughing up some details.
Feel free guys... enciphering minds want to know how it works.
(Tim Dierks: read the earlier posts about ring -1 to find the answer
to your question about feasibility in the case of Palladium; in the
case of TCPA your conclusions are right I think).
I thought we went over this before?  My hypothesis is: I presumed
there would be a stub TOR loaded bvy the hardware.  The hardware would
allow you to load a new TOR (presumably somewhat like loading a new
BIOS -- the TOR and hardware has local trusted path to some IO
I don't know what leads you to this conclusion.
How would the OS or user mode apps communicate with trusted agents
with this model?  The TOR I think would be the mediator of these
communications (and of potential communications between trusted
agents).  Before loading a real TOR, the stub TOR would not implement
talking to trusted agents.
I think this is also more symmstric and therefore more likely.  The
trusted agent space is the same as supervisor mode that the OS runs
in.  It's like virtualization in OS360: there are now multiple "OSes"
operating under a micro-kernel (the TOR in ring -1): the real OS and
the multiple trusted agents.  The TOR is supposed to be special
purpose, simple and small enough to be audited as secure and stand a
chance of being so.
The trusted agents are the secure parts of applications (dealing with
sealing, remote attestation, DRM, authenticated path to DRM
implementing graphics cards, monitors, sound cards etc; that kind of
thing).  Trusted agents should also be small, simple special purpose
to avoid them also suffering from remote compromise.  There's limited
point putting a trusted agent in a code compartment if it becomes a
full blown complex application like MS word, because then the trusted
agent would be nearly as likely to be remotely exploited as normal
trusted-agents will also need to use OS services, the way you have it
they can't.
I don't think it's a big problem to replace a stub TOR with a given
TOR sometime after OS boot.  It's analogous to modifying kernel code
with a kernel module, only a special purpose micro-kernel in ring -1
instead of ring 0.  No big deal.
In TCPA which does not have a ring -1, this is all the TPM does
(compute metrics on the OS, and then have the OS compute metrics on
While Trusted Agent space is separate and better protected as there
are fewer lines of code that a remote exploit has to be found in to
compromise one of them, I hardly think Palladium would discard the
existing windows driver signing, code signing scheme.  It also seems
likely therefore that even though it offers lower assurance the code
signing would be extended to include metrics and attestation for the
OS, drivers and even applications.
I take this to mean that as stated somewhere in the available docs the
OS can not observe or even know how many trusted agents are running.
So he's stating that they've made OS design decisions such that the OS
could not refuse to run some code on the basis that a given Trusted
Agent is running.
This functionality however could be implemented if so desired in the

@_date: 2002-08-12 16:14:42
@_author: Adam Back 
@_subject: Re: Palladium: technical limits and implications 
Here's a slightly updated version of the diagram I posted earlier:
+---------------+------------+  +----------------------------+  +----------------------------+  Integrity Metrics in a given level are computed by the level below.
The TOR starts Trusted Agents, the Trusted Agents are outside the OS
control.  Therefore a remote application based on remote attestation
can know about the integrity of the trusted-agent, and TOR.
ring -1/TOR is computed by SCP/hardware; Trusted Agent is computed by
The parallel stack to the right: OS is computed by TOR; Application is
computed OS.
So for general applications you still have to trust the OS, but the OS
could itself have it's integrity measured by the TOR.  Of course given
the rate of OS exploits especially in Microsoft products, it seems
likley that the aspect of the OS that checks integrity of loaded
applications could itself be tampered with using a remote exploit.
Probably the latter problem is the reason Microsoft introduced ring -1
in palladium (it seems to be missing in TCPA).

@_date: 2002-08-12 16:14:42
@_author: Adam Back 
@_subject: Re: Palladium: technical limits and implications 
Here's a slightly updated version of the diagram I posted earlier:
+---------------+------------+  +----------------------------+  +----------------------------+  Integrity Metrics in a given level are computed by the level below.
The TOR starts Trusted Agents, the Trusted Agents are outside the OS
control.  Therefore a remote application based on remote attestation
can know about the integrity of the trusted-agent, and TOR.
ring -1/TOR is computed by SCP/hardware; Trusted Agent is computed by
The parallel stack to the right: OS is computed by TOR; Application is
computed OS.
So for general applications you still have to trust the OS, but the OS
could itself have it's integrity measured by the TOR.  Of course given
the rate of OS exploits especially in Microsoft products, it seems
likley that the aspect of the OS that checks integrity of loaded
applications could itself be tampered with using a remote exploit.
Probably the latter problem is the reason Microsoft introduced ring -1
in palladium (it seems to be missing in TCPA).

@_date: 2002-08-07 17:54:31
@_author: Adam Back 
@_subject: Palladium: technical limits and implications (Re: more TCPA stuff (Re: "trust me" pseudonyms in TCPA 
I have one gap in the picture: In a previous message in this Peter Biddle said:
But what feature of Palladium stops someone taking a Palladium enabled
application and running it in a virtualized environment.  ie They
write software to emulate the SCP, sealing and attestation APIs.
Any API calls in the code to verify non-virtualization can be broken
by putting a different endoresment root CA public key in the
virtualized SCP.
The Palladium application (without the remote attestation feature) has
no way to determine that it is not virtualized.  If the Palladium
application contains the endoresement root CA key that can be changed.
If the application relies on the SCP to contain the endoresmenet key
but doesn't verify it that can be virtualized with a replacement fake
endoresment root CA public key using the existing SCP APIs.
Then Palladiumized application could be debugged and it's features
used without the Palladium non-virtualization guarantee.
Am I free to do this as the owner of palladium compatible hardware
running a version of windows with the proposed palladium feature set?
If so how does this reconcile with your earlier statement that:
Then we also have the remote attestation feature which can't be so
fooled, but for local attestation does the above break work, or is
there some other function preventing that?
Does that imply that the BORA protections only apply to:
- applications which call home to use remote attestation before
  - and even here the remote attestation has to be enforced to data
    separation levels -- ie it has to be that the server provides a
    required and central part of the application -- like providing the
    content, or keys to the content -- otherwise the remote
    attestation call can also be nopped out with no ill-effect (much
    as calls to dongles with no other effect than to check for
    existance of a dongle could be nopped)
- applications which are encrypted to a machine key which is buried in
  the SCP and endorsed by the hardware manufacturer
  - however you said in your previous mail that this is not planned
* now about my attempts to provide a concise, representative and
  readily understandable summary of what the essence of palladium is:
my previous attempt which you point out some flaws in:
OK that is true.  I presume you focussed on SCP because you took the
dongle to mean literally the SCP component alone.
Let's me try to construct an improved succint Palladium motivation and
function description.  We need to make clear the distinction that the
programmability comes from the hardware/firmware/software ensble
provided by:
hardware: the SCP, new ring0 and code compartmentatlization
(btw what are the Palladium terms for the new ring0 that the TOR runs
in, and what is the palladium term for the code compartment that
Trusted Agents run in -- I'd like to use consistent terminology)
super-kernel: TOR operating in new ring0
software: palladium enabled applications using the features such as
software attestation, and sealing with control rooted in hardware and
the TOR
So would it be fair to characterize the negative aspects of Palladium
as follows:
"Palladium provides an extensible, general purpose programmable
dongle-like functionality implemented by an ensemble of hardware and
software which provides functionality which can, and likely will be
used to expand centralised control points by OS vendors, Content
Distrbuters and Governments."
So I think that statement though obviously less possitively spun than
Microsoft would like perhaps addresses your technical objections with
the previous characterization.
btw I readily concede of course that Palladium platform offers the
security compartmentalization and software assurance aspects anonymous
and Peter Biddle have described; I am just mostly examining the
flip-side of the new boundary of applications buildable to data
separation standards of security with this platform.  One could
perhaps construct a more balanced characterization covering both
positive features and negative risks, but I'll let Palladium
proponents work on the former.
So to discuss your objections to the previous version of my attempted
Palladium characterization:
- as you say the hardware platform does not itself provide control
  points (I agree)
- as you say, the TOR being publicly auditable does not itself provide
  control points
however the platform can, and surely you can admit the risk, and even
the likelihood that it will in fact be used to continue and further
the existing business strategies of software companies, the content
industry and governments.
The dongle soldered to your motherboard can conspire with the CPU and
memory management unit to lock the user out of selected processes
running on their own machine.  If you focus on the subset of buildable applications which operate in
the users interests you can call this a good thing.  If you look at
the risks of buildable applications which can be used to act against
the users interests it can equally be a very dangerous thing.  Also if
you look at historic business practices, legal trends, and the wishes
of the RIAA/MPAA there are risks that these bad practices and trends
will be futher accelerated, exarcerbated and more strongly enforced.
I'd be interested to see you face and comment on these negative
aspects rather than keep your comments solely on beneficial user
functions, claim neutrality of low level features, and disclaim
negative aspects by claiming at a low level user choice.  The low
level user choice may in the long run prove impractical or almost
impossible for novice users, or even advanced users without hardware
hacking tools, to technically exercise.  (I elaborated on the
possiblities for robust format compatibility prevention, and related
possibilities a previous mail.)
I mean it is programmable in the sense that software attestation,
certification and the endoresed new privileged ring0 code, together
with the hardware enforced code compartments allow the protections of
the firmware and hardware running in the SCP to be extended up to
complete applications -- the Trusted Agents running in code
That makes it general purpse because it is programmable.  It could be
used to build many classes of application across a spectrum of good,
debatable and evil:
- more flexibile and secure anonymity systems (clear user
  self-interest as anonymous suggested)
- DRM (mixed positive and negative, debatable depends on your point of
  view)
- and for example key-escrow of SCP support crypto functions
  implemented in the TOR accessed with say CAPI (very negative)
You're focussing on the low level platform specifics, not on the
bigger implications of the overall hardware, TOR, OS and software
ensemble which I was talking about.  Yes you can run different TORS.
But using a specific (and audited published) TOR which the user may
find himself choosing to run in order to run Palladium-enabled
applications, all the applications, file formats, services and content
which are tied to doing that -- control points could and likely will
be built.
It is my opinion that the directions and business pressures are such
that meaningful distributed control is unlikely to be the long term
result of the things that will be built using the Palladium
hardware/software ensemble.
This seems a fairly clearly consistent and predictable outcome, unless
the software, IP law, RIAA/DMCA and legal systems all make an
_astounding_ complete U-turn in policy and tacitcs coincident with the
deployment of Palladium.  Can you defend the arguement that Palladium and TCPA don't change the
balance of control against the user and against personal control in
our current balance between technical feasibility of building software
systems enforcing third party control and law?
(btw I'll stop nagging about documentation now, previous mail crossed
with your reply on the topic).
"You do not examine legislation in the light of the benefits it will
convey if properly administered, but in the light of the wrongs it
would do and the harms it would cause if improperly administered."

@_date: 2002-08-07 17:54:31
@_author: Adam Back 
@_subject: Palladium: technical limits and implications (Re: more TCPA stuff (Re: "trust me" pseudonyms in TCPA 
I have one gap in the picture: In a previous message in this Peter Biddle said:
But what feature of Palladium stops someone taking a Palladium enabled
application and running it in a virtualized environment.  ie They
write software to emulate the SCP, sealing and attestation APIs.
Any API calls in the code to verify non-virtualization can be broken
by putting a different endoresment root CA public key in the
virtualized SCP.
The Palladium application (without the remote attestation feature) has
no way to determine that it is not virtualized.  If the Palladium
application contains the endoresement root CA key that can be changed.
If the application relies on the SCP to contain the endoresmenet key
but doesn't verify it that can be virtualized with a replacement fake
endoresment root CA public key using the existing SCP APIs.
Then Palladiumized application could be debugged and it's features
used without the Palladium non-virtualization guarantee.
Am I free to do this as the owner of palladium compatible hardware
running a version of windows with the proposed palladium feature set?
If so how does this reconcile with your earlier statement that:
Then we also have the remote attestation feature which can't be so
fooled, but for local attestation does the above break work, or is
there some other function preventing that?
Does that imply that the BORA protections only apply to:
- applications which call home to use remote attestation before
  - and even here the remote attestation has to be enforced to data
    separation levels -- ie it has to be that the server provides a
    required and central part of the application -- like providing the
    content, or keys to the content -- otherwise the remote
    attestation call can also be nopped out with no ill-effect (much
    as calls to dongles with no other effect than to check for
    existance of a dongle could be nopped)
- applications which are encrypted to a machine key which is buried in
  the SCP and endorsed by the hardware manufacturer
  - however you said in your previous mail that this is not planned
* now about my attempts to provide a concise, representative and
  readily understandable summary of what the essence of palladium is:
my previous attempt which you point out some flaws in:
OK that is true.  I presume you focussed on SCP because you took the
dongle to mean literally the SCP component alone.
Let's me try to construct an improved succint Palladium motivation and
function description.  We need to make clear the distinction that the
programmability comes from the hardware/firmware/software ensble
provided by:
hardware: the SCP, new ring0 and code compartmentatlization
(btw what are the Palladium terms for the new ring0 that the TOR runs
in, and what is the palladium term for the code compartment that
Trusted Agents run in -- I'd like to use consistent terminology)
super-kernel: TOR operating in new ring0
software: palladium enabled applications using the features such as
software attestation, and sealing with control rooted in hardware and
the TOR
So would it be fair to characterize the negative aspects of Palladium
as follows:
"Palladium provides an extensible, general purpose programmable
dongle-like functionality implemented by an ensemble of hardware and
software which provides functionality which can, and likely will be
used to expand centralised control points by OS vendors, Content
Distrbuters and Governments."
So I think that statement though obviously less possitively spun than
Microsoft would like perhaps addresses your technical objections with
the previous characterization.
btw I readily concede of course that Palladium platform offers the
security compartmentalization and software assurance aspects anonymous
and Peter Biddle have described; I am just mostly examining the
flip-side of the new boundary of applications buildable to data
separation standards of security with this platform.  One could
perhaps construct a more balanced characterization covering both
positive features and negative risks, but I'll let Palladium
proponents work on the former.
So to discuss your objections to the previous version of my attempted
Palladium characterization:
- as you say the hardware platform does not itself provide control
  points (I agree)
- as you say, the TOR being publicly auditable does not itself provide
  control points
however the platform can, and surely you can admit the risk, and even
the likelihood that it will in fact be used to continue and further
the existing business strategies of software companies, the content
industry and governments.
The dongle soldered to your motherboard can conspire with the CPU and
memory management unit to lock the user out of selected processes
running on their own machine.  If you focus on the subset of buildable applications which operate in
the users interests you can call this a good thing.  If you look at
the risks of buildable applications which can be used to act against
the users interests it can equally be a very dangerous thing.  Also if
you look at historic business practices, legal trends, and the wishes
of the RIAA/MPAA there are risks that these bad practices and trends
will be futher accelerated, exarcerbated and more strongly enforced.
I'd be interested to see you face and comment on these negative
aspects rather than keep your comments solely on beneficial user
functions, claim neutrality of low level features, and disclaim
negative aspects by claiming at a low level user choice.  The low
level user choice may in the long run prove impractical or almost
impossible for novice users, or even advanced users without hardware
hacking tools, to technically exercise.  (I elaborated on the
possiblities for robust format compatibility prevention, and related
possibilities a previous mail.)
I mean it is programmable in the sense that software attestation,
certification and the endoresed new privileged ring0 code, together
with the hardware enforced code compartments allow the protections of
the firmware and hardware running in the SCP to be extended up to
complete applications -- the Trusted Agents running in code
That makes it general purpse because it is programmable.  It could be
used to build many classes of application across a spectrum of good,
debatable and evil:
- more flexibile and secure anonymity systems (clear user
  self-interest as anonymous suggested)
- DRM (mixed positive and negative, debatable depends on your point of
  view)
- and for example key-escrow of SCP support crypto functions
  implemented in the TOR accessed with say CAPI (very negative)
You're focussing on the low level platform specifics, not on the
bigger implications of the overall hardware, TOR, OS and software
ensemble which I was talking about.  Yes you can run different TORS.
But using a specific (and audited published) TOR which the user may
find himself choosing to run in order to run Palladium-enabled
applications, all the applications, file formats, services and content
which are tied to doing that -- control points could and likely will
be built.
It is my opinion that the directions and business pressures are such
that meaningful distributed control is unlikely to be the long term
result of the things that will be built using the Palladium
hardware/software ensemble.
This seems a fairly clearly consistent and predictable outcome, unless
the software, IP law, RIAA/DMCA and legal systems all make an
_astounding_ complete U-turn in policy and tacitcs coincident with the
deployment of Palladium.  Can you defend the arguement that Palladium and TCPA don't change the
balance of control against the user and against personal control in
our current balance between technical feasibility of building software
systems enforcing third party control and law?
(btw I'll stop nagging about documentation now, previous mail crossed
with your reply on the topic).
"You do not examine legislation in the light of the benefits it will
convey if properly administered, but in the light of the wrongs it
would do and the harms it would cause if improperly administered."

@_date: 2002-08-07 21:21:42
@_author: Adam Back 
@_subject: Re: Challenge to TCPA/Palladium detractors 
The TCPA/Palladium folks have been working on this for apparently
around 5 years.  We don't yet have a complete definition of what
Palladium is, but anyway...
It may be that some interesting hardware, TOR and OS design changes
could be added which could change the balance.  Other aspects are as
John Denker said more to do with who will control keys and policies
and how much effective user control and choice remains over these
My initial thoughts were around hardware and TOR enforced in-flow and
out-flow control to trusted agents.
This idea was seeded by the smart-card setting of Stefan Brands
digital credentials.  (Read [1] if you are interested, it's a very
clever idea, related to observers in cryptogaphic protocols in
hardware settings).
Briefly the observer in Brands protocol (and observers have been
proposed in other cryptogaphic literature also) tackles an analogous
problem with cryptographic assurance in the special purpose case of
privacy preserving credentials, e-cash and other applications that can
be built from those techniques.
You have a tamper-resistant smart card.  However the user can't
reasonably audit the behavior of the smart-card processor because it
intentionally hides it's keys from the user.  Even if the source is
published, audited, and claims and endoresments about the hardware
made, the user still can't easily audit or reasonablly trust what is
actually in his smart-card.
The tamper-resistant smart card is somewhat related to the crypto
functions of the SCP in Palladium or the TPM in TCPA, but the observer
approach may offer lessons for TCPA/Palladium in general at higher
The tamper-resistant smart card is considered untrusted and hostile to
user privacy.  The tamper-resistant smart card processor and software
is acting in the interests of the credential issuer / ecash issuer to
prevent the user double-spending coins (*) / using credentials more
times than allowed.
The user has a general purpose computer running software he can
completely audit, control observe running and modify.  The smart-card
has to make all communications with ecash acccepting merchants,
certificate verifiers etc via the general purpose computer the
smart-card is connected to.  The general purpose computer implements
the observer protocols.
The smart-card setting variant of Brands protocol cryptographically
assures 2 things:
- the ecash issuer / credential CA can be assured that the user can
not double spend (or in general violate other properties mediated by
the tamper-resistant smart card)
- the user is cryptographically assured that the smart-card can not
invade his privacy.  This works because the in-flows and out-flows to the smart card are
hardware assured to pass via the general purpose computer, auditable,
use published formats and are cryptographically blinded, to the extent
of optimally frustrating even subliminal channels, via steganography
and the like.
In the same way that TCPA/Palladium are a generalisation of the dongle
concept, this would be a generalisation of the cryptographic concept
of observers.
So for your convenience here's a cut and paste of that initial thought
on applying the observer principle to general purpose TCPA/Palladium
platform from the previous message with subject "Palladium: hardware
layering model":
I wrote in that message:
this is not a fully fleshed out idea as I only thought of it
yesterday, and can't fully analyse it's implications because we don't
yet know proper details of what Palladium hardware is, nor how
microsofts proposed Palladium enhanced windows would be implemented on
that hardware.
(*) Actually he will still be caught and identified with Brands ecash
protocols when the coins are deposited if he does double-spend coins
after breaking hardware tamper-resistance, but that is a level of
detailed not central to this discussion.
[1] "A Technical Overview of Digital Credentials", Stefan Brands, Feb
2002, to appear in International Journal on Information Security.
See Section 8.

@_date: 2002-08-07 21:21:42
@_author: Adam Back 
@_subject: Re: Challenge to TCPA/Palladium detractors 
The TCPA/Palladium folks have been working on this for apparently
around 5 years.  We don't yet have a complete definition of what
Palladium is, but anyway...
It may be that some interesting hardware, TOR and OS design changes
could be added which could change the balance.  Other aspects are as
John Denker said more to do with who will control keys and policies
and how much effective user control and choice remains over these
My initial thoughts were around hardware and TOR enforced in-flow and
out-flow control to trusted agents.
This idea was seeded by the smart-card setting of Stefan Brands
digital credentials.  (Read [1] if you are interested, it's a very
clever idea, related to observers in cryptogaphic protocols in
hardware settings).
Briefly the observer in Brands protocol (and observers have been
proposed in other cryptogaphic literature also) tackles an analogous
problem with cryptographic assurance in the special purpose case of
privacy preserving credentials, e-cash and other applications that can
be built from those techniques.
You have a tamper-resistant smart card.  However the user can't
reasonably audit the behavior of the smart-card processor because it
intentionally hides it's keys from the user.  Even if the source is
published, audited, and claims and endoresments about the hardware
made, the user still can't easily audit or reasonablly trust what is
actually in his smart-card.
The tamper-resistant smart card is somewhat related to the crypto
functions of the SCP in Palladium or the TPM in TCPA, but the observer
approach may offer lessons for TCPA/Palladium in general at higher
The tamper-resistant smart card is considered untrusted and hostile to
user privacy.  The tamper-resistant smart card processor and software
is acting in the interests of the credential issuer / ecash issuer to
prevent the user double-spending coins (*) / using credentials more
times than allowed.
The user has a general purpose computer running software he can
completely audit, control observe running and modify.  The smart-card
has to make all communications with ecash acccepting merchants,
certificate verifiers etc via the general purpose computer the
smart-card is connected to.  The general purpose computer implements
the observer protocols.
The smart-card setting variant of Brands protocol cryptographically
assures 2 things:
- the ecash issuer / credential CA can be assured that the user can
not double spend (or in general violate other properties mediated by
the tamper-resistant smart card)
- the user is cryptographically assured that the smart-card can not
invade his privacy.  This works because the in-flows and out-flows to the smart card are
hardware assured to pass via the general purpose computer, auditable,
use published formats and are cryptographically blinded, to the extent
of optimally frustrating even subliminal channels, via steganography
and the like.
In the same way that TCPA/Palladium are a generalisation of the dongle
concept, this would be a generalisation of the cryptographic concept
of observers.
So for your convenience here's a cut and paste of that initial thought
on applying the observer principle to general purpose TCPA/Palladium
platform from the previous message with subject "Palladium: hardware
layering model":
I wrote in that message:
this is not a fully fleshed out idea as I only thought of it
yesterday, and can't fully analyse it's implications because we don't
yet know proper details of what Palladium hardware is, nor how
microsofts proposed Palladium enhanced windows would be implemented on
that hardware.
(*) Actually he will still be caught and identified with Brands ecash
protocols when the coins are deposited if he does double-spend coins
after breaking hardware tamper-resistance, but that is a level of
detailed not central to this discussion.
[1] "A Technical Overview of Digital Credentials", Stefan Brands, Feb
2002, to appear in International Journal on Information Security.
See Section 8.

@_date: 2002-08-08 01:48:19
@_author: Adam Back 
@_subject: Re: Palladiated? (was re: wow - palladiated! (Re: Palladium: 
[Trimmed Cc a bit, I'll let Bob decide where it goes beyond this].
Now that Bob coined the neologism "palladiated" (blame Bob -- my
"palladiumized" was not in jest, just used in the middle of a tech
discussion) it has to be done, so I asked the universal oracle
(google.com) about palladium and half-life, and lo the Pd-103 isotope
has a 17-day half-life, and Pd-109 of 13.5 hours and are classified as
having moderate radiotoxicity.  Unfortunately for Bob's neologism not
quite up there with fission grade isotopes like like Plutonium which
rate as very high toxicity, but still you wouldn't want to ingest to
much of the stuff...
Pd isotopes are obtained by bombarding Gold with neutrons apparently.
Anyway, now back to the intersting tech discussion on the balance of
of owner vs third party control in the MS Palladium and TCPA

@_date: 2002-08-08 05:34:15
@_author: Adam Back 
@_subject: Re: Palladium: hardware layering model 
No I think the above diagram is closer than what you propose.  Peter
also pointed us at Seth Schoen's blog [1] which is a write up of a
briefing Microsoft gave to EFF.  It contains the statement:
Looks consistent with my picture to me.
Your other objection:
I think would just be covered by the details of how the machine
switches from this picture:
to the one above.
For example imagine a default stub nub/TOR that leaves the new MMU
features wide open.  (Supervisor mode can access everything, no
Trusted Agent code compartments running).  The would be some API to
allow the supervisor mode code to load a TOR and switch the TOR code
to ring-0 while leaving the OS running in supervisor mode.
Or alternatively and with equivalent effect: with the boot state, the
OS runs in full ring-0 mode, but just isn't written to make use of any
of the extra ring-0 features.  When it switches to loading a nub/TOR
the OS is relagated to supervisor mode, some MMU permission bits are
juggled around and the TOR occupies ring-0, and the TOR is just an OS
micro-kernel which happens to be written to use the new hardware
features (code compartmentalization, new MMU features, sealing etc)
Clarification on this:
Not what I meant.  Say that you have some code that looks like this:
if ( ! remote_attest( /* ... */ ) ) { exit 0; }
then the remote attest is not doing anything apart from acting as a
remote dongle, so all I have to do to virtualize this code, or break
the licensing scheme based on the remote dongle is nop out the remote
attest verification, then the code can be run as a user application
rather than a trusted agent application and so can be run in a
debugger, have it's state examined etc.
If on the other hand the code says:
download_sealed_content( /* ... */ );
key = remote_attest_and_key_negotiate( /* ... */ );
decrypt_sealed_content( key, /* ... */ );
then nopping out the remote_attest will have a deleterious effect on
the applications function, and so virtualizing it with the remote
attests nopped out will not be useful in bypassing it's policies.

@_date: 2002-08-09 19:11:15
@_author: Adam Back 
@_subject: Re: Signing as one member of a set of keys 
Very nice.  Nice plausible set of candidate authors also:
pub  1022/5AC7B865 1992/12/01  loki
pub  1024/2B48F6F5 1996/04/10  Ian Goldberg pub  1024/97558A1D 1994/01/10  Pr0duct Cypher pub  1024/2719AF35 1995/05/13  Ben Laurie pub  1024/58214C37 1992/09/08  Hal Finney <74076.1041
pub  1024/C8002BD1 1997/03/04  Eric Young pub  1024/FBBB8AB1 1994/05/07  Colin Plumb Wonder if we can figure out who is most likely author based on coding
style from such a small set.
It has (8 char) TABs but other wise BSD indentation style (BSD
normally 4 spaces).  Also someone who likes triply indirected pointers
***blah in there.  Has local variables inside even *if code blocks*
eg, inside main() (most people avoid that, preferring to declare
variables at the top of a function, and historically I think some
older gcc / gdb couldn't debug those variables if I recall).  Very
funky use of goto in getpgppkt, hmmm.  Somewhat concise coding and
variable names.
Off the cuff guess based on coding without looking at samples of code
to remind, probably Colin or Ian.
Of course (Lance Cottrell/Ian Goldberg/Pr0duct Cypher/Ben Laurie/Hal
Finney/Eric Young/Colin Plumb) possibly deviated or mimicked one of
their coding styles.  Kind of interesting to see a true nym in there
Also the Cc -- Coderpunks lives?  I think the Cc coderpunks might be a
clue also, I think some of these people would know it died.  I think
that points more at Colin.
Other potential avenue might be implementation mistake leading to
failure of the scheme to robustly make undecidable which of the set is
the true author, given alpha code.

@_date: 2002-08-09 21:13:56
@_author: Adam Back 
@_subject: TCPA/Palladium -- likely future implications (Re: dangers of TCPA/palladium) 
The picture is related but has some extra wrinkles with the
TCPA/Palladium attestable donglization of CPUs.
- It is always the case that targetted people can have hardware
attacks perpetrated against them.  (Keyboard sniffers placed during
court authorised break-in as FBI has used in mob case of PGP using
Mafiosa [1]).
- In the clipper case people didn't need to worry much if the clipper
chip had malicious deviations from spec, because Clipper had an openly
stated explicit purpose to implement a government backdoor -- there's
no need for NSA to backdoor the explicit backdoor.
But in the TCPA/Palladium case however the hardware tampering risk you
identify is as you say relevant:
- It's difficult for the user to verify hardware.  - Also: it wouldn't be that hard to manufacture plausibly deniable
implementation "mistakes" that could equate to a backdoor -- eg the
random number generators used to generate the TPM/SCP private device
However, beyond that there is an even softer target for would-be
- the TCPA/Palladium's hardware manufacturers endoresment CA keys.
these are the keys to the virtual kingdom formed -- the virtual
kingdom by the closed space within which attested applications and
software agents run.
So specifically let's look at the questions arising:
1. What could a hostile entity(*) do with a copy of a selection of
hardware manufacturer endorsement CA private keys?
( (*) where the hostile entity candidates would be for example be
secret service agencies, law enforcement or "homeland security"
agencies in western countries, RIAA/MPAA in pursuit of their quest to
exercise their desire to jam and DoS peer-to-peer file sharing
networks, the Chinese government, Taiwanese government (they may lots
of equipment right) and so on).
a. Who needs to worry -- who will be targetted?
Who needs to worry about this depends on how overt third-party
ownership of these keys is, and hence the pool of people who would
likely be targetted.  If it's very covert, it would only be used plausibly deniably and only
for Nat Sec / Homeland Security purposes.  It if becomse overt over
time -- a publicly acknowledged, but supposedly court controlled
affair like Clipper, or even more widely desired by a wide-range of
entities for example: keys made available to RIAA / MPAA so they can
do the hacking they have been pushing for -- well then we all need to
To analyse the answer to question 1, we first need to think about
question 2:
2. What kinds of TCPA/Palladium integrity depending "trusted"
applications are likely to be built?
Given the powerful (though balance of control changing) new remotely
attestable security features provided by TCPA/Palladium, all kinds of
remote services become possible, for example (though all to the extent
of hardware tamper-resistance and belief that your attacker doesn't
have access to a hardware endorsement CA private key):
- general Application Service Providers (ASPs) that you don't have to
trust to read your data
- less traceable peer-to-peer applications
- DRM applications that make a general purpose computer secure against
BORA (Break Once Run Anywhere), though of course not secure against
ROCA (Rip Once Copy Everywhere) -- which will surely continue to
happen with ripping shifting to hardware hackers.
- general purpose unreadable sandboxes to run general purpose
CPU-for-rent computing farms for hire, where the sender knows you
can't read his code, you can't read his input data, or his output
data, or tamper with the computation.
- file-sharing while robustly hiding knowledge and traceability of
content even to the node serving it -- previously research question,
now easy coding problem with efficient
- anonymous remailers where you have more assurance that a given node
is not logging and analysing the traffic being mixed by it
But of course all of these distributed applications, positive and
negative (depending on your view point), are limited in their
assurance of their non-cryptographically assured aspects:
- to the tamper resistance of the device
- to the extent of the users confidence that an entity hostile to them
doesn't have the endorsement CA's private key for the respective
remote servers implementing the network application they are relying
and a follow-on question to question 2:
3. Will any software companies still aim for cryptographic assurance?
(cryptographic assurance means you don't need to trust someone not to
reverse engineer the application -- ie you can't read the data because
it is encrypted with a key derived from a password that is only stored
in the users head).
The extended platform allows you to build new classes of applications
which aren't currently buildable to cryptographic levels of assurance.
eg. It allows general purpose policies to be built just by writing
policy code that sits in a Trusted Agent code compartment, without
having to figure out how to do split-trust (a la mixmaster chaining),
or forward-secrecy or secret-sharing or any of the other funky stuff;
you can just implement some policy code and it becomes so.
The danger is people will use it to build applications with squishy
interiors, with no cryptographic assurance.  Forward-secrecy
implemented only by a policy in a Trusted Agent that sets a time-limit
on access.  Anonymity but only in the sense that you trust the
hardware isn't tampered with, etc etc.
It will be really tempting because:
- it's much easier: network distributed crypto protocols are
relatively complex
- you can build things you can't otherwise build, the are currently
unsolved problems with distributed crypto protocols
- even where good crypto protocols exist, people will defend not using
them by claims to paranoia: "What you think the NSA has tampered with
your CPU?", or just laziness, cost of implementation etc
So in short probably mostly the answer will be "No", people won't
still aim for cryptographic assurance.
And so a big networked world of distributed applications with a very
squishy and insecure interior inside the closed world will be built.
The new application spaces squishy interior -- like a corporate
firewall with poor to no internal security -- could be ok if you could
be sure the firewall is 100% guaranteed reliable.  TCPA/Palladium
proponents are effectively claiming it be an air-gap grade firewall
guarding the distributed closed world application spaces squishy
interior.  But there is a problem: there are master keys by-passing
all that -- the endorsement CA's private keys.
4. What coming political battles will result?
a. If TCPA/Palladium systems get built -- and it may be politically
unstoppable given the power of the distributed security paradigm it
opens up -- then the battle of the coming decades will center around
control of access to that squishy interior.  The keys that control
access to the closed world are the endoresment CA private keys.
b. You will see many clipper like attempts by governments attempting
to make policies surrounding conditional access to that closed world:
   - law enforcement access to the endorsement private CA keys
     controlling access, so they can setup sting operations,
     demand that ISPs and ASPs collaborate with virtualized versions
     of network services so they can trace things
   - NSA designed protocols to allow such things, black box mediated,
     court order approved, split database access to hardware
     manufacturers private keys
c. As b. progresses RIAA/MPAA will chime in protesting that:
   - Kazaa2 is distributing 10 exabytes a day of ripped recent release
     content not based on BORA (which is now somewhat harder), but on
     ROCA (Rip Once Copy Anywhere) as the content rippers move into
     hardware hacking territory
   - the RIAA/MPAA can't hack, spoof or jam kazaa2 with bogus content
     because cypherpunks have fixed the protocols using WoT, certified
     content, and other crypto-fu so they can't even observe who's
     downloading what or who's serving what
   - and therefore they also demand access to the closed world so they
     can exert their recent legal rights to hack and DDoS the file
     sharing networks
d. Unauthorised access to the closed environment (by hacking your own
hardware) will become illegal with DMCA like restrictions (if it
wouldn't already be where some relation to copyright could be drawn).
e. Software companies, and OS vendors will follow Microsoft's current
lead into an unholy battle with highlights such as:
   - undocumented APIs to gain advantage over competitors, not only
     hardware hacking required to discover APIs, but attestation to
     ensure only those companies who have licensed the right to use
     the API can use it
   - incompatible file formats to lock out competition with
     hardware tamper resistance levels of assurance, even file formats
     that must have certified documents for applications to open them,
     so even if you had the spec you couldn't be compatible
   - copyright protection with software encrypted for the CPU, so you
     can't even audit the static code
   - software renting models again enforced by hardware
   - whole collection of 2nd generation IP "innovations" which will be
     built on top of such things
     - charge per person you share file in a given document format;
     - charge per format conversion
f. Lucky's Documentat Revocation lists to allow governments, companies
etc to to some extent after the fact control distribution of data
g. Increasingly minute enforcement of repressive levels of IP
tracking, and arbitrarily user hostile, fair-use eroding document
viewing and use policies
5. What could be done to protect the user?
a. implement cryptographic assurance inside the closed space where
possible -- that way if you are targetted by someone able to get
inside it you still have the same protection as now.
b. use web-of-trust techniques to provide an overlay of trust on the
endorsement trust.  ie users endorse their own machines to say "this
is my machine" this implies that either:
- it's not tampered with (presuming the user himself was not a target
of some attack or investigation) - or the only tamperer is the certifying user
web-of-trust overlaid on the hardware endorsement helps as:
  - This makes the endorsement keys less useful in a
    covertly obtained endorsement CA private key scenario
  - Even if there are court authorized law enforcement access to
    endorsement CA private keys, or RIAA/MPAA access to endorsement CA
    private keys you can to the extent of your connectivity in the web
    of trust, better avoid using the services of rogue
    agents inside the closed space.
c. Demand ability to audit information in-flows into trusted agents
where there are unauditable out-flows; demand that this is implemented
in a way which allows code under user control to audit
d. Demand the ability to audit information out-flows, where there are
unauditable in-flows or sensitive user data processed by the
application; similarly demand that this is implemented in a way which
allows code under user control to audit
e. Demand cryptographically assured anonymity protection so that there
are no "trusted third parties" who can link your network usage and
identify you.
e. Other ideas?  (Other than to lobby to prevent the building or use
this model).
[1] "FBI Bugs Keyboard of PGP-Using Alleged Mafioso", 6 Dec 2000,

@_date: 2002-08-09 21:13:56
@_author: Adam Back 
@_subject: TCPA/Palladium -- likely future implications (Re: dangers of TCPA/palladium) 
The picture is related but has some extra wrinkles with the
TCPA/Palladium attestable donglization of CPUs.
- It is always the case that targetted people can have hardware
attacks perpetrated against them.  (Keyboard sniffers placed during
court authorised break-in as FBI has used in mob case of PGP using
Mafiosa [1]).
- In the clipper case people didn't need to worry much if the clipper
chip had malicious deviations from spec, because Clipper had an openly
stated explicit purpose to implement a government backdoor -- there's
no need for NSA to backdoor the explicit backdoor.
But in the TCPA/Palladium case however the hardware tampering risk you
identify is as you say relevant:
- It's difficult for the user to verify hardware.  - Also: it wouldn't be that hard to manufacture plausibly deniable
implementation "mistakes" that could equate to a backdoor -- eg the
random number generators used to generate the TPM/SCP private device
However, beyond that there is an even softer target for would-be
- the TCPA/Palladium's hardware manufacturers endoresment CA keys.
these are the keys to the virtual kingdom formed -- the virtual
kingdom by the closed space within which attested applications and
software agents run.
So specifically let's look at the questions arising:
1. What could a hostile entity(*) do with a copy of a selection of
hardware manufacturer endorsement CA private keys?
( (*) where the hostile entity candidates would be for example be
secret service agencies, law enforcement or "homeland security"
agencies in western countries, RIAA/MPAA in pursuit of their quest to
exercise their desire to jam and DoS peer-to-peer file sharing
networks, the Chinese government, Taiwanese government (they may lots
of equipment right) and so on).
a. Who needs to worry -- who will be targetted?
Who needs to worry about this depends on how overt third-party
ownership of these keys is, and hence the pool of people who would
likely be targetted.  If it's very covert, it would only be used plausibly deniably and only
for Nat Sec / Homeland Security purposes.  It if becomse overt over
time -- a publicly acknowledged, but supposedly court controlled
affair like Clipper, or even more widely desired by a wide-range of
entities for example: keys made available to RIAA / MPAA so they can
do the hacking they have been pushing for -- well then we all need to
To analyse the answer to question 1, we first need to think about
question 2:
2. What kinds of TCPA/Palladium integrity depending "trusted"
applications are likely to be built?
Given the powerful (though balance of control changing) new remotely
attestable security features provided by TCPA/Palladium, all kinds of
remote services become possible, for example (though all to the extent
of hardware tamper-resistance and belief that your attacker doesn't
have access to a hardware endorsement CA private key):
- general Application Service Providers (ASPs) that you don't have to
trust to read your data
- less traceable peer-to-peer applications
- DRM applications that make a general purpose computer secure against
BORA (Break Once Run Anywhere), though of course not secure against
ROCA (Rip Once Copy Everywhere) -- which will surely continue to
happen with ripping shifting to hardware hackers.
- general purpose unreadable sandboxes to run general purpose
CPU-for-rent computing farms for hire, where the sender knows you
can't read his code, you can't read his input data, or his output
data, or tamper with the computation.
- file-sharing while robustly hiding knowledge and traceability of
content even to the node serving it -- previously research question,
now easy coding problem with efficient
- anonymous remailers where you have more assurance that a given node
is not logging and analysing the traffic being mixed by it
But of course all of these distributed applications, positive and
negative (depending on your view point), are limited in their
assurance of their non-cryptographically assured aspects:
- to the tamper resistance of the device
- to the extent of the users confidence that an entity hostile to them
doesn't have the endorsement CA's private key for the respective
remote servers implementing the network application they are relying
and a follow-on question to question 2:
3. Will any software companies still aim for cryptographic assurance?
(cryptographic assurance means you don't need to trust someone not to
reverse engineer the application -- ie you can't read the data because
it is encrypted with a key derived from a password that is only stored
in the users head).
The extended platform allows you to build new classes of applications
which aren't currently buildable to cryptographic levels of assurance.
eg. It allows general purpose policies to be built just by writing
policy code that sits in a Trusted Agent code compartment, without
having to figure out how to do split-trust (a la mixmaster chaining),
or forward-secrecy or secret-sharing or any of the other funky stuff;
you can just implement some policy code and it becomes so.
The danger is people will use it to build applications with squishy
interiors, with no cryptographic assurance.  Forward-secrecy
implemented only by a policy in a Trusted Agent that sets a time-limit
on access.  Anonymity but only in the sense that you trust the
hardware isn't tampered with, etc etc.
It will be really tempting because:
- it's much easier: network distributed crypto protocols are
relatively complex
- you can build things you can't otherwise build, the are currently
unsolved problems with distributed crypto protocols
- even where good crypto protocols exist, people will defend not using
them by claims to paranoia: "What you think the NSA has tampered with
your CPU?", or just laziness, cost of implementation etc
So in short probably mostly the answer will be "No", people won't
still aim for cryptographic assurance.
And so a big networked world of distributed applications with a very
squishy and insecure interior inside the closed world will be built.
The new application spaces squishy interior -- like a corporate
firewall with poor to no internal security -- could be ok if you could
be sure the firewall is 100% guaranteed reliable.  TCPA/Palladium
proponents are effectively claiming it be an air-gap grade firewall
guarding the distributed closed world application spaces squishy
interior.  But there is a problem: there are master keys by-passing
all that -- the endorsement CA's private keys.
4. What coming political battles will result?
a. If TCPA/Palladium systems get built -- and it may be politically
unstoppable given the power of the distributed security paradigm it
opens up -- then the battle of the coming decades will center around
control of access to that squishy interior.  The keys that control
access to the closed world are the endoresment CA private keys.
b. You will see many clipper like attempts by governments attempting
to make policies surrounding conditional access to that closed world:
   - law enforcement access to the endorsement private CA keys
     controlling access, so they can setup sting operations,
     demand that ISPs and ASPs collaborate with virtualized versions
     of network services so they can trace things
   - NSA designed protocols to allow such things, black box mediated,
     court order approved, split database access to hardware
     manufacturers private keys
c. As b. progresses RIAA/MPAA will chime in protesting that:
   - Kazaa2 is distributing 10 exabytes a day of ripped recent release
     content not based on BORA (which is now somewhat harder), but on
     ROCA (Rip Once Copy Anywhere) as the content rippers move into
     hardware hacking territory
   - the RIAA/MPAA can't hack, spoof or jam kazaa2 with bogus content
     because cypherpunks have fixed the protocols using WoT, certified
     content, and other crypto-fu so they can't even observe who's
     downloading what or who's serving what
   - and therefore they also demand access to the closed world so they
     can exert their recent legal rights to hack and DDoS the file
     sharing networks
d. Unauthorised access to the closed environment (by hacking your own
hardware) will become illegal with DMCA like restrictions (if it
wouldn't already be where some relation to copyright could be drawn).
e. Software companies, and OS vendors will follow Microsoft's current
lead into an unholy battle with highlights such as:
   - undocumented APIs to gain advantage over competitors, not only
     hardware hacking required to discover APIs, but attestation to
     ensure only those companies who have licensed the right to use
     the API can use it
   - incompatible file formats to lock out competition with
     hardware tamper resistance levels of assurance, even file formats
     that must have certified documents for applications to open them,
     so even if you had the spec you couldn't be compatible
   - copyright protection with software encrypted for the CPU, so you
     can't even audit the static code
   - software renting models again enforced by hardware
   - whole collection of 2nd generation IP "innovations" which will be
     built on top of such things
     - charge per person you share file in a given document format;
     - charge per format conversion
f. Lucky's Documentat Revocation lists to allow governments, companies
etc to to some extent after the fact control distribution of data
g. Increasingly minute enforcement of repressive levels of IP
tracking, and arbitrarily user hostile, fair-use eroding document
viewing and use policies
5. What could be done to protect the user?
a. implement cryptographic assurance inside the closed space where
possible -- that way if you are targetted by someone able to get
inside it you still have the same protection as now.
b. use web-of-trust techniques to provide an overlay of trust on the
endorsement trust.  ie users endorse their own machines to say "this
is my machine" this implies that either:
- it's not tampered with (presuming the user himself was not a target
of some attack or investigation) - or the only tamperer is the certifying user
web-of-trust overlaid on the hardware endorsement helps as:
  - This makes the endorsement keys less useful in a
    covertly obtained endorsement CA private key scenario
  - Even if there are court authorized law enforcement access to
    endorsement CA private keys, or RIAA/MPAA access to endorsement CA
    private keys you can to the extent of your connectivity in the web
    of trust, better avoid using the services of rogue
    agents inside the closed space.
c. Demand ability to audit information in-flows into trusted agents
where there are unauditable out-flows; demand that this is implemented
in a way which allows code under user control to audit
d. Demand the ability to audit information out-flows, where there are
unauditable in-flows or sensitive user data processed by the
application; similarly demand that this is implemented in a way which
allows code under user control to audit
e. Demand cryptographically assured anonymity protection so that there
are no "trusted third parties" who can link your network usage and
identify you.
e. Other ideas?  (Other than to lobby to prevent the building or use
this model).
[1] "FBI Bugs Keyboard of PGP-Using Alleged Mafioso", 6 Dec 2000,

@_date: 2002-08-12 21:13:58
@_author: Adam Back 
@_subject: Re: trade-offs of secure programming with Palladium (Re: 
At this point we largely agree, security is improved, but the limit
remains assuring security of over-complex software.  To sum up:
The limit of what is securely buildable now becomes what is securely
auditable.  Before, without the Palladium the limit was the security
of the OS, so this makes a big difference.
Yes some people may design over complex trusted agents, with sloppy
APIs and so forth, but the nice thing about trusted agents are they
are compartmentalized:
If the MPAA and Microsoft shoot themselves in the foot with a badly
designed over complex DRM trusted agent component for MS Media Player,
it has no bearing on my ability to implement a secure file-sharing or
secure e-cash system in a compartment with rigorously analysed APIs,
and well audited code.  The leaky compromised DRM app can't compromise
the security policies of my app.
Also it's unclear from the limited information available but it may be
that trusted agents, like other ring-0 code (eg like the OS itself)
can delegate tasks to user mode code running in trusted agent space,
which can't examine other user level space, nor the space of the
trusted agent which stated them, and also can't be examined by the OS.
In this way for example remote exploits could be better contained in
the sub-division of trusted agent code.  eg. The crypto could be done
by the trusted-agent proper, the mpeg decoding by a user-mode
component; compromise the mpeg-decoder, and you just get plaintext not
keys.  Various divisions could be envisaged.
Given that most current applications don't even get the simplest of
applications of encryption right (store key and password in the
encrypted file, check if the password is right by string comparison is
suprisingly common), the prospects are not good for general
applications.  However it becomes more feasible to build secure
applications in the environment where it matters, or the consumer
cares sufficiently to pay for the difference in development cost.
Of course all this assumes microsoft manages to securely implement a
TOR and SCP interface.  And whether they manage to succesfully use
trusted IO paths to prevent the OS and applications from tricking the
user into bypassing intended trusted agent functionality (another
interesting sub-problem).  CC EAL3 on the SCP is a good start, but
they have pressures to make the TOR and Trusted Agent APIs flexible,
so we'll see how that works out.

@_date: 2002-08-12 20:07:59
@_author: Adam Back 
@_subject: trade-offs of secure programming with Palladium (Re: Palladium: 
I think you are making incorrect presumptions about how you would use
Palladium hardware to implement a secure DRM system.  If used as you
suggest it would indeed suffer the vulnerabilities you describe.
The difference between an insecure DRM application such as you
describe and a secure DRM application correctly using the hardware
security features is somewhat analogous to the current difference
between an application that relies on not being reverse engineered for
it's security vs one that encrypts data with a key derived from a user
In a Palladium DRM application done right everything which sees keys
and plaintext content would reside inside Trusted Agent space, inside
DRM enabled graphics cards which retrict access to video RAM, and
later DRM enabled monitors with encrypted digital signal to the
monitor, and DRM enabled soundcards, encrypted content to speakers.
(The encrypted contentt to media related output peripherals is like
HDCP, only done right with non-broken crypto).
Now all that will be in application space that you can reverse
engineer and hack on will be UI elements and application logic that
drives the trusted agent, remote attesation, content delivery and
hardware.  At no time will keys or content reside in space that you
can virtualize or debug.
In the short term it may be that some of these will be not fully
implemented so that content does pass through OS or application space,
or into non DRM video cards and non DRM monitors, but the above is the
end-goal as I understand it.
As you can see there is still the limit of the non-remote
exploitability of the trusted agent code, but this is within the
control of the DRM vendor.  If he does a good job of making a simple
software architecture and avoiding potential for buffer overflows he
stands a much better chance of having a secure DRM platofrm than if as
you describe exploited OS code or rogue driver code can subvert his
There is also I suppose possibility to push content decryption on to
the DRM video card so the TOR does little apart from channel key
exchange messages from the SCP to the video card, and channel remote
attestation and key exchanges between the DRM license server and the
SCP.  The rest would be streaming encrypted video formats such as CSS
VOB blocks (only with good crypto) from the network or disk to the
video card.
Similar kinds of arguments about the correct break down between
application logic and placement of security policy enforcing code in
Trusted Agent space apply to general applications.  For example you
could imagine a file sharing application which hid the data the users
machine was serving from the user.  If you did it correctly, this
would be secure to the extent of the hardware tamper resistance (and
the implementers ability to keep the security policy enforcing code
line-count down and audit it well).
At some level there has to be a trade-off between what you put in
trusted agent space and what becomes application code.  If you put the
whole application in trusted agent space, while then all it's
application logic is fully protected, the danger will be that you have
added too much code to reasonably audit, so people will be able to
gain access to that trusted agent via buffer overflow.
So therein lies the crux of secure software design in the Palladium
style secure application space: choosing a good break-down between
security policy enforcement, and application code.  There must be a
balance, and what makes sense and is appropriate depends on the
application and the limits of the ingenuity of the protocol designer
in coming up with clever designs that cover to hardware tamper
resistant levels the the applications desired policy enforcement while
providing a workably small and pracitcally auditable associated
trusted agent module.
So there are practical limits stemming from realities to do with code
complexity being inversely proportional to auditability and security,
but the extra ring -1, remote attestation, sealing and integrity
metrics really do offer some security advantages over the current

@_date: 2002-08-14 19:02:21
@_author: Adam Back 
@_subject: TCPA/Palladium user interst vs third party interest (Re: 
The remote attesation is the feature which is in the interests of
third parties.
I think if this feature were removed the worst of the issues the
complaints are around would go away because the remaining features
would be under the control of the user, and there would be no way for
third parties to discriminate against users who did not use them, or
configured them in given ways.
The remaining features of note being sealing, and integrity metric
based security boot-strapping.
However the remote attestation is clearly the feature that TCPA, and
Microsoft place most value on (it being the main feature allowing DRM,
and allowing remote influence and control to be exerted on users
configuration and software choices).
So the remote attesation feature is useful for _servers_ that want to
convince clients of their trust-worthiness (that they won't look at
content, tamper with the algorithm, or anonymity or privacy properties
etc).  So you could imagine that feature being a part of server
machines, but not part of client machines -- there already exists some
distinctions between client and server platforms -- for example high
end Intel chips with larger cache etc intended for server market by
their pricing.  You could imagine the TCPA/Palladium support being
available at extra cost for this market.
But the remaining problem is that the remote attesation is kind of
dual-use (of utility to both user desktop machines and servers).  This
is because with peer-to-peer applications, user desktop machines are
also servers.
So the issue has become entangled.
It would be useful for individual liberties for remote-attestation
features to be widely deployed on desktop class machines to build
peer-to-peer systems and anonymity and privacy enhancing systems.
However the remote-attestation feature is also against the users
interests because it's wide-spread deployment is the main DRM enabling
feature and general tool for remote control descrimination against
user software and configuration choices.
I don't see any way to have the benefits without the negatives, unless
anyone has any bright ideas.  The remaining questions are:
- do the negatives out-weigh the positives (lose ability to
reverse-engineer and virtualize applications, and trade
software-hacking based BORA for hardware-hacking based ROCA);
- are there ways to make remote-attestation not useful for DRM,
eg. limited deployment, other;
- would the user-positive aspects of remote-attestation still be
largely available with only limited-deployment -- eg could interesting
peer-to-peer and privacy systems be built with a mixture of
remote-attestation able and non-remote-attestation able nodes.

@_date: 2002-08-14 19:02:21
@_author: Adam Back 
@_subject: TCPA/Palladium user interst vs third party interest (Re: responding to claims about TCPA) 
The remote attesation is the feature which is in the interests of
third parties.
I think if this feature were removed the worst of the issues the
complaints are around would go away because the remaining features
would be under the control of the user, and there would be no way for
third parties to discriminate against users who did not use them, or
configured them in given ways.
The remaining features of note being sealing, and integrity metric
based security boot-strapping.
However the remote attestation is clearly the feature that TCPA, and
Microsoft place most value on (it being the main feature allowing DRM,
and allowing remote influence and control to be exerted on users
configuration and software choices).
So the remote attesation feature is useful for _servers_ that want to
convince clients of their trust-worthiness (that they won't look at
content, tamper with the algorithm, or anonymity or privacy properties
etc).  So you could imagine that feature being a part of server
machines, but not part of client machines -- there already exists some
distinctions between client and server platforms -- for example high
end Intel chips with larger cache etc intended for server market by
their pricing.  You could imagine the TCPA/Palladium support being
available at extra cost for this market.
But the remaining problem is that the remote attesation is kind of
dual-use (of utility to both user desktop machines and servers).  This
is because with peer-to-peer applications, user desktop machines are
also servers.
So the issue has become entangled.
It would be useful for individual liberties for remote-attestation
features to be widely deployed on desktop class machines to build
peer-to-peer systems and anonymity and privacy enhancing systems.
However the remote-attestation feature is also against the users
interests because it's wide-spread deployment is the main DRM enabling
feature and general tool for remote control descrimination against
user software and configuration choices.
I don't see any way to have the benefits without the negatives, unless
anyone has any bright ideas.  The remaining questions are:
- do the negatives out-weigh the positives (lose ability to
reverse-engineer and virtualize applications, and trade
software-hacking based BORA for hardware-hacking based ROCA);
- are there ways to make remote-attestation not useful for DRM,
eg. limited deployment, other;
- would the user-positive aspects of remote-attestation still be
largely available with only limited-deployment -- eg could interesting
peer-to-peer and privacy systems be built with a mixture of
remote-attestation able and non-remote-attestation able nodes.

@_date: 2002-08-16 00:44:58
@_author: Adam Back 
@_subject: Re: TCPA not virtualizable during ownership change (Re: 
I think a number of the apparent conflicts go away if you carefully
track endorsement key pair vs endorsement certificate (signature on
endorsement key by hw manufacturer).  For example where it is said
that the endorsement _certificate_ could be inserted after ownership
has been established (not the endorsement key), so that apparent
conflict goes away.  (I originally thought this particular one was a
conflict also, until I noticed that.)  I see anonymous found the same
But anyway this extract from the CC PP makes clear the intention and
an ST based on this PP is what a given TPM will be evaluated based on:
p 20:
(if only they could have managed to say that in the spec).

@_date: 2002-08-16 00:44:58
@_author: Adam Back 
@_subject: Re: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
I think a number of the apparent conflicts go away if you carefully
track endorsement key pair vs endorsement certificate (signature on
endorsement key by hw manufacturer).  For example where it is said
that the endorsement _certificate_ could be inserted after ownership
has been established (not the endorsement key), so that apparent
conflict goes away.  (I originally thought this particular one was a
conflict also, until I noticed that.)  I see anonymous found the same
But anyway this extract from the CC PP makes clear the intention and
an ST based on this PP is what a given TPM will be evaluated based on:
p 20:
(if only they could have managed to say that in the spec).

@_date: 2002-08-15 16:56:25
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
[resend via different node: cypherpunks seems to be dead --
primary MX refusing connections]
Phew... the document is certainly tortuous, and has a large number of
similarly and confusingly named credentials, certificates and keys,
however from what I can tell this is what is going on:
Summary: I think the endorsement key and it's hardware manufacturers
certificate is generated at manufacture and is not allowed to be
changed.  Changing ownership only means (typically) deleting old
identities and creating new ones.
The longer version...
- endorsement key generation and certification - There is one
endorsement key per TPM which is created and certified during
manufacture.  The creation and certification process is 1) create
endorsement key pair, 2) export public key endorsement key, 3)
hardware manufacturer signs endorsement public key to create an
endorsement certificate (to certify that that endorsement public key
belongs to this TPM), 4) the certificate is stored in the TPM (for
later use in communications with the privacy CA.)
- ownership - Then there is the concept of ownership.  The spec says
the TPM MUST ship with no Owner installed.  The owner when he wishes
to claim ownership choose a authentication token which is sent into
the TPM encrypted with the endorsement key.  (They give the example of
the authentication token being the hash of a password).  Physical
presence tests apply to claiming ownership (eg think BIOS POST with no
networking enabled, or physical pin on motherboard like BIOS flash
enable).  The authentication token and ownership can be changed.  The
TPM can be reset back to a state with no current owner.  BUT _at no
point_ does the TPM endorsement private key leave the TPM.  The
TPM_CreateEndorsementKeyPair function is allowed to be called once
(during manufacture) and is thereafter disabled.
- identity keys - Then there is the concept of identity keys.  The
current owner can create and delete identities, which can be anonymous
or pseudonymous.  Presumably the owner would delete all identity keys
before giving the TPM to a new owner.  The identity public key is
certified by the privacy CA.
- privacy ca - The privacy CA accepts identity key certification
requests which contain a) identity public key b) a proof of possession
(PoP) of identity private key (signature on challenge), c) the
hardware manufacturers endorsement certificate containing the TPM's
endorsement public key.  The privacy CA checks whether the endorsement
certificate is signed by a hardware manufacturer it trusts.  The
privacy CA sends in response an identity certificate encrypted with
the TPM's endorsement public key.  The TPM decrypts the encrypted
identity certifate with the endorsement private key.
- remote attestation - The owner uses the identity keys in the remote
attestation functions.  Note that the identity private keys are also
generated on the TPM, the private key also never leaves the TPM.  The
identity private key is certified by the privacy CA as having been
requested by a certified endorsement key.
The last two paragraphs imply something else interesting: the privacy
CA can collude with anyone to create a virtualized environment.  (This
is because the TPM endorsement key is never directly used in remote
attestation for privacy reasons.)  All that is required to virtualize
a TPM is an attestation from the privacy CA in creating an identity
So there are in fact three avenues for FBI et al to go about obtaining
covert access to the closed space formed by TCPA applications: (A) get one of the hardware manufacturers to sign an endorsement key
generated outside a TPM (or get the endorsement CA's private key), or
(B) get a widely used and accepted privacy CA to overlook it's policy
of demanding a hardware manufacturer CA endorsed endorsement public
key and sign an identity public key created outside of a TPM (or get
the privacy CA's private key).
(C) create their own privacy CA and persuade an internet server they
wish to investigate the users of to accept it.  Create themselves a
virtualized client using their own privacy CA, look inside.
I think to combat problem C) as a user of a service you'd want the
remote attestation of software state to auditably include it's
accepted privacy CA database to see if there are any strange "Privacy
CAs" on there.
I think you could set up and use your own privacy CA, but you can be
sure the RIAA/MPAA will never trust your CA.  A bit like self-signing
SSL site keys.  If you and your friends add your CA to their trusted
root CA database it'll work.  In this case however people have to
trust your home-brew privacy CA not to issue identity certificates
without having seen a valid hardware-endorsement key if they care
about preventing virtualization for the privacy or security of some
network application.
Also, they seem to take explicit steps to prevent you getting multiple
privacy CA certificates on the same identity key.  (I'm not sure why.)
It seems like a bad thing as it forces you to trust just one CA, it
prevents web of trust which could reduce your chances of getting
caught in attack scenarios B) and C) by demanding multiple
This section from the spec on trusting the privacy CA is interesting
section 8.3.1 p 195
(not sure what the maintenance policy of the TPM is or what it has to
do with trusting privacy CAs -- it is not otherwise discussed).
also the text on p268 relates to trusting the privacy CA.
Below is some text from the spec which tends to confirm the above.
(Anonymous may have some comments as he seemed to have read the TCPA
spec in more detail than I have.)
Here is an indicative quote from the spec:
informative comment:
normative text:
Anyway Occam's razor suggests that the intent is:
1. the TPM endorsement private key never leaves the TPM
2. the identity private keys never leave the TPM
3. the privacy CA will never issue identity private keys unless the
request is made in relation to a manufacturer certified endorsement
public key.
Note: The endorsement key has key usage restrictions and is marked as
encrypt only, so the assurance the privacy CA gets is not that it
receives a identity certificate request signed by the endorsement
private key, but rather that the issued certificate is encrypted with
the endorsement public key and so could only be decrypted by the TPM
which contains the corresponding private endorsement key.  (I suppose
the motivation might have been that then the privacy CA couldn't prove
to third parties that your endorsement key and identity key are bound

@_date: 2002-08-15 06:06:04
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming the potential downside of TCPA) 
Phew... the document is certainly tortuous, and has a large number of
similarly and confusingly named credentials, certificates and keys,
however from what I can tell this is what is going on:
Summary: I think the endorsement key and it's hardware manufacturers
certificate is generated at manufacture and is not allowed to be
changed.  Changing ownership only means (typically) deleting old
identities and creating new ones.
The longer version...
- endorsement key generation and certification - There is one
endorsement key per TPM which is created and certified during
manufacture.  The creation and certification process is 1) create
endorsement key pair, 2) export public key endorsement key, 3)
hardware manufacturer signs endorsement public key to create an
endorsement certificate (to certify that that endorsement public key
belongs to this TPM), 4) the certificate is stored in the TPM (for
later use in communications with the privacy CA.)
- ownership - Then there is the concept of ownership.  The spec says
the TPM MUST ship with no Owner installed.  The owner when he wishes
to claim ownership choose a authentication token which is sent into
the TPM encrypted with the endorsement key.  (They give the example of
the authentication token being the hash of a password).  Physical
presence tests apply to claiming ownership (eg think BIOS POST with no
networking enabled, or physical pin on motherboard like BIOS flash
enable).  The authentication token and ownership can be changed.  The
TPM can be reset back to a state with no current owner.  BUT _at no
point_ does the TPM endorsement private key leave the TPM.  The
TPM_CreateEndorsementKeyPair function is allowed to be called once
(during manufacture) and is thereafter disabled.
- identity keys - Then there is the concept of identity keys.  The
current owner can create and delete identities, which can be anonymous
or pseudonymous.  Presumably the owner would delete all identity keys
before giving the TPM to a new owner.  The identity public key is
certified by the privacy CA.
- privacy ca - The privacy CA accepts identity key certification
requests which contain a) identity public key b) a proof of possession
(PoP) of identity private key (signature on challenge), c) the
hardware manufacturers endorsement certificate containing the TPM's
endorsement public key.  The privacy CA checks whether the endorsement
certificate is signed by a hardware manufacturer it trusts.  The
privacy CA sends in response an identity certificate encrypted with
the TPM's endorsement public key.  The TPM decrypts the encrypted
identity certifate with the endorsement private key.
- remote attestation - The owner uses the identity keys in the remote
attestation functions.  Note that the identity private keys are also
generated on the TPM, the private key also never leaves the TPM.  The
identity private key is certified by the privacy CA as having been
requested by a certified endorsement key.
The last two paragraphs imply something else interesting: the privacy
CA can collude with anyone to create a virtualized environment.  (This
is because the TPM endorsement key is never directly used in remote
attestation for privacy reasons.)  All that is required to virtualize
a TPM is an attestation from the privacy CA in creating an identity
So there are in fact three avenues for FBI et al to go about obtaining
covert access to the closed space formed by TCPA applications: (A) get one of the hardware manufacturers to sign an endorsement key
generated outside a TPM (or get the endorsement CA's private key), or
(B) get a widely used and accepted privacy CA to overlook it's policy
of demanding a hardware manufacturer CA endorsed endorsement public
key and sign an identity public key created outside of a TPM (or get
the privacy CA's private key).
(C) create their own privacy CA and persuade an internet server they
wish to investigate the users of to accept it.  Create themselves a
virtualized client using their own privacy CA, look inside.
I think to combat problem C) as a user of a service you'd want the
remote attestation of software state to auditably include it's
accepted privacy CA database to see if there are any strange "Privacy
CAs" on there.
I think you could set up and use your own privacy CA, but you can be
sure the RIAA/MPAA will never trust your CA.  A bit like self-signing
SSL site keys.  If you and your friends add your CA to their trusted
root CA database it'll work.  In this case however people have to
trust your home-brew privacy CA not to issue identity certificates
without having seen a valid hardware-endorsement key if they care
about preventing virtualization for the privacy or security of some
network application.
Also, they seem to take explicit steps to prevent you getting multiple
privacy CA certificates on the same identity key.  (I'm not sure why.)
It seems like a bad thing as it forces you to trust just one CA, it
prevents web of trust which could reduce your chances of getting
caught in attack scenarios B) and C) by demanding multiple
This section from the spec on trusting the privacy CA is interesting
section 8.3.1 p 195
(not sure what the maintenance policy of the TPM is or what it has to
do with trusting privacy CAs -- it is not otherwise discussed).
also the text on p268 relates to trusting the privacy CA.
Below is some text from the spec which tends to confirm the above.
(Anonymous may have some comments as he seemed to have read the TCPA
spec in more detail than I have.)
Here is an indicative quote from the spec:
informative comment:
normative text:
Anyway Occam's razor suggests that the intent is:
1. the TPM endorsement private key never leaves the TPM
2. the identity private keys never leave the TPM
3. the privacy CA will never issue identity private keys unless the
request is made in relation to a manufacturer certified endorsement
public key.
Note: The endorsement key has key usage restrictions and is marked as
encrypt only, so the assurance the privacy CA gets is not that it
receives a identity certificate request signed by the endorsement
private key, but rather that the issued certificate is encrypted with
the endorsement public key and so could only be decrypted by the TPM
which contains the corresponding private endorsement key.  (I suppose
the motivation might have been that then the privacy CA couldn't prove
to third parties that your endorsement key and identity key are bound

@_date: 2002-08-15 06:06:04
@_author: Adam Back 
@_subject: TCPA not virtualizable during ownership change (Re: Overcoming 
Phew... the document is certainly tortuous, and has a large number of
similarly and confusingly named credentials, certificates and keys,
however from what I can tell this is what is going on:
Summary: I think the endorsement key and it's hardware manufacturers
certificate is generated at manufacture and is not allowed to be
changed.  Changing ownership only means (typically) deleting old
identities and creating new ones.
The longer version...
- endorsement key generation and certification - There is one
endorsement key per TPM which is created and certified during
manufacture.  The creation and certification process is 1) create
endorsement key pair, 2) export public key endorsement key, 3)
hardware manufacturer signs endorsement public key to create an
endorsement certificate (to certify that that endorsement public key
belongs to this TPM), 4) the certificate is stored in the TPM (for
later use in communications with the privacy CA.)
- ownership - Then there is the concept of ownership.  The spec says
the TPM MUST ship with no Owner installed.  The owner when he wishes
to claim ownership choose a authentication token which is sent into
the TPM encrypted with the endorsement key.  (They give the example of
the authentication token being the hash of a password).  Physical
presence tests apply to claiming ownership (eg think BIOS POST with no
networking enabled, or physical pin on motherboard like BIOS flash
enable).  The authentication token and ownership can be changed.  The
TPM can be reset back to a state with no current owner.  BUT _at no
point_ does the TPM endorsement private key leave the TPM.  The
TPM_CreateEndorsementKeyPair function is allowed to be called once
(during manufacture) and is thereafter disabled.
- identity keys - Then there is the concept of identity keys.  The
current owner can create and delete identities, which can be anonymous
or pseudonymous.  Presumably the owner would delete all identity keys
before giving the TPM to a new owner.  The identity public key is
certified by the privacy CA.
- privacy ca - The privacy CA accepts identity key certification
requests which contain a) identity public key b) a proof of possession
(PoP) of identity private key (signature on challenge), c) the
hardware manufacturers endorsement certificate containing the TPM's
endorsement public key.  The privacy CA checks whether the endorsement
certificate is signed by a hardware manufacturer it trusts.  The
privacy CA sends in response an identity certificate encrypted with
the TPM's endorsement public key.  The TPM decrypts the encrypted
identity certifate with the endorsement private key.
- remote attestation - The owner uses the identity keys in the remote
attestation functions.  Note that the identity private keys are also
generated on the TPM, the private key also never leaves the TPM.  The
identity private key is certified by the privacy CA as having been
requested by a certified endorsement key.
The last two paragraphs imply something else interesting: the privacy
CA can collude with anyone to create a virtualized environment.  (This
is because the TPM endorsement key is never directly used in remote
attestation for privacy reasons.)  All that is required to virtualize
a TPM is an attestation from the privacy CA in creating an identity
So there are in fact three avenues for FBI et al to go about obtaining
covert access to the closed space formed by TCPA applications: (A) get one of the hardware manufacturers to sign an endorsement key
generated outside a TPM (or get the endorsement CA's private key), or
(B) get a widely used and accepted privacy CA to overlook it's policy
of demanding a hardware manufacturer CA endorsed endorsement public
key and sign an identity public key created outside of a TPM (or get
the privacy CA's private key).
(C) create their own privacy CA and persuade an internet server they
wish to investigate the users of to accept it.  Create themselves a
virtualized client using their own privacy CA, look inside.
I think to combat problem C) as a user of a service you'd want the
remote attestation of software state to auditably include it's
accepted privacy CA database to see if there are any strange "Privacy
CAs" on there.
I think you could set up and use your own privacy CA, but you can be
sure the RIAA/MPAA will never trust your CA.  A bit like self-signing
SSL site keys.  If you and your friends add your CA to their trusted
root CA database it'll work.  In this case however people have to
trust your home-brew privacy CA not to issue identity certificates
without having seen a valid hardware-endorsement key if they care
about preventing virtualization for the privacy or security of some
network application.
Also, they seem to take explicit steps to prevent you getting multiple
privacy CA certificates on the same identity key.  (I'm not sure why.)
It seems like a bad thing as it forces you to trust just one CA, it
prevents web of trust which could reduce your chances of getting
caught in attack scenarios B) and C) by demanding multiple
This section from the spec on trusting the privacy CA is interesting
section 8.3.1 p 195
(not sure what the maintenance policy of the TPM is or what it has to
do with trusting privacy CAs -- it is not otherwise discussed).
also the text on p268 relates to trusting the privacy CA.
Below is some text from the spec which tends to confirm the above.
(Anonymous may have some comments as he seemed to have read the TCPA
spec in more detail than I have.)
Here is an indicative quote from the spec:
informative comment:
normative text:
Anyway Occam's razor suggests that the intent is:
1. the TPM endorsement private key never leaves the TPM
2. the identity private keys never leave the TPM
3. the privacy CA will never issue identity private keys unless the
request is made in relation to a manufacturer certified endorsement
public key.
Note: The endorsement key has key usage restrictions and is marked as
encrypt only, so the assurance the privacy CA gets is not that it
receives a identity certificate request signed by the endorsement
private key, but rather that the issued certificate is encrypted with
the endorsement public key and so could only be decrypted by the TPM
which contains the corresponding private endorsement key.  (I suppose
the motivation might have been that then the privacy CA couldn't prove
to third parties that your endorsement key and identity key are bound

@_date: 2002-08-16 01:23:05
@_author: Adam Back 
@_subject: employment market for applied cryptographers? 
On the employment situation... it seems that a lot of applied
cryptographers are currently unemployed (Tim Dierks, Joseph, a few
ex-colleagues, and friends who asked if I had any leads, the spate of
recent "security consultant" .sigs, plus I heard that a straw poll of
attenders at the codecon conference earlier this year showed close to
50% out of work).
Are there any more definitive security industry stats?  Are applied
crypto people suffering higher rates of unemployment than general
application programmers?  (From my statistically too small sample of
acquaintances it might appear so.)
If this is so, why is it?
- you might think the physical security push following the world
political instability worries following Sep 11th would be accompanied
by a corresponding information security push -- jittery companies
improving their disaster recovery and to a lesser extent info sec
- governments are still harping on the info-war hype, national
information infrastructure protection, and the US Information Security
Czar Clarke making grandiose pronouncements about how industry ought
to do various things (that the USG spent the last 10 years doing it's
best to frustrate industry from doing with it's dumb export laws)
- even Microsoft has decided to make a play of cleaning up it's
security act (you'd wonder if this was in fact a cover for Palladium
which I think is likely a big play for them in terms of future control
points and (anti-)competitive strategy -- as well as obviously a play
for the home entertainment system space with DRM)
However these reasons are perhaps more than cancelled by:
- dot-com bubble (though I saw some news reports earlier that though
there is lots of churn in programmers in general, that long term
unemployment rates were not that elevated in general)
- perhaps security infrastructure and software upgrades are the first
things to be canned when cash runs short?  - software security related contract employees laid off ahead of
full-timers?  Certainly contracting seems to be flat in general, and
especially in crypto software contracts look few and far between.  At
least in the UK some security people are employed in that way (not
familiar with north america).
- PKI seems to have fizzled compared to earlier exaggerated
expectations, presumably lots of applied crypto jobs went at PKI
companies downsizing.  (If you ask me over use of ASN.1 and adoption
of broken over complex and ill-defined ITU standards X.500, X.509
delayed deployment schedules by order of magnitude over what was
strictly necessary and contributed to interoperability problems and I
think significantly to the flop of PKI -- if it's that hard because of
the broken tech, people will just do something else.)
- custom crypto and security related software development is perhaps
weighted towards dot-coms that just crashed.
- big one probably: lack of measurability of security -- developers
with no to limited crypto know-how are probably doing (and bodging)
most of the crypto development that gets done in general, certainly
contributing to the crappy state of crypto in software.  So probably
failure to realise this issue or perhaps just not caring, or lack of
financial incentives to care on the part of software developers.
Microsoft is really good at this one.  The number of times they
re-used RC4 keys in different protocols is amazing!
Other explanations?  Statistics?  Sample-of-one stories?
yes, still employed in sofware security industry; and in addition have
been doing crypto consulting since 97 ( if
you have any interesting applied crypto projects; reference
commissions paid.

@_date: 2002-08-16 01:23:05
@_author: Adam Back 
@_subject: employment market for applied cryptographers? 
On the employment situation... it seems that a lot of applied
cryptographers are currently unemployed (Tim Dierks, Joseph, a few
ex-colleagues, and friends who asked if I had any leads, the spate of
recent "security consultant" .sigs, plus I heard that a straw poll of
attenders at the codecon conference earlier this year showed close to
50% out of work).
Are there any more definitive security industry stats?  Are applied
crypto people suffering higher rates of unemployment than general
application programmers?  (From my statistically too small sample of
acquaintances it might appear so.)
If this is so, why is it?
- you might think the physical security push following the world
political instability worries following Sep 11th would be accompanied
by a corresponding information security push -- jittery companies
improving their disaster recovery and to a lesser extent info sec
- governments are still harping on the info-war hype, national
information infrastructure protection, and the US Information Security
Czar Clarke making grandiose pronouncements about how industry ought
to do various things (that the USG spent the last 10 years doing it's
best to frustrate industry from doing with it's dumb export laws)
- even Microsoft has decided to make a play of cleaning up it's
security act (you'd wonder if this was in fact a cover for Palladium
which I think is likely a big play for them in terms of future control
points and (anti-)competitive strategy -- as well as obviously a play
for the home entertainment system space with DRM)
However these reasons are perhaps more than cancelled by:
- dot-com bubble (though I saw some news reports earlier that though
there is lots of churn in programmers in general, that long term
unemployment rates were not that elevated in general)
- perhaps security infrastructure and software upgrades are the first
things to be canned when cash runs short?  - software security related contract employees laid off ahead of
full-timers?  Certainly contracting seems to be flat in general, and
especially in crypto software contracts look few and far between.  At
least in the UK some security people are employed in that way (not
familiar with north america).
- PKI seems to have fizzled compared to earlier exaggerated
expectations, presumably lots of applied crypto jobs went at PKI
companies downsizing.  (If you ask me over use of ASN.1 and adoption
of broken over complex and ill-defined ITU standards X.500, X.509
delayed deployment schedules by order of magnitude over what was
strictly necessary and contributed to interoperability problems and I
think significantly to the flop of PKI -- if it's that hard because of
the broken tech, people will just do something else.)
- custom crypto and security related software development is perhaps
weighted towards dot-coms that just crashed.
- big one probably: lack of measurability of security -- developers
with no to limited crypto know-how are probably doing (and bodging)
most of the crypto development that gets done in general, certainly
contributing to the crappy state of crypto in software.  So probably
failure to realise this issue or perhaps just not caring, or lack of
financial incentives to care on the part of software developers.
Microsoft is really good at this one.  The number of times they
re-used RC4 keys in different protocols is amazing!
Other explanations?  Statistics?  Sample-of-one stories?
yes, still employed in sofware security industry; and in addition have
been doing crypto consulting since 97 ( if
you have any interesting applied crypto projects; reference
commissions paid.

@_date: 2002-08-24 16:11:42
@_author: Adam Back 
@_subject: Re: Cryptographic privacy protection in TCPA 
Since writing this I realised that there is a problem revoking
unlinkable multi-show credentials:
- I was presuming that revealing the credential and it's secret key is
sufficient to allow someone to link shows of that credential.
- but to link you'd have to try each revoked credential in turn.
Therefore the verifier would have to perform work linear in the number
of revoked credentials at each show, for the duration of the epoch.
Anonymous suggests one way out is to just define that the issuing CA
and the refreshing CA to be the same entity.  Then you already have to
trust the hardware manufacturer not to issue certs whose secrets are
outside of a TPM.  In this case Brands or Chaum credentials work.
The remaining desiderata are:
- it is not ideal from a risk management perspective to have to have
the hardware manufacturers endorsement private key online to refresh
certificates (or in general for there to be any private key online
that allows issuing of credentials whose private keys lie outside a
- not ideal to have to have an online protocol with an otherwise
non-existant third party (credential refresh CA) in order to avoid
Other ideas I gave in an earlier post towards fixing these remaining
issues now that it seems unlinkable multi-show credentials won't work:

@_date: 2002-08-24 16:11:42
@_author: Adam Back 
@_subject: Re: Cryptographic privacy protection in TCPA 
Since writing this I realised that there is a problem revoking
unlinkable multi-show credentials:
- I was presuming that revealing the credential and it's secret key is
sufficient to allow someone to link shows of that credential.
- but to link you'd have to try each revoked credential in turn.
Therefore the verifier would have to perform work linear in the number
of revoked credentials at each show, for the duration of the epoch.
Anonymous suggests one way out is to just define that the issuing CA
and the refreshing CA to be the same entity.  Then you already have to
trust the hardware manufacturer not to issue certs whose secrets are
outside of a TPM.  In this case Brands or Chaum credentials work.
The remaining desiderata are:
- it is not ideal from a risk management perspective to have to have
the hardware manufacturers endorsement private key online to refresh
certificates (or in general for there to be any private key online
that allows issuing of credentials whose private keys lie outside a
- not ideal to have to have an online protocol with an otherwise
non-existant third party (credential refresh CA) in order to avoid
Other ideas I gave in an earlier post towards fixing these remaining
issues now that it seems unlinkable multi-show credentials won't work:

@_date: 2002-08-21 02:24:21
@_author: Adam Back 
@_subject: Re: Cryptographic privacy protection in TCPA 
There was some off-list discussion about possibility for sharing these
credentials once a given credential is extracted from it's TPM by a
user who broke the tamper resistance of his TPM.
I also said:
Because Camenisch credentials are unlinkable multi-show it makes it
harder to recognize sharing, so the user could undetectably share
credentials with a small group that he trusts.  (By comparison with linkable pseudonymous credentials and a privacy CA
the issuer and/or verifier would see unusually high activity from a
given pseudonym or TPM endorsement key if the corresponding credential
were shared too widely.)
However if the Camenisch (unlinkable multi-show) credential were
shared too widely the issuer may also learn the secret key and hence
be able to link and so revoke the overly-shared credentials.  This
combats sharing though to a limited extent.
Another idea to improve upon this inherent risk of sharing too widely
may be to use a protocol which it is not safe to do parallel shows
with.  (Some ZKPs are not secure when you engage in multiple show
protocols in parallel.  Usually this is considered a bad thing, and
steps are taken to allow safe parallel show.)  For this application a show protocol which it is not safe to engage in
parallel shows may frustrate sharing: someone who shared the
credential too widely would have difficulty coordinating amongst the
sharees not to show the same credential in parallel.  I notice
Camenisch et al mention steps to avoid parallel showing problem, so
perhaps that feature could be reintroduced.
In contrast, the TPM can easily ensure that the credential is not used
in parallel shows.

@_date: 2002-08-21 02:24:21
@_author: Adam Back 
@_subject: Re: Cryptographic privacy protection in TCPA 
There was some off-list discussion about possibility for sharing these
credentials once a given credential is extracted from it's TPM by a
user who broke the tamper resistance of his TPM.
I also said:
Because Camenisch credentials are unlinkable multi-show it makes it
harder to recognize sharing, so the user could undetectably share
credentials with a small group that he trusts.  (By comparison with linkable pseudonymous credentials and a privacy CA
the issuer and/or verifier would see unusually high activity from a
given pseudonym or TPM endorsement key if the corresponding credential
were shared too widely.)
However if the Camenisch (unlinkable multi-show) credential were
shared too widely the issuer may also learn the secret key and hence
be able to link and so revoke the overly-shared credentials.  This
combats sharing though to a limited extent.
Another idea to improve upon this inherent risk of sharing too widely
may be to use a protocol which it is not safe to do parallel shows
with.  (Some ZKPs are not secure when you engage in multiple show
protocols in parallel.  Usually this is considered a bad thing, and
steps are taken to allow safe parallel show.)  For this application a show protocol which it is not safe to engage in
parallel shows may frustrate sharing: someone who shared the
credential too widely would have difficulty coordinating amongst the
sharees not to show the same credential in parallel.  I notice
Camenisch et al mention steps to avoid parallel showing problem, so
perhaps that feature could be reintroduced.
In contrast, the TPM can easily ensure that the credential is not used
in parallel shows.

@_date: 2002-08-18 15:58:56
@_author: Adam Back 
@_subject: Re: Cryptographic privacy protection in TCPA 
With Brands digital credentials (or Chaums credentials) another
approach is to make the endorsement key pair and certificate the
anonymous credential.  That way you can use the endorsement key and
certificate directly rather than having to obtain (blinded) identity
certificates from a privacy CA and trust the privacy CA not to issue
identity certificates without seeing a corresponding endorsement
However the idea with the identity certificates is that you can use
them once only and keep fetching new ones to get unlinkable anonymity,
or you can re-use them a bit to get pseudonymity where you might use a
different psuedonym for a different service where you are anyway
otherwise linkable to a given service.
With Brands credentials the smart card setting allows you to have more
compact and computationally cheap control of the credential from
within a smart card which you could apply to the TPM/SCP.  So you can
fit more (unnamed) pseudonym credentials on the TPM to start with.
You could perhaps more simply rely on Brands credential lending
discouraging feature (ability to encode arbitrary values in the
credential private key) to prevent break once virtualize anywhere.
For discarding pseudonyms and when you want to use lots of pseudonyms
(one-use unlinkable) you need to refresh the certificates you could
use the refresh protocol which allows you to exchange a credential for
a new one without trusting the privacy CA for your privacy.
Unfortunately I think you again are forced to trust the privacy CA not
to create fresh virtualized credentials.  Perhaps there would be
someway to have the privacy CA be a different CA to the endorsement CA
and for the privacy CA to only be able to refresh existing credentials
issued by the endorsement CA, but not to create fresh ones.
Or perhaps some restriction could be placed on what the privacy CA
could do of the form if the privacy CA issued new certificates it
would reveal it's private key.
"Also relevant is An Efficient System for Non-transferable Anonymous
Credentials with Optional Anonymity Revocation", Jan Camenisch and
Anna Lysyanskaya, Eurocrypt 01
These credentials allow the user to do unlinkable multi-show without
involving a CA.  They are somewhat less efficient than Chaum or Brands
credentials though.  But for this application does this removes the
need to trusting a CA, or even have a CA: the endorsement key and
credential can be inserted by the manufacturer, can be used
indefinitely many times, and are not linkable.
As you point out unlinkable anonymity tends to complicate revocation.
I think Camenisch's optional anonymity revocation has similar
properties in allowing a designated entity to link credentials.
Another less "TTP-based" approach to unlinkable but revocable
credentials is Stubblebine's, Syverson and Goldschlag, "Unlinkable
Serial Transactions", ACM Trans on Info Systems, 1999:
(It's quite simple you just have to present and relinquish a previous
pseudonym credential to get a new credential; if the credential is due
to be revoked you will not get a fresh credential.)
I think I would define away the problem of local breaks.  I mean the
end-user does own their own hardware, and if they do break it you
can't detect it anyway.  If it's anything like playstation mod-chips
some proportion of the population would in fact would do this.  May be
1-5% or whatever.  I think it makes sense to just live with this, and
of course not make it illegal.  Credentials which are shared are
easier to revoke -- knowledge of the private keys typically will
render most schemes linkable and revocable.  This leaves only online
lending which is anyway harder to prevent.

@_date: 2002-08-18 15:58:56
@_author: Adam Back 
@_subject: Re: Cryptographic privacy protection in TCPA 
With Brands digital credentials (or Chaums credentials) another
approach is to make the endorsement key pair and certificate the
anonymous credential.  That way you can use the endorsement key and
certificate directly rather than having to obtain (blinded) identity
certificates from a privacy CA and trust the privacy CA not to issue
identity certificates without seeing a corresponding endorsement
However the idea with the identity certificates is that you can use
them once only and keep fetching new ones to get unlinkable anonymity,
or you can re-use them a bit to get pseudonymity where you might use a
different psuedonym for a different service where you are anyway
otherwise linkable to a given service.
With Brands credentials the smart card setting allows you to have more
compact and computationally cheap control of the credential from
within a smart card which you could apply to the TPM/SCP.  So you can
fit more (unnamed) pseudonym credentials on the TPM to start with.
You could perhaps more simply rely on Brands credential lending
discouraging feature (ability to encode arbitrary values in the
credential private key) to prevent break once virtualize anywhere.
For discarding pseudonyms and when you want to use lots of pseudonyms
(one-use unlinkable) you need to refresh the certificates you could
use the refresh protocol which allows you to exchange a credential for
a new one without trusting the privacy CA for your privacy.
Unfortunately I think you again are forced to trust the privacy CA not
to create fresh virtualized credentials.  Perhaps there would be
someway to have the privacy CA be a different CA to the endorsement CA
and for the privacy CA to only be able to refresh existing credentials
issued by the endorsement CA, but not to create fresh ones.
Or perhaps some restriction could be placed on what the privacy CA
could do of the form if the privacy CA issued new certificates it
would reveal it's private key.
"Also relevant is An Efficient System for Non-transferable Anonymous
Credentials with Optional Anonymity Revocation", Jan Camenisch and
Anna Lysyanskaya, Eurocrypt 01
These credentials allow the user to do unlinkable multi-show without
involving a CA.  They are somewhat less efficient than Chaum or Brands
credentials though.  But for this application does this removes the
need to trusting a CA, or even have a CA: the endorsement key and
credential can be inserted by the manufacturer, can be used
indefinitely many times, and are not linkable.
As you point out unlinkable anonymity tends to complicate revocation.
I think Camenisch's optional anonymity revocation has similar
properties in allowing a designated entity to link credentials.
Another less "TTP-based" approach to unlinkable but revocable
credentials is Stubblebine's, Syverson and Goldschlag, "Unlinkable
Serial Transactions", ACM Trans on Info Systems, 1999:
(It's quite simple you just have to present and relinquish a previous
pseudonym credential to get a new credential; if the credential is due
to be revoked you will not get a fresh credential.)
I think I would define away the problem of local breaks.  I mean the
end-user does own their own hardware, and if they do break it you
can't detect it anyway.  If it's anything like playstation mod-chips
some proportion of the population would in fact would do this.  May be
1-5% or whatever.  I think it makes sense to just live with this, and
of course not make it illegal.  Credentials which are shared are
easier to revoke -- knowledge of the private keys typically will
render most schemes linkable and revocable.  This leaves only online
lending which is anyway harder to prevent.

@_date: 2002-08-22 15:54:51
@_author: Adam Back 
@_subject: Re: the underground software vulnerability marketplace and its 
Right.  And I fail to see how any of this is dangerous.
Clearly people are free to sell information they create to anyone they
choose under any terms they choose.  (For example the iDEFENSE promise
of the author to not otherwise reveal for 2 weeks to give iDEFENSE
some value.)
This commercialisation seems like a _good thing_ as it may lead to
more breaks being discovered, and hence more secure software.
(It won't remain secret for very long -- given the existance of
anonymous remailers etc., but the time-delay in release allows the
information intermediary -- such as iDEFENSE -- to sell the
information to parties who would like it early, businesses for example
people with affected systems.
Criminal crackers who can exploit the information just assist in
setting a fair price and forcing vendors and businesses to recognise
the true value of the information.  Bear in mind the seller can not
know or distinguish between a subscriber who wants the information for
their own defense (eg a bank or e-commerce site, managed security
service provider), and a cracker who intends to exploit the
information (criminal organisation, crackers for amusement or
discovery of further inforamtion, private investigators, government
agencies doing offensive information warfare domesticaly or
I don't see any particular moral obligation for people who put their
own effort into finding a flaw to release it to everyone at the same
time.  Surely they can release it earlier to people who pay them to
conduct their research, and by extension to people who act as
intermediaries for the purpose of negotiating better terms or being
able to package the stream of ongoing breaks into more comprehensive
subscription service.  I think HP were wrong, and find their actions in trying to use legal
scare tactics reprehensible: they should either negotiate a price, or
wait for the information to become generally available.

@_date: 2002-09-17 21:05:36
@_author: Adam Back 
@_subject: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The OS can stop user processes inspecting each others address space.
Therefor a remote exploit in one piece of application software should
not result in a compromise of another piece of software.  (So an IE
bug should not allow the banking application to be broken.)  (Note
also that in practice with must current OSes converting gaining root
once given access to local processes is not that well guaranteed).
However the OS itself is a complex piece of software, and frequently
remote exploits are found in it and/or the device drivers it runs.  OS
exploits can freely ignore the protection between user applications,
reading your banking keys.
Even if a relatively secure OS is run (like some of the BSD variants),
the protection is not _that_ secure.  Vulnerabilities are found
periodically (albeit mostly by the OS developers rather than
externally -- as far as we know).  Plus also the user may be tricked
into running trojaned device drivers.
So one approach to improve this situation (protect the user from the
risks of trojaned device drivers and too large and complex to
realistically assure security of OSes) one could run the OS itself in
ring0 and a key store and TOR in ring-1 (the palladium approach). Some seem to be arguing that you don't need a ring-1.  But if you read
the paper Peter provided a reference for, they conclude that the
pentium architecture is not (efficiently) securely virtualizable.  The
problem area is the existance of sensitive but unprivileged
The fact that VMWare works just means they used some tricks to make it
practically virtualize some common OSes, not that it is no longer
possible to write malicious software to run as user or privileged
level inside the guest OS and have it escape the virtualization.
(It is potentially inefficently securely virtualizable using complete
software emulation, but this is highly inefficient).
(Anonymous can continue on cypherpunks if Perry chooses to censor his
further comments.)

@_date: 2002-09-17 21:05:36
@_author: Adam Back 
@_subject: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The OS can stop user processes inspecting each others address space.
Therefor a remote exploit in one piece of application software should
not result in a compromise of another piece of software.  (So an IE
bug should not allow the banking application to be broken.)  (Note
also that in practice with must current OSes converting gaining root
once given access to local processes is not that well guaranteed).
However the OS itself is a complex piece of software, and frequently
remote exploits are found in it and/or the device drivers it runs.  OS
exploits can freely ignore the protection between user applications,
reading your banking keys.
Even if a relatively secure OS is run (like some of the BSD variants),
the protection is not _that_ secure.  Vulnerabilities are found
periodically (albeit mostly by the OS developers rather than
externally -- as far as we know).  Plus also the user may be tricked
into running trojaned device drivers.
So one approach to improve this situation (protect the user from the
risks of trojaned device drivers and too large and complex to
realistically assure security of OSes) one could run the OS itself in
ring0 and a key store and TOR in ring-1 (the palladium approach). Some seem to be arguing that you don't need a ring-1.  But if you read
the paper Peter provided a reference for, they conclude that the
pentium architecture is not (efficiently) securely virtualizable.  The
problem area is the existance of sensitive but unprivileged
The fact that VMWare works just means they used some tricks to make it
practically virtualize some common OSes, not that it is no longer
possible to write malicious software to run as user or privileged
level inside the guest OS and have it escape the virtualization.
(It is potentially inefficently securely virtualizable using complete
software emulation, but this is highly inefficient).
(Anonymous can continue on cypherpunks if Perry chooses to censor his
further comments.)

@_date: 2002-10-22 15:52:16
@_author: Adam Back 
@_subject: Palladium -- trivially weak in hw but "secure in software"?? (Re: palladium presentation - anyone go 
Remote attestation does indeed require Palladium to be secure against
the local user.  However my point is while they seem to have done a good job of
providing software security for the remote attestation function, it
seems at this point that hardware security is laughable.
So they disclaim in the talk announce that Palladium is not intended
to be secure against hardware attacks:
so one can't criticise the implementation of their threat model -- it
indeed isn't secure against hardware based attacks.
But I'm questioning the validity of the threat model as a realistic
and sensible balance of practical security defenses.
Providing almost no hardware defenses while going to extra-ordinary
efforts to provide top notch software defenses doesn't make sense if
the machine owner is a threat.
The remote attestation function clearly is defined from the view that
the owner is a threat.
Without specifics and some knowledge of hardware hacking we can't
quantify, but I suspect that hacking it would be pretty easy.  Perhaps
no soldering, $50 equipment and simple instructions anyone could
more inline below...
I think standard memory could be used.  I can think of simple
processor modifications that could fix this problem with hardware
tamper resistance assurance to the level of having to tamper with .13
micron processor.  The processor is something that could be epoxyied
inside a cartridge for example (with the cartridge design processor +
L2 cache housings as used by some Intel pentium class processors),
though probably having to tamper with a modern processor is plenty
hard enough to match software security given software complexity

@_date: 2002-10-25 01:37:32
@_author: Adam Back 
@_subject: internet radio - broadcast without incurring royalty fees 
Re. the recent rapacious "broadcast" royalties imposed on internet
radio in the US, it occurs to me it wouldn't be that hard to do the
following and it would probably avoid the royalties even under the
current imbalanced IP laws:
- have the station broadcast it's own content (commentary)
- have the station broadcast song titles, song authors, CDDB serial numbers
- the user would use third-party software capable of playing the
recommended track, such as:
- coincidentally owning the CD and having the CD in a CD jukebox
- owning (or not) the CD and having a mp3 rip of the track on hard
  disk
- queueing the track for download via kazaa
examples of the last are the morpheus plugin for winamp (I think it
was morpheus that had such a plugin -- though it is probably no longer
supported with the morpheus protocol switch).
For performance reasons the station could even pre-queue the tracks
during their commentary and then trigger the start of play after the
track has had some time to be selected by the jukebox / streaming
buffer fill from kazaa.
Seems to me this would pass current IP laws because it is like a radio
station which broadcast the name of a song and the user is expected to
insert the CD in his player and play along to keep up with the
commentary, only automated and with open APIs for the "load and play
this CD track" instructions so people can hook it up to whatever is
convenient to them.

@_date: 2003-08-03 04:41:21
@_author: Adam Back 
@_subject: Re: Secure IDE? 
I believe that is what some of them are doing.  I think it's a little
better to use some fast PRNG seeded from the sector (or eg HMAC of
sector number or encryption of sector number if you have hardware).
The sector number is changing in counter order and cancels with the
plaintext difference.  I did some tests on a 10GB disk full of windows
app and program data (accessed the raw windows partition from linux
number) you get a fair few collisions.
one of the products on show at RSA earlier this year would boot from
the IDE sector onto a virtual drive (it would pretend to be a boot
sector over the IDE connector), then that boot sector has code to ask
for your password, derive the key and load it, and then reboot onto
the real drive.  If you pulled power from the drive it would forget
the key.

@_date: 2003-08-03 04:41:21
@_author: Adam Back 
@_subject: Re: Secure IDE? 
I believe that is what some of them are doing.  I think it's a little
better to use some fast PRNG seeded from the sector (or eg HMAC of
sector number or encryption of sector number if you have hardware).
The sector number is changing in counter order and cancels with the
plaintext difference.  I did some tests on a 10GB disk full of windows
app and program data (accessed the raw windows partition from linux
number) you get a fair few collisions.
one of the products on show at RSA earlier this year would boot from
the IDE sector onto a virtual drive (it would pretend to be a boot
sector over the IDE connector), then that boot sector has code to ask
for your password, derive the key and load it, and then reboot onto
the real drive.  If you pulled power from the drive it would forget
the key.

@_date: 2003-08-06 18:05:27
@_author: Adam Back 
@_subject: Re: What happened to the Cryptography list...? 
The problems with closed lists relying on a single human for
forwarding and filtering...
Couldn't he just let people post in his absence?  It kind of detracts
from a list if it disappears for weeks time on a regular basis.
Also there are delays, and then there's Perry decisions that a
discussion is no longer worth persuing when contributors are still
interested to discuss.
Adam

@_date: 2003-08-06 18:05:27
@_author: Adam Back 
@_subject: Re: What happened to the Cryptography list...? 
The problems with closed lists relying on a single human for
forwarding and filtering...
Couldn't he just let people post in his absence?  It kind of detracts
from a list if it disappears for weeks time on a regular basis.
Also there are delays, and then there's Perry decisions that a
discussion is no longer worth persuing when contributors are still
interested to discuss.
Adam

@_date: 2003-08-28 05:05:07
@_author: Adam Back 
@_subject: Re: traffix analysis 
I agree with anonymous summary of the state of the art wrt
cryptographic anonymity of interactive communications.
Ulf Moeller, Anton Stiglic, and I give some more details on the
attacks anonymous describes in this IH 2001 [1] paper:
which explores this in the context of ZKS Freedom Network, and Pipenet
presenting attacks on the Freedom Network, Onion Network, Crowds and
Pipenet which affect privacy and availability.
"Traffic Analysis Attacks and Trade-Offs in Anonymity Providing
Systems", IH 2001, Adam Back, Ulf Moeller, and Anton Stiglic.

@_date: 2003-09-26 19:47:51
@_author: Adam Back 
@_subject: free hosting for cpunkly projects... 
remops and cpunks:
 are offering:
free for 3 years as an advertising ploy to get into small business /
personal web posting.
They use a computerized phone call back to prevent people being silly
and registering thousands of accounts, so you have to give them your
phone  (And that works for US and Canadian numbers only).
But they don't require a credit card (unless you start buying extras).
btw They also have the cheapest domain registration I've seen so far
at $6/year.
You also don't have to transfer your domain to them in order to host
there, you can just point your existing domain's DNS to them, and they
also will optionally keep your existing MX in their DNS so your mail
is handled however it was before.
Well just thought remops and cpunks might find some interesting uses
or it would be a bonus to people working on crypto / remailer coding,
or internet service type stuff to have some serious hosting to work
with or do dev work in.  (They have gcc, pgp, ssh etc installed).
They seem quite serious hosting-wise and I got a 12 hr turn around on
a fix for a system /etc/csh.cshrc bug that was preventing me changing
my shell to tcsh which is pretty impressive.  (Most hosting shops
wouldn't bother answering or would fob you off, or your query would
not get anywhere near someone clue-full enough to understand never
mind tweak and fix the minor problem).  Their bandwidth is supposed to be pretty good too as they have 12Gbits
connectivity.  Claim to have been in business for 11 years.
3 years free seems pretty generous (most free offers are a few months
or 1 year tops).  The offer is available only until 31st Dec this
year.  From what I could see from the two I did (one for someone else) based
on serial number on account it looks like they might be signing up
around 1000 accounts per day.  So I could imagine they might pull the
offer also if they got too much take up!
Have phun.

@_date: 2003-10-31 23:26:05
@_author: Adam Back 
@_subject: Re: ECC and blinding. 
So Chaumian blinding with public exponent e, private exponent d, and
modulus n is this and blinding factor b chosen by the client:
b^e.m mod n	 ->
 			= b.m^d mod n  (simplifying)
and divide by b to unblind:
m^d mod n
how are you going to do this over EC?  You need an RSA like e and d to
Brands DH based blinding scheme works in EC.  ECDH is directly
analogous, the usual conversion from discrete log (g^x mod p) to the
EC analog (x.G over curve E) works.

@_date: 2003-10-28 21:49:49
@_author: Adam Back 
@_subject: Re: ECC and blinding. 
There are two variants of Brands schemes: over RSA or DH.  The DH
variant can be used with the EC.  People don't do RSA over EC because
the security argument doesn't work (ie I believe you can do it
technically, but the performance / key size / security arguments no
longer work).
So for that reason I think Chaum's scheme practically would not be
viable over EC.  (Or you could do it but you'd be better off
performance, security and key/messag size doing Chaum over normal
There are other blinding schemes also such as David Wagner's blind MAC
approach, and that should work over EC as it is DH based.

@_date: 2003-11-02 17:26:56
@_author: Adam Back 
@_subject: Re: ECC and blinding. 
Fair enough.  But this is not Chaum's scheme, it is Wagners and it is
DH based (or ECDH based in your writeup).
You said earlier:
and the above scheme is not Chaumian blinding.  Chaum never invented
DH blinding, if you read Brands thesis even you'll see that Chaum (who
was Brands PhD supervisor for some of the time) told Brands to forget
about trying to do DH based blinding because it's not possible.
Brands credits Chaum for setting the challenge :-) which led him to
find ways to do DH based blinding.  (And the private key certificate
which is a generalisation of DH blinding to multiple attributes and
selective disclosure of those attributes).

@_date: 2003-12-28 18:29:27
@_author: Adam Back 
@_subject: Re: Microsoft publicly announces Penny Black PoW postage project 
Oh yes forgot one comment:
One down-side of memory bound is that it is memory bound.  That is to
say it will be allocated some amount of memory, and this would be
chosen to be enough memory to that a high end machine should not have
that much cache so think multiple MB, maybe 8MB, 16MB or whatever.
(Not sure what is the max L2 cache on high end servers).
And what the algorithm will do is make random accesses to that memory
as fast as it can.
So effectively it will play badly with other applications -- tend to
increase likelihood of swapping, decrease memory available for other
applications etc.  You could think of the performance implications as
a bit like pulling 8MB of ram or whatever the chosen value is.
hashcash / computationally bound functions on the other hand have a
tiny footprint and CPU consumption by hashcash can be throttled to
avoid noticeable impact on other applications.

@_date: 2003-12-27 02:37:18
@_author: Adam Back 
@_subject: Re: Microsoft publicly announces Penny Black PoW postage project 
I did work at Microsoft for about a year after leaving ZKS, but I quit
a month or so ago (working for another startup again).
But for accuracy while I was at Microsoft I was not part of the
microsoft research/academic team that worked on penny black, though I
did exchange a few emails related to that project and hashcash etc
with the researchers.
I thought the memory-bound approaches discussed on CAMRAM before were
along the lines of hash functions which chewed off artificially large
code foot-print as a way to impose the need for memory.  Arnold Reinhold's HEKS [1] (Hash Extended Key Stretcher) key stretching
algorithm is related also.  HEKS aims to make hardware attacks on key
stretching more costly: both by increasing the memory footprint
required to efficiently compute it, and by requiring operations that
are more expensive in silicon (32 bit multiplies, floating point is
another suggestion he makes).
The relationship to hashcash is you could simply use HEKS in place of
SHA1 to get the desired complexity and hence silicon cost increase.
"The main design goal of this algorithm is to make massively parallel
key search machines it as expensive as possible by requiring many
32-bit multiplies and large amounts of memory."
I think I also recall discussing with Peter Gutmann the idea of using
more complex hash functions (composed of existing hash functions for
security) to increase the cost of hardware attacks.
The innovation in the papers referred to by the Penny Black project
was the notion of building a cost function that was limited by memory
bandwidth rather CPU speed.  In otherwords unlike hashcash (which is
CPU bound and has minimal working memory or code footprint) or a
notional hashcash built on HEKS or other similar system (which is
supposed to take memory and generaly expensive operations to build in
silicon), the two candidate memory-bound functions are designed to be
computationally cheap but require a lot of random access memroy
utilization in a way which frustrates time-space trade-offs (to reduce
space consumption by using a faster CPU).  They then argue that this
is desirable because there is less discrepency in memory latency
between high end systems and low end systems than there is discrepency
in CPU power.
The 2nd memory [3] bound paper (by Dwork, Goldber and Naor) finds a
flaw in in the first memory-bound function paper (by Adabi, Burrows,
Manasse, and Wobber) which admits a time-space trade-off, proposes an
improved memory-bound function and also in the conclusion suggests
that memory bound functions may be more vulnerable to hardware attack
than computationally bound functions.  Their argument on that latter
point is that the hardware attack is an economic attack and it may be
that memory-bound functions are more vulnerable to hardware attack
because you could in their view build cheaper hardware more
effectively as the most basic 8-bit CPU with slow clock rate could
marshall enough fast memory to under-cut the cost of general purpose
CPUs by a larger margin than a custom hardware optimized
hashcash/computationally bound function.
I'm not sure if their conclusion is right, but I'm not really
qualified -- it's a complex silicon optimization / hardware
acceleration type question.
[1] [2] Abadi, Burrows, Manasse and Wobber "Moderately Hard, Memory-bound
Functions", Proceedings of the 10th Annual Network and Distributed
System Security Symposium, February 2003
[3] Dwork, Goldberg, and Naor, "On Memory-Bound Functions for Fighting
Spam", Proceedings of the 23rd Annual International Cryptology
Conference (CRYPTO 2003), August 2003.

@_date: 2004-05-18 22:49:11
@_author: Adam Back 
@_subject: Re: 3. Proof-of-work analysis 
Here's a forward of parts of an email I sent to Richard with comments on
his and Ben's paper (sent me a pre-print off-list a couple of weeks ago):
One obvious comment is that the calculations do not take account of
the CAMRAM approach of charging for introductions only.  You mention
this in the final para of conclusions as another possible.
My presumption tho don't have hard stats to measure the effect is that
much of email is to-and-fro between existing correspondents.  So if I
were only to incur the cost of creating a stamp at time of sending to
a new recipient, I could bear a higher cost without running into
However the types of levels of cost envisaged are aesthetically
unpleasing; I'd say 15 seconds is not very noticeable 15 mins is
noticeable and 1.5 hrs is definately noticeable.
Of course your other point that we don't know how spammers will adapt
is valid.  My presumption is that spam would continue apace, the best
you could hope for would be that it is more targetted, that there are
financial incentives in place to make it worth while buying
demographics data.  (After all when you consider the cost of sending
junk paper mail is way higher, printing plus postage, and yet we still
receive plenty of that).
Also as you observe if the cost of spamming goes up, perhaps they'll
just charge more.  We don't know how elastic the demand curve is.
Profitability, success rates etc are one part of it.  There is an
interplay also: if quantity goes down, perhaps the success rate on the
remaining goes up.  Another theory is that a sizeable chunk of spam is
just a ponzi scheme: the person paying does not make money, but a lot
of dummy's keep paying for it anyway.
Another potential problem with proof-of-work on introductions only, is
that if the introduction is fully automated without recipient opt-in,
spammers could also benefit from this amortized cost.  So I would say
something like the sender sent a proof-of-work, and the recipient took
some positive action, like replying, filing otherwise than junk or
such should be the minimum to get white-listed.
On the ebiz web site problem, I think these guys present a problem for
the whole approach.  An ebiz site will want to send lots of mail to
apparent new recipients (no introductions only saving), a popular ebiz
site may need to send lots of mail.
Well it is ebiz so perhaps they just pass the cost on to the consumer
and buy some more servers.
Another possibility is the user has to opt-in by pre-white-listing
them, however the integration to achieve this is currently missing and
would seem a difficult piece of automation to retrofit.
One of the distinguishing characteristics of a spammer is the
imbalance between mail sent and mail received.  Unfortunately I do not
see a convenient way to penalize people who fall into this category.
Also because of network effect concerns my current hashcash deployment
is to use it as a way to reduce false positives, rather than directly
requiring hashcash.  Well over time this could come to the same thing,
but it gives it a gentle start, so we'll see how long it is before the
1st genuine spam with hashcash attached.
CAMRAM's approach is distinct and is literally going straight for the
objective of bouncing mail without some kind of proof (hashcash or
reverse-turing, or short term ability to reply to email

@_date: 2004-05-12 11:20:33
@_author: Adam Back 
@_subject: Re: who goes 1st problem 
I think this is ok.  Would suggest you remove the nym field, have
one-use credentials (to avoid linkability across provers), and only
reveal separate nym cert after have satisfied policy.
Again ok.  You send either fake cert, or real cert for as many
attributes as the CA issues.  You may not even know what some of the
attributes that the CA issues are, all you know is the number of them.
You use and / or connectives between them (using k xor r, k; or r, r
respectively) but using OBSE algorithm (xor refers to improved HC
scheme by HC authors in Yes, that works, but is defined required part of protocol; that way
optimal cover (within limits of partial policy concealment) is given
for sensitive attributes, policies etc.
Sounds same as above.
That's true.  Think there is a trade-off between degree of
concealment, and amount of permutations prover has to try.  You could perhaps define an ordering of attributes safely, followed by
dealing with unordered undeclared attributes.  Other thought perhaps a FPGA like layout where all possible
connectives patterns are represented, might allow to specify arbitrary
boolean formulae with and / or connectives with full policy
concealment but less space and time efficient.
(Calling it prover is kind of odd I find when the prover convinces only
himselfhe satisfies policy by default and optionally chooses whether
to disclose that to verifier.  And "the prover" is the passive entity
receiving encrypted comms, which is back-to-front to usual
prover-verifier comms pattern.  Maybe sender and recipient is better.)

@_date: 2004-05-10 22:59:40
@_author: Adam Back 
@_subject: Re: more hiddencredentials comments (Re: Brands' private credentials) 
Gap may be I'm misunderstanding something about the HC approach.  We have:
 P = (P1 or P2) is encoded HC_E(R,p) = {HC_E(R,P1),HC_E(R,P2)}
so one problem is marking, the server sends you different R values:
so you described one way to fix that by using symmetric crypto (where
it is difficult to get a message to decrypt 2 different ways with
different keys and get other than line noise out of the 2nd key).
But next problem you mentioned, server could simply lie and send you
for random value R2 now if you reply he knows you have property P1.
So I was suggesting that after you decrypt HC_E(R,P1) you encrypt it
again to check if R2 == HC_E(R,P2) which you should be able to do if
you know P2, you have R (because you just decrypted it), and if you
tweak the crypto system so that there is no non-deterministic aspect
such as OAEP, randomization factors etc.
If one were not explicitly interested in the IBE communication
pattern, and to avoid the patents in IETF protocol problems, I would
think one could do something without IBE.
eg. you mentioned earlier the problem of issuing one cert per
attribute permutation.  Instead how about you issue one cert per
attribute to psuedonym plus attribute.  In the case where you are not
due the attribute, you just don't learn the corresponding private key.
One problem with this is you have to avoid the server learning the
private key for the one you don't.
Now it might be possible eg. with Elgamal / DH to make an efficient
non-interactive ZKP that convinces you that the server chose the
private key fairly (and so does not know any corresponding private
key).  But another way to side-step the issue is to have the CA issue
you two certs per attribute.  You choose the private key for one it
chooses the private key for the other.  Data is encrypted with both
keys.  In the case of you not being due the attribute the CA does not
give you the private key it generated.
You could probably use some of the key gen stuff from multi-party
signatures (where multiple parties are involved and each holds a
private key fragment), however they tend to be inefficient I think so
above is probably simple and efficient enough.

@_date: 2004-05-10 22:24:31
@_author: Adam Back 
@_subject: Re: blinding & BF IBE CA assisted credential system (Re: chaum's patent expiry?) 
But if I understand that is only half of the picture.  The recipient's
IBE CA will still be able to decrypt, tho the sender's IBE CA may not
as he does not have ability to compute pseudonym private keys for the
other IBE CA.
If you make it PFS, then that changes to the recipient's IBE CA can
get away with active MITM rather than passive eavesdropping.
An aside is that PKI for Psuedonym's is an interesting question.  The
pseudonym can't exactly go and be certified by someone else as an
introducer without revealing generally identifying things about the
network of trust.  But ignoring this presuming that the identities
were not subject to MITM from day one, and could build up a kind of
WoT despite lack of out-of-band way to check info to base WoT
signatures on.  It would still be interesting to defend the pseudonym
against MITM colluding with IBE CA that at some point after the
pseudonym has transferred keys without insertion of a MITM from.
This problem of addressing the who goes first problem for pseudonymous
communicants appears somewhat related to Public Key Steganography
where there is a similar scenario and threat model.  (Anderson and
Petitcolas"On The Limits of Steganography"
They also cite a "Prisoners' problem" which might be something you
could extend involving a warden who is eavesdropping and prisoners who
will be penalized if he can detect and identify communicants.
My earlier comment:
may not be that useful a distinction as the IBE CA server also gets
your private key, so he doesn't _need_ to generate a certificate
impersonating you as a conventional rogue CA would.
But if we could make the binding from pseudonym to the pseudonym's
non-IBE public key strictly first come first served, so that the IBE
CA's attemt to claim his later released non-IBE public key is the
correct one would be detectable.  Either secure time-stamping,
extending the psuedonym name to include fingerprint as
self-authenticator would allow this.

@_date: 2004-05-10 09:35:28
@_author: Adam Back 
@_subject: Re: Brands' private credentials 
Well SSL was just to convince you that you were talking to the right
server ("you have reached the AIDs db server").
After that I was presuming you use a signature to convince the server
that you are authorised.  Your comment however was that this would
necessarily leak to the server whether you were a doctor or an AIDs
However from what I understood from your paper so does your scheme,
from section 5.1:
P = (P1 or P2) is encoded HC_E(R,p) = {HC_E(R,P1),HC_E(R,P2)} With Hidden Credentials, the messages are in the other direction: the
server would send something encrypted for your pseudonym with P1 =
AIDs patient, and P2 = Doctor attributes.  However the server could
mark the encrypted values by encoding different challenge response
values in each of them, right?
(Think you would need something like Bert Jaap-Koops Binding
cryptography where you can verify externally to encryption that the
contained encrypted value is the same to prevent that; or some other
proof that they are the same.)
Another approach to hiding membership is one of the techniques
proposed for non-transferable signatures, where you use construct:
RSA-sig_A(x),RSA-sig_B(y) and verification is x xor y = hash(message).
Where the sender is proving he is one of A and B without revealing
which one.  (One of the values is an existential forgery, where you
choose a z value first, raise it to the power e, and claim z is a
signature on x= z^e mod n; then you use private key for B (or A) to
compute the real signature on the xor of that and the hash of the
message).  You can extend it to moer than two potential signers if
OK so the fact that the server is the AIDs db server is itself secret.
Probably better example is dissident's server or something where there
is some incentive to keep the identity of the server secret.  So you
want bi-directional anonymity.  It's true that the usual protocols can
not provide both at once; SSL provides neither, the anonymous IP v2
protocol I designed at ZKS had client anonymity (don't reveal
pseudonym until authenticate server, and yet want to authenticate
channel with pseudonym).  This type of bi-directional anonymity pretty
much is going to need something like the attribute based encryption
model you're using.
However it would be nice/interesting if one could do that end-2-end
secure without needing to trust a CA server.
this one is a feature auth based systems aren't likely to be able to
fullfil, you can say this because the server doesn't know if you're
able to decrypt or not
I think it would be fair to call it anonymity system, just that the
trust model includes a trusted server.  There are lots of things
possible with a trusted server, even with symmetric crypto (KDCs).

@_date: 2004-05-10 09:35:28
@_author: Adam Back 
@_subject: Re: Brands' private credentials 
Well SSL was just to convince you that you were talking to the right
server ("you have reached the AIDs db server").
After that I was presuming you use a signature to convince the server
that you are authorised.  Your comment however was that this would
necessarily leak to the server whether you were a doctor or an AIDs
However from what I understood from your paper so does your scheme,
from section 5.1:
P = (P1 or P2) is encoded HC_E(R,p) = {HC_E(R,P1),HC_E(R,P2)} With Hidden Credentials, the messages are in the other direction: the
server would send something encrypted for your pseudonym with P1 =
AIDs patient, and P2 = Doctor attributes.  However the server could
mark the encrypted values by encoding different challenge response
values in each of them, right?
(Think you would need something like Bert Jaap-Koops Binding
cryptography where you can verify externally to encryption that the
contained encrypted value is the same to prevent that; or some other
proof that they are the same.)
Another approach to hiding membership is one of the techniques
proposed for non-transferable signatures, where you use construct:
RSA-sig_A(x),RSA-sig_B(y) and verification is x xor y = hash(message).
Where the sender is proving he is one of A and B without revealing
which one.  (One of the values is an existential forgery, where you
choose a z value first, raise it to the power e, and claim z is a
signature on x= z^e mod n; then you use private key for B (or A) to
compute the real signature on the xor of that and the hash of the
message).  You can extend it to moer than two potential signers if
OK so the fact that the server is the AIDs db server is itself secret.
Probably better example is dissident's server or something where there
is some incentive to keep the identity of the server secret.  So you
want bi-directional anonymity.  It's true that the usual protocols can
not provide both at once; SSL provides neither, the anonymous IP v2
protocol I designed at ZKS had client anonymity (don't reveal
pseudonym until authenticate server, and yet want to authenticate
channel with pseudonym).  This type of bi-directional anonymity pretty
much is going to need something like the attribute based encryption
model you're using.
However it would be nice/interesting if one could do that end-2-end
secure without needing to trust a CA server.
this one is a feature auth based systems aren't likely to be able to
fullfil, you can say this because the server doesn't know if you're
able to decrypt or not
I think it would be fair to call it anonymity system, just that the
trust model includes a trusted server.  There are lots of things
possible with a trusted server, even with symmetric crypto (KDCs).

@_date: 2004-05-09 10:04:31
@_author: Adam Back 
@_subject: Re: Brands' private credentials 
[copied to cpunks as cryptography seems to have a multi-week lag these
OK, now having read:
and seeing that it is a completely different proposal essentially
being an application of IBE, and extension of the idea that one has
multiple "identities" encoding attributes.  (The usual attribute this
approach is used for is time-period of receipt .. eg month of receipt
so the sender knows which key to encrypt with).
so here is one major problem with using IBE: everyone in the system
has to trust the IBE server!
One claim is that the system should hide sensitive attributes from
disclosure during a showing protocol.  So the example given an AIDs
patient could authenticate to an AIDS db server without revealing to
an outside observer whether he is an AIDs patient or an authorised
However can't one achieve the same thing with encryption: eg an SSL
connection and conventional authentication?  Outside of this, the usual approach to this is to authenticate the
server first, then authenticate the client so the client's privacy is
Further more there seems to be no blinding at issue time.  So to
obtain a credential you would have to identify yourself to the CA /
IBE identity server, show paper credentials, typically involving True
Name credentials, and come away with a private key.  So it is proposed
in the paper the credential would be issued with a pseudonym.  However
the CA can maintain a mapping between True Name and pseudonym.
However whenever you show the credential the event is traceable back
to you by collision with the CA.
I would not say your Hidden Credential system _is_ an anonymous
credential system.  There is no blinding in the system period.  All is
gated via a "trust-me" CA that in this case happens to be an IBE
server, so providing the communication pattern advantages of an IBE
What it enables is essentially an offline server assisted oblivious
encryption where you can send someone a message they can only decrypt
if they happen to have an attribute.  You could call this a credential
system kind of where the showing protcool is the verifier sends you a
challenge, and the shower decrypts the challenge and sends the result
In particular I don't see any way to implement an anonymous epayment
system using Hidden Credentials.  As I understand it is simply not
possible as the system has no inherent cryptographic anonymity?

@_date: 2004-05-09 10:04:31
@_author: Adam Back 
@_subject: Re: Brands' private credentials 
[copied to cpunks as cryptography seems to have a multi-week lag these
OK, now having read:
and seeing that it is a completely different proposal essentially
being an application of IBE, and extension of the idea that one has
multiple "identities" encoding attributes.  (The usual attribute this
approach is used for is time-period of receipt .. eg month of receipt
so the sender knows which key to encrypt with).
so here is one major problem with using IBE: everyone in the system
has to trust the IBE server!
One claim is that the system should hide sensitive attributes from
disclosure during a showing protocol.  So the example given an AIDs
patient could authenticate to an AIDS db server without revealing to
an outside observer whether he is an AIDs patient or an authorised
However can't one achieve the same thing with encryption: eg an SSL
connection and conventional authentication?  Outside of this, the usual approach to this is to authenticate the
server first, then authenticate the client so the client's privacy is
Further more there seems to be no blinding at issue time.  So to
obtain a credential you would have to identify yourself to the CA /
IBE identity server, show paper credentials, typically involving True
Name credentials, and come away with a private key.  So it is proposed
in the paper the credential would be issued with a pseudonym.  However
the CA can maintain a mapping between True Name and pseudonym.
However whenever you show the credential the event is traceable back
to you by collision with the CA.
I would not say your Hidden Credential system _is_ an anonymous
credential system.  There is no blinding in the system period.  All is
gated via a "trust-me" CA that in this case happens to be an IBE
server, so providing the communication pattern advantages of an IBE
What it enables is essentially an offline server assisted oblivious
encryption where you can send someone a message they can only decrypt
if they happen to have an attribute.  You could call this a credential
system kind of where the showing protcool is the verifier sends you a
challenge, and the shower decrypts the challenge and sends the result
In particular I don't see any way to implement an anonymous epayment
system using Hidden Credentials.  As I understand it is simply not
possible as the system has no inherent cryptographic anonymity?

@_date: 2004-05-09 09:08:09
@_author: Adam Back 
@_subject: Re: Brands' private credentials 
It was implemented at least twice: once by ECAFE ESPRIT project years
ago, more recently by ZKS before they stopped licensing the patents.
I looked at Camenisch protocol briefly a couple of years ago and it is
not based Brands.  It is less efficient computationally, and more
rounds of communication are required if I recall.
But one feature that it does have that Brands doesn't have directly is
self-reblindability.  In their protocol it is the credential holder
who does the blinding, rather than the issuer / holder, and the issuer
can also re-blind to get a 2nd unlinkable show.  The way you do this
with Brands is to have the CA issue you a new credential in a
re-issuing protocol; Brands re-issuing protocol has the property that
you do not even have to reveal to the CA what attributes are in the
re-issued cert.
On re-showable/re-blindable approach, as with Ernie Brikell's
re-showable credential proposal for Palladium the converse side of
unlinkable re-showing is that there is no efficient way to revoke
credentials.  (If eg the private key is compromised, or the credential
owner violates some associated policy in the Palladium/DRM case).
(Caveat of course I think DRM is an unenforceable idea and the
schelling point ought to be not to even pretend to do it in software
or hardware, rip-once copy-everywhere *always* wins).
Is this the same as described in  with
interactive cut-and-choose and large credenitals?  There was some
discussion of that protocol in:
 Not read the new paper you cite yet.
The problem with the Yacobi's scheme (which is based on a composite
modulus variant of DH where you choose n=p.q such that p and q are
relatively smooth so you can do discrete log to setup the public key
for an identity) is that to get desirable security parameters for n
(eg 1024 bits) you have to expend huge amounts of resources per
identity public key.  So I would say it is not really practical.  It
is the only other semi-practical IBE scheme that I am aware of which
is why Boneh and Franklins IBE based on weil pairing was considered
such a break through.

@_date: 2004-05-09 11:07:47
@_author: Adam Back 
@_subject: anonymous IRC project needs new home... 
The anonymous IRC project (IIP -- provides encrypted anonymous IRC chat.
Haven't looked in the protocol in detail to see how they get their
anonymity, but the guy seemed aware of Chaum etc and they have crypto
protocols document up there.
They have resource problems in continuing to run it, and so have
announced end-of-life for the project, but source etc is available,
and they are calling for interest in taking over the project.
Anyone with a bit of bandwidth and interest in preserving anonymity of
IRC want to help them out?
(The way I first heard about the project is that they use hashcash to
throttle nym registration abuse -- before that people were creating
1000s of handles through it.)

@_date: 2004-05-10 22:24:31
@_author: Adam Back 
@_subject: Re: blinding & BF IBE CA assisted credential system (Re: chaum's patent expiry?) 
But if I understand that is only half of the picture.  The recipient's
IBE CA will still be able to decrypt, tho the sender's IBE CA may not
as he does not have ability to compute pseudonym private keys for the
other IBE CA.
If you make it PFS, then that changes to the recipient's IBE CA can
get away with active MITM rather than passive eavesdropping.
An aside is that PKI for Psuedonym's is an interesting question.  The
pseudonym can't exactly go and be certified by someone else as an
introducer without revealing generally identifying things about the
network of trust.  But ignoring this presuming that the identities
were not subject to MITM from day one, and could build up a kind of
WoT despite lack of out-of-band way to check info to base WoT
signatures on.  It would still be interesting to defend the pseudonym
against MITM colluding with IBE CA that at some point after the
pseudonym has transferred keys without insertion of a MITM from.
This problem of addressing the who goes first problem for pseudonymous
communicants appears somewhat related to Public Key Steganography
where there is a similar scenario and threat model.  (Anderson and
Petitcolas"On The Limits of Steganography"
They also cite a "Prisoners' problem" which might be something you
could extend involving a warden who is eavesdropping and prisoners who
will be penalized if he can detect and identify communicants.
My earlier comment:
may not be that useful a distinction as the IBE CA server also gets
your private key, so he doesn't _need_ to generate a certificate
impersonating you as a conventional rogue CA would.
But if we could make the binding from pseudonym to the pseudonym's
non-IBE public key strictly first come first served, so that the IBE
CA's attemt to claim his later released non-IBE public key is the
correct one would be detectable.  Either secure time-stamping,
extending the psuedonym name to include fingerprint as
self-authenticator would allow this.

@_date: 2004-05-10 10:02:51
@_author: Adam Back 
@_subject: blinding & BF IBE CA assisted credential system (Re: chaum's patent expiry?) 
I think you mean so that the CA/IBE server even though he learns
pseudonyms private key, does not learn the linkage between true name
and pseudonym.  (At any time during a show protocol whether the
private key issuing protocol is blinded or not the IBE server can
compute the pseudonyms private key).
Seems like an incremental improvement yes.
Note PFS does not make end-2-end secure against an adversary who can
compute the correspondents private keys, as vulnerable to MITM.  Could
say invulnerable to passive eavesdropper.  However you might have an
opening here for a new security model combining features of Hidden
Credentials with a kind of MITM resistance via anonymity.  What I mean
is HC allows 2 parties to communicate, and they know who they are
communicating with.  The CA colluding MITM however we'll say does not
apriori, so he has to brute force try all psuedonym, attribute
combinations until he gets the right one.  Well still not desirable
security margin, but some extra difficulty for the MITM.

@_date: 2004-05-10 10:02:51
@_author: Adam Back 
@_subject: blinding & BF IBE CA assisted credential system (Re: chaum's patent expiry?) 
I think you mean so that the CA/IBE server even though he learns
pseudonyms private key, does not learn the linkage between true name
and pseudonym.  (At any time during a show protocol whether the
private key issuing protocol is blinded or not the IBE server can
compute the pseudonyms private key).
Seems like an incremental improvement yes.
Note PFS does not make end-2-end secure against an adversary who can
compute the correspondents private keys, as vulnerable to MITM.  Could
say invulnerable to passive eavesdropper.  However you might have an
opening here for a new security model combining features of Hidden
Credentials with a kind of MITM resistance via anonymity.  What I mean
is HC allows 2 parties to communicate, and they know who they are
communicating with.  The CA colluding MITM however we'll say does not
apriori, so he has to brute force try all psuedonym, attribute
combinations until he gets the right one.  Well still not desirable
security margin, but some extra difficulty for the MITM.

@_date: 2004-05-10 21:54:59
@_author: Adam Back 
@_subject: more hiddencredentials comments (Re: Brands' private credentials) 
OK that sounds like it should work.  Another approach that occurs is
you could just take the plaintext, and encrypt it for the other
attributes (which you don't have)?  It's usually not too challenging
to make stuff deterministic and retain security.  Eg. any nonces,
randomizing values can be taken from PRMG seeded with seed also sent
in the msg.  Particularly that is much less constraining on the crypto
system than what Bert-Jaap Koops had to do to get binding crypto to
work with elgamal variant.
The above approach should fix that also right?
dissident computing I think Ross Anderson calls it.  People trying to
operate pseudonymously and perhaps hiding the function of their
servers in a cover service.
Unless it's signifcantly less efficient, I'd say use it all the time.
Yes.  But you could explore public key based without IBE.  You may
have to use IBE as a sub-protocol, but I think ideally want to avoid
the IBE server being able to decrypt stuff.  Sacrificing the IBE
communication pattern wouldn't seem like a big deal.
Hmm well IBE is has a useful side-effect in pseudonymity systems
because it also has the side-effect of saving the privacy problems in
first obtaining the other parties key.  Other way to counteract that
is to always include the psuedonym public key with the pseudonym name
(which works for mailto: style URLs or whatever that are
electronically distributed, but not for offline distributed).
Btw one other positive side-effect of IBE is the server can't
impersonate by issuing another certificate in a pseudonyms name
because there is definitionally only one certificate.
I was thinking particularly if you super-encrypt with the psuedonym's
(standard CA) public key as well as the IBE public key you get the
best of both feature sets.
btw You could probably come up with a way to prevent a standard (non
IBE) CA from issuing multiple certs.  eg. if he does that and someone
puts two certs together they learn CA private key, ala Brands
credential kind of offline double spending protection.
Kind of a cryptographically enforced version of the policy enforced
uniqueness of serial numbers in X.509 certs.  And we change the policy
to one cert per pseudonym (kind of sudden death if you lose the
private key, but hey just don't do that; we'd have no other way to
authenticate you to get a new cert in the same psuedonyms name anyway,
so you may just as well backup your pseudonym private key).

@_date: 2004-05-10 21:54:59
@_author: Adam Back 
@_subject: more hiddencredentials comments (Re: Brands' private credentials) 
OK that sounds like it should work.  Another approach that occurs is
you could just take the plaintext, and encrypt it for the other
attributes (which you don't have)?  It's usually not too challenging
to make stuff deterministic and retain security.  Eg. any nonces,
randomizing values can be taken from PRMG seeded with seed also sent
in the msg.  Particularly that is much less constraining on the crypto
system than what Bert-Jaap Koops had to do to get binding crypto to
work with elgamal variant.
The above approach should fix that also right?
dissident computing I think Ross Anderson calls it.  People trying to
operate pseudonymously and perhaps hiding the function of their
servers in a cover service.
Unless it's signifcantly less efficient, I'd say use it all the time.
Yes.  But you could explore public key based without IBE.  You may
have to use IBE as a sub-protocol, but I think ideally want to avoid
the IBE server being able to decrypt stuff.  Sacrificing the IBE
communication pattern wouldn't seem like a big deal.
Hmm well IBE is has a useful side-effect in pseudonymity systems
because it also has the side-effect of saving the privacy problems in
first obtaining the other parties key.  Other way to counteract that
is to always include the psuedonym public key with the pseudonym name
(which works for mailto: style URLs or whatever that are
electronically distributed, but not for offline distributed).
Btw one other positive side-effect of IBE is the server can't
impersonate by issuing another certificate in a pseudonyms name
because there is definitionally only one certificate.
I was thinking particularly if you super-encrypt with the psuedonym's
(standard CA) public key as well as the IBE public key you get the
best of both feature sets.
btw You could probably come up with a way to prevent a standard (non
IBE) CA from issuing multiple certs.  eg. if he does that and someone
puts two certs together they learn CA private key, ala Brands
credential kind of offline double spending protection.
Kind of a cryptographically enforced version of the policy enforced
uniqueness of serial numbers in X.509 certs.  And we change the policy
to one cert per pseudonym (kind of sudden death if you lose the
private key, but hey just don't do that; we'd have no other way to
authenticate you to get a new cert in the same psuedonyms name anyway,
so you may just as well backup your pseudonym private key).

@_date: 2004-05-25 20:10:21
@_author: Adam Back 
@_subject: Re: Reusable hashcash for spam prevention 
FYI Richard amended the figures in the paper which makes things 10x
more favorable for hashcash in terms of being an ecomonic defense
against spammers.
Richard wrote on asrg:

@_date: 2004-05-18 22:49:11
@_author: Adam Back 
@_subject: Re: 3. Proof-of-work analysis 
Here's a forward of parts of an email I sent to Richard with comments on
his and Ben's paper (sent me a pre-print off-list a couple of weeks ago):
One obvious comment is that the calculations do not take account of
the CAMRAM approach of charging for introductions only.  You mention
this in the final para of conclusions as another possible.
My presumption tho don't have hard stats to measure the effect is that
much of email is to-and-fro between existing correspondents.  So if I
were only to incur the cost of creating a stamp at time of sending to
a new recipient, I could bear a higher cost without running into
However the types of levels of cost envisaged are aesthetically
unpleasing; I'd say 15 seconds is not very noticeable 15 mins is
noticeable and 1.5 hrs is definately noticeable.
Of course your other point that we don't know how spammers will adapt
is valid.  My presumption is that spam would continue apace, the best
you could hope for would be that it is more targetted, that there are
financial incentives in place to make it worth while buying
demographics data.  (After all when you consider the cost of sending
junk paper mail is way higher, printing plus postage, and yet we still
receive plenty of that).
Also as you observe if the cost of spamming goes up, perhaps they'll
just charge more.  We don't know how elastic the demand curve is.
Profitability, success rates etc are one part of it.  There is an
interplay also: if quantity goes down, perhaps the success rate on the
remaining goes up.  Another theory is that a sizeable chunk of spam is
just a ponzi scheme: the person paying does not make money, but a lot
of dummy's keep paying for it anyway.
Another potential problem with proof-of-work on introductions only, is
that if the introduction is fully automated without recipient opt-in,
spammers could also benefit from this amortized cost.  So I would say
something like the sender sent a proof-of-work, and the recipient took
some positive action, like replying, filing otherwise than junk or
such should be the minimum to get white-listed.
On the ebiz web site problem, I think these guys present a problem for
the whole approach.  An ebiz site will want to send lots of mail to
apparent new recipients (no introductions only saving), a popular ebiz
site may need to send lots of mail.
Well it is ebiz so perhaps they just pass the cost on to the consumer
and buy some more servers.
Another possibility is the user has to opt-in by pre-white-listing
them, however the integration to achieve this is currently missing and
would seem a difficult piece of automation to retrofit.
One of the distinguishing characteristics of a spammer is the
imbalance between mail sent and mail received.  Unfortunately I do not
see a convenient way to penalize people who fall into this category.
Also because of network effect concerns my current hashcash deployment
is to use it as a way to reduce false positives, rather than directly
requiring hashcash.  Well over time this could come to the same thing,
but it gives it a gentle start, so we'll see how long it is before the
1st genuine spam with hashcash attached.
CAMRAM's approach is distinct and is literally going straight for the
objective of bouncing mail without some kind of proof (hashcash or
reverse-turing, or short term ability to reply to email

@_date: 2004-07-13 21:32:18
@_author: Adam Back 
@_subject: zks source (Re: Email tapping by ISPs, forwarder addresses, and crypto proxies) 
You could try sending an email to Austin Hill  to see
if he could organize releasing source for remaining freedom related
source that they are not currently using.

@_date: 2004-07-07 20:09:31
@_author: Adam Back 
@_subject: Re: Email tapping by ISPs, forwarder addresses, and crypto proxies 
This is somewhat related to what ZKS did in their version 1 [1,2] mail
They made a transparent local pop proxy (transparent in that it
happened at firewall level, did not have to change your mail client
config).  In this case they would talk to your real pop server,
decrypt the parts (they were reply-block like onions), remove
duplicates (as with mixmaster etc you can send duplicates via separate
remailers to improve reliability).  So the transparent proxy would
leave alone your normal mail that you received in the pop box and
remove duplicates only from the reply-block delivered pseudonymous
Actually they implemented the reply-block from scratch, it always
seemed to me it would have been less development work to use mixmaster
(it was implemented before I started).  The ZKS reply block did not
even use chunking (ala mixmaster) so traffic analysis would have been
trivial as the message size would show through.
At least that's what I recall, no chunking.  However I am finding the
security issues paper [1] says otherwise.  The 1.0 architecture
document [2] is ambiguous, there is no mention of chunking.
(I've sent mail to one of the original developers to check I have it
It was also unreliable because it did not use SMTP, it used its own
transport AMTP and its own retry-semantics on nodes called
MAIPs. (Mail AIPs, an AIP is an "Anonymous Internet Proxy").
Then we implemented a replacement version 2 mail system that I
designed.  The design is much simpler.  With freedom anonymous
networking you had anyway a anonymous interactive TCP feature.  So we
just ran a standard pop box for your nym.  Mail would be delivered to
it directly (no reply block) for internet senders.  Freedom senders
would send via anonymous IP again to get sender anonymity.  Used qmail
as the mail system.
Unfortunately they closed down the freedom network pretty soon after
psuedonymous mail 2.0 [3] was implemented.
There is an interesting trade-off here.  The interactive
communications are perhaps more vulnerable to real-time powerful
adversary traffic analysis than mixmaster style mixed chunked
delivery.  However they are less vunerable to subpoena because they
are forward-secret on a relativey short time-frame.  (1/2 hr if I
recall; however more recent designs such as chainsaw internal
prototype, and cebolla [4] by ex-ZKSer Zach Brown change keys down to
second level by using a mix of backward-security based on symmetric
key hashing (and deleting previous key) and forward security using DH.)
It would be nice to get both types of anonymity, but I suspect for
most typical users the discovery / subpeona route is the major danger,
and if that is thwarted it is unlikely that their activities would
warrant the effort of real time analysis.  Well we have carnivore now,
so they could potentially do real-time traffic analysis more routinely
if they were to distribute enough collaborating analysis carnivore
[1] [2] [3] [4]

@_date: 2004-08-18 20:33:12
@_author: Adam Back 
@_subject: hash attacks and hashcash (SHA1 partial preimage of 0^160) 
(This discussion from hashcash list is Cc'd to cryptography and
Hashcash uses SHA1 and computes a partial pre-image of the all 0bit
string (0^160).
Following is a discussion of what the recent results from Joux, Wang
et al, and Biham et al on SHA0, MD5, SHA1 etc might imply for hashcash
SHA1 (and for hypothetical hashcash SHA0, MD5 etc by way of seeing
what it will mean if SHA1 eventually suffers similar fate to SHA0).
(All as far as I understand so far).
Hashcash stresses the SHA1 function in a different direction than
sigantures and MACs -- in assuming partial pre-images are hard (ie an
k-bit partial pre-image should take about 2^k operations).  (Partial
2nd pre-images are also "interesting" against hashcash -- see below).
(As a security argument if partial pre-images say up to m To be clear:

@_date: 2004-08-02 09:36:26
@_author: Adam Back 
@_subject: you can't argue with economics (Re: On how the NSA can be generations ahead) 
But most cryptanalysis types of things are economic defenses.  (ie you
can spend $lots you can break; or you don't have enough $ to build
because the $ at current tech is an astronomical multiple of the US
national debt).
So if the NSA are being stupid, and uneconomical with the black budget
(and it's not that hard for large organizations even with competition
to be stupid), then they will be even less likely to break things that
they could break than if they outsourced the whole thing.
Probably to their advantage, I presume they do in fact outsource many
things and of course buy large expensive bits of machinery and
components, as anyone must do.
So anyway, doing uneconomical things with the black budge they would
lessen their chance of breaking various things, not increase it.
Now the sheer scale of the black budget allows some things, but no
doubt their best strategy will be to do economical things wrt their
objectives and priorities and put as much as they can out for
commercial tender, and/or try to create internal competition or

@_date: 2004-08-04 22:16:14
@_author: Adam Back 
@_subject: planet sized processors (Re: On what the NSA does with its tech) 
The planet sized processor stuff reminds me of Charlie Stross' sci-fi
short story "Scratch Monkey" which features nanotech, planet sized
processors which colonize space and build more planet-sized
processors.  The application is upload, real-time memory backup, and
afterlife in DreamTime (distributed simulation environment), and an
option of reincarnation.

@_date: 2004-08-12 18:41:40
@_author: Adam Back 
@_subject: maybe he would cash himself in? (Re: A Billion for Bin Laden) 
Maybe Bin Laden would turn himself in in return for a billion $ for
his cause (through a middle-man of course).
Seem to remember that Bin Laden was relatively wealthy himself (>100
M$?), but you'd have to balance these rewards to not be too
excessively much more than net worth of the individual.  As a rational
adversary would include in his game plan swapping himself for the
money for the cause.
Especially if it could be arranged in a way which tends to cast Bin
Laden in the martyr role him and encourage the hydra effect where it
galvanizes leutenants to step in.
Bin Laden would have to balance also with how valueable he thought his
leader ship was.
Of course the lieutenants themselves might do the calculation and
figure they would be closer to their goals after cashing in Bin Laden.

@_date: 2004-08-21 05:39:12
@_author: Adam Back 
@_subject: Re: RPOW - Reusable Proofs of Work 
It's like an online ecash system.  Each recipient sends the RPOW back
to the mint that issued it to ask if it has been double spent before
accepting it as valid.  If it's valid (not double spent) the RPOW
server sends back a new RPOW for the receiving server to reuse.
Very like Chaum's online ecash protocol, but with no blinding (for
patent reasons) and using hashcash as way to "buy" coins.  The other
wrinkle is he can prove the mint can not issue coins without
exchanging them for hashcash or previous issued coins (up to the
limits of the effectiveness of the IBM tamper resistant processor
card, and of course up to the limits of your trust in IBM not to sign
"hardware code signing keys" that are not generated on board one of
these cards).  This is the same as the "remote attestation" feature
used in "Trustworthy" Computing for opposite effect -- restricting
what users can do with their computers; Hal is instead using this to
have a verifiable server where the user can effectively audit and
check what code it is running.

@_date: 2004-10-05 21:13:47
@_author: Adam Back 
@_subject: Brands credential book online (pdf) 
For people interested in ecash / credential tech: Stefan Brands book
on his credential / ecash technology is now downloadable in pdf format
from credentica's web site:
(previously it was only available in hardcopy, and only parts of the
content was described in academic papers).
also the credentica web site has gone live, lots of content. (credentica is Stefan's company around digital credentials).

@_date: 2004-10-03 19:41:25
@_author: Adam Back 
@_subject: Re: Foreign Travelers Face Fingerprints and Jet Lag 
I don't know if my info is still current (and I did not read the
article), but the last time I went to the US (early this year) my H1B
was no longer in effect (I quit microsoft last year, and H1B visas are
tied to employer), and I did not get fingerprinted.
However they had a camera and fingerprinting equipment, and notice
saying that if you _did_ have H1B and other such temporary US visa
documents you would be photographed and fingerprinted.

@_date: 2004-12-16 10:50:22
@_author: Adam Back 
@_subject: pgp "global directory" bugged instructions 
So PGP are now running a pgp key server which attempts to consilidate
the inforamtion from the existing key servers, but screen it by
ability to receive email at the address.
So they send you an email with a link in it and you go there and it
displays your key userid, keyid, fingerprint and email address.
Then it says:
So here's the problem: it does not mention anything about checking
that this is your fingerprint.  If it's not your fingerprint but it is
your email address you could end up DoSing yourself, or at least
perpetuating a imposter key into the new supposedly email validated
keyserver db.
(For example on some key servers there are keys with my name and email
that are nothing to do with me -- they are pure forgeries).
Suggest they add something to say in red letters check the fingerprint
AND keyid matches your key.

@_date: 2004-12-16 10:50:22
@_author: Adam Back 
@_subject: pgp "global directory" bugged instructions 
So PGP are now running a pgp key server which attempts to consilidate
the inforamtion from the existing key servers, but screen it by
ability to receive email at the address.
So they send you an email with a link in it and you go there and it
displays your key userid, keyid, fingerprint and email address.
Then it says:
So here's the problem: it does not mention anything about checking
that this is your fingerprint.  If it's not your fingerprint but it is
your email address you could end up DoSing yourself, or at least
perpetuating a imposter key into the new supposedly email validated
keyserver db.
(For example on some key servers there are keys with my name and email
that are nothing to do with me -- they are pure forgeries).
Suggest they add something to say in red letters check the fingerprint
AND keyid matches your key.

@_date: 2005-01-07 20:34:32
@_author: Adam Back 
@_subject: Re: Hamachi "mediated" peer-to-peer sounds interesting (fwd from meltsner@gmail.com) 
Well if they really relayed traffic between peers on their back end
server their pipe would be saturated.  (Think kazaa or bit-torrent
over hamachi).
I hope they actually use the server just for mediation, and send the
traffic direct between peers.
Unfortunately the documentation is rather light so it's difficult to
tell what it does in this regard.
I've cc'd Alex Pankratov who is the author (I presume).
However maybe this beta version is not complete in that regard.  Some
other things such as the server mediated key exchange are obviously
not shipable grade (server knows all symmetric keys!)

@_date: 2011-06-13 14:54:04
@_author: Adam Back 
@_subject: [cryptography] Digital cash in the news... 
Bitcoin does not have to end with the pyramid scheme outcome - where it
stalls and all those still holding any lose - so long as there remain people
willing to exchange goods for bitcoin after the dust has settled.
Anyway my point is even if the deployment phase is a wild ride, with some
winners and some late losers who bought in above the final stable value, so
long as a stable value results at the end, I dont see that as a big problem.
Its not like we havent had bubbles and instability in various phases of any
other forms of money or assets.
If you take out the speculation, currently with people minting coins until
they get to 21 million coins that would be inflation (limited inflation due
to the mining cost); but also that more people are joining is deflationary
(less coins per person).  Then there is supply and demand - supply from
minting (so long as the sell price is above minting cost), supply from
people cashing out, and demand from people buying in.  Cashing out and
buying in maybe for trading or speculation.
Once the 21 million coins are created bitcoin would remain deflationary
during the next phase as until the user base grows to saturation.  Once
bitcoin grows to saturation, the remaining deflation would be limited by the
underlying population and economic growth.  That might be workable rate of

@_date: 2011-06-13 07:54:27
@_author: Adam Back 
@_subject: [cryptography] Digital cash in the news... 
Bitcoin is not a pyramid scheme, and doesnt have to have the collapse and
late joiner losers.  If bitcoin does not lose favor - ie the user base grows
and then maintains size of user base in the long term, then no one loses.
I think in the current phase the deflation (currency increasing in value)
helps increase interest and number of users.
Say that in the next phase bitcoin stops rapid expansion and reaches some
stable number of users, the deflationary period stops, and the remaining
users use it for transactions only (not speculation).  I dont see the losers
in that scenario.

@_date: 2011-06-12 12:15:50
@_author: Adam Back 
@_subject: [cryptography] attacks against bitcoin 
I was thinking a DoS might be a problem.  If you could prevent the p2p
network broadcasting or receiving broadcasts, maybe you could be the only
person able to proceed with minting.  If you could keep that up for a while
you could reduce the difficulty and create bitcoins with lower cost.  A full
enough DoS maybe difficult to do as the network is p2p.
Also maybe if you could temporarily come in with significantly more compute
power than the rest of the network - eg rent ec2's entire gpcpu farm for a
little while, or use of a huge bot farm, you could undo transactions.  If
those transactions were you selling bitcoins, you could then sell them
again.  eg buy, then sell $100k coins (minus the spread/fluctuation), rent
$50k worth of compute for a while; sell the $100k of coins again ...  profit

@_date: 2011-12-22 08:40:37
@_author: Adam Back 
@_subject: Re: [cryptography] How are expired code-signing certs revoked? 
Stefan Brands credentials [1] have an anti-lending feature where you have to
know all of the private components in order to make a signature with it.
My proposal related to what you said was to put a high value ecash coin as
one of the private components.  Now they have a direct financial incentive -
if they get hacked and their private keys stolen they lose $1m untraceably.
Now thats quite reassuring - and encapsulates a smart contract where they
get an automatic fine, or good behavior bond.  I think you could put a
bitcoin in there instead of a high value Brands based ecash coin.  Then you
could even tell that it wasnt collected by looking in the spend list.
[1]  a library implementing Brands
credentials - it has pointers to the uprove spec, Brands thesis in pdf form
cryptography mailing list

@_date: 2012-03-30 07:45:10
@_author: Adam Back 
@_subject: Re: [cryptography] Key escrow 2012 
As I recall people were calling the PGP ADK feature corporate access to
keys, which the worry was, was only policy + config away from government
access to keys.
I guess the sentiment still stands, and with some justification, people are
still worried about law enforcement access mechanisms for internet &
telecoms equipment and protocols being used in places like Syria, Iran etc,
which is a quite similar scenario.
And as we all know adding key recovery and "TTPs" etc is a risk, cf
"The Risks of Key Recovery, Key Escrow, and Trusted Third-Party Encryption"
by Abelson, Anderson, Bellovin, Benaloh, Blaze, Diffie, Gilmore, Neumann,
Rivest, Schiller & Schneier.
Not sure that we lost the crypto wars.  US companies export full strength
crypto these days, and neither the US nor most other western counties have
mandatory GAK.  Seems like a win to me :)
cryptography mailing list

@_date: 2012-05-11 23:39:48
@_author: Adam Back 
@_subject: Re: [cryptography] Bitcoin-mining Botnets observed in the wild? (was: Re: Bitcoin in endgame 
An never mind... the 555MH and 12.4TH its 23,000 AMD 7970 GPU equivalents,
that back in the land of plausible.
Or 158 petflops if you accept bitcoinwatch.com's sha256 to "flop" conversion
ratio.  A human brain by comparison is generally considered to be about
10-100x less than the current bitcoin network (10^15 - 10^16 ops/sec).
So maybe the bitcoin network has just enough power to simulate a single
human brain.  Except a human brain consumes 25watts and even with 23,000
7970s the bitcoin network must be using more than 5 MW.
Moore's law still has a lot of work to do to catchup with biologial
computers in efficiency and horsepower.
cryptography mailing list

@_date: 2013-07-22 07:41:14
@_author: Adam Back 
@_subject: Re: [cryptography] a Cypherpunks comeback 
Could you please get another domain name, that name is just ridiculous.
It might tickle your humour but I guarantee it does not 99% of potential
Unless your hidden objective is to drive away potential subscribers.
cryptography mailing list

@_date: 2013-07-26 14:10:57
@_author: Adam Back 
@_subject: Re: Feds put heat on Web firms for master encryption keys 
I suspect the companies cleverly saying they do not give keys are giving
account access or emails directly, and just engaging in misleading PR spin. There's a lot of it been going on lately and people are seemingly niave
about reading PR spin.  (Push vs pull access to data under blanket FISA blah
blah, right).
Basically the defacto behavior of justice system supoenas for ISPs is that
they'll try to get anything you have, and they'll even try to get things
they are legally prohibited from getting.  So your best bet is to not have
anything useful to give.
Like "zero-knowledge" (spider oak, mozy online backup) meaning end2end
secure so only the user has the keys and the ISP holds cyphertext.
I do think asking for the server keys is too far, probably contravenes
multiple laws, and is ridiculously intrusive - giving access to everything.
Forward secrecy is a good step, and its confusing why not everyone is using
it.  Google apparently is.  Others not so much.  People have been talking
about that since early 1990s.  Still not prefering or enforcing forward
secret ciphersuite, seriously?
Probably time to deprecate HTTP (in favor of HTTPS) and deprecate
non-forward-secret ciphersuites, to a should-not or whathave you
(implementations might implement but must warn).
ps Pretty cool to see cypherpunks list back in action.  And first post I
read was John Youngs longer dense poetic post.  Just like old times :)

@_date: 2013-08-31 08:05:07
@_author: Adam Back 
@_subject: why not disable external mail, keep intenal mail (Re: Who bought off Zimmermann?) 
More precisely its the exposed meta-data in the SMTP.  But why would you use
meta-data rich transport for silent circle internal-mail?  (Internal-mail I
mean silent circle user to silent circle user vs external mail being smtp
mail to silent circle user or silent circle user to smtp mail user).
I said it before, but again: why not cancel external mail, and leave the
internal mail working - silent circle obviously have the tech for that
because they have SMS equivalent in-mail.  Good for you: users who want to
continue to communicate will encourage the people they are communicating
with to also pay for subscriptions.  Maybe you could allow people to give
each other gifts of 1month membership, which you hope they extend
themselves; or some referal system with a bonus free month to the existing
user etc.
Now there might be some software legacy, but that seems straight forward
enough.  The crypto gap is purely the in and out mail.  (Other than forced
software changes, but others have discussed how to combat that issue, and
some claim legal advice is that its harder for the mil-int community to
legally force companies to change their software.  (Hushmail saga not

@_date: 2013-08-23 00:25:43
@_author: Adam Back 
@_subject: Re: [Doctrinezero] HTTPS 
Are you sure about that?  (*.com and *. being valid).  I thought the MITM
boxes were loaded with a sub-CA cert - a cert with a bit set authorizing it
to generate certs for sites, some of the smaller CAs are not directly in the
trusted browser databases, and have bought sub-CA certs from CAs that are.
Then what actually happens in the MITM box is to load a fake cert for a
domain (issued by its sub-CA cert), or to generate fake certs on the fly for
any targetted domains (or all domains) again issued by its sub-CA cert.  So
I thought the CA that got warned by mozilla had issued a sub-CA cert for
MITM purposes.
(I really dont think a browser vendor would accept *.com nor especially *.
as a valid site cert wildcard.  It does get fiddly because you also want
*.co.uk etc to be invalid but they have some built in tables of such things
to differentiate a TLD from a domain).

@_date: 2013-08-21 05:52:25
@_author: Adam Back 
@_subject: Re: no encryption even worse? (Re: Groklaw shuts down) 
Yes but my point was they didnt have to throw out the baby with the
bathwater; silent circles email I think was basically two products combined:
1. end2end secure, store-and-forward encryption between silent circle users;
2. server-side encryption of opportunistically SSL encrypted (potentially
unencrypted) incoming emails + presumably unencrypted outgoing emails.
Why not keep 1?  They obviously have the technology for it because they have
retained encrypted SMS-like functionality which is the same key management
and information flow.
Not forgetting there is a 3rd "product" which is the defacto which is normal
3. opportunistically encrypted (SSL) email
(as well as SMIME (dont trust due to CA malfeasance) or self-managed PGP/GPG
which for some reason people find difficult).
and users who lose 1 & 2 due to the no-notice product end-of-life will
probably just switch to 3 as an alternative to stopping communicating.  Even catching a flight with a USB drive apparently is risky via UK re the
curiously named David Miranda (Miranda rights eh) seems they demanded
decryption keys.  Seems like people who are couriering data ought to encrypt
it with the recipients public key before travel.

@_date: 2013-08-21 00:46:27
@_author: Adam Back 
@_subject: Re: Google to encrypt cloud storage 
Well I think its fair to denigrate it as obfuscation not encryption if the
key lives on the same machine as the ciphertext.  At best it makes it less
risky to dispose of dodgy disks - now and then such things turn up on ebay
with client data.  At least if you encrypt it properly, and do NOT put the
key on the disk, then you can safely toss them in a dumpster, not physically
destroy them etc.

@_date: 2013-08-08 00:20:32
@_author: Adam Back 
@_subject: Re: [cryptography] a Cypherpunks comeback 
OK let me put it this way, given each person only has so many hours in the
day, or so much energy and resources for politically-fighting or
write-code-fighting things which would you rather fight: defense of spurious
attention arising from a stupid domain name, or I dunno operating a
remailer, a tor exit node, a hidden tor server.
Apparently operating a hidden tor server as a service is pretty high risk as
the guy in Ireland is finding.  You can see in that they are trying to pin
the content on him, as if he authored it, whereas I am presuming he is no
more responsible for the content than a hosting company or youtube.  If he
was prominently using al-qaeda.net you can be sure they'd have spun that
into the story.
There is some history also - recall Jim Bell, he got in some fight over
taxes or something stupid, that took him out of the picture for a while.  I
wasnt really sold on his assassination politics idea anyway (gotta be a way
to vote someone out of office without assasinating them!), but at least it
was a political discussion which he thought had some merit vs a losers game
of tax protestation ending in jail time, anyone can see thats never going to
work out.
I wouldnt be so sure that using stupid domain names is entirely safe in the
US, europe etc.  IMO the US is past its peak in terms of a place of freedom
and others have overtaken it.  It doesnt seem likely the US will recover its
ranking, seems to be falling year on year.  Probably China itself will
overtake US economically, politically and for freedoms within 50-100 years. Not sure how you recover freedoms from a panopticon state with a one dollar
one vote and a 100 billion dollar+ military-spy-industrial complex and a
significantly biased politicial- judicial system.  If you watch RT which
airs a lot of the snowden thing, the stuff the USG is saying about snowden
is just ludicrous.  Pressuring european countries to deny overflight to a
presidents plane is an alarming breach of international law and shows how
far the US rogue state influence goes in seemingly other countries willing
to go along with its actions.
Also why would you even want to do it?  You care about crypto deployment, so
I dont see the logic in picking the most stupid, unrelated and controversial
domain name you can think of hitting as many peoples distaste as you can and
use that?  wtf back at you :)  cypherpunks  what next.
I guess we should go write some code!

@_date: 2013-08-06 23:16:37
@_author: Adam Back 
@_subject: Re: [cryptography] a Cypherpunks comeback 
Cypherpunks and privacy tech had enough on their plate post 9-11 without
inexplicably using an Al-Qaeda related domain name presumably chosen by
someone's amusement at being controversial.  Its not related to the list,
and it just invites spurious trouble.  Why not the ownder of the domain use
it as his personal address.  Heck he can use the user name osama@ the domain
if he wants.  I have to say I see no upside whatsoever to using that domain
name for a mailing list on any topic.
You only have to look at various court cases to see how everything gets
heavily misinterpreted and nothing spun into something to pause and see why
using such a domain name is a "bad idea" tm.
I appreciate the "fearless crypto coder" mentality, but focus on the crypto,
not inviting stupid fights with authoritarian systems over non-topics eh.

@_date: 2013-08-07 09:05:42
@_author: Adam Back 
@_subject: Re: [cryptography] Paillier Crypto for homomorphic computation 
No recall that the simplified paillier is c=g^m*r^n mod n^2 so
multiplication gives addition:
g^a*r1^n * g^b*r2^n = g^{a+b}*(r1*r2)^n
ie multiplication of ciphertexts gives you homomorphic addition of
but g^a*r1^n ^ (g^b*r2^n) != g^{a*b}*r3^n
(the core part is g^a ^ (g^b) = g^{a*g^b} != g^{a*b}).
what does work is as I said raising to the power of a constant eg k:
(g^m*r^n)^k = g^{k*m}*(r^k)^n so you can still decrypt and the operation is multiply by constant k).

@_date: 2013-08-06 23:09:07
@_author: Adam Back 
@_subject: Re: [cryptography] Paillier Crypto for homomorphic computation 
I dont get it.  Paillier is additively homomorphic only.  (And obviously by
implication multiplyable by non-encrypted constants.)
RSA is multiplicatively homomorphic.  And Elgamal additive.
Why is paillier proposed as "might scale homomorphic" the interesting
property is dual homomorphic crypto which Gentry and variants provide (but
at impractical computational and large space overhead huge).  Dual or fully
homomorphic is the interesting property because then you can do arbitrary
computations (using multiplication as single-bit AND and addition as
single-bit OR and building a CPU from gates - still expensive even if the
base algorithm was as efficient as Paillier/RSA/Elgamal but interesting).
Also why would they send the "encrypted numbers" to two peers and have them
do the encrypted computation?  The whole point is its zero-trust secure from
the point of view of the client - client encrypts, server does computations
on encrypted values, sends encrypted result back to client, client decrypts
- and you dont need to trust the server.  No need for threshold crypto,
having multiple peers do some kind of multi-party computation etc.

@_date: 2013-08-10 01:19:59
@_author: Adam Back 
@_subject: Re: [cryptopolitics] Silent Circle and Secure Email 
(Thanks for the link.)  It says hushmail had a simplified web-only version (no
java applet) and that the disclosure of client emails did not involve
pressured code changes (at least code shipped to clients), rather that as a
natural consequence of the way passwords would be processed on the server
side and decryption happened on the server side so hushmail had the
passwords, private keys, and decrypted plaintexts at leas in memory to hand
over on request.

@_date: 2013-08-10 00:56:15
@_author: Adam Back 
@_subject: Re: [cryptopolitics] Silent Circle and Secure Email 
Reading what Jon Callas wrote he said silent circle interoperated with
unencrypted SMTP email (unencrypted other than SSL over the transport), and
they used some bump in the wire PGP thingy that encrypts incoming email with
the silent circle users public key, and presumably sends out cleartext
possibly SSL SMTP where available, for non silent-circle recipients. Clearly therefore anyone tampering with the SSL (and often those mail
transport systems are not that smart about SSL as there is no security UI)
or just getting the NSA camel's nose inside the silent circle SSL
termination point prior to encrytion.
As they didnt think that would end well they decided to close it down. Alternatively they might have considered disabling the mail-in and mail-out
Its less clear what lavabit were talking about.  Perhaps something similar
in terms of an SMTP interoperability encryption gap, or alternatively about
being pressured to modify code (which people seem to assume, but I didnt see
explicitly stated).
There were some hushmail rumors about code modification some years back -
does anyone know what actually at hushmail?

@_date: 2013-08-11 10:13:28
@_author: Adam Back 
@_subject: NSLs, gag-orders, code-changes, coerced backdoors - any tech response? (Re: Lavabit and End-point Se 
About physical access - there is one non-physical solution to this - hide
the location of the server behind tor, proxies etc.  Seems to work
remarkably well for pirate bay.  I cant imagine its that big a secret as to
where the packets are routed from the current proxy to the current physical
host, but seemingly NSA type resources have not been brought to bear against
it.  Step one for the attacker is to find it.  Maybe physical tamper
detection can wipe the RAM, cold reboot as the cage unlocked, or box is
opened, and immediately switch to the back up server in a different tor
hidden physical location.
One thing that occurs to me is that aside from the laundering of NSA tip
offs to FBI etc with faked plausible trails, that have been reported on
lately; there was an aspect that they would be hesitant to reveal what they
could tap, correlate etc, or under what circumstances they would abuse
national security (military) resources for various levels of criminal
activity (major, organized to minor, petty, or political misuse).  But the very fact that Snowden did the world a favour in disclosing the
illegal activities of the NSA and global partners, now people know what they
are doing or can better imagine, and not discount as paranoia, consequently
maybe once the dust has settled they will feel freer to feed ever more petty
or political or corporate espionage related information.  After all they'd
no longer be risking knowledge of information capability, or political
willingness.  Everyone pretty much figures they're in it up to their elbows
with corporate espionage (boeing vs airbus wiretaps), minor crimes with
fabricated evidence trails (maybe they wont bother fabricating them even in
future) and perhaps the political stuff though that is really evil and
anti-democractic (eg tea-party member IRS audits, blackmail etc).
It seems to me companies need to delegate code review and signing to a civil
society charitable organization with smart use of jurisdictions.  eg Germany
(chaos computer club code signing silent circle code?), Switzerland,
Iceland, or psuedonymous but high reputation individuals or groups.  Or
privacy groups which may have a more clear disinterest and immunity from
financial blackmail (like USG will cancel contracts if ISP, internet
service, or softwre company doesnt fold to NSL or other extra-legal
threats).  Or maybe EFF, privacy international etc.  Via their lawyers they
could retain a highly competent and pseudonymous team of technical reviews
and code signing that companies that care to demonstrate their alignment to
providing end to end secure services to their users would if it became
popular given an explanation of why they were not protected by independent
review based code signatures.

@_date: 2013-08-21 00:51:02
@_author: Adam Back 
@_subject: no encryption even worse? (Re: Groklaw shuts down) 
I was thinking something like that about the silent circle shutdown.  It
seems to me their problem case was the mail in (they would be encrypting
that to the user PGP key or equivalent, after sender optional use of SSL to
deliver it to them).  So would not a more sensible change be to disable
mail in?  So then only silent circle users could encrypt messages to each
other.  Even that would add pressure to other users to also get a silent
circle account and so be a business advantage.
Puzzlingly spun "to protect our users privacy we removed their encryption
feature" - so they'll probably send it plaintext instead, great.

@_date: 2013-09-30 19:41:00
@_author: Adam Back 
@_subject: Re: NYT: N.S.A. Gathers Data on Social Connections of U.S. Citizens 
This bit is interesting so the NSA digitally tails all americans without
suspicion and if it turns up "evidence of a crime" (not related to
terrorism, just crime in general mind) it can disclose that law enforcement.
So what are we talking about here - jay-walking?  Buying CFDs?  (Widely sold
financial instruments in other countries).  Online gambling?  (Apparently
some types of online gambling are not legal in the US).  And so forth.
Also I'm not sure how you find evidence of a crime from a social graph
analysis without a content wire tap, but maybe a location and time would do
as suspicion or some level of evidence it if there is some crime that was
known to have occurred at a certain place and time range and they have the
phone calling and GPS records.
Anyway it yet again violates the US governments claims: that their citizens
dont have anything to worry about, its just targetted at terrorists.  Not so
- they digitally tail everyone and if they find anything suspicious period
they forward it to law enforcement.
Are they also forwarding such tips to other spy-partner countries?

@_date: 2013-09-30 10:27:43
@_author: Adam Back 
@_subject: three crypto lists - why and which 
I am not sure if everyone is aware that there is also an unmoderated crypto
list, because I see old familiar names posting on the moderated crypto list
that I do not see posting on the unmoderated list.  The unmoderated list has
been running continuously (new posts in every day with no gaps) since mar
2010, with an interesting relatively low noise, and not firehose volume.
The actual reason for the creation of that list was Perry's list went
through a hiatus when Perry stopped approving/forward posts eg
originally Nov 2009 - Mar 2010 (I presume the mar 2010 restart was motivated
by the creation of randombit list starting in the same month) but more
recently sep 2010 to may 2013 gap (minus traffic in aug 2011).
I have no desire to pry into Perry's personal circumstances as to why this
huge gap happened, and he should be thanked for the significant moderation
effort he has put into create this low noise environment, but despite that
it is bad for cryptography if people's means of technical interaction
spuriously stops.  Perry mentioned recently that he has now backup
moderators, OK so good.
There is now also the cypherpunks list which has picked up, and covers a
wider mix of topics, censorship resistant technology ideas, forays into
ideology etc.  Moderation is even lower than randombit but no spam, noise
slightly higher but quite reasonable so far.  And there is now a domain name
that is not al-quaeda.net (seriously?  is that even funny?): cpunks.org.  At least I enjoy it and see some familiar names posting last seen decade+
Anyway my reason for posting was threefold: a) make people aware of
randombit crypto list, b) rebooted cypherpunks list (*), but c) about how to
use randombit (unmoderated) and metzdowd.  For my tastes sometimes Perry will cut off a discussion that I thought was
just warming up because I wanted to get into the detail, so I tend more
prefer the unmoderated list.  But its kind of a weird situaton because there
are people I want views and comments from who are on the metzdowd list who
as far as I know are not on the crypto list, and there's no convenient way
to migrate a conversation other than everyone subscribing to both.  Cc to
both perhaps works somewhat, I do that sometimes though as a general
principle it can be annoying when people Cc to too many lists.
Anyway thanks for your attention, back to the unmoderated (or moderated)

@_date: 2013-09-22 08:32:31
@_author: Adam Back 
@_subject: motivation & organizational criminality (Re: Jim Bell's fiber-optic patent application.) 
Hmm maybe the chances are not so slim that the NSA people, and/or their
families are indeed starting to question their being on the right side of
history (eg that building STASI 2.0 is not a fantastic idea for the future
of society and democracy over this century).
The NSA seems to be worried anyway (skip to the leaked letter, to literally
'employees and family of NSA', at the bottom of this article):
Maybe public peer pressure and a call to ethics can achieve something after

@_date: 2013-09-15 11:47:13
@_author: Adam Back 
@_subject: Re: [Cryptography] prism proof email, namespaces, and anonymity 
Well you could certainly allow people to opt-in to receiving anonymous
email, send them a notification mail saying an anonymous email is waiting
for them (and whatever warning that it could be a nastygram, as easily as
the next thing).
People have to bear in mind that email itself is not authenticated - SMTP
forgeries still work - but there are still a large number of newbies some of
whom have sufficiently thin skin to go ballistic when they realize they
received something anonymous and not internalized the implication of digital
At ZKS we had a pseudonymous email system.  Users had to pay for nyms (a
pack of 5 paid per year) so they wouldnt throw them away on nuisance pranks
too lightly.  They could be blocked if credible abuse complaint were
Another design permutation I was thinking could be rather interesting is
unobservable mail.  That is to say the participants know who they are
talking to (signed, non-pseudonymous) but passive observers do not.  It
seems to me that in that circumstance you have more design leverage to
increase the security margin using PIR like tricks than you can with
pseudonymous/anonymous - if the "contract" is that the system remains very
secure so long as both parties to a communication channel want it to remain
that way.
There were also a few protocols for to facilitate anonymous abuse resistant
emails - user gets some kind of anonymously refreshable egress capability
token.  If they abuse they are not identified but lose the capability.  eg
Finally there can be different types of costs for nyms and posts - creating
nyms or individual posts can cost real money (hard to retain pseudonymity),
bitcoin, or hashcash, as well lost reputation if a used nym is canceled.
The cryptography mailing list

@_date: 2013-09-15 11:47:13
@_author: Adam Back 
@_subject: Re: [cryptography] [Cryptography] prism proof email, namespaces, and anonymity 
Well you could certainly allow people to opt-in to receiving anonymous
email, send them a notification mail saying an anonymous email is waiting
for them (and whatever warning that it could be a nastygram, as easily as
the next thing).
People have to bear in mind that email itself is not authenticated - SMTP
forgeries still work - but there are still a large number of newbies some of
whom have sufficiently thin skin to go ballistic when they realize they
received something anonymous and not internalized the implication of digital
At ZKS we had a pseudonymous email system.  Users had to pay for nyms (a
pack of 5 paid per year) so they wouldnt throw them away on nuisance pranks
too lightly.  They could be blocked if credible abuse complaint were
Another design permutation I was thinking could be rather interesting is
unobservable mail.  That is to say the participants know who they are
talking to (signed, non-pseudonymous) but passive observers do not.  It
seems to me that in that circumstance you have more design leverage to
increase the security margin using PIR like tricks than you can with
pseudonymous/anonymous - if the "contract" is that the system remains very
secure so long as both parties to a communication channel want it to remain
that way.
There were also a few protocols for to facilitate anonymous abuse resistant
emails - user gets some kind of anonymously refreshable egress capability
token.  If they abuse they are not identified but lose the capability.  eg
Finally there can be different types of costs for nyms and posts - creating
nyms or individual posts can cost real money (hard to retain pseudonymity),
bitcoin, or hashcash, as well lost reputation if a used nym is canceled.
cryptography mailing list

@_date: 2013-09-14 18:38:37
@_author: Adam Back 
@_subject: Re: [Cryptography] RSA equivalent key length/strength 
100-11 = 89 bits.  Bitcoin is pushing 75 bits/year right
now with GPUs and 65nm ASICs (not sure what balance).  Does that place ~2000
bit modulus around the safety margin of 56-bit DES when that was being
argued about (the previous generation NSA key-strength sabotage)?
Anyone have some projections for the cost of a TWIRL to crack 2048 bit RSA? Projecting 2048 out to a 2030 doesnt seem like a hugely conservative
estimate.  Bear in mind NSA would probably be willing to drop $1b one-off to
be able to crack public key crypto for the next decade.  There have been
cost and performance, power, density improvements since TWIRL was proposed. Maybe the single largest employer of mathematicians can squeeze a few
incremetal optimizations of the TWIRL algorithm or implementation strategy.
Tin foil or not: maybe its time for 3072 RSA/DH and 384/512 ECC?
The cryptography mailing list

@_date: 2013-09-02 18:18:07
@_author: Adam Back 
@_subject: Re: Help with JPEG Stego app? 
PGP stealth by Henry Hastur has the stego support for pgp2 formats and RSA. (Aside from stripping boiler plate Hal Finney had observed that you have to
make sure the RSA encryption part doesnt narrow down which key it could be
addressed to.  (A message m > user A's n public value could not be addressed
to A (as m is computed mod n, it is always < n)).
Its C code, quite old and not really maintained but perhaps you could use it
for comparison or ideas.

@_date: 2013-09-21 09:18:19
@_author: Adam Back 
@_subject: Re: Jim Bell's fiber-optic patent application. 
I'd say one problem is cultural amongst the security cleared and ex-TLA
people with security people or current double agent security people on the
telco payroll.
Until they internalize that they are part of a dangerous to democracy and
civilization STASI 2.0 system, the problem will continue, because these kind
of gag order things are going to be handled by security cleared people only. In that way they can probably legally hide it from the CEO and the rest of
the company, by gagging the intercept request handling people.  And they can
surely require, if they do not already, that the intercept handling people
be security cleared.  And in that environment its got to be easy on
$250m/year black budget to stack the intercept handling departments in the
important (large) telco's with not just security cleared, but true-believer
ex-TLA types, or simply double agents.  They dont hve to pay the full salary
just an off the books loyalty bonus, as the telco is paying for its own
So I think the main hope which is probably fairly slim, is that society
views shift to make even those ex-TLA people start to question whether they
are on the right side of history to the extent they have any ethics.
Another thought you've got to wonder if people dieing is a problem.  Whats
to stop an extremely conservative risk mentality security cleared person,
writing his memoirs spilling all in complete detail, parked with a lawyer
for release on death.  (Eg envelopes to be posted to NYT et al on his
eventual death).  Maybe that means old, and terminal people are going to
find it hard to be employed in security cleared roles.

@_date: 2013-09-29 18:53:31
@_author: Adam Back 
@_subject: steganography & mimic function? (Re: [17] hidden links) 
============================== START ==============================
I am becoming convinced Brian is using this list as a steganographic channel
using steganography and english mimic functions based on some corpous of
text (eg using markov chains - its surprisingly effective if you try it,
given the complete lack of actual textual understanding of the model).

@_date: 2013-10-30 21:05:54
@_author: Adam Back 
@_subject: Re: Gentlemen do not read each other's mail... 
I was reading that as the Brits are monitoring her calls, and the Americans
are just receiving a copy via five eyes.  They've been reading too much
Lewis Carroll.

@_date: 2013-10-16 20:06:54
@_author: Adam Back 
@_subject: Re: [Cryptography] "/dev/random is not robust" 
Their objectives are not that bad, though you do have to wonder if
"adversary got root and copied your RNG state" is that realistic - if they
can do that wont they just install a rootkit or take your private key out of
But Schneier and co designed yarrow, and newer replacement fortuna to those
design objectives.  I guess there should be open source for that.  If its
clean code maybe someone should have a go at beating Torvalds over the head
with logic until he replaces it.
It does make a difference even for bootstrap issues.  Years ago at ZKS I
tried watching /dev/random while generating entropy, there was clearly not
nice things about the way they were added.  For example if you start a
server or boot a machine with a known initialized state, and the machine is
talking on the network releaseing RNG output, and the entropy what little
there is is added incrementally so that you can bruteforce the gap (eg
20-bits of entropy added max in one lump) then you can infer what the new
RNG state is.  If the rate of entropy addition is low enough and the
computer too chatty with RNG outputs over the network (eg maybe it gives you
some in TCP sequence numbers or on demand just by hitting it on various
ports) you can then retain knowledge of its state perpetually.
Thats what Fortuna (and before it Yarrow) are about: you collect entropy
into a lump before you add it, to protect against that problem.

@_date: 2013-10-15 12:26:13
@_author: Adam Back 
@_subject: ecash & bitcoin & privacy tech - never better (Re: Bitcoin mining efficiency and Botnets) 
The mining pools have no protocol policy control.  The users have control,
if miners change their policy in a protocol incompatible way, they will have
created an alt-coin which contains only them and their mining profits will
evaporate.  See bitcointalk thread on committed coins I posted in previous
post for a big discussion of this topic.  Its much better than you think,
clearly committed-coins are not implemented, but they could be added
relatively easily.
About privacy features its not actually clear if that was intended or not. Some privacy fig leafs are offered in terms of new addresses automatically
and no names on addresses.  But the entire transaction log is public, clear
text for anyone to see.  If credit card transaction logs were that public
(even with just card numbers and no name) people would be outraged.
It also not clear if more privacy would have helped bitcoin to date - too
much privacy too early could be inviting regulatory problems.  Maybe its
better for users to work on privacy themselves, or others to add privacy
separately, or privacy features to be added to alt-coins etc.
See also zerocoin, and homomorphic encrypted value coins.
btw speaking as someone who was fascinated by blind ecash and spent a lot of
energy on this list years ago trying with others to figure out someway to
make something deployable, I have to say bitcoin is a stellar success.
Ever since the digicash betabucks $1m capped coins went out of existence
with digicash filing bankruptcy, it became clear to everyone that a single
company with a cental server was not going to work.  From there we had a lot
of interest to solve that deployment and design problem: hashcash
distributed mining, Wei Dai's B-money/Nick Szabo's bitgold, Hal Finney's
RPOW and finally bitcoin!  As well numerous other cool stuff like David
Wagner's blind-MAC (implemented by Ben Laurie as Lucre) (chaum patent
workaround), Niels Ferguson's single term offline coins (still blindable but
with more efficient offline fraud tracing than Chaum's cut-and-choose),
Stefan Brands ecash/credentials (multiple attributes, efficient, many
features) as well as Sander & Ta-Schma auditable anonymous ecash.  Its not
clear Satoshi is related to the other ones (other than using hashcash like
B-money/bitgold & RPOW), he seemed to not be aware of B-money (or bitgold),
but he couldnt pontificate for risk of narrowing the potential authorship :)
Then you have open transactions.
Anyway for deployed ecash and privacy tech political environment life has
literally never looked better - NSA shot themselves in the foot, so public
opinion is strongly in their dis-favor, the 9/11 death-pall to security vs
privacy arguments finally get swept away.
So by all means lets see some work on improving privacy, security,
decentralization and scalability of bitcoin via alt-coins or direct protocol

@_date: 2013-10-14 10:02:04
@_author: Adam Back 
@_subject: Re: [tor-relays] NSA's "Tor Stinks" 
Btw speaking of GCHQ or NSA operating Tor nodes, of course that is
inevitable; and to the extent that they are not perfectly policy aligned a
good thing, and they'll try to do a professional job of securing their own
tor nodes :) eg if you are a chinese dissident maybe you want to use them as
one hop.
You just dont want them controlling to many nodes.  And probably the
Russians, French, Israelis, Chinese etc are all running Tor nodes and even
less mutually cooperative.  What we could really do with is North Korea, and
Iran intelligence services running some also.
I suspect to the extent that they are experiencing limited success you could
imagine its because not ony are some nodes controlled by users, but more
that some are operated by mutually distrustful competing intelligence
The intelligence agency nodes are probably better secured than user nodes,
though some user nodes maybe run by security capable and conscious users. The intelligence agencies however have a budget for and hoard of unpublished
0-days on PC & router operating systems so they have a slight edge.
Also the intelligence agency is not going to cave under legal pressure when
someone from law enforcement comes with threats and demands relating to exit
traffic so they have that advantage too.
It would be better to my mind if they just came out and said yes this is our
node and ran it from their own domain tor.gchq.gov.uk or tor.nsa.gov; then
users could opt to use it.  However I suspect they think no one would use
it, or the people they actively want to use it (who they are trying to
trace) would avoid it.  Could be useful if they used an identified one and a
plausibly hidden one.
Speaking of plausibly hidden I notice there is mention of code word 'NEWTONS
CRADLE' in one of the docs for a GCHQ tor node operation, speculating could
that be some MoD funded student at cambridge in their dorm?  (Quite commnon
in the UK for students to be sponsored by a company they will work for
afterwards or a government career they took a break from.  A couple of my
classmates at BSc, University of Exeter (UK) comp sci BSc were openly MoD
No matter, its trivial for establishment to provide perfect cover for node
operation, just run from home address, or persuade ISP/telco to route
traffic via DSL lines identifying IP address range as a IP forwarding proxy. They can do whatever they want, you'd think that more likely, however a
university dorm IP address range would look nice and plausible/credible
also, maybe more so than a DSL address.  Probably a university upstream or
the university IT itself (universities often take defense contracts) could
fake it or operate it under contract with intelligence cleared dual-hat
admin if they cared enough.
I do think it would be very useful if the intelligence agencies running tor
nodes also ran one on their own domain.  Then you could route via one who's
government is overtly supportive of your political cause.  (Doesnt protect
you from backroom information exchange deals and horse trading, which I'm
sure happens even with sworn enemies, but its a start if you are
unintersting enough!) However I expect another reason they dont want to do
that is they dont want to enable people to get stronger privacy period. They have a dual hat, they want internet privacy for their own open source
research, but they selfishly dont want other users to have privacy or gain
any privacy as a side-effect from their own.

@_date: 2013-10-07 10:46:27
@_author: Adam Back 
@_subject: Re: Analysis of Silk =?utf-8?Q?Road?= =?utf-8?B?4oCZcw==?= Historical Impact on Bitcoin 
Great article, good to see bitcoin is at this stage less influenced by news
flow and transaction volume of silk road like activities.  (Silk Road while
interesting to proponents of agoric defacto legal reform by weakening
enforceability of drug laws, which it seems was DPR/Ulbricht was a proponent
of, the strong short-term success might have been hazardous to bitcoin
itself.  And I think doesnt really rely on bitcoin anyway as there are in
many countries cash pre-paid credit-cards and similar types of systems.)
Its a curious effect that part of bitcoins bootstrap to non-toy price, may
have been accelerated by silk road itself historically.  I suppose its not a
unique situation that criminal privacy or non-criminal but privacy sensitive
business areas (eg online porn) are innovators in adopting new technolgy
with some media reputation for privacy.
I am not sure bitcoin is particularly anonymous given the fact exchange
interactions are covered by AML/KYC and all payments publicly logged in the
clear for anyone to see, and that mixing attempts have been shown to be weak
and of questionable effectiveness by statisticians and data analysts
(complete with pretty graphs), and that most clients have no automated coin
control or sub-wallet feature.  There maybe more conventional higher
anonymity systems like perhaps pre-paid credit cards, phone cards and such
things which can actualy be bought for paper cash, and sometimes deposited
electronically (I'm sure criminals and money laundering experts have a
better understanding of which systems provide anonymity, those were just off
the top of head non-bitcoin, non-cash, payment anonymity examples.  An
envelope of cash in the mail probably works too though I think that is
technical illegal to do even with nominal amounts in some countries).  I'm
quite sure the criminal options at all levels of scale for payment remote
and local involve many more options than bitcoin.  At the higher end it
blends with and solicits and obtains likely knowing service from established
but greedy market players.  You can see the odd dataset, eg HSBC being
caught laundering $880m of drug cartel and even dirtier money.
At least to say its clearly quite dangerous to rely strongly on bitcoin
anonymity for non-technical people, or even technical people without serious
operational attention and careful tool analysis and selection.
A few trivia items from the Silk Road story that seem ambiguous - they
opened a package coming from canada to him with fake ids.  Seemingly he
wanted fake ids for some reason relating to operating the servers (more on
that next).  But why did they open his package?  Bad luck random spot check? Or (more plausible) he bought the fake ids from an ongoing fake-id sting
operation in Canada?  Or alternatively evidence of parallel constructon
disguising NSA Tor backtrace?
About why did he even think he needed fake ids.  Surely you only need a
physical fake id if you are renting servers in person.  But why would you do
that - physical servers you interact with physically are a needless risk no
- if backtraced to IP by Tor attack, the physical connection is made?  (Why
not rent cloud servers?  If renting cloud servers why not pay with prepaid
credit cards bought for cash (or bitcoin)?  If need to send fake ID to rent
cloud servers, surely its lower risk to photoshop them than accept physical
delivery of illegal forged ID to your own address, from a criminal or sting
fake-id service with a copy of your photo!  I dont really get it.

@_date: 2013-10-03 19:57:47
@_author: Adam Back 
@_subject: Re: Pen register request used to force disclosure of SSL private keys - LavaBit hearings 
[Man there's a lot of names from the old days on this list.  Good to hear
from you Lance :-]
I think the take-away from this issue is CAs should issue certifictes on
keys used for signing only.  Say its a DSA, or ECDSA which is a damn good
choice because it is not even directly possible to encrypt with it (*), and
the key usage will be marked sign only, so there is no argument about its
Then we disable any non-forward-secret ciphersuites (and forward secret
ciphersuites are not coincidentally the only ciphersuites that work with a
signing only server key).  Then the only plausible reason to demand the
signing key is to perform a MITM not to access "encrypted data".
Firstly MITM is more work, and secondly theyd at that point just as well
play nicely and ask the operator with a subpoena to hand over some info
inside the SSL stream if there's anything useful in there.
In some countries there are explicit legal protections for signature only
keys.  At best they subpoena could ask the operator to record the session
keys via the SSL web server, however that feature is not present as far as I
I also think the weak point with lavabit was probably the in-mail and
out-mail, as with silentcircle, and I presume the reason silent circle
disabled email (though they could have secured internal sc-sc mail using eg
the same end2end secure messaging architcture they use for messaging).
A further weak point of lavabit as I understand it is it was actually taking
the password to the server!!  So the user private key was in the server ram
temporarily.  Which is complete misdesign and makes you start to question
Snowden's crypto tradecraft which up to that point was looking pretty damn
strong from the news reports.
Anyway signature only keys and forward-secrecy FTW already.
About software updates, I think we've reached the point of multiple
independent public interest code review bodies with signing authority
together with the software vendor.  The other thing with opensource it can
be forked if the main vendor goes wrong or is coerced.  You see this kind of
reasoning with bitcoin foundation etc as its probably the highest open
software assurance level on the planet protecting > $1bn in bearer bitcoin
value :)
The only possible exception to the coerced code change might be the hushmail
thing thogh I am kind of fuzzy about what exactly did happen.  There were
two versions, one like lavabit (server has key temporarily) and one real
end2end as I recall and one version of the story is it was the non-end2end
one that got the user info info subpoenaed.
(*) Yes yes I know you could abuse DSA public key for another discrete
encryption log algorithm, however such practice is considered risky to reuse
an asymmetric key for two different algorithms in case there is a way to use
one as an oracle to attack the other.

@_date: 2013-10-01 10:28:11
@_author: Adam Back 
@_subject: Re: Surveillance 
Apparently the UK is worse than the US even - less pretense about not spying
on their own subjects, less legal restrictions (to the extent the NSA and
their nominal oversight even respected the restrictions, which clearly they
did not much respect and subverted with clear internal complaints of the
oversight to the extent that the info was disclosed to them.).
You are her majesty's subject not a citizen, and the royal family hasnt
exercised their powers nor even expressed displeasure such is the etiquette
in a century.  The best you've got is the house of lords, however even their
powers have been weakened and dilluted by politically appointed peers by
parliament, which in my view was two steps backwards; at least the
hereditary peers were a break on change, are typically wealthy people who
dont want the politicians to screw up the country and to some extent have
more aligned interests with the people than policitians who typically have
no actual views, just play to opinion with no regard for the direction their
actions push civil society and democracy.
It may well be that for most westerners the best you could do is use a
russian or chinese internet proxy for internet, voice SIP, video chat/IM
etc.  The chinese are interesting in having their own source of backdoors
(electronics manufacturing) possibly rivaling the US software and key
backdoors.  They may have a state level interest and competence to find and
eliminate US originated backdoors.  Similarly for russia.

@_date: 2013-10-14 17:30:22
@_author: Adam Back 
@_subject: Re: [linux-elitists] Browser fingerprinting 
Well you should say the web developers regressed since then.

@_date: 2013-10-07 09:57:44
@_author: Adam Back 
@_subject: Re: [linux-elitists] Browser fingerprinting 
Scary numbers.  Even with chrome incognito unique to 1 in 1.7 m on linux. Maybe better on windows.
I wonder if no-script would help or is this passive headers only?  Seems
like the leak was fonts, plugins and user agent in that order at 1 in 128k,
266k, and 1.7m respectivey.  Need less chatty browsers.

@_date: 2013-10-21 14:49:27
@_author: Adam Back 
@_subject: Re: [Cryptography] Mail Lists In the Post-Snowden Era 
(Oh yeah, I top-posted by habit, better copy some text above to preclude an
excuse for censorship, there done!)
In the context of crypto lists I prefer open, unmoderated/uncensored.
For example the paranoid might note that its desirable for the forces of
darkness to control the medium by which the open community communicates,
delete the odd message with plausible deniability, use moderation as a
platform to squelch traffic with a little hidden bias, who's going to know. Viz this crypto list went dark for a year or so (ostensibly because -
actually we're not sure - anyway no traffic flowed; and finally the list was
reopened - temporarily, when randombit opened up as an unmoderated list, and
threatened to take over as a continuously flowing open medium.) Then again
another long hiatus on this list followed by only reopening when the world
was exploding with Snowden revelations and recriminations.
Paranoid or not?  If Snowden's episode showed one thing its that people were
too niave, and not paranoid enough.  Its easy pickings to step up for admin
positions in organizations, because momentum and laziness dictates that
others will not, and then some regime of sabotage, discussion shaping,
control can ensue.  Anyone who's done any standardization work, will have
found defense research people holding unlikely chair positions - medical
health care message security - UK defense research agency.  Really?  Why? Probably to make avoid use of forward-secrecy or such like soft-sabotage. People should re-read the declassified old sabotage manual and dwell on what
could be done with $250m/year against open discussion forum, protocols, open
source software, chairman/organizational positions etc.  Because, its
probaly be actively done right now.  Accident that android is using crap
ciphersuites - or plausibly deniable sabotage.
(copied to the unmoderated list)
The cryptography mailing list

@_date: 2013-10-21 14:49:27
@_author: Adam Back 
@_subject: Re: [cryptography] [Cryptography] Mail Lists In the Post-Snowden Era 
(Oh yeah, I top-posted by habit, better copy some text above to preclude an
excuse for censorship, there done!)
In the context of crypto lists I prefer open, unmoderated/uncensored.
For example the paranoid might note that its desirable for the forces of
darkness to control the medium by which the open community communicates,
delete the odd message with plausible deniability, use moderation as a
platform to squelch traffic with a little hidden bias, who's going to know. Viz this crypto list went dark for a year or so (ostensibly because -
actually we're not sure - anyway no traffic flowed; and finally the list was
reopened - temporarily, when randombit opened up as an unmoderated list, and
threatened to take over as a continuously flowing open medium.) Then again
another long hiatus on this list followed by only reopening when the world
was exploding with Snowden revelations and recriminations.
Paranoid or not?  If Snowden's episode showed one thing its that people were
too niave, and not paranoid enough.  Its easy pickings to step up for admin
positions in organizations, because momentum and laziness dictates that
others will not, and then some regime of sabotage, discussion shaping,
control can ensue.  Anyone who's done any standardization work, will have
found defense research people holding unlikely chair positions - medical
health care message security - UK defense research agency.  Really?  Why? Probably to make avoid use of forward-secrecy or such like soft-sabotage. People should re-read the declassified old sabotage manual and dwell on what
could be done with $250m/year against open discussion forum, protocols, open
source software, chairman/organizational positions etc.  Because, its
probaly be actively done right now.  Accident that android is using crap
ciphersuites - or plausibly deniable sabotage.
(copied to the unmoderated list)
cryptography mailing list

@_date: 2013-10-17 12:32:57
@_author: Adam Back 
@_subject: Re: [Cryptography] /dev/random is not robust 
I think the more worrying case is a freshly imaged rack mount server,
immediately generating keys or outputting random numbers to the network or
in response to network queries.
The initial entropy is known (ie 0) and if what little entropy there is is
added in brute-forceable chunks, then an attaker able to observe or get
responses including RNG outputs over the network can keep in step with the
RNG state.  (Eg say 20-bits of entropy at a time, and needing 10-bits of
guessing to account for only seeing one in 1000 of the RNG outputs, then you
can for small cost of 2^32 per interval keep up.)
A similar issue could arise with a VM rollback (to a previous un-initialized
state, or repeating the same random outputs to different messages - eg
breaking DSA without the deterministic k=H(d,m) defense.
Yarrow, and the replacement Fortuna try to address this problem by
accumulating entropy and adding it in bigger lumps..
The cryptography mailing list

@_date: 2013-10-19 16:36:33
@_author: Adam Back 
@_subject: Re: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
I know its obvious and you mentioned the risks, but this is in principle a
band-aid or worse; it gives the illusion of entropy in the face of actually
no entropy to an attacker who can readily obtain the serial numbers in
question (eg because the MAC is broadcast on the LAN) or simply brute forced
because the guid is while large, highly structured and sparse.
It would seem safer to fail/stop and depand user action.  I know thats not a
popular decision in a distro/package/boot sequence, but churning out
0-entropy keys disguised as having entropy being E_0( mac ) and such analogs
is a bad outcome and wont be observable via identical P, Q key searches.
People are seemingly in a hurry, or dont care much, dont understan when it
come to packaging and thinking about security.
The cryptography mailing list

@_date: 2013-10-28 12:14:33
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment protocol for onion URLs. 
I think its a mistake relying directly on X509, its subject to corrpution
attacks, involves ASN.1 and enough openSSL X.500 encoding abiguity (or other
code base) to be a security nightmare.
Why not make the payment messages signed by bitcoin keys.  If someone wants
to associate with X.509 they can put a bitcoin address on their SSL site.
If someone can get into your site deeply enough to modify your SSL served
code and you're trying to run ecommerce you have other problems.
Never the less if they care they can clear sign the bitcoin addr with xmlsig
and their X.509 private key, and/or with PGP and a WoT.
I think its smarter to pollute bitcoin main with X509.  Make a little helper
util if there are not enough xmlsig tools that you cant pick one up
opensource for multiple languages.
This then avoids the binding to Tor - you can publish a bitcoin address
authenticated anyway you like.  Eg tahoe-LAFS auth/integrity, i2p, tor, pgp
- you name it.
Maybe I voice this opinion a bit late in the cycle, but I think it would do
bitcoin a favor otherwise its a camels nose in the tent into the TLAs that
control their own X.509 CAs, or issue NSL letters for CA private keys, or
forged certs.  It's all happening and thanks to Snowden we now have even
more evidence...

@_date: 2013-10-02 19:31:08
@_author: Adam Back 
@_subject: Re: Silk Road founder arrested ... 
Without getting into the rights and wrongs of drugs policy, and the case, I
suspect the silk road shutdown is good for bitcoin because it shows that
even if bitcoin were very anonymous (which its not due to being a public
ledger system), people are not completely immune from tracing even aside
from that if they become a big enough target, and it is easy enough to slip
up on security in operating a service.
(Surely the NSA could've figured it out anyway with their Utah datacenter
full of global internet traffic tracking info if anyone cared enough to get
the clout to demand it from them, and from recent news its stated they will
give evidence of crime to law enforcement if they find it as part of foreign
national security activities.  Whether they take requests to go trace things
from law enforcement is a different question - seemingly not hinted
in the Snowden revelations.  But apparently they're not beyond covering up
the source with fake cover stories of how they found the info even to
Also I read somewhere that silk road was using an offchain payment (not
strictly bitcoin but bitcoin converted into some silk road operated server. maybe its described somewhere for people who dont have ToR running, perhaps
it was a chaumian token server?).
Well not so fast there, without slip by operator being reported, that guy in
Ireland operating a ToR focussed mini-ISP got identified, and/or his clients
did and it spilled over on to him, or whatever happened.  (That one based on
some browser bug and jscript attack inserted by law enforcement somehow). High grade security is not for the careless - need to follow advise, eg ToR
browser bundle and scripts off or such?

@_date: 2013-10-04 14:40:38
@_author: Adam Back 
@_subject: Re: how to use Tor securely (Re: Silk Road founder arrested ...) 
Seems to me if people care about anonymous publication security and
robustness they need static content, distributed, encrypted and integrity
protected.  eg like say tahoeLAFS over Tor or something like that.  Of
course not as jscript, form, click etc but thats just asking to be hacked
As I recall Zooko mentioned you can actually do that - back a web server in
LAFS, then the webserver is nothing but a read-only consumer of LAFS data.
Presumably someone can figure out how to route encrypted, authenticated
change sets or form submussions back to the underlying LAFS over Tor.

@_date: 2013-10-04 09:01:27
@_author: Adam Back 
@_subject: how to use Tor securely (Re: Silk Road founder arrested ...) 
But the jscript malware was installed via remote compromise onto the Tor
hidden web server.  Being behind Tor does not particularly add any
protection to your server, in terms of remote hacking.  Probably static
content is safer in general even if it doesnt make flashy cursor hover boxes
and client-side form pre-validation.  Ie instal and turn on noscript - 99%
of jscript is of no particular use other than making your browser blink and
show animated ads ;)
Ideally you need Tor to be in a routing box, not your computer so that there
is no way for your computer to connect to the non Tor network, so your
computer doesnt even know its physical IP and has no power to disclose it.
Or simulate that setup in software you need Tor on the main machine, and a
VM that has access to and knowledge only of Tor network for connectivity. Do not put ANY identifying information inside the vm.  That rules out vmware
because they leak in your disk serial number as a result of a microsoft law
suit.  (Microsoft accused them of making it easy for people to share windows
serial numbers, because the "is this the same machine" calculation based on
various HW serial numbers always comes up with the same answer in a virtual
machine at that level.) Similarly the VM must not know your physical network
card MAC addresses etc.
Thats the way to do it properly on the client side.  There are Tor focused
distros that let you boot into Tor only OS.  For my taste the Tor connection
and code and physical device identifiers (physical MAC addr, HD serial etc)
should be OUTSIDE of a VM and all client software should be inside the VM. The VM should be open so you know they are not leaking physical MAC
addr/serial into the the client in the name of copy-protection.  (It was
microsoft's fault, not vmware).

@_date: 2013-10-04 10:02:32
@_author: Adam Back 
@_subject: Re: <nettime> A CEO who resisted NSA spying is out of prison. 
People frown at Russian suspected political prosecution (eg oligarchs
falling with someone politically powerful and then with coincidental timing
finding themselves incarcerated for probably trumped up financial
irregularity or other charges.)
Here we see it US style.  A judicial inquiry should be heard, he should
receive a pardon and compensation.  This is a horrendous judicial fraud
sanctioned at high levels and carried out by a complicit justice system. The perpetrators in NSA, government and justice system should receive long
prison sentences.  Otherwise the rule of law in the US has received a big
credibility hit, the only fig leaf is the shaky plausibility of the trumped
up charges.
Unfortunately its not completely impluasible in isolation because wealthy
business people from time to time have committed these exact crimes in
showing poor judgement by backdating options, and insider trading and such
shenanigans despite already being wealthy enough to not have their grand
children work a day in their lives.
But it sure looks suspicious and the political cover story has been blown. At minimum he should get a judicial review or inquiry and probable

@_date: 2013-10-07 09:48:18
@_author: Adam Back 
@_subject: legal game-theory, case for smart-contracts & snow crash (Re: <nettime> A CEO who resisted NSA spyin 
I think maybe you are neglecting game theory for the accused, its hard to
incentivize people to act in their collective interests, when they are
thinking of their own future freedom and lost earning capacity.  I imagine
you even have researched the statistics for this, but to summarise the game
theory scenario: plea bargaining clearly results in less accurate justice
(more innocent people do jail time), but has the real-politic benefit of
reducing the cost of implementing justice.  The usual pattern (made up
average numbers) is accept the plea do a discounted (lower than sentencing
guideline) 3 years, reject the plea, the prosecution will make less
reasonable/inflated charges (higher than sentencing guidelines, based on
more tenuous/unlikey to be provable charges) threatening a scary 30 years,
which in reality will be moderated down by a judge if the accused has the
money for a decent lawyer to 5 years, if they lose, or 0 years if they win;
if they are relying on an overworked, less capable public defender because
they dont have the money to buy proper representation, their chances of
winning are lower, and if they lose their post-trial sentencing will be
higher at 10 years.
Now law is a remarkably imprecise subject, especially when muddied with some
not-so-scrupulous and politically motivated prosecutors, police entrapment,
police bias (push for conviction based on opinion/bias, but statements given
disproportionate weight by a system that believes it's officers over the
public).  (Prosecutors and police are politically motivated because their
career depends on conviction rates, headlines).  The system seems to largely
ignore or not give adequate weight to investigating significant
prosecutorial abuse or police bias.  Prosecutorial abuse has to be strongly
proven, and the perpetrators are career ambitiuous, and legally qualified so
know the grey areas they can exploit where the abuse will be unprovable even
when it is very rarely alleged, or actually prosecuted.  Like police they
have the benefit of the doubt, in a judicial system that favors its own
officers, and so they are defacto largely immune from sanction from even
significant systemic abuse, unless stupid enough to be caught red handed
with with a smoking gun.  Which is to say even if you have millions to your
name for the most capable legal defense, and completely innocent with
reasonable but not iron clad alibi, its still subject to a high degree of
randomness depending on political motivations surrounding.
So therefore people will not fully follow game theory of going for the
lowest expected sentencing.  Ie if p is probabity of winning, and the
numbers above: then its 3 vs expected (p*0+(1-p)*5) so even p=2.5 its 2.5
expected vs 3, so if that was an investment you'd say good lets do it.  But
if its choice between 3years and no more stress, vs legal defense cost and
years of stress followed by 5 years if you're unlucky.  The dillema still
holds if the odds p=0.75 and you have lots of money I suspect sadly that
thats about as high as p gets for many areas of law.  You also have to
factor in the loss of income (at the average income for prisoners) into the
equation, and a premium because people would sooner earn less and have their
You cant reform the system via kickstart fund and incentivize people to not
accept pleas, well not at $5k anyway, because they'd need compensation for
lost earnings and a huge loss of liberty premium if they lose, a stress
premium for going to trial, and expenses for high quality legal defense. Those figures may no longer make game theory economic sense for society,
though I do think the centuries old principle that its more important for
one innocent party to go free than 100 guilty to be imprisoned is not
properly incorporated into the current system as plea bargaining removes
most of that intended objective.
Even with best attempts at fairness and balance from police, prosecutor and
judge (and there are genuine public spirited ethical people in some of those
roles, who would ignore the perverse career motivations on principle, so it
probably happens some of the time), the outcome STILL has an unfair plea
imbalance and STILL high randomness.  Its an imperfect system even under the
most favorable conditions.
I think the solution is to politically vote to arbitrarily cap the
incarceration rate to 10,000/annum; the justice system is not allowed to go
over that limit by law.  They will then focus on cases where they think the
incarceration is of most value to society (eg of making the public safer by
taking a violent criminal off the streets).  Maybe the cap should be
adjusted based on false conviction rates, if the false conviction rate
increases, the cap decreases.  Independent review of potential prosecutor
abuses should be increased.  Also the system should be restructured to
remove the career/political motivation for prosecutors to achieve high
conviction rates.  Their conviction rate should include a heavy mallus for a
false conviction, so they strive to avoid convicting innocent people, and
the system should somehow be adjusted to be less adversarial and to remove
sentencing penalties for going to trial.  eg Maybe the trial sentencing
level and charges should be set by an independent neutral body, not the
prosecutor, with the objective of keeping the trial and plea sentencing the
same.  Maybe simpler bargaining should be made illegal.
Another specific problem in the US is its a one dollar one vote system, and
operating privatized prisons is a high profit business.  The prison
operators votes therefore likely outweigh the proportion of the public that
is aware of the system problems or care enough to vote about it.  I believe
other eg european justice systems are in fact less prone to these issues. So another solution is to vote with your feet.
Basically in such a system you want to avoid even interacting with the legal
system or justice system, period.  Even volutarily interacting as a random
by-stander is unfortunately likely to be net loss to your finances or even
freedoms.  Even to complain publicly about the defects of the system is
probably risky once you have interacted with them.  Which is ridiculous but
thats the reality.
And finally some of the laws on the books are ridiculous on their face in
the opinions of the accused's peers.  eg computer abuse act which sees Weev
in jail and such like stories, and the sentencing guidelines are also often
ridiculous and non-proportional eg the sentencing threats to Swartz for what
was probably not even a copyright crime (going on the theory from his
previous activism pattern that he was aiming to republish the subset of
articles that were public domain).  His trial sentencing threat was above a
1st degree homicide with iron clad evidence, something's got to be wrong
with that.  The sentencing board are failing in their task.  Something
should also be done to restrict scope for judicial vengence also - Swartz
made a mockery of a stupid law, with his previous popularly supported
activism stunt, and so prosecutors were out to get him.
Also legal systems generally seem to lag 50-200 years behind the opinions of
the public Some jurisdictions are better than others, but the system of case
law mixed with precedent creates a built in brake on legal theory evolution. Out of touch with reality and public opinion prosecutors, judges and
sentencing another issue.  Probably law should be restricted to 1MByte of
ascii text and any law not approved by 90% referendum (1 person one vote,
not 1 dollar one vote) struck off automatically every year.
This state of significantly imperfect, and hard to reform, high cost legal
system issue is why smart-contracts look so attractive.  Its not even
obvious how to improve the legal systems and they evolve slowly and resist
experimental change.  Mathematical aprior enforcement, deference to mutually
agreed competing impartial arbitrators for dispute.  Pseudonymous
smart-contracting parties FTW.  Of course it doesnt work for in-person
crimes, except in a "Snow Crash" sense (cometing legal systems/governments
in the same physical space) but a justice system that leant heavily on
smart-contracts and refused as a principle to revise contracts where both
parties received competent legal advice, nor overturn arbitrator decisions,
would be a step forward for society.

@_date: 2013-10-15 12:04:56
@_author: Adam Back 
@_subject: Re: Bitcoin mining efficiency and Botnets 
I dont think that matters so much as that everyone gets the same hashing
power per dollar.  I had some rant I posted on bitcointalk a while back
(first post there) to say using hashcash-scrypt(1) would be better than
hashcash-SHA256.  (scrypt(1) meaning scrypt(iter=1)).
However there are some valid counter arguments.  SHA256 is simple and easy
to put into silicon blurprints for fabrication replicated multiple times. Even small and seemingly significantly incompetent outfits like butterfly
can just about do it.  Apparently many more are coming online.  Thats good
because you could do it yourself with a modest budget and necessary skills.
If the mining function was really complex it would create eg $10m or $100m
barrier to make a very fast implementation of it, then you hae a real
barrier to entry and a mining centralization problem.
The not so good part is maybe anyone with the skills will get the chips
fabricated and mine them themselves.  So it depends on ready market
availility from multiple competitors, that question is a bit up in the air
at present but there is some evidence of improvements in availability.
Dont think mining is a get rich quick scheme, its very easy to lose money at
this stage, as its an arms race as the fab tech used quickly catches up to
moore's law and then tracks it.
Also the miners dont actually have that much power, all they are doing
really is ordering transactions, so for double-spends you can chose the
first one as valid.  A big company or individual who invested millions and
is earning big bucks from their mining operation probably doesnt want to
commit spending fraud - they'll get sued and lose their investment and
Now if governments or other organized criminals do it, thats a different
issue as there is no useful legal sanction at that level.
They cant really censor tansactions btw even then see the committed-coins
proposal if you want to know how that can be fixed.
Rumor is there are people working on a litecoin ASIC.  Scrypt wasnt even
designed to protect against memory-time tradeoffs, nevermind intentional
large design mm^2/minimum gatecount.  I think if you can make the algorithm
complex and dynamic enough, and yet still efficienty verifiable, (and to
have no progress so its like a lottery) you should be able to push thing so
that whoever does make ASICs is basically making a custom multi-core chip
and competing head on with scientific and graphics GPUs.  AMD & Nvidia are
probably going to win there, or if they dont people will buy your dynamic
agile algorithm miners for programmeable scientific uses.
I see you had the same idea, and I dont think thats so unrealistic.  Making
it fast to verify is a bit harder.  For example include all 16 AES
encryption finalists and 16 SHA3 finalists etc and combine them with data
dependent selection of algorithms.  This will push the gate count up.  Scale
that design process a few times and you're there.  Mix in some memory
(apparently memory is not so fun to put on ASICs, if you need lots of memory
per execution instance (whih is not memory cpu tradeable like scrypt) that
makes it expensive to ASIC.
I do think CPUs are probabl a losing bet should aim for GPUs.  Consider they
are largely not made but better CPUs can be made for mining than are sold. eg consider a 100 core intel atom.  They have the gate-count to do it, its
just people would sooner have a faster single thread (via super-scalar
design & higher clocks, better cache etc) lower core chip.  Most of the
silicon on an i7 is wasted in achieving blistering single thread
performance, that is a complete waste for mining.  (atom 47mil transistors, and
there are multiple 4.7 billion transistor GPUs on the market.) If you
succeeded in wedding an algorithm to the intel instruction set, this is what
would get built.  Its remarkably like a GPU really right?  Lots of cores. Clearly if you strip out the intel backwards compat overhead and add SIMD in
groups of 16 cores, you can get 2048 cores per chip as that is what AMD is
doing in the 7970 (or 7990 two cores!) So be careful what you wish for :)
You can always do better in hardware.
The harder part is to have a relatively fast verification, but thats
probably reasonably doable per scrypt design.

@_date: 2013-10-27 16:49:54
@_author: Adam Back 
@_subject: Re: Another Snowden News Story. Another Lesson in Proper Whistleblowing. 
Suppose it was clear but the quoted text was Coderman paraphrasing the
article, and he proceeded to rubbish it also.
I find Glen Greenwald makes the most biting and meticulously informed
criticism of main-stream-medias systematic and repeated failure to fulfill
its fourth estate role.
It really is embarrassing how far main stream media have fallen, they seem
to have no shame or concept of how craven and spineless they look to the
world at large.  Even the NYT did some pretty rubbish and spineless stuff on
wikileaks, though they've picked up the Snowden story finally a bit. (Approximate impression - I dont read nor listen to much of the MSM for
above stated reasons).

@_date: 2013-11-12 09:10:13
@_author: Adam Back 
@_subject: Re: NIST Randomness Beacon 
(Top posted, so sue me, my text explains itself without the history).
Thats a big cc list.  I think you could create a beacon with bitcoin hash
chain by having miners reveal a preimage for 6 old, consecutive blocks where
the newest of the 6 old blocks is itself 6-blocks confirmed.  (ie reveal
preimage on blocks 7-12.  The xor of those preimages defines a rolling
beacon (new output every block, just with reference to blocks 7-12 relative
to the current block depth).
The security against insider foreknowledge is not fantastic, as its relating
to the trustworthiness of the 6 random miners (which have probabilty of
winning relating to hashpower, which doesnt always relate to

@_date: 2013-11-12 09:10:13
@_author: Adam Back 
@_subject: Re: [cryptography] NIST Randomness Beacon 
(Top posted, so sue me, my text explains itself without the history).
Thats a big cc list.  I think you could create a beacon with bitcoin hash
chain by having miners reveal a preimage for 6 old, consecutive blocks where
the newest of the 6 old blocks is itself 6-blocks confirmed.  (ie reveal
preimage on blocks 7-12.  The xor of those preimages defines a rolling
beacon (new output every block, just with reference to blocks 7-12 relative
to the current block depth).
The security against insider foreknowledge is not fantastic, as its relating
to the trustworthiness of the 6 random miners (which have probabilty of
winning relating to hashpower, which doesnt always relate to
cryptography mailing list

@_date: 2013-11-04 10:02:23
@_author: Adam Back 
@_subject: Re: NSA - What for. 
I think you hit it on the risk on head there.  Its like the stasi; stasi 2.0
- they are creating a risk to democracy, and even without exaggeration
civilization itself with their actions.  The Germans get it because they
remember the Stasi.
I expect they are doing it for geo-political influence to tap phones and
internet equivalent of intersting people, and economic-espionage to the
benefit of US companies, to exert political control, to be able to
selectively leak inforamtion to law enforcement (they admit this now).
Thats all internationally illegal, immoral, unethical etc, governments do
stuff that their citizens would reject on a daily basis under cover of
secrecy.  Its a systemic problem with the worlds current goverments.  They
also dont that well control even their own spy apparatus, it has somewhat of
a life and self-interest of its own, and inter-goverment allegiances
independent of the political sphere.
The risks are much worse however: Americans are traditionally ignorant of
lessons of history, look at Bush junior.  The Brits were furious with the
mismanagement of Iraq.  The Brits at least had some historically acquired
wisdom and common sense of knowing how to run an imperially controlled
government without enraging the locals more than strictly necessary.  As the
Iraqis said they had more freedom and independence of political rule under
British colonial rule than after american "liberation".  (ps I am against
imperialism whether former overt British imperialism or current American
If the Americans get an even worse government (and the Bush/Obama government
is pretty damn bad - drone assasinations, internationall illegal strikes,
wars, torture, rendition, guantanamo, persecution of whistleblowers on these
illegal activities, and suppression of press via legal threats).  They've
shown the world their democratic system is very vulnerable to Reichstag fire
like events, they have too much military power amassed, and stasi 2.0
dossiers on most people of interest on the planet.
I think the solution is encryption, privacy tech; lots of it, soon, widely
deployed.  You have rights - if you dont exercise them, illegal government
and/or spy organizations will remove those rights, regardless of what law
says, domestically, and certainly internationally.  The spy apparatus has
shown a strong willingness to bend rules, eg reciprocal arrangements, Brits
or Israelis spy on Americans and then provide the DB query engine to
Americans etc.  Or require the telcos to retain the information, and then
require them to provide an unmonitored DB query interface, or have NSA mole telco "employees" be the only employees authorized to maintain and use
the system.  New US domestic laws will just result in the latter.
Its time to use encryption.  Its a use it or lose it situation, and its
important to civilization.  The law says you have rights of freedom of
speech, freedom of association, but you arent really exercising them unless
you're using cryptographically assured free speech (which means privacy
networks, encrypted emails, unobservable encrypted emails (hiding who is
sending to who) etc.  Subpoenas still work if individuals and businesses
have their own records.  But people have to stop using centralized large
business services; use p2p or end2end security and privacy sytems, cloud to
the extent you use it should be blind to your data and communication
patterns.  Subpoenas still work in the sense that targetted investigatins
succeed as now: present a subpoena to a car rental company and their
business recors will tell you who rented the car, even if the email
confirmation is identifiable only to the renter and the car company, etc. This drives cryptographically enforced law: they can only do targetted
subpoenas, by getting a court to approve a warrant based on reasonable
suspicion, not drag net if there are no central entities to coerce, tap, put
moles into etc, because its too expensive to do it to every computer.
They never give up, so like with clipper, the former export laws, and their
15 year diversion into hacking everything, and subverting laws; they will
continue.  Probably their next step beyond requiring telcos to keep records,
will be to up the ante on pre-emptive hardware hacking - requiring hardware
companies to put remote triggerable hardware backdoors in processors,
chipsets, firmware etc.  Time to buy chinese probably.  Pick your vendor
depending on your use-case.  If you're a big US business guy buy US, if
you're a US citizen probably buy chinese.  Hardware arbitrage.  They might
have a go at requiring licenses to write and publish code as Stallman warns
about.  I dont think that can flies in a notionally free society, but they
had a go at clipper, and export laws also.  I hope that common sense
prevails and that also fails.
Interesting times.

@_date: 2013-11-08 11:13:31
@_author: Adam Back 
@_subject: patents in a free society (Re: Brother can you help a fiber?) 
In my opinion patents and copyright are incompatible with a free society and
crypto-anarchy: ie with the right to privately contract, and right to
cryptograhically enforced privacy (encryption), and freedom of association
(pseudonymous/anonymous networks).
You'd think Jim would get that given is previous explorations of the darker
side of Tim May's cyphernomicon catalog of ideas...
Patents are also stupidly destructive as the technical world is filled with
literally millions of junk patents, with redudant overlap, so you cant do
anything without tripping over 100s of junk patents.  Even the USG finally
started to try to belatedly reform the idiocy.
(Without any aspersions of the junk or non junk status of Jim's patent as I
am not a hardware guy).  My threshold is if any strongly competent engineer
can dream this idea up in a week when asked the same questions, its clearly
a junk patent designed to sabotage and leach off other peoples productivity.

@_date: 2013-11-11 15:31:20
@_author: Adam Back 
@_subject: donate to a starving patent troll?  nah, skip (Re: patents in a free society (Re: Brother can you he 
I cant see any rational player voluntarily opting to honor a grossly abused
ideas monopoly concept that only a force monopoly form of government could
even pretend to enforce.  You know how many companies in China had a history
of laughing in the face of western "IP" and just cloning whatever they
wanted.  Yep thats what it looks like, and its a good thing for human
progress.  I hope when they overtake the US in economic and geo-political
strength in the next few decades they say "screw that" and treat it as a
strongly defensible ethical stance, and not pay lip service to the WIPO
bully tactics.
I mean the best you could say is put a donate link in the doco if you want
to pore through the list of starving former patent trolls, small investors,
and IP luddites who are climbing over each other to claim to be first to
have thought of the 1,000,000 stupid and overlapping "xor cursor" grade
things that went into making the given product.  Or just skip.  Let the
market choose, thats a nice euphamism for it shriveling and dieing.

@_date: 2013-12-23 10:02:03
@_author: Adam Back 
@_subject: Re: [Cfrg] Requesting removal of CFRG co-chair 
Dan perhaps this last round is overly focussing on details of how Alyssa
Rowan expressed the post.  Probably modulo the wording you agree at
principle leve, if I had to guess.  I'm more interested to see an
articulation of principle (from you and others).
As I said in my comments this is completely impartial.  It has nothing to=
 do
with Kevin Igoe.  Its his employer the NSA, and the egregious, sustained
well funded, systemic full-spectrum APT attack on societal security.=20
Society is the victim, NSA is the outed criminal organization.
Lets make an analogy:=20
   Its like you had a massive electronic bank robbery where many ordinary
   people lost their retirement funds, never recovered, to the level of
   affecting US national budget and business reputation globally, and it =
   discovered that it was an insider job, one of the technical consultant=
   slipped a backdoor into the security system.  For plea bargaining reas=
   he escaped prosecution (flipped a higher up player) but guilt is
   completely not in doubt.
   Ok now you have a standards body for open peer reviewed design of bank=
   security sytems.  Do you invite this guy to chair the group?  Or one o=
   his co-consipirators already is co-chair, do you remove him given his
   criminal association.  Lets say despite criminal record these people a=
   exceedinly well qualified and clever, so there is real possibility wit=
   careful closed planning and bribes, hidden contracts etc that they mig=
   systematically succed in outwitting the public, or gaining some non-ze=
   advantage in soft-sabotage/brittleness/disruption of robust architectu=
   in plausibly deniable ways.
This is basically what happened.
But now the game is up, should we invite a new chair who was explicitly N=
I think it would be a resounding NO. =20
If we would not, why would we sit by inactive while an NSA employee is in
such a position for historical reasons.  What cold considered logical mot=
could there be to defend him staying?
The public interest and security of society is much bigger than emotive
attachment to being nice to an individual.  An individual for whatever
reason who chose to align himself with NSA.  Sorry for him, but he made t=
choice, informed or not, thats the luck of anyone who worked at NSA and n=
faces the employment record implications.  And companies that were outed =
collaborating or taking or falling for bribes.  There are many victims of
this criminal organization, including their technical employees.
But the open standards process, the biggest victim, has the clear obligat=
to clean house and avoid any association.  Even as participants everythin=
an NSA contributor says will be viewed with extreme caution and scepticis=
To have an NSA personnel as the co-chair is clearly absolutely untennable=
 in
this new reality.
ps Whether or not something untoward happened with Dragonfly isnt really =
principle, often there will be plausible deniability, soft-sabotage, unkn=
collaborators, duped participants etc and there are specific allegations =
academic cryptanalysis as I see you are aware (and various papers).
But lets please put that detail to one side.  Its not personal to dragonf=
either, nor to you.

@_date: 2013-12-22 08:27:03
@_author: Adam Back 
@_subject: Re: [Cfrg] Requesting removal of CFRG co-chair 
Sorry but no.  His employer has been exposed as spending $250m/year to
sabotage amongst other things standards.  Including paying RSA (with its
influence as defacto industry standards author (PKCS# series)) $10m to
include EC_DBRG as a random number generator.  And influencing NIST to
publish the same.  We do not know what other overt or soft-sabotage has b=
going on where.  Note the disclosed information included mention of more
plausibly deniable soft-sabotage - eg architectural decisions pushing
towards less decentralization (concentrated tap points), fragility, and
'accidental' biases that leak keys (like the original DSA flaw that
Bleichenbacker found), and seeming organizational inability to deploy MUS=
forward-secrecy ciphersuites throughout IETF protcools. =20
Kevin is free to resign from his employment or not now he knows these fac=
He surely must have some view on it whether he thinks its a good thing an=
was actively working on it, or a bad thing, but unable to comment.
Regardless I do not think it remotely acceptable in the circumstances tha=
current NSA employees are in any position of administrative control withi=
any public standardization processes period.  Positions of administrative
control are weak points of public policy processes.  Often individuals ha=
insufficient energy to take on the administrative work-load, so they are
relatively easy to fill.  I have seen in the distant past UK MoD (presuma=
GCHQ) people popping up in unlikely positions (eg national health medical
record security standards).
Cleary, and perhaps where your sympathies come from, this could be emotiv=
misread as a slight to Kevin personally.  Its not, its completely imparti=
But its just a thats the risk you take when you work for government and
government is exposed to have committed outrageous, well funded and
persistent societal interest sabotage.  Society hardly needs to defend it=
actions in reacting.  We are the egregiously wronged party.
How also should the US industry players feel, knowing how many $ billions
they are losing to european competitors as a result of this bad faith on =
part of NSA via NIST and abuse or collusion of commercial vendors.  Think
USG or NSA is going to write them a check to compensate?  They were all
What do we have to do to finalize the process of removing?
e core
 I

@_date: 2013-12-21 11:10:42
@_author: Adam Back 
@_subject: soft backdoors: ECDSA vs RSA vs EdDSA (aka EC Schnorr) (Re: BlueHat v13 crypto talks - request for l 
Vaudenay's report writes up an attack developed by Daniel Bleichenbacher
which he presented to some standards groups but did not publish.  As a
result of that the DSA standard was modified.  As I recall with about
1million signatures he could recover the private key due to the small bias
from the formual Peter mentioned: k = G(t,KKEY) mod q ie if |n| = 256-bits
where n is the order of the group, then G(t,KKEY) is distributed with a
rectangular distribution in {0,2^256-1} and q is < 2^256-1.  As I read it
Bleichenbacker did not try that hard to optimize his attack, it was enough
to show the NIST/NSA designed DSA RNG was biased enough to break DSA in a
server / automated environment.
Maybe further optimizations would have been possible...  Maybe this DSA flaw
spotted by Bleichenbacker was another NSA soft-sabotage attempt (making
standards security brittle in the knowledge that it some people will fail to
harden it, and also it gives a plausibly deniable backdoor design for
colluding business entities, or double-agents on the payroll (former NSA
people say)).  In fact DSA was even designed by a former NSA cryptographer.
 (Dr David Kravitz,
a former NSA employee).
The approach I prefer is the deterministic DSA approach where k = MAC(d,M)
where d is the private DSA/ECDSA key and M is the message, plus bias
removal.  Bernsteins EdDSA (which despite the name is actually a Schnorr
signature over an Edwards curve) also uses the same technique.  This is
standardized in an RFC.  If people are going to use DSA/ECDSA they should
use this deterministic DSA.  Personally I prefer EC Schnorr because Schnorr
is just a better, simpler, more secure and more flexible signature (supports
simplel blinding, compact multi-sig, clearer security proofs, better
security margin, less dependence on hash properties etc).  To my mind DSA's
only reason for existence is historic due to patents.  It is inferior by all
metrics to Schnorr, just that Scnorr's patent didnt expire until
 feb 2008.
Anyway as Bernstein has put forward EdDSA with parameters and multiple
security, speed, simple constant time, non-key related, nor message
execution time, and provably non-cooked curve parameters (and there perhaps
remains some needless ambiguity about the magic constants used to seed the
ECDSA parameters) there is no reason in my opinion not to use EdDSA aka EC
Schnorr in any new systems.
Of course RSA is good also, and simpler parameter definition, the main
downside being the large keys for same security margin (3072-bit).

@_date: 2013-12-22 18:43:30
@_author: Adam Back 
@_subject: Re: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: Jim Bidzous of RSA) 
Ask Gwen he wrote the OP.  My response was about the potential complicity not the personnel.  The bit you quoted that I wrote was me putting a ps to point out that Gwen
mispelt his name (and I saw you wrote Bidzous also below - again I believe
its Bidzos).

@_date: 2013-12-21 11:13:58
@_author: Adam Back 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: Jim Bidzous of RSA) 
Its hard to prove unfortunately, unless more leaks come out.  Probably there
exists no documentation to prove or disprove it within RSA, as to whether
RSA knew about the backdoor at the time it signed the deal.  Maybe there
would be documents within NSA.
However what you could say is no one at RSA, or in general, reacted much
following Ferguson et al's pointing out the design issue of there being an
undetectable backdoor in the RNG.
ps I think its Bidzos.

@_date: 2014-01-18 14:31:16
@_author: Adam Back 
@_subject: Re: more NTRU fun: Homomorphic AES Evaluation Using NTRU 
Seems like still 7-orders of magnitude slower than native.  Thats is
progress though and 1-minute for a single AES block might start to have some
niche areas of use if there are no direct algorithms to do whatever it is
that needs to be done.
(Plus a bunch of esoteric crypto stuff and hardness assumptions that might
get weakened over time.)

@_date: 2014-01-08 15:52:29
@_author: Adam Back 
@_subject: Re: Brag About Exploits, Go to Jail 
I love it, pure poetry :)  Go JYA!
(reformatted slightly)

@_date: 2014-01-07 17:07:18
@_author: Adam Back 
@_subject: Re: Come to think of it where in hell is TC May?     Re: Swartz, Weev & radical libertarian lexicon 
Yeah I think this might be a claim by Charlie Stross to having outed T C
May's blog nom-de-plume, or not; its ambiguous.
The article itself is a Charlie Stross bitcoin-hate diatribe.  Amusingly
bitcoin bounced back to $1000 since his doom and gloom of "$500 and falling"
and I personally just bought more at the nice price of $390 and $500, having
confidence in the long term value of the new digital scarcity commodity
class, and its now approximately double that, put in yer pipe Strossy.  (Its
not like fiat doesnt have problems and having banksters eat a largely
unearned big % portion of GDP doesnt exactly benefit society and isnt green
either, they could be doing societally useful work instead).
I happened to notice the T C May reference in the Stross replies to comments
number 15:
not sure if that means he thinks Moldbug is May or the "or" means other
person rather than other name.  Take a gander at the prose style on Mencius
Moldbug's political rants form your own opinion.

@_date: 2014-01-07 15:24:20
@_author: Adam Back 
@_subject: NSA, FBI creep & rule of law, democracy itself (Re: [cryptography] To Protect and Infect Slides) 
This is indeed an interesting and scary question:
maybe the most interesting and portenteous shift in power towards
Orwellianism and totalitarianism in a century, as it affects the
effectiveness of rule of law, and already weak separation of politics from
law enforcement and justice system in the (current though slipping)
super-power with unfortunate aspirations of extra-territorialism and
international bullying.  We're still a few decades from the cross over of
financial dominance to Asia and BRICs, and most of those places are probably
worse than the US by aspiration if thats possible, though less internet
spying budget and capability.  Unless something shapes up towards democracy
in the super-power competitors we're in for a dismal century seemingly.
That the NSA, and now seemingly FBI, see this I think maybe this FBI mission
creep suggests the national security / law enforcement separation is
slipping badly:
so if even the FBI are getting their nose into the tent of unfetter access
to historical data on everyone, plus informal channels and "tip-offs" on
dirt on politically unpopular pepople - eg say effective security
researchers like Applebaum, or effective journalists like Greenwald.  (No
"foreigners" dont feel very comforted, and the explict acknowledgment of
tip-offs, and inforation channels to US domestic and international law
enforcement, basically puts the entire planet at risk of politicaly
motivated interference.)
With retroactive search of your entire lifes electronic foot print including
every "encrypted" IM, skype voip channel, contacts, emails, attorney client
privileged and not, with no warrant or evidence presented to a judge for
subpoena, the Orwell 2.0 system can probably fabricate or concoct trouble
for 99% of the adult population of the planet.  George Orwell 30 years late.
We're pretty close to fucked as a civilization unless something pretty
radical shifts in the political thinking and authorizations.  And
realistically it not even clear the NSA can politically be controlled
anymore by the political system.  Its very hard to influence something with
that much skull-duggery built into its DNA, that many 10s of billions in
outsourced defense contractor lobbying power, that much inertia and will to
survive as an org, with military PSYOPs to turn on its own populace and
political system, and black bag covert ops ties to dirty tricks in CIA, and
judicial and law virtual immunity.  They probably realistically went full
speed ahead since the 11 Sep 2001, if not earlier on such things, and the
scrapping.  TIA wiki
Probably even before since we nominally won the export regulation debacle
and democractic countries were forced to admit it was inconsistent with
their self-perception as open democratic countries, to be controlling and
banning encryption software.  The 21st century equivalent of book burning.
Can we rectify this with the cypherpunks write code?  Maybe as Schneier said
in a discussion on this topic with Eben Moglen (at Moglen's respective
university) maybe we can make it more expensive by deploying more crypto
that is end to end secure, secure by default.  ie more TOFU, more cert
pinning, more certificate transparency distributed cert validation.  Even
the cert valiation maybe behind the game, perhaps NSA really do already have
a lot of actual SSL private keys via hardware, software hacking and
backdoors with manufacturer complicity or not, as well as just demanding
them with NSL orders, gag orders as Lavabit showed finally with evidence.  I
wonder what proportion of SSL certs worldwide the five eyes/Orwell 2.0
shadow orwell 2.0 government have copies of?

@_date: 2014-01-07 20:02:48
@_author: Adam Back 
@_subject: Re: Swartz, Weev & radical libertarian lexicon (Re: Jacob Appelbaum in Germany - Aaron Swartz) 
I imagine its all out there on the wikis or interwebs, thats where I read
it, so here I am just repeating what was written about extensively at the
time.  From memory there was some escalation.  He was detected, blocked,
reacted (mac tumble etc) blocked again, then proceed to enter presumably
restricted areas, hide equipment to bypass limits the admins had placed only
on wifi users, and still download the heck out of it.  It was a big risk and
not very smart move.
What do you think a competent sysadmin would do about on on going security
investigation in that situation - try to find the equipment or say "on noes
the hacker is too smart for us - we capitulate".  So they found the
equipment, called the local cops, and found Aaron when he came to collect
the equipment.  Its not as if he asked their permission, nor that they knew
it was someone even authorized to be on campus or to use JSTOR.
Once that happened it spiraled out of control, even though several
influential faculty knew Aaron, and as I recall his father also worked
there; and various people caved or chickened out of supporting him from what
others said on this threa, and the guy was like depressed obviously about
this situation.  From robin hood white knight hacktivist to soon to be felon
with soulless politicaly motivated fed prosecutor trying to make an example
of him by twisting the max out of some already egregious laws.
So yes I am against the state subsidized "copyright" censorship of some bit
strings, patents and monstrosities like CFAA, so I support Aaron's political
objective of his hactivism target there (and on the previous one) but Aaron
did screw up a bit also.  Maybe he got the wrong message from dodging a
justice system bullet on the previous succesful hactivism with the legally
liberated tax payer owned legal dox.

@_date: 2014-01-07 16:48:09
@_author: Adam Back 
@_subject: Re: Swartz, Weev & radical libertarian lexicon (Re: Jacob Appelbaum in Germany - Aaron Swartz) 
Yeah but my point was James (and multiple others) were ever thus, and yet if
you catch them focussed on something useful they are capable, even gifted
some of them.  A guy can say a bunch of things on political rambling, its
not like you'd expect political uniformity or political correctness, or
mellow, nuanced balanced views out of a bunch of crypto-anarchists: the
political side of this list could always be like a USENET flame fest at its
worst.  Flame-retardent underwear and 'n' button at the ready.  Read the
bits that interest you, skip the rest.  It just amuses me, there goes (some
person prone to such rants) again, on some pet topic or highly non-PC, or
offensive to sensitive ears diatribe, smirk, time to use the 'n' button
(next email).
If you dont like the noise, create some signal is another concept (and dont
fan the noise).
This kind of level of argument was however interspersed with some highly
interesting technical innovations like remailers, ecash, anonymity networks,
smart-contracts, pseudonymity theory and political implications thereof etc.
Sometimes I think the younger generation missed out on USENET flame wars as
a comparative baseline of signal/noise ratio and civilized discourse ;)

@_date: 2014-01-07 15:31:29
@_author: Adam Back 
@_subject: Re: Swartz, Weev & radical libertarian lexicon (Re: Jacob Appelbaum in Germany - Aaron Swartz) 
Sure has.  Welcome back to the party RAH :)
Bitcoin aka actual digital bearer certs & mostly irrevocable but not quite
blind ecash deployed, Snowden triggered NSA self-sabotage to unwind the
post-911 privacy pall, mountains of evidence that the most paranoid
cypherpunk imaginations are fully real and worse, US security researchers
and journalists taking overseas residence to escape harrassment or worse.
Interesting time for enciphering cypherpunkly minds.
Heh, thats probably true given in your own propensity to engage in fun flame
wars :)

@_date: 2014-01-07 11:50:38
@_author: Adam Back 
@_subject: Swartz, Weev & radical libertarian lexicon (Re: Jacob Appelbaum in Germany - Aaron Swartz) 
Dont worry about James hyperbole, he's just channeling Tim May who was one
of the three or four list co-founders, wrote the cyphernomicon [1], and had
a habit of using that phrase 'needed killing' now and then, as I recall as
phrase to express his distaste for someone's actions.  Its an expression,
not something literal... but James' black & white, non-PC, absolutist
personality precludes him saying that :)  You just have to read it with a
USENET flame war mentality and parse for what he's actually saying.
Apart from the refusal to bow to PC, James is actually a pretty smart guy
from what I recall.  He implemented some simplifed UX, ECC crypto email
stuff called 'crypto kong' [2] way back in 1997.
Cypherpunks write code & all that, gives James some brownie points.
About Aaron's case and suicide, it seems to me that Aaron miscalculated, and
the hacking was pretty escalated, engaged in multiple escalating
counter-measures when it was obvious the sysadmins were on to him as an
intruder, he didnt back off but took it to the next level including physical
intrusion & hiding equipment.  But MIT (and to a lesser extent JSTOR) let
him down badly as did some of his academic friends and its tragic that he
was a victim of some extremely over reaching imbalanced law the CFAA [3],
aggressively prosecuted by self-agrandizing politically motivated, and
almost legally immune deeply flawed US federal prosecution and plea bargain
system, which also saw Weev [4] put in jail over the most ridiculous and
egregious abuse of law (noticing a defect in AT&T web site and giving the
information to the media).  Yes Weev enjoys trolling, but thats an art-form
and since when has unpopular speech been illegal, freedom of speech means
unpopular speech too.  Aaron's earlier hacktivism was pretty spectacularly
successful in demonstrating the stupidity of charging for access to publicly
funded legal information, in a way that ultimatey they could find no legal
fault with, though the feds were not doubt pretty pissed that they couldnt
get him for anything.  But even the legal dox hacktivism stunt was very high
risk, the US legal system is hard to rely on, even when you are doing legal
but politically unpopular to things to a subset of the higher echelons of
office holder.  It seems to me that particularly in the US the
political/legal system tends to hold grudges and fail spectacularly at
balance and impartiality and legal independence from political influence. Its better than Russia still, but its falling in world rankings of rule of
law and political indendence for sure.  There are probably some independent
rankings on this aspect of the government/jurisdiction comparison.
[1] [2] [3] en.wikipedia.org/wiki/Computer_Fraud_and_Abuse_Act
[4]

@_date: 2014-01-09 11:03:57
@_author: Adam Back 
@_subject: Re: hacker != cracker (Re: Swartz, Weev & radical libertarian lexicon) 
How about a disclosure post-mortem, cant be sued, jailed or assets frozen
then.  Its maybe an interesting thought if there are some quite old or
life-threated medical problems people who coincidentally are subject to gags
or self-imposed silence for personal safety for things they strongly feel
should be in the public knowledge..

@_date: 2014-01-08 16:03:39
@_author: Adam Back 
@_subject: Re: hacker != cracker (Re: Swartz, Weev & radical libertarian lexicon) 
What you said is correct, that is what needs to happen (society and law
needs to move out of the dark ages), and the only way for that to happen
is brave canaries with squeaky clean reps, and sharp lawyers to blaze the
My version was just to say be aware of the risks, that you would take by
even putting your name to a hack, with any disclosure at all.  If you dont
want to be a canary.
Possibly would be advisable to use a laywer for some anonmyity insulation to
even sell a hack to one of the disclosure service pimping sites.  (They
probably are selling them to the NSA/Orwell 2.0 crew so taking their money is
probably dirty money.)
Independent security researcher can be risky.  Get a legal signed doc from
the people you audit people say (yeah like they're gonna give you one for an
unsolicited investigation).
Weev was an independent security researcher after all, in a team even. Goatse security   They did find
some interesting and news worthy hacking stuff, even won awards from Tech
Crunch seemingly.

@_date: 2014-01-07 22:15:52
@_author: Adam Back 
@_subject: Re: hacker != cracker (Re: Swartz, Weev & radical libertarian lexicon) 
A cryptographically secure pseudonym would probably work even better.  Weev
didnt actually do anything wrong that I could see, by any sane
interpretation of even something as egregious as CFAA and he's serving 41
months.  A lawyer is a last resort, step  is not identifying yourself even
for non-malicous research I suspect.  Probably the biggest risk is the incompatibility of real-space bragging
rights to the discovery for people who like to speak at conferences.

@_date: 2014-01-07 21:20:08
@_author: Adam Back 
@_subject: hacker != cracker (Re: Swartz, Weev & radical libertarian lexicon) 
Not that its relevant to the Aaron discussion, but I think you got that
etymology sequence wrong, the original meaning of hacker was more like doing
clever but non-malicious things with computers, aka squeezing interesting
things out of them that they were not intended or expected to do.  And/or
relatedly people were less uptight about computer access as most of them
were in open collaborative university settings so using computers was less
of a locked up possessive mind set.  Those were the days before CFAA and
Weev getting his door kicked in by a swat team for stumbling upon a broken
network API.
Hacker in the sense of cracker was a later and much hated co-option and
perversion of the term.  I expect that's what Rysiek was reacting to partly.
Seems like the original hackers lost that etymology battle however long ago.

@_date: 2014-01-09 11:09:18
@_author: Adam Back 
@_subject: godwins law, detweiler, and anons... awesome (Re: the idiot cari machet 
Awesome, some anon-remailer input also :)  Godwins law invocation to boot. We've got it all now.  Well it'd be nice if T C May would come back to the party, other than that.
ps I never got the fascination with detweiler nor baiting, and ultimately
apparently self-destructing him.  I guess I must've hit 'n' too many times
at the time because I never heard of the reply-block hack (though that is
kind of cool).  Probably overly mean if he was mentally unstable though to
sabotage his job.

@_date: 2014-01-11 13:47:38
@_author: Adam Back 
@_subject: base58 vs alt-alpha base64? (Re: Re: Curve p25519 Replacements for GnuPG?(x2 now) Re: Pretty Curved 
Bitcoin base58 seemed a to have some minor unfortunate side effects to me,
the intent is good to avoid transcription error, but surely one could find
64-chars.  it could have easily been base 60 to start with (dont delete both
0 and O, and 1 and l just make the equivalent!).  Then you have URL encoding
ambiguity, C/python/bash programming string quoting that rules out some more
non alphanum chars.  (base 64 includes +/).  Just seems some ugly code mess
and implications for vanity address etc to deal with non-power-of-2

@_date: 2014-03-21 19:12:53
@_author: Adam Back 
@_subject: anyone have libtech archives? 
For old cpunks, who may have been subscribed to libtech:
I am curious to lookup some discussion around b-money, bit-gold those parts
which were not posted on cpunks at the time.  This would be like 1997-2005
say.  (I believe it is different to, or an earlier incarnation of the
libertech list, at least the archives on that list dont go back that far.)
(Presuming that it was not a private/invite only list!  If so offlist doc

@_date: 2014-03-21 12:25:42
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
According to Bernstein it's patent FUD (expired, ancient and solid prior

@_date: 2014-03-21 10:59:06
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
Maybe its time to explore raw ECDSA signed message based certs.
btw I dont think its quite 4kB.  eg bitpay's looks to be about 1.5kB in der
format.  And they contain a 2048-bit RSA server key, and 2048-bit RSA
signatures (256byte each right there = 512bytes).  And even 2048 is weaker
than 256-bit ECDSA.

@_date: 2014-03-20 12:12:21
@_author: Adam Back 
@_subject: [Bitcoin-development] Payment Protocol for Face-to-face Payments 
Whats a sensible limit on practical/convenient QR code size?
How much of the payment protocol message size comes from use of x509?
(Just exploring what the options are).

@_date: 2014-04-03 03:29:47
@_author: Adam Back 
@_subject: Re: Geoff Stone, Obama's Review Group 
That review does not really jive with the complaints from the people on the
intelligence oversight committee, the NSA leaders who explicitly lied to
congress, the complaint from the author of the patriot act that he thought
the NSA was breaking the law, and the subsequently released complaints from
the FISA court about illegal and non-credible abusive interpretations of
the law and the FISA courts instructions.  Also there were several reports
saying nothing of consequence was prevented by the entire program.
If you know him, maybe you want to ask him how he reconciles all these now
matter of public record conflicts with what he just said.
Of course he did leave wiggle room in the form of "[not] so clearly
unlawful that it would have been appropriate for the NSA to refuse to
fulfill its responsibilities." ie it was only somewhat illegal so hey thats
ok then?
Have to say it seems more plausible to me that they did a faux-forthright
job of answering questions from this new review process.  I mean what else
could they do?  Stonewall?

@_date: 2015-01-01 18:40:30
@_author: Adam Back 
@_subject: Re: [cryptography] John Gilmore: Cryptography list is censoring my emails 
nah what am I thinking probably! 1988 if not earlier, 27 years :)
The point is block lists suck, they're always blocking false things,
and vigilante abusive takes 3x longer to take you off than for you to
complain or unresponsive etc.
They'll also falsely block you not because your config is insecure but
because it doesnt match their preferred configuration.  Quite
irritating if you ever tried running your own mail server.
cryptography mailing list

@_date: 2015-01-01 18:12:15
@_author: Adam Back 
@_subject: Re: [cryptography] John Gilmore: Cryptography list is censoring my emails 
He's been running an open relay since like 2000 or something... why
not its his relay.
cryptography mailing list

@_date: 2015-03-25 11:56:21
@_author: Adam Back 
@_subject: Re: maidsafe 
btw apparently maidsafe also patented some things.  Not a fan of
patents really (bit of an understatement - IMO they should be banned).
Maidsafe took some flak for it and tried to claim they were defensive
patents.  I think the misunderstanding is that when startups fail,
patents get sold to the highest bidder.  Ie the entrepreneur who
thinks its a useful thing to do creates 30 years of headache for an
ecosystem from his 2 year time-horizon thinking.  We've even seen it
before in ecash specifically with the digicash patents that were sold
at bankruptcy to infospace and so there was a period where no one
could use basic blind sigs and various work arounds were tried
(blinding agnostic server, Wagner's blind MAC/ZKP/Lucre,
server-privacy/systemix/ricardian server).  That sucked.
I am not sure about Maidsafe.  But there are a lot of scams in
alt-coin space.  Its very easy to take investors money and then fail
to deliver.  The investors are non-qualified investors, so the
legality is also questionable.  But even on an ethical basis, the
investors are not having legal or professional review of the
prospectus, and the "investment contract" is typically ridiculous such
that a professional would ROFL about the proposal.  You own nothing.
Its a pattern repeated a few times in alt-coin space.
The other fallacy in my view is that this is somehow plausible that a
service (aka app-coin) with value could defend a floating valued
alt-coin.  Lets say maidsafe as an example - so far I guess its
vaporware, or under research & development vs zooko's LAFS for example
which has been running and incrementally improving for years.  But
lets say they manage to develop something useful with usable
functionality and reliability etc which is no small task, lets say
they get workably close to matching LAFS functionality after spending
the $10m or whatever they raised.  Now why would people use it over
LAFS which is free?
If maidsafe offered better functionality than LAFS (seems doubtful but
hypothetically) its FOSS software.  Why would someone not fork it and
remove the maidsafe token.  The resources that provide the service are
after all not provided by maidsafe nor the holders of the maidsafe
coins - so why would users and peers in the network choose to support
the enrichment of maidsafe the company nor the naive people who put
money into the "investment".  You often hear people talking about
these schemes as "donations" and thats probably closer to the truth -
if you think the tech is interesting and you donate some money to it
to see it get built, without expectations of getting your money back,
you're going to get less of an unpleasant surprise when it fails to
materialise or it simply gets forked if it even works.
I can see that Zooko for example might look at this and go huh? WTF?
He implemented LAFS with various modest funding models and has a
working system - and yet some folks with hand wavy ideas that may or
may not be mathematically possible even jump into the tech space paint
an exciting hypothetical system picture and grab $10m+ of
non-qualified investor money with an "investment contract" that says
the investor owns nothing (other than sort of undefined value service
tokens, that are not backed by control or ownership of the resources
that might operate the to-be-implemented service).
If nothing else these token sale contracts are fraught with moral
hazard.  Investment contracts are structured the way they are by
mutual negotiation between investor and startup for reasons of
interest alignment and incentive.  Those structures were arrived at
via 100+ years of experience of what works and what doesnt, and prior
generations investment scams and bubbles.  It seems like a bit of a
rerun of some early last century investment scams that motivated the
regulations we currently have to protect investors from scammers.
(Someone did ask, thats my opinion anyway:)

@_date: 2015-06-28 22:16:30
@_author: Adam Back 
@_subject: Re: [Bitcoin-development] questions about bitcoin-XT code fork & non-consensus hard-fork 
It's not the miners that count, rather than economic majority.  It's a
surprising fact, but here's how it works: lets imagine 75% of the
miners decided they'd change the economic rules, in a protocol
incompatible way.  Result: the miners form a new alt-coin with no
users.  Bitcoin difficulty adjusts, and carries on as if nothing
happened.  The hostile miners earn 25 forkcoins which have  a market
price of 0.  They are burning electricity so they either go bankrupt
or the give up and rejoin the network.
There's a lot to game theory that is subtle.  It could do with a FAQ
writing on it really.

@_date: 2015-06-19 05:07:44
@_author: Dr Adam Back 
@_subject: Re: [Bitcoin-development] questions about bitcoin-XT code fork & non-consensus hard-fork 
Its clear Gavin knows more about Bitcoin code and detailed micro
algorithms than I do (there are many detailed algorithms for anti-DoS
etc at code level which I do not know).
Its possible I know more than Gavin or have a better internalised
reasoning about the logic and design parameters for about
decentralised systems and distributed trust systems, and ecash
protocols, threat models in p2p privacy systems - which is quite a big
slice of what Bitcoin is trying to do.  Or not - I dont know all of
Gavin's expertise nor career experience!  Something you may not
realise is a bunch of us on the cypherpunks list back in like
1995-2005 spent a lot of applied research effort into finding a way to
do something with the characteristics of bitcion.    My PhD is in
distributed systems also.
Anyway I do not mean to have claims to authority, particularly because
I believe firmly in pure meritocracy philosophically and detest such
argumentation as a failure of reason, but coincidentally I do actually
know something about it and worked on it on Bitcoin-like system design
and p2p novel trust-model & security model on and off for 20 years.
But I do think people who are proposing big-blocks are underestimating
and being super-optimistic about a range of things, almost to naive
extent.  I am not imputing unsaid things, Gavin wrote many blog posts
on these topics.  Mike Hearn made some videos and posts about his
views, and they are quite disconnected from p2p privacy system design
thinking.  Someone should probably respond to some of those posts to
clarify why they think some of these assumptions are incorrect and
optimistic to prior experience and precedent.

@_date: 2015-06-17 22:51:54
@_author: Dr Adam Back 
@_subject: Re: [Bitcoin-development] questions about bitcoin-XT code fork & non-consensus hard-fork 
I would recommend to read the post.  I thought it was fairly
comprehensive and this is extremely bad both in network fork risk
(which not everyone may understand details of as its quite intricate).
Hard-forks should only be done with wide-spread consensus, and are
fairly risky even then, but by doing it in a contentious divisive way
- understand everyone in the technical community is very concerned -
just  unnecessarily magnifies the risk of failure.  The other major
issues being the precedent set and loss of decentralised code
governance described in the post.  A "distributed system" which has
one or two developers, one who has a slight history for proposed a
number of objectionable things in the past (red-lists etc) is not
really distributed.  How do we know other than assumption that they
are not taking money to push preferred features, or under
duress/blackmail etc.  This is the point of the existing code change
approval system to review and cross check against things like that.
If people on *cypherpunks* cant get the points in the post, I think
the world has a problem.  The price of security in a distributed
system like bitcoin is eternal vigilance, but if people dont
understand what constitutes a risk and hence what to be vigilant for,
the meta-system can be unreliable and lose its assurances.  I think we
need to explain some more concepts and probably people will over time
learn things and and an influencer pyramid emerge as happened in
privacy technology.

@_date: 2015-06-15 18:03:25
@_author: Adam Back 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & non-consensus hard-fork 
Hi Mike
Well thank you for replying openly on this topic, its helpful.
I apologise in advance if this gets quite to the point and at times
blunt, but transparency is important, and we owe it to the users who
see Bitcoin as the start of a new future and the$3b of invested funds
and $600m of VC funds invested in companies, we owe it to them that we
be open and transparent here.
I would really prefer on a personal nor professional basis to be
having this conversation period, never mind in public, but Mike - your
and Gavin's decision to promote a unilateral hard-fork and code fork
are extremely high risk for bitcoin and so there remains little
choice.  So I apologise again that we have to have this kind of
conversation on a technical discussion list.  This whole thing is
hugely stressful and worrying for developers, companies and investors.
I strongly urge that we return to the existing collaborative
constructive review process that has been used for the last 4 years
which is a consensus by design to prevent one rogue person from
inserting a backdoor, or lobbying for a favoured change on behalf of a
special interest group, or working for bad actor (without accusing you
of any of those - I understand you personally just want to scale
bitcoin, but are inclined to knock heads and try to force an issue you
see, rather than work collaboratively).
For you (and everyone)
- Should there be a summit of some kind, that is open attendance, and
video recorded so that people who are unable to attend can participate
too, so that people can present the technical proposals and risks in
an unbiased way?
(It is not theoretical question, I may have a sponsor and host - not
Blockstream, an independent, its a question for everyone, developers,
users, CTOs, CEOs.)
So here I come back to more frank questions:
The rest of the developers are wise to realise that they do not want
exclusive control, to avoid governance centralising into the hands of
one person, and this is why they have shared it with a consensus
process over the last 4 years.  No offence but I dont think you
personally are thinking far enough ahead to think you want personal
control of this industry.  Maybe some factions dont trust your
motives, or they dont mind, but feel more assured if a dozen other
people are closely reviewing and have collective review authority.
- Do you understand that attempting to break this process by
unilateral hard-fork is extremely weakening of Bitcoin's change
governance model?
- Do you understand that change governance is important, and that it
is important that there be multiple reviewers and sign-off to avoid
someone being blackmailed or influenced by an external party - which
could potentially result in massive theft of funds if something were
- Secondarily do you understand that even if you succeed in a
unilateral fork (and the level of lost coins and market cap and damage
to confidence is recoverable), that it sets a precedent that others
may try to follow in the future to introduce coercive features that
break the assurances of bitcoin, like fungibility reducing features
say (topically I hear you once proposed on a private forum the concept
of red-lists, other such proposals have been made and quickly
abandoned), or ultimately if there is a political process to obtain
unpopular changes by unilateral threat, the sky is the limit - rewrite
the social contract at that point without consensus, but by
calculation that people will value Bitcoin enough that they will
follow a lead to avoid risk to the system?
As you probably know some extremely subtle bugs in Bitcoin have at
times slipped past even the most rigorous testings, often with
innocuous but unexpected behaviours, but some security issues  Some
extremely intricate and time-sensitive security defect and incident
response happens from time to time which is not necessarily publicly
disclosed until after the issue has been rolled out and fixed, which
can take some time due to the nature of protocol upgrades,
work-arounds, software upgrade via contacting key miners etc.  We
could take an example of the openSSL bug.
- How do you plan to deal with security & incident response for the
duration you describe where you will have control while you are
deploying the unilateral hard-fork and being in sole maintainership
- Are you a member of the bitcoin security reporting list?
As you know the people who have written 95% of the code (and reviewed,
and tested, and formally proved segments etc) are strenuously advising
not to push any consensus code into public use without listening to
and addressing review questions which span beyond rigorous code &
automated guided fuzz testers, simulation and sometimes formal proofs,
but also economics, game-theory and critically very subtle
determinism/consensus safety that they have collectively 4-5 years
experience of each.
- Will you pause your release plans if all of the other developers
insist that the code or algorithm is defective?
- Please don't take this the wrong way, and I know your bitcoinj work
was a significant engineering project which required porting bitcoin
logic.  But If the answer to the above question is no, as you seemed
to indicate in your response, as you not have not written much bitcoin
core code yourself (I think 3 PRs in total), do you find yourself more
qualified than the combination of peer review of the group of people
who have written 95% of it, and maintained it and refactored most of
it over the last 4-5 years?
I presume from your security background you are quite familiar with
the need for review of crypto protocol changes & rigorous code review.
That is even more the case with Bitcoin given the consensus
That you are frustrated, is not a sufficient answer as to why you are
proposing to go ahead with a universally acknowledged extreme network
divergence danger unilateral hard-fork, lacking wide-spread consensus.
People are quite concerned about this.  Patience, caution and prudence
is necessary in a software system with such high assurance
So I ask again:
- On the idea of a non-consensus hard-fork at all, I think we can
assume you will get a row of NACKs.  Can you explain your rationale
for going ahead anyway?  The risks are well understood and enormous.
Note the key point is that you are working on a unilateral hard-fork,
where there is a clear 4 year established process for proposing
improvements and an extremely well thought out and important change
management governance process.  While there has been much discussion,
you nor Gavin, have not actually posted a BIP for review.  Nor
actually was much of the discussion even conducted in the open: it was
only when Matt felt the need to clear the air and steer this
conversation into the open that discussion arose here.  During that
period of private discussion you and Gavin were largely unknown to
most of us lobbying companies with your representation of a method
that concerns everyone of the Bitcoin users.  Now that the technical
community aware aware they are strenuously discouraging you on the
basis of risks.
- Do you agree that bitcoin technical discussions should happen in the open?
- As this is a FOSS project, do you agree that companies should also
be open, about their requirements and trade-offs they would prefer?
- Can you disclose the list of companies you have lobbied in private
whether they have spoken publicly or not, and whether they have
indicated approval or not?
- Did you share a specific plan, like a BIP or white paper with these
companies, and if so can we see it?
- If you didnt submit a plan, could you summarise what you asked them
and what you proposed, and if you discussed also the risks?  (If you
asked them if they would like Bitcoin to scale, I expect almost
everyone does, including every member of the technical community, so
that for example would not fairly indicate approval for a unilateral
I and others will be happy to talk with the CTO and CEOs of companies
you have lobbied in private, for balance to assure ourselves and the
rest of the community that their support was given - and with full
understanding of the risks of doing it unilaterally, without peer
review, benefit of maintenance and security inidence management, and
what exactly they are being quoting as having signed up for.
(This maybe more efficiently and openly achieved by the open process,
on a mailing list, maybe a different one even special purpose to this
topic, with additional option of the open public meeting I proposed at
the top).
- Do you agree that it would be appropriate, that companies be aware
of both the scaling opportunities (of course, great everyone wants
scalability) as well as the technical limits and risks with various
approaches?  And that these be presented by parties from a range of
views to ensure balance?
- Do you consider your expression of issues to hold true to the ideal
of representing balanced nuanced view of all sides of a technical
debate, even when under pressure or feeling impatient about the
You may want to review the opening few minutes of your epicenter 82
bitcoin for example where you claimed and I quote "[the rest of the
technical community] dont want capacity to ever increase and want it
to stay where it is and when it fills up people move to other
- Do you think that is an accurate depiction of the complex trade-offs
we have been discussing on this list?
(For the record I am not aware of a single person who has said they do
not agree with scaling Bitcoin.  Changing a constant is not the
hard-part.  The hard part is validating a plan and the other factors
that go into it.  It's not a free choice it is a security/scalability
tradeoff.  No one will thank us if we "scale" bitcoin but break it in
hard to recover ways at the same time.)
- Were you similarly balanced in your explanations when talking to
companies in private discussions?
- Do you understand that if we do not work from balanced technical
discussion, that we may end up with some biased criteria?
Neither you nor Gavin have any particular authority here to speak on
behalf of Bitcoin (eg you acknowledge in your podcast that Wladimir is
dev lead, and you and Gavin are both well aware of the 4 year
established change management consensus decision making model where
all of the technical reviewers have to come to agreement before
changes go in for security reasons explained above).  I know Gavin has
a "Chief Scientist" title from the Bitcoin Foundation, but sadly that
organisation is not held in as much regard as it once was, due to
various irregularities and controversies, and as I understand it no
longer employs any developers, due to lack of funds.  Gavin is now
employed by MIT's DCI project as a researcher in some capacity.  As
you know Wladimir is doing the development lead role now, and it seems
part of your personal frustration you said was because he did not
agree with your views.  Neither you nor Gavin have been particularly
involved in bitcoin lately, even Gavin, for 1.5 years or so.
- Do you agree that if you presume to speak where you do not have
authority you may confuse companies?
But I think this is a false dichotomy.  As I said in previous mail I
understand people are frustrated that it has taken so long, but it is
not the case that no progress has been made on scalability.
I itemised a long list of scalability work which you acknowledged as
impressive work (CPU, memory, network bandwidth/latency) and RBF, CPFP
fee work, fee-estimation, and so on, which you acknowledged and are
aware of.
There are multiple proposals and BIPs under consideration on the list right now.
- what is the reason that you (or Gavin) would not post your BIP along
side the others to see if it would win based on technical merit?
- why would you feel uniquely qualified to override the expert opinion
of the rest of the technical community if your proposal were not
considered to have most technical merit? (Given that this is not a
simple market competition thing where multiple hard-forks can be
considered - it is a one only decision, and if it is done in a
divisive unilateral way there are extreme risks of the ledger
Network Divergence Risk
But this is not a soft-fork, it is a hard-fork.  Miner voting is only
peripherally related.  Even if in the extremis 75% of miners tried a
unilateral hard-fork but 100% of the users stayed on the maintained
original code, no change would occur other than those miners losing
reward (mining fork-coins with no resale value) and the difficulty
would adjust.  The miners who made an error in choice would lose money
and go out of business or rejoin the chain.
However if something in that direction happens with actual users and
companies on both sides of it users will lose money, the ledger will
diverge as soon as a single double-spend happens, and never share a
block again, companies will go instantly insolvent, and chaos will
break out.  This is the dangerous scenario we are concerned about.
So the same question again:
- How do you propose to deal with the extra risks that come from
non-consensus hard-forks?  Hard-forks themselves are quite risky, but
non-consensus ones are extremely dangerous for consensus.
Being sensitive to alarming the market
It is something akin to Greece or Portugal or Italy exiting the euro
currency in a disorderly way.  Economists and central bank policy
makers are extremely worried about such an eventuality and talk about
related factors in careful, measured terms, watch Mario Draghi when he
Imagine that bitcoin is 10x or 100x bigger.  Bitcoin cant have people
taking unilateral actions such as you have been proposing.  It is not
following the consensus governance process, and not good policy and it
is probably affecting bitcoin confidence and price at this moment.
This is not a soft-fork, and the community will not want to take the
risks once they understand them, and they have months in which to
understand them and at this point you've motivated and wasted 100s of
developer man hours such that we will feel impelled to make sure that
no one opts into a unilateral hard-fork without understanding the
risks.  It would be negligent to allow people to do that.  Before this
gets very far FAQs will be on bitcoin.org etc explaining this risk I
would imagine.  Its just starting not finished.
What makes you think the rest of the community may not instead prefer
Jeff Garzik's BIP after revisions that he is making now with review
comments from others?
Or another proposal.  Taken together with a deployment plan that sees
work on decentralisation tying into that plan.
- If you persisted anyway, what makes you think bitcoin could not make
code changes defensively relating to your unilateral fork?
(I am sure creative minds can find some ways to harden bitcoin against
a unilateral fork, with a soft-fork or non-consensus update can be
deployed much faster than a hard-fork).
I tried to warn Gavin privately that I thought he was under-estimating
the risk of failure to his fork proposal due to it being unilateral.
Ie as you both seem sincere in your wish to have your proposal
succeed, then obviously the best way to do that is to release a BIP in
the open collaborative process and submit it to review like everyone
else.  Doing it unilaterally only increases its chance of failure.
The only sensible thing to do here is submit a BIP and stop the
unilateral fork threat.
Scalability Plans
Yes people have proposed other plans.  Bryan Bishop posted a list of them.
Jeff Garzik has a proposal, BIP-100 which seems already better than
Gavin's having benefit of peer review which he has been incorporating.
I proposed several soft-fork models which can be deployed safely and
immediately, which do not have ledger risk.
I have another proposal relating to simplified soft-fork one-way pegs
which I'll write up in a bit.
I think there are still issues in Jeff's proposal but he is very open
and collaborating and there maybe related but different proposals
It does not seem to me that you understand the issue.  Of course they
want to increase the scalability of bitcoin.  So does everyone else on
this mailing list.
That they would support that is obvious.  If you presented your
unilateral action plan without explaining the risks too.
I think I covered this further above.  If you would like to share the
company list, or we can invite them to the proposed public physical
meeting, I think it would be useful for them to have a balanced view
of the ledger divergence risks, and alternative in-consensus proposals
underway, as well as the governance risks, maintenance risks, security
incident risks.
Note that other people talk to companies too, as part of their day to
day jobs, or from contacts from being in the industry.  You have no
special authority or unique ability to talk with business people.  Its
just that the technical community did not know you were busy doing
I can not believe that any company that would listen to their CTO, CSO
or failing that board would be ok with the risks implied by what you
are proposing on full examination.
I know you want scale bitcoin, as I said everyone here does. I think
what you're experiencing is that you've had more luck explaining your
pragmatic unilateral plan to non-technical people without peer review,
and so not experienced the kind of huge pushback you are getting from
the technical community.  The whole of bitcoin is immensely
complicated such that it takes an uber-geek CS genius years to
catchup, this is not a slight of any of the business people who are
working hard to deploy Bitcoin into the world, its just complicated
and therefore not easy to understand the game-theory, security,
governance and distributed system thinking.  I have a comp sci PhD in
distributed systems, implemented p2p network systems and have 2
decades of applied crypto experience with a major interest in
electronic cash crypto protocols, and it took me a several years to
catchup and even I have a few hazy spots on low-level details, and I
addictively into read everything I could find.  Realistically all of
us are still learning, as bitcoin combines so many fields that it
opens new possibilities.
What I am expecting that yourself and Gavin are thinking is that
you'll knock heads and force the issue and get to consensus.
However I think you have seriously misjudged the risks and have not
adequately explained them to companies you are talking with.  Indeed
you do not fully seem to acknowledge the risks, nor to have a well
thought out plan here of how you would actually manage it, nor the
moral hazards of having a lone developer in hugely divisive
circumstances in sole control of bitcoins running code.  Those are
exactly the reasons for the code change governance process!
Even though you are trying to help, the full result is you are not
helping achieve anything by changing a constant and starting a
unilateral hard-fork (not to trivialise the work of making a patch to
do that).
The work to even make the constant change be feasible was a result of
1000s of hours of work by others in the development community, that is
emphatically and unilaterally telling you that hard-forks are hugely
You are trying to break the code change governance security procedure
that were put in place for good reason for the security of $3b of
other peoples money, even if you have a pragmatic intent to help, this
is flat out unacceptable.
There are also security implications to what you are proposing, which
I have heard you attempting to trivialise, that are core to Bitcoins
security and core functionality.
I think this is a significant mischaracterisation, and I think almost
everybody is on board with a combination plan:
1. work to improve decentralisation (specific technical work already
underway, and education)
2. create a plan to increase block-size in a slow fashion to not cause
system shocks (eg like Jeff is proposing or some better variant)
3. work on actual algorithmic scaling
In this way we can have throughput needed for scalability and security
work to continue.
As I said you can not scale a O(n^2) broadcast network by changing
constants, you need algorithmic improvements.
People are working on them already.  All of those 3 things are being
actively worked on RIGHT NOW, and in the case of algorithmic scaling
and improve decentralisation have been worked on for months.
You may have done one useful thing which is to remind people that
blocks are only 3x-4x below capacity such that we should look at it.
But we can not work under duress of haste, nor unilateral ultimatums,
this is the realm of human action that leads to moral hazard, and
ironically reminds us of why Satoshi put the quote in the genesis
Bitcoin is too complex a system with too much at stake to be making
political hasty decisions, it would be negligent to act in such a way.
Again please consider that you did your job, caused people to pay
attention, but return to the process, submit a BIP, retract the
unilateral hard-fork which is so dangerous and lets have things be
calm, civil and collaborative in the technical zone of Bitcoin and not
further alarm companies and investors.

@_date: 2015-06-15 00:04:44
@_author: Adam Back 
@_subject: [Bitcoin-development] questions about bitcoin-XT code fork & non-consensus hard-fork 
I understand why you would be keen to scale bitcoin, everyone here is.
But as you seem to be saying that you will do a unilateral hard-fork,
and fork the code-base simultaneously, probably a number of people
have questions, so I'll start with some:
( I noticed some of your initial thoughts are online here
 or the full podcast
 )
- Are you releasing a BIP for that proposal for review?
- If the reviewers all say NACK will you take on board their suggestions?
- On the idea of a non-consensus hard-fork at all, I think we can
assume you will get a row of NACKs.  Can you explain your rationale
for going ahead anyway?  The risks are well understood and enormous.
- How do you propose to deal with the extra risks that come from
non-consensus hard-forks?  Hard-forks themselves are quite risky, but
non-consensus ones are extremely dangerous for consensus.
- If you're going it alone as it were, are you proposing that you will
personally maintain bitcoin-XT?  Or do you have a plan to later hand
over maintenance to the bitcoin developers?
- Do you have contingency plans for what to do if the non-consensus
hard-fork goes wrong and $3B is lost as a result?
As you can probably tell I think a unilateral fork without wide-scale
consensus from the technical and business communities is a deeply
inadvisable.  While apparently some companies have expressed interest
in increased scale, I can only assume they do no yet understand the
risks.  I suggest before they would actually go ahead that they seek
independent advice.
Of the overall process, I think you can agree we should not be making
technical decisions with this level of complexity and consensus risk
with financial implications of this magnitude under duress of haste?
This seems otherwise a little like the moral hazard of the 2008
financial collapse that Satoshi put the quote in the genesis block
I think its best that we progress as Jeff Garzik has done to have
engineering discussions centre around BIPs, running code for review,
simulation and careful analysis.
I understand this has been going on for a long time, and some people
are frustrated with the rate of progress, but making hasty,
contentious or unilateral actions in this space is courting disaster.
Please use your considerable skills to, along with the rest of the
community, work on this problem collaboratively.
I can sincerely assure you everyone does want to scale bitcoin and
shares your long term objective on that.
