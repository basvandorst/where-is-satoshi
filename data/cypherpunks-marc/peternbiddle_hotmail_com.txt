
@_date: 2002-08-07 03:42:12
@_author: "Peter N. Biddle" 
@_subject: Re: USENIX Security TCPA/Palladium Panel Wednesday 
I consider it a Bad Thing that we don't have more clearly organized
technical documentaion to show right now, and I can only say that we are
working on providing this post haste. I certainly am not happy to be
pointing you to blogs as primary sources. I apologize for this, and I will
send stuff out to this alias when we have it.
----- Original Message -----
X-Orig-To: "AARG!Anonymous" Sent: Tuesday, August 06, 2002 4:57 PM

@_date: 2002-08-07 02:08:25
@_author: "Peter N. Biddle" 
@_subject: Re: Privacy-enhancing uses for TCPA 
Neither of us really had the time to clearly articulate things last time, so
I am glad you brought it up. My perspective is primarily from an
architectural one, and it boils down to this:
Platform security shouldn't choose favorites.
I don't want there to be any second class data citizens, as the
determination of who is a "first class" citizen and who isn't seems
arbitrary and unfair, especially if you happen to be second class. The
technology should be egalitarian and should be capable of treating all data
the same. If a user wants data to be secure, or an application wants it's
execution to be secure, they should be able to ask for and get the highest
level of security that the platform can offer.
You point out that legal and societal policy likes to lump some kinds of
data together and then protect those lumps of data in certain ways from
certain things. Policy may also leave the same data open for some kinds of
usage and or exploitation in some circumstances. This is a fine and
wonderful thing from a policy perspective. This kind of rich policy is only
possible in a PC if that machine is capable of exerting the highest degrees
of security to every object seeking it. You can't water the security up; you
can only water it down.
I don't think that the platform security functions should have to decide
that some data looks like copyrighted information and so it must be treated
in one way, while other data looks like national secrets and so should be
treated differently. The platform shouldn't be able to make that choice on
it's own. The platform needs someone else (eg the user) to tell it what
policies to enforce. (Of course the policy engine required to automatically
enforce policy judgement on arbitrary data would be impossible to manage. It
would vary from country to country, and most importantly (from my
architectural perspective) it's impossible to implement becuase the only SW
with access to all data must be explicitly non-judgemental about what good
or bad policy is.)
More in-line:
----- Original Message -----
X-Orig-To: ; ;
Sent: Tuesday, August 06, 2002 12:11 PM
You say above that you don't agree the the problems are the same, but you
don't specify in what domain - policy, technical, legal, all of the above,
something else? The examples you give below are not technical examples - I
think that they are policy examples. What about from the technical
The term I use is "a blob is a blob"...
Isn't copyright a legal protection, and not a technical one? The efficacy of
copyright has certainly benefited greatly from the limitations of the
mediums it generally protects (eg books are hard and expensive to copy;
ideas, quotes, reviews and satires are allowed and also (not coincidentally)
don't suffer from the physical limitations imposed by the medium) and so
those limitations can look like technical protections, but really they
I agree that copyrighted material is subject to different policy from other
kinds of information. What I disagree on is that the TOR should arbitrarily
enforce a different policy for it becuase it thinks that it is copyrighted.
The platform should enforce policy based on an external (user, application,
service, whatever) policy assertion around a given piece of data.
Note that data can enter into Pd completely encrypted and unable to be
viewed by anything but a user-written app and the TOR. At that point the
policy is that the app, and thus the user, decides what can be done with the
data. The TOR simply enforces the protections. No one but the app and the
TOR can see the data to attempt to exert policy.
I swear that *I* was arguing this very point last time, and you were saying
something else! Hmmm. Maybe we agree or something.
The platform should treat this kind of data with the highest degree of
security and integrity available, and the level of security available should
support local policy like "no SW can have access to this data without my
explicit consent". The fact that the data is small makes it particularly
sensitive as it is so highly portable, so there must be law to allow the
legal assertion of policy independently from the technical exertion of
policy, and there has to be some rationalization between the two approaches.
While bandwidth limits the re-distribution of many kinds of content, it
doesn't with this kind of info. (And of course bandwidth limitations aren't
really technical protections and are subject to the vagaries of increased
bandwidth. Not a good security model.)
Not only should the platform be able to exert the highest degrees of control
over this information on behalf of a user, it should also allow the user to
make smart choices about who gets the info and what the policy is around the
usage of this info remotely. This must be in a context where lying is both
extremely difficult and onerous.
Common sense dictates that the unlawful usage of some kinds of data is far
more damaging (to society, individuals, groups, companies) than other kinds
of data, and that some kinds of unlawful uses are worse than others, but
common sense is not something that can be exercised by a computer program.
This will need to be figured out by society and then the policy can be
exerted accordingly.
I am not sure I understand the dichotomy; technical enforcement of user
defined policies around access to, and usage of, their local data would seem
to be the right place to start in securing privacy. (Some annoying cliche
about cleaning your own room first is nipping at the dark recesses of my
brain ; I can't seem to place it.) When you have control over privacy
sensitive information on your own machine you should be able to use similiar
mechanisms to achieve similiar protections on other machines which are
capable of exerting the same policy. You should also have an infrastructure
which makes that policy portable and renewable.
This is, of course, another technical / architectural argument. The actual
policy around data like "X is gay" must come from society, but controls on
the information itself originates with the user X, and thus the control on
the data that represents this information must start in user X's platform.
The platform should be capable of exerting the entire spectrum of possible

@_date: 2002-08-07 02:08:25
@_author: "Peter N. Biddle" 
@_subject: Re: Privacy-enhancing uses for TCPA 
Neither of us really had the time to clearly articulate things last time, so
I am glad you brought it up. My perspective is primarily from an
architectural one, and it boils down to this:
Platform security shouldn't choose favorites.
I don't want there to be any second class data citizens, as the
determination of who is a "first class" citizen and who isn't seems
arbitrary and unfair, especially if you happen to be second class. The
technology should be egalitarian and should be capable of treating all data
the same. If a user wants data to be secure, or an application wants it's
execution to be secure, they should be able to ask for and get the highest
level of security that the platform can offer.
You point out that legal and societal policy likes to lump some kinds of
data together and then protect those lumps of data in certain ways from
certain things. Policy may also leave the same data open for some kinds of
usage and or exploitation in some circumstances. This is a fine and
wonderful thing from a policy perspective. This kind of rich policy is only
possible in a PC if that machine is capable of exerting the highest degrees
of security to every object seeking it. You can't water the security up; you
can only water it down.
I don't think that the platform security functions should have to decide
that some data looks like copyrighted information and so it must be treated
in one way, while other data looks like national secrets and so should be
treated differently. The platform shouldn't be able to make that choice on
it's own. The platform needs someone else (eg the user) to tell it what
policies to enforce. (Of course the policy engine required to automatically
enforce policy judgement on arbitrary data would be impossible to manage. It
would vary from country to country, and most importantly (from my
architectural perspective) it's impossible to implement becuase the only SW
with access to all data must be explicitly non-judgemental about what good
or bad policy is.)
More in-line:
----- Original Message -----
Sent: Tuesday, August 06, 2002 12:11 PM
You say above that you don't agree the the problems are the same, but you
don't specify in what domain - policy, technical, legal, all of the above,
something else? The examples you give below are not technical examples - I
think that they are policy examples. What about from the technical
The term I use is "a blob is a blob"...
Isn't copyright a legal protection, and not a technical one? The efficacy of
copyright has certainly benefited greatly from the limitations of the
mediums it generally protects (eg books are hard and expensive to copy;
ideas, quotes, reviews and satires are allowed and also (not coincidentally)
don't suffer from the physical limitations imposed by the medium) and so
those limitations can look like technical protections, but really they
I agree that copyrighted material is subject to different policy from other
kinds of information. What I disagree on is that the TOR should arbitrarily
enforce a different policy for it becuase it thinks that it is copyrighted.
The platform should enforce policy based on an external (user, application,
service, whatever) policy assertion around a given piece of data.
Note that data can enter into Pd completely encrypted and unable to be
viewed by anything but a user-written app and the TOR. At that point the
policy is that the app, and thus the user, decides what can be done with the
data. The TOR simply enforces the protections. No one but the app and the
TOR can see the data to attempt to exert policy.
I swear that *I* was arguing this very point last time, and you were saying
something else! Hmmm. Maybe we agree or something.
The platform should treat this kind of data with the highest degree of
security and integrity available, and the level of security available should
support local policy like "no SW can have access to this data without my
explicit consent". The fact that the data is small makes it particularly
sensitive as it is so highly portable, so there must be law to allow the
legal assertion of policy independently from the technical exertion of
policy, and there has to be some rationalization between the two approaches.
While bandwidth limits the re-distribution of many kinds of content, it
doesn't with this kind of info. (And of course bandwidth limitations aren't
really technical protections and are subject to the vagaries of increased
bandwidth. Not a good security model.)
Not only should the platform be able to exert the highest degrees of control
over this information on behalf of a user, it should also allow the user to
make smart choices about who gets the info and what the policy is around the
usage of this info remotely. This must be in a context where lying is both
extremely difficult and onerous.
Common sense dictates that the unlawful usage of some kinds of data is far
more damaging (to society, individuals, groups, companies) than other kinds
of data, and that some kinds of unlawful uses are worse than others, but
common sense is not something that can be exercised by a computer program.
This will need to be figured out by society and then the policy can be
exerted accordingly.
I am not sure I understand the dichotomy; technical enforcement of user
defined policies around access to, and usage of, their local data would seem
to be the right place to start in securing privacy. (Some annoying cliche
about cleaning your own room first is nipping at the dark recesses of my
brain ; I can't seem to place it.) When you have control over privacy
sensitive information on your own machine you should be able to use similiar
mechanisms to achieve similiar protections on other machines which are
capable of exerting the same policy. You should also have an infrastructure
which makes that policy portable and renewable.
This is, of course, another technical / architectural argument. The actual
policy around data like "X is gay" must come from society, but controls on
the information itself originates with the user X, and thus the control on
the data that represents this information must start in user X's platform.
The platform should be capable of exerting the entire spectrum of possible

@_date: 2002-08-07 03:12:20
@_author: "Peter N. Biddle" 
@_subject: Re: dangers of TCPA/palladium 
----- Original Message -----
Sent: Tuesday, August 06, 2002 8:29 AM
TCPA and Palladium are different.
In the Pd case your existing data will probably be sealed (provided you
decided to seal it) under the control of the current TOR. The TOR thus has
to agree to be upgraded in order to name a new TOR to receive it's secrets.
In the case of the MS TOR the decision to upgrade will be under the control
of the user. As the TOR will be highly reviewed and the source made
available, you can look at the it yourself (or talk with others who have
done so) and see that Pd can't be forced into an upgrade.
Pd explicitly negates the efficacy of keyboard sniffers, screen dumpers,
web-cache readers, and other related snoopware, but it only negates it for
new software. Existing software is backwards compatible, and thus continues
to run the way is always has. This means it doesn't gain or lose anything in
a Pd system.
Some options to if you are concerned about the trustworthiness of Pd SW
would be to only run code that:
has been made available for review to people you trust
    and/or
has certs from people you trust
    and/or
you have compiled yourself
    and/or
you wrote yourself
Of course making this a local policy is up to you; the TOR will enforce that

@_date: 2002-08-07 02:16:11
@_author: "Peter N. Biddle" 
@_subject: Re: dangers of TCPA/palladium 
I think that I have been called "heniously evil" as well as "devious"
recently, so "authoritarian" isn't so bad. (I don't actually know for sure
if they were calling me that; I think that it was something like "whomever
thought of this is evil" and that would mean a group of which I am a part.)
Thanks for the apologies though.
----- Original Message -----
Sent: Tuesday, August 06, 2002 11:55 AM

@_date: 2002-08-05 23:35:46
@_author: "Peter N. Biddle" 
@_subject: Re: dangers of TCPA/palladium 
There are a lot of misconceptions about TCPA and Palladium. I am not going
to address TCPA per se, but I do want to try to clear up differences and
misconceptions around what Pd does.
Comments are in-line:
----- Original Message -----
Sent: Sunday, August 04, 2002 10:00 PM
Like anonymous and Adam, I have also been reading lots on Palladium lately.
I have also been working on Pd since 1997.
I agree, and from my perspective this is a problem. We have a great deal of
information we need to get out there.
We have done technical reviews of Palladium, as shown by Seth Schoen's notes
(a), which I think talk directly about many of the things discussed in this
thread. I suggest anyone who wants to start to understand Pd read these
You don't cite the MS whitepaper. This is not a technical paper but it does
set precedent and declare intent. See (b).
The suggestions for TCPA responses that William Arbaugh raises seem quite
good (c). 1 and 2 are already true for Pd, I believe that 3 is true but I
would need to talk with him about what he means here to confirm it, 4 is
covered in Eric Norlin's blog (d), and 5 is something we should do.
The current TPM (version 1.1) doesn't have the primitives which we need to
support Palladium, and the privacy model is different. We are working within
TCPA to get the instruction set aligned so that Palladium and TCPA could use
future silicon for attestation, sealing, and authentication, but as things
stand today the approaches to the two of them are different enough so that
TCPA 1.1 can't support Pd.
Pd is an OS feature set based on new hardware. Pd requires changes to the
CPU, chipset and/or memory controller, graphics and USB, as well as new
silicon (we call an SCP or SSP), . Microsoft currently has no announced
plans to support TCPA directly, and as things stand today there is no SW or
HW compatibility between the two.
This is not how Palladium works. Palladium loads a small piece of code
called the TOR after the OS has booted and is running (this could be days
later). Pd treats the BIOS, firmware, and privileged Windows OS code as
untrusted. Pd doesn't care if the SW is certified or not - that is a
question left to users.
In Palladium, SW can actually know that it is running on a given platform
and not being lied to by software. In 1, you say that SW virtualization
doesn't work, and that is part of the design. (Pd can always be lied to by
HW - we move the problem to HW, but we can't make it go away completely).
As SW is capable of knowing its own state, it can attest this state to
others - users, services, other apps, etc. It can't lie when it uses Pd to
say what it is. It's up to third parties (again, the user of the machine, or
an app, or service) to decide if it likes the answer and trusts the
application. Disclosure of the apps identity is up to the user and no one
Note that in Pd no one but the user can find out the totality of what SW is
running except for the nub (aka TOR, or trusted operating root) and any
required trusted services. So a service could say "I will only communicate
with this app" and it will know that the app is what it says it is and
hasn't been perverted. The service cannot say "I won't communicate with this
app if this other app is running" because it has no way of knowing for sure
if the other app isn't running.
Confusion. The memory isn't encrypted, nor are the apps nor the TOR when
they are on the hard drive. Encrypting the apps wouldn't make them more
secure, so they aren't encrypted. The CPU uses HW protections to wall new
running programs from the rest of the system and from each other. No one but
the app itself, named third parties, and the TOR can see into this apps
space. In fact, no one (should the app desire) can even know that the app is
running at all except the TOR, and the TOR won't report this information to
anyone without the apps permission. You can know this to be true because the
TOR will be made available for review and thus you can read the source and
decide for yourself if it behaves this way.
Correct enough for this thread; it is actually the TOR that will manage the
keys for the apps, as this makes the concept of migration and data roaming
far more manageable. (Yes, we have thought about this.)
Comparing xBox and Pd isn't particularly fruitful - they are different
problems and thus very different solutions. (Also note that xBox doesn't use
the PID or any other unique HW key.)
Palladium mostly doesn't care about the BIOS and considers it to be an
untrusted system component. In Pd the BIOS can load any OS it wants, just
like today, and in Pd the OS can load any TOR specified by the user. The MS
TOR will run any app, as specified by the user. The security model doesn't
depend on some apps being prevented from running.
I believe that there isn't a single thing you can do with your PC today
which is prevented on a Palladium PC. I am open to being challenged on this,
so please let me know what you think you won't be able to do on a Pd PC that
you can do today.
I think you mean CSS, not DSS.
I don't want people snooping my passwords from the keyboard buffer, nor my
account info from the frame buffer, and HW protections in those HW areas
prevent that.
Palladium doesn't boot strap the OS. Pd loads a secure piece of SW, called
the TOR, which runs in a secure space and loads other apps that want
security. Anyone can load an app into this environment and get the full
protections Pd offers. MS doesn't require that you show them the SW first -
you wanna run, you get to run - provided the user wants you to run. If a
user doesn't like the looks of your app, then you (the developer) have a
problem with that user.
The privacy model in Pd is different from TCPA. I could go on for a long
time about it, but the key difference is that the public key is only
revealed to named third parties which a user trusts. You are right in
thinking that you need to trust them, but you don't have to show anyone your
key if you don't trust them, so you (the user) are always in control of
Pd is not about user authentication - it is about machine and SW
authentication. User auth can be better solved on a Pd platform than on a PC
today, but it isn't required. Pd doesn't need to know who you are to work.
I don't know where to begin on this one. It deserves a long, well thought
out response, and I don't have the time to do it at the moment. I will
follow up on this. Let me state that I think that much of the energy around
DRM and HW is misplaced, and that Pd is designed to enable seamless
distribution of encrypted information, not to disable distribution of clear
text information.
MS will not have the root keys to the world's computers. The TOR won't have
access to the private keys either. No one but the HW does. The TOR isn't
"MS" per se - it is a piece of SW written by users but vetted and examined
by hopefully thousands of parties and found to do nothing other than manage
the local security model upon which Pd depends. You can read it and know it
doesn't do anything but effectively manage keys and applications. And if you
don't trust it, you won't run it.
If you don't trust the TOR, you don't trust Palladium. Trust is the *only*
feature we are attempting to achieve, so every decision we make will be made
with trust and security in mind.
I am confused as to how this would work in Pd. Anyone can write apps to the
Pd API. Zero restrictions. (API's are full of restrictions - by their nature
they limit things to a protocol, and potentially HW, both of which have
understood limitations; I am dodging this concept in saying there are no
This is a problem anyone who wants to compete in the security and trust
space will need to overcome. I don't think that it is particularly new or
different in a world with Pd. Writing a TOR is going to be really hard and
will require processes and methods that are alien to many SW developers. One
example (of many) is that we are generating our header files from specs. You
don't change the header file, you change the spec and then gen the header.
This process is required for the highest degrees of predictability, and
those are cornerstones for the highest degree of trust. Unpredictable things
are hard to trust.
Everything in the TCB (Trusted Computing Base) for Pd will be made available
for review to anyone who wants to review it;  this includes software which
the MS TOR mandates must be loaded.
This doesn't happen in Pd. There is no secure boot strap feature in Pd. The
BIOS boots up the PC the same way it does today. Root control is held by the
owner of the machine. There is no certification master key in Pd.
One of the beauties of Pd is that if there is any SW backdoor, you will know
about it. HW robustness will be something for manufacturers to work out. For
most systems, I think that extensive HW tamper resistance will be a waste of
time, but for some (e.g. highly secure govt systems) it will be a necessity
and one that works well in Pd.
I know that we aren't using undocumented API's and that we will strive for
the highest degree of interoperability and user control possible. Pd
represents massive de-centralization of trust, not the centralization of it.
I think that time is going to have to tell on this one. I know that this
isn't true. You think that it is. I doubt that my saying it isn't true is
going to change your mind; I know that the technology won't do much of what
you are saying it does do, but I also know that some of these things boil
down to suspicion around intent, and only time will show if my intent is
aligned with my stated goals.
Pd does not give root control of your machine to someone else. It puts it
into your hands, to do with as you so desire, including hacking away at it
to your hearts content.
I think that Pd represents an enhancement to personal freedoms and user
control over their machines. I hope that over time I will be able to explain
Pd sufficiently well so that you have all the facts you need to understand
how and why I say this.
(a) Seth Schoens Blog (b) MS Paper
(c) William Arbaugh on TCPA
(d) Eric Norlin's blog

@_date: 2002-08-07 03:42:12
@_author: "Peter N. Biddle" 
@_subject: Re: USENIX Security TCPA/Palladium Panel Wednesday 
I consider it a Bad Thing that we don't have more clearly organized
technical documentaion to show right now, and I can only say that we are
working on providing this post haste. I certainly am not happy to be
pointing you to blogs as primary sources. I apologize for this, and I will
send stuff out to this alias when we have it.
----- Original Message -----
Sent: Tuesday, August 06, 2002 4:57 PM

@_date: 2002-09-18 16:15:12
@_author: PeterNBiddle@hotmail.com 
@_subject: Re: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The issue isn't whether or not the architeture as it existed in the past is
or isn't able to securely isolate user and kernel mode processes in an OS
which may not exist. If an OS can be written to securely isolate user and
kernel mode processes then I am sure that someone clever will find a way to
use it to do such a thing and may have an excellent security solution for
that OS which runs on current chips. I wish whomever tries to do this the
best of luck.
In Windows there are a number of reasons we can't use the current isolation
model for absolute enforcement of isolation. The biggest business reasons
are backwards compatibility for applications and kernel mode drivers, both
of which count on the current architecture and all of it's strengths (and
quirks). As we have stated before, we designed Pd with the assumption that
we couldn't break apps and we couldn't break device drivers.
Arbitrary Windows code which runs today must continue to run and function in
Pd as it does today, and yet Pd must still be able to provde protection.
Someone with a niche OS who doesn't care about breaking things may use a
different approach - they don't have gazllions of lines of 3rd party code
counting on version to version compatibility. We do.
From a technical perspective, there are also a number of reasons that the
current isolation models don't work My guess is that a hard core Linux or
Unix kernel dev could probably explain this just as well as MS could,
however I will see if I can get someone on our end to outline the issues as
we see them.
I think that you are talking about separating user-mode processes in VMWare
(right?). What about SCSI controllers? The BIOS? Option ROMS? Kernel mode
device drivers? DMA devices? Random kernel foo.sys? What if the attack uses
SMM to attack VMWare itself? How does VMWare prove that the environment it
inherited when it booted is valid?
Lastly - Pd is only partially about process isolation. Nothing in the
current architecture even attmempts to address SW attestation, delegated
evaluation, authentication, or the sealing of data.
----- Original Message -----
Sent: Tuesday, September 17, 2002 3:01 PM

@_date: 2002-09-20 00:36:50
@_author: "Peter N. Biddle" 
@_subject: Re: Cryptogram: Palladium Only for DRM 
Hi Nomen
I am sending to crypto only as I am not on any of the other aliases you sent
to. Feel free to fwd.
How about "hacked" instead of "broken"? Broken implies that a machine
doesn't work; hacked implies it has been changed somehow but that it still
works. Let's say that a hacked Pd machine is a machine whose root keys have
been discovered through any means outside of the security model for that
machine. So a machine designed to give up its keys or to take keys in from
an outisde source isn't hacked. A machine whose security model includes
protecting the keys from everything, but whose keys have become known, is a
hacked machine. I can certainly imagine situations where Pd will be on a
hacked machine and won't know it.
Once the machine has been hacked, a user (or process, or piece of SW, or
whatever) can unlock all secrets which use the local keys as root keys. So
the symmetric keys used to protect a given piece of data would be
compromised, and all data which uses the same symmetric key can now be
unlocked. Rather than having to hand someone data, you could hand them keys
(presuming they have the data already). The "less global" a secret, the less
vulnerable it is to key hand-offs, but if more than one existence of
something is protected by the same key, that key represents an easily
distributed attack.
Even in cases where a given piece of data is secured with a unique key or
keys, once you have hacked those keys (or more likely the root keys used to
gen those keys) you can decrypt the data itself.  If all data in the world
only existed in Pd virtual vaults and was encrypted using different unique
keys, the data itself is still it's own secret. You can still extract
everything in Pd via a HW attack. Now rather than hand off the keys, you
hand off the data.
How is this BORE resistant? The Pd security model is BORE resistant for a
unique secret protected by a unique key on a given machine. Your hack on
your machine won't let you learn the secrets on my machine; to me that's
BORE resistant. Any use of Pd to protect global secrets reduces the BORE
resistance for the information protected by those secrets.
Only the Pd nexus (sorry, new name for the nub, er I mean TOR, er I mean
secure kernel, ...) knows each applications secrets, and it protects those
secrets from everything else absolutely. The nexus won't analyze data and
decide if it should or shouldn't be there; no Pd DRL's. (A DRM scheme on top
of Pd could enforce DRL's for content within its own vault, of course, but
it can't cross the vault boundary to try to enforce a DRL in someone else's
vault.) The goal is to protect data for whomever is asking for protection,
and to keep that data secure for that application. (I must note that we are
basing our design on existing US law. Should the law change and require
different behaviors, or should other countries require different behaviors,
we will need to find a way to comply.)
Palladium systems won't seek out and destroy anything, either locally or
remotely. Additionally the nexus has no understanding of what "legitmate" or
"illicit" means, so Pd really couldn't do this if it wanted to (it doesn't).
Data will be protected by Pd (in memory; on disk). Only applications with
the right hash (or those named by the original hashee) can access any given
piece of data.
----- Original Message -----
; ;
; Sent: Wednesday, September 18, 2002 5:10 PM

@_date: 2002-09-18 23:56:43
@_author: "Peter N. Biddle" 
@_subject: Re: Cryptogram: Palladium Only for DRM 
Hey Ed - I think that we may be in agreement. Most of what you say below
makes sense to me.
This is how I have been describing trust recently - including it so that you
will at least understand where I am coming from when I use the term (cribbed
directly from a recent slide deck).
I can't trust anything (a component, an entity, etc.) unless.
    I know who/what it is, and that it's not an imposter
    I know its state-it has been properly initialized
    I know that it cannot be tampered with
    I know that my communication with it is private and tamper-proof
These are four elements of trust
    I still need to be able to verify (trust) that the component behaves
properly in an ongoing manner
Everyone has different perspectives on "trust," some even contextual
    "Doctrine of Multiple Trustors": trust is multi-way, not two-way
        It is up to each party to determine its own trust criteria, and to
use mechanisms (e.g., certification) to measure and assert those criteria
        Anything / everything can be certified to be trusted by anyone /
     Many "things" (HW or SW) will be certified by more than one entity
        Each entity may be measuring different facets of trust
I'd love to see your papers.
----- Original Message -----
Sent: Wednesday, September 18, 2002 10:04 AM
"Trust Points"
trust on
set a

@_date: 2002-09-18 17:34:23
@_author: PeterNBiddle@hotmail.com 
@_subject: Re: Cryptogram: Palladium Only for DRM 
Bruce Schneier has a great take on this - secure systems should fail well.
Pd is designed to fail well - failures in SW design shouldn't result in
compromised secrets, and compromised secrets shouldn't result in a BORE
attack. I've talked about the processes we are using to make sure that this
is true but for a start we are gen'ing headers from formal specs. The specs
are reviewed for architecture and security before spec changes are approved,
and only then can you get a new header. We are doing a formal proof on parts
of the design (those upon which the security model depends). This process
should keep the bugs we do get from jeopardizing the security model.
----- Original Message -----
; ;
Sent: Monday, September 16, 2002 1:32 PM

@_date: 2002-09-18 17:15:22
@_author: PeterNBiddle@hotmail.com 
@_subject: Re: Cryptogram: Palladium Only for DRM 
Hi Pete - I'm confused. Are you suggesting that I should enjoy these
freedoms on SW which I don't have legal rights to?
If not, then I don't see how any of these freedoms are affected by Pd. If
you are suggesting that *all* SW should be made free, well that has nothing
to do with Pd, does it?
----- Original Message -----
Sent: Tuesday, September 17, 2002 4:16 AM

@_date: 2002-09-18 17:04:52
@_author: PeterNBiddle@hotmail.com 
@_subject: Re: Cryptogram: Palladium Only for DRM 
There is nothing in Pd which assumes that the keys weren't put there by a
crazy hobbit named Mel who waves a magic wand on every system and then
tattoos the keys on the users chest. Pd doesn't really know how the keys got
there (how would it?). Pd wants a HW cert which it can show to others, who
can then go to the signing authority for that cert and decide for themselves
whether or not they trust that HW. Note that "others" can represent SW,
remote users, local users, whatever...
Some organizations will certainly want to gen their own keys. Some users
will want to do this as well. The cost / benefit anaylsis involved in the
way the keys got into the machine, how they are managed, and how they are
preserved is up to a given market segment (where each segment happens to
consist of thousands or millions of users).
I believe that the chips in most Pd PC's will offer some relatively low bar
for HW tamper resistance, will self-gen keys, and will manage those keys
such that no one gets to know the private key and only third parties named
by the user will ever see the public key. I think that this serves the
widest set of needs. There may also be chips which squirt the keys out based
on some event and there will almost certainly be ones which take keys
squirted in from a remote source. Pd will run.
Think about the graphics business - one vendor sells a decent percentage of
silicon but there are other vendors who have been very succesful building
products which sell into different segments based on different feature sets,
and the same should hold true here. It is up to the makers of chips and
users to decide how they want to assert the trustworthiness of a given
system, and it is up to application writers to decide how they want to
operate on that machine based on the certs they are given by Pd.
re: smart cards - I agree, as a user I want the added protections I will get
from a combination of a smart card and biometrics on top of Pd. I ultimately
also want a mechanism I can use to ask an anonymous machine if I can trust
it, and a combined smart-card / bioemtirc device doing an authentication
with a Pd machine should let me do that.
----- Original Message -----
Sent: Tuesday, September 17, 2002 10:54 AM

@_date: 2002-09-18 16:34:47
@_author: PeterNBiddle@hotmail.com 
@_subject: Re: Cryptogram: Palladium Only for DRM 
I disagree with your first sentence (I believe that Pd must be trustworthy
for *the user*), but I like much of the rest of the first paragraph.
I am not sure what value my mother would find in defining her own
signatures. She doesn't know what they are, and would thus have no idea on
who or what to trust without some help.
What my mother might trust is some third party (for example she might trust
Consumer's Union). We assumed we needed a structure which lets users
delegate trust to people who understand it and who are investing in
"branding" their take on the trustworthiness of a given "thing" (think UL
label, Good Housekeepking Seal of Approval, etc.). I totally agree that some
small segment of users will have an active interest in managing the trust on
their machines directly (like, maybe, us) but any architecture that you want
to be used by "normal" PC users needs to also let users delegate this
managment to others who can manage it for users (just like we might decide
to use others to manage our retirement funds, defend us in a court of law,
or operate on our kidneys).
To delegate trust, you need to start out trusting something to do that
delegation. That's part of what Pd is addressing - Pd needs to be
trustworthy enough so that when a user sets policy (eg "don't run any SW in
Pd which isn't signed by the EFF" or "don't run any SW which isn't
debuggable"), it is enforced.
----- Original Message -----
Sent: Tuesday, September 17, 2002 2:51 PM

@_date: 2002-09-18 16:15:12
@_author: PeterNBiddle@hotmail.com 
@_subject: Re: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The issue isn't whether or not the architeture as it existed in the past is
or isn't able to securely isolate user and kernel mode processes in an OS
which may not exist. If an OS can be written to securely isolate user and
kernel mode processes then I am sure that someone clever will find a way to
use it to do such a thing and may have an excellent security solution for
that OS which runs on current chips. I wish whomever tries to do this the
best of luck.
[Moderator's Obnoxious Note: I believe such an operating system is
called "Unix"...]
In Windows there are a number of reasons we can't use the current isolation
model for absolute enforcement of isolation. The biggest business reasons
are backwards compatibility for applications and kernel mode drivers, both
of which count on the current architecture and all of it's strengths (and
quirks). As we have stated before, we designed Pd with the assumption that
we couldn't break apps and we couldn't break device drivers.
Arbitrary Windows code which runs today must continue to run and function in
Pd as it does today, and yet Pd must still be able to provde protection.
Someone with a niche OS who doesn't care about breaking things may use a
different approach - they don't have gazllions of lines of 3rd party code
counting on version to version compatibility. We do.
current isolation models don't work My guess is that a hard core Linux or
Unix kernel dev could probably explain this just as well as MS could,
however I will see if I can get someone on our end to outline the issues as
we see them.
I think that you are talking about separating user-mode processes in VMWare
(right?). What about SCSI controllers? The BIOS? Option ROMS? Kernel mode
device drivers? DMA devices? Random kernel foo.sys? What if the attack uses
SMM to attack VMWare itself? How does VMWare prove that the environment it
inherited when it booted is valid?
Lastly - Pd is only partially about process isolation. Nothing in the
current architecture even attmempts to address SW attestation, delegated
evaluation, authentication, or the sealing of data.
----- Original Message -----
Sent: Tuesday, September 17, 2002 3:01 PM
