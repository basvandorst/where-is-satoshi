
@_date: 1995-08-01 12:48:25
@_author: Scott Brickner 
@_subject: U.S. Banks are not all that bad 
Sure they're happier if you use the ATM.  It costs them less
per transaction, plus they *charge* you to use the damn thing.
How many people do you think would put up with $1.00 or $1.25
to do a transaction at a human teller?  Most banks charge about
that much for "foreign" (other than those they own) ATM use.
Some even charge that much for *all* ATM use if you don't get
the "premium" accounts.

@_date: 1995-08-07 13:10:18
@_author: Scott Brickner 
@_subject: "The Net" 
Ray Arachelian writes
Nope.  *Any* IP address starting with 127 is a host loopback address
and shouldn't appear outside the host.  This from the "Assigned
Numbers" RFC (STD 2).
127.0.0.1 is only convention.  It's the "first" loopback address.

@_date: 1995-08-14 17:59:44
@_author: Scott Brickner 
@_subject: votelink - some discussions on Phil Z & ITAR 
aba at atlas.ex.ac.uk writes
Interesting reaction, if I understand it rightly... "We're afraid
that having less than two tenths of a percent of PGP on our system
causes us to violate ITAR, but we're also afraid to remove it because
it may constitute 'moderation' of the discussion and thereby open us
to liability."

@_date: 1995-08-23 11:20:04
@_author: Scott Brickner 
@_subject: A glance at the future of missing child identification 
Lucky Green writes
If you mean something like Idea Futures, but with real money, I haven't
seen one.  I *have* seen "Iowa Electronic Markets", run by U Iowa, which
has real-money futures on certain political campaigns (Presidential,
Republican Convention, and Powell Nomination).
See .
I e-mailed the person responsible asking about the legalities of doing
such a thing, as true gambling on presidential elections isn't legal.
She indicated that they had a letter from the appropriate governmental
agency which promised to take no action as long as they adhered to
certain limits (a maximum on purchases being the most significant).
Apparently they've already had requests to make some fairly *large*
($500,000 or more) investments.

@_date: 1995-08-23 13:53:04
@_author: Scott Brickner 
@_subject: Out of state gambling 
Anonymous writes
None of this is necessary.  The University of Iowa's Business School
already has this sort of thing.  They run a real-money futures market
which currently has the following three markets:
1996 Presidential Election
1996 Republican Convention Nomination
1996 Colin Powell Nomination
The URL is Just as an aside, the Republicans have been steadily losing ground
over the last three months.  The difference between the Republican
coupon and the Clinton reelection coupon is about 1/2 cent, down
from 10 cents or so a few months ago.

@_date: 1995-08-23 14:57:33
@_author: Scott Brickner 
@_subject: RUB_han 
How does one go about retrieving the text of these articles?

@_date: 1995-08-24 09:21:07
@_author: Scott Brickner 
@_subject: Spooks and Hackers, etc. 
Brad Dolan writes
Duh!  Isn't the law *supposed* to be slower than technology?  It would
take a *complete* idiot to try to make a law about, say, intelligent
programs or uploads or whatever.  It's when law wants to be *faster*
than technology that we get stupidity like the CDA.

@_date: 1995-08-25 15:22:53
@_author: Scott Brickner 
@_subject: random coincidences 
Sam Quigley writes
The most common one is "linear correlation" between successive
random values.  The typical PRNG supplied with compilers is what's
called a "linear congruential random number generator", which has
something like:
    S0 = (user supplied seed)
    Sn+1 = ( a * Sn + b ) mod c
    Rn = f(n)
The choice of constants a, b, and c are critical to the process.
A decent practical discussion is in "Numerical Recipes in C".
If you take N successive random numbers and interpret them as a
point in an N-dimensional space, then the points generated by the
linear congruential PRNG don't tend to fill up the space as they
would in the "true" random case.  They tend to lie on N-1 dimensional
planes instead, and when a, b, and c are chosen poorly, sometimes
*very* few such planes.
If NetScape uses such a PRNG to select 40bit keys for SSL, then the
work to be done in brute-force search going on right now might be
*significantly* reduced by knowing the planes on which the numbers
lie.  If the constants are particularly poor, there might be as little
as ten or twelve bits of real key.  You could search that on a *Newton*
in less than an hour or so --- nevermind the MasPars and such being used
in the current project.

@_date: 1995-08-28 13:14:43
@_author: Scott Brickner 
@_subject: SSL trouble 
Will French writes
Then what do you care about the group's procedures?  It doesn't
"prevent you from participating" --- you *aren't* participating.
You're attempting to solve the problem on your own.
Statistically, the "random" methods are no different than everyone just
working independently at solving the problem.
I, too, don't recall my statistics well enough, but let me take a shot
at it, and anyone who wants to, please check me...
The probability of having failed to search a particular segment (the
one with the key) after selecting k of n segments at random with
replacement is (1-1/n)^k, whereas in a sequential search from a random
starting point, (or, equivalently, random without replacement) the
probability is k/n.
Assume the segments are farmed out in 2^24 segments of 2^16 keys each
(I don't recall what the current programs use).  In the sequential case,
it's even money you'll find the key after searching 8,388,609
segments.  In the random case, it's not even money until 11,629,080
segments --- 39% longer.  It's when you're "unlucky" that the random
case gets *much* worse.  To search 90% of the keyspace takes 15,099,495
sequential searches, but 38,630,967 --- a 156% difference.
Here's the table:
% k-space	  random	sequential	percent
searched	  method	  method	difference
--------	----------	---------	----
10		   1767657	  1677722	  5
25		   4826505	  4194305	 15
50		  11629080	  8388609	 39
75		  23258160	 12582913	 85
90		  38630967	 15099495	156
99		  77261933	 16609444	365
99.9		 115892899	 16760439	591
Changing the segment size doesn't affect the results very much, as
a table for 10 bit segments shows:
50		 744261117	536870912	 37
90		2472381916	966367641	156
The random method is a little more than 1/3 worse in the typical
case, but *lots* worse in the worst cases.

@_date: 1995-08-29 10:22:16
@_author: Scott Brickner 
@_subject: SSL trouble 
Will French writes
We've identified several forms of "real-world retaliation:"
1) "Result hoarding" - failure to report a found key
2) "Segment hoarding" - requesting more segments than one can hope to search
3) Denial of service - preventing access to the server
The "random search" method eliminates all three of these at about 37%
higher cost in search time, on the average.  I submit that if we
*really* were trying to break something important, we could design a
system which eliminated the first two and adequately limited the third,
but at *much* less cost.
The problems in the current system were to be expected of a first
attempt.  In the future:  Only the server assigns segments, only the
assignee may report the status of a segment, and after all segments are
NAKed we know condition 1 has occurred, at which time we start over,
but never assign the same segment to the same searcher.  Limit the
number of segments which may be outstanding with one searcher at one
time as a function of work rate.  Deploy redundant servers.
As to whether the distinction is valid, I'd still say the only
difference between working on your own and working "with" the group,
but using an uncoordinated, random search method is one of intent ---
that is, it's all in your mind.
So what information wouldn't you be getting?  To "go your own way", you
need exactly the same information that the client workstations use to
test one key.  The difference in your code and the clients exists
solely in how they determine the next key to try.
You're not "participating" when you go your own way.  You're working on
cracking the cipher, but you're not adding your efforts to the group
effort, you're working independently.  I'm not saying this is "wrong".
You're supposedly a free person, do what you think is right.

@_date: 1995-08-29 11:48:20
@_author: Scott Brickner 
@_subject: Florida Drivers Permits and a Hello 
Alan Olsen writes
What possible value could the LEAs get by having your thumbprint digitally
encoded on your driver's license?  It's not like the average cop-on-the-beat
is qualified to lift a fingerprint and compare it.  Even if he was, how
does it benefit that the fingerprint is on the license?
This seems silly.

@_date: 1995-08-29 18:01:35
@_author: Scott Brickner 
@_subject: SSL search attacks 
don at cs.byu.edu writes
I suppose this does seem like a "statist" protocol, but let's look at
the purpose.  The whole idea of the central server was to permit a
*coordinated* attack on the key.  We've established that there is a 1/e
cost factor in removing the central server.  I just threw out these
items as specific changes which could defend against the identified
attack modes *without* losing the benefit of the central coordination.
In order for the coordinator to be successful, there must be a
mechanism to ensure that someone who knows the key can't break the
system by just reporting "I searched this segment and didn't find it."
This means that the server should consider such statements as
irrelevant, unless it was the *server* who suggested that the user
search the space.  This makes the likelihood of the key's segment being
assigned to a "bad guy" pretty low.
The server *could* take unsolicited NAKs "under advisement", and hand
them out at a slower rate than unACKed segments, but this still allows
the "result hoarder" to slow down the attack.
Hence the prohibition against (as Tim put it) "J. Random User claiming
I don't think it would really be that hard, if one were willing to go
with less than "cryptographic" strength in the PRNG, which I don't think
is really necessary here.
The problem is that it's irrelevant to the problem.  Random allocation
at the server is equivalent to simply "shuffling" the segments before
assignment, which doesn't affect the rate at which the space is searched.
I'm not sure I follow you, here.  The search wraps around on the unACKed
segments because the work was assigned, but not (as far as the server
knows) completed.  This doesn't slow down the discovery of the key,
it just reflects the *real* composite key testing rate as opposed to
the *apparent* rate (which is based on the rate at which the segments
are assigned).  The server doesn't consider a segement "done" until it
gets an ACK or NAK.
NB: Elsewhere, Tim provides an argument showing the efficiency of the
random attack to be 1/e worse than the coordinated attack (about 37%).
I still don't see how the server can use unsolicited NAKs as anything
other than a nominal reduction in the probability that the key is in
the NAKed segment.  Perhaps this does give an idea of a server strategy
to do *just* that, though.
The server maintains a list of the unique users who have reported an
unsolicited NAK for each segment.  Requests for work are filled by
randomly selecting segments, with the highest weight going to the
segments with the fewest unsolicited NAKs, but only segments with
*solicited* NAKs and those assigned, but with no response, are not
If the weight were inversely proportional to the square of the number
of unsolicited NAKs (plus one), then segments which have a lot of NAKs
won't likely be assigned until the end of the jobs.
When a segment with unsolicited NAKs is assigned, further weight might
be given to unsolicited NAKs from those users in the future, reflecting
an improvement in their reputation.
The biggest problem with this scenario is that it requires a
potentially *huge* amount of storage on the server.
Another alternative that comes to mind is to hand out segments with
unsolicited NAKs to some of the slower machines.  Since their
contribution to the overall search rate is small, there's less of a hit
taken by assigning them potentially redundant work.  As they provide
verification of the data reported as unsolicited NAKs, the server's
reputation data is improved, and the search can concentrate even more
on the unACKed segments.

@_date: 1995-08-30 11:14:55
@_author: Scott Brickner 
@_subject: SSL search attacks 
David R. Conrad writes
An excellent point.  One I'd missed.  I agree that a random shuffle
of segments is appropriate.

@_date: 1995-08-30 15:03:32
@_author: Scott Brickner 
@_subject: SSL search attack 
don at cs.byu.edu writes
Well, the only real issue is that the requestor *not* be able to
reliably predict which segments will be assigned.  The server may adopt
a strategy of picking a random block of segments for each request.
This introduces a certain amount of fragmentation into the process, but
there are strategies to minimize this.  It may be enough to break up
keyspace into, say, 32 "regions", and fill requests sequentially, but
from a randomly selected region.

@_date: 1995-08-31 10:30:38
@_author: Scott Brickner 
@_subject: SSL search attacks 
Jiri Baum writes
This only reduces the cost if everyone is playing fair.  In practice,
it will usually *increase* the cost.  A denial of service attack can be
mounted by the owner of the key just by anonymously NAKing the segment
with the key.  Then you have to search the *whole* keyspace, fail to
find it, and start over with a new strategy.
An attack on what?  The overall model here is that someone presents
the world at large with a problem to solve.  Someone else volunteers
to coordinate the effort by providing a server.  Providing a bogus
server is an attack in the sense that it wastes the CPU cycles of
the clients, but they're junk cycles anyway.  It's kind of like the
issue about being "unable to participate" because the group effort
ignores the efforts of random searchers.  Those searchers *aren't*
participating, and not ignoring them opens the server to attack.
An "effort" coordinated by a bogus server is no effort at all.
My point is that the "random" efforts are no different than everyone
working on the problem independently, each picking a random place to
start and going sequentially from there.
This is similar to what I outlined yesterday afternoon.  Let unsolicited
NAKs and IGRABs represent adjustments to the probability that a segment
is assigned to a client *inside* the group.  Invalid unsolicited NAKs
don't destroy the current search, they only slow it down slightly ---
but less than a fully random effort.
This could be done in any case.  It just slows down the effective search
rate of the e-mail participants.
This might be an argument in favor of requesting more space as you get
near the end of your current space, though.  When the communications
latency starts to approach the segment search time, you cut down your
waiting time by prefetching work.

@_date: 1995-08-31 16:32:13
@_author: Scott Brickner 
@_subject: A problem with anonymity 
I was thinking about some issues related to electronic commerce, and it
occurred to me that there is a significant problem in conducting
business with untraceable pseudonyms (anonyms?).  The problem occurred
to me while considering inheritance.
If one operates a business under an anonym (as opposed to the sort of
conditionally traceable pseudonym proposed by AT&T in "Anonymous Credit
Cards" ),
there's a strategy for transferring unlimited funds to one's
Consider a business which typically has a lot of assets, but which are
offset by a lot of liabilities --- almost any sort of VAR will do, for
instance.  In your will, you leave the key to unlock a private message
to your heir, in which you hand over the information necessary to
assume your anonym.  Since the heir presumably has his own identity
(whether anonymous or not is immaterial, except to *his* heirs), and
the anonym can't be linked to you, he has no reason to care about
maintaining the reputation of the anonym.  In dismantling the anonym,
he sells its assets to his own identity at a fraction of their worth,
and defaults on the liabilities.
Since the anonym behaved reputably during its life, it developed what
would have been a credit-worthy reputation, had it been a (traceable)
pseudonym.  But, since there's nothing to link the anonym to its heirs
(or ancestors), the creditors of the anonym must eat the loss.
Since the process of taking an anonym from scratch to a positive
reputation would be reasonably short (presumably not too much longer
than taking a real name or pseudonym the same distance), especially when
helped along by being fed the profits from the legitimate business of
an ancestor anonym, it's likely that a single individual could pull off
such an asset transfer at least two or three times a decade, as well as
at inheritance time.
A market which permits anonyms to have credit based on reputation will
probably have a constant stream of defaults caused by such behavior,
representing a significant risk factor in extending credit to anonyms
which can't be predicted by reputation.

@_date: 1995-07-25 09:11:10
@_author: Scott Brickner 
@_subject: Three strikes you're out! for politicians... yeah we wish! 
This might be nice, but questions of "upsetting the system of checks
and balances" aside, you can't do it.  It would violate Article I,
Section 6 of the Constitution, which says that "for any speech or
debate in either House, [the Senators and Representatives] shall not be
questioned in any other place".  "Speech or debate" would cover the
vote on any question.
Therefore, the only organization which can hold a
senator/representative liable for passing a bad law is the one which
passed the law. :(

@_date: 1995-07-25 09:59:56
@_author: Scott Brickner 
@_subject: Exporting from Canada (was Re: Let's try breaking an SSL RC4 key) 
So?  The ITAR doesn't control export to Canada.  Export the source code
to Canada, compile, validate, sign, and put on CD in Canada, and export
to the world.
I also seem to remember a while back (Mar/Apr) someone reported here that the
Canadian bureaucrat responsible for executing import/export rules said
that he didn't consider crypto to be restricted by Canada's rules.

@_date: 1995-07-26 08:28:57
@_author: Scott Brickner 
@_subject: Three strikes you're out! for politicians... yeah we wish! 
Not in the sense of being able to punish him.  The voters may only
withold their support in the next election.  Not nearly enough to
deter morons like Exon.

@_date: 1995-09-01 09:57:15
@_author: Scott Brickner 
@_subject: SSL search attack 
Daniel R. Oelke writes
I agree.  ACKing partial segments is a bad idea.  But, when a client
is given a block of segments, partial ACKing can let poorly connected
clients communicate with the server via e-mail, and still stay busy.
When the client completely finishes half of its segments, it ACKs them
and asks for that many more segments.  The fraction can be adjusted as
mean communications latency to the server is measured.  Ideally the
new segments arrive just as the client finishes the second half of its
original segments.  This way the segments are allocated as late as
possible, letting better connected clients have a better shot at them.

@_date: 1995-09-20 16:29:25
@_author: Sjb@austin.ibm.com 
@_subject: A problem with anonymity 
I was thinking about some issues related to electronic commerce, and it
occurred to me that there is a significant problem in conducting
business with untraceable pseudonyms (anonyms?).  The problem occurred
to me while considering inheritance.
If one operates a business under an anonym (as opposed to the sort of
conditionally traceable pseudonym proposed by AT&T in "Anonymous Credit
Cards" ),
there's a strategy for transferring unlimited funds to one's
Consider a business which typically has a lot of assets, but which are
offset by a lot of liabilities --- almost any sort of VAR will do, for
instance.  In your will, you leave the key to unlock a private message
to your heir, in which you hand over the information necessary to
assume your anonym.  Since the heir presumably has his own identity
(whether anonymous or not is immaterial, except to *his* heirs), and
the anonym can't be linked to you, he has no reason to care about
maintaining the reputation of the anonym.  In dismantling the anonym,
he sells its assets to his own identity at a fraction of their worth,
and defaults on the liabilities.
Since the anonym behaved reputably during its life, it developed what
would have been a credit-worthy reputation, had it been a (traceable)
pseudonym.  But, since there's nothing to link the anonym to its heirs
(or ancestors), the creditors of the anonym must eat the loss.
Since the process of taking an anonym from scratch to a positive
reputation would be reasonably short (presumably not too much longer
than taking a real name or pseudonym the same distance), especially when
helped along by being fed the profits from the legitimate business of
an ancestor anonym, it's likely that a single individual could pull off
such an asset transfer at least two or three times a decade, as well as
at inheritance time.
A market which permits anonyms to have credit based on reputation will
probably have a constant stream of defaults caused by such behavior,
representing a significant risk factor in extending credit to anonyms
which can't be predicted by reputation.
