
@_date: 2013-12-31 06:14:56
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
References: <4dcbb04f7c2c485eb43d18e222b0f9a8
Actually, somehow, I have a feeling of relief to see that major hardware
vendors don't seem to specifically work hand in hand with the NSA to
implement backdoors. I don't see that having a JTAG connector publicaly
accessible on a RAID controller as a hint for that. The other disclosures
also point to my conclusion that the NSA is mostly working on their
own. Of course, not all of Snowden's documents are released yet and
hence my feeling could be deceiving.
I thought it could be worse.
  Hannes

@_date: 2013-12-31 07:01:43
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
References: <4dcbb04f7c2c485eb43d18e222b0f9a8
 <20131231051456.GC25536
to work. The slide from der Spiegel shows that this infection only works
via close access method and a remote infection path would be available in the
future (the slide is from 2008, but we don't know if this actually exists
I guess the slide got accidentally chopped off in the talk or am I missing
The UPD+RC6 story does not make sense to me, too (how could they know
about the encryption algorithm if they didn't dissect the actual bytes). I
also don't believe that current state of TLS would help much preventing
those redirection attacks.
  Hannes

@_date: 2013-12-31 19:04:52
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
References: <4dcbb04f7c2c485eb43d18e222b0f9a8
 <20131231051456.GC25536
Sorry, no. It is absolutely important to be exhaustive and correct here.
Otherwise this whole thing could get out of hands and could get much worse.
There is a very big difference e.g. I (and a lot of other people too, I
guess) will react to vendors whose debug interfaces where just hijacked
by the NSA to install backdoors and where the vendors worked hand in
hand with the NSA to do so deliberately. And we cannot just assume that
because it looks like the easiest way to deal with this for us now and
blame others! Also, if this talk does not specifically say that those
vendors were working with the NSA, it would have been important to make
clear that we don't know and we cannot judge them by the facts presented
now. A lot of people, which seem to be really loud, often get this wrong.
If such FUD is spread against vendors, which in my opinion, do actually have a
valid interest in trying to stop those back doors, what do you think will a
lot of members of this community do? Cut off communication with those vendors,
place them on their I-will-never-work-there lists? And I say, that they will
still sell shitloads of trucks of hardware.
As a manager with no technical background on such an accused company,
what do you think will they do? Will they push things like secure boot
down our throats?  Will they make all the hardware much more closed
in fear this community does bad PR against them otherwise? Is that the
outcome we want?
On past Chaos Communication Congresses I really think those vendors would have
been cheered for having an open JTAG interface on a board. It seems days have
Until now I saw no facts that I distrust the major hardware vendors. I
already have a bad feeling with that but I need to be still reasonable
here, too. I cannot accuse those companies by the facts presented
until now. But essentially, it is important that this community does
work hand in hand with those vendors who are willing to and just got
exploited by the NSA to not bring them to the wrong conclusions and
make tampering with the hardware more hard but instead make open source
bios and firmwares that users can build and verify themselves. Make
documentation more open, show them people do care about that. If secure
boot or other means get established, show the users how they can use
that for *their* own good, build up *their* own crypto chains etc. Make
firmware source-code trackable via source repos, provides ways to rebuild
those code bit-by-bit. Provide repositories with changes, instead of
giant source code drops. Otherwise a new generation of NSA backdoors
will have it much easier to be really hidden in those hardware.
That may add additional costs for those companies. So show them it is worth
Yeah, the NSA and NSA only. Until now I have no facts that anyone but
the NSA does so deliberately.
Let's don't make it worse ourselfs. ;)
I don't want to see what the PR persons on those accused companies' twitter
feeds will have to go through now. I guess lots of overreaction is happening
now, which is not helpful at all.
  Hannes

@_date: 2014-01-01 05:02:05
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
References: <4dcbb04f7c2c485eb43d18e222b0f9a8
 <20131231051456.GC25536
 <20131231180452.GG31072
Most of the implants are installed without we surely know if the vendors
did know about that or am I missing something? Every implant needs a
dropper which installs it or access to the supply chain etc.
I also don't count RSA as a hardware vendor in this case, as the
backdoored RNG was included in their bSafe suite, which is purely
  Hannes

@_date: 2013-11-07 20:34:56
@_author: Hannes Frederic Sowa 
@_subject: fuck these guys 
References: <20131106085614.GA5661
 <87wqkkrryn.fsf
Also large scale encryption deployments mostly use hardware acclerated
crypto offloading which (I think for historical reasons) are not as easy
to audit and recompile as open-source code (at least currently). I guess
some companies can work around that and do their own ASIC designs but
most companies don't have the resources to do that.
I wonder how Google deals with the encryption of their links between
datacenters.  Either this could be done on a per node basis, i.e.
opportunistic encryption, or centralize encryption to their border routers.
My guess is that per-flow ipsec state resolving is too costly, processing
and memory wise, because either packets get dropped or get buffered
(leading to a waste of memory in case of a high peer count) before keys
could be resolved leading to degeneration in performance or having impacts
to the programs error handling, thus not being transparent. Maybe this
can be dealt with in some time but is certainly no drop-in replacement.
This makes me believe that centralizing approaches are mostly in use today
which use unverifiable crypto implementations in hardware and it depends
on how far we trust these implementations to protect us from goverment
spying activities. IMHO target dispersal is something one should strive
for especially when encryption is in use, but this is difficult and I
don't think it is possible to realize this currently in the scale it
would be needed.
Thus large-scale interception programs must become illegal, otherwise
it is just a matter of how much the intelligence services can throw
at it to technically break down such easier to implement centralized
encryption approaches. Certainly there are other subsystems on such a
router to exploit on those routers to make the encryption meaningless.
  Hannes

@_date: 2013-11-16 20:47:21
@_author: Hannes Frederic Sowa 
@_subject: RetroShare 
References: <2273118.NEfOTMQzUT
Of course it is possible to add memory/type unsafe implementations later.
Actually, RetroShare is developed in C++/QT.

@_date: 2014-01-01 20:33:28
@_author: Hannes Frederic Sowa 
@_subject: "To Protect and Infect" - the edges of privacy-invading technology 
References: <4dcbb04f7c2c485eb43d18e222b0f9a8
 <20131231051456.GC25536
 <20131231180452.GG31072
 <20140101040205.GJ31072
I was just referring to the Snowden documents.
Ok, CryptoAG is a story of its own, I agree. But they are not that much of a
major hardware vendor, either. Depends on which customer base you consider.
Agreed, but in the end it is important how they act in the long term. But
that needs more time to come until conclusions can be drawn. It is much more
difficult for hardware vendors to strike such good PR stunts as Google did.
Also, I guess, Google had this change in the works for a longer time,
otherwise I don't know if they could make the switch to crypto for
their internal cross-DC links so rapidly. It still seems a lot of work
+ testing and their services seem highly depending on good latency.
  Hannes

@_date: 2014-01-19 19:43:43
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
References: <1389950750.79148.YahooMailNeo
 <52DAE12D.9080406
 <52DC1500.3020805
Hi Hannes! :)
MPX was already committed to gcc trunk, so I hope this situation could
improve in future (it is reverted for 4.9 but I think it will come back
after the release in March).
(I am still not sure how this will be rolled out, maybe by switching
some software back to 32 bit to reduce the load on the pointer length
lookup tables.)
Maybe you can comment a bit on the code extraction process into compilable
There seems to be a semantic differences between the proofable
language and the language the extraction process targets in e.g. array
handling(e.g.  ocaml code) or just overflow handling in integers.
I guess Idris does not have this problem?
I always wondered if ats-lang would be the most suitable language for
writing more typesafe code?
  Hannes

@_date: 2014-01-20 14:42:34
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
References: <1389950750.79148.YahooMailNeo
 <52DAE12D.9080406
 <52DC1500.3020805
 <20140119184343.GF6302
 <52DD0E22.6000307
MPX uses CPU managed out-of-band (but in application memory)
page-table-alike structures to store the bound table entries. They get
updated via specific cpu instructions, which result in nops on todays
cpus (so you can execute mpx code on non-mpx cpus and just won't have the
bounds checking). They also made sure that non-MPX code that is linked
against MPX code can propagate unbounded pointers, so you don't need to
switch your whole operating system to MPX enabled code at once (I guess
that would aslo be a problem memory-wise, but Intel entered the DRAM
business again, too :) ). The x64 linux ABI has also been updated.
While passing parameters and returning, MPX will introduce new registers
to pass those bounds checks automatically between function calls. I
guess this enables faster function calls because the cpu does not need
to store those pointer bounds in the permanent pointer bound tables thus
eliminating the stress on the cpu caches.
You can find details here:
What would be interesting, especially for the linux kernel, is to restrict
jmp and callq addresses so it is impossible for an attacker to get control
over them and e.g. dispatch own code on network packet dismantling without
needing whole pointer checking infrastructure e.g.
IIRC this was already addressed in the talk.
I would also like to point to Software Foundations from Benjamin Pierce here,
as it also has some great material to learn Coq:
Code generation without heavy runtime would also be nice.
ATS uses plain C as an intermediate language and the whole language feels
pretty low-level, too. So it seems it is easily possible to compile these
programs freestanding and also link those to other programs, which is
quite a nice feature, especially if one wants to make incrementally use
of more checked languages.
Thanks for your additional remarks,
  Hannes

@_date: 2014-01-20 17:03:22
@_author: Hannes Frederic Sowa 
@_subject: Programming languages for a safe and secure future 
References: <1389950750.79148.YahooMailNeo
 <52DAE12D.9080406
 <52DC1500.3020805
 <20140119184343.GF6302
 <52DD0E22.6000307
 <20140120134234.GB27626
Just remembered there was some research on this already:
  Hannes
