
@_date: 1994-12-01 22:34:35
@_author: Matt Blaze 
@_subject: Paper available on new cipher 
Bruce Schneier and I have been designing a block cipher, called
"MacGuffin", based on a new variant of the standard Feistel (S-P)
network structure.  We are presenting a paper describing the cipher,
entitled "The MacGuffin Block Cipher Algorithm", at the
Leuven crypto algorithms workshop later this month.  Here's the
This paper introduces MacGuffin, a 64 bit ``codebook'' block cipher.
Many of its characteristics (block size, application domain,
performance and implementation structure) are similar to those of the
U.S. Data Encryption Standard (DES).  It is based on a Feistel
network, in which the cleartext is split into two sides with one side
repeatedly modified according to a keyed function of the other.
Previous block ciphers of this design, such as DES, operate on equal
length sides.  MacGuffin is unusual in that it is based on a {\em
generalized unbalanced Feistel network (GUFN)} in which each round of
the cipher modifies only 16 bits according to a function of the other
48.  We describe the general characteristics of MacGuffin architecture
and implementation and give a complete specification for the 32-round,
128-bit key version of the cipher.
A PostScript preprint of the paper is available via anonymous FTP from:
A forthcoming paper will discuss the characteristics GUFN structure
in more detail.
Comments and analysis greatly appreciated.

@_date: 1994-12-02 16:26:58
@_author: Matt Blaze 
@_subject: New version (1.2) of CFS now available 
Source code for the latest version of CFS (release 1.2), the Cryptographic
File System, is now available upon request for research and experimental
use in the US and Canada.
CFS pushes encryption services into the Unix(tm) file system.  It
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.  You must have
"root" access to install CFS.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze, "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.)
and in
    Matt Blaze, "Key Management in an Encrypting File System", Proc.
    Summer '94 USENIX Tech. Conference, Boston, MA, June 1994.
    (PostScript available by anonymous ftp from research.att.com
    in the file dist/mab/cfskey.ps.)
The new version differs from the version described in the
papers in a few ways:
* The encryption scheme has been strengthened, and now provides
greater security but with the online latency of only single-DES.
* Support for the smartcard-based key management system is not
included and a few of the tools are not included.
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. * User-contributed ports to a number of additional platforms.
* Hooks for adding new ciphers.
* 3-DES and MacGuffin encryption options.
* Timeout options allow automatic detach of encrypted directories
after a set time or period of inactivity.
CFS is being distributed as a research prototype; it is COMPLETELY
UNSUPPORTED software.  No warranty of any kind is provided.  We will
not be responsible if the system deletes all your files and emails the
cleartext directly to the NSA or your mother.  Also, we do not have
the resources to port the software to other platforms, although you
are welcome to do this yourself.  The software was developed under
SunOS and BSDI, and there are also unsupported user-contributed ports
available for AIX, HP/UX, Irix, Linux, Solaris and Ultrix.  We really
can't promise to provide any technical support at all, beyond the
source code itself.  We also maintain a mailing list for CFS users and
developers; subscription information is included with the source code.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US and Canada to US and
Canadian citizens and permanent residents.  Unfortunately, we cannot
make it available for general anonymous ftp or other uncontrolled
access, nor can we allow others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.  You may
 * not export it, in whole or in part, or cause or allow such export,
 * through act or omission, without prior authorization from the United
 * States government and written permission from AT&T.  In particular,
 * you may not make any part of this software available for general or
 * unrestricted distribution to others, nor may you disclose this software
 * to persons other than citizens and permanent residents of the United
 * States and Canada.  *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please send email to:
DO NOT REPLY DIRECTLY TO THE SENDER OF MESSAGE.  You must include a
statement that you are in the US or Canada, are a citizen or legal
permanent resident of the US or Canada, and have read and understand
the license conditions stated above.  Also include an email address in
a US or Canada-registered domain. The code will be sent to you via
email in a uuencoded compressed tarfile.

@_date: 1994-12-23 11:01:41
@_author: Matt Blaze 
@_subject: export adventure 
Some of you may recall that about a month ago I posted the long and tortured
story of my efforts to obtain a tempory export license for a so-called
"exportable" telephone security device.  I promised to report on what
actually happened when I tried to go in and out of the country with the
thing.  Well, I just got back. I'm in the process of writing up the details,
and I'll post a full account in the next few days, but the bottom line
is that I've come to the conclusion that Kafka must have been trying
to export something under one of these licenses when he wrote "The Trial".
(Nothing bad happened, mind you, just a twistied maze of essential
bureaucratic processes each more arcane than the last and all well
beyond the grasp of humble travelers such as I).

@_date: 1994-12-23 12:25:24
@_author: Matt Blaze 
@_subject: Why I have a 512 bit PGP key 
A while back, I generated a PGP key pair for use on my machine at
work, a Sun SparcStation sitting on the reasonably-well-protected-
from-outside-attack AT&T internal research network.  I selected a key
length of 512 bits.
My number theory friends tell me that this is weak by modern
standards; cracking my key would probably require within an order of
magnitude of the total computational effort expended in the recent
attack on RSA-129.  I even volunteered my key as a ``target'' for the
next such attack.  Still, I'm happy with my choice, or rather, I've
got so many other security things to worry about that compromise of my
private mail based on cryptanalysis of my dinky little public key to
obtain my private key is the last thing on my mind.  In fact, I kind
of like it that my key doesn't advertise pretensions of high
theoretical security when, in fact, there is very little at all in
The first problem, of course, is secret storage.  Modern networked
computers are awful at storing secrets.  (This, after all, is one of
the problems that crypto software like PGP aims to solve).  I suspect
my situation is reasonably typical, if not better than most.  My
computer at work sits on my desk (in my locked office), has its own
local disk, only I know the root password, I try to keep up with the
latest security patches, and I keep most of my files in encrypted form
under CFS.  I'm the only regular user of my workstation, and while I'm
at work I access it directly from the console.  The network to which
it is attached is AT&T's ``R&D Internet'', the same one that sits
behind the firewall described in Cheswick and Bellovin's great new
book.  I probably have at least average system administration and
general computer security skills, and I'm reasonably good about
practicing what I preach.  Sounds like a pretty secure machine from
which to run PGP, right?
I don't think so.  While my machine's operating system is pretty
self-sufficient, my own home directory sits on a remote file server
administered by people who are good at and have time to perform
essential services for me like taking backups.  This means that, no
matter how hard I try, it's impossible for me to be sure that none of
my files have been tampered with.  We use off-the-shelf NFS, which
means that for all practical purposes anyone with access inside the
firewall (that's about 50k people in my case) can replace any of my
files.  Furthermore, even though my office has a lock, I'm not the
only person in the world with the key (Bell Labs escrows office keys,
after all), and I've managed to pick the lock once or twice on days
when I left my key at home.  I really have no idea where my machine
has been or what software I'm typing at when I run PGP.
So where should I store my private key?  Well, I could, as some have
suggested, keep it on a floppy disk that I carry around with me
everywhere I go, but first of all, that's too inconvenient.  It also
sounds dangerous in practice.  A floppy disk is about the size of a US
passport, and I've already lost two of those.  That means I'd have to
replicate the key somehow anyway, so I might as well rely on the
reasonably well established backup procedure that protects me from
loss of the rest of the files in my home directory.  For all practical
purposes, I have to assume that my secret key file is public.
That leaves the passphrase to protect the secret key.  According to
Shannon, English text contains just over 1 bit per letter of real
information.  Even if we assume twice that to account for the added
twists and turns of phrase I'm inclined to add to a passphrase, I just
can't remember (or type) a phrase with anywhere near enough entropy to
approach the level needed to do justice to even a little 512 bit RSA
key.  I think the simplest cryptanalytic attack against me would be to
go after the passphrase-based encryption of the secret key file.
(You'd need a way to enumerate the most likely keys based on a hashed
passphrase, which is a problem not yet well studied in the
unclassified literature.  I suspect a solution not out of reach of a
determined adversary, however).
An even simpler attack would be to break in to my machine and replace
my copy of PGP (or my kernel, or my shell, or whatever) with one that
records the passphrase as I type it.  (No, I don't leave this as an
exercise to the reader!)
The next problem is with PGP itself.  While I haven't looked
carefully, it seems to be a well-engineered program, and it has a
number of design features that I admire.  However, I think the basic
model it implements sits at two high a level, making it inherently
unreliable for really sensitive traffic.  It's just too hard to use.
(Most of the problems could be fixed by pushing things to a lower
level, and I understand a number of people are working on this).  In
particular, I'm have to have too much involvement in each PGP
operation, and it's just too easy for me to do stupid things like:
I've done each of these dumb things at least once, and probably others
Don't get me wrong - I advocate the use of strong encryption as much
as the next nerd.  I'm just concerned about focusing so narrowly that
we lose sight of the larger security picture.  Perry Metzger once made
reference to cryptographic "size queens" who worry about key size and
nothing else - it's a phrase that rings true.  There's something to be
said for systems that chose their security parameters provide about
the same strength everywhere.  DES is a good example - a 128 bit key
DES could be made that is no less secure than the current 56 bit
version - by a few bits.  The engineering triumph is that the
"advertised" DES security parameter - the key size - tells close to
the truth about the overall security of the system.  (Of course, in
RSA-based systems, there's an added variable - advances in factoring -
that may make it prudent to include a significant margin for error,
especially for keys that must retain their strength over time).
I have a 1024 bit key at home.
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: 2.6.1
-----END PGP PUBLIC KEY BLOCK-----

@_date: 1994-12-23 12:38:05
@_author: Matt Blaze 
@_subject: Why I have a 512 bit PGP key 
[Speaking of dumb things: when I added my PGP key to the bottom of this
file a few minutes ago, I attached it to the pre-proofread version and
sent that instead of this one.  Sigh.  Here's the correct version.]
A while back, I generated a PGP key pair for use on my machine at
work, a Sun SparcStation sitting on the reasonably-well-protected-
from-outside-attack AT&T internal research network.  I selected a key
length of 512 bits.
My number theory friends tell me that this is weak by modern
standards; cracking my key would probably require within an order of
magnitude of the total computational effort expended in the recent
attack on RSA-129.  I even volunteered my key as a ``target'' for the
next such attack.  Still, I'm happy with my choice, or rather, I've
got so many other security things to worry about that compromise of my
private mail based on cryptanalysis of my dinky little public key to
obtain my private key is the last thing on my mind.  In fact, I kind
of like it that my key doesn't advertise pretensions of high
theoretical security when, in fact, there is very little at all in
The first problem, of course, is secret storage.  Modern networked
computers are awful at storing secrets.  (This, after all, is one of
the problems that crypto software like PGP aims to solve).  I suspect
my situation is reasonably typical, if not better than most.  My
computer at work sits on my desk (in my locked office), has its own
local disk, only I know the root password, I try to keep up with the
latest security patches, and I keep most of my files in encrypted form
under CFS.  I'm the only regular user of my workstation, and while I'm
at work I access it directly from the console.  The network to which
it is attached is AT&T's ``R&D Internet'', the same one that sits
behind the firewall described in Cheswick and Bellovin's great new
book.  I probably have at least average system administration and
general computer security skills, and I'm reasonably good about
practicing what I preach.  Sounds like a pretty secure machine from
which to run PGP, right?
I don't think so.  While my machine's operating system is pretty
self-sufficient, my own home directory sits on a remote file server
administered by people who are good at and have time to perform
essential services for me like taking backups.  This means that, no
matter how hard I try, it's impossible for me to be sure that none of
my files have been tampered with.  We use off-the-shelf NFS, which
means that for all practical purposes anyone with access inside the
firewall (that's about 50k people in my case) can replace any of my
files.  Furthermore, even though my office has a lock, I'm not the
only person in the world with the key (Bell Labs escrows office keys,
after all), and I've managed to pick the lock once or twice on days
when I left my key at home.  I really have no idea where my machine
has been or what software I'm typing at when I run PGP.
So where should I store my private key?  Well, I could, as some have
suggested, keep it on a floppy disk that I carry around with me
everywhere I go, but first of all, that's too inconvenient.  It also
sounds dangerous in practice.  A floppy disk is about the size of a US
passport, and I've already lost two of those.  That means I'd have to
replicate the key somehow anyway, so I might as well rely on the
reasonably well established backup procedure that protects me from
loss of the rest of the files in my home directory.  For all practical
purposes, I have to assume that my secret key file is public.
That leaves the passphrase to protect the secret key.  According to
Shannon, English text contains just over 1 bit per letter of real
information.  Even if we assume twice that to account for the added
twists and turns of phrase I'm inclined to add to a passphrase, I just
can't remember (or type) a phrase with anywhere near enough entropy to
approach the level needed to do justice to even a little 512 bit RSA
key.  I think the simplest cryptanalytic attack against me would be to
go after the passphrase-based encryption of the secret key file.
(You'd need a way to enumerate the most likely keys based on a hashed
passphrase, which is a problem not yet well studied in the
unclassified literature.  I suspect a solution is not out of reach of a
determined adversary, however).
An even simpler attack would be to break in to my machine and replace
my copy of PGP (or my kernel, or my shell, or whatever) with one that
records the passphrase as I type it.  (No, I don't leave this as an
exercise to the reader!)
The next problem is with PGP itself.  While I haven't looked
carefully, it seems to be a well-engineered program, and it has a
number of design features that I admire.  However, I think the basic
model it implements sits at too high a level, making it inherently
unreliable for really sensitive traffic.  It's just too hard to use.
(Most of the problems could be fixed by pushing things to a lower
level, and I understand a number of people are working on this).  In
particular, I'm forced to have too much involvement in each PGP
operation, and it's just too easy for me to do stupid things like:
I've done each of these dumb things at least once, and probably others
as well.
Don't get me wrong - I advocate the use of strong encryption as much
as the next nerd.  I'm just concerned about focusing so narrowly that
we lose sight of the larger security picture.  Perry Metzger once made
reference to cryptographic "size queens" who worry about key size and
nothing else - it's a phrase that rings true.  There's something to be
said for systems that offer security parameters that provide about
the same strength across various attacks.  DES is a good example - a 128
bit key DES could be designed that is at least as secure as the current
56 bit version - by at least few bits.  The engineering triumph is that
the "advertised" DES security parameter - the key size - tells close to
the truth about the overall security of the system.  (Of course, in
RSA-based systems, there's an added variable - advances in factoring -
that may make it prudent to include a significant margin for error,
especially for keys that must retain their strength over time).
I have a 1024 bit key at home.
-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: 2.6.1
-----END PGP PUBLIC KEY BLOCK-----

@_date: 1994-12-31 13:30:14
@_author: Matt Blaze 
@_subject: Exporting cryptographic materials, theory vs. practice 
DRAFT - PLEASE DO NOT QUOTE OR REDISTRIBUTE
Mostly to find out what the process was like, I recently applied for,
and received, a temporary export license for a so-called ``exportable''
telephone voice encryption device to take with me on a business trip to
England and Belgium.  I returned from the trip last week, device in hand.
Here's how it went.
The device in question is an AT&T ``Telephone Security Device (TSD)'',
model 3600-F.  This is the ``bump in a cord'' voice encryptor.  The
``F'' model is supposed to be approved for ``fast track'' export; it
doesn't use Clipper or DES, but rather some ``exportable'' algorithm.
This model is aimed primarily, I presume, at international business
travelers who want to communicate in a reasonably secure manner with
their home offices in the states.  In other words, a typical user
carries it with him or her when traveling abroad.  The particular
options that I got for the device included a James Bond-ish looking
acoustic coupler and handset for dealing with the hardwired phones
that are common in European hotel rooms.
About two months before my trip I called our (AT&T's) export lawyer
division.  It turns out that while there was some discussion in the
government about exempting from the export licensing process temporary
exports of cryptographic equipment used on business trips, this
exemption never actually took effect.  So even though the device I had
was already approved for sale abroad, I still needed to get a
temporary export license.  But they assured me that ``this is an easy,
routine process''.  Well, sure enough, about two weeks before I was to
leave I got back my official US State Department ``license for the
temporary export of unclassified defense articles''.  So far, so good.
According to the information printed on the license and additional
information from the lawyer, I have to leave from an international
airport with a customs agent present (no problem there).  At the
airport, I have to fill out a form called a ``shipper's export
declaration'' (SED) on which I have to declare that ``these
commodities are authorized by the US government for export only to
Belgium and the United Kingdom [the countries I'm visiting].  They may
not be resold, transshipped, or otherwise disposed of in any country,
either in their original form or incorporated into other end-items
without the prior written approval of the US Department of State''.
Then I'm to present the SED and export license to a customs official
at the airport before I leave.  The Customs officer is supposed to
take my SED and endorse my license to show what I'm actually taking
out of the country.
On the way back in, I'm supposed to ``declare'' my item (even though
it was manufactured in the US) and show them my license, and they're
supposed to endorse the license again to show that I have, in fact,
returned the ``defense article''.
The first hitch I ran into was that no one could actually tell me
where I could get an SED form.  But when I called customs they assured
me that this was no big deal.  ``Just come by customs at the airport,
and we stamp the form.  I guess you can just fill out the SED there''
they said, assuring me this is not a big deal.
I made sure to get to the airport early anyway.
Although there was moderately heavy traffic near the airport, I made
it to JFK two and a half hours before my 10pm flight.  I was flying
United, which has their own terminal at JFK, so Customs has an office
right there in the same building from which I was to depart (JFK is
awful to get around, so I was glad for this).  I checked in for my
flight (and got upgraded to first class, which bolstered my
expectation that everything was going to be really easy from here on).
Then, luggage, license and TSD in hand, I made my way downstairs to
Customs, expecting to fill out the SED form and ``just have my license
stamped'' as they had assured me earlier on the telephone.  I
explained my situation to the security guard whose job is to keep
people from going in to the Customs area, and he led me to ``the back
office'' without much argument or delay.  The head uniformed customs
guy in the back office (which I think is same office where they take
the people suspected of being ``drug mules'' with cocaine-filled
condoms in their stomaches) looked approachable enough.  He had a sort
of kindly, grandfatherly manner, and he was playing ``Doom'' on a
laptop computer.  I got the impression that most of the people he
encounters are suspected drug smugglers, and he seemed pleased enough
to be dealing with something a little different from the norm.  When I
explained what I was doing he looked at me as if I had just announced
that I was a citizen of Mars who hadn't even bothered to obtain a Visa
before leaving.
He explained, carefully, that a) I really do need the SED form; b) Not
only that, I should have already filled it out, in duplicate; c) He
doesn't have blank SED forms; d) he, like everyone else in the entire
US government that I had spoken to, has no idea where one gets them
from, but people must get them from somewhere; and e) it doesn't
really matter, because I'm in the wrong place anyway.
I asked him where the right place is.  ``The cargo building, of
course,'' he explained patiently.  I remembered the cargo building
because we passed it in the taxi just as the traffic jam began, about
half an hour before I got to the United terminal.  The airport shuttle
bus doesn't stop there.  I'd have to call a taxi.  ``But I think
they're closed now, and even if they were open you'd never make it
before your flight'' he helpfully added, saving me the trip.  He also
complemented for going to the trouble to get the license.
I must have looked hurt and confused.  Eventually he called in some
fellow in a suit who must have been his boss.
``Are you the guy who wants to export the fancy gun?'' the fellow in
the suit asked me.
``It's not a gun, it's a telephone,'' I responded, with a straight
``Why do you have a license to export a telephone?''  Good question, I
thought.   I explained about the ITAR, and showed him the thing.  He
agreed that it looked pretty harmless.
The guy in the suit reiterated points a through e almost verbatim (do
they rehearse for these things?) and explained about how this is a
State Department license, not a Customs license, and this doesn't
happen very much because exports usually go via The Cargo Building.
He'd love to help me, but the computer in which these things get
entered is over in Cargo.  ``That's how the records get made.  But you
do have a valid license, which is nice.''  He also suggested that I
would have an easier time had I shipped the device instead of carrying
it with me.
I asked what I should do, given that my plane was scheduled to leave
in less than an hour.  Neither was sure, but the fellow in the suit
seemed willing leave it to the discretion of the uniformed guy.  ``How
does this thing work, anyway?'' he asked.  I tried to explain as best
as I could, trying to make it sound as harmless as it is.  ``You mean
like that Clipper chip?'' he asked.
At this point, given that he has a laptop and knows something about
the Clipper chip, I figured that maybe there was some hope of making
my flight.  Or maybe I was about to spend the night in jail.  In my
mind, I put it at about a 90:10 hope:jail ratio.
Then he asked, ``Do you know about this stuff?''
So we chatted about computers and cryptography for a while.  Finally,
the two of them decided that it wouldn't really hurt for them to just
sign the form as long as I promise to call my lawyer and get the SED
situation straightened out ASAP.  They assured me that I won't be
arrested or have any other trouble upon my return.
I made my flight, validated license in hand.
An aside: Throughout my trip, I discovered an interesting thing about
the TSD and the various options I was carrying with it.  Under X-ray
examination, it looks just like some kind of bomb.  (I suspect it was
the coiled handset cords).  Every time I went through a security
checkpoint, I had to dig the thing out of my luggage and show it to
the guard.  I almost missed the new ``Eurostar'' chunnel train (3hrs
15mins nonstop London->Brussels, airport-style checkin and security)
as the guards were trying to figure out whether thing thing was about
to explode.
On the way back to the US, it took me a little over an hour to get
through Customs.  I carried all my luggage with me, and, expecting a
bit of a hassle, made sure to be the FIRST person to reach Customs.
The inspector was ready to wordlessly accept my declaration form and
send me on my way when I opened my mouth and explained that I needed
to get my export license stamped.  The inspector explained that this
had to be done by something called the ``Ships Office''.  I was sent
to an unoccupied back room (a different back room than before), and
told to wait.  I thought about the recent Customs experiences of Phil
Zimmermann.  After about half an hour of waiting, an officer came in
and asked me what I needed.  I explained that I needed to get my
export license endorsed, and she shrugged and told me that she had to
``process the flight'' first.  As best as I could tell, her job was to
clear the airplane itself through customs, that being, technically
speaking, a very expensive import.  It would take a little wile.  She
was pleasant enough, though, and at least didn't look at me as if she
intended to send me to jail or have me strip searched.
Finally, she finished with the plane and asked me for my form.  She
studied it carefully, obviously having never seen one before, and
eventually asked me what, exactly, she was supposed to do.  I
explained that I had never actually gone through this process before
but I understood that she's supposed to record the fact that I was
re-importing the device and stamp my form somewhere.  She explained
that she didn't know of any place for her to record this.  After some
discussion, we agreed that the best thing to do was to make a Xerox
copy of my license and arrange for it to go wherever it had to go
later.  She stamped the back of the license and sent me out.  It was a
little over an hour after I first reached the Customs desk.
My conclusion from all this is that it just isn't possible for an
individual traveler to follow the rules.  Even having gone through the
process now, I still have no idea how to obtain, let alone file, the
proper forms, even for a device that's already been determined to be
exportable.  The export of ITAR-controlled items is ordinarily handled
by cargo shipment, not by hand carrying by travelers, and the system
is simply not geared to deal with exceptions.  Technically speaking,
everyone with a laptop disk encryption program who travels
internationally is in violation of the ITAR, but since no one actually
knows this, no mechanism exists to deal with it.  While (fortunately)
everyone I dealt with was sympathetic, no one in the government who I
spoke with was actually able to help me follow the rules.  I was able
to leave and come back only because everyone involved eventually
recognized that my telephone was pretty harmless, that my intentions
were good, and that the best thing to do was be flexible.  Had anyone
taken a hard line and tried to enforce the letter of the law, I simply
wouldn't have been able to take the thing with me, even with my
license.  Had I simply taken it with me and come back instead of
calling attention to myself by trying to follow the rules, no one would
have noticed.
DRAFT - PLEASE DO NOT QUOTE OR REDISTRIBUTE

@_date: 1994-02-02 13:11:01
@_author: Matt Blaze 
@_subject: Notes on key escrow meeting with NSA 
A group from NSA and FBI met the other day with a group of us at Bell
Labs to discuss the key escrow proposal.  They were surprisingly
forthcoming and open to discussion and debate, and were willing to at
least listen to hard questions.  They didn't object when asked if we
could summarize what we learned to the net.  Incidentally, the people
at the meeting seemed to base a large part of their understanding of
public opinion on Usenet postings.  Postings to sci.crypt and
talk.politics.crypto seem to actually have an influence on our
A number of things came out at the meeting that we didn't previously
know or that clarified previously released information.  What follows
is a rough summary; needless to say, nothing here should be taken as
gospel, or representing the official positions of anybody.  Also,
nothing here should be taken as an endorsement of key escrow, clipper,
or anything else by the authors; we're just reporting.  These notes
are based on the collective memory of Steve Bellovin, Matt Blaze, Jack
Lacy, and Mike Reiter; there may be errors or misunderstandings.
Please forgive the rough style.  Note also the use of "~ ~" for
'approximate quotes' (a marvelous Whit Diffie-ism).
NSA's stated goals and motives for all this:
They indicated that the thinking was not that criminals would use key
escrowed crypto, but that they should not field a system that
criminals could easily use against them.  The existence of key escrow
would deter them from using crypto in the first place.  The FBI
representative said that they expect to catch "~only the stupid
criminals~" through the escrow system.
Another stated reason for key escrow is that they do not think that
even government-spec crypto devices can be kept physically secure.
They do expect enough to be diverted to the black market that they feel
they need a response.  NSA's emphasis was on the foreign black market...
There seems to be a desire to manipulate the market, by having the
fixed cost of key escrow cryptography amortized over the government
market.  Any private sector devices would have to sell a much larger
number of units to compete on price.  (This was somewhere between an
implication and an explicit statement on their part.)
When asked about cryptography in software, "~...if you want US
government cryptography, you must do it with hardware~".
Clipper chips should be available (to product vendors) in June.  You
can't just buy loose chips - they have to be installed in approved
products.  Your application interface has to be approved by NIST for
you to get your hands on the chips.
An interesting point came up about the reverse-engineering resistance
of the chips: they are designed to resist reverse engineering the data
in the chip without destroying the chip.  It is not clear (from the
information presented at the meeting) whether the chips are equally
resistant to destructive reverse-engineering to learn the skipjack
algorithm.   They said the algorithm was patented, but they may have
been joking.  ("~And if that doesn't scare you enough, we'll turn the
patent over to PKP.~")
The resistance to reverse engineering is not considered absolute by
NSA.  They do feel that "~it would require the resources of a national
laboratory, and anyone with that much money can design their own
cryptosystem that's just as strong.~"
They repeated several times that there are "~no plans to regulate the
use of alternate encryption within the US by US citizens.~"  They also
indicated they "~weren't naive~" and didn't think that they could if
they wanted to.
There were 919 authorized wiretaps, and 10,000 pen register monitors,
in 1992.  They do not have any figures yet on how often cryptography
was used to frustrate wiretaps.
They do not yet have a production version of the "decoder" box used by
law enforcement.  Initially, the family key will be split (by the same
XOR method) and handled by two different people in the athorized
agencies.  There is presently only one family key.  The specifications
of the escrow exploitation mechanism are not yet final, either; they
are considering the possibility of having the central site strip off
the outer layers of encryption, and only sending the session key back
to the decoder box.
The escrow authorities will NOT require presentation of a court order
prior to releasing the keys.  Instead, the agency will fill out a form
certifying that they have a legal authorization.  This is also backed
up with a separate confirmation from the prosecutor's office.  The
escrow agencies will supply any key requested and will not themselves
verify that the keys requested are associated with the particular court
The NSA did not answer a question as to whether the national security
community would obtain keys from the same escrow mechanism for their
(legally authorized) intelligence gathering or whether some other
mechanism would exist for them to get the keys.
The masks for the Clipper/Capstone chip are unclassified (but are
protected by trade secret) and the chips can be produced in an
unclassified foundry.  Part of the programming in the secure vault
includes "~installing part of the Skipjack algorithm.~" Later
discussion indicated that the part of the algorithm installed in the
secure vault are the "S-tables", suggesting that perhaps unprogrammed
Clipper chips can be programmed to implement other 80-bit key, 32 round
The Capstone chip includes an ARM-6 RISC processor that can be used for
other things when no cryptographic functions are performed.  In
particular, it can be used by vendors as their own on-board processor.
The I/O to the processor is shut off when a crypto operation is in
They passed around a Tessera PCMCIA (type 1) card.  These cards
contain a Capstone chip and can be used by general purpose PC
applications.  The cards themselves might not be export controlled.
(Unfortunately, they took the sample card back with them...)  The card
will digitally sign a challenge from the host, so you can't substitute
a bogus card.  The cards have non-volatile onboard storage for users'
secret keys and for the public keys of a certifying authority.
They are building a library/API for Tessera, called Catapult, that
will provide an interface suitable for many different applications.
They have prototype email and ftp applications that already uses it.
They intend to eventually give away source code for this library.
They responded favorably to the suggestion that they put it up for
anonymous ftp.
Applications (which can use the library and which the NSA approves for
government use) will be responsible for managing the LEAF field.  Note
that they intend to apply key escrowed Skipjack to other applications,
including mail and file encryption.  The LEAF would be included in
such places as the mail header or the file attributes.  This implies
that it is possible to omit sending the LEAF -- but the decrypt chip
won't work right if it doesn't get one.
When asked, they indicated that it might be possible wire up a pair of
Clipper/Capstone chips to not transmit the LEAF field, but that the
way to do this is "~not obvious from the interface we give you~" and
"~you'd have to be careful not to make mistakes~".  They gave a lot of
attention to obvious ways to get around the LEAF.
The unit key is generated via Skipjack itself, from random seeds
provided by the two escrow agencies (approximately monthly, though
that isn't certain yet).  They say they prefer a software generation
process because its correct behavior is auditable.
Capstone (but not Clipper) could be configured to allow independent
loading of the two key halves, in separate facilities.  "~It's your
money [meaning American taxpayers].~"
The LEAF field contains 80 bits for the traffic key, encrypted via the
unit key in "~a unique mode ~", 32 bits for the unit id, and a 16
bit checksum of some sort.  (We didn't waste our breath asking what the
checksum algorithm was.)  This is all encrypted under the family key
using "~another mode ~".
They expressed a great deal of willingness to make any sort of
reasonable changes that vendors needed for their products.  They are
trying *very* hard to get Skipjack and key escrow into lots of

@_date: 1994-02-04 12:10:15
@_author: Matt Blaze 
@_subject: Followup: Notes on key escrow meeting with NSA 
Newsgroups: sci.crypt,talk.politics.crypto,comp.org.eff.talk,alt.privacy.clipper
A couple of clarifications and new recollections.  Same disclaimers as
The NSA people were asked whether they would consider evaluating
ciphers submitted by the private sector as opposed to simply proposing a
new cipher as a "black box" as they did with Skipjack.  They said they
can't do this because, among other things, of the extraordinary effort
required to properly test a new cipher.  They said that it often takes
from 8-12 years to design, evaluate and certify a new algorithm, and
that Skipjack began development "~about 10 years ago.~"  I asked if we
should infer anything from that about the value of the (limited time
and resource) civilian Skipjack review.  They took that with good
humor, but they did say that the civilian review was at least
presented with and able to evaluate some of the results of NSA's
previous internal reviews.
Regarding the scale of the escrow exploitation system, they said that
they did not yet have a final operational specification for the escrow
protocols, but did say that the escrow agencies would be expected to
deliver keys "~within about 2 hours~"  and are aiming for "~close to
real time.~" Initially, the FBI would have the decoder box, but
eventually, depending on costs and demand, any law enforcement agency
authorized to conduct wiretaps would be able to buy one.  The two
escrow agencies will be responsible for verifying the certification
from and securely delivering the key halves to any such police
As an aside, we've since been informed by a member of the civilian
Skipjack review committee that the rationale for not having the escrow
agency see the actual wiretap order is so that they do not have access
to the mapping between key serial numbers and people/telephones.
Also, on second reading, I wasn't at all clear about the reverse
That is, the chips are designed to resist non-destructive reverse
engineering to obtain the unit keys.  They do not believe that it is
possible to obtain the unit key of a particular chip without destroying
the chip.  They did not present any assertions about resistance to
destructive reverse engineering, such that several chips can be taken
apart and destroyed in the process, to learn the Skipjack algorithm.
Finally, I should have made clear that "Clipper" is more properly
called the "MYK-78T".

@_date: 1994-02-23 13:45:55
@_author: Matt Blaze 
@_subject: Dorthoy Denning editorial, Newsday 
Note: I'm just passing this on.  I am only the messenger.
------- Forwarded Message
Return-Path: research!cs.georgetown.edu!denning
Received: from big.l1135.att.com by codex.UUCP (4.1/4.7)
Received: from research (research.research.att.com) by big.l1135.att.com (4.1/4.7)
Posted-Date: Wed, 23 Feb 1994 16:16:09 -0500 (EST)
Received: by ninet.research.att.com; Wed Feb 23 16:17 EST 1994
Received: from cs (cs.cosc.georgetown.edu) by guvax.acc.georgetown.edu (PMDF
 V4.2-11  id <01H98BXBMQA88YCH3A at guvax.acc.georgetown.edu>; Wed,
 23 Feb 1994 16:16:33 EST
Received: from chair by cs (4.1/SMI-4.1.2) id AA01896; Wed,
 23 Feb 94 16:16:09 EST
Message-Id: <9402232116.AA01896 at cs>

@_date: 1994-02-25 14:13:01
@_author: Matt Blaze 
@_subject: Rivest's response to Denning Newsday Editorial 
Forwarded with permission...
------- Forwarded Message
Return-Path: research!theory.lcs.mit.edu!rivest
Received: from big.l1135.att.com by codex.UUCP (4.1/4.7)
Received: from research (research.research.att.com) by big.l1135.att.com (4.1/4.7)
Posted-Date: Fri, 25 Feb 94 16:24:20 EST
Received: by ninet.research.att.com; Fri Feb 25 16:21 EST 1994
Received: from SWAN.LCS.MIT.EDU by theory.lcs.mit.edu (5.65c/TOC-1.2S) Received: by swan.lcs.mit.edu (5.65c/TOC-1.2C) Message-Id: <199402252124.AA01277 at swan.lcs.mit.edu>
        silvio at theory.lcs.mit.edu, smb at research.att.com, mab at research.att.com,
        jim at rsa.com, diffie at eng.sun.com
Hi Dorothy --
Thanks for sending me a copy of your editorial.  But I find the
reasoning you present misleading and unpersuasive.
First, you argue that the clipper chip will be a useful law
enforcement tool.  Given the small number of currently authorized
wiretaps per year (under 1000) and the ease of using alternative
encryption technology or superencryption, it seems plausible to me
that law enforcement could expect at most ten "successful" clipper
wiretaps per year.  This is a pretty marginal basis for claiming that
clipper will "block crime".
Second, you seem to believe that anything that will "block crime" must
therefore be a "good thing" and should therefore be adopted.  This is
not true, even if it is not subject to government abuse.  For example,
a system that could turn any telephone (even when on-hook) into an
authorized listening microphone might help law enforcement, but would
be unacceptable to almost all Americans.  As another example, tatooing
a person's social security number on his or her buttocks might help
law enforcement, but would also be objectionable.  Or, you could
require all citizens to wear a bracelet that could be remotely queried
(electronically, and only when authorized) to return the location of
that citizen.  There are all kinds of wonderfully stupid things one
could do with modern technology that could "help" law enforcement.
But merely being of assistance to law enforcement doesn't make a
proposal a good thing; many such ideas are objectionable and
unacceptable because of the unreasonably large cost/benefit ratio (real or psychological cost). The clipper proposal, in
my opinion, is of exactly this nature.
Third, you seem unnecessarily polly-annish about our government and the
potential for abuse.  The clipper proposal places all trust for its
management within the executive branch; a corrupt president could
direct that it be used for inappropriate purposes.  The unspecified
nature of many of the associated procedures leaves much room to
speculate that there are "holes" that could be exploited by government
officials to abuse the rights of American citizens.  Even if the
proposal were modified to split the trust among the various branches
of government, one might still reasonably worry about possible abuse.
Merely because you've met the current set of representatives of
various agencies, and feel you can trust them, doesn't mean that such
trust can be warranted in their successors.  One should build in
institutional checks and balances that overcome occasional moral
lapses in one or more office holders.
Fourth, your discussion of "searching your home and seizing your
papers" is misleading.  You seem to imply that because law enforcement
can be issued a warrant to search your home, that we should adopt
clipper.  Yet this analogy only makes sense if individuals were
required to deposit copies of their front door keys with the
government.  I can build any kind of house I wish (out of steel, for
example), and put any kind of locks on it, and wire up any kind of
intrusion detectors on it, etc.  The government, armed with a search
warrant, is not guaranteed an "easy entry" into my home at all.  The
appropriate analogical conclusion is that individuals should be able
to use any kind of encryption they want, and the government should be
allowed (when authorized, of course) to try and break their
Finally, you argue (elsewhere, not in this editorial) that the decision
rests in part on "classified" information.  Such an argument only makes
sense if there is a specific law-enforcement situation that makes such
classified information timely and relevant.  (E.g., if there was a
current investigation as to whether the Department of the Treasury had
been infiltrated by organized crime.)  The use of "classified information"
is otherwise generally inappropriate in discussing communications policy
that will last over decades.  This hardly covers all of the relevant issues, but it covers the
points that came immediately to mind in reading your editorial...
P.S. Feel free to pass along, quote, or otherwise re-distribute this...

@_date: 1994-01-01 21:14:19
@_author: Matt Blaze 
@_subject: CFS source code available January 12 
Source code for version 1.0 of CFS, the Cryptographic File System,
will be distributed upon request in the United States starting on
January 12, 1994.
CFS pushes encryption services into the Unix(tm) file system.  CFS
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze, "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.
The version being released differs from the version described in the
paper in a few ways:
* The encryption scheme has been strengthened, and now provides
approximately the security of 3-DES with the online latency of only
* Support for the smartcard-based key management system is not included.
* A few of the tools are not included (in particular, cname and ccat).
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. CFS is being distributed as COMPLETELY UNSUPPORTED software.  No
warranty of any kind is provided.  We will not be responsible if it
deletes all your files and emails the cleartext directly to the NSA or
your mother.  Also, we do not have the resources to port the software
to other platforms, although you are welcome to do this yourself.
(Note in particular that CFS has not been tested on either Solaris or
Linux, and we have no plans ourselves to support either of these
systems.)  We really can't promise to provide any technical support at
all, beyond the source code itself.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US to US citizens and
permanent residents.  Unfortunately, we cannot make it available for
general anonymous ftp or other uncontrolled access, nor can we allow
others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.  You may
 * not export it, in whole or in part, or cause or allow such export,
 * through act or omission, without prior authorization from the United
 * States government and written permission from AT&T.  In particular,
 * you may not make any part of this software available for general or
 * unrestricted distribution to others, nor may you disclose this software
 * to persons other than citizens and permanent residents of the United
 * States.  *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please send email to:
DO NOT REPLY TO DIRECTLY TO THIS MESSAGE.  Be sure to include a
statement that you are in the United States, are a citizen or
permanent resident of the US, and have read and understand the license
conditions stated above.  Also include an email address in a
US-registered domain, and say whether you'd also like to be included
on a developer/user mailing list that is being set up.
For a number of reasons, I am unable actually send out code until
January 12, 1994.  Unless you specify some other format, you'll get a
uuencoded compressed tarfile.
I'll be at the January USENIX conference in San Francisco, and will
announce CFS at the WIP session there.

@_date: 1994-01-05 15:44:26
@_author: Matt Blaze 
@_subject: automatic mail scanning software 
I was just cleaning up my office, throwing out a bunch of vendor
literature from a recent unixexpo, when a flyer for a product called
"MpScan" from an outfit called "CyberSoft" caught my eye.
This product, as advertised, "automatically searches outgoing email
for company classified material".  Aside from being configurable
to do stuff like block mail to certain addresses, it also " ...uses
the powerful, user-tested CVDL scanning language..." and can generate
"...reports which  can be used to look for unusual changes in
Email usage...". A "version 2" promises "many more feaures using
an AI engine".
All this can be yours for only $49,000 per mail server, or $200,000
per site license.  You get free upgrades until the end of 1997.

@_date: 1994-07-19 11:56:32
@_author: Matt Blaze 
@_subject: CFS 1.1.0 now available 
A new release of CFS, my encrypting file system for Unix-ish platforms,
is now available,  This version includes a number of bug fixes and ports
to new platforms, reasonably friendly hooks for adding new ciphers, and an
online 3-DES mode.  Details in the announcement attached below.
Source code for version 1.1 of CFS, the Cryptographic File System, is
now available upon request for research and experimental use in the US
and Canada.
CFS pushes encryption services into the Unix(tm) file system.  It
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.  You must have
"root" access to install CFS.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze, "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.)
The version being released differs from the version described in the
paper in a few ways:
* The encryption scheme has been strengthened, and now provides
approximately the security of 3-DES with the online latency of only
* Support for the smartcard-based key management system is not
* A few of the tools are not included (in particular, cname and ccat).
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. New features in CFS 1.1 include:
* User-contributed ports to a number of additional platforms.
* Better hooks for adding new ciphers.
* 3-DES encryption option.
CFS is being distributed as a research prototype; it is COMPLETELY
UNSUPPORTED software.  No warranty of any kind is provided.  We will
not be responsible if the system deletes all your files and emails the
cleartext directly to the NSA or your mother.  Also, we do not have
the resources to port the software to other platforms, although you
are welcome to do this yourself.  The software was developed under
SunOS and BSDI, and there are also unsupported user-contributed ports
available for AIX, HP/UX, Irix, Linux, Solaris and Ultrix.  We really
can't promise to provide any technical support at all, beyond the
source code itself.  We also maintain a mailing list for CFS users and
developers; subscription information is included with the source code.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US and Canada to US and
Canadian citizens and permanent residents.  Unfortunately, we cannot
make it available for general anonymous ftp or other uncontrolled
access, nor can we allow others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.  You may
 * not export it, in whole or in part, or cause or allow such export,
 * through act or omission, without prior authorization from the United
 * States government and written permission from AT&T.  In particular,
 * you may not make any part of this software available for general or
 * unrestricted distribution to others, nor may you disclose this software
 * to persons other than citizens and permanent residents of the United
 * States and Canada.  *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please send email to:
DO NOT REPLY DIRECTLY TO THIS MESSAGE.  Be sure to include a statement
that you are in the US or Canada, are a citizen or permanent resident
of the US or Canada, and have read and understand the license
conditions stated above.  Also include an email address in a US or
Canada-registered domain. The code will be sent to you via email in a
uuencoded compressed tarfile.

@_date: 1994-07-26 12:18:32
@_author: Matt Blaze 
@_subject: CYPHERPUNKS TO THE RESCUE 
As Eric Hughes points out (a couple of messages after these), you don't
need public-key signatures for this; any secret key cipher or hash function
will do, since the  base and remote trust each other unconditionally
(at least for garage doors; nuclear weapons may be a different story).
Both base and remote need to store a shared key and a counter; the remote needs a transmitter and the base needs a receiver.  To authenticate
itself, the remote sends {counter, hash(key,counter)} and then increments
its counter.  The base calculates the hash for the received counter value,
verifies that it matches the received hash value, verifies that the counter
increases the stored counter value, stores the new value,  and opens
the door.  A practical system system also probably include some mechanism
for rekeying and for zeroizing the counters.
There is no need for public key cryptography, two way communication (except
for key setup), synchronized clocks, or extensive storage at either side.
This protocol as described is very simple, almost trivial; given the right
constraints it follows almost directly from the problem.  I mention
it because very small variations and poorly chosen parameters render
it vulnerable to several classic protocol failures.
First, observe that this system has a work factor to break of no more than
the SMALLER of the secret hash key and the size of the hash output.  Clearly,
a single {counter, hash(key,counter)} message contains enough information to
permit an conventional exhaustive search for key.  If the hash space is
too small (say, 16 bits or so), the adversary can select an unused counter
value and probe the receiver with random hash values until the door opens.
Worse, if the bad guy selects a counter value that is much larger than the remote's counter value, it has the added bonus of denial-of-service to the
real user.
Also, note that the order of operation on the receiver's part is critical.
If the received counter value is stored BEFORE the hash is received, we
are also vulnerable to denial-of-service (but at least not false
Finally, there is the "man in the middle" attack, in which the bad guy
intercepts a message intended for but never received by the base,
records it, and plays it back later (but before the real owner returns
to increment the counter again).  A likely scenario involves pushing the
button twice on return home, but where only the first message is received by
the base. One way to deal with this is to encourage frequent resyncs between
the base and remote; for example, the remote, when in the garage, could send
periodic "null" commands that increment the counters without actually
opening the door.  (Of course, you'd need to make sure that these messages
themselves cannot be used to construct spoofed open-door messages.)  Basing
the counter in part on a real-time clock would also help here, but again,
this complicates the protocol greatly and increases the opportunities for
both denial-of-service (if the clocks get too far out of sync) and false
authentication (if the clocks get reset - say at daylight savings time...)
My point is not that this is a particularly hard problem, only that
even simple cryptographic protocols can have serious bugs.

@_date: 1994-07-26 13:53:29
@_author: Matt Blaze 
@_subject: New Threat on the Horizon: Software Key Escrow 
If it's the same scheme that I'm thinking of (that Dorothy Denning
presented at the Karlshrue workshop), it was developed by Stephen
Walker and David Balenson of Trusted Information Systems, in
cooperation with NIST.
It's a cute scheme - it doesn't involve secret hardware or algorithms, but
does involve public key cryptography, roughly in place of the clipper
unit and family keys.  You can thwart the system with cooperation at both
ends, but you can't interoperate with legal users; in this sense it's
more robust against abuse than the Clipper hardware-based system
The basic idea is that each user gets a unique public key from the
government, which is used to encrypt the session key.  You encrypt the
session key with this key and send both it and the certified public key
to the reciever, who verifies the signature to confirm that it really was
issued by the government.  Now the receiver also encrypts the session key
and compares the result with what you sent, refusing to operate if they
don't match.
Of course, two parties can cheat by patching their verification routines.
But it's very hard to interoperate with non-rogues.

@_date: 1994-06-06 16:54:56
@_author: Matt Blaze 
@_subject: Paper available via ftp 
A preliminary draft of my paper, "Protocol Failure in the Escrowed
Encryption Standard" is now available via anonymous ftp from
research.att.com in the file /dist/mab/eesproto.ps .  The paper is
in PostScript format and seems to print on most PS printers.
This is only a preliminary draft; the final published version
will likely include additional material on the production version
of the PCMCIA card, which, I understand, will differ in some
respects from the prototype I examined.

@_date: 1994-09-04 10:54:27
@_author: Matt Blaze 
@_subject: Final version of Clipper Protocol Failure paper 
The "final" pre-print version (dated August 20, 1994) of my paper, "Protocol
Failure in the Escrowed Encryption Standard" is now available.  You can get
it in PostScript form via anonymous ftp from research.att.com in the file
(June 3) version that previously occupied the same file.  Most of the
substance is identical, although few sections are expanded and a few minor
errors are now corrected.  I'd appreciate it if anyone who's citing
the paper use this version.
Only PostScript format is available.  Sorry.
This paper will be presented at the 2nd ACM Conference on Computer and
Communications Security in Fairfax in November.

@_date: 1994-09-16 21:56:19
@_author: Matt Blaze 
@_subject: RC4 article in Saturday (Sept 17) New York Times 
John Markoff has a piece on the RC4 betrayal in the Business section of
the Saturday NY Times (page 37), "A secret computer code is out -- Key
to data security appears on internet".  Not much that hasn't already
been said here or on sci.crypt, but there was an interesting quote from
Jim Bidzos that suggested that one of the conditions RSADSI agreed to
in order to get approval of 40 bit RC4 for export in shrink-wrap software
included keeping the algorithm confidential.  Bidzos speculated that
the NSA could revoke RC4's export status as a result of the
Also, the piece reports that "The RC4 formula was first circulated on
Tuesday to a specialized computer network mailing list of computer
researchers who oppose the Government's stringent controls on data
encryption technology.  The mailing list, which has thousands of
computer users around the world, is known as Cypherpunks, and the
mailings usually consist of highly technical discussions of data
encryption technology."
I guess Markoff gets Eric Blossom's moderated version of the list :-)

@_date: 1995-12-05 12:51:51
@_author: Matt Blaze 
@_subject: latest librand source now available 
[Sorry if this is a duplicate; my machine had a bogus sendmail.cf
when I first sent this.]
Souce code for the latest version of librand (a random
number package based on event interval variations) for
Unix-like machines is now available in:
  ftp://ftp.research.att.com/dist/mab/librand.shar
There are no restictions on use or distribution of this
code, which was written by Matt Blaze, Jack Lacy, and
Don Mitchell.

@_date: 1995-12-07 14:12:45
@_author: Matt Blaze 
@_subject: revised librand now on ftp.research.att.com site 
Sorry for the noise, but I just discovered that the librand.shar file
that I put in my ftp directory got garbled somewhere along the line.
I've repaired the damage and put a correct version of the file in:
    ftp://ftp.research.att.com/dist/mab/librand.shar
(Worst of all, the garbled version actually compiled, but you can
tell you have it by its failure to link properly).
Again sorry for the noise.

@_date: 1995-12-10 19:14:36
@_author: Matt Blaze 
@_subject: Paul Kocher's timing attack 
Paul Kocher's brutally clever timing attack against on-line
implementations of RSA, DSA and fixed-exponent Diffie-Hellman
is reported on page A1 of Monday's New York Times ("Secure Digital
Transactions Just Got a Little Less Secure" by John Markoff).
The attack requires only a few thousand ciphertext samples and works
against most implementations of public-key cryptosystems in which
the attacker can measure accurately the target's computation time for
each sample.
I think Kocher's paper is online somewhere; I'll post the URL
when I find it.

@_date: 1995-12-14 10:59:02
@_author: Matt Blaze 
@_subject: CryptoLib 1.0 now available 
[Note! This is posted for Jack Lacy; please direct responses to him
at cryptolib at research.att.com.  -matt]
Announcing CryptoLib - Release 1.0		12/13/95
   Jack Lacy, AT&T Bell Labs
CryptoLib is a portable and efficient library of primitives
for building cryptographic applications.  It runs under most versions
of Unix as well as DOS, Windows and Windows-NT (and 95).
We are pleased to make CryptoLib source code available without charge
to researchers and developers in the US and Canada.  (Because of export
restrictions on cryptographic software, we are only able to make the
software available within the US and Canada to US and Canadian citizens
and US permanent residents.)
CryptoLib is intended for research and experimental use, and is
distributed without warranty or support.  In particular, please
note the following license conditions:
 *              Copyright (c) 1995 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software may be subject to export controls.
 *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
The attached describes the library and some timing results.
To obtain source code send electronic mail to:
with a statement of the following form:
CryptoLib includes the following:
Some timing information:
All times assume 32X32 bit assembly of multiplication primitives.
bigPow times (a^b mod c with a,b,c the same length)
bigPow		0.12s		0.72s		Sparc II Brickell bigpow
Strong Prime Generation -- ProbTestAttempts = 5
100 primes generated in each test.  Times below are:
(total test time)/100 = avg. time per prime generated.
Using Gordon's strong prime algorithm.
    256		512	1024	Machine
    ---		---	----	-------
    2.8s	24.0s	5.11m	Sparc II     .45s	2.7s	77s	100 MHz pentium
encrypt		30ms	50ms	80ms		sparc II
decrypt		160ms	480ms	930ms
encrypt		15ms	33ms	56ms		100 MHz Pentium (Under NT)
decrypt		38ms	104ms	214ms
sign		99ms	166ms	216ms		sparc II (Brickell speedup)
verify		156ms	316ms	416ms
sign		21ms	38ms	49ms		100 MHz Pentium (Under NT)
verify		27ms	43ms	71ms

@_date: 1995-12-15 02:26:49
@_author: Matt Blaze 
@_subject: CryptoLib 1.0 now available 
Although it's very tempting to come up with an elaborate confession to
being part of the big conspiracy of the powers-that-be to suppress our
most brilliant cryptologic discoveries, I must admit that in fact you have
reached an incorrect conclusion.  I added the quantization stuff to
CryptoLib over the weekend right after I read Kocher's paper.  I posted the
routines to cypherpunks and sci.crypt yesterday.  You must have missed it.
Also, it should be pointed out that the idea that timing information
can leak information (like bit density) about keys has been well-known
for a long time.  I understand that NSA cryptosystems have long required
fixed response time for some (but not all...) cryptographic primitives in
comsec equipment.  But understanding that timing information might be
a threat in principle is not the same as understanding how to exploit it
in practice.
Kocher's observations are very, very surprising.

@_date: 1995-12-15 18:56:29
@_author: Matt Blaze 
@_subject: simple Unix CPU time quantization functions available 
I've made available a simple CPU time quantization package that
provides a simple interface to encapsulate code blocks that must
run in a multiple of a coarse-grained "quantized" amount of CPU
time.  It is useful in building various on-line cryptographic
protocols in which an attacker could otherwise learn key information
by observing the time the target takes to perform calculations that
use the secret (c.f., Paul Kocher's recent attacks).
The basic idea is that you can specify a "quantum" such that at the
end of an encapsulated block the CPU will busy-wait until the next
quantum multiple.  Fine-grained (below the quantum) timing information
is thereby denied to the observer (including unprivileged processes on
the same machine).  The code is quick-and-dirty and only runs on
Unix-centric platforms.  Test and use at your own risk.
There are (basically) no restrictions on the use or distribution
of the (very simple) code.
Get it from:
The quantize package is also part of Jack Lacy's cryptolib package (watch
this space for details).

@_date: 1995-12-19 12:02:14
@_author: Matt Blaze 
@_subject: revised time quantization package (Unix & WIN32) available 
A revised version of my simple CPU time quantization package is
now available for most Unix and, thanks to the efforts of Frank
O'Dwyer (Rainbow Diamond Ltd), WIN32 platforms.  The package provides
a simple interface to encapsulate code blocks that must run in a
multiple of a coarse-grained "quantized" amount of CPU time.  It
is useful in building various on-line cryptographic protocols in
which an attacker could otherwise learn key information by observing
the time the target takes to perform calculations that use the
secret (c.f., Paul Kocher's recent attacks).
The basic idea is that you can specify a "quantum" such that at
the end of an encapsulated block the CPU will busy-wait until the
next quantum multiple.  Fine-grained (below the quantum) timing
information is thereby denied to the observer (including unprivileged
processes on the same machine).  The code is quick-and-dirty and
only runs on Unix-centric and WIN32-based platforms.  Test and use
at your own risk.
There are (basically) no restrictions on the use or distribution
of the (very simple) code.
Get it from:
The quantize package is also part of Jack Lacy's cryptolib package
(watch this space for details).

@_date: 1995-12-26 05:43:09
@_author: Matt Blaze 
@_subject: New release (v1.3.2) of CFS encrypting file system available 
[Perhaps this isn't the best time for me to post this here; apologies
in advance if this sparks another flame fest.  -matt]
Source code for the latest version (release 1.3.2) of CFS, the Cryptographic
File System, is now available upon request for research and experimental
use in the US and Canada.
CFS pushes encryption services into the Unix(tm) file system.  It
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.  You must have
"root" access to install CFS.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze. "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.)
and in
    Matt Blaze. "Key Management in an Encrypting File System", Proc.
    Summer '94 USENIX Tech. Conference, Boston, MA, June 1994.
    (PostScript available by anonymous ftp from research.att.com
    in the file dist/mab/cfskey.ps.)
Version 1.3 of CFS also includes ESM, the Encrypting Session Manager.
ESM provides shell-to-shell encrypted sessions across insecure links
and requires no OS or network support.  It is useful for typing cfs
passphrases when logged in over the network.  ESM needs RSAREF 2.0 to
compile and is tested only on SunOS and BSDI.  ESM is the first released
part of a suite of session encryption tools that are described in
    Matt Blaze and Steve Bellovin. "Session-layer Encryption."
    Proc. 1995 USENIX Security Workshop, Salt Lake City, June 1995.
    (PostScript is available from
    ftp://research.att.com/dist/mab/sesscrypt.ps)
The new version of CFS differs from the version described in the
papers in a few ways:
* The DES-based encryption scheme has been strengthened, and now
provides greater security but with the online latency of only single-DES.
* Support for the smartcard-based key management system is not
included and a few of the tools are not included.
* An impoved key management scheme now allows chaning the passphrase
associated with a directory.
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. * User-contributed ports to a number of additional platforms.
* Hooks for adding new ciphers.
* 3-DES, MacGuffin, and SAFER-SK128 encryption options.
* Timeout options allow automatic detach of encrypted directories
after a set time or period of inactivity.
CFS is distributed as a research prototype; it is COMPLETELY
UNSUPPORTED software.  No warranty of any kind is provided.  We will
not be responsible if the system deletes all your files and emails the
cleartext directly to the NSA or your mother.  Also, we do not have
the resources to port the software to other platforms, although you
are welcome to do this yourself.  The software was developed under
SunOS and BSDI, and there are also unsupported user-contributed ports
available for AIX, HP/UX, Irix, Linux, Solaris and Ultrix.  We really
can't promise to provide any technical support at all, beyond the
source code itself.  We also maintain a mailing list for CFS users and
developers; subscription information is included with the source code.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US and Canada to US and
Canadian citizens and permanent residents.  Unfortunately, we cannot
make it available for general anonymous ftp or other uncontrolled
access, nor can we allow others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994, 1995 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.
 *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please read to the end
of this message and then send email to:
DO NOT REPLY DIRECTLY TO THIS MESSAGE.  You must include a statement
that you are in the US or Canada, are a citizen or legal permanent
resident of the US or Canada, and have read and understand the license
conditions stated above.  Be sure to include an email address in a US-
or Canada-registered domain. The code will be sent to you via email in
a "shar" shell archive (a little over 300K bytes long).

@_date: 1995-12-28 22:32:28
@_author: Matt Blaze 
@_subject: New! Improved!  CryptoLib 1.1 now available. 
[Note:  This is posted on behalf of Jack Lacy, whose news posting
software is hopelessly broken; please direct responses to him at
cryptolib at research.att.com.  Jack spent the better part of the last
two weeks getting a new release of CryptoLib out the door.  This version,
which should be stable for a while, fixes a few x86 problems, improves
the interfaces to some of the functions, and now allows you to use your
own random number generator to create key material.  If you don't
already have CryptoLib, be the first nerd on your block to get the
new release.  -matt]
Announcing CryptoLib - Release 1.1		12/21/95
   Jack Lacy, AT&T Bell Labs
CryptoLib is a portable and efficient library of primitives
for building cryptographic applications.  It runs under most versions
of Unix as well as DOS, Windows and Windows-NT (and 95).
We are pleased to make CryptoLib source code available without charge
to researchers and developers in the US and Canada.  (Because of export
restrictions on cryptographic software, we are only able to make the
software available within the US and Canada to US and Canadian citizens
and US permanent residents.)
CryptoLib is intended for research and experimental use, and is
distributed without warranty or support.  In particular, please
note the following license conditions:
 *              Copyright (c) 1995 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software may be subject to export controls.
 *
 * NOTE:
 * Some of the algorithms in cryptolib may be covered by patents.
 * It is the responsibility of the user to ensure that any required
 * licenses are obtained.
 *
 * SOME PARTS OF CRYPTOLIB MAY BE RESTRICTED UNDER UNITED STATES EXPORT
 * REGULATIONS.
 *
 *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
The attached describes the library and some timing results.
To obtain source code send electronic mail to:
with a statement of the following form:
CryptoLib includes the following:
Some timing information:
All times assume 32X32 bit assembly of multiplication primitives.
bigPow times (a^b mod c with a,b,c the same length)
bigPow		0.12s		0.72s		Sparc II Brickell bigpow
Strong Prime Generation -- ProbTestAttempts = 5
100 primes generated in each test.  Times below are:
(total test time)/100 = avg. time per prime generated.
Using Gordon's strong prime algorithm.
    256		512	1024	Machine
    ---		---	----	-------
    2.8s	24.0s	5.11m	Sparc II     .45s	2.7s	77s	100 MHz pentium
encrypt		30ms	50ms	80ms		sparc II
decrypt		160ms	480ms	930ms
encrypt		15ms	33ms	56ms		100 MHz Pentium (Under NT)
decrypt		38ms	104ms	214ms
sign		99ms	166ms	216ms		sparc II (Brickell speedup)
verify		156ms	316ms	416ms
sign		21ms	38ms	49ms		100 MHz Pentium (Under NT)
verify		27ms	43ms	71ms

@_date: 1995-02-07 14:16:43
@_author: Matt Blaze 
@_subject: CFP, Cypherpunks, and Crypto 
Tim May says:
While we're at it, let me put in a plug for my tutorial, "Everything
you need to know to argue about cryptography", to be held Tuesday
afternoon (March 28) as part of the pre-conference tutorial track.
Spread the word...

@_date: 1995-02-07 17:48:48
@_author: Matt Blaze 
@_subject: noiz-0.5: simple noise-emitting package 
This looks really cute, especially given the ability to "precompute"
entropy before the process that needs it is running.
An interface I'd rather see is one that allows a process to grab
random bits that it can be sure are not correlated to bits that
have been given to other processes.  Since everything runs asynchronously
with cron, you have no way of knowing how much time has elapsed since
the last time the file was read or updated, and hence don't know
how "fresh" the bits are.  Also, /etc/noiz is an attractive target
on multi-user machines....
So I'd rather see a /dev/noise, although a portable implementation
of somthing like that is out of the question now that there are
10 gazillion unix vendors.  Perhaps a more reasonable implementation
would be a tcp or rpc service that processes can query to get random bits,
where the server delays responding until it can guarantee that its
state is sufficiently decorrelated from previous responses.  Because you can
"bank up" entropy during idle periods, most requests could probably
be served without delay, making this technique a real advantage over
just implementing the same functions in a library called directly.
(Since good randomness is still rather expensive even when you can store it
up, and useless when sent over a network, you'd probably want the noise
server to refuse requests from outside the local machine.)
Anyway, I'm looking forward to playing with it.  It's a very nice idea.

@_date: 1995-02-08 15:35:55
@_author: Matt Blaze 
@_subject: a new way to do anonymity 
You could just send a stream of some uncomon ascii character, which you
filter out on the receiving end (if you wanted to this right, you could add
a simple escape mechanism for actually passing that character).
To avoid flooding the network and also bringing the machines on which its
running to its knees, you'd probably want to add a bandwidth-choke
mechanism to run the white noise at some reasonable rate.  You'd have to limit
the real traffic output to the same rate.  Link encryption over a broadcast
network is a tricky business.
For the next couple of months, I have absolutely no free hacking time.
Things on the stack include:
So I don't even have the time to figure out whether I have the interest.

@_date: 1995-01-01 09:21:32
@_author: Matt Blaze 
@_subject: Exporting cryptographic materials, theory vs. practice 
s
The license itself has a space for the customs people to "endorse"
each export/re-import.  Interestingly, I can't figure out how to
distinguish between the signature I got when I left and the one I
got when I came back.  There are just two signatures and stamps
on the back of the license, one dated when I left and one dated when
I returned.

@_date: 1995-01-06 13:56:39
@_author: Matt Blaze 
@_subject: My life as an international arms courier 
Under an obscure provision of US law, devices and computer programs
that use encryption techniques to hide information from prying eyes
and ears are considered ``munitions'' and subject to the same rules
that govern the international arms trade.  In particular, taking such
items out of this country requires the approval of the State
Department, which decides whether exporting something might endanger
national security.  In the past, these restrictions were of little
concern to the average citizen; encryption found most of its
application in military and diplomatic communications equipment.
Today, however, growing concern over electronic fraud and privacy
means that encryption techniques are starting to find their way into
more conventional commercial products like laptop computers and
portable phones.
Mostly to find out what the process was like, I recently applied for a
temporary export license for a portable telephone encryption product
that I wanted to take with me on a business trip to England and
The item in question is more properly called a ``telephone security
device.''  This is a little box that scrambles telephone conversations
to protect them against eavesdroppers; this sort of protection is
sometimes important when discussing confidential business matters from
faraway places.  The particular model I bought was already approved
for export; it employs a cipher algorithm that the government has
already decided is not a threat to national security even should it
fall into the hands of some rogue government.  This model is aimed
primarily, I presume, at international business travelers who want to
communicate in a reasonably secure manner with their home offices in
the states.  In other words, a typical user buys two of them, leaving
one at the home office and carrying the other when traveling abroad.
The options that came with my device included a James Bond-ish looking
acoustic coupler and handset to facilitate its connection to the
hardwired phones that are still common in European hotel rooms.
It turns out that there was recently some discussion in the government
about exempting products like my secure phone from the licensing
paperwork requirements.  Unfortunately, however, this exemption never
actually took effect.  So even though the device I had was already
approved for sale abroad, I still needed to get a temporary export
license before I could take it with me.  But I was assured that ``this
is an easy, routine process''.  Well, sure enough, about two weeks
before I was to leave I got back my official US State Department
``license for the temporary export of unclassified defense articles''.
So far, so good.
a few conversations with an export lawyer), I'm required to leave from
an international airport with a Customs agent present (no problem
there, although Customs is geared to arriving, rather than departing,
travelers).  At the airport, I'm supposed to fill out a form called a
``shipper's export declaration'' (SED) on which I have to declare that
``these commodities are authorized by the US government for export
only to Belgium and the United Kingdom.  They may not be resold,
transshipped, or otherwise disposed of in any country, either in their
original form or incorporated into other end-items without the prior
written approval of the US Department of State''.  Then I'm to present
the SED and export license to a Customs official at the airport before
I leave.  The Customs officer is supposed to take my SED and endorse
my license to show what I'm actually taking out of the country.
On the way back in, I'm supposed to ``declare'' my item at Customs
(even though it was manufactured in the US) and show them my license,
and they're supposed to endorse the license again as proof that I
have, in fact, returned the ``defense article'' to the safety of the
United States.
The first hitch I ran into was that no one could actually tell me
where I could get an SED form.  But when I called Customs they assured
me that this was no big deal.  ``Just come by when you get to the
airport and we stamp the license.  I guess you can just fill out the
SED there,'' they said.
I made sure to get to the airport early anyway.
Although there was moderately heavy traffic near the airport, I made
it to JFK two and a half hours before my 10pm flight.  I was flying
United, which has their own terminal at JFK, so Customs has an office
right there in the same building from which I was to depart (JFK is
awful to get around, so I was glad for this).  I checked in for my
flight (and got upgraded to first class, which bolstered my
expectation that everything was going to be really easy from here on).
Then, luggage, license and phone in hand, I made my way downstairs to
Customs, expecting to fill out the SED form and ``just have my license
stamped'' as they had assured me earlier on the telephone.  I
explained my situation to the security guard who controls entry to the
Customs area, and he led me to ``the back office'' without much
argument or delay.  The head uniformed Customs guy in the back office
(which I think is same office where they take the people suspected of
being ``drug mules'' with cocaine-filled condoms in their stomaches)
looked approachable enough.  He had a sort of kindly, grandfatherly
manner, and he was playing a video game on a laptop computer.  I got
the impression that most of the people he encounters are suspected
drug smugglers, and he seemed pleased enough to be dealing with
something a little different from the norm.  When I explained what I
was doing he looked at me as if I had just announced that I was a
citizen of Mars who hadn't even bothered to obtain a visa.
He explained, carefully, that a) I really do need the SED form; b) not
only that, I should have already filled it out, in duplicate; c) he
doesn't have blank SED forms; d) he, like everyone else in the entire
US government that I had spoken to, has no idea where one gets them
from, but people must get them from somewhere; and e) it doesn't
really matter, because I'm in the wrong place anyway.
I asked him where the right place is.  ``The cargo building, of
course,'' he told me, patiently.  I remembered the cargo building
because I passed it in the taxi just as the traffic jam began, about
half an hour before I got to the United terminal.  The airport shuttle
bus doesn't stop there.  I'd have to call a taxi.  ``But I think
they're closed now, and even if they were open you'd never make it
before your flight'' he helpfully added, saving me the trip.  He also
complemented me for going to the trouble to get the license.
I must have looked hurt and confused.  Eventually he called in some
fellow in a suit who I presume to have been his boss.
``Are you the guy who wants to export the fancy gun?'' the fellow in
the suit asked me.
``It's not a gun, it's a telephone,'' I responded, with a straight
``Why do you have a license to export a telephone?''  Good question, I
thought.  I explained about the export law and showed him the thing.
He agreed that it looked pretty harmless.
The fellow in the suit reiterated points a through e almost verbatim
(do they rehearse for these things?) and explained that this isn't
really their department, since my license was issued by the State
Department, not Customs, and my situation doesn't come up very often
because exports usually go via the cargo building.  He'd love to help
me, but the computer in which these things get entered is over in
Cargo.  ``That's how the records get made.  But you do have a valid
license, which is nice.''  He also suggested that I would have had an
easier time had I shipped the device instead of carrying it with me.
I asked what I should do, given that my plane was scheduled to leave
in less than an hour.  Neither was sure, but the fellow in the suit
seemed willing leave it to the discretion of the uniformed guy.  ``How
does this thing work, anyway?'' he asked.  I explained as best as I
could, trying to make it sound as harmless as it is.  ``You mean like
that Clipper chip?'' he asked.
At this point, given that he has a computer and knows something about
the Clipper chip, I figured that maybe there was some hope of making
my flight.  Or maybe I was about to spend the night in jail.  In my
mind, I put it at about a 90:10 hope:jail ratio.
Then he asked, ``Do you know about this stuff?''
So we chatted about computers and cryptography for a while.  Finally,
the two of them decided that it wouldn't really hurt for them to just
sign the form as long as I promised to call my lawyer and get the SED
situation straightened out ASAP.  They assured me that I won't be
arrested or have any other trouble upon my return.
I made my flight, validated license in hand.
An aside: Throughout my trip, I discovered an interesting thing about
the phone and the various options I was carrying with it.  Under X-ray
examination, it looks just like some kind of bomb.  (I suspect it was
the coiled handset cords).  Every time I went through a security
checkpoint, I had to dig the thing out of my luggage and show it to
the guard.  I almost missed the new ``Eurostar'' chunnel train (3hrs
15mins nonstop from London to Brussels, airport-style checkin and
security) as the guards were trying to figure out whether my telephone
was likely to explode.
Coming back to the US was less eventful, though it did take me an
extra hour or so to get through Customs.  Expecting a bit of a hassle
I didn't check any luggage and made sure to be the first person from
my flight to reach the Customs line.  The inspector was ready to
wordlessly accept my declaration form and send me on my way when I
opened my mouth and explained that I needed to get an export license
stamped.  That was obviously a new one for him.  He finally decided
that this had to be handled by something called the ``Ships Office''.
I was sent to an unoccupied back room (a different back room from
before) and told to wait.  I thought about the recent Customs
experiences of Phil Zimmermann.  (Zimmermann, the author of a popular
computer encryption program, was recently detained, questioned and
searched by Customs officials investigating whether he violated the
same regulations I was trying so hard to follow.)  After about half an
hour, an officer came in and asked me what I needed.  I explained
about my export license that had to be endorsed.  She just shrugged
and told me that she had to ``process the flight'' first.  As best as
I could tell, her job was to clear the airplane itself through
Customs, that being, technically speaking, a very expensive import.
It would take a little while.  She was pleasant enough, though, and at
least didn't look at me as if she intended to send me to jail or have
me strip searched.
Finally, she finished with the plane and asked me for my form.  She
studied it carefully, obviously never having seen one before, and
eventually asked me what, exactly, she was supposed to do.  I
explained that I had never actually gone through this process before
but I understood that she's supposed to record the fact that I was
re-importing the device and stamp my license somewhere.  She told me
that she didn't know of any place for her to record this.  After some
discussion, we agreed that the best thing to do was to make a Xerox
copy of my license and arrange for it to go wherever it had to go
later.  She stamped the back of the license and sent me on my way.  It
was a little over an hour after I first reached the Customs desk.
My conclusion from all this is that it just isn't possible for an
individual traveler to follow all the rules.  Even having gone through
the process now, I still have no idea how to obtain, let alone file,
the proper forms, even for a device that's already been determined to
be exportable.  The export of export-controlled items is ordinarily
handled by cargo shipment, not by hand carrying by travelers, and the
system is simply not geared to deal with exceptions.  Technically
speaking, everyone with a laptop disk encryption program who travels
abroad is in violation of the law, but since no one actually knows or
checks, no mechanism exists to deal with those who want to follow the
rules.  While (fortunately) everyone I dealt with was sympathetic, no
one in the government who I spoke with was able to actually help me
follow the rules.  I was permitted to leave and come back only because
everyone involved eventually recognized that my telephone was pretty
harmless, that my intentions were good, and that the best thing to do
was be flexible.  If anyone had taken a hard line and tried to enforce
the letter of the law, I simply wouldn't have been able to take the
thing with me, even with my license.  Had I just put my telephone in
my suitcase without telling anyone instead of calling attention to
myself by trying to follow the rules, chances are no one would have
noticed or cared.
Unfortunately, however, these absurd rules carry the full force of
law, and one ignores them only at the risk of being prosecuted for
international arms trafficking.  While it may seem far-fetched to
imagine US citizens prosecuted as arms smugglers simply for carrying
ordinary business products in their luggage, the law as written allows
the government to do just that.  At the same time, anyone who is aware
of and who tries to follow the regulations is made to jump through
pointless hoops that are so obscure that even the people charged with
enforcing them don't know quite what to make of them.
Copyright 1995 by Matt Blaze. All rights reserved.
Electronic redistribution permitted provided this article is reproduced
in its entirity.

@_date: 1995-01-09 13:10:05
@_author: Matt Blaze 
@_subject: AT&T produces video encryptor -- is it clipper based? 
I wasn't involved in this product, but i know the people who are.  No, It's
not clipper based.  It's beimngh announced (right now) at the rsa
cinference (which I'm at tat the moment).
More later

@_date: 1995-01-18 13:27:32
@_author: Matt Blaze 
@_subject: Threats in real life - what are we worried about? 
I disagree.  "TEMPEST" risks and countermeasures are but one entry on
a long list of subjects in which our ignorance (and that of the
civilian security community in general) may well come back to bite us.
Granted, this is the "cypherpunks" list, not the "securitypunks" list,
but it behooves anybody interested in developing strong mechanisms to
accomplish some security objective to be at least acquainted with how
those mechanisms fit in the larger picture.
One of the most dangerous aspects of cryptology (more dangerous,
perhaps, than fact that there are almost no solid theorems that tell us
how secure practical ciphers really are) is that you can measure it.
It's all too tempting to misuse an estimate for the cryptographic work
factor for some cipher as if it were some kind of overall security
metric for the systems in which it is deployed.  In real life, there
are lots of ways to violate system security, including cryptanalysis,
protocol attacks, Trojan horses, viruses, electromagnetic monitoring,
physical compromise, rubber hose cryptanalysis, OS bug exploitation,
application bug exploitation, hardware bug exploitation, user error
exploitation, physical monitoring, social engineering, court orders,
dumpster diving, and so on and so on.  Most of us on the list like to
think about cryptography and cryptographic protocols, and that's fine,
but it isn't the same as thinking about building secure systems that
are strong enough to withstand attackers who aren't willing to
restrict themselves to a strictly cryptographic threat model.
Unfortunately, the world outside the cypherpunks list isn't much
better off than we are in understanding these "informal", but all too
real, threats.  Thinking about some of them would, I think, go a long
way toward contributing to "Cypherpunk goals" as I understand them.
(Practical TEMPEST shielding is one such problem.  Another good one is
the almost completely ignored problem of storing secret keys on
networked computers.  Still another is the problem of using security
software remotely with limited local computation.  There are lots
That said, no one can force these discussions to happen, and no one,
much less me, has a right to complain that everyone else is talking
about the "wrong stuff".  So let me raise a question:
I'll post my own thoughts later.

@_date: 1995-01-20 00:34:57
@_author: Matt Blaze 
@_subject: Threats in real life - what are we worried about? 
Matt's Top Ten Underappreciated Threats to Privacy on the Internet
1.  The sorry state of software.  Everyone knows that nobody knows how
to write software.  Modern systems give hundreds of thousands of lines
of code the chance to violate security policy.  How can we be sure
that the software we trust does the right thing?  How can we reduce
the opportunities for problems?
2.  Ineffective protection against denial of service attacks.  While
not a direct threat to privacy, the ease with which almost anyone can
mount effective denial of service attacks threatens the ability to
deploy anonymous services.  You'll note that no one worries very much
about the millions of anonymous entry points to more robust networks
like the telephone system or the postal service, where it's relatively
hard (and expensive) for an individual to cause large-scale service
disruption.  How can we make the 'net robust enough to withstand
mailbombs and newsgroup spamming?
3.  Poor secret storage on networked computers.  Cryptosystems allow
you to manage large secrets by protecting smaller ones (keys).
Unfortunately, modern computers are awful at protecting even the
smallest secrets.  Multi-user networked workstations can be broken
into and their memories compromised.  Standalone, single-user
machines can be stolen or compromised through viruses that leak
secrets asynchronously.  What are the right mechanisms for storing
and managing keys on various platforms?  Remote servers, where there
may be no user available to enter a passphrase (but see threat below), are an especially hard problem.
4.  Poorly understood random number generation techniques.  Keys and
session variables need good sources of unpredictable bits.  Currently
used techniques (like event inter-arrival times) are only marginally
well understood and depend to a great deal on low-level
characteristics of the platforms on which they are run.  We need a
wider range of techniques (especially ones that work without relying
on user input), and to better understand their risks and failure
modes.  Some interesting ideas have been proposed that deserve further
study.  At CRYPTO '94 there was an interesting paper on using disk
airflow variation to get random bits.  Another interesting technique,
first proposed by Don Mitchell, involves exploiting clock skew.  Here's
a C program that seems to produce one pretty random bit per second on
most platforms.  How good are the bits?  Can we get more bandwidth out
of it?
  int count=0;
void printbit()
5.  Weak passphrases.  Most crypto software addresses the key storage
and key generation problems by relying on user-generated passphrase
strings, which are presumed to contain enough entropy to produce good
key material and are also easy enough to remember that they do not
require secure storage.  While dictionary attacks are a well known
problem with short passwords, much less is known about lines of attack
against user- selected passphrase-based keys.  Shannon tells us that
English text has just over 1 bit of entropy per character, which would
seem to leave most passphrases well within reach of brute-force
search.  Less is known, however, about good techniques for enumerating
passphrases in order to exploit low entropy.  Until we have a better
understanding of how to attack passphrases, we really have no idea how
weak or strong they are.
6.  Limited support for remote trusted agents.  Almost all currently
available cryptographic software assumes that the user is in direct
control over the systems on which they run and has a secure path to
it.  For example, the interfaces to programs like PGP and CFS assume
that their input is comes from the user over a secure path like the
local console.  This is not always the case, of course; consider the
problem of reading your mail remotely when logged in over the
Internet.  We need better mechanisms for transferring the trusted
operations to the local trusted machine while keeping the logical
operations (like where the mail is) where they logically belong.
7.  Poorly understood protocol and service interactions.  Features
frequently come back to bite us, and its hard to know even where to
look.  The Internet worm was propagated via an obscure and
innocent-looking feature in sendmail; how many more features in how
many more programs have unexpected consequences just waiting to be
discovered?  Is the conventional wisdom of hiding behind firewalls
and turning off services really the only answer?
8.  Lack of scalable security infrastructure.  No comment...
9.  Poorly understood "out of band" attack risks.  Security people
tend to focus on what's easy to model.  Unfortunately, attackers focus
on what's easy to exploit.  We need a better understanding of just how
easy some non-traditional attacks are.  Most of the answers are
probably too scary to think about.  How long do our keys need to be in
the face of electromagnetic radiation, physical monitoring, Trojan
horses, social engineering, and so on and so on?
10. No broad-based demand for security.  This is a well-known problem
among almost everyone who has tied his or her fortune to selling
security products and services.  Until there is widespread demand for
transparent security, the tools and infrastructure needed to support
it will be expensive and inaccessible to many applications.  This is
partly a problem of understanding the threats and risks in real
applications, and of building systems that include security as a basic
feature rather than as a later "add on".
There's a lot missing from this list, and a lot you can disagree with
among the things that are on it.
Flame away...

@_date: 1995-01-30 08:01:55
@_author: Matt Blaze 
@_subject: ESP Unix encrypted session protocol software 
I'm releasing, for experimental use, source code for a preliminary
version of my simple Unix->Unix encrypted terminal session manager,
ESP.  Basically, ESP provides a pseudo-terminal interface to a local
shell session that you can use to establish an encrypted terminal
session with a another, remote machine.  (See the README file below
for usage examples).  Once the bugs are shaken out, ESP will become
part of my CFS package, which will eventually grow into a larger
suite of free, practical tools for secure internetworked computing.
At the present time, I'm just releasing a "pre-release" to small
groups, including interested folks on the cypherpunks list.
I've tested ESP under BSDI and SunOS 4.x; you'll also need RSAREF 2.0
(from rsa.com) to compile it.  You're on your own for other platforms.
This release is NOT production software; it is by no means "ready for
prime time".  It's slow (reduces bandwidth by 50% and takes 45 seconds
to start up).  The user interface needs a bit of work, and you really
have to know what's going on to make effective use of it.  Future
versions of ESP will use a more efficient encoding and will add
features like a "palmtop" mode that allows secure remote entry of
things like passwords from dumb terminals with the encryption done on
a disconnected palmtop machine.  I hope to eventually have a PC
terminal client as well.  This version, however, only supports
Unix->Unix terminal sessions.
Because of export restrictions (and a large cabal of paranoid, rabid
lawyers watching my every move), I'm not able to send ESP out of the
US or Canada or make it available by anonymous ftp.  Sorry.  If you
want a copy of the ESP-beta sources, send an Email message to
cfs at research.att.com (NOT mab at research.att.com) telling me the
Remember, you also need to get RSAREF 2.0 to build ESP.
========================= ESP README ==============================
This is Version 0.5c (BETA) of ESP, the Encrypted Session Protocol.
 * The author of this software is Matt Blaze.
 *              Copyright (c) 1995 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.
 *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
ESP is an encrypted session protocol layer for managing remote
encrypted sessions.  It does 1024 bit DH key exchange (from RSAREF)
and 3-des in 8bit cfb mode for the traffic encryption.  See the
man page (esp.1 in this distribution).
To compile ESP you'll need the RSAREF 2.0 library, available for free
for non-commercial use in the US and Canada from RSA Laboratories
(anonymous ftp to rsa.com for details).
Once you have RSAREF working, this distribution should compile without
problems under SunOS 4.x and BSDI; you're on your own with other platforms.
The best way to explain esp is with an example.  Here's
an encrypted session from alice to bob:
You can also use ESP to provide an encrypted login session;
simply create a user "esp" with "esp -s -e login" as the login
shell.  (Getting this to work properly will require some tweaking
on your local system). Run esp -l on the local machine and from
there log in to the esp account on the remote machine.  Such a
configuration encrypts the real account name and password over
the network:
It's primitive and slow, but seems to work.  Comments, bug fixes,
ports to new platforms and complaints are welcome.
Matt Blaze
mab at research.att.com
(for esp or cfs questions, use cfs at research.att.com).

@_date: 1995-01-30 10:07:52
@_author: Matt Blaze 
@_subject: ESP Unix encrypted session protocol software 
Well, cryptographically speaking, it's trivial for an active attack and
probably infeasible for a passive attack.  But you knew that...
So there are two questions - first, what's the threat model for TCP/IP,
and second, what are the alternatives?
I'm not sure about the threat model.  Spoofing attacks on TCP sessions
are not exactly easy - there's a lot to do to pull it off - but not
out of the question either (as demonstrated by the recent NYT articles
and CERT advisories).  Probably the easiest way to receive packets intended
for another host is to convince the routing tables between you and your
victim to route to you instead of the real host.  I'm not aware of this
every being done ON PURPOSE, but it's not out of the question, either.
As for the alternatives, I think the picture is pretty bleak, to tell
the truth.  The cryptographically sound way to prevent spoofing is
with authentication of the agreed key.  But for the remote host to
authenticate itself, it has to have a secret signature key.  Where to
store it?  A typical machine, especially a multi-user, unattended server
simply has no safe place to store keys.  And if you had a trusted secure key
store on the remote host, you wouldn't really need to use Diffie-Hellman
to establish the session key in the first place, since you could just
store each user's pre-established session key in advance.
At the extreme, fixing this is a Hard Problem.  In practice for establishing
a reasonably secure session, it all depends on how much you worry about a
full-blown (two way) spoofing attack against IP.
session ke

@_date: 1995-01-30 10:12:02
@_author: Matt Blaze 
@_subject: ESP -> ESM 
I've just been informed that there's a reasonably confusing name
collision between my ESP and a working name for IPSP.
I'm easy.  ESP is now called ESM (encrypting session manager).  Everything
else is the same.

@_date: 1995-01-30 21:48:38
@_author: Matt Blaze 
@_subject: ESP Unix encrypted session protocol software 
Sure, but as you point out in your second sentence, systems that are
secure enough for secret storage aren't exactly "typical" of what's out
there on the Internet.  And even an Orange book A rated system has to
be kept locked up, under guard and administered properly if you want to
be sure that the secret data stored on it remain secret.
The vast majority of unattended "server" machines in my online life are
neither located in well-controlled environments (especially considering
backup tapes) nor administered particularly well.  I'm not sure that
persistent signature keys stored on such hosts provide much extra
assurance of machine identity beyond what already comes from their
answering to the expected IP address (which is hardly saying much, of
course).  I think better than expecting the world to switch over to
cumbersome, multilevel secure OSs is to equip such servers with
inexpensive tamper-resistant cryptographic modules that never reveal
their secrets.  At least then you're guaranteed that there can be only
one instance of a machine's identity out there at a time, and have some
hope of detecting the theft of the key material.  (There may be some
hope on this front.  PCMCIA crypto modules like the NS iPower card are
beginning to hit the market already, and products like that may well be
commonplace by the time host authentication protocols start to be
deployed for real on the Internet.)

@_date: 1995-01-31 15:37:57
@_author: Matt Blaze 
@_subject: ESP Unix encrypted session protocol software 
I've got one of those, too (it won't be ready for release too soon, though - telnet is big and ugly).  An encrypting telnet and telnetd
almost always provide a more appropriate way to do session encryption.
However, there are some situations where ESM is really the only
option.  One is when you can't or don't want to install a daemon
(e.g., for very occasional use).  More importantly, by running within
the session, ESM can provide end-to-end encryption across an untrusted
application-layer firewall (like the one I go through to get
between home and work).
Since part of my motivation for working on these tools comes from wanting to use them myself, I'm building the stuff I need the most

@_date: 1995-07-12 16:36:54
@_author: Matt Blaze 
@_subject: the sound of another shoe dropping... 
Forwarded message:
Posted-Date: Wed, 12 Jul 1995 15:28:18 -0400
X-Sender: farber at linc.cis.upenn.edu
Message-Id: Mime-Version: 1.0
X-Priority: 1 (Highest)
X-Proccessed-By: mail2list
Heavy sigh.
On June 27, Sen. Grassley introduced extensive criminal amendments to the
federal racketeering act.  S. 974, the "Anti-Electronic Racketeering Act of
1995," would amend U.S. Code sections 18 USC 1961 (criminal RICO statute),
18 USC 1030A (new section on computer crime), 18 USC 2515, 2516
(wiretapping), and 42 USC 2000aa (Privacy Protection Act).
This proposed legislation is Very Bad. It would make all encryption
software posted to computer networks that are accessible to foreigners
illegal *regardless of whether the NSA has classified the software as a
munition!!!*  Here's the language:
 "Sec. 1030A.  Racketeering-related crimes involving computers
   "(a) It shall be unlawful--
. . .
      "(2) to distribute computer software that encodes or encrypts
    electronic or digital communications to computer networks that the
    person distributing knows, or reasonably should know, is accessible to
    foreign nationals and foreign governments, regardless of whether such
    software has been designated nonexportable."
I'm up to my ears in analyses that need to be written, but I'll send around
something more complete when I'm able to pull it together.
------- End of Forwarded Message

@_date: 1995-07-28 10:17:52
@_author: Matt Blaze 
@_subject: New release (v1.3) of CFS Unix encrypting file system now available 
Source code for the latest version (release 1.3) of CFS, the Cryptographic
File System, is now available upon request for research and experimental
use in the US and Canada.
CFS pushes encryption services into the Unix(tm) file system.  It
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.  You must have
"root" access to install CFS.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze. "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.)
and in
    Matt Blaze. "Key Management in an Encrypting File System", Proc.
    Summer '94 USENIX Tech. Conference, Boston, MA, June 1994.
    (PostScript available by anonymous ftp from research.att.com
    in the file dist/mab/cfskey.ps.)
Version 1.3 of CFS also includes ESM, the Encrypting Session Manager.
ESM provides shell-to-shell encrypted sessions across insecure links
and requires no OS or network support.  It is useful for typing cfs
passphrases when logged in over the network.  ESM needs RSAREF 2.0 to
compile and is tested only on SunOS and BSDI.  ESM is the first released
part of a suite of session encryption tools that are described in
    Matt Blaze and Steve Bellovin. "Session-layer Encryption."
    Proc. 1995 USENIX Security Workshop, Salt Lake City, June 1995.
    (PostScript is available from
    ftp://research.att.com/dist/mab/sesscrypt.ps)
The new version of CFS differs from the version described in the
papers in a few ways:
* The DES-based encryption scheme has been strengthened, and now
provides greater security but with the online latency of only single-DES.
* Support for the smartcard-based key management system is not
included and a few of the tools are not included.
* An impoved key management scheme now allows chaning the passphrase
associated with a directory.
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. * User-contributed ports to a number of additional platforms.
* Hooks for adding new ciphers.
* 3-DES and MacGuffin encryption options.
* Timeout options allow automatic detach of encrypted directories
after a set time or period of inactivity.
CFS is distributed as a research prototype; it is COMPLETELY
UNSUPPORTED software.  No warranty of any kind is provided.  We will
not be responsible if the system deletes all your files and emails the
cleartext directly to the NSA or your mother.  Also, we do not have
the resources to port the software to other platforms, although you
are welcome to do this yourself.  The software was developed under
SunOS and BSDI, and there are also unsupported user-contributed ports
available for AIX, HP/UX, Irix, Linux, Solaris and Ultrix.  We really
can't promise to provide any technical support at all, beyond the
source code itself.  We also maintain a mailing list for CFS users and
developers; subscription information is included with the source code.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US and Canada to US and
Canadian citizens and permanent residents.  Unfortunately, we cannot
make it available for general anonymous ftp or other uncontrolled
access, nor can we allow others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.  You may
 * not export it, in whole or in part, or cause or allow such export,
 * through act or omission, without prior authorization from the United
 * States government and written permission from AT&T.  In particular,
 * you may not make any part of this software available for general or
 * unrestricted distribution to others, nor may you disclose this software
 * to persons other than citizens and permanent residents of the United
 * States and Canada.  *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please read to the end
of this message and then send email to:
DO NOT REPLY DIRECTLY TO THIS MESSAGE.  You must include a statement
that you are in the US or Canada, are a citizen or legal permanent
resident of the US or Canada, and have read and understand the license
conditions stated above.  Be sure to include an email address in a US-
or Canada-registered domain. The code will be sent to you via email in
a "shar" shell archive (a little over 300K bytes long).

@_date: 1995-10-12 13:17:13
@_author: Matt Blaze 
@_subject: smartcard encryption: revised paper available 
I've put a (slightly expanded) revised version of my paper, "High-
Bandwidth Encryption with Low-Bandwidth Smartcards" in my ftp directory.
This paper describes a little protocol for exploiting the secure key
storage facilities of slow smartcards but with the host processor
doing most of the actual work.

@_date: 1995-10-24 14:15:57
@_author: Matt Blaze 
@_subject: CRYPTO '96 CFP 
CRYPTO '96
         August 18-22, 1996, Santa Barbara, California, USA
                          CALL FOR PAPERS
General Information:
Crypto '96, the Sixteenth Annual Crypto Conference, is organized by the
International Association for Cryptologic Research (IACR), in cooperation with the IEEE Computer Society Technical Committee on Security and Privacy, and the Computer Science Department of the University of California, Santa Barbara.  Original papers are solicited on all technical aspects of cryptology.
Instructions for Authors:
Please send a cover letter, one title page and 16 copies of an extended
abstract.  They must be received by the Program Chair at the address given below no later than February 14, 1996 (or postmarked by February 4, 1996 and sent via airmail).
The cover letter must state: "This paper does not substantially duplicate work that any of the authors have published elsewhere or have submitted in parallel to any other conference or workshop that has The title page should contain the title, the names of the authors, their
postal and e-mail addresses and the short abstract; it should be made clear who is the author to whom correspondence should be sent.  The first page of the extended abstract should be an informal one-page
statement (that will not be published in the Proceedings) describing the content of the oral presentation that will be given at Crypto '96 in the event the paper is accepted (this statement is expected to be different from the short abstract of the paper).  This page and the extended abstract must be ANONYMOUS, i.e., they must contain no indication whatsoever of the identity of the author(s).  The main body of the extended abstract should start with the title, short abstract, and list of keywords.  This should be followed by a succinct statement appropriate for a non-specialist reader specifying the subject addressed, its background, the main achievements, and their significance to cryptology.  Technical details directed to the specialist should then Submissions are limited to 10 single-spaced pages of 11pt type, not counting the bibliography and clearly marked appendices.  Since referees are not required to read the appendices, the paper should be intelligible without them.  The paper including bibliography and appendices must run to no more than 15 pages.  These limits will be strictly enforced.
Unfortunately, because of the burdens on the Program Committee and
the need to keep strictly to the time schedule, we will have to
summarily reject any submission not in keeping with the above
Authors are encouraged to make 2-sided copies if possible.  Please send submissions by post; unfortunately, we cannot accept submissions by    e-mail or fax.
Notification of acceptance or rejection will be sent to authors on April 22, 1996.
Conference Proceedings:
Proceedings will be available at the meeting.  They will be published in
the Springer-Verlag Lecture Notes in Computer Science.  Clear
instructions about the final copy will be sent to authors of accepted
papers.  The final copies of the accepted papers will be due on June 1,
1996.  Final papers arriving too late will be removed from the main
program.  Authors of accepted papers must guarantee that their paper will be presented at the conference.
A limited number of stipends are available to those unable to obtain
funding to attend the conference.  Students whose papers are accepted and who will present the paper themselves are encouraged to apply if such assistance is needed.  Requests for stipends should be addressed to the general chair.
Send submissions to:
Neal Koblitz, Program Chair, Crypto '96
Dept. of Mathematics, Box 354350
University of Washington
Seattle, WA 98195 U.S.A.
Internet: koblitz at math.washington.edu
Phone: 1-206-543-4386
For other information contact:
Richard Graveman, General Chair, Crypto '96
444 Hoes Lane RM 1K-221
Piscataway, NJ 08854 U.S.A.
Internet: rfg at ctt.bellcore.com
Phone: 1-908-699-4611
Program Committee:
Neal Koblitz, Chair (Mathematics, University of Washington, USA)
Mihir Bellare (Computer Science, University of California at San Diego, USA)
Josh Benaloh (Microsoft, USA)
Matt Blaze (AT&T Bell Laboratories, USA)
Johannes Buchmann (Computer Science, Universitaet des Saarlandes, Germany)
Don Coppersmith (IBM Research, USA)
Joan Feigenbaum (AT&T Bell Laboratories, USA)
Andrew Klapper (Computer Science, University of Kentucky, USA)
Lars Knudsen (Computer Science, Ecole Normale Superieure, France)
Peter Landrock (Mathematics, Aarhus University, Denmark)
Tsutomu Matsumoto (Electrical & Computer Engineering, Yokohama National
University, Japan)
Chris Mitchell (Computer Science, University of London, UK)
Paul Van Oorschot (Bell-Northern Research, Canada)
Bart Preneel (Catholic University at Leuven, Belgium)
Rainer Rueppel (R3 Security Engineering, Switzerland)
Jacques Stern (Computer Science, Ecole Normale Superieure, France)

@_date: 1995-10-28 03:19:49
@_author: Matt Blaze 
@_subject: New release of CFS Unix encrypting file system available 
Source code for the latest version (release 1.3.1) of CFS, the Cryptographic
File System, is now available upon request for research and experimental
use in the US and Canada.
CFS pushes encryption services into the Unix(tm) file system.  It
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.  You must have
"root" access to install CFS.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze. "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.)
and in
    Matt Blaze. "Key Management in an Encrypting File System", Proc.
    Summer '94 USENIX Tech. Conference, Boston, MA, June 1994.
    (PostScript available by anonymous ftp from research.att.com
    in the file dist/mab/cfskey.ps.)
Version 1.3 of CFS also includes ESM, the Encrypting Session Manager.
ESM provides shell-to-shell encrypted sessions across insecure links
and requires no OS or network support.  It is useful for typing cfs
passphrases when logged in over the network.  ESM needs RSAREF 2.0 to
compile and is tested only on SunOS and BSDI.  ESM is the first released
part of a suite of session encryption tools that are described in
    Matt Blaze and Steve Bellovin. "Session-layer Encryption."
    Proc. 1995 USENIX Security Workshop, Salt Lake City, June 1995.
    (PostScript is available from
    ftp://research.att.com/dist/mab/sesscrypt.ps)
The new version of CFS differs from the version described in the
papers in a few ways:
* The DES-based encryption scheme has been strengthened, and now
provides greater security but with the online latency of only single-DES.
* Support for the smartcard-based key management system is not
included and a few of the tools are not included.
* An impoved key management scheme now allows chaning the passphrase
associated with a directory.
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. * User-contributed ports to a number of additional platforms.
* Hooks for adding new ciphers.
* 3-DES, MacGuffin, and SAFER-SK128 encryption options.
* Timeout options allow automatic detach of encrypted directories
after a set time or period of inactivity.
CFS is distributed as a research prototype; it is COMPLETELY
UNSUPPORTED software.  No warranty of any kind is provided.  We will
not be responsible if the system deletes all your files and emails the
cleartext directly to the NSA or your mother.  Also, we do not have
the resources to port the software to other platforms, although you
are welcome to do this yourself.  The software was developed under
SunOS and BSDI, and there are also unsupported user-contributed ports
available for AIX, HP/UX, Irix, Linux, Solaris and Ultrix.  We really
can't promise to provide any technical support at all, beyond the
source code itself.  We also maintain a mailing list for CFS users and
developers; subscription information is included with the source code.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US and Canada to US and
Canadian citizens and permanent residents.  Unfortunately, we cannot
make it available for general anonymous ftp or other uncontrolled
access, nor can we allow others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994, 1995 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.  You may
 * not export it, in whole or in part, or cause or allow such export,
 * through act or omission, without prior authorization from the United
 * States government and written permission from AT&T.  In particular,
 * you may not make any part of this software available for general or
 * unrestricted distribution to others, nor may you disclose this software
 * to persons other than citizens and permanent residents of the United
 * States and Canada.  *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please read to the end
of this message and then send email to:
DO NOT REPLY DIRECTLY TO THIS MESSAGE.  You must include a statement
that you are in the US or Canada, are a citizen or legal permanent
resident of the US or Canada, and have read and understand the license
conditions stated above.  Be sure to include an email address in a US-
or Canada-registered domain. The code will be sent to you via email in
a "shar" shell archive (a little over 300K bytes long).

@_date: 1995-09-04 12:04:13
@_author: Matt Blaze 
@_subject: my crypto rump session abstract 
(the one that david sternlight misinterpreted to mean that we
had proven that clipper has no back doors):
M. Blaze, J. Feigenbaum, F.T. Leighton, "Master Key Cryptosystems",
Crypto '95 "rump session" abstract, August 1995.  Available as:

@_date: 1996-08-14 12:19:06
@_author: Matt Blaze 
@_subject: key escrow idea from David Satelin of MIT Lincoln Labs 
My comments included below Rivest's message.
------- Forwarded Message
Received: from amontillado.research.att.com (amontillado.research.att.com [135.104.21.154]) by nsa.research.att.com (8.7.3/8.7.3) with ESMTP id QAA04438 for ; Tue, 13 Aug 1996 16:17:05 -0400 (EDT)
Received: from research.research.att.com (research.att.com [135.104.117.5]) by amontillado.research.att.com (8.7.5/8.7) with SMTP id QAA24830 for ; Tue, 13 Aug 1996 16:20:09 -0400 (EDT)
Received: from theory.lcs.mit.edu by research; Tue Aug 13 16:17:17 EDT 1996
Received: from swan.lcs.mit.edu by theory.lcs.mit.edu (5.65c/TOC-1.2S) Received: by swan.lcs.mit.edu (5.65c/TOC-1.2C) Message-Id: <199608132016.AA00335 at swan.lcs.mit.edu>
        mab at research.att.com, denning at cs.georgetown.edu
Hi --
Here is another MIT professor's (Dave Staelin's) suggestion for a
national crypto policy.  I thought you might be interested in seeing
it; given the difficulty of the debate, any variant, even if only
slightly different from previous ones, should be considered.  Feel
free to pass this note around, or to post it...
Here is Staelin's idea:
            must turn over your private papers in such a situation.
            (There would have to be an appropriate penalty for losing the
            key...)
The attractive feature of this proposal is that it puts encrypted
communications in the same category as private papers; the government
is required to give notice to (at least one of) the affected
individual(s) _before_ the search can be undertaken.  This cures what is
in my mind a defect in the current wire-tapping laws.  In a variant of Staelin's proposal (my twist) you could append to each
encrypted message an encrypted form of the message key.  The
encryption could be with the public-key of a trusted third party who
will not (and legally may not) reveal the message key without
notifying you first (or ensuring that you have been appropriately
served with the corresponding warrant).  For example, the ACLU might
be such a TTP.  This protects the government's right to access and
protects the individual from the penalties (or benefits) of losing the
key.  This procedure is technically simple; what is more complex is
ensuring that the TTP's are appropriately registered and protected
from undue government influence.  The use of such a TTP would in any
case be optional; the communicants need not use a TTP if they
understand their obligation to keep the crypto keys around for some
period of time afterwards.  In Staelin's proposal government gains access to the communications,
but does not gain "real-time access" as desired by the FBI.  This loss
may be tolerable, given the benefit obtained (forcing access to be
made in accordance with the Constitutional requirements for
notification before search).  The use of wiretapping encrypted
communications as a preventive measure might be severely limited, but
its use as a means of gathering evidence to force a conviction would
be preserved.
For international communications, each communicant might be required to
use a TTP that is bound to honor the laws of his country (which TTP to be
used should be the choice of the communicant).  It may be seem a bit strange to force individuals to keep around
information (keys) that they no longer really need.  However, this is
more-or-less the case for financial records right now.
The fundamental idea is to give the government a right to access
encrypted communication in return for a guarantee that access may not
be obtained until there is BOTH proper legal authorization AND proper
prior notice to (at least one of) the communicants.
Is this workable??
------- End of Forwarded Message
[Matt's comments follow]
The requirement to store your keys for some period of time would,
I think, be very unusal, legaly.
As far as I know there are virtually no records that an ordinary
individual is required to keep today under criminal penalty of law.
One has to keep tax records if one expects to be able to document
deductions if audited, but for people without deductions, no records
need be kept (and even those who do but who destroy their records
risk having their deductions disallowed, but face criminal penalties
only if the govenment can prove you intended fraud.  Not having
records does not by itself constitute fraud, as far as I know).
According to the original message:
Yes and no.  True, it makes it impossible to recover communication
without the knowledge of one party.  But it still goes well beyond
the  norms for private papers.  The vast majority of private papers
are, according to the law, just that - private.  One is under no
obligation to maintain "private papers" in any particular manner
or for any period of time.   Only very limited types of private
papers (none for most people) have to be maintained at all.  While,
in general, the government can get a court order to force one to
turn over documents that exist, one is not obligated to keep
documents that are otherwise of no use in order to be ready should
a court order happen sometime in the future.  One can burn one's
old love letters any time one feels like it.
But enough philosophy.  There are technical reasons to consider
this proposal a bad idea.
The main technical problem with the Staelin proposal is the
requirement that the user maintain a large store of no longer useful
but highly sensitive data in a secure manner for a period of time.
This introduces an obvious storage burden (how does an encrypting
phone or network connection store old keys?) that would make many
kinds of otherwise simple encryption hardware and software far more
complex and difficult to design and expensive to implement and
operate.  Consider a secure phone (like the TSD 3600 or STU III).
A critical design feature of these devices is that they never have
to emit secret keys outside their internal security boundaries.
Consider, too, software that runs on PCs and workstations.
Ordinarily, software that establishes, say, a secure Internet
connection has no need to store any secret associated with the
session anywhere.  And that's a good thing - the file systems on
most computers aren't secure enough to store keys, so including
the key storage feature required by the Staelin scheme would entail
implementing some kind of secure storage system that isn't otherwise
needed by the application.  Even if the design complexity is solved,
there is the problem of maintaining the stored keys in a secure
manner, introducing what would in most cases be a more serious
security vulnerability than any other aspect of the application
(since the keys would continue to exist long after the secure
session has ended).   Under the Staelin proposal, the design,
implementation, and use of encryption software and hardware becomes
much more complex, so complex that I honestly don't think we know
how to do it.  I touch on these points in discussing key escrow in
general in my Senate testimony,

@_date: 1996-08-14 12:28:51
@_author: Matt Blaze 
@_subject: resend: key escrow idea from David Staelin of MIT Lincoln Labs 
My first send of this message was garbled and truncated.  Here it is again.
My comments included below Rivest's message.
------- Forwarded Message
Received: from amontillado.research.att.com (amontillado.research.att.com [135.104.21.154]) by nsa.research.att.com (8.7.3/8.7.3) with ESMTP id QAA04438 for ; Tue, 13 Aug 1996 16:17:05 -0400 (EDT)
Received: from research.research.att.com (research.att.com [135.104.117.5]) by amontillado.research.att.com (8.7.5/8.7) with SMTP id QAA24830 for ; Tue, 13 Aug 1996 16:20:09 -0400 (EDT)
Received: from theory.lcs.mit.edu by research; Tue Aug 13 16:17:17 EDT 1996
Received: from swan.lcs.mit.edu by theory.lcs.mit.edu (5.65c/TOC-1.2S) Received: by swan.lcs.mit.edu (5.65c/TOC-1.2C) Message-Id: <199608132016.AA00335 at swan.lcs.mit.edu>
        mab at research.att.com, denning at cs.georgetown.edu
Hi --
Here is another MIT professor's (Dave Staelin's) suggestion for a
national crypto policy.  I thought you might be interested in seeing
it; given the difficulty of the debate, any variant, even if only
slightly different from previous ones, should be considered.  Feel
free to pass this note around, or to post it...
Here is Staelin's idea:
            must turn over your private papers in such a situation.
            (There would have to be an appropriate penalty for losing the
            key...)
The attractive feature of this proposal is that it puts encrypted
communications in the same category as private papers; the government
is required to give notice to (at least one of) the affected
individual(s) _before_ the search can be undertaken.  This cures what is
in my mind a defect in the current wire-tapping laws.  In a variant of Staelin's proposal (my twist) you could append to each
encrypted message an encrypted form of the message key.  The
encryption could be with the public-key of a trusted third party who
will not (and legally may not) reveal the message key without
notifying you first (or ensuring that you have been appropriately
served with the corresponding warrant).  For example, the ACLU might
be such a TTP.  This protects the government's right to access and
protects the individual from the penalties (or benefits) of losing the
key.  This procedure is technically simple; what is more complex is
ensuring that the TTP's are appropriately registered and protected
from undue government influence.  The use of such a TTP would in any
case be optional; the communicants need not use a TTP if they
understand their obligation to keep the crypto keys around for some
period of time afterwards.  In Staelin's proposal government gains access to the communications,
but does not gain "real-time access" as desired by the FBI.  This loss
may be tolerable, given the benefit obtained (forcing access to be
made in accordance with the Constitutional requirements for
notification before search).  The use of wiretapping encrypted
communications as a preventive measure might be severely limited, but
its use as a means of gathering evidence to force a conviction would
be preserved.
For international communications, each communicant might be required to
use a TTP that is bound to honor the laws of his country (which TTP to be
used should be the choice of the communicant).  It may be seem a bit strange to force individuals to keep around
information (keys) that they no longer really need.  However, this is
more-or-less the case for financial records right now.
The fundamental idea is to give the government a right to access
encrypted communication in return for a guarantee that access may not
be obtained until there is BOTH proper legal authorization AND proper
prior notice to (at least one of) the communicants.
Is this workable??
------- End of Forwarded Message
[Matt's comments follow]
The requirement to store your keys for some period of time would,
I think, be very unusal, legaly.
As far as I know there are virtually no records that an ordinary
individual is required to keep today under criminal penalty of law.
One has to keep tax records if one expects to be able to document
deductions if audited, but for people without deductions, no records
need be kept (and even those who do but who destroy their records
risk having their deductions disallowed, but face criminal penalties
only if the govenment can prove you intended fraud.  Not having
records does not by itself constitute fraud, as far as I know).
According to the original message:
Yes and no.  True, it makes it impossible to recover communication
without the knowledge of one party.  But it still goes well beyond
the  norms for private papers.  The vast majority of private papers
are, according to the law, just that - private.  One is under no
obligation to maintain "private papers" in any particular manner
or for any period of time.   Only very limited types of private
papers (none for most people) have to be maintained at all.  While,
in general, the government can get a court order to force one to
turn over documents that exist, one is not obligated to keep
documents that are otherwise of no use in order to be ready should
a court order happen sometime in the future.  One can burn one's
old love letters any time one feels like it.
But enough philosophy.  There are technical reasons to consider
this proposal a bad idea.
The main technical problem with the Staelin proposal is the
requirement that the user maintain a large store of no longer useful
but highly sensitive data in a secure manner for a period of time.
This introduces an obvious storage burden (how does an encrypting
phone or network connection store old keys?) that would make many
kinds of otherwise simple encryption hardware and software far more
complex and difficult to design and expensive to implement and
operate.  Consider a secure phone (like the TSD 3600 or STU III).
A critical design feature of these devices is that they never have
to emit secret keys outside their internal security boundaries.
Consider, too, software that runs on PCs and workstations.
Ordinarily, software that establishes, say, a secure Internet
connection has no need to store any secret associated with the
session anywhere.  And that's a good thing - the file systems on
most computers aren't secure enough to store keys, so including
the key storage feature required by the Staelin scheme would entail
implementing some kind of secure storage system that isn't otherwise
needed by the application.  Even if the design complexity is solved,
there is the problem of maintaining the stored keys in a secure
manner, introducing what would in most cases be a more serious
security vulnerability than any other aspect of the application
(since the keys would continue to exist long after the secure
session has ended).   Under the Staelin proposal, the design,
implementation, and use of encryption software and hardware becomes
much more complex, so complex that I honestly don't think we know
how to do it.  I touch on these points in discussing key escrow in
general in my Senate testimony,
ftp://research.att.com/dist/mab/testimony.txt .
While Ron's twist decreases some of the burden on the user it
eliminates the main benefit of the Staelin proposal - that one
cannot obtain cleartext without the knowledge of at least one party.
The TTP could be compelled (as the phone company is now for regular
wiretaps) to keep the request secret, under court order.  And the
design complexity problem doesn't even go away - in fact, it gets
worse, since now there's a protocol with a third party involved.

@_date: 1996-08-14 16:20:00
@_author: Matt Blaze 
@_subject: key escrow idea from David Satelin of MIT Lincoln Labs 
[Please include me on any mail you want me to see, as I don't
read the cypherpunks list these days]
You may be confused.  I hope it was clear that I didn't write that.
That text was part of the message from Ron Rivest that I included in my
Well, I don't know what went on before 1968, but these days phone companies
don't keep wiretap orders secret because they are being nice to the police,
they keep them secret because the court order for the weretap also orders
them to.  Perhaps you aren't aware of this, but when a third party  is
ordered to turn over records or access to something, the order often
includes a provision that prohibits them from revealing the order to the
subject.  This is not unique to phone records; orders for bank records
frequently have secrecy provisions as well.

@_date: 1996-12-19 13:07:53
@_author: Matt Blaze 
@_subject: "Cryptography Policy and the Information Economy" draft available 
I've got a new draft available of my critique of US cryptography
policy and its impact on the future of the "information economy".
It summarizes comments I made to a recent meeting of the Computer
and Communications Industry Association, and is an updated version
of testimony I gave to the Senate commerce committee earlier this
Postscript is at ftp://ftp.research.att.com/dist/mab/policy.ps
ASCII text is at ftp://ftp.research.att.com/dist/mab/policy.txt

@_date: 1996-02-06 19:19:28
@_author: Matt Blaze 
@_subject: Report available: "Minimal Key Lengths for Symmetric Ciphers" 
At the request of the Business Software Alliance (BSA), an ad hoc
panel of seven cryptologists and computer scientists met last November
to address the question of the minimum key length required to provide
adequate security against exhaustive search in commercial applications
of symmetric cryptosystems.  We have just completed our report.
We adopted a simple, and somewhat conservative, methodology in an
effort to gain a realistic understanding of what size keys might
actually be vulnerable in practice.  It is common in analysis of key
length to give all benefit of the doubt to the capabilities of the
potential attacker and to make very generous assumptions about the
technology and resources that might be available to mount an attack.
In our analysis, however, we assumed that the attacker would employ
only conventional, commercially-mature technologies and would be
limited by budget and time constraints.  We used several different
technologies to design attack strategies that accommodate the budgets
of various hypothetical attackers, from individual ``hackers'' to
well-funded enterprises.  Our conclusions, therefore, represent an
approximation of an ``upper bound'' on the strength of various size
keys; I believe more efficient attacks than those we considered might
also be possible and should be taken into account by the prudent
cryptosystem designer.
The abstract of the report follows below.
A PostScript copy of the full text of the report is available in
     ftp://ftp.research.att.com/dist/mab/keylength.ps
An ASCII version is available in
     ftp://ftp.research.att.com/dist/mab/keylength.txt
(The report will also likely appear on the BSA's web site shortly).
-matt (speaking only for himself)
    Encryption plays an essential role in protecting the privacy of
electronic information against threats from a variety of potential
attackers.  In so doing, modern cryptography employs a combination of
_conventional_ or _symmetric_ cryptographic systems for
encrypting data and _public key_ or _asymmetric_ systems for
managing the _keys_ used by the symmetric systems.  Assessing the
strength required of the symmetric cryptographic systems is therefore
an essential step in employing cryptography for computer and
communication security.
    Technology readily available today (late 1995) makes _brute-force_ attacks against cryptographic systems considered adequate
for the past several years both fast and cheap.  General purpose
computers can be used, but a much more efficient approach is to employ
commercially available _Field Programmable Gate Array (FPGA)_
technology.  For attackers prepared to make a higher initial
investment, custom-made, special-purpose chips make such calculations
much faster and significantly lower the amortized cost per solution.
    As a result, cryptosystems with 40-bit keys offer virtually no
protection at this point against brute-force attacks.  Even the U.S.
Data Encryption Standard with 56-bit keys is increasingly inadequate.
As cryptosystems often succumb to `smarter' attacks than brute-force
key search, it is also important to remember that the keylengths
discussed here are the minimum needed for security against the
computational threats considered.
    Fortunately, the cost of very strong encryption is not
significantly greater than that of weak encryption.  Therefore, to
provide adequate protection against the most serious threats ---
well-funded commercial enterprises or government intelligence agencies
--- keys used to protect data today should be at least 75 bits long.
To protect information adequately for the next 20 years in the face of
expected advances in computing power, keys in newly-deployed systems
should be at least 90 bits long.
1. AT&T Research, mab at research.att.com
2. Sun Microsystems, diffie at eng.sun.com
3. MIT Laboratory for Computer Science, rivest at lcs.mit.edu
4. Counterpane Systems, schneier at counterpane.com
5. San Diego Supercomputer Center, tsutomu at sdsc.edu
6. Access Data, Inc., eric at accessdata.com
7. Bell Northern Research, wiener at bnr.ca

@_date: 1996-02-17 07:54:36
@_author: Matt Blaze 
@_subject: ITAR personal use exemption 
I just happen to be heading to the airport shortly, and am looking
forward to being among the first exports under the new rule...
------- Forwarded Message
Received: from research.att.com by nsa.tempo.att.com (8.6.10/4.7)
Received: from cs.cosc.georgetown.edu by research; Fri Feb 16 14:16:40 EST 1996
Received: from chair.georgetown.edu (chair.cosc.georgetown.edu) by cs.cosc.georgetown.edu (4.1/1a-eef)
Message-Id: <9602161913.AA00958 at cs.cosc.georgetown.edu>
In case you haven't heard ...
Best regards,

@_date: 1996-01-05 04:39:06
@_author: Matt Blaze 
@_subject: USENIX anyone? 
I'm going to be at the USENIX conference in San Diego later this month,
as are, I suspect, many other crypto/cypherpunk types.
Any interest in a crypto BOF?

@_date: 1996-01-07 06:42:51
@_author: Matt Blaze 
@_subject: "trust management" vs. "certified identity" 
The discussions here of the limits of PGP's certification and
revocation model are close to the core of some work I've been doing
(with Joan Feigenbaum and Jack Lacy) on what we call the "trust
management" problem.
Essentially we consider the consequences of abandoning the notion
of "certified identity" implicit in systems like X.509 and PGP and
subsuming identity under the more general umbrella of specifying
and determining what a key is trusted to do.
We've built a system, called "PolicyMaker", that allows the certifier
of a key to specify what the key is trusted to do rather than to
whom the key is trusted to belong.  The same mechanism is also used
to specify and interpret local policies.  The PolicyMaker system
is designed to be called as a service by local applications,
which could be email systems like PGP or network-layer security
protocols or any other application that requires complex trust
Some early, local experience suggests that this approach is a good
one.  It's easy to specify X.509- and PGP-style policies and
certificates, but you can also say things like "valid for transactions
over $500 only if countersigned" in a fairly natural way.
I'll be happy to send a (very early) draft of our paper, "Decentralized
trust management" to anyone who's interested.  I've made the draft
available in the CFS-users email archive server.  To request a copy
(PostScript format) by email:
   echo get cfs-users pmdraft.ps | mail cfs-users-request at research.att.com
(For non-unix shell people, just send a message to
With the line:
in the BODY of the message (NOT on the subject line).)
Comments and discussion appreciated.
This is an early draft, and I'd appreciate it if it not be directly
quoted, cited, or re-distributed.
PS  We expect to give away our reference implementation, too.  (Probably
by May or so.)  Note that this is just research, and does not represent
any current, past, or future product or service offering on the part
of AT&T or anyone else.

@_date: 1996-01-07 09:13:47
@_author: Matt Blaze 
@_subject: "trust management" vs. "certified identity" 
Yes.  That's pretty much PolicyMaker in a nutshell.

@_date: 1996-01-17 23:25:30
@_author: Matt Blaze 
@_subject: Microsoft's CAPI 
I attended a meeting at Microsoft the other day at which they
described their Crypto API project.  As CAPIs go, it's reasonable
enough; nothing particularly exciting about it or especially wrong
with it (though they don't yet support nonblocking calls to crypto
They've defined 23 cryptographic services (establish key, encrypt,
etc.) that an application is expected to use for its cryptographic
needs.  The idea is to hide the crypto details (and keys) from
applications, and to make it easy to switch from, say, wimpy
export-approved crypto to good crypto just by switching to another DLL
at load-time.  The cryptography used depends on the crypto modules in
use at runtime.  The API will be part of the WIN32 interface.  The
next version of NT (and windows 95, I think), to be released in a few
months will support loading ``Cryptographic Service Providers'' (CSPs)
that contain the crypto functions that sit below the API.  They have
(or will have soon) an application development kit to allow you to
write code that uses the API, and a CSP development kit to let you
write the crypto functions.
The interesting part is that they say they've made a deal with the
government to allow applications that use the API to be exportable as
long as they don't also try to implement crypto on their own.
Ordinarily, the government claims that ``crypto with a hole''
(applications that call a crypto API) are just as export-controlled as
crypto functions themselves, so this is something of a surprise and
would represent considerable forward progress.  But, of course,
there's a catch.
The OS will not load just any old CSP.  CSPs have to be signed by
Microsoft.  The kernel contains a (hardcoded?) 1024 RSA public key
that it uses to check the signature when the user tries to load a CSP.
If the signature check fails, the CSP won't load.  Microsoft says it
will sign any CSP from anyone AS LONG AS THEY CERTIFY THAT THEY WILL
FOLLOW THE EXPORT RULES.  So you can get your CSP signed if you use
exportable cryptography or if you agree not to send it outside the US
and Canada, etc.  But an end user can't just compile crypto code and
use it as a CSP, even for his or her own use, without getting it
signed by Microsoft first (actually, the CSP development kit does
allow this, but it uses a special version of the OS).
I'm not sure whether this whole thing is good or bad.  One important
issue is whether MS will really sign anyone's CSP or whether they will
start charging high fees or making business-based decisions on who's
CSPs they will allow (with they sign Netscape's CSP, for example).
They say they won't even look or keep a copy of your CSP (at my
suggestion, they are probably going to change the process so that you
send them a hash of your CSP instead of your CSP code when you get the
signature).  For now they promise to sign CSPs for anyone who returns
the export certificate, at no charge.
We (Jack Lacy and I) will probably implement, get signed, and give
away a CryptoLib-based CSP (not for export) for which we will also
make source available so people examine the source to their crypto
(most CSPs will, presumably, not include source).
Despite all this, I think it will be easy to get around the CSP
signature requirements and use homebrew, unsigned crypto even with
pre-compiled .exe files from other sources.  I suspect it will be easy
to write a program, for example, that takes an executable program
and converts CryptoAPI calls to calls that look like just another DLL.
And I'm sure someone will write a program to patch the NT/Windows
kernel to ignore the signature check.  Needless to say, it would be
nice if someone outside the US were to write and distribute programs
to do this.  It would also be nice if someone would write a Unix/Linux
version of the API/CSP mechanism.  It might make it possible to export
applications for those platforms as well.
I haven't tried any of this out yet, but they say they will have beta
versions of the API and CSP developers kits out in a few weeks.  They
say that the API kit will not be export-controlled but the CSP kit
will be.  They plan to announce all this at the RSA conference this

@_date: 1996-01-30 09:50:38
@_author: Matt Blaze 
@_subject: Page one, NY Times, 29 January 1996 
One of those microscopic bottom-of-page-one ads from John Young:
"BOYCOTT ESPIONAGE-ENABLED SOFTWARE", with phone number and email
address to contact for more information.
I'd be curious as to what the response has been like.

@_date: 1996-07-19 04:36:30
@_author: Matt Blaze 
@_subject: NSA response to key length report 
July 18, 1996
There is currently being circulated, to members of Congress and
possibly elsewhere, a four page document entitled ``Brute-Force
Cryptanalytic Attacks'' that calls into question some of the
conclusions of the ``Minimum Key Lengths for Symmetric Ciphers'' white
paper [1].  The document bears no author or organization attribution,
but we are told that it originated from NSA.
The NSA document argues that ``physical realities'' make parallel key
search much more expensive and time consuming than our white paper
estimated.  However, the NSA document appears to have been written
from the perspective of general parallel processing or cryptanalysis
rather than exhaustive key search per se.  It ignores several
elementary principles of parallel processing that apply specifically
to exhaustive key search machines of the type that our white paper
In particular, NSA argues that interconnections, heat dissipation,
input/output bandwidth, and interprocessor communication make it
difficult to ``scale up'' a key search machine by dividing the task
among a large number of small components.  While these factors do
limit the scalability of more general purpose multiprocessor computers
(such as those made by Cray), they do not apply at all to specialized
exhaustive key search machines.  The NSA argument ignores the most
fundamental feature of brute-force key search: the processors
performing the search have no need to communicate with other
components of the system while they perform their share of the search,
and therefore the system has no need for any of the global
interconnections that limit scaling.  Indeed, there is no reason that
all the components of a parallel search machine must be located even
within the same city, let alone the same computer housing.  We note
that one of our co-authors (Eric Thompson, of Access Data, Inc.)
designs and builds medium-scale FPGA-based key search machines with
exactly this loosely-coupled structure, and regularly uses them to
recover keys for clients that include the FBI.
The NSA document also calls into question our cost estimates for ASIC
components, suggesting that ASIC chips of this type cost NSA
approximately $1000.00 each.  However, our $10.00 per chip estimate is
based on an actual price quote from a commercial chip fabrication
vendor for a moderate-size order for an exhaustive search ASIC
designed in 1993 by Michael Wiener [2].  Perhaps NSA could reduce its
own costs by changing vendors.
Finally, the NSA report offers estimates of the time required to
perform exhaustive search using a Cray model T3D supercomputer.  This
is a curious choice, for as our report notes, general-purpose
supercomputers of this type make poor (and uneconomical) key search
engines.  However, even the artificially low performance results for
this machine should give little comfort to the users of 56 bit keys.
According to NSA, 56 bit keys can be searched on such a machine in
less than 453 days.  ``Moore's law'' predicts that it will not be long
before relatively inexpensive general-purpose computers offer similar
computational capability.
     Whitfield Diffie
[1] Blaze, M., Diffie, W., Rivest, R., Schneier, B., Shimomura, T.,
    Thompson, E., and Wiener, M.  ``Minimum Key Lengths for Symmetric
    Key Ciphers for Commercial Security.''  January 1996.  Available
    from ftp://ftp.research.att.com/dist/mab/keylength.txt
[2] Wiener, M.  ``Exhaustive DES Key Search.''  Presented at
    Crypto-93, Santa Barbara, CA.  August 1993.
[Transcription of document circulated to various members of congress
and others in June, 1996, apparently by NSA]
BRUTE-FORCE CRYPTANALYTIC ATTACKS
Two published theoretical estimates of cost versus time to perform
brute-force hardware attacks on selected cryptography key lengths
differ between themselves and differ significantly from what we find
when we buy or build computers to carry out such attacks.
The differences lie in assumptions made in the theoretical estimates,
which are not fully spelled out by the authors, and in scaling up
hypothesized small machines to ever larger ones without accounting for
physical realities.
The factors not accounted for are:
  o R&D costs for the first machine, typically on the order of $10
    million.
  o As more and more chips are added to a machine, two effects occur:
      o Interconnections increase and increase running time;
      o Heat from the chips eventually limit [sic] the size of a
        machine.
  o Memory costs are not included.
  o When get [sic] to the very fast processing speed estimates,
    machines can become Input/Output bound; so [sic] it cannot achieve
    the estimated speed.
  o Assuming every algorithm can be tested in same amount of time and
    key length is the only difference.
Table 1 are [sic] the average time estimates made for a given cost
done by Michael Wiener of Bell Norther Research in 1995.  These are
published in Bruce Schneier's Applied Cryptography book.
Note that these are average times, one-half of the total exhaust time.
Table 2 are [sic] the estimates for total exhaust times using Field
Programmmable Gate Arrays (FPGA) and Application Specific ICs (ASICs)
done for the Business Software Alliance by Blaze, Diffie, Rivest,
Schneier, Shimomura, Thompson, and Wiener in 1996.  In addition to the
above factors not accounted for they have assumed ASICs cost as low as
$10.  We find ASICs more typically cost $1000 and their capabilities
can vary considerably depending upon the specific task.
Table 3 are out estimates based on our experience with a Cray T3D
supercomputer with 1024 nodes.  This machine costs $30 million.
[Tables 1, 2, and 3 not transcribed here.]

@_date: 1996-06-12 15:20:59
@_author: Matt Blaze 
@_subject: Oblivious key escrow 
I've revised and expanded the paper on decentralized key escrow that I
presented at the Cambridge Information Hiding workshop and mentioned
here a few weeks ago.  The paper is now called "Oblivious Key Escrow",
and is available from:
We propose a simple scheme, based on secret-sharing over large-scale
networks, for assuring recoverability of sensitive archived data ({\em
e.g.,} cryptographic keys).  In our model anyone can request a copy of
the archived data but it is very difficult to keep the existence of a
request secret or to subvert the access policy of the data ``owner.''
We sketch an architecture for a distributed key escrow system that
might be suitable for deployment over very large-scale networks such
as the Internet.  We also introduce a new cryptographic primitive,
{\em oblivious multicast,} that can serve as the basis for such a

@_date: 1996-06-28 08:48:32
@_author: Matt Blaze 
@_subject: my senate testimony 
I sent a copy of my senate testimony here last night, discovered
that a bunch of lines were truncated, and (I thought) immedately sent out
a revised, corrected version.  The garbled version got forwarded to
a bunch of mailing lists, however, and now I'm getting dozens of
messages from people telling me about the truncated lines.
The corrected version can be found at
Please forward this version (or the URL) to anyone to whom you forwarded
the the garbled version.  Thanks.

@_date: 1996-03-16 13:18:54
@_author: Matt Blaze 
@_subject: PolicyMaker paper available 
A number of people have been asking me about some work I've been doing
(with Joan Feigenbaum and Jack Lacy) on alternatives to traditional
(X.509, PGP, etc.) identity-based certificates.  We've just finished
up our paper on the concept, "Decentralized Trust Management", to
appear at the Oakland Security Conference in May.
A PostScript pre-print is available in
     ftp://research.att.com/dist/mab/policymaker.ps
[NB: I no longer read the cypherpunks list with any regularity, so
please cc me directly on any comments or discussion.  Thanks.]

@_date: 1996-03-17 17:51:20
@_author: Matt Blaze 
@_subject: new release (v.1.3.3) of CFS encrypting filesystem 
Source code for the latest version (release 1.3.3) of CFS, the Cryptographic
File System, is now available upon request for research and experimental
use in the US and Canada.  This version works under most BSD-derived Unix
systems and should now run without modification under most current Linux
releases as well.
CFS pushes encryption services into the Unix(tm) file system.  It
supports secure storage at the system level through a standard Unix
file system interface to encrypted files.  Users associate a
cryptographic key with the directories they wish to protect.  Files in
these directories (as well as their pathname components) are
transparently encrypted and decrypted with the specified key without
further user intervention; cleartext is never stored on a disk or sent
to a remote file server.  CFS employs a novel combination of DES
stream and codebook cipher modes to provide high security with good
performance on a modern workstation.  CFS can use any available file
system for its underlying storage without modification, including
remote file servers such as NFS.  System management functions, such as
file backup, work in a normal manner and without knowledge of the key.
CFS runs under SunOS and several other BSD-derived systems with NFS.
It is implemented entirely at user level, as a local NFS server
running on the client machine's "loopback" interface.  It consists of
about 5000 lines of code and supporting documentation.  You must have
"root" access to install CFS.
CFS was first mentioned at the work-in-progress session at the Winter
'93 USENIX Conference and was more fully detailed in:
    Matt Blaze. "A Cryptographic File System for Unix", Proc. 1st ACM
    Conference on Computer and Communications Security, Fairfax, VA,
    November 1993. (PostScript available by anonymous ftp from
    research.att.com in the file dist/mab/cfs.ps.)
and in
    Matt Blaze. "Key Management in an Encrypting File System", Proc.
    Summer '94 USENIX Tech. Conference, Boston, MA, June 1994.
    (PostScript available by anonymous ftp from research.att.com
    in the file dist/mab/cfskey.ps.)
Version 1.3 of CFS also includes ESM, the Encrypting Session Manager.
ESM provides shell-to-shell encrypted sessions across insecure links
and requires no OS or network support.  It is useful for typing cfs
passphrases when logged in over the network.  ESM needs RSAREF 2.0 to
compile and is tested only on SunOS and BSDI.  ESM is the first released
part of a suite of session encryption tools that are described in
    Matt Blaze and Steve Bellovin. "Session-layer Encryption."
    Proc. 1995 USENIX Security Workshop, Salt Lake City, June 1995.
    (PostScript is available from
    ftp://research.att.com/dist/mab/sesscrypt.ps)
The new version of CFS differs from the version described in the
papers in a few ways:
* The DES-based encryption scheme has been strengthened, and now
provides greater security but with the online latency of only single-DES.
* Support for the smartcard-based key management system is not
included and a few of the tools are not included.
* An impoved key management scheme now allows chaning the passphrase
associated with a directory.
* The performance has been improved.
* The security of the system against certain non-cryptanalytic attacks
has been improved somewhat. * User-contributed ports to a number of additional platforms.
* Hooks for adding new ciphers.
* 3-DES, MacGuffin, and SAFER-SK128 encryption options.
* Timeout options allow automatic detach of encrypted directories
after a set time or period of inactivity.
CFS is distributed as a research prototype; it is COMPLETELY
UNSUPPORTED software.  No warranty of any kind is provided.  We will
not be responsible if the system deletes all your files and emails the
cleartext directly to the NSA or your mother.  Also, we do not have
the resources to port the software to other platforms, although you
are welcome to do this yourself.  The software was developed under
SunOS and BSDI, and there are also unsupported user-contributed ports
available for AIX, HP/UX, Irix, Linux, Solaris and Ultrix.  We really
can't promise to provide any technical support at all, beyond the
source code itself.  We also maintain a mailing list for CFS users and
developers; subscription information is included with the source code.
Because of export restrictions on cryptographic software, we are only
able to make the software available within the US and Canada to US and
Canadian citizens and permanent residents.  Unfortunately, we cannot
make it available for general anonymous ftp or other uncontrolled
access, nor can we allow others to do so.  Sorry.
Legal stuff from the README file:
 *              Copyright (c) 1992, 1993, 1994, 1995 by AT&T.
 * Permission to use, copy, and modify this software without fee
 * is hereby granted, provided that this entire notice is included in
 * all copies of any software which is or includes a copy or
 * modification of this software and in all copies of the supporting
 * documentation for such software.
 *
 * This software is subject to United States export controls.
 *
 * THIS SOFTWARE IS BEING PROVIDED "AS IS", WITHOUT ANY EXPRESS OR IMPLIED
 * WARRANTY.  IN PARTICULAR, NEITHER THE AUTHORS NOR AT&T MAKE ANY
 * REPRESENTATION OR WARRANTY OF ANY KIND CONCERNING THE MERCHANTABILITY
 * OF THIS SOFTWARE OR ITS FITNESS FOR ANY PARTICULAR PURPOSE.
If you would like a copy of the CFS source code, please read to the end
of this message and then send email to:
DO NOT REPLY DIRECTLY TO THIS MESSAGE.  You must include a statement
that you are in the US or Canada, are a citizen or legal permanent
resident of the US or Canada, and have read and understand the license
conditions stated above.  Be sure to include an email address in a US-
or Canada-registered domain. The code will be sent to you via email in
a "shar" shell archive (a little over 300K bytes long).

@_date: 1996-05-25 18:53:42
@_author: Matt Blaze 
@_subject: "Key Escrow without Escrow Agents" 
Here's a draft of a (rather half-baked) data recovery scheme that I'll be
presenting at a workshop next week.  I've included the LaTeX source below;
sorry for the length and for the formatting (which should be reasonably
easy to ignore for those without LaTeX).
Please include me in any response, since I don't read the list these days.
======cut here====
\title{Key Escrow without Escrow Agents}
\author{{Matt Blaze}\\
AT\&T Research\\
Murray Hill, NJ 07974\\
{\tt mab at research.att.com}}
\date{DRAFT -- 24 May 1996 -- Extended Abstract -- DRAFT}
We propose a simple scheme, based on secret sharing over large-scale
networks, for assuring recoverability of sensitive archived data
(e.g., cryptographic keys).  In our model anyone can request a copy of
the archived data but it is very difficult to keep the existence of a
request secret or to subvert the access policy of the data ``owner''.
We sketch an architecture for such a system that might be suitable for
deployment over very large-scale networks such as the Internet.
In any system in which sensitive information must be stored for future
use, there is a fundamental tension between ensuring the {\em secrecy}
of data against those who are not authorized for access to it and
ensuring its {\em availability} to those who are.  Secrecy is often
best served by making only a small number of carefully-guarded copies
of the data, while availability favors a policy of the widest possible
dissemination in the hope that at least one copy will be intact at the
time it is required.  In general, a balance has to be struck between
these two goals based on the requirements of and resources available
to the particular application, but in any case copies of the sensitive
data must be controlled in some careful manner (e.g., through the use
of an off-site, trusted backup facility employing guards and other
effective, if expensive, security practices).
Another approach is ``key escrow'', in which the sensitive data is
encrypted so that the ciphertext can be widely copied and backed-up
via conventional methods, but the decryption keys are controlled in
some careful manner by trusted third parties who assume responsibility
for revealing the keys to authorized entities in the event of an
emergency.  One advantage of key escrow over controlled backup of the
data itself is that keys can be escrowed at any time, even prior to
the creation of the actual sensitive data, and one escrowed key can
represent arbitrarily much encrypted information.  A number of key
escrow schemes have been proposed for a variety of applications, most
with the aim of facilitating law enforcement access to encrypted data,
but also for commercial data recovery
Third party backup, whether of data or keys, has a number of
disadvantages, however.  The ``escrow agents'' must be highly trusted
and carefully protected, since compromise single escrow site (or small
set of sites, in the case of split data) can result in an irrevocable
loss of security.  Since protecting such data is likely to be
expensive, one escrow site can be expected to serve many different
sets of data, making each site an attractive ``fat target'' for
attack.  Finally, legal, liability, and conflict-of-interest issues
sometimes make it difficult to ensure that an escrow agent will act
only in the best interests of the data owner, especially when served
with a legal demand to turn over keys or tempted with some inducement
to misbehave.  One of the frequently-raised objections to
government-run key escrow systems (e.g., the ``Clipper'' chip) is the
fear that the escrow centers will, perhaps secretly, assist a rogue
government in violating its citizens' privacy.
In this abstract, we propose a different model for assuring both
recoverability and protection of sensitive data based on two concepts:
secret sharing and the decentralized nature of large, heterogeneous
networks such as the Internet.  In our model, anyone can request a
copy of anyone's data but it is not possible to keep the existence of
such a request secret or to subvert the access policy of the data
``owner'' without subverting a significant fraction of participants in
the network.  There are no explicit ``escrow agents''; instead, key
shares are distributed widely to ordinary networked computers spread
across a wide variety of administrative and geographic boundaries.
\section{``The Net'' as an Escrow Agent}
The goal of our scheme is to make it difficult to recover escrowed
data without the knowledge and consent of the data owner, while still
assuring high availability in an emergency.  Its security rests on the
premise that highly distributed systems spread over many
administrative, political, and geographic domains (such as the
Internet), are more robust than any single site or small set of
sites, no matter how well protected.  Other systems, such as Eternity
\cite{eternity}, have recognized and exploited this property of
global networks for maintaining information availability; we simply
expand this notion to include secrecy as well.
We assume that each node (or a large number of nodes) in the network
runs an ``escrow server'' that performs most of the steps of the
protocol, and that there is some broadcast mechanism for reaching them
(which could be based on existing mechanisms such as Usenet news).
The first step in using ``the net'' as an escrow agent is to split the
key to be escrowed using some secret sharing scheme \cite{simmons}
with a very large number of shares (e.g., a $k$ out-of $n$ scheme
where $k=500$ and $n=5000$, but we leave the details of determining an
appropriate access structure to the reader).  Next, we package each
share along with a key identifier, a digital signature of the share,
and a policy describing the circumstances under which the share should
be disclosed (discussed below).  Finally, we select, at random (or
according to some other policy) as many sites as we have shares and
send one share to each site, over a secure channel.  We then destroy
the shares and the list of sites to which they were sent.
To recover escrowed data, we broadcast a request for shares for the
key identifier we want to recover, using some mechanism that is likely
to be received by the shareholders' escrow servers.  Upon receipt of a
request for shares, each escrow server logs the request and, if it
holds a share for the key in question, checks the policy contained in
the share package.  If the request conforms to the policy we send the
share to the requester.  The requester (who can verify the
authenticity of each share by checking the signature) can recover the
key once enough shares have been received.
Whether such a scheme is robust, secure, or otherwise adequate depends
primarily on three factors: the reliability (in terms of continued
existence, security against compromise, and ability to follow
instructions) of the nodes that handle the key shares, the access
structure of the secret sharing scheme, and the nature of the policy
that each node is supposed to follow.
If the nature of today's Internet is any indication, we must assume
that the individual nodes are not very reliable, especially over time.
Some nodes will simply disappear.  Others will maliciously fail to
follow instructions.  Still others will fail to safeguard their
shares, sometimes due to malice but more often as a result of mistake,
incompetence, or failure of some underlying security mechanism.  It is
likely that as the net grows these issues will become even more
pronounced.  Therefore, the security of the scheme depends on a choice
of access structures and policies that assumes that a large fraction
of shareholders will not follow the correct protocol.
The secret sharing access structure must be chosen to require enough
shares to prevent key recovery by collusion among a few nodes, yet
with enough redundancy to allow recovery in the likely event that most
nodes are not available or did not retain their shares at the time key
recovery is required.  Scale appears to help here; consider, for
example, a 500 out of 5000 threshold scheme, which permits key
recovery even when 90\% of nodes have failed and yet retains its
security until 500 nodes have been compromised.  The distribution of
nodes could also play a part here, particularly when the key is split
with a more sophisticated access structure.  For example, key shares
could be distributed to nodes selected across a variety of
administrative, legal, political and geographic domains, with the
access structure selected to require that shares be collected from
nodes in several different categories.
Each shareholder is also asked to respect the access policy included
with the share.  The policy must be designed to facilitate emergency
access without also permitting undetected disclosure.  Because shares
can only be recovered by broadcasting, we can take advantage of the
inherently public nature of requests in formulating the access
policies.  For example, the policy might specify a public signature
key to which the real key holder knows the corresponding secret and a
request to delay revealing key shares for some period of time, say one
week.  If an unauthorized request for a key is broadcast, the real key
holder would have one week to notice the request and broadcast another
message, signed with this key, requesting that the shareholders ignore
the original request and turn over information that might aid in
tracking down the source of the unauthorized request.  Policies might
also include instructions on the minimum identification that share
requests must include and instructions on how share requests should be
logged (e.g., by posting to a news group or even advertisements in
newspapers).  They might also include an expiration date beyond which
the share is to be deleted.  We defer the question of how policies
should be specified, but it may be sufficient for the server, upon
receipt of a share request, to send a message to its (human) operator
containing instructions (written in English) that were included in the
share package.
Some infrastructure is required.  Key holders would need a directory
or other mechanism for identifying and communicating with escrow
servers at the time the shares are created.  A broadcast mechanism for
key recovery is also required.  It is possible that existing
mechanisms could suffice for both these purposes (e.g., DNS for server
identification and Usenet news for broadcasting) but more specialized
systems would be required if this scheme were to be fielded on a large
Share distribution must be secure against both eavesdropping and
traffic analysis.  The need for security against eavesdropping is
obvious, since observing all the shares allows recovery keys without
the assistance of the shareholders.  Resistance to traffic analysis is
required to ensure that shares can only be recovered by broadcasting.
If the identities of the shareholders are known, an attacker could
``target'' the sites believed to be weakest, and, if successful,
recover shares without broadcasting the request and without following
the share access policy.  Shares could be distributed via an anonymous
communication network or some other mechanism (such as oblivious
transfer) that obscures the nature of the transaction from an observer
(and perhaps even from the participants themselves).  Key identifiers
should be chosen so that an outside attacker cannot derive the purpose
or owner of the key from its identifier and so that shareholders do
not know exactly what their shares are for.  In any case, the list of
shareholders should not be retained by the key owner once the shares
have been distributed.
\subsection{Emergency Access -- ``Angry Mob Cryptanalysis''}
In general, the key identifier should be stored with all copies of the
ciphertext (since without the key ID, it is impossible to recover a
key).  Under ordinary circumstances when a key recovery is required,
the original key owner will initiate the request.  The owner extracts
the key ID and broadcasts the request to the network, performing
whatever (presumably public) logging is required by the policy that
was sent to the shareholders.  Upon receipt of the broadcast,
each server checks whether it is a shareholder for the requested key.
If it is, it checks whether request satisfies the access policy
(perhaps by transmitting a copy of the English-specified policy to the
server operator, perhaps by automated means if the policy is more
formally specified).  If the access policy is satisfied (e.g., a
message announcing the request appeared in some established place, a
certificate of the identity of the requester was included in the
request, or whatever) and after waiting however log the policy
specified to allow for repudiation of the request by the legitimate
key owner, the share is transmitted back to the requestor over a
secure channel.  The requestor can then combine the shares to recover
the key; corrupted shares will not affect the protocol since the
shares should have been digitally signed by the original key owner at
the time they were distributed.
Sometimes, however, an extreme emergency might make it necessary to
recover keys in a manner contrary to the policy specified in the
original shares.  For example, it may be necessary to recover keys
before the policy-imposed delay has elapsed, or to obtain access in
spite of the objections of the original key owner.  Such a situation
is most likely to arise from some kind of law enforcement or public
safety emergency in which the requestor makes the case that public
policy should supersede the access policy of the key holder.  Of
course, such a situation is fraught with difficult issues of judgement
and policy, and fears of abuse, fraud, or coercion are among the
primary objections raised against key escrow in general.  Our scheme
places the burden of determining whether such an exceptional access
request should be granted on the shareholders.
Indeed, the dependence on the collective judgement of the widely
distributed shareholder operators may be the scheme's most important
property.  Under normal circumstances, the shareholders can be
expected to behave approximately as specified in the share policies
(with occasional pathological exceptions, limited in their effect by
the nature of the secret sharing access structure).  In exceptional
situations, however, a public appeal can be made in an attempt to
convince the shareholders to reveal their shares in a manner not
permitted by the stated policy (e.g., the police could broadcast an
appeal for key shares on television news, stating the facts of the
case under investigation).  In particular, because the identities of
the shareholders are not known, such an appeal must be done publicly
and in a manner designed to attract considerable attention.  It is not
possible to secretly induce, through legal means or otherwise,
shareholders to reveal their shares.  For some applications (e.g.,
personal information associated with an individual), such a scheme
could be acceptable even when key escrow is not.  (We introduce the
rather lighthearted term ``angry mob cryptanalysis'' to refer to the
threat of enough shareholders being convinced to violate the share
access policy to permit key recovery.  It is distinguished from
``rubber hose cryptanalysis,'' which involves obtaining keys by legal
or extra-legal intimidation\footnote{The phrase ``rubber hose
cryptanalysis'' appears to be due to Phil Karn.}.)
Key escrow is a confusing subject, especially so because there is
little general agreement as to even its basic goals and requirements.
We have proposed a scheme that has a number of interesting properties
that may make it appropriate for protecting secrecy and availability in
certain kinds of applications.  A number of open problems remain, of
course, before such a scheme could be made completely practical.
Areas for further study include the effects of different access
structures, specification of policy, and economic, performance, and
reliability analysis.
Of course, we do not in complete seriousness propose this scheme as a
general solution to the key recovery problem, but intend instead to
open a new avenue of discussion.  In particular, the scheme appears to
address many of the concerns of both opponents of ``government'' key
escrow as well as many of the (stated) concerns of law enforcement.
Much of the inspiration for this scheme arose from Ross Anderson's
description of the motivation and principles behind the Eternity file
service, in conversations at Cambridge and at AT\&T Bell Labs.
\newblock Ross Anderson.
\newblock ``The Eternity Service.''
\newblock Invited paper to appear at {\em Pragocrypt 96.} 30 Sep - 3
Oct 1996, Prague.
\newblock Dorothy Denning.
\newblock ``A Taxonomy for Key Escrow Encryption Systems.''
\newblock {\em CACM.}  March 1996.
\newblock Silvio Micali.
\newblock ``Fair Cryptosystems.''
\newblock {\em MIT/LCS/TR-579.c}  Laboratory for Computer Science,
Massachusetts Institute of Technology, Cambridge, MA, August 1994.
\newblock National Institute for Standards and Technology.
\newblock Escrowed Encryption Standard, {\em Federal Information
Processing Standards Publication 185}, U.S.  Dept. of Commerce, 1994.
\newblock G.J. Simmons.
\newblock ``An introduction to Shared Secret and/or Shared Control
Schemes and their Applications.''
\newblock In {\em Contemporary Cryptolgy,} Simmons, ed. IEEE Press,
\newblock Stephen T. Walker, Stephen B. Lipner, Carl M. Ellison, and
David M. Balenson.
\newblock ``Commercial Key Recovery.''
\newblock {\em CACM.} March 1996.

@_date: 1996-05-27 07:14:02
@_author: Matt Blaze 
@_subject: net-based key archival 
I've put a revised version of my "Key Escrow without Escrow Agents"
abstract in my ftp directory, in PostScript and Latex formats.

@_date: 1996-11-20 11:36:32
@_author: Matt Blaze 
@_subject: FYI - Anderson & Kuhn's new "Improved DFA" paper 
My appologies if this has been posted already.
------- Forwarded Message
Precedence: bulk
                   Improved Differential Fault Analysis
                      Ross J Anderson, Markus G Kuhn
In [1], Biham and Shamir announce an attack on DES based on 200 ciphertexts in
which one-bit errors have been induced by environmental stress. Here we show an
attack that requires less than ten ciphertexts. Furthermore, our attack is
practical in that it uses a fault model that has been implemented in attacks on
real smartcards.
In [2], Biham and Shamir show how their method can be extended to reverse
engineer algorithms whose structure is unknown. Our attack can also be extended
to such cases and is more efficient there too. In [3], Boneh, De Millo and
Lipton discuss how such techniques can be used to attack RSA. Again, their
attack is theoretical only, We show how to do it in practice.
A recent research announcement by Biham and Shamir shows that if DES is
implemented in a tamper-resistant package, and this package has the property
that by applying ionising radiation (or some other environmental stress) we can
cause random one-bit errors in the round keys, then we can break DES. If we can
get a series of ciphertexts, each generated from the same plaintext but with a
different one-bit random round key error, then we will need about 200 faulty
ciphertexts to recover the key. In a further announcement [2], they show how on
a similar fault model, the structure of unknown Feistel ciphers can be deduced
from an adequate number of faulty ciphertexts. In each case, the critical
observation is that errors that occur in the last few round of the cipher leak
information about the key, or algorithm structure, respectively.
This work is inspired by a paper of Boneh, DeMillo and Lipton [3] who assume (as Biham and Shamir do) that one-bit errors can be caused by radiation or
other environmental stresses. That paper goes on to show that with this fault model, RSA can be attacked.
These results have been widely publicised in the press. A frequently voiced
criticism is that the attacks are purely theoretical: no-one has demonstrated
that single bit errors can actually be induced in a DES key schedule or an RSA
computation in any fielded device. In fact, most smartcards hold keys in EEPROM
which also contains much or all of the device's application software. Thus
errors induced by ionising radiation would be much more likely to corrupt
software, thus leading either to a system crash or to uninformative wrong
We show here that much faster, and completely practical, attacks are possible.
The trick is to induce small changes in the code rather than trying to cause
them in keys or other data.
The Improved Attack Methodology
In a note posted to relevant Internet newsgroups on the 7th November, one of us
pointed out that using clock and power glitches gives a practical way of
implementing the Lenstra variant of the Boneh attack. In this announcement, we
will expand on the threat model, and also show how attacks using clock and
power glitches can give attacks on DES that require many fewer ciphertexts -
less than ten rather than the 200 or so previously required.
In a paper on tamper resistance due to be published next week, we describe a
number of techniques for attacking smartcards and other security processors
[5]. This paper was written some time ago (the first results were presented at
the Isaac Newton Institute, Cambridge, in June) but has been withheld by
agreement with the manufacturer of one of the security processors we have
attacked, so that banking industry clients had some time to take suitable
precautions. Some of the attacks we describe in this paper are new, while
others are already known in various small communities (such as hackers and chip
testers) and are included for the benefit of the wider crypto and security
One of the latter type is that smartcards and other security processors can
often be attacked using clock and power glitches. The application of a clock
pulse that is much faster than normal, or of a transient in the power supply,
can often cause faulty behaviour in a microprocessor, under which the program
counter is incremented but the current instruction is executed either
incorrectly or not at all. A standard version of this attack is to replace a
single 5MHz clock pulse to a smartcard with four 20MHz pulses.
We do not claim to have invented this attack; it appears to have originated in
the pay-TV hacking community, which has known about it for at least a year. In
that context, it has not been used for attacks on cryptographic algorithms, but
in order to cause output loops to run for longer than the card's programmer
intended, thus dumping key material to the output port.
The glitch attack is described more fully in our tampering article, which will
appear at next week's Usenix Electronic Commerce Workshop [5].
In this note, we point out that attacks based on faulty instructions are not
only proven practical, unlike the as yet undemonstrated fault model of random
single bit errors induced by radiation. They also provide a much more powerful
attack on many cryptographic algorithms. This holds both when we are seeking to
recover a key for a known algorithm such as DES, and when we are trying to
reverse engineer an unknown algorithm that has been provided in a smartcard or other tamper resistant processor.
Attacking RSA
A simplified version of the Boneh-DeMillo-Lipton attack, due to Lenstra, is
quoted in [3]: if a smart card computes an RSA signature S on a message M
modulo n = pq by computing it modulo p and q separately and then combining them
using the Chinese Remainder Theorem, and if an error an be induced in (say) the
latter computation, then we can factor n at once as p = gcd(n,S^e-M) where e is
the public exponent.
This is absolutely ideal for a glitch attack. As the card spends most of its
time calculating the signature mod p and mod q, and almost any glitch that
affects the output will do, we do not have to be at all selective about where
in the instruction sequence the glitch is applied. Since only a single
signature is needed, the attack can be performed online: a Mafia eftpos
terminal applies the glitch, factors the modulus, calculates what the correct
signature should be, and sends this on to the bank. Thus the Mafia can harvest RSA secret keys without the customer or his bank
noticing anything untoward about the transaction he did at their shop. Given
that implementers of the new EMV electronic purse system propose to have only
10,000 different RSA secret keys per issuing bank, the Mafia will soon be able
to forge cards for a substantial proportion of the user population.
Attacking DES
When we can cause an instruction of our choice to fail, then attacking DES is
simple. We remove one of the xor operations that are used to combine the round
keys with the inputs to the S-boxes from the last two rounds of the cipher, and
repeat this for each of these key bytes in turn. The erroneous ciphertext
outputs that we receive as a result of this attack will each differ from the
genuine ciphertext in the output of usually two, and sometimes three, S-boxes.
Using the techniques of differential cryptanalysis, we obtain about five bits
of information about the eight keybits that were not xor'ed as a result of the
induced fault. So, for example, eight faulty ciphertexts should give us about
40 bits of the key, leaving a trivial keysearch.
Thus DES can be attacked with about one correct and eight faulty ciphertexts.
But how realistic is it to assume that we will be able to target particular
In most smartcards, the manufacturer supplies a number of routines in ROM.
Though sometimes presented as an `operating system', the ROM code is more of a
library or toolkit that enables application developers to manage communications
and other facilities. Its routines usually include the DES algorithm (or a
proprietary algorithm such as Telepass), and by buying the manufacturer's
smartcard development toolkit (for typically a few thousand dollars) an
attacker can get full documentation plus real specimens for testing. In this
case, individual DES instructions can be targeted.
When confronted with an unfamiliar implementation, we may have to experiment
somewhat (we have to do this anyway with each card in order to find the correct
glitch parameters [5]). However the search space is relatively small, and on
looking at a few DES implementations it becomes clear that we can usually
recognise the effects of removing a single instruction from either of the last
two rounds. (In fact, many of these instructions yield almost as much
information when removed from the implementation as the key xor instructions
do.) We will discuss this at greater length in a later paper.
The ROM Overwrite Attack
Where the implementation is familiar, there is yet another way to extract keys
from the card - the ROM overwrite attack.
Single bits in a ROM can be overwritten using a laser cutter, and where the DES
implementation is well known, we can find one bit (or a small number of bits)
with the property that changing it will enable the key to be extracted easily.
The details will depend on the implementation but we might well be able, for
example, to make a jump instruction unconditional and thus reduce the number of
rounds in the cipher to one or two. Where the algorithm is kept in EEPROM, we
can use two microprobing needles to set or reset the target bit [6].
Where we have incomplete information on the implementation, ROM overwriting
attacks can be used in other ways. For example, if the DES S-boxes in ROM, we
can identify them using an optical microscope and use our laser cutter to make
all their bits equal. This turns DES into a linear transfromation over GF(2),
and we can extract the key from a single plaintext/ciphertext pair.
Although ROM overwrite (unlike the other attacks suggested in this paper)
involves access to the chip surface, it can be carried out using tools that are
relatively cheap and widely available. So it may be used by attackers who do
not have access to the expensive semiconductor test equipment that professional
pirates use to extract keys directly from smartcards [5].
Returning to the non-invasive attack model, we can always apply clock and power
glitches until simple statistical tests suddenly show a high dependency between
the input and output of the encryption function, indicating that we have
succeeded in reducing the number of rounds. This may be practical even where
the implementation details are unknown.
Reverse Engineering an Unknown Block Cipher
In [2], Biham and Shamir discuss how to identify the structure of an unknown
block cipher in a tamper resistant package (e.g., Skipjack) using one-bit
random errors. As before, they identify faults that affected only the last
round or rounds; this can be done by looking for ciphertexts at a low Hamming
distance from each other. They then identify which output bits correspond to
the left and right halves, and next look at which bits in the left half are
affected by one bit changes in the last-but-one right half. In the case of a
cipher such as DES with S boxes, the structure will quickly become clear and
with enough ciphertexts the values of the S-boxes can be reconstructed. They
report that with 500 ciphertexts the gross structure can be recovered, and with
about 10,000 the S-box entries themselves can be found.
Our technique of causing faults in instructions rather than in data bits is
more effective here too. We can attack the last instruction, then the second
last instruction, and so on.
We will give detailed estimates for DES in the final paper. Let us now consider
an actual classified algorithm. `Red Pike' was designed by GCHQ for encrypting
UK government traffic classified up to `Restricted', and the Department of
Health wishes to use it to encrypt medical records. The British Medical
Association, advised by one of us (Anderson) instead recommended that an
algorithm be chosen that had been in the open literature for at least two years
and had withstood attempts to find shortcuts (triple-DES, Blowfish, SAFER
K-128, WAKE,...).
In order to try and persuade the BMA that Red Pike was sound, the government
commissioned a study of it by four academics [7]. This study states that Red
Pike `uses the same basic operations as RC5' (p 4) in that the principal
operations are add, exclusive or, and left shift. It `has no look-up tables,
virtually no key schedule and requires only five lines of code' (p 4). Other
hints include that `the influence of each key bit quickly cascades' (p 10) and
`each encryption involves of the order of 100 operations' (p 19).
We can thus estimate the effort of reverse engineering Red Pike from a tamper resistant hardware implementation by considering the effort needed to mount a similar attack on RC5.
Removing the last operation - the addition of key material - yields an output
in which the right hand side is different (it is (B xor A) shl A). This
suggests, correctly, that the cipher is a balanced Feistel network without a
final permutation. Removing the next operation - the shift - makes clear that
it was a 32 bit circular shift but without revealing how it was parametrised.
Removing the next operation - the xor - is transparent, and the next - the
addition of key material in the previous round - yields an output with the
values A and B in the above expression. It thus makes the full structure of
the data-dependent rotation clear. The attacker can now guess that the
algorithm is defined by
          A = ((A xor B) shl B) op key
          B = ((B xor A) shl A) op key
Reverse engineering RC5's rather complex key schedule (and deducing that `op'
is actually +) would require single-stepping back through it separately; but
once we know that `op' is +, we can find the round key bits directly by working
back through the rounds of encryption.
So, apart from its key schedule, RC5 may be about the worst possible algorithm
choice for secret-algorithm hardware applications, where some implementations
may be vulnerable to glitch attacks. If Red Pike is similar but with a simpler
key schedule, then it could be more vulnerable still. However, since the
government plans to make Red Pike available eventually in software, this is not
a direct criticism of the design or choice of that algorithm.
It does all suggest, though, that secret-hardware algorithms should be more
complex; large S-boxes kept in EEPROM (that is separate from the program
memory) may be a sensible way of pushing up the cost of an attack. Other
protective measures that prudent designers would consider include error
detection, multiple encryption with voting, and designing the key schedule so
that the key material from a small number of rounds is not enough for a break.
We have improved on the Differential Fault Analysis of Biham and Shamir. Rather
than needing about 200 faulty ciphertexts to recover a DES key, we need less
than ten. We can factor RSA moduli with a single faulty ciphertext. We can also
reverse engineer completely unknown algorithms; this appears to be faster than
Biham and Shamir's approach in the case of DES, and is particularly easy with
algorithms that have a compact software implementation such as RC5.
Finally, our attacks - unlike those of Biham, Shamir, Boneh, DeMillo and Lipton
- - use a realistic fault model, which has actually been implemented and can be
used against fielded systems.
Mike Roe pointed out that the glitch attack on RSA can be done in real time by
a Mafia owned eftpos terminal.
[1] ``A New Cryptanalytic Attack on DES'', E Biham, A Shamir, preprint, [2] ``Differential Fault Analysis: Identifying the Structure of Unknown Ciphers
Sealed in Tamper-Proof Devices'', E Biham, A Shamir, preprint, 10/11/96
[3] ``On the Importance of Checking Computations'' D Boneh, RA DeMillo, RJ Lipton, preprint
[4] ``A practical variant of the Bellcore attack'', RJ Anderson, posted to
sci.crypt as message ID <55picf$dm3 at lyra.csx.cam.ac.uk>, 7/11/96
[5] ``Tamper Resistance - A Cautionary Note'', RJ Anderson, MG Kuhn, to appear
in Usenix Electronic Commerce workshop, 19/11/96
[6] ``Hardwaresicherheit von Mikrochips in Chipkarten'', Osman Kocar,
Datenschutz und Datensicherheit v 20 no 7 (July 96) pp 421--424
[7] ``Red Pike --- An Assessment'', C Mitchell, S Murphy, F Piper, P Wild,
Codes and Ciphers Ltd 2/10/96
------- End of Forwarded Message

@_date: 1996-10-18 12:15:19
@_author: Matt Blaze 
@_subject: FYI - Biham/Shamir Differential Fault Analysis of DES, etc. 
Message-Id: <199610181430.QAA20359 at white.wisdom.weizmann.ac.il>
        canetti at theory.lcs.mit.edu, crepeau at iro.umontreal.ca,
        david at digicash.com, daw at cs.berkeley.edu, mab at research.att.com,
        mihir at watson.ibm.com, rogaway at cs.ucdavis.edu, schneier at counterpane.com
Research announcement: A new cryptanalytic attack on DES
Eli Biham                                 Adi Shamir
Computer Science Dept.                    Applied Math Dept.
The Technion                              The Weizmann Institute
Israel                                    Israel
                 October 18, 1996
                     (DRAFT)
In September 96, Boneh Demillo and Lipton from Bellcore announced an
ingenious new type of cryptanalytic attack which received widespread attention (see, e.g., John Markoff's 9/26/96 article in the New York Times). Their full paper had not been published so far, but Bellcore's press release and the authors' FAQ (available at   specifically state that the attack is applicable only to public key cryptosystems such as RSA, and not to secret key algorithms such as the Data Encryption Standard (DES). According to Boneh, "The algorithm that we apply to the device's faulty computations works against the algebraic structure used
in public key cryptography, and another algorithm will have to be devised to work against the nonalgebraic operations that are used in secret key techniques." In particular, the original Bellcore attack is based on specific algebraic properties of modular arithmetic, and cannot handle the complex bit manipulations which underly most secret key algorithms.
In this research announcement, we describe a related attack (which we call Differential Fault Analysis, or DFA), and show that it is applicable to almost any secret key cryptosystem proposed so far in the open literature. In particular, we have actually implemented DFA in the case of DES, and demonstrated that under the same hardware fault model used by the Bellcore researchers, we can extract the full DES key from a sealed tamperproof DES encryptor by analysing fewer than 200 ciphertexts generated from unknown cleartexts.
The power of Differential Fault Analysis is demonstrated by the fact that even if DES is replaced by triple DES (whose 168 bits of key were assumed to make it practically invulnerable), essentially the same attack can break it with essentially the same number of given ciphertexts. We would like to greatfully acknowledge the pioneering contribution
of Boneh Demillo and Lipton, whose ideas were the starting point of
our new attack. In the rest of this research announcement, we provide a short technical
summary of our practical implementation of Differential Fault Analysis of DES. Similar attacks against a large number of other secret key cryptosystems
will be described in the full version of our paper.
TECHNICAL DETAILS OF THE ATTACK
The attack follows the Bellcore fundamental assumption that by exposing a sealed tamperproof device such as a smart card to certain physical effects (e.g., ionizing or microwave radiation), one can induce with reasonable probability a fault at a random bit location in one of the registers at some random intermediate stage in the cryptographic computation. Both the bit location and the round number are unknown to the attacker. We further assume that the attacker is in physical possesion of the tamperproofdevice, so that he can repeat the experiment with
the same cleartext and key but without applying the external
physical effects. As a result, he obtains two ciphertexts derived from
the same (unknown) cleartext and key, where one of the ciphertexts is correct and the other is the result of a computation corrupted by a single bit error during the computation. For the sake of simplicity,
we assume that one bit of the right half of the data in one of the 16 rounds of DES is flipped from 0 to 1 or vice versa, and that both the bit position and the round number are uniformly distributed.
In the first step of the attack we identify the round in which the fault occurred.  This identification is very simple and effective: If the fault occurred in the right half of round 16, then only one bit in the right half of the ciphertext (before the final permutation) differs
between the two ciphertexts. The left half of the ciphertext can
differ only in output bits of the S box (or two S boxes) to which this
single bit enters, and the difference must be related to non-zero
entries in the difference distribution tables of these S boxes.  In
such a case, we can guess the six key bit of each such S box in the
last round, and discard any value which disagree with the expected
differences of these S boxes (e.g., differential cryptanalysis). On
average, about four possible 6-bit values of the key remain for each
active S box.
If the faults occur in round 15, we can gain information on the key
bits entering more than two S boxes in the last round: the difference
of the right half of the ciphertext equals the output difference of
the F function of round 15.  We guess the single bit fault in round
15, and verify whether it can cause the expected output difference,
and also verify whether the difference of the right half of the
ciphertext can cause the expected difference in the output of the F
function in the last round (e.g., the difference of the left half of
the ciphertext XOR the fault).  If successful, we can discard possible
key values in the last round, according to the expected differences.
We can also analyse the faults in the 14'th round in a similar way.
We use counting methods in order to find the key.  In this case, we
count for each S box separately, and increase the counter by one for
any pair which suggest the six-bit key value by at least one of its
possible faults in either the 14'th, 15'th, or 16'th round.
We have implemented this attack on a personal computer.  Our analysis
program found the whole last subkey given less than 200 ciphertexts,
with random single-faults in all the rounds.
This attack finds the last subkey.  Once this subkey is known, we can
proceed in two ways: We can use the fact that this subkey contains 48 out of the 56 key bits in order to guess the missing 8 bits in
all the possible 2^8=256 ways. Alternatively, we can use our knowledge
of the last subkey to peel up the last round (and remove faults that we already identified), and analyse the preceding rounds with the same data using the same attack. This latter approach makes it possible to
attack triple DES (with 168 bit keys), or DES with independent subkeys
(with 768 bit keys).
This attack still works even with more general assumptions on the
fault locations, such as faults inside the function F, or even faults
in the key scheduling algorithm.  We also expect that faults in
round 13 (or even prior to round 13) might be useful for the analysis,
thus reducing the number of required ciphertext for the full analysis.
OTHER VULNERABLE CIPHERS
Differential Fault Analysis can break many additional secret key cryptosystems, including IDEA, RC5 and Feal.  Some ciphers, such as Khufu, Khafre and Blowfish compute their S boxes from the key material.  In such ciphers, it may be even possible to extract the S boxes
themselves, and the keys, using the techniques of Differential Fault
Analysis.  Differential Fault Analysis can also be applied against
stream ciphers, but the implementation might differ by some technical
details from the implementation described above.
------- End of Forwarded Message

@_date: 1996-10-23 15:56:16
@_author: Matt Blaze 
@_subject: Quisquater's improvement on fault exploitation 
Message-Id: <199610232205.AAA28550 at absil.dice.ucl.ac.be>
        mab at research.att.com, mihir at watson.ibm.com, schneier at counterpane.com
here is my small contribution in the field. Your comments are welcome.
Kind regards,

@_date: 1996-10-23 22:03:16
@_author: Matt Blaze 
@_subject: Netescrow & Remailers? 
Someone wrote me a while back asking about using oblivious key escrow
("netescrow") to build a remailer.  I was a bit skeptical of using
exactly the mechanism that I outlined in my paper on the subject,
but I think the idea raises some intersting avenues to look at.
I'll try to dig up the message I sent on the subject.
The paper in question, by the way, can be found at:

@_date: 1996-10-23 22:09:17
@_author: Matt Blaze 
@_subject: Netescrow & Remailers? 
Here were my thoughts on the subject when this was mentioned to me
a month and a half ago.
[by the way, please note that I don't read cypherpunks, so please include
me in any reply that you want me to see.  Thanks.]
------- Forwarded Message
cc: peter.allan at aeat.co.uk, cypherpunks at toad.com
             <199609061322.OAA01150 at server.test.net> [well-thought-out stuff deleted]
This is the first I've seen this proposal to use Oblivious Key Escrow
(OKE) as a store for a remailer database; apologies if all this has
been discussed already (I don't ready cypherpunks very often these
My original idea for OKE was as a way to backup long-term,
slow-changing sensitive data without also introducing a single point
of failure for either security or availability.  The remailer model is
a bit different, and I'm not sure it's a good fit, in particular
because I haven't thought about the various new failure modes in this
application.  But let me think ``out loud.''
Suppose we want to build a persistent-reply address anonymous
messaging service (like the late anon.penet.fi) with the following
        a) The database that maps anonymous addresses to real
           addresses is secure against erasure or other permanent loss
           of availability
        b) The database is also secure against accidental or coerced
           disclosure.
Requirement (a) implies backups and persistent storage.  Requirement
(b) implies that both access to both operational and backup copies
must be carefully controlled, preferably by technical means.  So far,
this looks like a good candidate for distributed security, in the
style of OKE, Mike Reiter's Rampart service, or Ross Anderson's
Eternity service.
Actually, I think the best solution would be for the remailer itself
to be a distributed process, split among enough places to make it
difficult for anyone to attack enough nodes to compromise or recover
the address translation database.  It is not at all obvious how to do
this in practice, however, since any solution would need to combine
secure distributed computation (to calculate the mapping for each
message sent without revealing to any party, including the sender,
what the mapping is) with anonymous networking techniques such as
mixes to prevent traffic analysis from revealing the mapping.  There
are a number of unsolved theoretical and practical problems here, and
I think working out the details of such a system would make for a good
PhD thesis or two (quite seriously, and I'd be interested in talking
with anyone who wanted to pursue such a line of research).
So for now let's limit ourselves to existing tools and techniques, or
at least tools and techniques that are close to existing.  Let's say,
for the moment, that we wanted to base the system on OKE.
Assume an unconditionally trusted remailer operator whose goal is to
construct a system that resists attempts to force him or her to
UNILATERALLY reveal the database.  That is, it should not be possible
to force the remailer operator to reveal the database contents without
also enlisting the aid of the (collectively anonymous) oblivious key
holders.  My (not carefully considered) first thought is that the
address database would be encrypted and stored locally, using a key
that is escrowed using OKE.  The key would never be locally stored;
preferably, the key would exist only in memory.  The operating system
on which the remailer is run would delete this key ``at the drop of a
hat,'' e.g., any time the system was rebooted, any time someone logs
in, any time unusual activity of any sort is detected.  The key
release policy is controlled by a public key, for which the secret key
is stored in a more persistent manner (e.g., in the file system).  Whenever the database key is deleted, the OKE recovery process is used
to recover the key automatically, and the database is re-encrypted
with a new key that is distributed to a new set of shareholders.
Under normal operation, this might happen once a month or so, and
might entail (because of policy-based delays and the time required to
collect shares) a few days of downtime.
Under unusual conditions that might precipitate some kind of coercive
situation, the remailer operator (or some automatic process on the
remailer machine) would delete the signing key as well as the
database.  It might be reasonable, for example, to delete this file
any time someone logs in to the remailer machine (which shouldn't be
needed ordinarily).  The OKE share policy would require that the
shareholder operator examine unsigned key requests manually before
releasing them.  If the keys were deleted because of a false alarm or
machine failure, the remailer operator would send a message saying
something to the effect of ``Hey, I blew it.  Please send me the key
shares once you're convinced no one has a gun to my head.''  In the
event of a public safety emergency, the police are free to attempt to
issue their own appeal for key shares, but the ability to for them to
do this is not a design goal, but rather a side-effect of the design.
I see a number of problems with using OKE for this.  In particular,
key recovery is moderately expensive and key distribution with the
oblivious multicast protocol in my paper can be very expensive.  If
keys are deleted regularly, the downtime could be unacceptable.  I'm
not sure OKE is entirely workable for this application, but perhaps a
more clever design could prove me wrong.
There are a whole bunch of engineering issues here, particularly
related to automatically detecting ``unusual'' situations.
So can this scheme be improved upon?  Is there a better way to run a
persistent-reply-address remailer?  These are interesting, and I think
largely open, questions.
- -matt
NB  The oblivious key escrow paper that I presented at the Information Hiding
workshop at the Isaac Newton Institute in May, is available at:
        ftp://ftp.research.att.com/dist/mab/netescrow.ps
------- End of Forwarded Message

@_date: 1996-10-24 10:21:21
@_author: Matt Blaze 
@_subject: to whoever runs the "best of security list" 
I've noticed that frequently when I send a message to cypherpunks,
someone forwards the message to some list called, I think, "best-of-security".
I have no idea who runs this list or who is forwarding my messages to it,
but the list appears to be badly misconfigured.  Every time one of my
messages gets forwarded, I get literally two or three dozen bounce messages
from undeliverable recipients on that list.  My guess is that the best-
of-security list software incorrectly identifies the original message
author, instead of the list owner, as the originator of messages, and so
errors are sent to the wrong place.
So to whoever is running the best-of-security list: please either
fix your list software or stop forwarding my messages to it.

@_date: 1996-10-31 19:48:10
@_author: Matt Blaze 
@_subject: new center at U. Wisconsin - Milwaukee 
In case anyone is interested.
------- Forwarded Message
Received: from amontillado.research.att.com (amontillado.research.att.com [135.205.42.32]) by nsa.research.att.com (8.7.3/8.7.3) with ESMTP id UAA01185 for ; Thu, 31 Oct 1996 20:17:34 -0500 (EST)
Received: from research.att.com (research.research.att.com [135.205.32.20]) by amontillado.research.att.com (8.7.5/8.7) with SMTP id UAA07551; Thu, 31 Oct 1996 20:17:02 -0500 (EST)
Received: from miller.cs.uwm.edu by research; Thu Oct 31 20:14:30 EST 1996
Received: (from cccns at localhost) by miller.cs.uwm.edu (8.8.2/8.8.2) id SAA12464; Thu, 31 Oct 1996 18:17:54 -0600 (CST)
Message-Id: <199611010017.SAA12464 at miller.cs.uwm.edu>
        OOO     PPPP    EEEEE   N     N   IIIIIII   N     N      GGGGG        O   O    P   P   E       NN    N      I      NN    N     G     G
      O     O   P   P   E       N N   N      I      N N   N    G             O     O   PPPP    EEE     N  N  N      I      N  N  N    G             O     O   P       E       N   N N      I      N   N N    G    GGG
       O   O    P       E       N    NN      I      N    NN     G     G
        OOO     P       EEEEE   N     N   IIIIIII   N     N      GGGGG                                     OF THE
            CENTER FOR CRYPTOGRAPHY, COMPUTER AND NETWORK SECURITY
                 Electrical Engineering and Computer Science
                  College of Engineering and Applied Science
                     University of Wisconsin - Milwaukee
                              November 21, 1996
            SCHEDULE:
               10:45 Welcome                                                           11:00 Friction in Secret Sharing and Key Escrowing                              by G. R. BLAKLEY, Texas A&M University.                                    co-inventor of secret sharing                                 11:45 Break                                                             13:45 The State of the Art in Computational Number Theory
                       by ERIC BACH, University of Wisconsin, Madison.                            co-author of "Algorithmic Number Theory, Vol. 1"
               14:30 Coffee                                                            15:15 Opening ceremony and introduction of keynote speaker.                       15:30 The Importance of Cryptography to Industry                                by WHITFIELD DIFFIE, Sun Microsystems
                          co-inventor of public key
               16:30 Reception                                         Related event: Friday November 22, Computer Science Seminar Series Lecture.
               15:30 Robust and Efficient Sharing of RSA Functions
                       by TAL RABIN, IBM T.J. Watson Research Center
You are cordially invited to attend the opening. For upcoming information,
such as the vitae of the speakers, the location of the event, the Center's
faculty members, driving instructions, consult: Free registration requested. Please register by e-mail to cccns at cs.uwm.edu
Indicate whether you also plan to attend the lecture of Tal Rabin on Friday
November 22. Include your name and address.
          George Davida           Yvo Desmedt           Rene Peralta
          Professor               Professor             Associate Professor
                                  Director
The Center's purpose will be to advance knowledge in the areas of cryptography,
computer security, network security and related areas from theoretical as well
as practical viewpoints. For more details see: ------- End of Forwarded Message

@_date: 1996-09-07 10:24:49
@_author: Matt Blaze 
@_subject: Job for netescrow ? (was Secure anonymouse server protocol... 
[well-thought-out stuff deleted]
This is the first I've seen this proposal to use Oblivious Key Escrow
(OKE) as a store for a remailer database; apologies if all this has
been discussed already (I don't ready cypherpunks very often these
My original idea for OKE was as a way to backup long-term,
slow-changing sensitive data without also introducing a single point
of failure for either security or availability.  The remailer model is
a bit different, and I'm not sure it's a good fit, in particular
because I haven't thought about the various new failure modes in this
application.  But let me think ``out loud.''
Suppose we want to build a persistent-reply address anonymous
messaging service (like the late anon.penet.fi) with the following
        a) The database that maps anonymous addresses to real
           addresses is secure against erasure or other permanent loss
           of availability
        b) The database is also secure against accidental or coerced
           disclosure.
Requirement (a) implies backups and persistent storage.  Requirement
(b) implies that both access to both operational and backup copies
must be carefully controlled, preferably by technical means.  So far,
this looks like a good candidate for distributed security, in the
style of OKE, Mike Reiter's Rampart service, or Ross Anderson's
Eternity service.
Actually, I think the best solution would be for the remailer itself
to be a distributed process, split among enough places to make it
difficult for anyone to attack enough nodes to compromise or recover
the address translation database.  It is not at all obvious how to do
this in practice, however, since any solution would need to combine
secure distributed computation (to calculate the mapping for each
message sent without revealing to any party, including the sender,
what the mapping is) with anonymous networking techniques such as
mixes to prevent traffic analysis from revealing the mapping.  There
are a number of unsolved theoretical and practical problems here, and
I think working out the details of such a system would make for a good
PhD thesis or two (quite seriously, and I'd be interested in talking
with anyone who wanted to pursue such a line of research).
So for now let's limit ourselves to existing tools and techniques, or
at least tools and techniques that are close to existing.  Let's say,
for the moment, that we wanted to base the system on OKE.
Assume an unconditionally trusted remailer operator whose goal is to
construct a system that resists attempts to force him or her to
UNILATERALLY reveal the database.  That is, it should not be possible
to force the remailer operator to reveal the database contents without
also enlisting the aid of the (collectively anonymous) oblivious key
holders.  My (not carefully considered) first thought is that the
address database would be encrypted and stored locally, using a key
that is escrowed using OKE.  The key would never be locally stored;
preferably, the key would exist only in memory.  The operating system
on which the remailer is run would delete this key ``at the drop of a
hat,'' e.g., any time the system was rebooted, any time someone logs
in, any time unusual activity of any sort is detected.  The key
release policy is controlled by a public key, for which the secret key
is stored in a more persistent manner (e.g., in the file system).  Whenever the database key is deleted, the OKE recovery process is used
to recover the key automatically, and the database is re-encrypted
with a new key that is distributed to a new set of shareholders.
Under normal operation, this might happen once a month or so, and
might entail (because of policy-based delays and the time required to
collect shares) a few days of downtime.
Under unusual conditions that might precipitate some kind of coercive
situation, the remailer operator (or some automatic process on the
remailer machine) would delete the signing key as well as the
database.  It might be reasonable, for example, to delete this file
any time someone logs in to the remailer machine (which shouldn't be
needed ordinarily).  The OKE share policy would require that the
shareholder operator examine unsigned key requests manually before
releasing them.  If the keys were deleted because of a false alarm or
machine failure, the remailer operator would send a message saying
something to the effect of ``Hey, I blew it.  Please send me the key
shares once you're convinced no one has a gun to my head.''  In the
event of a public safety emergency, the police are free to attempt to
issue their own appeal for key shares, but the ability to for them to
do this is not a design goal, but rather a side-effect of the design.
I see a number of problems with using OKE for this.  In particular,
key recovery is moderately expensive and key distribution with the
oblivious multicast protocol in my paper can be very expensive.  If
keys are deleted regularly, the downtime could be unacceptable.  I'm
not sure OKE is entirely workable for this application, but perhaps a
more clever design could prove me wrong.
There are a whole bunch of engineering issues here, particularly
related to automatically detecting ``unusual'' situations.
So can this scheme be improved upon?  Is there a better way to run a
persistent-reply-address remailer?  These are interesting, and I think
largely open, questions.
NB  The oblivious key escrow paper that I presented at the Information Hiding
workshop at the Isaac Newton Institute in May, is available at:
        ftp://ftp.research.att.com/dist/mab/netescrow.ps

@_date: 1996-09-18 06:18:32
@_author: Matt Blaze 
@_subject: DIMACS Trust management workshop, Sept 30 - Oct 2. 
Received: from amontillado.research.att.com (amontillado.research.att.com [135.205.42.32]) by nsa.research.att.com (8.7.3/8.7.3) with ESMTP id LAA24206 for ; Tue, 17 Sep 1996 11:52:51 -0400 (EDT)
Received: from research.research.att.com (research.research.att.com [135.205.32.20]) by amontillado.research.att.com (8.7.5/8.7) with SMTP id LAA14378 for ; Tue, 17 Sep 1996 11:52:30 -0400 (EDT)
Received: from ns.research.att.com by research; Tue Sep 17 11:48:17 EDT 1996
Received: from henson.rutgers.edu by ns; Tue Sep 17 11:06:01 EDT 1996
Received: (from bquigley at localhost) by henson.rutgers.edu (8.6.12+bestmx+oldruq+newsunq+grosshack/8.6.12) id LAA26298; Tue, 17 Sep 1996 11:02:41 -0400
Message-Id: <199609171502.LAA26298 at henson.rutgers.edu>
        dimacs-dimacs at dimacs.rutgers.edu,
        dimacs-current-postdocs-industry at dimacs.rutgers.edu,
        dimacs-current-postdocs-univ at dimacs.rutgers.edu,
        dimacs-current-visitors at dimacs.rutgers.edu,
        rutgers-list at dimacs.rutgers.edu, theorynt at vm1.nodak.edu,
        finite-model-theory at informatik.rwth-aachen.de
DIMACS Workshop on Trust Management in Networks
September 30 - October 2, 1996
DIMACS, Rutgers University

@_date: 1996-09-19 15:12:45
@_author: Matt Blaze 
@_subject: cfs users group dead? 
Yes, it seems to be.  It runs on a machine  that is no longer in
my office, and may have gotten mis-configured when the AT&T breakup
happened.  I'll check it out (but not for at least a week; unfortunately
I'll be out of the office 'till then).

@_date: 1998-12-03 01:37:52
@_author: Matt Blaze 
@_subject: Matt Blaze arrested??? 
I'm afraid you're confused.  Whoever this report was about, it
wasn't me:
* I'm not at Lucent Bell Labs. (I'm at AT&T Shannon Labs.)  I'm not
  aware of any other Blaze at either AT&T or Lucent, however.
* I'm not a mathematician. (I'm a computer scientist.)
* I've not been to ORD, or any other midwestern airport,
  since August.  (I do fly a lot, though).
* I never been arrested.  (I did accently set off the antitheft
  detector at a local bookstore once, but they let me go
  when I showed them my reciept).

@_date: 2001-08-21 16:13:13
@_author: Matt Blaze 
@_subject: CFP: Financial Cryptography '02 
Call for Papers
                         Financial Cryptography '02
                              March 11-14, 2002
                            Sonesta Beach Resort
                            Southhampton, Bermuda
      Sponsored by the International Financial Cryptography Association
Original papers are solicited on all aspects of financial data security and
digital commerce for submission to the Sixth Annual Conference on Financial
Cryptography (FC '02). FC '02 brings together researchers in the financial,
legal, cryptologic, and data security fields to foster cooperation and
exchange of ideas. Relevant topics include:
             Anonymity                Infrastructure Design
             Audit                    Legal and Regulatory
             Authentication and       Issues
             Identification           Loyalty Mechanisms
             Certification and        Peer-to-Peer Systems
             Authorization            Payments and
             Commercial Transactions  Micropayments
             and Contracts            Privacy
             Digital Cash             Risks Management
             Digital Rights           Secure Banking
             Management               Smart Cards
             Electronic Purses        Trust Management
             Implementation Issues    Watermarking
             Information Economics
Instructions for Authors: Complete papers (or complete extended abstracts)
must be at most fifteen (15) single-spaced standard pages in length and must
be received before 23h59 UTC on November 4, 2001. All papers must be
submitted electronically. (In exceptional circumstances, paper submissions
can be accepted, but special arrangements must be made with the program
chair prior to October 31, 2001.) Papers must be in either standard
PostScript or PDF format, and should be submitted via electronic mail to
fc02submit at crypto.com prior to the deadline. Note that submissions in
formats other than PostScript or PDF, including word processor source
formats such as MS Word or LaTeX, will be rejected.
Submitted papers should include on the first page the title, all authors and
their affiliations, a brief abstract, and a list of topical keywords. Papers
must be original; submission of previously published material or papers
under consideration in other conferences or journals is not permitted.
Proposals for panels are also solicited, and should include a brief
description of the panel as well as prospective participants. Panel
proposals should be submitted by electronic mail to the same address, in
plain ASCII format.
Important Dates:
Submissions due: November 4, 2001
Notifications to authors: December 23, 2001
Camera-ready papers due: February 4, 2002
General Chair:
Nicko van Someren (nCipher)
Program Committee:
Matt Blaze, Program Chair (AT&T Labs)
Dan Boneh (Stanford University)
Stefan Brands (Zero Knowledge)
Dan Geer (
Ian Goldberg (Zero Knowledge)
Angelos Keromytis (Columbia University)
Paul Kocher (Cryptography Research)
Ron Rivest (MIT)
Tomas Sander (Intertrust)
Rebecca Wright (AT&T Labs)

@_date: 2002-11-07 13:50:43
@_author: Matt Blaze 
@_subject: Did you *really* zeroize that key?  
2002 10:13:52 PST."
  <3.0.5.32.20021107101352.0083fa60 And, of course, the very act of putting in the check could cause a compiler
to not optimize out the zeroize code.  (Writing a proper test program for
such behavior is very difficult).
Like most programming language discussions, it's hard to tell whether the
arguments support writing critical code languages that abstract at a
higher level or a lower level.

@_date: 2003-03-02 19:06:41
@_author: Matt Blaze 
@_subject: Roger Needham Died - from The Register  
of "Sun, 02 Mar 2003 12:29:22 PST." <5.1.1.6.2.20030302122746.02cf5dc0 Sad, sad news.
Roger's pioneering contributions to our art speak (volumes) for
themselves, and our field is diminished by the loss of his future
But I will miss him most for his enormous generosity, his sharp wit,
and his personal integrity.
