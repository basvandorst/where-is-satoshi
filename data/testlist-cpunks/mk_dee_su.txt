
@_date: 2012-08-07 18:02:28
@_author: Maxim Kammerer 
@_subject: [liberationtech] What I've learned from Cryptocat 
Are you equating passive attacks with active attacks? If I understand
how CryptoCat works correctly, it is resistant against passive
interception attacks, whereas Google Chat stores cleartext on Google
servers, which are easily accessible to law enforcement. Active
attacks against SSL can be mitigated by pinning CryptoCat
certificates, so you are left with what, compromise of server
infrastructure? That requires LE jurisdiction where the servers are
located, domain expertise, and dealing with the risk that the
compromise is detected. All that vs. Google servers, which, if I
remember right, provide a friendly interface to user accounts once
served with a simple wiretapping order (and as has been already
mentioned, Google is a multinational corporation, subject to a
multitude of jurisdictions, and is known to bend over for whoever is
in charge).

@_date: 2012-08-08 09:07:49
@_author: Maxim Kammerer 
@_subject: [tor-talk] Tor as ecommerce platform 
I doubt that the Tor Project will ever acknowledge such support. Tor
developers (or at least policy people) like to pretend that Tor is
used for purposes that they consider morally right, and ignore the
uses that are morally wrong [1]. I believe it's an american thing b
one typical tell-tale sign is treating criminals as some masterminds
who can already easily achieve what they want [2] (americans are used
to doing the same in gun control debates), and ignoring the fact that
the project facilitates anonymity for criminals just as well, if not
more, as for non-criminals. Personally, I don't understand what's the
big deal about the make-believe game, and it probably detracts from
the project's credibility, but it's fun to watch nevertheless.
I actually intend to write a Tor server patch to be able to gather
.onion access statistics on relays, once I am sufficiently bored, just
for the fun cognitive dissonance potential (expecting drugs and CP
forums to top the list).
[1] [2] [3]

@_date: 2012-08-20 10:33:29
@_author: Maxim Kammerer 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
Hello gentlemen,
Here and there I see references to bglobalb or bstate-levelb powerful
adversaries when it comes to end-to-end traffic correlation b i.e.,
it's supposed to be very hard. Because Tor network has many nodes,
there are guard nodes, there is research, blog posts, CIA funding
(well, not anymore, but similar funding from EU is in the works),
useless bureaucracy, college kids playing in serious development, yada
yada b you know the drill.
Anyway, let's do some math. Below, you will find a table where left
column denotes the number of Guard+Exit+Fast+Stable Tor relays one
needs to sniff at Class-C level, and right column denotes the
probability that a given circuit will go through both intercepted
entry and exit nodes. This is slightly imprecise, because same node
can't be both entry and exit for a circuit, and there are other
ignored intricacies (e.g., port policies) that push the estimates in
the other direction b the reason is that I am better with writing
quick scripts [1] than with Excel. The consensus taken for analysis is
from a few hours ago, and I read Tor server code from current stable
version in Gentoo (0.2.2.35) b this probably doesn't matter.
10 11.50%
11 14.56%
12 16.52%
13 16.80%
14 17.69%
15 17.98%
16 18.90%
17 19.20%
18 19.50%
19 20.46%
20 20.46%
21 21.76%
22 22.77%
23 23.43%
24 23.43%
25 24.48%
26 24.48%
27 24.82%
28 25.55%
29 25.90%
30 25.90%
As you can see, sniffing just 25 Class-C networks (or 42 individual
nodes) lets an adversary correlate ~25% of (non-.onion) circuits.
Which networks are these?
DE 31.172.30.[1-4]
GB 146.185.23.179
NL 77.247.181.{162,164}
RO 109.163.233.{200,201,205}
CA 198.96.155.3
US 199.48.147.{35,36,37,38,39,40,41}
DE 212.84.206.250
FR 178.32.211.{130,140}
US 204.8.156.142
US 173.254.216.[66-69]
SE 78.108.63.44
US 96.44.189.102
GB 178.33.169.35
CZ 212.79.110.28
US 66.180.193.219
DE 88.198.100.{230,233}
LU 212.117.180.65
SE 81.170.186.175
CH 62.220.135.129
SE 84.55.117.251
DE 85.31.187.132
CA 8.18.172.156
FR 213.251.185.74
US 69.42.212.2
FR 37.59.82.50
All of these servers are in US/CA or EU jurisdiction, so even an
unsophisticated LE operation can issue ~20 wiretapping orders at ISP
level (many of these networks are operated by same hosting providers),
and immediately deanonymize ~25% of Tor traffic. So far for anonymity!
Oh, and if you are just into looking what sites Tor users visit, the
situation is even better b intercepting the same 25 Class-C networks
will let you see 72% of the traffic. Picking better non-Guard Exits
will improve this figure to 78%. That's right b 4/5th of Tor traffic
exits through just 25 LANs.
[1]

@_date: 2012-08-22 00:52:04
@_author: Maxim Kammerer 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
Bingo b in the first graph in the ticket [1] you see that the
probability gets to ~80% when the number of nodes gets to 40. What
this graph doesn't show, however, is that many of these nodes are
attached to the same switch, or even run on same machine in different
VMs, or on different IPs (some even run on same IP). After accounting
for that, the number of tiny networks (at least /28, from going over
the list in the original message) and nodes one needs to intercept in
order to get the same 80% figure gets down to ~25. And of course, if
one is smarter about choosing which nodes to intercept, the
probability of seeing both entry and exit traffic is also significant:
~25%. It is also possible to do something in between (choose some
Guard-only and Exit-only nodes), but my laziness kicked in at that
point of analysis.
Here are the network again, in case anyone has the resources and is
curious enough about who does what with Tor:
DE 31.172.30.[1-4]
GB 146.185.23.179
NL 77.247.181.{162,164}
RO 109.163.233.{200-201,205}
CA 198.96.155.3
US 199.48.147.[35-41]
DE 212.84.206.250
FR 178.32.211.{130,140}
US 204.8.156.142
US 173.254.216.[66-69]
SE 78.108.63.44
US 96.44.189.102
GB 178.33.169.35
CZ 212.79.110.28
US 66.180.193.219
DE 88.198.100.{230,233}
LU 212.117.180.65
SE 81.170.186.175
CH 62.220.135.129
SE 84.55.117.251
DE 85.31.187.132
CA 8.18.172.156
FR 213.251.185.74
US 69.42.212.2
FR 37.59.82.50
[1]

@_date: 2012-08-22 04:42:35
@_author: Maxim Kammerer 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
Manually, using WHOIS and traceroute. This can be done automatically
using GeoIP, but I wanted to be sure in the results (also visited some
hosting sites), and writing a proper program would deviate too much
from the initially intended bquick hackb design.
That's why I insist that everyone should be a relay by default, even
if there are some theoretical issues that weren't worked out yet [1].
Making everyone a relay also results in a healthier users community (I
think I2P is one), and more intrinsic network growth.
Quoting [2] (referenced by fakefake): bTor has been long suspected,
and later confirmed [11,12], to be vulnerable to an attacker who could
observe both the entry and exit point of a connection through an
anonymity network.b
Well, it of course depends on what one calls bunsophisticatedb. E.g.,
if one judges by IACIS email dump [3], then most investigators hardly
understand what they are doing when it comes to unfamiliar
technologies (like Tor). So maybe you need them to be bsophisticatedb,
after all, but my point was that you don't need something exceptional
like involving state security agencies b i.e., FBI + UK Police + DE
Police + a couple of other countries, coordinating via Interpol does
not sound impossible to me. I will also expand on that in a reply to
[1] [2] [3]

@_date: 2012-08-22 09:48:19
@_author: Maxim Kammerer 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
I don't think that buying the software would be that difficult. For a
big project, LE could outsource it to one of those shady companies
selling exploits, or (more likely) to a government contractor with
security clearance. For something smaller, a hungry grad student
should do, after making them sign an NDA, or, in case of a really
arrogant LE, some national secrecy act. Writing the service as
something innocent in accounting is probably par for the course.
Closer to the topic, I think that traffic correlation can be performed
in a distributed fashion, if you know the target IPs to watch for
(which can be gathered beforehand locally on exit nodes, and
aggregated and analyzed afterwards). Exit nodes that see packets
to/from target hosts aggregate their exact timestamps for a few
seconds, and then send the chunks to all other nodes (so yes, you
can't correlate too much traffic). All other (guard) nodes then try to
locally correlate the received packets with their own traffic, and
aggregate successes for later reports. In this fashion, each node
needs to keep perhaps a minute of timestamped traffic. It is also
possible to play with traffic / disk space / success probability
tradeoffs: send chunks to rotating sets of nodes, increase recorded
traffic window (to be able to send old chunks to nodes that didn't see
traffic to a given IP yet), etc.
Maxim Kammerer
Liberti Linux: tor-talk mailing list
tor-talk at lists.torproject.org
Eugen* Leitl leitl ICBM: 48.07100, 11.36820  8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE

@_date: 2012-08-25 02:51:29
@_author: Maxim Kammerer 
@_subject: [tor-talk] End-to-end correlation for fun and profit 
Wrt. the first proof, it seems to me that the assumed correlation
accuracy rate of 99.9% is incredibly low, and I think that the Raccoon
recognized that by referring to sampling and retention at the end of
his post. With the targeted attack that's similar to bExample 3b in
Raccoon's post that I described in my previous comment here, where one
analyzes all exit traffic without missing packets, I would expect the
correlation accuracy (and as a result, match confidence) to
exponentially approach 100% very quickly with the number of relevant
packets seen, and extremely quickly if the traffic is interactive
(i.e., browsing).
Actually, c/n of 30% in bExample 3b is close to the 25% that's
discussed in the OP here, so let's redo the example with c/n=25% and
different correlation accuracies (leaving the other numbers intact):
(using bbc -lb)
ca  = 0.999
pm  = (1/5000) * (0.25)^2
ca*pm / (pm*ca + (1-pm)*(1-ca))
ca  = 0.999
ca = 0.9999
ca = 0.99999
ca = 0.999999
ca = 0.9999999
ca = 0.99999999
ca = 0.999999999
So reducing correlation accuracy error to 10^-9 will give you 99.99%
confidence in end-to-end correlation match. I suspect that a few
seconds of interactive traffic will give you a correlation accuracy
that's much better than a 10^-9 error.

@_date: 2012-12-09 06:09:31
@_author: Maxim Kammerer 
@_subject: [tor-talk] Botnets through Tor 
I was going to write that for I2P it is highly unlikely due to
autonomous daemon configuration complexity, a dependency on Java, and
unreliability wrt. network configuration changes, but here is a botnet
advertisement that mentions I2P support:
Maxim Kammerer
Liberti Linux: tor-talk mailing list
tor-talk at lists.torproject.org

@_date: 2012-12-19 18:36:43
@_author: Maxim Kammerer 
@_subject: [liberationtech] Quantum computation & communication 
I am yet to see evidence that quantum computing is viable for any
non-trivial number of qubits. I think it is more likely that we will
see the idealized notion of quantum superposition break once QC is
pushed far enough, resulting in physics, but not computation
breakthrough, and in ability to still use finite fields-based
cryptography, just with bigger key lengths. Also, as pointed by Matt
Mackall above, there is a frequent misconception that quantum
computers are anything like non-deterministic Turing machines b they
are not, and shuffling-based (i.e., symmetric-key, classical)
cryptography is still resistant to QC, assuming it's actually
resistant to classic computing as well (which is generally seen as a
much stronger assumption than, e.g., assuming that factoring is hard).
Caveat emptor: not my field, inb4 hate from QC people.

@_date: 2012-01-02 19:40:52
@_author: Maxim Kammerer 
@_subject: [tor-talk] Postfix + Tor (Was: remailers) 
You don't need mail servers for such an infrastructure over networks
like Tor and I2P, where nodes can have persistent addresses b see
cables communication in LibertC) Linux, which sends messages directly
between hidden services / eepsites:

@_date: 2012-01-03 23:14:56
@_author: Maxim Kammerer 
@_subject: [tor-talk] Real basic questions for linux 
Tor is an infrastructure with a history of being developed and
researched by experts in network security and anonymity. Due to its
unique standing in the congregation of similar networks, Tor attracted
a fair amount of academic scrutiny, which resulted in many refinements
and extensions for resisting various types of attacks, and in
documenting its known weaknesses. While I dislike the excessive
stiffness of the project (RSA-1024? The 90's called, and they want
their bmilitary-grade encryptionb back) and inability to resist the
status quo (what again is the reason for not making nodes relay
traffic by default? beyond you publishing another conference paper on
the oh-so-terrible dangers of that in order to pad your CV, that is),
I trust the Tor project to produce something solid and to not grossly
overestimate its security and anonymity guarantees.
Tor Browser, on he other hand, is just some hack for mangling browser
headers. Who develops it? Who scrutinized it? Were it the same people
who recommended using the unstable and inadequate Polipo proxy
(over-64 MiB files? Nah, never heard of those) instead of
well-established Privoxy previously, due to some imaginary perceived
limitations of the latter (reading manuals is boring, I guess)? Is
there any evidence that Tor Browser prevents fingerprinting which is
marginally more sophisticated than looking at some subset of browser
headers regardless of their order? Or is the joke that is Panopticlick
with its bbits of identifying informationb as state-of-the-art as it
gets (mind you, I was able to fool it with Privoxy rules in LibertC),
masquerading as TBB, although I know that the specific browser in use
can still be fingerprinted differently)? Who cares b let's ship this
junk in a bundle, and convince everyone of its utmost necessity.
inb4: Yeah, well, that's just, like, your opinion, man.

@_date: 2012-07-06 14:37:51
@_author: Maxim Kammerer 
@_subject: [tor-talk] Transparent e-mail encryption? 
If you do not require interoperability with SMTP, cables communication
[1] does what you require b encryption and authentication are
transparent, and server issues can be ignored, since there are no
servers. Cables communication also has delivery verification and other
features. Note that PGP / S/MIME-type encryption is undesirable for
most users, since it ties authentication to non-repudiability [2].
[1] [2]

@_date: 2012-03-07 01:18:01
@_author: Maxim Kammerer 
@_subject: [tor-talk] Tor and HTTPS graphic 
While this may be true in the theoretical sense, it doesn't mean that
one can't make correlation attacks less practical. I find it hard to
believe that right now NSA, for instance, has Tor traffic analysis
tightly integrated into its worldwide communications sniffing
framework, simply because it's too much of a logistic problem, and
anonymous networks are unlikely to be sufficiently high-profile
targets so as to warrant expending the resources to deal with the
logistics (yet). But I think that it is entirely believable that NSA
has a dedicated project (even if only for research purposes) where the
traffic from all known relays (a relatively stable pool of ~3000
nodes?) is sniffed and analyzed b that would be relatively simple to
setup and maintain, given the unlimited interception capabilities. And
you can combat the latter b by extending and popularizing the entry
bridges concept, implementing exit bridges, making all clients relays
by default (even if that won't contribute significant bandwidth), etc.

@_date: 2012-03-07 16:43:03
@_author: Maxim Kammerer 
@_subject: [tor-talk] Tor and HTTPS graphic 
I think that's quite unlikely. The nodes must reside in commercial
data centers and run untrusted software (including necessarily
modified Tor clients), all of which exposes them to hacking risks and
to the resulting possibility of discovering the interception framework
employed (which is probably not unique to Tor, so that's a huge risk).
But one could try correlating Tor relays and Tor clients growth graphs
since, say, 2000 b if at some point there was a sharp growth in
USA-located relays without a corresponding growth in total clients,
and if those relays have similar bandwidth / data center quality
capabilities, then that could be "The Man".

@_date: 2012-03-30 04:52:40
@_author: Maxim Kammerer 
@_subject: [tor-talk] Choosing a name for a .onon 
Due to proliferation of Bitcoin, there are now very efficient SHA-256
generators for off-the-shelf GPUs. The numbers at [1] suggest
performance that's at least two orders of magnitude faster than your
laptop b and for double-SHA-256 instead of a single SHA-1 (which I
assume can be done by the same software after some simple adaptation).
[1] Not necessarily b you can generate the hash first, and then check
whether the public key is legal. I.e., generate a 512-bit prime p, and
then go on with producing a completely random 512-bit e, and checking
whether SHA-1(ASN.1-RSAPublicKey(modulus=p*e, exponent=65537)) (which
is how Tor computes the .onion address) produces the desired result.
If it does, check whether e is prime. Density of primes in the range
of e is ~1/512, so that's just 9 bits more of search space, and
primality checking efficiency doesn't matter much.

@_date: 2012-11-05 23:51:00
@_author: Maxim Kammerer 
@_subject: [liberationtech] Bitcoin and The Public Function of Money 
I am not sure the situation is that simple. I believe (this is a long
thread) that the underlying question discussed here is whether Bitcoin
has a potential as a viable alternative unregulated currency. In order
to answer that, one has to first answer the question of what Bitcoin
is backed by. There are lots of demagogical claims on Bitcoin-related
discussion venues, but ultimately gold is backed by its scarcity,
longevity, and subcortex appeal of shiny things; fiat currency is
backed by state-protected ability to pay debt; MMORPG currencies are
exchangeable for game items; and Bitcoin, if one discards the time
period when it was worthless, is backed by its capacity to be
exchanged for illicit drugs on international black markets, due to
lack of better alternatives. *If* black markets are Bitcoin's claim to
fame, then it has absolutely no chance of becoming anything but a
small-scale under-the-radar drug trade currency, the reason being that
a thriving black market is an early sign of stagnating economy, chaos,
anarchy, and civil war, in which case people will revert to more
tangible currency alternatives like jewelry and food. There is a
widespread opinion that one of the major *economic* reasons for the
fall of Soviet Union was producers of goods (i.e., factories,
collective farms, etc.) establishing a massive black market between
them, avoiding the inflexible system of planned economy.
And this is where economic ideology and religious faith become
relevant, because according to libertarian views (which are at the
core of faith in Bitcoin) post-USSR republics, with their
well-educated population that was highly receptive to capitalistic
ideals, abundance of resources (natural and factories), extremely weak
governments, huge territory, etc. etc., should have somehow formed a
libertarian utopia with people-supported militias, thriving free
markets, and whatnot. What happened, however, was theft of all
available resources, rise of oligarchs who took control of factories,
dismantled them and sold the components abroad, organized banditism,
pervasive racketeering, demographical catastrophe, several civil wars
and population transfers, shortened life expectancy, and all other
complete opposites of an utopia. Contradiction? Only if one doesn't
use religious faith to reason about economic reality.

@_date: 2012-10-03 05:41:03
@_author: Maxim Kammerer 
@_subject: [liberationtech] Security / reliability of cryptoheaven ? 
Why? The notion that easy encrypted email is hard is a myth, perhaps
resulting from people being trapped inside the concept of using PGP
and its non-scalable bweb of trustb. LibertC) Linux implements cables
communication [1], which provides just that b easy encrypted email.
The catch is that there is no interoperability with SMTP, and there
are no easy-to-remember usernames.
CryptoHeaven's servers can't trivially MITM retrieval of recipient's
public key:
bThe public portion of the key is then sent to the server where it can
be picked up by others connecting to the system.b
bCryptoHeaven manages public keys automatically and securely. User
simply allows others to communicate with him through the use of
"Contacts" within the CryptoHeaven system. The system takes care of
the exchange of the public keys automatically.b
The underlying problem is that the username (apparently) does not
include a hash of the public key. It is possible that user ID
mentioned in bHow can I verify that I am sending messages to whom I
think I am?b entry in the FAQ is such a hash, but it is not clear from
the brief description.
It is also not clear whether the server can decide to make a message
disappear b i.e., are there mandatory authenticated receipts?
And of course, due to the centralized nature of the system,
CryptoHeaven can perform traffic analysis, building social networks of
correspondents, etc.
I am also not sure why they mention bnon-repudiation and anonymityb in
the FAQ. Non-repudiation is seen as problematic in encrypted
communications nowadays (together with lack of perfect forward
secrecy, which seems to be an attribute of the protocol as well), and
is differentiated from communication authenticity per se (e.g., see
OTR [4]). The claim of anonymity looks like an overstatement.
All of the above is written based on high-level descriptions on
CryptoHeaven website b I didn't look at the code (which is available
[5]), so some points could be incorrect.
[1] [2] [3] [4] [5]

@_date: 2012-10-09 23:53:46
@_author: Maxim Kammerer 
@_subject: [liberationtech] best practices - roundup 
For anyone interested: Liberti Linux already has full UEFI support for
all installation types (USB, CD, OVF), and is also the first Linux
distribution to use Secure Boot as a trusted boot chain mechanism.

@_date: 2012-10-10 00:55:06
@_author: Maxim Kammerer 
@_subject: [liberationtech] best practices - roundup 
Thanks, getting it to work was a real pain. PAX / grsecurity kernel
patches had UEFI-related bugs, and the most suitable UEFI signing tool
(sbsigntool) lacked support for 32-bit EFI binaries. All of this is
now fixed / integrated upstream (sbsigntool is used in Ubuntu, by the
LibertC) ships its own Secure Boot certificate, which signs the GRUB
bootloader, and the trusted chain continues from there. After
experimenting with Secure Boot in OVMF builds, I think that enrolling
such a certificate is not difficult b it is not more difficult than
changing the order of boot devices in BIOS, for instance (back then
before a menu could be invoked by pressing a key). Most controversy
about Secure Boot support in Linux one finds online is about making
the process completely transparent for users, which requires either
using Microsoft-signed binaries (Fedora) / intermediate certificate,
or embedding one's keys in firmware (Ubuntu). If you forgo the
requirement of complete boot transparency, which I think is reasonable
for a special-purpose live distribution, using an own certificate is
an obvious choice.

@_date: 2013-08-15 15:38:56
@_author: Maxim Kammerer 
@_subject: [liberationtech] Google confirms critical Android crypto flaw 
Unbelievable… It seems that PRNG implementers suffer from NIH
syndrome. If you are going to use /dev/urandom, then use it all the
time, and rely on code that's reviewed and maintained by thousands of
kernel people, not just your favorite buggy seeded PRNG du-jour. And
even sans the bugs, consider something like the following in Apache
Harmony (precursor of Dalvik's class library) [1, p. 131]:
  iv = sha1(iv,concat(state, cnt));
  cnt = cnt + 1;
  return iv;
So they're essentially constructing a state-based bit stream that
varies in each block, and hash it with SHA-1 — exposing each
intermediate hash value in the middle. Who the hell told them it's
safe from cryptanalysis POV? E.g., SP800-90A's Hash_DRBG [2, p. 40]
resembles nothing of the sort.
[1] [2]

@_date: 2013-08-22 00:27:21
@_author: Maxim Kammerer 
@_subject: [liberationtech] Bradley Manning's sentence: 35 years for exposing us to the truth 
Col. Morris Davis: “Military has detailed regs on confinement credits
& parole eligibility. My best est is he'll do about 8-9 yrs, out by
age 33-34.”
If true, a pretty fitting sentence, I think, for indiscriminately
publishing huge amount of classified information that potentially
endangered many people, and considering that USA has unusually harsh
sentences for a developed country.
An interesting comment on Reddit, of all places:
“Significant amounts of foreign service agent names were released.
These are civilians working for their government in some official
capacity (think spies, except not all of them are cloak and dagger
types). These were people stationed in hostile countries (Pakistan, SE
Asia, Middle East, Africa) and if their cover had been blown while in
country they could have been sought out.
Luckily, as I understand it most of the people that were exposed were
notified by their handlers in advance (basically as soon as word go
out that diplomatic cables had been compromised) and were extracted. A
friend of mine works in a field that draws a lot of foreign service
agents to it due to the nature of the work, and they were camped out
in northern Pakistan with her crew. She woke up one morning (the
morning after the diplomatic cables were released) and half her crew
was gone. They got word in the middle of the night and left. They
couldn't even tell the people they were with why they were gone, and I
imagine it was quite unsettling to be there and be missing people all
of the sudden.”

@_date: 2013-08-23 00:43:11
@_author: Maxim Kammerer 
@_subject: [liberationtech] Open Whisper Systems' neat asynch FPS "pre-keying" 
Not sure if I understand all iOS-related issues described, but this
seems like overcoming engineering problems with a synchronous protocol
like OTR on iOS at the expense of exposing the clients to a DOS attack
of exhausting the prekeys.
However, an asynchronous protocol does not mean that all information
must be delivered in one push. In cables communication [1], I chose
simple asynchronous messages because I don't trust complex SSL
handshakes or the cumbersome OTR protocol, and because I believe that
reliable delivery receipts and resilience to DOS attacks are as
important as the message itself. The exchange goes similar to the
following (each line describes what is sent by sender (s) / receiver
(r)) [2]:
(s) peer request
    (r) certificate, signed peer key
(s) certificate, signed peer key, encrypted message+MAC
    (r) receipt+MAC
(s) acknowledgement+MAC
and is similar to a state machine where each state is retried in
sender / receiver until a new state is reached. The exchange above is
somewhat implementation-specific for short requests followed by long
fetches (implementation is HTTP-based and targeted for .onions), and
for generic messages it can be reformulated as:
(s) certificate, signed peer key
    (r) certificate, signed peer key
(s) encrypted message+MAC
    (r) receipt+MAC
(s) acknowledgement+MAC
(In cables, username is certificate's fingerprint, so MITM'ing the
certificate is not an issue.)
So, with a centralized DB / prekeys I guess it's possible to shave off
the first two messages, but does it really matter if the protocol is
asynchronous to begin with?
[1] [2]

@_date: 2013-08-23 01:33:37
@_author: Maxim Kammerer 
@_subject: [liberationtech] Deterministic Builds Part One: Cyberwar and Global Compromise 
A very interesting project! Does the following:
mean that upgrading a library due to e.g. security fixes requires
recompiling all packages that depend on it?

@_date: 2013-01-09 22:50:37
@_author: Maxim Kammerer 
@_subject: [liberationtech] Google Bows Down To Chinese Government On 
The impression I am getting from my contacts at Google is that this is
not true. That is, Google apparently lost to Chinese Cyber experts in
being able to keep this censored keywords system up, and decided to
drop it altogether. PR team then, for whatever other reasons, decided
to keep complete silence on the subject.
Of course, one can then ask why didn't Google simply force HTTPS on
Chinese users to begin with, but they probably considered complete
block of Google by GFC too real a possibility, and were too afraid to
lose market share.

@_date: 2013-10-18 23:20:58
@_author: Maxim Kammerer 
@_subject: [liberationtech] RiseUp 
First, users raving about a service typically has very little to do
with quality of the service as a security product. I believe that's
why you posted the original question, after all.
Second, the unusual stress of ideology in such a service is very
relevant to product's security in this case. When I read RiseUp's
social contract page [1] some time ago, I found the mild creepiness
and passive-aggressiveness quite amusing, but immediately thought the
following: these guys seem pretty radicalized in whatever hippie
ideology they seem to be adepts of. This probably indicates that in
their closed group, they value ideological loyalty at least as highly
as technical expertise. It means that one of them could be incompetent
and still have administrative access to security-critical systems, or
that one of them could be recruited at some point under a suitable
ideological pretense — compromising the service in either case.
[1]

@_date: 2013-09-07 00:51:19
@_author: Maxim Kammerer 
@_subject: [liberationtech] Random number generation being influenced - rumors 
Nearly nothing from what you wrote is relevant to RDRAND, which is not
a pure HWRNG, but implements CTR_DRBG with AES (unclear whether
128/192/256) from NIST SP 800-90A [1,2]. Interaction with hardware
entropy source (ES) is implemented in microcode, so in case the
relevant microcode is reverse-engineered (or relevant documentation
obtained from Intel), it is possible to verify correctness of most of
RDRAND operation. ES operation could be perhaps analyzed in a lab.
The choice of CTR_DRBG over (probably much faster) Hash_DRBG seems
weird on first sight, but secure hashes are not yet available in Intel
processors [3]. Of course, an interesting conspiracy theory would then
be that NSA influenced Intel to delay secure hash instructions
deployment after breaking AES in order to exploit an AESNI-based
[1] [2] [3]

@_date: 2014-07-04 15:40:01
@_author: Maxim Kammerer 
@_subject: [tor-dev] XKeyscore rules probably are from Snowden, after all 
There has been some speculation that the recent XKeyscore rule leaks
[1] do not come from Snowden — particularly, by Schneier [2]. I
believe that there is a good case that the leaks do come from Snowden,
since it is possible to pinpoint the date range when the rule sources
[3] have been last updated.
The earliest possible date is 2011-08-08, when the Linux Journal
writeup about Tails [4], referenced by the glob pattern
"linuxjournal.com/content/linux*" has been published. The pattern is
not a generic Linux Journal filter, as implied in [1].
The likely latest possible date is 2012-02-28, when "maatuska"
directory authority has changed its IP [5]. A less likely upper bound
is 2012-09-21, when "Faravahar" directory authority has been added
[6]. NSA either took the 8 authorities from the actual consensus, or
picked them from Tor's sources [7]. However, Tor sources list more
than 8 authorities, and are not properly maintained (e.g., see entry
for "moria1" wrt. its last .34/.39 octet tweaks), so I doubt NSA would
use that. Moreover, it is hard to miss the port number in the sources,
whereas NSA did miss that some authorities do not (and did not) use
ports 80/443. E.g., "moria1" (the MIT campus server mentioned in [1])
would not be matched as a Tor authority by the rules.
Snowden most likely tried to contact Greenwald at the end of 2012 [8],
which is entirely consistent with the above. Another NSA employee
leaking XKeyscore rules after being inspired by Snowden's leaks, would
have probably downloaded a more up-to-date rules file.
Cross-posting to tor-dev, in case I got any historical directory
authority changes wrong.
[1] [2] [3] [4] [5] [6] [7] [8]
