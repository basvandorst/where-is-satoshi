
@_date: 1995-08-18 19:42:20
@_author: JMKELSEY at delphi.com 
@_subject: Time-memory tradeoff in SSL's RC4 code? 
This would work with straight 40-bit keys, but I believe SSL uses
128-bit keys, and then intentionally leaks 88 bits to comply with
export requirements, to prevent this kind of attack from working.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-08-18 20:04:12
@_author: JMKELSEY at delphi.com 
@_subject: Export policy change 
I think this is an important and somewhat subtle political move on
the part of the administration.  If they can get at least a few
large businesses (the ones who buy into the key-escrow scheme) on
their side, by making it in their economic interests for everyone
to use escrowed crypto, they will have manufactured some
potentially powerful allies in the computer industry.  Certainly,
once any major company has spent a lot of money to set up a key
escrow facility, they will help lobby *against* any easing of the
requirement to use escrowed crypto, based simply on self-interest.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-08-18 20:04:41
@_author: JMKELSEY at delphi.com 
@_subject: Anonymous certificates 
There are definitely cases where something is lost by prohibiting
that kind of certificate.  For example, there may be cases where a
certified key gives someone some right that doesn't need any
further identification, such as a right to run up $100 in phone
bills, or a right to receive a year's subscription to cp-lite.  As
long as the person has paid for that right, who cares who he or she
is?  In other words, you may wind up sometimes binding a key to a
function, rather than a person.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-08-21 17:18:13
@_author: JMKELSEY at delphi.com 
@_subject: Why 64 bit keys? 
I think it's much more important to the powers that be (and that
eavesdrop) that a key of up to 64 bits includes DES, which means
that lots of system designers will use DES instead of (say) 3DES,
IDEA, or Blowfish.  It's virtually certain that NSA and others have
built keysearch machines for DES.  This gives NSA, et. al., a way
around the key escrow scheme when they want it.  Better yet, NSA
can tell the FBI and BATF and such agencies where to find the
technical papers on how to build one, without releasing any
highly-classified information--those agencies can hire someone to
build them one.  This also keeps NSA from having to dirty its hands
with law-enforcement surveilance.
Of course, it will be interesting to see whether pressure is
applied to keep people from offering "nonstandard" ciphers,
especially things like Blowfish and SEAL, which have key scheduling
algorithms that need a lot of memory and time. It's really only
practical to build keysearch machines for very commonly used
ciphers, and even then, it may be complicated. (For example,
imagine a DES variant whose key schedule required several
exponentiations modulo some 1024-bit prime.)
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-11-04 10:17:34
@_author: JMKELSEY at delphi.com 
@_subject: public random numbers 
Imagine there is a stream of totally random bits over which neither
Alice nor Bob has any control.  We can use this to make a lot of
interactive protocols non-interactive.
Suppose we have a protocol where we need a random challenge from
Bob.  Alice sends a message to Bob starting the protocol, stating
that the public random bit stream is currently at bit i, and
committing to use the n-bit string starting at position (i+t) as the
challenge.  The t parameter here needs to be large enough to ensure
that Bob receives and logs the message before the public random bit
stream outputs bit i+t.  Alice proceeds with the protocol, using the
n-bit string starting at bit (i+t) as the challenge.  She sends the
resulting message to Bob.  No interaction was required of Bob--he
merely had to log the times of the messages, and keep track of the
public random bits.  This could be really useful implementing
noninteractive digital cash schemes, I think, because the merchant
wouldn't have to send anything back.  (The merchant can also be very
hard to track down by following the messages, since these messages
of Alice's can be encrypted under his public key and posted to a
newsgroup or something, though this implies really large values of
Naturally, this only works if Alice and Bob get the same random
string, and if it's not possible for anyone to alter the public
random bit string either one receives.  For large-scale
applications, the way to do this is probably to put a hardware RNG
into a communications satellite, and devote one channel to
continuous digitally-signed packets of random data.  For
smaller-scale or underground applications, it might be sufficient to
use some digitized transmission that would probably not be worth the
trouble for an attacker to alter, even if one could.  For example,
if we used the entire digital video feed off some major satellite,
it would be enormously expensive to take control of that for any
length of time, to attack some protocol.  To prevent simple attacks,
we can hash the digitized input, and we can make each shared random
packet dependent on previous packets by some relation like
random_packet[i+1] = SHA1(random_packet[i],SHA1(digital_video_packet[i])).
I was just thinking of the unintended entropy in the stuff going on
on the screen.  Static would mess this idea up, though there are
some ways to recover.
No, of course not.  Public random bits can be used in the derivation
of a shared key, to prevent replay attacks in key-exchange
protocols, but you certainly wouldn't want to use the public random
bits directly as key material!
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-11-04 16:08:00
@_author: JMKELSEY at delphi.com 
@_subject: wiretaping ability and future plans 
I can see at least two possible alternative explanations:
1.   The FBI envisions a time when voice-recognition systems will be
cheap enough to do "keyword searches" on digital voice in something
close to real time, with high accuracy.  This might be useful when
fishing for crime, dissent, etc.  It would certainly require some
rather broader powers, but they may expect this, especially if they
expect more acts of domestic and foreign terrorism.
2.   After implementing some kind of widespread escrowed crypto, the
FBI envisions recording lots of encrypted phone conversations,
perhaps targeted on suspiscious people, perhaps random.  Since the
whole conversation is encrypted, this might not violate any laws,
since they still have to get the warrant to recover the
conversation.  This would get them past the obvious practical
problem with most wiretap-based investigations--if you start your
wiretap three days after the target becomes a suspect, you've
probably missed all the juicy stuff.  Only a bit of carelessness or
stupidity on the part of your target will get the desired
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-11-04 16:09:30
@_author: JMKELSEY at delphi.com 
@_subject: video as a source of public randomness 
This seems like a potential source of a stream of public random
bits.  If these can be authenticated and matched, this kind of thing
can be useful in a lot of protocols.  For example, if there is some
packet structure in the digital video transmission which has at
least 160 bits of entropy, then we can take the SHA1(packet[i]) to be
public random bit block i.  (If we're worried about entropy, we can
collect arbitrarily large numbers of packets to hash per 160-bit
public random block.)
For large-scale, above ground protocols, these packets would need
some kind of signature or other authentication.  However, for
protocols that could handle having the public random string checked
offline later, this idea provides a reasonably good public random
string that can be used without any knowledge or consent of the
broadcaster/cable system/satellite system/whatever.  If the
transmission is sent under encryption, so much the better.  This
does still leave the possibility that an attacker could control the
broadcaster's transmissions for a few seconds, but this seems
unlikely in practice.
An alternative might be the encrypted transmissions from any
communications satellite.  How many telephone calls are your
opponents able to reroute?
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-11-14 19:37:59
@_author: JMKELSEY at delphi.com 
@_subject: No Subject 
Has anyone tried using neural nets or similar techniques for
searching for useful nonrandom properties of the round functions of
block ciphers or hash functions?  This might be useful in trying to
prepare some new kind of attack, find a balanced binary function
that is useful in using the generalization of linear cryptanalysis
discussed by Harpes, Kramer, and Massey at Eurocrypt '95, find a
better "difference" function for use in a differential attack, etc.
Merkle's paper on Khufu and Khafre addresses this idea, I think.
Merkle comments that it's not going to be useful against a full
cipher, but that it might be useful against (say) Khufu with one or
two octets.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-11-30 07:49:53
@_author: JMKELSEY at delphi.com 
@_subject: GOST for sale 
If this is the cipher I've seen, it's not overwhelmingly impressive,
though it may be OK.  I haven't ever made any serious attempt to
attack it, and I don't know anyone else who has.  Anyone have a
Basically, GOST is a balanced Feistel network (like DES), but its
F-function is much simpler.  Basically,
F(X,K) = Rotate_Left(S(X+K),11),
where S(t) denotes parallel application of eight 4:4 S-boxes.
Depending on the implementation, these S-boxes' contents could be
key-dependent and pseudorandom, or fixed.  Unfortunately small
random S-boxes are likely to have some weaknesses W.R.T.
differential cryptanalysis.  This can be true even when the S-boxes
are secret and key-derived--see Biham and Birkyov's paper in
Auscrypt '94 on a DES variant with variable S-boxes, for a quick
discussion of this.  And the security of this scheme is very much
going to depend upon the S-boxes used.  If the S-boxes are generated
at (pseudo)random from the key, I'd expect there to be some pretty
nasty weak key conditions that could occasionally come up.
On the other hand, GOST is defined with 32 rounds, so it may be hard
to find any useful differential or linear characteristics, even for
relatively bad S-box choices, that have high enough probability to
get through 29-31 rounds.  And it has a 256-bit key, so even if
someone determines some attack which recovers 160 bits of the key,
there are still 96 bits of key left to provide security.
The GOST key schedule is really simple, though it avoids the most
obvious kind of related key attack.  I wouldn't be surprised to see
some interesting related key attacks be possible.  This is
interesting because there is also a hash function based on GOST--I'd
be pretty reluctant to use this without a lot of analysis.
I may be mistaken, but wasn't there some other internal organization
in the USSR that did cryptography?
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1995-10-26 23:53:01
@_author: JMKELSEY at delphi.com 
@_subject: MD4-derived hash functions 
In the MD5 RFC, I seem to recall the statement that MD4 was trading
off too much strength for additional speed.  However, sometime
around that time, it came out that there were attacks on two-round
variants of MD4, which is the stated reason for the development of
RIPE-MD.  Does anyone know whether Rivest was motivated to design
MD5 by the partial attacks on MD4, or whether those came later?
(This is totally idle curiousity.)
All of the well-known software hash functions seem to be based on
MD4 these days, but that doesn't mean much about the security of
MD4--3DES with three independent keys looks pretty strong, as does
3DES with two independent keys, but that doesn't mean that single
DES is a strong enough cipher for modern applications.
One issue that exists with MD5, but not with SHA or the longer hash
versions of Haval, is that MD5 has only a 128 bit hash function
output, which corresponds loosely to having a 64-bit key.  This
implies that a wealthy enough opponent could determine a pair of MD5
inputs that collide, and conceivably use this in an attack.  I think
we should stick to 160 bit or longer hashes for future designs.
(See P. van Oorschot and M. Weiner, "Parallel Collision Search with
Application to Hash Functions and Discrete Logarithms," in the
proceedings of the 1994 Fairfax Conference, for example).
As an aside, what hash functions are there out there that look
reasonably strong, have hash outputs of at least 160 bits, and
aren't based on MD4?  Some of the Snefru variants with many passes
(eight?) come to mind, and the GOST hash function fits all the
criteria, except I have a hard time convincing myself it's as strong
as it claims to be.  Is there a generic construction for
arbitrary-length hash functions from good block or stream ciphers?
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-08-03 13:04:22
@_author: JMKELSEY at delphi.com 
@_subject: Paranoid Musings 
[ To: cypherpunks  Date: 08/02/96 12:29 pm   Subject: Paranoid Musings ]
Actually, this makes sense for another reason.  Academic
cryptanalysis is often about finding any attack on a cipher that's
easier than keysearch, even if the requirements for that attack are
still completely impractical.  (Differential and linear attacks on
DES are a good example of this.)  However, if you're interested in
actually recovering data in your attacks with high probability and
low cost, then it makes sense to focus on protocol and
implementation weaknesses, and then on attacks like keysearch which
can be done with either ciphertext-only or known-plaintext.
I would guess that some of NSA's best people work on optimizing
keysearches.  This especially makes sense because of the widespread
use, first of DES, and more recently of exportable 40-bit ciphers
like RC2 and RC4.
The paranoid conclusion is that there is a significant weakness in
any cipher you're counting on.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-02-04 08:39:58
@_author: JMKELSEY at delphi.com 
@_subject: RC2--Some very preliminary analysis 
[ To: sci.crypt, cypherpunks  Date: 02/02/96 06:21 pm   Subject:  Alleged RC2--some very preliminary analysis ]
I just wanted to post some corrected comments here, regarding
1.   The best differential characteristic I can think of looks like
it will have a probability of 2^{-4} per round.  It's a one-round
iterative characteristic.  In my earlier post, I miscalculated this
to be 2^{-8} per round.  Sorry.
2.   Each round of RC2 represents four "steps."  This means that RC2
has 64 "steps," the same number as MD5.  (I find this interesting,
since MD5 has twice as many bits to diffuse through, and the
attacker can choose its key, but not its input block.)
3.   I don't see how to build useful linear characteristics.  Our
S-box is one bit wide.  There may be some very low-round confusion
failures, but they don't seem particularly useful here.  I'd like to
hear from anyone who can see a way to do a linear attack here.
It looks to me (though I haven't spent enough time to be certain)
that the best differential characteristics to push through the block
are going to be one-bit characteristics.  (These are certainly easy
to analyze.)
Let's throw some terminology in here:
This is one step:
A = rotl(A + f(B,C,D) + sk[i], 1);
A round is all four of these steps.  In the step above, A is the
target block (it's the one that's getting stomped by the other
values) and B, C, and D are the source block.  f(B,C,D) is the
bitwise-select function.  For each bit position i, if B_i is a one,
then f_i = C_i, otherwise f_i = D_i.
Now, when a one-bit difference is anywhere in the target block (the
block getting all the stuff added into it) except for the high bit,
its probability of not propogating to other bits in that block seems
to be about 0.5.  (This is just based on its chances of affecting
the carry into the next bit position.)  When the flipped bit is the
high-order bit of the target block, it has no chance of propogating.
When a one-bit difference is in the source block, if the rest of the
bits are approximately random, then it has a 0.5 probability of not
affecting the target block at all.  If it does affect the target
block, it has a 0.5 probability of only affecting one bit in that
Note that I messed up the calculations in my earlier post on RC2 by
combining these three events in each round.  Let me try to fix that:
We flip some bit, t, making certain that if this bit doesn't
cause other bits to change, it won't ever affect the low six bits of
any block during rounds 4 and 11, when it would have a radical
effect on the encryption process.  (In other words, we choose an
input XOR delta with only bit t on.)  This bit then has the
following effect:
a.   Whenever it's in the target block, it passes through the
encryption step with probability 0.5.  (This means that changing
this bit doesn't change the carry into the next higher bit.) This
happens once per round.
b.   Whenever it's in the source block, it fails to affect the
target block with probability 0.5.  This happens three times per
Note the reasons for this.  The source block affects the target
block only through this function:  ((A&B)|((~A)&C)).  This function
looks somewhat complicated, but it's really just a bitwise IF-THEN
statement:  If bit A is on, then choose bit B, otherwise choose bit
C.  Assume that A, B, and C are random.  Now, imagine flipping A.
If you were choosing bit B before, now you're choosing bit C.  Since
they're both random, half the time, B=C, so there's no change.  On
the other hand, imagine flipping bit C.  About half the time, bit A
is a one, and so C has no effect on the output.
All of this gives us a total per-round probability of 2^{-4} (NOT
2^{-8}). Getting through 14 rounds with this characteristic thus
happens with probability 2^{-56}.  *IF* single-bit characteristics
are the best ones to use, I'm doing the calculations right, and
there aren't some improvements in splitting out and dealing with
several possible characteristics in the later rounds, then it looks
to me like straight differential attacks aren't going to be too
practical against alleged RC2, though they will be possible. The
trick is going to be detecting the right pairs reliably. (This
analysis is guaranteed to be worth at least what you paid me for it.
:-) )
If this really is RC2, I suspect the number of rounds needed was
determined by imagining flipping a bit, and then seeing what the
odds were that it wouldn't flip any other bits all the way through.
My guess is that a probability of 2^{-64} of this happening was
deemed acceptably low.
That takes care of diffusion--now how about confusion?  Has anyone
looked at this cipher with regard to linear attacks?  In general, it
seems like source-heavy UFNs can often be attacked by linear
attacks.  However, it's not clear to me how to build linear
characteristics that will make it through more than a few rounds of
alleged-RC2.  Linear characteristics that are spread across many
subblocks (i.e., partly in A and partly in B) seem to get messed up
quickly by the rotations.  However, just keeping a linear
characteristic in A doesn't seem to work too well, either--if the
bits in the other blocks are random, then the bits in our
characteristic will quickly become random, as well, because the
bit-selection function has balanced outputs.  Intuitively, I think
the problem here is that we're applying a three-bit to one-bit
balanced S-box here, and each output from this S-box has at least
one different input bit.  This seems to make it really hard to find
correlations between multiple S-box output bits and their
corresponding input bits that span more than one or two rounds.
Also, we have to deal with the carry-bits from addition, which make
things significantly harder.  Am I missing something?
There are some other plaintext patterns that will make it through a
single round, but I can't see any way to exploit them for more
rounds.  Anyone want to point something out to me?
The other interesting area is the key schedule.  Recall that phase
one of the key schedule in alleged-RC2 works by filling the leftmost
k bytes with the k bytes of key, and then using a byte-wide S-box to
expand this out to 128 bytes.  Phase two then works from the
opposite direction, taking the last t bits of the expanded key
buffer, and making the entire expanded key dependent only upon those
bytes.  As someone on cypherpunks pointed out, this seems to be
meant to make it possible to use the key schedule directly on user
passphrases, and then reduce the effective key length to t bits to
meet export control requirements.
In general, I don't think it's a good idea to use that key schedule
to hash long user passphrases, because the first few subkeys wind up
with some badly skewed bits. (This may or may not translate into an
attack, but there isn't any good reason for allowing it.)  If you
had (say) a 64-byte user passphrase, this would mean that the first
four rounds' subkeys were badly skewed in this way, and the next
four rounds' subkeys were probably not all that well-mixed.  As I
said, I don't see a specific attack based on this, but it seems like
a bad idea, since I might be able to plan out (for example)
differential characteristics that took advantage of the skewed
subkey bits.
If you're using the key schedule to hash passphrases, then it's
probably better that you use phase two as well, perhaps with bits =
256 or something similar.  If you limit user passphrases to
something reasonable, such as 64 characters, then this is probably
okay.  Has anyone else looked at this?  (Naturally, it would make
more sense to just hash the passphrase intelligently, and then use
the export control hack if you had to.)
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-02-10 06:16:37
@_author: JMKELSEY at delphi.com 
@_subject: RC2's key schedule and passphrase hashing 
[ To: Cypherpunks, sci.crypt  Date: 02/09/96 01:44 am   Subject:  RC2's key schedule and passphrase hashing ]
Summary:  Don't use RC2's key schedule to hash passphrases, with or
          without the export hack.  Instead, hash the passphrase
          first, and then pass the result into RC2 to get expanded.
          (This is a good rule to follow with any cipher whose
          designers didn't specifically intend for it to be used to
          hash passphrases.)  Don't use phase two at all--if you
          need exportable 40 bit security, generate a 128-bit random
          value, and leak 88 bits as salt.  Hash the value, and use
          the result as the RC2 key.
These are good questions, and they turn out to be pretty
educational.  I no longer can see any good reason for the
effective-bits hack.  Let me explain why, briefly:
Suppose I'm hashing a 64-character passphrase, where each character
has about three bits of actual entropy, for use in an export-
controlled application.  Clearly, this gives us plenty of entropy
(192 bits) in the whole user key.  How much entropy can make it to
the last five bytes of expanded key?  These last five bytes are a
function of the last five bytes of user passphrase (total entropy of
about 15 bits), and the previous byte of expanded key (which can't
have more than eight bits of entropy). Those are the only inputs
that aren't known to all attackers.  This means that in this (very
degenerate) case, we'd have 23 bits of entropy in our last 40 bits.
If we then did phase two of the key expansion based on those 40
bits, we'd wind up with a total expanded key entropy of 23 bits. In
short:  Guess an 8 bit random number and the last five bytes of the
user key, and you've got the entire expanded key.  (Of course, if
the application always padded the user passphrase with blanks or
something to make it 64 bytes long, and then scheduled the key with
the export control hack, we'd have *eight* bits of entropy in our
expanded key, which probably qualifies for some kind of special
thank-you note from Fort Meade or something.)
Suppose I'm hashing a 32 character user passphrase.  The same
analysis applies to the bytes 60..63 of expanded key.  Those bytes
can have no more than 23 bits of entropy, if each character of the
passphrase carries three bits of entropy. This means that bytes
92..95 carry at most 31 bits of entropy, and thus that bytes
124..127 carry at most 39 bits of entropy.  If we pad the passphrase
out on the right to 32 bytes before sending it into the RC2 key
schedule, then we wind up with about 24 bits of entropy in the whole
expanded key after phase two.
Our assumptions about entropy in user passphrases can make this
better or worse.  For example, we may assume that it costs us 16
bits to guess the first three characters of any passphrase
substring, and one bit per character after that.  In that case,
processing a 64-character passphrase into 40 effective key bits gives
us 26 bits of entropy.  If we assume that each additional character
after the first three guessed costs us two bits, then processing a
64-character passphrase down to 40 bits gives us 28 bits of entropy,
and processing it down to 56 bits gives us 32 bits.
Without phase two, long user passphrases simply leave most of the
expanded key fairly predictable, especially in the high couple of
bits of each byte.  I haven't tried to analyze yet what this does to
RC2's security, but it's almost never a good idea to have
highly-predictable bits anywhere in a cipher's expanded key.
The moral of the story seems to be this:  Don't use key schedules as
passphrase hashing functions, unless they're specifically designed
as such.  In particular, don't use RC2's key schedule to hash your
passphrase.  Pass your passphrase (and salt) through a good one-way
hash function like SHA1, and feed the result into your key schedule.
Can anyone guess what the effective-bits parameter is used for?  It
doesn't seem to be secure for hashing passphrases, making some
reasonable-sounding assumptions about actual entropy per character
in user passphrases.  I can't see an intelligent use for it.
Note that setting effective bits to 40, we have only a 40-bit
key--no salt.  This means that using alleged-RC2 with phase two of
the key schedule and effective bits set to 64 or less, we're
potentially vulnerable to a precomputation attack.  This is a real
problem if we're doing something like encrypting a constant block at
the beginning of the encryption, to verify that we have the right
decryption key.  (It's enough of a potential problem to be
worrysome, anyway--note that almost all of my notes to John Smith
start out with the same eight characters:  "[ To: Jo".
Precomputation attacks based on this are quite feasible.)  I am very
curious about how this is dealt with in practical implementations.
The obvious way to do this would be to leave effective-bits = 1024,
and just hash an 88-bit salt with a 40-bit session key to get the
128-bit key actually passed into RC2.  But that begs the question:
What's the use of having the effective-bits parameter? I certainly
hope nobody is using RC2 with effective-bits set to 40 in any
important, real-world applications.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-01-13 00:27:54
@_author: JMKELSEY at delphi.com 
@_subject: Limiting Reuse of Certificates 
[To: cypherpunks list  Date: 01/11/96 02:25 pm  Subject: Limiting reuse of certificates. ]
This is a hard problem.  The only way I can see to do this is to
require interaction with the CA (or its proxy) for each signature.
The good news is that if you're doing certificate revokation lists
online, then there is probably already some interaction with the
server to verify that a certificate is still valid, before it is
The trick here is to flip around who has to check the CRL server.
a.   Bob forms
     D = document he wants signed,
     ID_B = Bob's ID,
and sends to Alice
     M_0 = ID_B, D.
b.   Alice forms
     T = timestamp
and sends to the Server
     M_1 = T, hash(ID_B, D), Sign_{SK_A}(T, hash(ID_B, D)).
c.   The Server verifies the timestamp, the signature, and that
Alice is currently allowed to sign things (her certificate is valid
and hasn't been overused today).  If not, it drops the connection
and ends the protocol.  If things all check out, however, it forms
     M_2 = T, Sign{SK_S}(T, hash(ID_B, D), Certificate_A).
d.   Alice now has (until the timestamp T becomes too stale) an
authorization to sign D.  She does so, and sends to Bob (who's been
waiting all this time)
     M_3 = T, ID_B, D, Certificate_A, M_2, Sign_{SK_A}(ID_B, D).
Now, the trick is to redefine valid signatures as only those that
look like M_3.  The recipient has to verify the timestamp, and that
he hasn't received an identical signature from Alice recently, and
has to verify the two signatures.
(I make no promises about the soundness of this protocol--it's meant
to illustrate the idea, not to be used directly.)
Other than that, I can't think of anything that will fit the bill.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-01-25 11:31:19
@_author: JMKELSEY at delphi.com 
@_subject: Lotus Notes 
Perhaps in this case, c2.org could have a "patch Lotus" contest,
instead.  Help us patch this dumb security hole, by which we're
leaking 24 bits of each session key.
I think this is the case.  The guy who spoke at the RSA conference
made reference to the fact that this new version would interoperate
with full-strength domestic versions.  Getting domestic versions to
check for LEAFs only from foreign users is possible, but it would
seem to require that Lotus was working on this idea several versions
back.  Otherwise, when an old domestic version gets a message from a
new foreign version, it's going to accept the message without a
LEAF.  Depending on how Lotus Notes does their key exchange
protocol, it may be possible to graft this kind of checking on, so
that the older programs will work with it, too, but this doesn't
seem likely at all.
This problem is solvable, though I doubt they've bothered.  Two
ways come to mind--both using information that third parties won't
have to fix the problem.
1.   Put another 64 bits of random salt into the RSA key exchange
blob.  Use this to pad the LEAF, so that it's not feasible to
dictionary search the LEAF.
2.   Define the LEAF as part of the RSA key exchange blob.  Pad the
LEAF with random bits, unknown to the receiver.  Sign the whole key
exchange blob.
Note that  can be countered by hacking the software's copy of the
public key.  I don't see a way of countering  on the sender side
only.  (Once you get the sender and receiver working together,
key escrow seems to become really hard to do.)
Now, I'm very interested in whether they thought about this as a
potential problem, and thus padded their LEAF intelligently, or left
themselves vulnerable to a dictionary-style attack on the LEAF.
This translates, roughly, to "was someone with a basic understanding
of cryptography involved in this design?"  Clearly, IBM has some
really good people, and I suspect Lotus did/does, as well.  But were
they involved enough in the implementation to ensure that this was
done intelligently?
- From what I heard at the conference, though, I don't think they're
even checking to ensure compliance.  This implies that the security
patch can be pretty simple--clobber the LEAF field with a bunch of
random-looking bits.  Of course, this tells us nothing about the
other possible weaknesses.  How well does Notes generate key
material?  How big are the RSA keys?  How well do things like the
key exchange protocols work?  It looks to me like there are a lot of
programs with encryption out there that are lucky to manage even 40
bits of actual security, even if they're allowed 64- or 128-bit
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-01-26 14:13:11
@_author: JMKELSEY at delphi.com 
@_subject: Lotus Notes 
[To: Cypherpunks, Lucky Green  Date: 01/24/96 12:33 pm  Subject: Re: Lotus Notes]
I'm sorry, I don't think I was very clear in this post.  I wasn't
concerned with whether Lotus left the escrow feature easy to
disable, I wanted to know whether they'd intelligently padded their
RSA-encrypted 24-bit key leak.  If they thought this through, they
did, but if not, then they have essentially left their exportable
security level at 40 bits, because of the dictionary attack David
and some other people pointed out.  This ought to be relatively easy
to check from disassembled code, but it can also be checked by
simply generating a few thousand messages (maybe six or seven
thousand, to be safe), and seeing whether or not we ever get a
duplicate LEAF. We expect to, after about 2^12 encryptions, if
they're using fixed padding.  Of course, RSA key exchange blobs for
short keys must always be padded out like this, or be vulnerable to
dictionary attacks.
P.S. Does anyone know whether or not the RSA key used to partially
escrow the session key is a reasonable length (i.e., 1024 bits)?  If
it's another 512-bit RSA key, then it was born with a bullseye on
its chest.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-01-26 14:17:09
@_author: JMKELSEY at delphi.com 
@_subject: CAs and Digital Timestamping 
[ To: sci.crypt, sci.crypt.research, cypherpunks   Date: 01/25/96 10:08 am   Subject: CAs and Digital Timestamping ]
At the RSA conference last week, it occurred to me that there's a
really neat application for digital timestamps (as done by Surety).
Whenever a CA (Certification Authority) issues a public key
certificate, it should also digitally timestamp it.  This provides a
relatively clean way to recover from top-level CA key compromises.
First of all, let's talk about hash trees.  A hash tree (I think the
idea was originated by Merkle, but I could be wrong) is a binary
tree.  Each node in the tree eventually winds up with a hash value
associated with it.  The bottom nodes on the tree are hashes of
individual messages.  The other nodes are made up of the hash of all
their children.  This looks like this:  (pardon the ASCII art)
:                               H_0 = hash(H_1,H_2)
:                          /                    \
:              H_1 = hash(H_3, H_4)        H_2 = hash(H_5,0)
:             /         \                     /         \
:    H_3 = hash(M_0)  H_4 = hash(M_1)  H_5= hash(M_2)     0
The neat thing about this is that, if I know H_0, then when someone
wants to verify that M_0 appears in the hash, they only have to tell
me H_4 and H_2, and the position of M_0 in the tree.  I can then
find H_3 = hash(M_0), H_1 = hash(H_3,H_4), and verify that the final
H_0 = hash(H_1, H_2) gives the right output value.  (Compare this
with a hashing chain, and you can see that, for large trees, there's
a big advantage here.  The number of values needed to authenticate a
given message in this tree is log_2(number of messages in the tree),
while the number of values to verify a chain is is the whole number
of messages in the chain.
In the digital timestamping service offered by Surety, they use hash
trees of this kind, because this allows efficient verification of a
digital timestamp.  The idea behind this is that a message is hashed
into the hash tree, and that the final value of the hash tree each
day or week is widely published, including in the New York Times. So
long as it's not feasible to go back and change that value, and it's
not possible to find collisions for the hash function, any hash
value that appears in that tree must have been presented to the
timestamping people at some point before that tree's final hash was
Here's how we use this in having a CA sign certificates.
CA Signs a Key:
1.   At the beginning of the day, the CA generates a random value,
R_0, and has it digitally timestamped.  The resulting timestamp is
used as one of the entries in today's hash tree.  (If the CA is
their own digital timestamp service, then they'll have to use the
previous day's ending value, which is reasonable enough.  They'll
also have to work a lot harder to publish this value each day or
2.   For each certificate to be generated:
     a.   The CA verifies the information in Certificate_u, then
          signs it with SK_{CA}.  Certificate_u contains some
          indication of the time and date.
     b.   The CA hashes Certificate_u into its daily hash tree.
     c.   The CA sends Certificate_u to user u.
3.   At the end of the day, the CA publishes its own final hash tree
value, and gets this value digitally timestamped.  (This amounts to
having the CA act as its own "node." for the timestamping service.)
Now, imagine that the CA's key has been compromised.  (We have to
assume that the CA's daily operations were OK--if not, there doesn't
seem to be a clean way to recover cleanly.)  The person who has the
CA's key can issue false certificates.  After a while, one of these
false certificates are noticed.  How do we recover?
We use the hash trees and the digital timestamps which we've made in
the past to verify each certificate presented for recertification.
The digital timestamps allow us to immediately verify that this
certificate was issued by this CA at this time.  And we can be
certain that this hash tree is correct because it's been digitally
What this does, essentially, is to allow us to quickly recover from
key compromises.  We still have problems if someone can take over
the operations of our CA for a few days or weeks, though in that
case, we probably know the likely dates.  No compromise at the CA
can put a different date's timestamp on a certificate.
Now, I should note that I think Merkle talks about using hash trees
to do public key certificates, in his thesis, and the idea may be
patented.  (It's been a couple of years since I looked at his
thesis, so this is a little hazy.)  However, we're not using them
here to provide certificates--we're using them to authenticate the
certificates in the event of a catastrophic key compromise.  I don't
think Merkle talked about this, but I could be mistaken.  (At any
rate, I've never seen any mention of it since then, though it's an
obviously useful idea.)  If the tree structure is patented, then we
could still do this by using chains or some other structure.  Does
anyone know if this basic idea has been proposed before?  I am
pretty sure I haven't seen it, but it seems pretty obvious now that
I think about it.  We could even recover using this method from a
break of the hash function supported (so long as the timestamps are
done with multiple hash functions, and at least one isn't broken),
or the public key algorithm used.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-01-28 09:18:53
@_author: JMKELSEY at delphi.com 
@_subject: This post is rated LTC for `Low Technical Content' 
[ To: cypherpunks  Time: 01/27/96 02:19 am   Subject: This post is rated LTC for `Low Technical Content.' ]
The best solution has always seemed to me to be one of these three:
a.   Tags appended to notes/posts, from various reviewers, digitally
signed and otherwise coded to allow intelligent filtering, or
b.   Electronic distributions of reviewers' evaluations tagged to
notes in some simple way.  (I.e. give each note or post a unique ID
which appears in the message.)  Then, a smart newsreader/mail
program sorts the notes accordingly, or
c.   The reviewer reads the group/list, and rates posts according to
some useful criteria.  He then resends it out to his users, filtered
as desired.  (CP-LITE seems like a very early version of this.)
Any of these can be pretty easily ported to that magical set-top box
we hear so much about (no doubt running Windows '05).  In many ways,
(a) and (b) are easier.
Actually, I think in practice this will mean that programs get a
given rating, which is renewed every so often.  You don't rate
Melrose Place episode  you rate the entire series.
This whole idea offers two wonderful opportunities to control
content on TV.
First, get the TVs shipped with the V-chip filter turned on.  Most
people don't bother setting their VCR timers, and they also won't
bother setting this unless it denies them access to lots of shows
they like.  And, if turning the filter on and off is hard enough to
actually keep the average 12-year-old out, then it will be hard
enough that many families with kids will simply never change its
setting. They may even forget or lose the PIN that allows them to do
so. This means that you have a sizeable audience who depend on this
rating system, the only one readily available.
Second, apply pressure to television networks in whatever ways
necessary, by threatening a re-evaluation of their top-rated shows.
After all, ER really is a little gory for kids to be watching.  Oh,
you've decided to spend less time on covering the losses in the
great Bosnian Peace Initiative?  Well, I suppose a little real-life
drama won't hurt anyone.  If the V-chip is used widely at all, this
represents a really useful threat.  What happens to the network
executive who gets ER to lose half its audience, even just for a few
weeks while a review board takes up the network's appeal?
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-07-04 14:15:30
@_author: JMKELSEY at delphi.com 
@_subject: anonymous remailers 
[ To: Cypherpunks  Date: 07/02/96 03:36 pm   Subject: Re:  anonymous mailing lists ]
Yes, this makes sense.  As I said before, this is related to the way
timing attacks work.  A little correlation that shouldn't be there,
over many messages, turns out to be enough to unravel a lot of
At the very least, this is susceptible to a flooding attack.  At any
rate, this is analogous to the fixed-delay solution to timing
attacks.  (Make all PK operations with long-term secret keys take
the same amount of time.)  Unfortunately, I can't see a solution to
this that's analogous to blinding out the values in the timing
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-07-04 14:29:05
@_author: JMKELSEY at delphi.com 
@_subject: anonymous remailers 
[ To: cypherpunks  Date: 07/02/96 03:35 pm   Subject: Re: anonymous mailing lists ]
Yes.  This is a simpler version.  The advantage of the attack I was
describing over this attack is that an attacker doesn't have to know
how to send messages to the recipient--just where the stream of
messages is originating.
The flaw here is that only a small number of people will be willing
to plow through any volume of messages at all, in order to
occasionally get a single readable message.  There are also some
potential problems with giving the right recipient a cheap way to
determine whether or not this message is for him, without giving
anyone else a cheap way to determine this.  (An application for
``Rabin for Paranoids,'' anyone?)
If this is a small enough group, that may still be a problem.  And
the bandwidth and processing requirements are probably enough to
ensure that it's a small group.
This makes the attack only a little harder.  If the other 20 are
selected randomly, then for a stream of many messages, only one
recipient will correlate properly with sender volume and timing.  If
it's the same 20 every time for a given receiver, then the attacker
will be able to narrow the recipient down to 20 people.  At that
point, he can use other techniques (wiretaps, black-bag jobs,
TEMPEST attacks, etc.) to make his final determination.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-07-25 05:36:31
@_author: JMKELSEY at delphi.com 
@_subject: No Subject 
[ To: cypherpunks  Date: 07/19/96 08:13 pm   Subject: Message pools ]
There are two other factors.
1.   If you're trying to figure out who anonymously posted the ``All
faggots must die'' message on alt.sex.motss, you have a very large
number of potential suspects. However, if you're trying to figure
out who anonymously posted the ``how to manufacture nerve gas''
post, your suspect list is quite a bit smaller.  The condition for
technical information about cryptography or computer security is
2.   It may be that the way you test your suspects is parallelizable
enough that you can do a ``dictionary attack,'' in which you go down
a list of people who you might suspect of posting something for one
reason or another, and test the hypothesis that each of them
actually did post it.  Suppose I have such a test which can rule out
75% of my suspect list.  This becomes a useful tool--especially if I
can track multiple posts by the same user and rule out more and more
of my suspect list as more and more messages are posted.
I wouldn't count on even heavily-chained anonymous remailer messages
to protect my identity from moderately wealthy and determined
attackers, if I did many anonymous posts.  Writing style and topic
alone may narrow the suspect list down to a manageable number.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-06-29 12:09:15
@_author: JMKELSEY at delphi.com 
@_subject: anonymous mailing lists 
[ To: Cypherpunks  06/28/96 04:34 pm   Subject: anonymous mailing lists ]
I was thinking about attacks that can be carried out on remailers in
general, and came up with something that is potentially pretty
nasty, especially for anonymous mailing lists and people who post a
lot of stuff anonymously using ``nyms.''
Let's imagine an anonymous remailer network as a ``black box'' which
functions perfectly.  Messages (broken into equal-sized packets and
strongly encrypted) are sent into the network by the sender, and at
some later time, they come out at the receiver.  Let's assume there
is no possible way for an attacker to trace a message through this
network.  Now, we still have to deal with two more issues--mail goes
into the network, and comes out of the network.  At those two
points, there is trafic analysis available.  Specifically, we can
see how much data goes over the line.  (Naturally, if it's
encrypted, we can't tell how much of it is real data and how much is
Generally, when we're attacking this system, we're trying to figure
out either the sender or the receiver of a message (or a sequence of
messages), based on what we can observe coming into and going out of
the network of remailers.  There are basically five scenarios:
1.   The sender wants to know who the receiver of his message is.
2.   The receiver wants to know who the sender of his message is.
3.   An outsider wants to identify the sender of a message.
4.   An outsider wants to identify the receiver of a message.
5.   One receiver of a message wants to know who the other receivers
     of the message are.  (This is the case for anonymous lists.)
Now, there are a couple of different ways these attacks can be
carried out.  Usually, I've seen people talk about ``tracing''
attacks, in which a message is traced from one side of the network
to another, without any clear idea of who might be on the other
side.  However, I think a more realistic situation is to imagine the
attacker trying to test the hypothesis that some person is on the
other end of the anonymous transmission.  If relatively few people
regularly send or receiver anonymous e-mail, then this is practical
for many kinds of test.  It's even more practical when we're dealing
with relatively small populations of interested people in some
technical subject.  (This is conceptually similar to the
``dictionary attack'' on passphrases.)  Basically, what we're
looking at, in that case, is some test which (with some reasonably
high probability) determines whether some person is the sender or
receiver of a given message or stream of messages.
This leads to some interesting insights.
1.   In reasonably large text messages, it's probably easy to test
hypotheses about senders.  There are metrics that can more-or-less
identify the writer of a piece of prose.  While it's no doubt
possible to defeat this kind of analysis for some things (i.e.,
blackmail notes or rigidly-defined messages in a cryptographic
protocol), I suspect that this is very hard to defeat for a mailing
list where the objective is to discuss serious technical issues.
(This kind of analysis also causes headaches for people trying to do
strong steganography in text.)
2.   If an attacker (i.e., the NSA) logs the total volume of all
traffic in and out of the remailer network, and to whom each message
came from or went to, then that attacker can probably mount some
very powerful hypothesis-testing attacks.
It's these attacks I want to discuss.
If Alice sends a message to Bob through the remailer network, two
things must happen to prevent it from being trivially traceable.
1.   The message has to change size.  If the message is already
encrypted, then compression isn't much of an option--so what's left
is padding it out by a random amount.  The amount of padding per
message is probably a uniformly distributed random variable.
2.   The message has to be delayed somewhat.  The delay is probably
also a uniformly distributed random variable, or possibly the result
of adding N such variables, where N is the number of chained
For a single mailing, this is probably not much of a threat. There
will be enough ``noise'' in the delay and padding that most
transmissions will be masked.  However, consider the situation of a
mailing-list.  Alice and Bob are both recipients of the list. Alice
wants to decide whether Bob is receiving the list.  Let D be a delay
such that, if Alice received her copy at time T, 90% of the other
list members received their copy between T-D and T+D.  Now, Alice
looks at Bob's anonymous e-mail volume during that time span vs. at
all other times.  If he's receiving the same stuff she is, then
there should be an increase within that span of time, on average.
The random distribution of the arrival time will mask individual
transmissions, but with many messages, it probably will not.  (This
is conceptually similar to the situation in Paul Kocher's timing
attacks--adding some random pauses doesn't hurt the attack as much
as most people expect it to, because those random pauses, summed up
over many messages, become a normally distributed random variable.)
The average amount of anonymous e-mail Bob gets per day doesn't have
much effect, nor do occasional worst-cases. The only ways I can see
to prevent this attack are either to ensure that Bob gets a constant
rate of information from the anonymous remailer network, or to make
the arrival time span so large that other randomness in the sample
makes the change in volume undetectable.  In general, I don't think
this second one will work without accepting incredible delays on
This can also be adapted to tracing back anonymous posters to
newsgroups and mailing lists, when they use a consistent nym.
(They could also be traced by textual analysis.)  In this case, the
attacker starts by posting some anonymous messages (not using a
nym--he doesn't need one), to get some statistics on what the
average delay is, and also what the average amount of padding is. He
may do this for several different ways of putting things
together--he's got almost unlimited time to gather acceptable data.
At this point, he observes in/out traffic logs for each hypothesized
sender during a wide timespan before the arrival of the post at its
destination.  He compares activity inside that span with activity
outside, over a large number of posts.  If there is a correlation,
then he's got the e-mail address of the nym.
There are ways to get around this second attack, at least to some
extent.  However, I don't think it's wise to count on even very good
remailer networks (i.e., the Mixmaster stuff) to protect your
anonymity in this situation.  (However, note that I'm thinking in
terms of a very well funded, determined adversary.  It's probably
not too bad to count on it to protect your anonymity from
technically unsophisticated attackers--but I wouldn't recommend
using it for things that (say) the FBI or NSA might get very
interested in.)
I think the best defense against this will be something like this:
Each user sets a quota of how much trafic he will take in and send
out per day.  Once per day, he goes through an interaction in which
he downloads and uploads that much stuff, whether there's any of it
for him or not.  (Naturally, this won't be detectable from looking
at the transmission, timing the interaction, etc.)  This makes any
volume variations per day disappear.  Unfortunately, it also limits
the user's total inflow and outflow, which means he'll have to set
it to something larger than the maximum he ever expects to get.  (It
would be possible to have occasional overflow onto the next day's
downloads, but not too often, or the user would fall further behind,
on average, each day.)  The size of these quotas will still leak
some information, though not enough for the kinds of attacks I
discussed above.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-03-09 13:06:17
@_author: JMKELSEY at delphi.com 
@_subject: POTP Jr. 
[ To: cypherpunks  Date: 03/04/96 07:59 pm   Subject: POTP Jr. ]
Surely I can't be the only person who's noticed the obvious--that if
you break their system, their company is worth nothing, since this
is their only shipping product.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-03-09 13:10:04
@_author: JMKELSEY at delphi.com 
@_subject: numbers don't lie 
[ To: cypherpunks  Date: 03/04/96 07:59 pm   Subject: Re: Numbers don't lie... ]
Coming up with a short length of known plaintext isn't usually a big
problem.  For example, attacking DES, you need to know one 64-bit
block.  In many cases, this is easy to do.  While it is possible
(and a good idea) to build communications software so that it's
relatively hard to get known plaintext, this shouldn't be necessary
to use a cipher securely.  And in any case, if you're encrypting
ASCII text, the bit distributions give you a big clue about whether
this is a reasonable key guess or not, after just a few decrypted
plaintexts.  This increases the cost of the search machines, but I'm
not convinced that this will be an enormous increase in all cases.
True, DES is probably good enough for the very lowest-value
messages.  But why use something that's barely acceptable, when it
costs you almost nothing at all to make it really secure against
keysearch attacks.  Blowfish, SAFER-SK128, GOST, and 3DES are all
apparently quite hard to break, and they are all far more resistant
to keysearch attacks than DES.
The problem here is that it's not really reasonable to expect the
users of a secure e-mail package to know what the state of the art
is in terms of keysearch machines, and it's not always reasonable to
expect the person that's sending some piece of information to know
whether this is "you-bet-your-company" material.  There's no excuse
for leaving yourself vulnerable to keysearch attacks, when there are
so many good, unpatented ciphers with key lengths of more than 100
bits.  It's like building a car with an engine that you know will
catch fire if it's ever run at more than 80 MPH, but justifying it
by saying "well, most trips don't require more than 80 MPH to get
where they're going anyway.  In those special cases where greater
speed is necessary, they'll just have to take a bullet train."
Limiting your key to 56 bits means that an attacker has more
options--if he can't bribe, blackmail, or threaten his way into your
private communications, he can spend some money, and still get in.
(Escrowing your key adds to the list, because he now has more people
to bribe/threaten/blackmail, and he may also be able to carry out
protocol attacks against the key escrow mechanism.)
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-05-07 16:06:54
@_author: JMKELSEY at delphi.com 
@_subject: Escrowing signing vs. encryption keys 
[ To: cypherpunks  Date: 05/06/96 06:55 pm   Subject: Escrowing signing vs. encryption keys. ]
There is another angle to this.  If extralegal key escrow accesses
are occurring, it will probably take a long time to come out, if it
ever does.  There's a good chance that anyone successfully
eavesdropping on people by use of the key escrow mechanism will
simply keep quiet about it, and while the victim may *suspect*
what's happened, they won't be able to prove it.  However, forged
signatures *will* be noticed directly, and there will be
high-profile court cases about them.  Even if untrue, serious
allegations of forged signatures based on escrowed signing keys will
make it into the papers, and cause all kinds of chaos.  Presumably,
this is seen by the Feds to balance out the downside that, if I have
the ability to do secure signatures with certificates, I can always
use Diffie-Hellman to establish a secure session with someone else.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-10-10 07:18:03
@_author: JMKELSEY at delphi.com 
@_subject: Copyright protection schemes discussed at ESORICS 
[ To: cypherpunks, sci.crypt  Date: 10/08/96 01:05 pm   Subject:  Copyright protection schemes discussed at ESORICS ]
Last week, I returned from the ESORICS conference in Italy.  (If
you're going to have a computer security conference, Rome is a nice
place for it.)  I've been busy since then, but I wanted to post
something about a panel discussion held there about copyright
protection in the electronic world.
This is all a little fuzzy in my memory, since it's been about two
weeks and my notes aren't terribly detailed.  However, I think the
discussion's contents will be of interest to a lot of people out
there working at the interface between cryptography and politics.
There were three people involved in the presentation:
1.   Gerard Eizenberg (chairing the discussion)
2.   Dominique Gonthier (from the European Commission)
3.   Alstair Kelman (a lawyer from the UK specializing in computer
                     and copyright issues)
Jean-Jacques Quisquater was supposed to be there, but had to cancel
(I think he was ill).
The basic problem they're trying to solve is that it's very hard to
conditionally control access to digital information.  It's not too
hard to keep me from ever seeing some piece of information (encrypt
it with a key I can't guess and a cipher I can't break), but it *is*
hard to keep me from giving a copy to a few hundred friends and
acquaintances, once I *have* seen it.  The solutions they proposed
(and I can't think of anything better) centered around two ideas:
1.   Embedding identification codes into the digital information which
are hard to remove, but easy for the authorities to detect.  (They
call this ``tattooing.'')  This allows the authorities or copyright
holders to trace the source of illegal copies.  (This is a little
like asking for the user's name and phone number during software
installation--it makes things a little less convenient if he later
wants to give away copies to all his friends.)
2.   Providing users with access to the information only on hardware
controlled by the copyright holder or people working in his
interests.  (They call this the ``black box.'')  This black box is
sold or given away to users to allow them to have limited access to
copyrighted data, and presumably goes through various gyrations to
make sure that the user continues to pay for what he gets.  The
market for these would probably be similar to the market for
satellite decoder boxes.  (I don't think the system would be hard to
design intelligently, though there would be a lot of opportunity for
implementation errors.)
Some of the interesting comments I recall from Mr. Gonthier:
1.   The ultimate solution to this problem will have to involve
technical, legal, and political methods.
2.   Not every country in Europe (let alone the world) recognizes
the same kind of rights with regard to copyright.
3.   The solution will have to involve published and well-accepted
standards--if we wind up with a dozen noninteroperable systems, then
they will probably all fail in the marketplace.
4.   The current solution is basically that owners of really
valuable content don't make it electronically available.
5.   Either people plan out a solution, or they wait and see what
kind of market-driven solutions emerge.  Mr. Gonthier favored the
first approach, and appeared to take it as a given that almost
everyone else would, too.  (I suppose if he didn't feel this way, he
would be in some other line of work....)
Some of the interesting comments I recall from Mr. Kelman:
1.   Copyright law isn't terribly well designed, and there are lots
of ambiguities.  It's going to be impossible to try to make
automated systems that make the kinds of judgements that are
currently done over several days or weeks, in court.
2.   Some of the ambiguities in copyright law get much worse with
digital systems.
3.   There are serious privacy implications in many of the solutions
discussed here.  Is it acceptable to have someone have a list of
every movie you've rented and every book you've bought or checked
out of the library?  Is it okay if they sell that list to (say) the
government, people who might want to sell you things, people who
might want to sue you, etc?
4.   The system has to be expensive to start breaking.  If we put
out an easy-to-break system now, and successively harder-to-break
ones later, then we train and provide capital for people who break
the copyright systems for money.  This is essentially what happened
with satellite decoder boxes.
Mr. Eizenberg made some interesting points, but since they were
mostly technical, I didn't take a lot of notes.  (I already have
thought about the technical side--it's the political and legal side
that I don't understand so well.)  One comment of his I *do*
remember, which I thought was an excellent point, was that
electronic commerce systems were a prerequisite for electronic
copyright management systems, and that the properties of those
electronic commerce systems would constrain what was possible for
the electronic commerce systems.  (For example, if your commerce
system doesn't support anonymous payments, then it's going to be
very hard to support the anonymous purchase of books in the
copyright management systems.)
I thought the discussion was very interesting, though I wish there
had been more time for questions and discussion.
My main comments are:
1.   I think the privacy issues are potentially monstrous,
especially when we add in consideration of billing records.  We have
to worry, not only about police-state measures (``lock up everyone
who has purchased more than three books on this list''), but also
about blackmail (``gee, Senator, I suppose you read `Naughty Boys in
Leather' each month for their incisive political commentary.'').
2.   Item 1 becomes more problematic when we consider the likely
unwillingness of many governments to accept any kind of anonymous
payment system that they can't trace.  How many people think it
would be okay for the government to have access to a list of what
books you read, so long as they promised not to misuse it?
3.   The ``tattooing of copyrighted data is really only interesting
if the tattoo can be used to trace the person who made the illegal
copies in the first place.  This also relates to item 1.
4.   Many countries have restrictions on what their citizens can
read.  This includes not only places like China, Iran, and
Singapore, but also places like the US, Canada, the UK, Germany, and
France.  It seems unlikely that the copyright management system
would be acceptable to many of these governments, if it ignored
these restrictions.  However, we're publishing the standards for how
these are to work--so with a little extra work, each country can
have their own implementation.  The US version can restrict what we
consider to be hardcore pornography (though someone will probably
have to come up with a usable definition of this term), the French
version can keep its citizens from watching too many American-made
movies, and the Chinese system can prevent citizens from reading or
watching anything with unacceptable political or social commentary.
After the election of the Buchanan administration in 2000, the US
version can even be modified (if designed well) to restrict the
number of non-English-language movies and books you can see/read.
Even if the US doesn't misuse this system, we'll be providing every
dictator on Earth with a turnkey system for tracking or censoring
what his citizens read and watch.
1.   For most ways I can see these systems being built, the initial
cost to break the system will be somewhat high, but the marginal
cost per piece of copyrighted data extracted will be relatively low.
This means that it may pay for someone to break one box, then buy a
large number of movies, books, etc., and then, all at once, start
offering to sell them.  Each sale is available for only a few hours,
so they don't have to worry about preventing people from making
further copies too much.  So long as there is any way they can do
this (and if there's not, it means that anonymous cash and
communications have been totally stamped out), they can make a nice
profit on this stuff.  If I were designing this system, I'd probably
build in the ability to lock out the box that had violated the
rules, if I could identify it--hence, the plan to buy up lots of
titles, and only *then* to sell them.
2.   The simplest attack on this kind of system is what I'd call an
``end-run'' attack--we do an end-run around their defenses.  In the
case of music, books, and movies, this is done by digitizing the
output from the tamper-resistant viewing machine, and then making as
many copies as we like.  The cost for this attack is
     a.   A one-time cost for building the equipment to intercept
     and digitize the output.
     b.   A marginal cost per batch of sales--for each batch of
     sales we do, we probably lose the ability to buy anymore
     copyrighted items with that black box.  Thus, we have to be
     able to buy a new black box with the revenues from each pirate
     sale, plus eventually pay back what we spent buying or building
     the machine.
     c.   A marginal cost per item--we pay what all other consumers
     pay.
Note that (a) shouldn't be *that* much money, though it will almost
certainly be illegal in many places to have or sell such equipment.
For many designs of the black boxes, we just intercept the video
signal coming out of the box, which is trivial.
For (b), we have to either buy the black boxes under a false name,
or buy them somewhere that doesn't have much of a penalty for
hacking them and won't extradite us somewhere that does have a harsh
penalty for it.  If people demand too much proof of identity before
they'll sell you a black box, then they'll find it impossible to
sell many of them.  (Would you buy a television if they required
three forms of ID, a fingerprint, and a blood sample?)
3.   Ross Anderson's eternity service would obviously destroy this
system.  However, this is overkill.  It takes only one country which
is connected to the net, and which doesn't enforce laws against
electronic copyright violations, to ruin the whole system.
This raises an interesting point that's been raised many times
before:  What do you suppose will happen to countries that don't
enforce these laws?  I suspect the US and other countries will see
countries that don't enforce these laws as damaging their interests,
and will act accordingly.  Some precedents include US threats to
start a trade war with China over failure to protect copyright, and
a long list of US interventions into Central and South American
governments that did things we didn't like, i.e. Argentina,
Nicaragua, Panama.  (I'm sure other countries have done similar
things, but being from the US, these are the cases I'm familiar
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36

@_date: 1996-09-18 07:32:32
@_author: JMKELSEY at delphi.com 
@_subject: Dealing with junk mail 
[ To: cypherpunks  09/17/96 09:26 am   Subject: Dealing with junk mail ]
There has been a bunch of stuff in the news lately about junk
e-mail, including the recent judge's ruling that AOL must allow
known junk-mail through to its subscribers while the judge hears
arguments from AOL and the junk-mailers.
I'm a little divided on the whole issue of whether or not it's wise
policy for ISPs to, in general, refuse to deliver suspected junk
mail.  The obvious problem is that it puts ISPs in the position of
deciding whether or not some piece of e-mail is worthwhile.  (To
clarify, I certainly *don't* think that ISPs should be prohibited by
law from blocking delivery of or access to anything they choose
to--there are plenty of ISPs to choose from, and users can move if
they don't like their current ISPs' policies.  I just don't think
I'd like Delphi to start filtering my e-mail without asking me for
permission and instructions first.)
I've been thinking about an alternative approach to blocking
commercial spam.  It has some potential technical problems, but I
think it could be made into a workable 95% or so filter.
Many people are already filtering messages locally, and now some
providers are getting into the act, as well.  Unfortunately, because
of the economics of junk e-mail, I think that this, by itself, will
probably lead to people refusing to accept almost all e-mail from
people they don't know.  This is a really bad outcome.
What I'm proposing is an extension to this, in which many peoples'
filters coordinate their actions to detect and block spam.
Each user has a mail filter with a set of rules written either by or
for that user.  The mail filter does one of four things with each
piece of e-mail it receives:
a.   It lets the e-mail through immediately.  (E-mail from friends,
employers, employees, family members, etc. would probably be in this
b.   It discards the e-mail immediately.  (E-mail from people you
really didn't like, and from people who have spammed you in the past
would probably be in this category.)
c.   It puts the e-mail on hold in some storage area.
d.   Send e-mail back to the sender, informing him of conditions
     under which the user is willing to accept this e-mail.  This
     might be things like requiring anonymous users to provide some
     minimal kind of identity, or telling senders ``I'll read your
     e-mail for one dollar in digicash,'' or ``I'll read your e-mail
     if you carry out this computationally expensive calculation, or
     some other thing.
For e-mail in the third category, some kind of summary report is
sometimes generated, to be sent to a server.  The server collects
these reports, and uses some kind of system (maybe rule-based, but
probably involving scores to estimate probability of spam or other
unwanted e-mail) to determine what is and is not spam, and with what
probability.  It then sends to each of its subscribers, every day or
so, a report indicating scores for users' messages.  (These should
be individualized.)
The mail filters then do one of four things to each piece of mail
rated, based on the scores:
a.   Pass the message through immediately.
b.   Discard the message immediately.
c.   Add the message to a list of low-priority messages, to be read
     when the user has some spare time.
d.   Send e-mail back to the sender, informing him of conditions
     under which the user is willing to accept this e-mail.  This
     might be things like requiring anonymous users to provide some
     minimal kind of identity, or telling senders ``I'll read your
     e-mail for one dollar in digicash,'' or ``I'll read your e-mail
     if you carry out this computationally expensive calculation, or
     some other thing.
The junk e-mailers can try various countermeasures to this.  The
most obvious are:
a.   Try to hit people who aren't using a good junk-mail filter.
b.   Try to make it against the law to use a junk-mail filter.
     (Perhaps this would be the case only for PSA spams?)
c.   Try to disguise their e-mail to make it not obviously junk
     e-mail, and simultaneously to alter each message to avoid
     detection by the servers, by making changes to each message,
     timestamp, and claimed sender ID.
I think (c) will be somewhat difficult for the junk e-mailers, if
the people who run the servers are reasonably clever.  The servers
should run indexes that find many identical or similar sentences,
paragraphs, etc, in messages sent to many people.  I think either
the junk e-mailers would give up on these filters immediately, or
there would be an endless arms race between advertisers and filter
There are some potential problems with this approach, though.  The
servers will be getting a lot of information about what e-mail is
coming to each of their users.  There will be serious privacy
concerns, especially if the filters work after decrypting public-key
encrypted messages.  (Note that if the user's public key is
reasonably long, PK encrypting the message will actually be pretty
hard for thousands or millions of messages at a time.  Also, there
will be various denial-of-service attacks, where I know Alice is
getting ready to send Bob some e-mail I don't want him to get, so I
intercept Alice's e-mail and forward it to 10,000 other people--thus
ensuring that it will be classified as spam.
Note:  Please respond via e-mail as well as or instead of posting,
as I get CP-LITE instead of the whole list.
   --John Kelsey, jmkelsey at delphi.com / kelsey at counterpane.com
 PGP 2.6 fingerprint = 4FE2 F421 100F BB0A 03D1 FE06 A435 7E36
