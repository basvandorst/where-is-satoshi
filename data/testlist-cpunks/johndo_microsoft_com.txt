
@_date: 1994-08-08 12:39:33
@_author: John Douceur 
@_subject: Remailer ideas 
There is an important distinction between systems for which the only
observable behavior is the probabilistic average and those for which
the observable behavior is that of the individual actions.  An example
of the former system is a hash table with open addressing:  The
absolute worst case for a lookup is as bad as that in an unsorted
list; however, this is not usually a problem, because programs
generally perform large numbers of lookups, and the performance that
the user observes is therefore equal to the probabilistic average.
An example of the latter system is the case in point, a remailer:
If a message is delayed unduly, the sender is unlikely to be
contented by the fact that many other users' messages were serviced
with considerably greater promptness.
Therefore, the probabilistic distribution of service times is as
important a metric of a remailer's performance as the probabilistic
average service time.  It may thus be quite reasonable to build in a
hard cutoff in service time, such that any message that has been
delayed by more than a set amount will be guaranteed to be sent on
the next transmission.  For some user of the remailer, this will make
an observable improvement in performance; and since the extreme delay
which triggers the expedited transmission is an unpredictable and
infrequent event, it will not make cryptanalysis of the remailer any

@_date: 1994-08-09 09:21:35
@_author: John Douceur 
@_subject: Remailer ideas 
The context of my above assertion was a hypothetical message-mixing
system proposed by Hal Finney.  Although I must confess that I
haven't examined the statistics that you cited, I do not see their
relevance to this hypothetical system.
Of his own proposal, Hal says, "...it does have one disadvantage,
which is that there is no upper bound on the latency of a
message....  there is a small chance of having very large
latencies....  it might be possible to modify [this system] so
that messages never waited more than some maximum number of hours
without seriously hurting the entropy."
I believe that this is correct.  The message delays introduced by
Hal's proposed system were of exponentially diminishing
probability; thus, linear increases in delay cutoff become
multiplicative decreases in cutoff probability, and it is
therefore easy to set a cutoff value for delay which will occur
with sufficient infrequency as to be useless to the cryptanalyst.
By "cryptanalysis," I mean traffic analysis.  Considering the
remailers to be a cryptosystem was suggested recently on this list
by someone (I forget whom).

@_date: 1994-08-29 14:38:29
@_author: John Douceur 
@_subject: iterated prisoner's dilemma 
. . .
Axelrod's second tournament had a variable number of interactions,
precisely to defeat penultimate-interaction attacks.  He added this
specifically because his first tournament had a fixed and known number
of interactions, and several programs took advantage of it.  However,
even in the first tournament, the "nice" programs did better than the
"mean" programs, and Tit-for-Tat was the winner.
I suppose this doesn't prove much, insofar as a Tit-for-Tat-but-
Screw-Em-on-the-Last-Round program would probably have come in first
had it been entered.  Even so, I expect that the marginal increase in
score over Tit-for-Tat would have been vanishingly small for a large
number of interactions.

@_date: 1994-08-29 16:46:13
@_author: John Douceur 
@_subject: e$ as "travellers check? 
This comment, unless I misunderstand it, supports (rather than refutes)
Perry's rebuttal to the claim that forging digital traveller's checks
would be "extremely easy."
Sticking in my nose where it doesn't belong,

@_date: 1994-07-18 11:03:17
@_author: John Douceur 
@_subject: Why triple encryption instead of split+encrypt? 
I read this as something like the following:
int munge[16] = {0x0, 0xE, 0xD, 0x3, 0xB, 0x5, 0x6, 0x8,
                 0x7, 0x9, 0xA, 0x4, 0xC, 0x2, 0x1, 0xF};
for (int i = 0; i < num_blocks/2; i++)
  {
  unsigned int s0 = source[2*i], s1 = source[2*i+1];
  unsigned int d0 = 0, d1 = 0;
  for (int j = 0; j < 8; j++)    // 32-bit ints assumed
    {
    d0 |= munge[(s0>>(4*j)) & 0xF] << (4*j);
    d1 |= munge[(s1>>(4*j)) & 0xF] << (4*j);
    }
  dest0[i] = (d1 & 0xAAAAAAAA) | (d0 & 0x55555555);
  dest1[i] = (d1 & 0x55555555) | (d0 & 0xAAAAAAAA);
  }
This fragment splits alternating bits from each contiguous pair of 64-bit
blocks in the source[] array into two blocks, each of which is placed
into one of the two dest[] arrays.  The inner loop first makes each bit
in the pre-split data dependent on the three other bits in the same
nibble.  Is this consistent with your suggestion?
I believe these claims hold true for the above code.
Yes and no.  Meet-in-the-middle does not work, per se, or more precisely
has no applicability.  Recall that meet-in-the-middle is a method of
extending a known-plaintext attack on a single encryption to multiple
encryptions by means of an enormous amount of memory to hold intermediate
results.  In the split+encrypt proposal (as I have implemented it above),
a known-plaintext attack can be applied directly, with only twice as much
computation as that needed for a single encryption, and no need for large
amounts of memory.
The cryptanalytic approach is simple:
     1) Split the known plaintext, P, with the splitting algorithm, into
        P0 and P1.
     2) Apply known-plaintext attack to P0 and C0 to determine key K0.
     3) Apply known-plaintext attack to P1 and C1 to determine key K1.
No.  It is not even as secure as double DES, since cryptanalysis of the
former has the same computational complexity as the latter, but without
the extreme memory requirements of meet-in-the-middle.
A noble goal.  It would also have allowed multi-threaded crypto code on
multiprocessor machines to perform the separate encryptions in parallel.

@_date: 1994-07-18 18:02:10
@_author: John Douceur 
@_subject: Why triple encryption instead of split+encrypt? 
The assumptions about the information available to the cryptanalyst
vary with the type of attack.  The essence of a known-plaintext attack
is that both plaintext and cyphertext of several messages are known,
and the task is to deduce the key.  This is more practical than it may
sound, since there may be (for example) header information that has
small or no variability among messages.
For that matter, one could have a third key which is used by the
splitting algorithm.  If one chooses to make this splitting key a
function of the two DES keys, then this approach reduces to your
suggestion, at the expense of a smaller keyspace.  It could be said
that, in the code fragment of my previous message, the splitting key
is fixed at 0x55555555.
So now the meet-in-the-middle attack regains its earlier applicability:
A known-plaintext attack would encrypt P with the splitter, decrypt
C0 with DES, and attempt to meet in the middle to discover key K0;
similarly, decrypting with C1 to get K1.  If you can design a splitter
that is as cryptographically secure as DES (good luck), then the
resulting algorithm is as secure as double DES.  Actually, the
computational complexity of a cryptanalysis would be somewhere between
one and two times that of double DES, since it requires one encryption
analysis and two decryption analyses.
In your previous message, you commented:
If your secret-splitting algorithm is as secure as DES, then it probably
runs as slowly as DES does, making your hunch correct.  However, even if
this were not the case, the security of this scheme is significantly less
than that of triple DES.

@_date: 1994-07-19 09:50:08
@_author: John Douceur 
@_subject: Why triple encryption instead of split+encrypt? 
Wrong.  (sorry to sound so authoritative; just wanted to make my position
clear.)  If you knew how to perform the split, there would be no need for
a meet-in-the-middle attack; you could just attack each of the DES
encryptions of the split data separately.
Recall that a meet-in-the-middle attack is a method for cryptanalyzing a
message that has been doubly encrypted, as the following:
By this nomenclature, I mean to imply that not only the keys but also the
algorithms may be different between the first and second encryptions.
Meet-in-the-middle works by encrypting from P towards I, decrypting from
C towards I, and attempting to meet in the middle.  For algorithms with
large keyspaces, this attack requires so much memory for storing
intertext as to be almost absurd in today's world, but it is a valuable
theoretical technique for demonstrating that double encryption provides
little more computational security than single encryption.
I am claiming that your technique:
Can be decomposed into parallel double encryptions, and is therefore just
as vulnerable to a meet-in-the-middle attack as double DES (or more so,
if your splitting algorithm is less secure than DES).  NB:  When I use
the term "double encryption" here, I am not referring to your use of DES
multiple times after the split; I am referring to the splitting itself as
the first encryption, and the DES as the second encryption.  Let us
define the function Sx_KS(P) as the portion of the splitting algorithm
which produces Px:
We now have a parallel set of double encryptions as follows:
Each of these double encryptions is vulnerable to a known-plaintext
meet-in-the-middle attack from P to Cx.
In my above argument, I assumed a splitting key which is completely
independent of the DES keys.  This will be more secure than a splitting
key which is *any* function of the DES keys, since it increases the size
of the keyspace.
Perhaps you do now?
If you still maintain this position, then either you have not understood my
argument above, or I seriously misunderstand your algorithm.  If you have
not yet been convinced that you have not eliminated the meet-in-the-middle
attack as triple encryption does, then I welcome your algorithm in code, so
that I may see if I am missing something fundamental in your approach.
However, I strongly suggest that you review meet-in-the-middle attacks as
described by Merkle and Hellman and judge for yourself their applicability
to and effectiveness against your algorithm.
The security of the generation of the splitting key from the DES keys is
almost irrelevant.  You can guarantee that the splitting key is completely
uninferable from the DES keys by making them independent, yet the
split+encrypt algorithm is still as weak as (or weaker than) double DES.

@_date: 1994-07-19 16:18:59
@_author: John Douceur 
@_subject: Why triple encryption instead of split+encrypt? 
There are two separate operations here.  One is splitting the plaintext:
The other is generation of the splitting key.  I assume independent
generation of the splitting key both because it maximizes the total
keyspace and because it avoids the confusion that I believe is evidenced
by the above quoted paragraph.  To wit:  You have suggested generating
the split key with a one-way hash of the DES keys:
If the concatenation of the DES keys is 112 bits, then there are 2^112
possible values of the concatenation.  However, the hashing of this
value is not the first of the two encryptions; the splitting of the
plaintext is the first encryption, and the hash is merely a mechanism
for generating the splitting key.  The domain of KS is the determinant
of the size of the intermediate memory in a brute-force
meet-in-the-middle attack.
Furthermore, even for an independently generated splitting key, if the
size of the domain of KS is greater than the size of the domain of K0
or K1, then the DES-decrypted values can be stored as the intertext,
requiring no more memory than that required for decrypting double DES.
I suspect that you mean better, not worse [smiley deleted by censor].
I do not contest this claim, but I consider a more pertinent metric to
be the security of this scheme relative to that of double DES.  One
decomposite of the split+encrypt algorithm can be viewed as:
And an analogous double DES encryption is:
For the sake of argument, I'll assume that the domains of KS and K1 are
equal in size.  Thus, a brute-force meet-in-the-middle attack will
require the same number of encryptions and the same amount of memory in
both cases, although the amount of computational power required will be
somewhat less in the case of split+encrypt because the splitting is less
computationally intensive than DES.
However, the splitting algorithm is relatively simple, far more so than
DES.  It is unlikely that a brute-force approach is necessary to
cryptanalyze the splitter.  For example, consider the following
splitting algorithm:
This is particularly simple, and I chose it to be so for simplicity of
discussion.  Imagine that our cryptanalytic algorithm begins as follows:
Decrypt first block of ciphertext with each possible DES key; check to
see if the resulting intertext could possibly have come from first block
of known plaintext; if so, store the key; continue.  Without looping
through all possible split keys, we can determine whether the intertext
could have come from the plaintext:
inside loop:
This greatly shortens the amount of memory required for the search,
making the algorithm much less secure than double DES.  You may respond
by suggesting improvements to the splitting algorithm, such as
multiple-bit dependency; but there are doubtless other weaknesses that
could be exploited.  I did not spend a lot of time on the above
technique; persons more qualified than I am, devoting serious time to
the problem, will certainly develop better cryptanylitic attacks.  I
think you will be very hard pressed to develop an algorithm anywhere
near as secure as DES.

@_date: 1995-01-27 09:45:54
@_author: John Douceur 
@_subject: Get ready to start breaking rocks for Herr Klinton 
This may have been intended tongue-in-cheek, but I think this is an excellent idea.  Think Phil would be interested in the position?  How could we get this going?
