
@_date: 2008-12-06 22:51:23
@_author: Jerry Leichter 
@_subject: Attacking a secure smartcard 
I've previously mentioned Flylogic as a company that does cool attacks  on chip-level hardware protection.  In  , they talk about attacking the ST16601 Smartcard - described by the  vendor as offering "Very high security features including EEPROM flash  erase (bulk-erase)".  The chip is covered by a metal mesh that, if cut  or shorted, blocks operation.  However, Flylogic reports:
"Using our techniques we call, bmagicb (okay, itbs not magic but webre  not telling), we opened the bus and probed it keeping the chip alive.   We didnbt use any kind of expensive SEM or FIB.  The equipment used  was available back in the 90bs to the average hacker!  We didnbt even  need a university lab.  Everything we used was commonly available for  under $100.00 USD.
This is pretty scary when you think that they are certifying these  devices under all kinds of certifications around the world."
                                                        -- Jerry

@_date: 2009-08-09 21:48:45
@_author: Jerry Leichter 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
Since people do keep bringing up Moore's Law in an attempt to justify  larger keys our systems "stronger than cryptography," it's worth  keeping in mind that we are approaching fairly deep physical limits.   I wrote about this on this list quite a while back.  If current  physical theories are even approximately correct, there are limits to  how many "bit flips" (which would encompass all possible binary  operations) can occur in a fixed volume of space-time.  You can turn  this into a limit based solely on time through the finite speed of  light:  A computation that starts at some point and runs for n years  can't involve a volume of space more than n light years in radius.   (This is grossly optimistic - if you want the results to come back to  the point where you entered the problem, the limit is n/2 light years,  which has 1/8 the spacial volume).  I made a very approximate guess at  how many bit-flips you could get in a time-space volume of a 100 light- year sphere; the answer came out somewhere between 2^128 and 2^256,  though much closer to the former.  So physical limits prevent you from  doing a brute force scan - in fact, you can't even enumerate all  possible keys - in 100 years for key lengths somewhere not much more  than 128 bits.
It's rather remarkable that such fundamental limits on computation  exist at all, but physics over the last 100 years - and especially  over the last couple of decades - has increasingly shown us that the  world is neither continuous nor infinite; it has solid finite limits  on almost everything.  Even more remarkable is that we've pretty much  reached some of those limits.  For any recently designed cryptosystem,  brute force is simply out of the question, and will remains so forever  (unless we are very much mistaken about physics).  Moore's Law as a  justification for using "something more" makes no sense.
As you point out, the story for advances in cryptographic theory is  much more complex and impossible to predict.  That cryptographic  advances would render the "safer" AES-256 at risk while AES-128  remains secure (for now) is something no one could have predicted,  though in retrospect some of the concerns about the key scheduling may  have been right.  All the protocols and standards out there calling  for AES-256 - it's obviously "better" than AES-128 because after all  256 is *twice as large* as 128! - were just a bunch of nonsense.  And,  perhaps, dangerous nonsense.
                                                        -- Jerry

@_date: 2009-08-11 19:47:54
@_author: Jerry Leichter 
@_subject: brute force physics Was: cleversafe... 
When the first results about exponential speedup of factoring came  out, people assumed that this was true in general.  But it isn't.  In  particular, simple search, where you have only an equality test so you  can't build a hash table or some kind of ordered structure - is O(N)  on a traditional computer - and O(sqrt(N)) on a quantum computer.  I'm  not sure what the current knowledge about what a quantum machine can  do for NP computations, but there's no "probably" here.
The physical arguments to which I was referring say *nothing* about  how the computation is done.  It can be a QM computation as well.
In any case, the simple search result above applies directly to brute  force:  For that problem, you only get a polynomial speedup anyway.
That's a ... bizarre point of view.  :-)  Should freedom from related- key attacks be part of the definition of a "secure" encryption  algorithm?  We should decide that on some rational basis, not on  whether, with care, we can avoid such attacks.  Clearly, a system that  *is* secure against such attacks is more robust.  Do we know how to  build such a thing?  What's the cost of doing so?  But to say it's an  *advantage* to have a weakness seems like some kind of odd moral  argument:  If you're hurt by this it's because you *deserve* to be.
                                                        -- Jerry

@_date: 2009-08-11 21:27:09
@_author: Jerry Leichter 
@_subject: Ultimate limits to computation 
It must be the summer weather or something.  I've received a whole  bunch of messages - mainly privately - that say either "Here's another  result that has a higher upper bound on computation" or "Here's a  design for a machine that exceeds your bound".  Both ignore (a) how  bounds work:  That fact that you have a weaker bound doesn't mean I  don't have a stronger one; (b) that impossibility results can exist in  physics, not just in mathematics.  True, the nature of such results  are a bit different, since all our current physical theories might  turn out to be wrong.  But, hey, maybe our understanding of  computation or even mathematics has some fundamental flaw, too.
The estimate on the limits to brute-force search are mine, based on a  *very* rough estimate that draws on the results in the following  paper:  (I haven't actually read the paper; my analysis was based on an  article I can't find that discussed the implications of this one.)
The basic summary of the author's result is:  "[T]he total number of  bits and number of operations for the universe does not exceed  O(10^123)."  I guessed about how this value scales (as the cube of the  time - one factor for time, two for the size of the light sphere you  can reach in that time; not 3 because the information content of space  goes up as the area, *not* the volume - a very weird but by now  standard result).
Now, my scaling technique may be completely flawed, or my computations  may be wrong.  Or the paper could be wrong.  (I wouldn't bet on it.)   Or the physics may be wrong.  (I *really* wouldn't bet on that.)  But  the fact that there are other bounds around that are not as tight, or  that one can describe a machine that would do better if there were a  way to realize it, aren't evidence for any of these.  Bounds can be  improved, and a description isn't a working machine.
In fact, the whole point of the article that I *did* read is that this  result should make use re-examine the whole notion of a "possible"  computation.  It's easy to describe a computation that would take more  than 10^123 steps.  Ackerman's function exceeds that for pretty small  input values.  We've traditionally said that a computation is  "possible" if we can describe it fully.  But if it couldn't be  realized by the entire universe - is that really a *useful* notion of                                                          -- Jerry

@_date: 2009-08-28 11:03:16
@_author: Jerry Leichter 
@_subject: "Defending Against Sensor-Snifo,ng Attacks on Mobile  Phones" 
Modern mobile phones possess three types of capabilities:
computing, communication, and sensing. While these capa-
bilities enable a variety of novel applications, they also raise
serious privacy concerns. We explore the vulnerability where
attackers snoop on users by sniffing on their mobile phone
sensors, such as the microphone, camera, and GPS receiver.
We show that current mobile phone platforms inadequately
protect their users from this threat. To provide better pri-
vacy for mobile phone users, we analyze desirable uses of
these sensors and discuss the properties of good privacy pro-
tection solutions. Then, we propose a general framework for
such solutions and discuss various possible approaches to
implement the frameworkbs components.
They have some suggestions, but feel the problems are deep and  probably not completely solvable.
                                                        -- Jerry

@_date: 2009-08-31 07:20:42
@_author: Jerry Leichter 
@_subject: Source for Skype Trojan released 
It can b...intercept all audio data coming and going to the Skype  process.b
Proof of concept, but polished versions will surely follow.
                                                        -- Jerry

@_date: 2009-09-01 22:55:31
@_author: Jerry Leichter 
@_subject: "Fed's RFIDiocy pwnd at DefCon" 
"NSA spooks gather for a colleaguebs retirement party at a bar. What  they  donbt know is that an RFID scanner is picking them out - and a  wireless Bluetoothwebcam is taking their picture.
Could that really happen? It already did.
(The Feds got a taste of the real world risks of RFID passports and  IDs at DefCon, the annual hacker conference. According to Wired  :
. . . federal agents at the conference got a scare on Friday when they  were told they might have been caught in the sights of an RFID reader.
The reader, connected to a web camera, sniffed data from RFID-enabled  ID cards and other documents carried by attendees in pockets and  backpacks as they passed a table where the equipment was stationed in  full view...."
                                                        -- Jerry

@_date: 2009-09-08 22:08:59
@_author: Jerry Leichter 
@_subject: "Fed's RFIDiocy pwnd at DefCon" 
Remember:  Before it's actually happened, any discussion is just  reckless speculation, rumor-mongering, or worse.
After it's actually happened, it's either (a) not a real issue; (b) a  major new attack that could not have been foreseen but that will be  dealt with immediately by top people.  Top people.
                                                        -- Jerry

@_date: 2010-03-25 21:42:36
@_author: Jerry Leichter 
@_subject: New Research Suggests That Governments May Fake SSL Certificates 
While the paper provides a nice analysis and description of the  situation, what surprises me most about it is ... that anyone was  surprised.  Hardware to support man-in-the-middle splicing of HTTPS  sessions has been available in the marketplace for several years.   They are sold by companies like Bluecoat who build appliances to  monitor incoming and outgoing traffic at the interconnection points  between corporate networks and the greater Internet.  They're sold as  means to monitor and control what sites can be accessed (they block  things like gambling sites, pornography - whatever the corporation  doesn't want its employees browsing from work) and also inspect the  data for auditing/information leakage control purposes.
In the corporate environment, where desktops/laptops are managed, the  way such a device is given the ability to do MitM attacks is  straightforward:  The corporation simply pushes a new root CA - for a  CA that actually lives inside the intercept device - into the  browser's pool.  The device can then generate and sign any certs it  needs to to wedge into any HTTPS session invisibly.  Even when the  corporation allows personal machines onto the network, it will often  require users to accept a corporate CA for access to internal sites.   Of course, since browsers only have one pool of CA's, once you've  accepted that CA, you've accepted invisible MitM attacks by the  monitoring device.
Since the techniques and hardware for doing this has been around for a  while, it should come as no surprise that someone would notice that  governments are another good market - in fact, one that tends to be  fairly price-insensitive.  It's distressing how much government  intrusion technology is basically relabeled corporate security/ compliance technology.
Governments may or may not be in a position to force CA's onto a  machine, so it would be natural for them to compel existing CA's, as  the paper rightly points out.
                                                        -- Jerry

@_date: 2013-10-16 13:58:31
@_author: Jerry Leichter 
@_subject: [Cryptography] /dev/random is not robust 
Backwards security is a prerequisite for building PFS, among other things.  Without it, if an attacker seizes your system, he can (in principle, but we're considering *potential capabilities*, not what we know how to do in detail today) "run the random number generator backwards", which would allow any keys that were created using the RNG - like those created in "secure" DH negotiations, for example - to be generated.  (But keep in mind the adage:  Attacks only get better.)
I'm amazed and disturbed by the nature of the responses to this paper.  They are *indistinguishable* from the typical PR blather we get from every commercial operation out there when someone reports a potential attack:  It's just theory, it can't be translated into practice, we have multiple layers of security so even if one of them can be attacked the others still protect you, yadda yadda yadda.
Yes, this is a theoretical attack.  Yes, the Linux RNG is much more complex than the attackers assume in their model.  (Complexity is a good thing?)  No, no one is likely to be able to be able to use the attack actually get much out of the Linux RNG.  But attacks only get better.  The fact is, the Linux RNG, like all the "stir entropy into the pool" RNG's out there, were developed in an essentially ad hoc fashion, without even a solid idea of what the desired properties of such a primitive are supposed to be.  This paper is a step along a path begun in 2005 by Barak and Halevi (the instant paper has extensive references), and, frankly, it's about time.  For such RNG's we've been in about the same position we were in for ciphers in the 1980's and even beyond, before a great deal of theoretical effort got us to characterizations like IND-CPA and all sorts of excellent work on tight reductions and concrete security.
There's always been a strain of anti-academy bias - even anti-intellectualism - in the OSS community.  It's highly undesirable.  There's good academic academic work, and there's bad academic work.  Even with the domain of good academic work, some is of practical interest today, and some isn't.  (And some that isn't of interest today may be tomorrow.)
This kind of "shoot the messenger" approach is just plain wrong.  Look at the definition of robustness they come up with and tell me what parts of it aren't things you'd *like* to get in your RNG, if you could.  Can you come up with anything beyond hand-waving to show that the Linux RNG actually provides you with those properties?  Suppose someone was able to build on the current paper's work and design a Linux RNG that actually *did* provide those properties, with performance comparable to what we have today?  That's how, for example, we've gotten beyond the old, ineffective Modes of Operation to modern ones that have actual security techniques.  Wouldn't it be nice to be able to make the same transition for RNG's?  How will we ever do so without the initial work of the theoreticians to provide a target to aim for?
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-10-16 17:10:00
@_author: Jerry Leichter 
@_subject: [Cryptography] /dev/random is not robust 
Can we separate the issue of actual issues in the Linux RNG from the responses to this paper?
It's been a long time since I worked on an RNG, for a product that ran on systems that provided nothing like /dev/random; our Linux version mixed in /dev/random, and our Windows system mixed in whatever Windows calls its random number provider.  I have no doubt that the Linux RNG is immensely better than the hack we were able to cobble together - not being in the kernel severely restricted our entropy sources.  From what I've seen of the Linux RNG, it does a pretty good job.  I'm not about to suggest any code changes, and frankly nothing in this paper suggests any either.  Following its guidelines would produce an entirely new RNG, which absolutely *no one should trust* until it's been kicked around in the community for a while.
My problem is *entirely* with some of the intemperate responses I've seen out there, which basically say "Oh, that's just some academic nonsense, pay no heed".  I've seen nothing of that sort from you, Theodore Ts'o, personally; while you clearly have an emotional attachment to the work you've done - don't we all? - your responses are mainly technical in nature, pointing out places where the paper misunderstood what the current implementation does.  However, you do let a bit of an attitude come through in remarks about "not caring about publishing papers".  As I said, there is good and bad academic work, and not even all the good academic work is actually useful.  As an example of good and useful academic work, I usually point to Phil Rogoway's papers.  While the papers themselves are highly technical and get into a great deal of detail, what comes out at the end are simple designs that can be, and have been, implemented.  If you implement one of Rogoway's designs, you know t
 here's a formal proof behind it.  You don't need to actually understand the proof - others have reviewed it and it's almost certainly solid.  Merge sort will run in O(n log n) for you whether you understand the theory or not.
I see the paper as valuable for proposing strong security definitions for "PRNG's with input", showing that neither Barak and Halevi's algorithm nor the Linux RNG's algorithm attain those definitions, but suggesting an algorithm that does.  The answer "well, yes, the Linux generator fails if its entropy sources are bad in a particular way, but we have entropy sources that aren't" misses the point.  At one time, not so very long ago, no one knew how to build a cipher secure against a known-plaintext attack.  Today, that's assumed.  A defense of a modern cipher as "well, we won't let anyone see the plaintext" isn't good enough.  (Even worse is the claim that "you can only see the state of the PRNG from root, and then there are other attacks".  This isn't even true - a Linux system frozen into a VM can't prevent anyone from reading that state if they want it hard enough.)
Just as they extended Barak and Halevi's definitions, others may well come along and "tune" theirs.  That doesn't make theirs, or Barak and Halevi's, work "wrong"; it just makes them incomplete.  Others will also eventually come along and produce better algorithms that attain the various definitions that the community comes to agree on.
I'm not sure how the whole business of entropy estimation feeds into this.  There are others who've criticized it as just guesswork.  Frankly, they have a point.  John Denker's work on Turbid provides a much more principled approach to the problem.  Still, the Linux kernel has to work with what it has.
Whether new, better, strong approaches will come along in the future is impossible to tell.  If they do, I would hope those engineering Linux and other systems will have the humility to admit that the direction they've been going was, as it turns out, incorrect, and that they should start again from scratch.  That's how science and engineering are supposed to work.
Random numbers are way too important to allow the kinds of dismissive responses we've been seeing.
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-10-17 13:53:11
@_author: Jerry Leichter 
@_subject: [Cryptography] /dev/random is not robust 
One answer to this question appears in the FIPS standards for RNG's.  At times, they've required a continuous on-line test of the numbers being generated, with automatic shutdown if the test fail.  These requirements almost certainly came from the hardware background of the FIPS standards.  For hardware, certain failure modes - stuck at 0/stuck at 1 are the most obvious; short cycles due to some internal oscillation may be another - are extremely common, and worth checking for.  For software-based deterministic PRNG's, such tests are mainly irrelevant - code doesn't develop such failures in the field.  As the FIPS standards were adjusted for a more software-based world, the requirement for on-line testing was dropped.
Looking through some old messages on the subject here on the Cryptography list, I found one from Francois Grieu back in July of 2010:
More recently, David Johnston, who I gather was involved in the design of the Intel on-chip RNG, commented in a response to a question about malfunctions going undetected:
Of course, with generators like the Linux /dev/random, we're in some intermediate state, with hardware components that could fail feeding data into software components.
My own view on this is that there's no point in testing the output of a deterministic PRNG, but the moment you start getting information from the outside world, you should be validating it.  You can never prove that a data stream is random, but you can cheaply spot some common kinds of deviation from randomness - and if you're in a position to "pay" more (in computation/memory) you can spot many others.  You have no hope of spotting a sophisticated *attack*, and even spotting code bugs that destroy randomness can be hard, but it's hard to come up with an example of an actual real-world hardware failure that would slip through.  So you might as well do the testing.
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-01 07:11:06
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
Well, sure.  But ... I find it hard to be quite so confident.
In practical terms, the vast majority of encrypted data in the world, whether in motion or at rest, is protected by one of two algorithms:  RSA and AES.  In some cases, RSA is used to encrypt AES keys, so an RSA break amounts to a bypass of AES.  If you want to consider signatures and authentication, you come back to RSA again, and add SHA-1.
This is not to say there aren't other techniques out there, or that new ones aren't being developed.  But to NSA it's clearly a game of numbers - and any kind of wedge into either of just two algorithms would expose huge amounts of traffic to interception.
Meanwhile, on the authentication side, Stuxnet provided evidence that the secret community *does* have capabilities (to conduct a collision attacks) beyond those known to the public - capabilities sufficient to produce fake Windows updates.  And recent evidence elsewhere (e.g., using a bug in the version of Firefox in the Tor Browser Bundle) has shown an interest and ability to actively attack systems.  (Of course, being able to decrypt information without an active attack is always the ideal, as it leaves no traces.)
I keep seeing statements that "modern cryptographic algorithms are secure, don't worry" - but if you step back a bit, it's really hard to justify such statements.  We *know*, in a sense, that RSA is *not* secure:  Advances in factoring have come faster than expected, so recommended key sizes have also been increasing faster than expected.  Most of the world's sites will always be well behind the recommended sizes.  Yes, we have alternatives like ECC, but they don't help the large number of sites that don't use them.
Meanwhile, just what evidence do we really have that AES is secure?  It's survived all known attacks.  Good to know - but consider that until the publication of differential cryptanalysis, the public state of knowledge contained essentially *no* generic attacks newer than the WW II era attacks on Enigma.  DC, and to a lesser degree linear cryptanalysis not long after, rendered every existing block cipher (other than DES, which was designed with secret knowledge of DC) obsolete in one stroke.  There's been incremental progress since, but no breakthrough of a similar magnitude - in public.  Is there really anything we know about AES that precludes the possibility of such a breakthrough?
There's a fundamental question one should ask in designing a system:  Do you want to protect against targeted attacks, or do you want to protect against broad "fishing" attacks?
If the former, the general view is that if an organization with the resources of the NSA wants to get in, they will - generally by various kinds of bypass mechanisms.
Of the latter, the cryptographic monoculture *that the best practices insist on* - use standard protocols, algorithms and codes; don't try to invent or even implement your own crypto; design according to Kirchoff's principle that only the key is secret - are exactly the *wrong* advice:  You're allowing the attacker to amortize his attacks on you with attacks on everyone else.
If I were really concerned about my conversations with a small group of others being intercepted as part of dragnet operations, I'd design my own small variations on existing protocols.  Mix pre-shared secrets into a DH exchange to pick keys.  Use simple steganography to hide a signal in anything being signed - if something shows up signed without that signal, I'll know (a) it's not valid; (b) someone has broken in.  Modify AES in some way - e.g., insert an XOR with a separate key between two rounds.  A directed attack would eventually break all this, but generic attacks would fail.  (You could argue that the failure of generic attacks would cause my connections to stand out and thus draw attention.  This is, perhaps, true - it depends on the success rate of the generic attacks, and on how many others are playing the same games I am.  There's no free lunch.)
It's interesting that what what little evidence we have about NSA procedures - from the design of Clipper to Suite B - hints that they deploy multiple cryptosystems tuned to particular needs.  They don't seem to believe in a monoculture - at least for themselves.
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-02 00:06:21
@_author: Jerry Leichter 
@_subject: [Cryptography] NSA and cryptanalysis 
I'll make just a couple of comments:
- Given the huge amount of material classified these days, SECRET doesn't seem to be a very high level any more, whatever its official definition.  TOP SECRET still means a great deal though.  But the really important stuff is compartmented (SCI), and Suite B is not approved for it - it has to be protected by unpublished Suite A algorithms.
- To let's look at what they want for TOP SECRET.  First off, RSA - accepted for a transition period for SECRET, and then only with 2048 bit moduli, which until the last year or so were almost unknown in commercial settings - is completely out for TOP SECRET.  So clearly they're faith in RSA is gone.  (Same for DH and DSA.)  It looks as if they are betting that factoring and discrete logs over the integers aren't as hard as people had thought.
The whole business of AES-128 vs. AES-256 has been interesting from day one.  Too many recommendations for using it are just based on some silly idea that bigger numbers are better - 128 bits is already way beyond brute force attacks. The two use the same transforms and the same key schedule.  The only clear advantage AES-256 has is 4 extra rounds - any attack against the basic algorithm would almost certainly apply to both.  On the other hand, many possible cracks might require significantly heavier computation for AES-256, even if the same fundamental attack works.  One wonders....
NSA also wants SHA-384 - which is interesting given recent concerns about attacks on SHA-1 (which so far don't seem to extend to SHA-384).
I don't want to get into deep conspiracy and disinformation campaign theories.  My read of the situation is that at the time NSA gave its approval to this particular combination of ciphers, it believed they were secure.  They seem to be having some doubts about RSA, DSA, and DH, though that could be, or could be justified as, ECC being as strong with much smaller, more practical, key lengths.
Now, imagine that NSA really did find a way in to AES.  If they were to suddenly withdraw approval for its use by the government, they would be revealing their abilities.  A classic conundrum:  How do you make use of the fruits of your cryptanalytic efforts without revealing that you've made progress?  England accepted bombing raids on major cities to keep their crack of Enigma secret.  So the continuation of such support tells us little.  What will be interesting to see is how long the support continues.  With work under way to replace SHA, a new version of the NSA recommendations will eventually have to be produced.  Will it, for example, begin a phase-out of AES-128 for SECRET communications in favor of requiring AES-256 there as well?  (Since there's no call so far to develop a cipher to replace AES, it would be difficult for NSA to recommend something else.)
It's indeed "a wilderness of mirrors", and we can only guess.  But I'm very wary of using NSA's approval of a cipher as strong evidence, as the overall situation is complex and has so many tradeoffs.
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-07 07:25:43
@_author: Jerry Leichter 
@_subject: [Cryptography] XORing plaintext with ciphertext 
The question is much more subtle than that, getting deep into how to define a the security of a cipher.
Consider a very simplified and limited, but standard, way you'd want to state a security result:  A Turing machine with an oracle for computing the encryption of any input with any key, when given as input the cyphertext and allowed to run for time T polynomial in the size of the key, has no more than an probability P less than (something depending on the key size) of guessing any given bit of the plaintext.  (OK, I fudged on how you want to state the probability - writing this stuff in English rather than mathematical symbols rapidly becomes unworkable.)  The fundamental piece of that statement is in "given as input..." part:  If the input contains the key itself, then obviously the machine has no problem at all producing the plaintext!  Similarly, of course, if the input contains the plaintext, the machine has an even easier time of it.
You can, and people long ago did, strengthen the requirements.  They allow for probabilistic machines as an obvious first step.  Beyond that, you want semantic security:  Not only shouldn't the attacking machine be unable to get an advantage on any particular bit of plaintext; it shouldn't be able to get an advantage on, say, the XOR of the first two bits.  Ultimately, you want so say that given any boolean function F, the machine's a postiori probability of guessing F(cleartext) should be identical (within some bounds) to its a priori probability of guessing F(cleartext).  Since it's hard to get a handle on the prior probability, another way to say pretty much the same thing is that the probability of a correct guess for F(cleartext) is the same whether the machine is given the ciphertext, or a random sequence of bits.  If you push this a bit further, you get definitions related to indistinguishability:  The machine is simply expected to say "the input is the result of apply
 ing the cipher to some plaintext" or "the input is random"; it shouldn't even be able to get an advantage on *that* simple question.
This sounds like a very strong security property (and it is) - but it says *nothing at all* about the OP's question!  It can't, because the machine *can't compute the XOR of the plaintext and the ciphertext*.  If we *give* it that information ... we've just given it the plaintext!
I can't, in fact, think of any way to model the OP's question.  The closest I can come is:  If E(K,P) defines a strong cipher (with respect to any of the variety of definitions out there), does E'(K,P) = E(K,P) XOR P *also* define a strong cipher?  One would think the answer is yes, just on general principles: To someone who doesn't know K and P, E(K,P) is "indistinguishable from random noise", so E'(K,P) should be the same.  And yet there remains the problem that it's not a value that can be computed without knowing P, so it doesn't fit into the usual definitional/proof frameworks.  Can anyone point to a proof?
The reason I'm not willing to write this off as "obvious" is an actual failure in a very different circumstance.  There was work done at DEC SRC many years ago on a system that used a fingerprint function to uniquely identify modules.  The fingerprints were long enough to avoid the birthday paradox, and were computed based on the result of a long series of coin tosses whose results were baked into the code.  There was a proof that the fingerprint "looked random".  And yet, fairly soon after the system went into production, collisions started to appear.  They were eventually tracked down to a "merge fingerprints" operation, which took the fingerprints of two modules and produces a fingerprint of the pair by some simple technique like concatenating the inputs and fingerprinting that.  Unfortunately, that operation *violated the assumptions of the theorem*.  The theorem said that the outputs of the fingerprint operation would look random *if chosen "without knowledge" of the coi
 n tosses*.  But the inputs were outputs of the same algorithm, hence "had knowledge" of the coin tosses.  (And ... I just found the reference to this.  See ftp://ftp.dec.com/pub/dec/SRC/research-reports/SRC-113.pdf, documentation of the Fingerprint interface, page 42.)
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-08 07:32:14
@_author: Jerry Leichter 
@_subject: [Cryptography] Why prefer symmetric crypto over public key crypto? 
Indeed, that was exactly what I had in mind when I suggested we might want to do without private key cryptography on another stream.
Not every problem needs to be solved on Internet scale.  In designing and building cryptographic systems simplicity of design, limitation to purpose, and humility are usually more important the universality.  Most of the email conversations I have are with people I've corresponded with in the past, or somehow related to people I've corresponded with in the past.  In the first case, I already have their keys - the only really meaningful notion of "the right key" is key continuity (combined with implied verification if we also have other channels of communication - if someone manages to slip me a bogus key for someone who I talk to every day, I'm going to figure that out very quickly.)  In the second case - e.g., an email address from a From field in a message on this list - the best I can possibly hope for initially is that I can be certain I'm corresponding with whoever sent that message to the list.  There's no way I can bind that to a particular person in the real world wit
 hout something more.
Universal schemes, when (not if - there's no a single widely fielded system that hasn't been found to have serious bugs over its operation lifetime, and I don't expect to see one in *my* lifetime) they fail, lead to universal attacks.  I need some kind of universal scheme for setting up secure connections to buy something from a vendor I never used before, but frankly the NSA doesn't need to break into anything to get that information - the vendor, my bank, my CC company, credit agencies are call collecting and selling it anyway.
The other thing to keep in mind - and I've come back to this point repeatedly - is that the world we are now designing for is very different from the world of the mid- to late-1990's when the current schemes were designed.  Disk is so large and so cheap that any constraint in the old designs that was based on a statement like "doing this would require the user to keep n^2 keys pairs, which is too much" just doesn't make any sense any more - certainly not for individuals, not even for small organizations:  If n is determined by the number of correspondents you have, then squaring it still gives you a small number relative to current disk sizes.  Beyond that, everyone today (or in the near future) can be assumed to carry with them computing power that rivals or exceeds the fastest machines available back in the day - and to have an always-on network connection whose speed rivals that of *backbone* links back then.
Yes, there are real issues about how much you can trust that computer you carry around with you - but after the recent revelations, is the situation all that different for the servers you talk to, the routers in the network between you, the crypto accelerators many of the services use - hell, every piece of hardware and software.  For most people, that will always be the situation:  They will not be in a position to check their hardware, much less build their own stuff from the ground up.  In this situation, about all you can do is try to present attackers with as many *different* targets as possible, so that they need to split their efforts.  It's guerrilla warfare instead of a massed army.
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-10 07:42:55
@_author: Jerry Leichter 
@_subject: [Cryptography] The One True Cipher Suite 
I'm not so sure I agree.  You have to consider the monoculture problem, combined with the threat you are defending against.
The large burst of discussion on this list was set off by Perry's request for ways to protect against the kinds of broad-scale, gather-everything attacks that Snowden has told us the NSA is doing.  So consider things from the side of someone attempting to mount this kind of attack:
1.  If everyone uses the same cipher, the attacker need only attack that one cipher.
2.  If there are thousands of ciphers in use, the attacker needs to attack some large fraction of them.
As a defender, if I go route 1, I'd better be really, really, really sure that my cipher won't fall to any attacks over its operational lifetime - which, if it's really universal, will extend many years *even beyond a point where a weakness is found*.
On the other hand, even if most of the ciphers in my suite are only moderately strong, the chance of any particular one of them having been compromised is lower.
This is an *ensemble* argument, not an *individual* argument.  If I'm facing an attacker who is concentrating on my messages in particular, then I want the strongest cipher I can find.
Another way of looking at this is that Many Ciphers trades higher partial failure probabilities for lower total/catastrophic failure probabilities.
Two things are definitely true, however:
1.  If you don't remove ciphers that are found to be bad, you will eventually pollute your ensemble to the point of uselessness.  If you want to go the "many different ciphers" approach, you *must* have an effective way to do this.
2.  There must be a large set of potentially good ciphers out there to choose from.  I contend that we're actually in a position to create reasonably good block ciphers fairly easily.  Look at the AES process.  Of the 15 round 1 candidates, a full third made it to the final round - which means that no significant attacks against them were known.  Some of the rejected ones failed due to minor "certificational" weaknesses - weaknesses that should certainly lead you not to want to choose them as "the One True Cipher", but which would in and of themselves not render breaking them trivial.  And, frankly, for most purposes, any of the five finalists would have been fine - much of the final choice was made for performance reasons.  (And, considering Dan Bernstein's work on timing attacks based on table lookups, the performance choices made bad assumptions about actual hardware!)
I see no reason not to double-encrypt, using different keys and any two algorithms from the ensemble.  Yes, meet-in-the-middle attacks mean this isn't nearly as strong as you might naively think, but it ups the resource demands on the attacker much more than on the defender.
Now, you can argue that AES - the only cipher really in the running for the One True Symmetric Cipher position at the moment - is so strong that worrying about attacks on it is pointless - the weaknesses are elsewhere.  I'm on the fence about that; it's hard to know.  But if you're going to argue for One True Cipher, you must be explicit about this (inherently unprovable) assumption; without it your argument fails.
The situation is much more worse for the asymmetric case:  We only have a few alternatives available and there are many correlations among their potential weaknesses, so the Many Ciphers approach isn't currently practical, so there's really nothing to debate at this point.
Finally, I'll point out that what we know publicly about NSA practices says that (a) they believe in multiple ciphers for different purposes; (b) they believe in the strength of AES, but only up to a certain point.  At this point, I'd be very leery of taking anything NSA says or reveals about it practices at face value, but there it is.
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-25 14:12:25
@_author: Jerry Leichter 
@_subject: [Cryptography] RSA recommends against use of its own products. 
In favor off ... who?
We already know that GCHQ is at least as heavily into this monitoring business as NSA, so British providers are out.  The French have been playing the "oh, we're shocked, shocked that there's spying going on" game - but they have a long history of their own.  It's been reported for many years that all Air France seats are bugged by the French security services and the information recorded has been used to help French economic interests.  And even if you don't think a particular security service has been going after in-country suppliers, recall decades of US spiking of the Swiss Crypto AG machines.
It's a really, really difficult problem.  For deterministic algorithms, in principle, you can sandbox the implementation (both physically and in software) and compare inputs and outputs to a specification.  That leaves you to worry about (a) holes in the specification itself; (b) physical leakage of extra information (Tempest-like).  Both of these can be dealt with and you can gain any degree of assurance you consider necessary, at least in principle.  Sure, someone can spike your hardware - but if it only does what the spec says it's supposed to do, what does that gain them?  (Storing some of your secrets within the sandboxed system does them no good if they can't get the information out.  Of course, physical security is essential, or your attacker will just walk the system, with all its contained information, out the door!)
For probabilistic algorithms - choosing a random number is, of course, the simplest example - it's much, much harder.  You're pretty much forced to rely on some mathematics and other analysis - testing can't help you much.
There are really no absolutes; you really have to think about who you want to protect yourself from and how much you are willing to spend, because there's no limit on how much you *could* do.  Build your own foundry?  Create your own circuit synthesis code?  You very quickly get yourself into a domain where only a handful of companies or countries can even begin to go.
My take on this:  I don't much worry about attacks against general-purpose hardware.  The difficulty of modifying a processor so that you can tell when it's implementing a cipher and then do something useful about it seems insurmountable.  The exception is when the hardware actually gets into the crypto game - e.g., the Intel AES extensions and the random number generator.  If you're going to use these, you need to do so in a way that's secure even if those features are spiked - e.g., use the random number generator only as one of a couple of sources.
Still, *much* more worrisome are the badly implemented, insecure extensions to allow remote control of the hardware, which are being discussed in a separate thread here.  These are really scary - there's no protection against an attacker who can send a magic packet to your network interface and execute code with full privileges.
Code, at least for symmetric cryptography primitives and modes, is simple enough that you can find it all over the place.  Realistically, the worst attacks against implementations these days are timing attacks.  Bernstein's ciphers have the advantage of being inherently secure against these, showing that this is possible (even if you don't necessarily trust his particular constructions).
Denker's ideas about how to get random numbers whose safety is based on physical principles are great.  You do have to be careful of the hardware and software you use, but since the hardware is designed for entirely different purposes (A/D sound converters) it's unlikely anyone has, or really could, spike them all.
It's the asymmetric algorithms and implementations that seem to be the most vulnerable.  They are complex and difficult to get right, much less to get both efficient *and* right, and protocols that use them generally need to be probabilistic - so "black box testing" isn't feasible.  At the same time, they have rich mathematical structures in which we know things can be hidden.  (In the symmetric case, the algorithms are generally have little mathematical structure, and we *assume* nothing can be hidden in there - but who can really say with absolute confidence.)  I had a long debate here earlier on this subject, and my own conclusions remain:  Use symmetric crypto as little as you possibly can.  (What would be really, really nice is something like DH key exchange without all the mathematical structure.)
                                                        -- Jerry
The cryptography mailing list
cryptography at metzdowd.com
