
@_date: 1995-08-23 19:25:00
@_author: Tatu Ylonen 
@_subject: The sorry state of non-US crypto 
Finland: ftp://ftp.funet.fi/pub/crypt
Sweden: ftp://ftp.sunet.se/pub/security/tools/crypt
Russia: ftp://ftp.kiae.su/pub/unix/crypto
Norway: ftp://ftp.unit.no/pub/unix/security
Australia: ftp://ftp.psy.uq.os.au/pub/Crypto
I have created a set of WWW pages at that contains a lot of material and references to cryptographic
software, algorithms, and related information available outside the
US.  I'll expand the pages as I get suggestions for more things to put
there.  The pages will probably be quite stable and I'll try to
maintain them, so it is possible to refer to those pages wherever
references are needed for crypto archives outside the United States.
    Tatu

@_date: 1995-08-26 18:21:39
@_author: Tatu Ylonen 
@_subject: proliferation of voicesystems 
Maybe someone could start writing an internet draft about "encrypted
voice transmission on the internet".  It should address several
  - compression methods, sampling rate differencies, encoding methods
  - encryption methods used for bulk data: at least IDEA, 3DES, DES
    (3DES and DES required, IDEA optional but recommended (for patent reasons))
  - key exchange and authentication methods.  One good model could be
    that used in Photuris (see the internet draft
    draft-ietf-ipsec-photuris-02.txt at e.g.     Photuris is essentially Diffie-Hellman followed by authenticating
    the other party via signing the exchange.  (Authentication is
    important to avoid man-in-the-middle attacks).
  - specification of the protocol for modem-to-modem connections
Provided that the compression method is patent-free, all of the
related crypto patents expire within about two years (assuming
something other than RSA can be used for the signatures - see the
Photurs draft).  (IDEA should be optional because its patent will not
expire in near future).
I think it would be a good idea to set up a mailing list for this.
    Tatu Ylonen

@_date: 1995-08-30 18:26:14
@_author: Tatu Ylonen 
@_subject: CIA & Espionage 
There was a fairly large article about this in Helsingin Sanomat, the
largest newspaper in Finland, some weeks ago.  It was quoted as being
originally from the New York Times.  (I have the clip saved at home
and can check the date if anyone is interested.)
I do find it rather shocking that the most powerful country in the
world sets industrial espionage as the primary task of their
intelligence services.
Now talking about those crypto restrictions worldwide and the real
reasons why the United States is driving them...
    Tatu Ylonen International Cryptography Pages - check

@_date: 1995-12-17 19:25:19
@_author: Tatu Ylonen 
@_subject: Encrypted telnet... 
A preliminary windows client for SSH was recently announced by Cedomir
Igaly .  I am also working on a windows client myself,
and expect early beta versions to be available in early January, and
an official version in late February.
[For more information on ssh, see     Tatu

@_date: 1995-12-18 14:35:04
@_author: Tatu Ylonen 
@_subject: Motorola Secure Phone 
I got the following from mjos at math.jyu.fi a couple of months ago.
Unfortunately I was unable to attend or listen on mbone.  Does anyone
know more about this?
[Free translation: The GSM encryption algorithm is of exactly that
type.  Chambers tried to have this particular presentation over an
year ago, but at that time the official side interfered and the
presentation was cancelled.  He has found holes in the algorithm that
can be used to make decryption (without key) very quick.
PS.  Those of us who can get access to mbone, can follow it live.]
    Tatu

@_date: 1995-07-12 07:52:34
@_author: Tatu Ylonen 
@_subject: ANNOUNCEMENT: Ssh (Secure Shell) remote login program 
Looking for a secure rlogin?
Want to deter IP-spoofing, DNS-spoofing, and routing-spoofing?
Want to run X11 connections and TCP/IP ports securely over an insecure network?
Worried about your privacy?
Then read this.
Introducing SSH (Secure Shell) Version 1.0
Ssh (Secure Shell) is a program to log into another computer over a
network, to execute commands in a remote machine, and to move files
from one machine to another.  It provides strong authentication and
secure communications over insecure channels.  Its features include
the following:
   o    Strong authentication.  Closes several security holes (e.g., IP,
   o    All communications are automatically and transparently
        encrypted.  Encryption is also used to protect against spoofed
        packets.
   o    X11 connection forwarding provides secure X11 sessions.
   o    Arbitrary TCP/IP ports can be redirected over the encrypted
        channel in both directions.
   o    Client RSA-authenticates the server machine in the beginning of
        every connection to prevent trojan horses (by routing or DNS
        spoofing) and man-in-the-middle attacks, and the server RSA-
        authenticates the client machine before accepting .rhosts or
        /etc/hosts.equiv authentication (to prevent DNS, routing, or IP
        spoofing).
   o    An authentication agent, running in the user's local workstation
        or laptop, can be used to hold the user's RSA authentication
        keys.
   o    Multiple convenience features fix annoying problems with rlogin
        and rsh.
   o    Complete replacement for rlogin, rsh, and rcp.
Ssh is freely available, and may be used by anyone (see the file
COPYING in the distribution for more details).  There is no warranty
of any kind, and patents may restrict your right to use this software
in some countries.
Ssh is currently available for anonymous ftp at the following locations
   ftp.funet.fi:/pub/unix/security/ssh-1.0.0.tar.gz
   ftp.cs.hut.fi:/pub/ssh/ssh-1.0.0.tar.gz
Please let me know if you willing to have your site act as a
distribution site.  (US sites warning: although this software was
developed outside the United States using information available in any
major bookstore or scientific library worldwide, it is illegal to
export anything containing cryptographic software from the United
States.  Putting this openly available for ftp in the US may make you
eligible for charges on ITAR violations, with penalties up to 10 years
in prison.  French and Russian sites warning: it may be illegal to use
or even posses this software in your country, because your government
wants to be able to monitor all conversations of its citizens.)
There is a WWW home page for ssh: There is a mailing list for ssh.  Send mail to ssh-request at clinet.fi
to get instructions (or mail directly to majordomo at clinet.fi with
"subscribe ssh" in body).
All official distributions of ssh are accompanied by a pgp signature
by the key "pub  1024/DCB9AE01 1995/04/24 Ssh distribution key
".  (Included below.)

@_date: 1995-07-12 12:16:58
@_author: Tatu Ylonen 
@_subject: ANNOUNCEMENT: Ssh (Secure Shell) remote login program 
Ssh has already been registered with IANA (Internet Assigned Numbers
Authority) as the name of the service.  I would rather not change it
without a compelling reason.  It is also easy to obtain from rsh by
replacing the r by s (which also makes for scp, sshd, and in future
maybe also sdist).  It is my understanding that CFS is in rather
limited use (especially outside the US), and the ssh shar extractor is
not widely used either (neither can be found from the archie database
at archie.funet.fi).  IETF has a thing called Site Security Handbook
that they abbreviate SSH, but it is probably sufficiently different
not to be confused.
The agent protocol can currently be used to forward a connection to
any program (which can mean device) that can perform RSA
authentication.  New authentication methods can be compatibly added
S/Key can be used by making skeysh you login shell.  Then you will
first be asked for a normal password (if any), and then for the
one-time password.  I did not want to incorporate skey functionality
directly into the software, because it is not clear to me if the
arrangements in use (file names, formats, algorithms) have stabilized
yet.  Also, there is less need for skey as no passwords are
transmitted in the clear.
Maybe, *maybe*, TCP/IP port forwarding could be used for this?  (I
don't know what CFS does because I have never seen CFS.)
    Tatu

@_date: 1995-07-12 14:17:00
@_author: Tatu Ylonen 
@_subject: ANNOUNCEMENT: Ssh (Secure Shell) remote login program 
I last configured and compiled ssh on Linux yesterday and had no
problems.  I have slackware 2.2.0.1, kernel 1.2.8, gcc-2.7.0.
Please include version numbers in your report.
    Tatu

@_date: 1995-07-13 15:15:12
@_author: Tatu Ylonen 
@_subject: Crisis Overload (re Electronic Racketeering) 
One motivation behind SSH is trying to make it a de-facto standard
replacement for rlogin and rsh.  That would make it very hard to
replace.  It provides important benefits in authentication and
protection against intruders - and as a side effect it provides hard
to break encryption for anyone.  Plus, it was created and is primarily
distributed *outside* the United States, in a country where none of
the algorithms are patented.  It can thus be openly available for
anyone, and is not limited by US export restrictions.  It currently
includes two algorithms that I know to be patented: RSA and IDEA.
IDEA can be eliminated from it without breaking compability if it
turns out necessary (and, several sources say that non-commercial use
of IDEA is permitted).  RSA is not patented anywhere but in the US,
and there it may be possible for most people to get away by using
There is more information at   The RFC
describes the protocol.
The current list of distribution sites includes:
   ftp.funet.fi:/pub/unix/security
   ftp.unit.no:/pub/unix/security    ftp.net.ohio-state.edu:/pub/security/ssh    ftp.kiae.su:/unix/crypto    ftp.cs.hut.fi/pub/ssh More sites are welcome.
    Tatu Ylonen

@_date: 1995-07-13 15:42:14
@_author: Tatu Ylonen 
@_subject: Crisis Overload (re Electronic Racketeering) 
I agree.  If you forgive me for again taking the opportunity to
advertise SSH, one goal was to make it as simple to use as possible.
To get all the benefits of encryption and most benefits of improved
authentication, the users need to know absolutely nothing in addition
to what they need to know with rlogin.  Plus, there are many
convenient features, such as automatic X11 forwarding (encrypted;
DISPLAY is set to point to a fake display), command exit status is
returned properly, etc.
Of course, rlogin and rsh are much less important applications for the
general public than e-mail.  I think the currently the most critical
problem areas are exactly e-mail and interactive messaging programs
(like irc, rwrite etc).
Most mail (at least on the internet) is currently propagated
automatically from the sending host to the receiving host.  A fairly
simple, 90% of the benefit at 10% of the effort solution could be to
have sendmail (or equivalent) encrypt all communications that go
through the network.  This would make electronic mass surveillance and
scanning difficult.  It is much more expensive (and dangerous
publicity-wise) to read messages by breaking into a computer system.
This kind of system could be installed without the user even being
aware that something like that is in use.  It is not a perfect
solution - some sites will not support encryption, and some messages
might get sent without it.  Still, the bulk of the messages would be
encrypted, and any really sensitive data could be additionally PGP (or
similar) encrypted.  The procotol and implementation would have to be
well made and established as internet standards.
    Tatu Ylonen For more information about SSH, see

@_date: 1995-07-13 16:03:36
@_author: Tatu Ylonen 
@_subject: Ssh security hole? 
(I'll forward your message to a couple of lists where it might be
of interest; the original message is at end.)
I think you are right in your analysis.  There is indeed a problem
with RSA authentication.  Basically what this means is that if you log
into a corrupt host, that host can at the same time log into another
host with your account (by fooling you to answer to the request)
provided that you use the same RSA identity for both hosts.
A workaround is to use a different identity for each host you use.
The default identity can be specified on a per-host basis in the
configuration file, or by -i options.
And, yes, I think the same problem might occur with client host
authentication.  Though, there you would still have to do IP-spoofing,
DNS spoofing or similar to get through (breaking RSA based host client
effectively reduces RhostsRSAAuthentication to conventional .rhosts
The protocol will need to be changed somewhat because of this.  I'll
think about it tomorrow and let you say you opinion about it.
    Tatu Ylonen I believe there is a serious problem with the RSA authentication
scheeme used in ssh, but then again I could be misreading the proposed
RFC.  Is the following really the case?
As I understand the protocol, here is what happens during SSH_AUTH_RSA
Suppose the holder of SKu, is allowed access to account U on machine B
(which holds SKb).  Both PKu and PKb are widely known.  In addition,
machine B has a session key, PKs, which changes every hour.  When U on
machine A wants to log into machine B, here's what I think happens
based on my reading of the RFC:
A -> B: A
B -> A: (PKb, PKs, COOKIE)
A -> B: (COOKIE, {{Kab}_PKs}_PKb)
A -> B: {U}_Kab
A -> B: {PKu}_Kab
B -> A: {{N}_PKu}_Kab
A -> B: {{N}_MD5}_Kab (*)
B -> A: access to acount U with all data encrypted by Kab.
The problem is, suppose U actually wanted to log into machine C, which
was maintained by an untrusted person.  The person maintaining C could
initiate a connection to B the minute U tried to log into C.  When
given a challenge {{N}_PKu}_Kbc, C could simply give this to A as the
challenge to respond to, and then forward the response to B.
To fix the problem, A must at the very least include B in the
response line marked (*).  I have reason to believe (after having just
seen a lecture on authentication), that you might even need to include
more.  A safe bet might be (but then again I am no expert):
A -> B: {(N, A, B, Kab)}_MD5
I think similar problems arise for the other authentication methods.
Other than that, though, I am really impressed by by ssh.  It's easy
to install and easy to use.  In fact, it is even more convenient to
use than standard rsh, because the X forwarding happens
Thanks for such a great package!

@_date: 1995-07-13 16:26:45
@_author: Tatu Ylonen 
@_subject: Anti-Electronic Racketeering Act of 1995 (fwd) 
Finland, as far as I know, does not have any restrictions on
encryption, and has a friendly population.  Finnish is indecipherable
at first, but almost everybody can speak English (at least the younger
population).  There is a big shortage of competent computer and
electronics engineers.  Nokia Telecommunications (a major mobile phone
manufacturer) for example would need much more competent people than
they can get - not to mention the smaller companies.
Finland has excellent network connections - typical ftp rates from the
US are tens of kilobytes per second (except at peak hours).  There is
a lot of competition among the internet service provides.  About
$20/months gets you 28.8k dialup ppp (1-2 hours/day at that rate, I
think).  Another provider charges about 5 cents per minute.  A leased
64k line is around $100/month.
The climate is nice during the summer (15-25 Celsius typical), and
cold during the winter.
Taxes are outrageous though, so you really had better check that
first.  But, the taxes include things like medical insurance, pension
insurance, etc., and are thus not directly comparable.
And of course, we are now a member of the European Union, which
worries me a little on this front...
(Sorry, I just couldn't resist the temptation :-)
   Tatu

@_date: 1995-07-15 08:02:37
@_author: Tatu Ylonen 
@_subject: Ssh "security hole": proposed fix 
I am thinking about the following solution to the issues pointed out
by David Mazieres.
These changes propose solutions to the following problems:
  - replay of password-authenticated sessions
  - corrupt server can use RSA authentication to log into another server
When the client receives SSH_SMSG_PUBLIC_KEY, it computes a 128 bit
(16 byte) value by converting the modulus of the public key into a
stream of bytes, msb first.  The cookie sent by the server is appended
to this stream.  Both sides compute the MD5 of the resulting stream.
This value will be called the "session id".
In the SSH_CMSG_SESSION_KEY message, the first 16 bytes of the session
key (before encryption) are xored with the 16 bytes of the session id.
This does not reveal plain text from the RSA-encrypted part, but binds
the encrypted session key to a specific cookie and server.  This
should eliminate the possibility of replay, because the cookie is
unique for each connection.
In all SSH_CMSG_AUTH_RSA_RESPONSE messages (used both in user and
client host authentication), append the session id to the decrypted
challenge before computing MD5.  The MD5 is computed from the
resulting 48 bytes.  This makes the response bound to the server
cookie and the server key, and should elinate using the same response
for another server.  (Faking the server key is hard, because the
client verifies that it matches the one stored in its database.)
If a server supports this revision of the protocol, it reports its
protocol version as 1.1.  If the server protocol version is 1.0, the
client displays a warning (recommending to update server software) and
uses the old protocol for compatibility.  The client reports the
protocol version that it will use.  The compatibility code will be
removed in a later release.  (The changes are easy to implement
I would like to receive comments on this.
    Tatu

@_date: 1995-07-23 04:45:41
@_author: Tatu Ylonen 
@_subject: ssh protocol 
People have also suggested using the Photuris protocol that is part of
the IP Security work being done at IETF
The basic idea behind the protocol goes roughly like this:
  1. Exchange session keys using Diffie-Hellman
  2. Each side sends a signature of the Diffie-Hellman exchange (the
     signature can be with any of a number of algorithms; RSA and
     Elliptic Curve systems have been defined).
If this were adapted to ssh, the protocol would look roughly like
  1. Exchange session keys using Diffie-Hellman
  2. Each side sends a signature of the Diffie-Hellman exchange by its
     host key
  3. RSA and Rhosts authentication requests would include a signature
     by the requesting key.
This would get rid of the server key and the need to regenerate it,
because the diffie-hellman exchange already prevents decrypting old
conversations.  The challenge-dialogs could be avoided (unless they
are needed for performance reasons to avoid unnecessary signature
One could also eliminate RSA in future and start using some other
public key cryptosystem if desired.  The Diffie-Hellman patent and the
generic public key patent expire in 1997; the RSA-patent does not
expire until about year 2000.
Anyway, this would be a major change that probably cannot easily be
made compatibly.  Maybe an incompatible ssh-2.x?  Anyway, I don't want
to rush into making major changes in the protocol.
I would very much like to hear comments on this approach.
    Tatu

@_date: 1995-11-09 08:33:11
@_author: Tatu Ylonen 
@_subject: Photuris Primality verification needed 
*NO*, because you have to break the 56-bit DES separately every time,
whereas doing the precomputation for the 512 bit prime is a one-time
job.  Once anyone has done the precomputation, *all* communications
will be open to whoever is in possession of the database.
I think there is good reason to believe that if the 512 bit prime is
allowed, it will be widely used, and even if it is found breakable, it
will not be easily changed (just think about the experience with Sun's
"secure" rpc, and how quickly their primes have been changed - and it
still has much narrower deployment than what is hoped for ipsec).
Let me include below a message I sent to Bill Simpson.  The remarks there apply equally well to organized criminals, large
corporations, and hostile governments.  Or, suppose some group manages
to get access to enough idle time, computes the database, and posts it
on the Internet.  I for one would be willing to contribute CPU time on
machines where I have access to help such a group, because I think it
is better that it is widely known and publicized when there is little
security and privacy.
Including the provision for the 512 bit prime is *HARMFUL* and
*DANGEROUS*.  Export control is not really an issue here, because if
companies in the United States cannot provide secure networking,
there are other companies in the world that can.
    Tatu Ylonen

@_date: 1995-11-15 03:41:48
@_author: Tatu Ylonen 
@_subject: NSA, ITAR, NCSA and plug-in hooks. 
Luckily, a lot of cryptographic materials are available outside the
United States (see e.g.  for pointers).
If the United States chooses to restrict export of IP security
products, it simply helps create a flourishing network security
and other communications industry in other countries.  There are
already several implementations of the IP security stuff abroad -
including at least one in the former Soviet Union.
    Tatu

@_date: 1995-11-21 20:46:35
@_author: Tatu Ylonen 
@_subject: NSA, ITAR, NCSA and plug-in hooks. 
PKZIP "encryption" is self-deception.  A program for cracking it,
PKCRACK, is widely available on the internet.  See e.g.
ftp.funet.fi:/pub/crypt/analysis.  [For information about internationally available cryptographic
software, see     Tatu Ylonen

@_date: 1995-11-22 05:39:27
@_author: Tatu Ylonen 
@_subject: Are there enough FBI agents to handle Digital Telephony????? 
On the other hand, if you only want to collect rough background
information about people who might become significant later, it is
enough to store the conversations in a computer (storage is orders of
magnitude cheaper than the man-time to listen to the tapes), and only
listen the tapes if the person becomes interesting.
Besides, computer technology is approaching the point where you can
eliminate the human from the link entirely, except for final
1. Speech recognition already works quite well.  There was an article
about a 20.000 word speaker-independent system a few years ago,
operating 1/7th of real time on an alpha workstation.
2. Automatic speaker recognition from voice works quite well if my
understanding is correct.  (Useful for picking up interesting
conversations for futher analysis when you get them from sources you
don't normally monitor).
3. Computers have been able to pick up potentially interesting
conversations by keywords for decades.  Also useful for picking up
interesting conversations for further analysis from sources you don't
monitor very actively.  (Of course, you can additionally use phone
numbers, mobile phone *phone* identification codes, etc.)
4. A lot of work is being done in classifying transcript based on
their content, on message understanding.  In other words, lot of the
analysis work can be automated now or in near future.  The computer
can then answer questions from the data and for example select
individuals for futher analysis based on complex criteria.
5. A lot of work is being done on data mining (i.e., finding new data
from small pieces of individual data in a database, such as purchase
logs, etc.).  This is one of the hot topics in database conferences
right now.
6. Research is being done in massive databases.  There was an
Intelligence Community research initiative a couple of years ago on
massive databases; if my memory serves me right, they were talking
about 2-3 *petabytes* (10^12) as the size of the final database (no,
it was not gigabytes and it was not terabytes).  I believe I still
have the announcement saved somewhere if somebody wants it.
All of these technologies are feasible now or in near future.
Then add a little spices: the hundreds of thousands of surveillance TV
cameras around (did you know that there is a computer system that can
recognize and look up 25 faces per second from a database of a million
faces - used to control football huligans and shoplifters for
instance, but has other uses as well), car movement records from
highway payment systems, purchase records obtained from
credit card companies, banks and retail chains, link up to medical
records, tax databases, employment records, etc.  Add full knowledge
of flight and other travel reservations, some fax, e-mail and telegram
Now, what have you got (besides effective tools for finding criminals)?
    Tatu

@_date: 1995-11-23 06:32:25
@_author: Tatu Ylonen 
@_subject: Design proposal: crypto-capable generic interface 
There is a patent by someone (I think it was IBM) on how to pack RSA
keys in small space.  I think they were putting them on the magnetic
strips that you have behind every card.  I'm afraid I don't have the
patent number saved, but I have seen the patent document myself at the
patent office.  No, I'm afraid I don't remember how they encoded it.
Using that method (or something similar), you could probably encode
the keys into acceptably short strings with S/KEY-style encoding.
    Tatu

@_date: 1995-11-23 06:44:09
@_author: Tatu Ylonen 
@_subject: Intelligence Community Massive Digital Data Systems Initiative 
Below is some information about the Intelligence Community Massive
Digital Data Systems Initiative.
 - new data 2 - 5 terabytes (10^12 bytes) per day
 - total size about 20 petabytes (20 * 10^15 bytes)
 - 300 terabytes on-line, the rest accessible in a few minutes
 - funding (for the research initiative, not for the final system):    3-5 million USD per year estimated for investments
Now, how much is 2 - 5 terabytes per day?
  - 20 - 50.000.000 jpeg images (100kB/image, relatively high-quality) per day
  - 20 - 50.000.000 minutes of GSM-quality phone intercepts per day
  - 1.000.000 - 2.500.000 minutes of compressed (256kbit/sec) video per day
  - 1.000.000.000 - 3.000.000.000 e-mail messages per day
  - you can continue the list; most available data sets turn out to be
    much smaller
How much is 20 petabytes?  Assuming you want to collect
information about 100.000.000 people worldwide, this makes 200
megabytes per person (on the average for each of those 100 million
200 megabytes per person on the average is quite a lot, since for many
of those people you probably don't have all that much data.  Maybe 90%
of the data for 10% of the people?  (Of course, in a database like this you might also have a lot of data
like aerial imaginery, satellite imaginery, economical information,
etc., so it is a little exaggarated to talk about all of it being on
individual people.)
The full text is below.
Crypto relevance?  Makes you think whether you should protect your data.
    Tatu
        gray at sfbay.enet.dec.com, livny at cs.wisc.edu, ragrawal at almaden.ibm.com,
        manola at gte.com, heiler at gte.com, dayal at hplabs.hpl.hp.com,
        shan at hplabs.hpl.hp.com, toby at almaden.ibm.com, reiner at ksr.com,
        jag at allegra.att.com, randy at allspice.berkeley.edu, mcleod at vaxa.isi.edu,
        nick at MIMSY.CS.UMD.EDU, ake at purdue.edu, laney at ccr-p.ida.org,
        darema at watson.ibm.com, grossman at math.uic.edu, dbusa at cs.wisc.edu,
        metadata at llnl.gov, jmaitan at mosaic.uncc.edu, whm at thumper.bellcore.com
Resent-To: dbworld-people at cs.wisc.edu
Comments: IF YOU REPLY TO THIS MESSAGE, BE SURE TO EDIT THE to: AND cc: LISTS.
 The dbworld alias reaches many people, and should only be used for
 messages of general interest to the database community.  Mail sent
 to dbedu goes to the subset of addresses with a .edu suffix; mail
 sent to dbusa goes to the subset of US addresses.  Please use
 the smaller lists when appropriate.  Requests to
 get on or off dbworld should go to dbworld-request at cs.wisc.edu.
Reply-To: (Susan L. Hanlon) Resent-Reply-To: (Susan L. Hanlon) Dear Colleague:
data (i.e., multi-terabytes or greater).  Issues such as scalability, design,
and integration need to be addressed to realize a wide spectrum of intelligence
systems ranging from centralized terabyte and petabyte systems comprised of many
large objects (e.g., images) to distributed heterogeneous databases that contain
many small and large objects (e.g., text).  The Community Management Staff's
Massive Digital Data Systems (MDDS) Working Group on behalf of the intelligence
community, is sponsoring a two day invitation-only unclassified workshop on the
data management of massive digital data systems with government, industry, and
Virginia.  The objective of the workshop is to make industry and academia aware
of intelligence community needs, stimulate discussion of the technical issues
and possible solutions, and identify potential research efforts that warrant
further investigation for possible government funding.  The amount of funding
estimated for investments is three to five million dollars per year over the
next 2-3 years.
characterize the magnitude of the problem and identify the major challenges. The needs, issues, and in some cases, lessons learned, were presented for
different data types including Imagery, Text, Voice, Video, and Multi-media. Enclosure 1, "Massive Digital Data System Issues", is an unclassified
description of the consolidated challenges.
related to the issues of the data management of massive digital systems
including (but not limited to) scalability, architecture and data models, and
database management functions.  The focus of the abstract should be on potential
solutions for the longer term research challenges (i.e., 5-10 years out) that
must be addressed today in order to effectively manage data of massive
proportions in the future.  The solutions need not be limited to proven
approaches today but can foster new approaches and paradigms.  Issues relating
to the storage media and analysis tools, while important to the intelligence
community, are not within the scope of the workshop.  Selection for attendance
will be based upon technical relevance, clarity, and quality of the proposed
Call for Abstracts								Page 2
2).  All submissions must be UNCLASSIFIED.  To allow enough time for proper
evaluation of each abstract, the deadline for submission is 01 December 1993. You will be notified of acceptance to attend by 17 December 1993.  Abstracts
should be forwarded to one of the following:
solutions in this area.
  1.  Massive Digital Data Systems Issues
  2.  Abstract Format
Enclosure 2
ABSTRACT FORMAT
Phone:					FAX:
Status:  (Research, Prototype, Operational)
Scope:  (Size of effort in terms of dollars and/or staff months;  Size of system
in terms of amount of data, number of databases, nodes, users, etc.)
Customer:  (if applicable)
Operational Use:  (if applicable)
Forward to one of the following:
MASSIVE DIGITAL DATA SYSTEMS ISSUES
EXECUTIVE SUMMARY
Future intelligence systems must effectively manage massive amounts of digital
data (i.e., multi-terabytes or greater). Issues such as scalability, design, and
integration need to be addressed to realize a wide spectrum of intelligence
systems ranging from centralized terabyte and petabyte systems comprising many
large objects (e.g. images) to distributed heterogeneous databases that contain
many small and large objects (e.g. text). Consequently, Massive Digital Data
Systems (MDDS) are needed to store, retrieve, and manage this data for the
intelligence community (IC). While several advances have been made in database
management technology, the complexity and the size of the database as well as
the unique needs of the IC require the development of novel approaches. This
paper identifies a set of data management issues for MDDS. In particular,
discussions of the scalability issues, architectural and data modeling issues,
and functional issues are given. The architectures for MDDS could be
centralized, distributed, parallel, or federated. The functions of MDDS include
query processing, browsing, transaction management, metadata management,
multimedia data processing, integrity maintenance, and realtime data processing.
Representing complex data structures, developing appropriate architectures,
indexing multimedia data, optimizing queries, maintaining caches, minimizing
secondary storage access and communications costs, enforcing integrity
constraints, meeting realtime constraints, enforcing concurrency control,
recovery, and backup mechanisms, and integrating heterogeneous schemas, are some
of the complex tasks for massive database management. The issues identified in
this paper will provide the basis for stimulating efforts in massive database
management for the IC.
1.0  INTRODUCTION	
1.1  The Challenge
The IC is challenged to store, retrieve, and manage massive amounts of digital
information. Massive Digital Data Systems (MDDS), which range from centralized
terabyte and petabyte systems containing many large objects (e.g., images) to
distributed heterogeneous databases that contain many small and large objects
(e.g., open source), are needed to manage this information. Although
technologies for storage, processing, and transmission are rapidly advancing to
support centralized and distributed database applications, more research is
still needed to handle massive databases efficiently. This paper describes
issues on data management for MDDS including scalability, architecture, data
models, and database management functions. Issues related to storage media,
analysis tools, and security while important to the IC are not within the scope
of this paper. The key set of data management issues for MDDS include:  1.2  Background
The IC provides analysis on current intelligence priorities for policy makers
based upon new and historical data collected from intelligence sources and open
sources (e.g., news wire services, magazines). Not only are activities becoming
more complex, but changing demands require that the IC process different types
as well as larger volumes of data. Factors contributing to the increase in
volume include continuing improvements in collection capabilities, more
worldwide information, and open sources. At the same time, the IC is faced with
decreasing resources, less time to respond, shifting priorities, and wider
variety of interests. Consequently, the IC is taking a proactive role in
stimulating research in the efficient management of massive databases and
ensuring that IC requirements can be incorporated or adapted into commercial
products. Because the challenges are not unique to any one agency, the Community
Management Staff (CMS) has commissioned a Massive Digital Data Systems Working
Group to address the needs and to identify and evaluate possible solutions.
1.3  Assumptions and Project Requirements
Future intelligence systems must provide a full suite of services for gathering,
storing, processing, integrating, retrieving, distributing, manipulating,
sharing and presenting intelligence data. The information to be shared is
massive including multimedia data such as documents, graphics, video, and audio.
  It is desired that the systems be adapted to handle new data types.  The goal is to be able to retain the data for potential future analysis in a
cost effective manner. The more relevant data would remain on-line, say for 5
years, organized with the most relevant data accessible in the least amount of
time. It is expected that 2 to 5 terabytes of new data has to be processed each
day.  Thus, the total size of the database (both on-line and off-line) could be
as large as 20 petabyes with about 300 terabytes of data stored on-line. It is
assumed that storage devices (primary, secondary, and even tertiary) for the
large multimedia databases as well as data pathways with the required capacity
will exist. The access times are about 5 seconds for the data less than a week
old, about 30 seconds for data under two months old, and on the order of minutes
for data up to 10 years old. 2.0  SCALABILITY ISSUES
A particular data management approach can be scaled to manage larger and larger
databases. That is, a database can often sustain a certain amount of growth
before it becomes too large for a particular approach. For example, more memory,
storage, and processors could be added, a new hardware platform or an operating
system could be adopted, or a different microprocessor could be used (e.g. using
a 32 bit microprocessor instead of a 16 bit microprocessor). Once the size of
the database has achieved its limit with a particular approach, then a new
approach is required. This new approach could be a new architecture, a new data
model, or new algorithms to implement one or more of the functions of the
database management system (DBMS), or a combination of these features.
Discussions of these three features are given below.  	Architectures:  The type of architecture impacts the size and response time
of the DBMS.  approaches to handle large databases.  Some architectures such as a the shared
nothing parallel architectures are scalable to thousands of processors, but will
have multiprocessor communication issues.  Current approaches need to be
assessed to determine their scalability limits.  New approaches may be required
for handling massive databases.
 	Data Models:  Data models which support a rich set of constructs are desired
for next generation database applications.  However, the search and access time
of the DBMS would depend on the data model used. For example DBMSs which support
complex data structures use large caches, access data through pointers, and work
well with large main memory in general, while DBMSs based on simpler data models
maintain index files and provide associative access to the secondary storage.
The limits of these models within the context of massive databases need to be
understood.  New or modified approaches may be required.
modified to handle massive databases.  For example, as the size of the database
increases, new approaches for  query optimization, concurrency control,
recovery, and backup, access methods and indexing, and metadata management will
be required.  The architectural, data modeling, and functional issues that need to be
addressed for MDDS will be elaborated in sections 3 and 4. 3.0  ARCHITECTURAL AND DATA MODELING ISSUES
3.1  Architectural Issues
This section describes some of the architectural issues that need to be
addressed for an MDDS. In the case of the centralized approach, a major issue is
managing the data transfer between the main memory and secondary storage. One
could expect the data that is a week old to be cached in main memory, the data
that is less than two months old to be in secondary memory, and data that is a
few years old to be in tertiary storage. In designing the data management
techniques (such as those for querying, updating, and transaction processing),
data transfer between the main and secondary memories needs to be minimized.
There is also a need to reflect patterns of use (e.g., in migrating items to
lower/higher levels of storage hierarchy). Another issue is the relationship
between the size of the cache and the size of the database. When one migrates to distributed and parallel architectures, a goal is to
maintain a larger number of smaller databases. It is assumed that processors and
storage devices are available. A  major issue is the communication between the
processors. In designing the data management mechanisms, an objective would be
to minimize the communication between the different processors. For example, in
the case of a join operation between several relations in a relational DBMS,
each fragmented across multiple sites, an issue is whether to merge all of the
fragments of a relation and then perform the join operation or whether to do
several join operations between the fragments and then merge the results to form
the final result. Different configurations of the distributed and parallel
architectures also need to be examined. For example, there could be
point-to-point communication between every processor, or the processors could be
arranged in clusters and communication between clusters is carried out by
designated processors. Another issue in migrating to a distributed architecture
is handling data distribution. For example, if the data model is relational,
then how could one fragment the various relations across the different sites? If the relations are to be replicated for availability, then how could
consistency of the replicated copies be maintained?  Another issue is what data
could be cached within the distributed system, how could data be cached, and for
how long could the cache be maintained. While distributed and parallel architectures are being investigated for managing
massive databases, federated architectures are needed to integrate the existing
different and disparate databases.  The existing databases could be massive
centralized databases or they could be distributed databases. Furthermore, they
could be relational, object-oriented and even legacy systems.  An issue in
heterogeneous database integration is developing standard uniform interfaces
which can be accessed via an integration backplane.  If the environment is a
federated one, where the nodes have some autonomy, then a major issue is the
ability to share each other's data while maintaining the autonomy of the
individual DBMSs. This is hard because cooperation and autonomy are conflicting
goals.  The techniques to implement the DBMS functions for data retrieval,
updates, and maintaining integrity have to be adapted or new approaches have to
be developed for federated architectures.  Extensible architectures are also being investigated for massive databases. With such architectures, DBMSs are extended with inferencing modules which make
deductions from data already in the database.  This way, one need not store all
of the data in the database explicitly. Instead, appropriate inference rules are
used to make deductions and derive new data.  This way the size of the database
is reduced.  The issues include determining what data is to be stored in the
database and what data is to be stored in the knowledge base manipulated by the
inferencing module, effective management of the knowledge base, and adapting the
functions of the DBMS to handle extensible architectures.  3.2  Data Modeling Issues
In selecting an appropriate data model for massive databases, several issues
must be considered. Providing a data model powerful enough to support the
representation of complex data must be addressed.  For example, with a
multimedia document, one may need to devise a scheme to represent the entire
document in such a way to facilitate browsing and updating. Since the age of a
document could be used to move it between different storage media, it is
desirable for the data model to support the representation of temporal
constructs. The representation of different types of multimedia devices and
grouping of documents are also important considerations in selecting a data
model. The data model chosen has an impact on the techniques to implement the
functions of a DBMS. For example, DBMSs based on some models use associative
access while those based on some other models use pointer traversal. In migrating to a distributed/parallel architecture, if it is assumed that the
data model is the same for all databases, then a major issue is whether it is
feasible to provide a conceptual view of the entire massive database to the
user. However, in the case of a federated architecture, since it is generally
assumed that the individual data models are different, several additional issues
need to be considered. For example, could the users have a global view of the
massive database or could they have their own individual views?  In either case,
it would be desirable for the users to access the distributed databases in a
transparent manner. If a global view is enforced, the query processor could
transform the queries on the global view to the views of the individual
databases. If each user has his own view, then the query processor could
transform the users view into the views of the individual databases. Other
issues for a federated architecture include the representation of the individual
schemas (which describe the data in the databases), determining which schemas to
be exported to the federation, filtering appropriate information from the
schemas at different echelons, integrating the schemas to provide a global view,
and generating the external schemas for the users. In integrating the different
schemas, the semantic and syntactic inconsistencies between the different
representations need to be resolved. For example, the address in database A
could include the house number and the street name while in database B it could
just be the city and the state. 4.0  FUNCTIONAL ISSUES   The techniques to implement the functions of MDDS will be impacted by the
architectures and data models as well as requirements such as integrity and
multimedia data processing. Therefore some of the functional issues have already
been addressed in section 3. This section provides a more detailed overview of
the functional issues.  First the basic functional issues for MDDS (such as
issues on query processing and transaction management) will be discussed and
then the impact of maintaining integrity, realtime processing, and multimedia
data processing will be given. 4.1 Querying, Browsing, and Filtering
The query operation is a means by which users can retrieve data from the
database. Closely related is the browsing operation where users traverse various
links and subsequently scan multiple documents either sequentially or
concurrently. To determine if the new information warrants viewing by the
analyst and/or to enforce access control, automatic filtering of the data is
needed. Some issues in query management for massive databases are using an
appropriate language for specifying queries and developing optimization
techniques for the various operations involved in a query. The goals are to make
it easier for users to formulate queries and also to minimize data transfer
between primary and secondary storage. Query management in a federated environment must provide the means for
formulating and processing queries seamlessly and efficiently. This involves
designing an interface for formulating queries over multiple sources. There is a
need for query optimization, in order to prevent degradation in performance in
the distributed system. In addition to determining the execution strategy for a
query, query optimization techniques could also determine which portion of the
query processing is to remain under direct and unshared control at the analyst's
workstation. Methods need to be developed for browsing the integrated
information space and for displaying results obtained from multiple sources.
Finally, data from local databases have to be filtered according to the various
constraints (such as security constraints) and enforced before sending it to the
remote sites. Query processing algorithms in an extensible architecture need to incorporate
inferencing techniques.  The usefulness of inferencing techniques for
intelligence applications can best be illustrated with a simple example. Suppose
parts A, B, C and D are needed to build a nuclear weapon, and also suppose that
the following constraint is enforced:  " if three of the four parts are shipped
to country X, then the fourth part should not be shipped to X."   Therefore, if
parts A, B, and C are already shipped to X and there is a request from X for
part D, then the inferencing module will determine that this part cannot be
shipped.  An issue in developing an inference module is determining the
deduction strategies to be implemented. These strategies could be just logical
deduction or could include more sophisticated techniques such as reasoning under
uncertainty and inductive inference. With most inference strategies one runs
into the problem of an infinite loop; therefore appropriate time limits must be
enforced to control the computation.
In general, the issues to be addressed in query management will include:
4.2  Update Transaction Processing
Multi-user updates are supported in general to improve performance. The goal is
for multiple users to be able to update the database concurrently. A major issue
here is ensuring that the consistency of the database is maintained. The
techniques that ensure consistency are concurrency control techniques. Often
update requests are issued as part of transactions. A transaction is a program
unit that must be executed in its entirety or not executed at all. Therefore, if
the transaction aborts due to some error, such as system failure, then the
database is recovered to a consistent state.
Several concurrency control algorithms have been designed and developed for
different environments. Some algorithms are suitable for short transactions in
business processing applications and some others are suitable for long
transactions which often involve multimedia data. To handle long transactions
efficiently, weaker forms of consistency conditions have been formulated.
Several recovery techniques have also been developed to maintain the consistency
of the database. If the transaction is long, then the log files that record the
actions of the transaction may be quite large. Efficient management of log files
becomes an issue. As the size of the database increases, a transaction would
take a longer time for execution. Adapting the concurrency control and recovery
algorithms or developing new algorithms to work with the massive databases
becomes an issue.  Update transaction processing gets more complicated in distributed and federated
environments. For example, if replicated copies are to be maintained, then
making them consistent will have an impact on the performance. Therefore, an
issue here is whether to maintain strict consistency or select a subset of the
copies and make them consistent immediately so that the remaining copies could
be updated at a later time. One of the problems with a federated environment is
the different concurrency control and recovery algorithms used by the individual
DBMSs. In such a situation synchronizing the different techniques becomes a
major issue. 4.3  Access Methods and Index Strategies
To enhance the performance of query and update algorithms, efficient access
methods and index strategies have to be enforced. That is, in generating
strategies for executing query and update requests, the access methods and index
strategies that are used need to be taken into consideration. The access methods
used to access the database would depend on the indexing methods. Therefore
creating and maintaining appropriate index files is a major issue in a DBMS.
Usually, the size of the index file grows with the size of the database. In some
cases, the index file could be larger than the database itself. Some of the
issues include determining what type of indexes are to be maintained for massive
databases.  Is it feasible to have dense indexing where there is an entry in the
index file for every entry in the database?  If so, the index file could have as
many entries as there are in the database. Is it better to have sparse indexing
so that the size of the index file could be reduced?  If so, is there a strategy
to determine which entries in the database are to be indexed?  For multimedia
data, indexing could be done not only by content but by type, language, context
(i.e., where, how, when it was collected), author (i.e., for documents), and
speaker (i.e., for voice). The challenge is how to index and to provide improved
mechanisms for extraction of the information used for indexing.   For example,
the ability to automatically index voice is desired.  Additionally, the ability
to index voice and video (with associated voice) with their transcriptions
(i.e., time alignment) is necessary.
Various storage structures have been proposed. These include B-Trees and
Parent-Child links. The question is, are these methods suitable for massive
databases? Voice and video data require segmentation into logical units for
storage and access. Additionally, the ability for automatic segmentation within
documents of embedded drawings and figures and their interpretation (via
seamless integration with image handling tools) is needed. Other challenges
include providing user transparent hierarchical storage management (i.e., store
the most relevant or most recent information on the fastest media) and the
ability to reposition data in the storage hierarchy based upon changing
importance, migration mechanisms for transferring information to newer storage
media or a new architecture (failure to do so can lead to exorbitant costs to
maintain discontinued storage media drives or inaccessible data), archival
technology/policies for older/less important information, and synchronization of
information distributed across multiple repositories
Compression can decrease the costs of storage and transmission especially for
the larger objects such as vector and raster spatial data types, voice, imagery,
and video. Real-time conversion of heterogeneous voice and video compression and
file formats in network broadcasts/multicasts is an issue.  For imagery, a
capability such as pyramidal decomposition for providing reduced resolution
images is needed for browsing purposes.
4.4  Managing the Metadata
The metadata includes a description of the data in the database (also referred
to as the schemas), the index strategies and access methods used, the integrity
mechanisms enforced, and other information for administrative purposes. Metadata management functions include representing, querying, and updating, the
metadata.  In massive databases, if the metadatabase is much smaller than the
database, then the traditional techniques could be applied to manage the
metadata. If the metadatabase becomes massive, then new techniques need to be
developed. An issue here is whether the techniques for massive databases could
be applied for massive metadatabases also. Support for schema evolution is
desired in many new generation applications.  For example, the structures of the
entities in the database could change with time.  An entity could acquire new
attributes or existing attributes could be deleted. The metadata needs to be
represented in a manner that would facilitate schema evolution. That is,
appropriate models to represent the metadata are desired. Since the metadata has
to be accessed for all of the functions of a DBMS, the module that is
responsible for accessing the metadata needs to communicate with all the other
modules. Efficient implementation of this module is necessary to avoid
performance bottlenecks. Certain types of metadata, such as the schemas, are usually accessible to the
external users.  An issue here is whether to provide a view to the users that is
different from the system's view of the metadata.  For example, a different
representation of the metadata could be sued for the users.  Also, if the
metadatabase is massive, then subsets of it could be presented to the users.    4.5  Integrity
Concurrency control and recovery issues discussed in section 4.3 are some of the
issues that need to be dealt with in order to maintain the integrity  (i.e.
consistency) of the database. Other types of integrity include maintaining the
referential integrity of entities and enforcing application dependent integrity
constraints. Referential integrity mechanisms must ensure that the entities
referenced exist. The question is, how could the references to an entity be
deleted when an entity itself is deleted?  If the databases are massive, then
there will probably be more references to the deleted entity. Deleting all these
references in a timely manner is an issue.
In the case of application specific integrity constraints, they could trigger a
series of updates when one or more items in the database gets updated. Again, as
the size of the database increases, the number of updates that are triggered
could also increase. The issue here is ensuring that the updates are carried out
in a timely manner. 	
4.6  Realtime / Near Realtime Processing
Within a massive digital data system, the challenges of realtime or near
realtime processing will be compounded.  For realtime or near realtime
applications, timing constraints may be enforced on the transactions and/or the
queries. In the case of a hard realtime environment, meeting the timing
constraints may cause the integrity of the data to suffer. In the case of soft
realtime constraints (also referred to as near real-time), there is greater
flexibility in meeting the deadlines. The issues for real-time processing
 	If a transaction misses its deadline, then what are the actions that could be
taken?   determine whether the transaction should continue after it misses its deadline?
present?  possible to maintain the consistency of the replicated copies and still meet the
timing constraints?  4.7  Multimedia Data Processing
By nature, multimedia data management has to deal with many of the requirements
for indexing, browsing, retrieving, and updating of the individual media types. Implementing multimedia data types will require new paradigms for representing,
storing, processing, accessing, manipulating, visualizing, and displaying data
from various sources in different media. One of the major issues here is
synchronizing the display of different media types such as voice and video. Other issues include selecting/developing appropriate data models for
representing the multimedia data and developing appropriate indexing techniques
such as maintaining indexes on textual, voice, and video patterns.  For example,
the ability to index voice and video simultaneously may be desired.  In addition to the manipulation of multimedia data, frameworks for the
integration of multimedia objects as well as handling different granularity of
multimedia objects (i.e., 1 hour video clip versus a spreadsheet cell) need to
be considered. A flexible environment has to be provided so that the linked and
embedded distributed multimedia objects can accommodate geographic/network
changes.  Finally, the data manipulation techniques as well as the frameworks
need to be extensible to support new and diverse data types.  4.8  Backup and Recovery
On-line backup procedures are being used for massive databases. This is because
off-line procedures will consume too much time for massive databases. Even if
the backup procedures are carried out on-line, the system could be slowed down
and therefore the performance of other data management functions would suffer.
The issue here is to develop improved techniques for backup so that it will not
impact functions such as querying, browsing, and updating. Recovery issues for transaction management were discussed in section 4.3. Other
recovery issues include whether to maintain multiple copies of the database, and
if so, the number of copies to be maintained, and whether the checkpointing,
roll-back and recovery procedures proposed for traditional databases could be
used for massive databases or is there a need to develop special mechanisms?
5.0  SUMMARY
Massive digital data systems will require effective management, retrieval, and
integration of databases which are possibly heterogeneous in nature. Achieving
this concept of massive intelligence information systems will require new
technologies and novel approaches for data management. While hardware is rapidly
advancing to provide massive data storage, processing, and transmission, the
software necessary for the retrieval, integration, and management of data
remains an enormous challenge.
This paper has identified a set of issues for managing the data in massive
digital data systems with a focus on intelligence applications. First, an
overview of the current approaches to data management and the scalability of the
current approaches were discussed  Then some architectural and data modeling
issues were given. Finally, a discussion of the issues for the various functions
of MDDS were given. The set of issues identified is by no means considered a
complete list. As the progression of research, prototyping, and deployments
continue, new or hidden challenges will arise.

@_date: 1995-10-01 17:05:45
@_author: Tatu Ylonen 
@_subject: DNS Security ( was Re: NetScape's dependence upon RSA down...) 
Does anyone know if this is available outside the United States?  If
it is, please let me know.  I'd like add a link to it from the WWW
pages at     Tatu Ylonen

@_date: 1995-10-06 12:08:18
@_author: Tatu Ylonen 
@_subject: Council of Europe on Crypto: Finland 
People have been digging the background of the Council of Europe
recommendations here in Finland.  It has turned out that Finland was
represented an assistant director of the Criminal Police who is known
as a supporter of extremely broad powers for the police.  At the
meeting, he has apparently presented his personal opinion as the
official opinion of Finland; later the Council unanimously decided to
recommend banning strong crypto.  I hear a representative from Norway
had apparently tried to speak some sense at the meeting, but had
quickly been quieted by the others.
It has also turned out that there have been studies on legislating
cryptography and related issues in Finland during the past two years,
but without concrete plans.  The officials in charge of these issues
were not aware of what was happening in Strasburg, and what Finland
apparently has presented there certainly does not represent a
concensus within the government.  Things appear to be developing in
promising directions, but much more work is still needed until things
are on the right track.
I strongly urge people in other European countries to contact their
government officials, the press, and use other possible channels to
dig out what exactly was happening and why, and make the officials and
politicians understand the other issues that are related (computer
security, universal surveillance, ability of independent political
groups to function, trade secrets, etc).
Professors and other persons in expert positions are important sources
of information to the government in the preparatory process, and you
should provide the officials with experts to consult on the issues.
At least here it has turned out that preparations were being carried
out with very few independent experts, input coming almost exclusively
from the law enforcement side.
    Tatu Ylonen

@_date: 1995-10-19 13:19:45
@_author: Tatu Ylonen 
@_subject: Motherload of Crypto Site... 
There are a lot of cryptographic software packages available outside
the United States.  The International Crypto WWW Pages at
 contain a large index of available
packages and algorithms.
    Tatu

@_date: 1995-10-20 10:56:21
@_author: Tatu Ylonen 
@_subject: responce to graphic encryption replies 
Uhhuh, 32 bit keys?  Now you have really convinced me that it is not
secure even against the neighbours bright young kids.
    Tatu Ylonen

@_date: 1995-10-22 16:49:15
@_author: Tatu Ylonen 
@_subject: Encrypted TCP Tunneler 
You are aware that RSADSI claims they have exclusive licensing rights
for DSA?
Are you familiar with ssh [  It has many of
the features that you are planning.
    Tatu

@_date: 1995-10-24 09:13:40
@_author: Tatu Ylonen 
@_subject: Encrypted TCP Tunneler 
You are quite right here; some kind of account is needed on the
forwarder machine.  (It can, though, be an account without password
and a login shell that just sleeps.)  But anyway, TCP port forwarding
is not its main function.  (I don't think the packetizing is such a
major overhead though - it currently transfers around 400kbytes/sec
over ethernet encrypted with RC4 between P90 machines.)
The reasons for this key exchange are mostly historical.  If I was
starting the implementation now, I would use DH + signatures.  The
performance difference is not very big, but DH + signature would be simpler.
    Tatu

@_date: 1995-11-01 09:43:02
@_author: Tatu Ylonen 
@_subject: Cryto article in SJ Mercenary 
I don't think ITAR is very relevant here.  After all, there are dozens
of RSA implementations available from outside the US, and they are not
patent-restricted like in the US.  It is really much easier to use and
get RSA *outside* the US than inside.  (For some pointers, see
"  Besides, as far as I understand, one
of the RSA inventors wasn't even a US citizen...
    Tatu

@_date: 1995-09-03 13:32:00
@_author: Tatu Ylonen 
@_subject: SSLRef (SSLtelnet) 
SSLeay (Eric Young's free SSL implementation from Australia) is
available from ftp.psy.uq.oz.au:/pub/Crypto/SSL.
For information on other cryptographic software available outside the
United States, see     Tatu

@_date: 1995-09-11 06:18:28
@_author: Tatu Ylonen 
@_subject: Voice Encryption 
There are several packages references in the software section of
  It also contains links to ftp sites
outside the United States.
    Tatu

@_date: 1995-09-12 09:17:52
@_author: Tatu Ylonen 
@_subject: Elliptic Curve Public Key Crypto available 
See It was in Italy (ftp.dsi.unimi.it) earlier today...
    Tatu

@_date: 1996-02-13 09:17:00
@_author: Tatu Ylonen 
@_subject: Free end-to-end encryption code? 
Sounds like something that could be directly done with ssh
[ using TCP/IP forwarding.  I've myself used
it to encrypt the connection to the smtp port on a remote server.
I configured sendmail to use "localhost" as the major relay host,
disabled the sendmail daemon, and ran sendmail from cron to process
the queue every now and then.  Incoming mail was fetched via ssh from
a remote file server using a couple of small scripts.
    Tatu

@_date: 1996-02-14 07:58:43
@_author: Tatu Ylonen 
@_subject: Free end-to-end encryption code? 
There is already a windows version of ssh, though it is not very
stable.  I am myself working on an "official" Windows version, and it
should be available after a few weeks.
    Tatu

@_date: 1996-01-21 15:06:32
@_author: Tatu Ylonen 
@_subject: NSA vacuuming down Internet traffic 
To me it does sound completely feasible (you don't need very good
accuracy).  I've personally run packet filters (for statistical
purposes only) on busy 10-mbit ethernets using BPF, FreeBSD, and 486
or pentium machines.  They easily keep up with little packet loss.
I understand T3 is 34 mbits, so only three times faster.  No problem
to optimize that much by specially written software, especially if you
can do some of the low level stuff in hardware.
As for the keyword search problem, it would easily be possible to scan
much of the data (say, tcp ports smtp, nntp, login, exec, ident) in
real time against a million-phrase dictionary (containing keywords,
e-mail addresses, names, abbreviations, etc.).  If there are
performance problems, you can first limit by
source/destination/protocol/port.  Only intercepts (e.g., entire tcp
connections) that pass this initial screening are passed on to other
machines for more complicated analysis.
Note also that many parts of the filtering problem parallelize quite
nicely.  For example, you can split the traffic to a number of
machines based on the value of the numerically smaller of the
source/destination addresses.
I don't see any technical problems in doing large-scale internet
monitoring.  The equipment needed is even cheap enought to be done by
motivated amateurs/individuals, assuming they can get a copy of the
raw data from the T3.
This is one of the reasons why strongly encrypting internet data is so
    Tatu
See  for information on SSH, the secure remote
login program.
See  for information cryptography available
to anyone worldwide.

@_date: 1996-01-30 11:56:57
@_author: Tatu Ylonen 
@_subject: FV Demonstrates Fatal Flaw in Software Encryption of Credit Cards 
I find this kind of marketing extremely inappropriate.  PLEASE STOP IT.
The "flaw" you describe is quite obvious.  The key issue is whether
untrusted code gets executed on your computer, and what your risk
model is.  When balancing the probable damage due to this risk against
the benefits of easily obtained software (from sources one chooses to
trust) and easy electronic commerce, at least I find that this "flaw"
is no cause for special alarm.  The risk is analogous to that posed by
computer viruses.
I detest people causing public hysteria to advance their private
commercial goals.
    Tatu Ylonen ------- start of forwarded message (RFC 934 encapsulation) -------
Received: from relay3.UU.NET by hutcs.cs.hut.fi with SMTP id AA23837
  (5.65c8/HUTCS-S 1.4 for ); Mon, 29 Jan 1996 22:29:47 +0200
Received: from toad.com by relay3.UU.NET with SMTP Received: by toad.com id AA21824; Mon, 29 Jan 96 12:07:29 PST
Received: from zloty.fv.com by toad.com id AA21818; Mon, 29 Jan 96 12:07:22 PST
Received: from nsb.fv.com (nsb.fv.com [152.160.80.42]) by zloty.fv.com (8.7.3/8.7.3) with SMTP id MAA00050 for ; Mon, 29 Jan 1996 12:07:38 -0800 (PST)
Received: by  nsb.fv.com (4.1/SMI-4.1)
Received: from Messages.8.5.N.CUILIB.3.45.SNAP.NOT.LINKED.nsb.fv.com.sun4.41
          via MS.5.6.nsb.fv.com.sun4_41;
          Mon, 29 Jan 1996 15:07:46 -0500 (EST)
Message-Id: Precedence: bulk
Sender: owner-cypherpunks at toad.com
[My apologies in advance if you see several copies of this message.  I
am posting this fairly widely due to the severity and importance of the
problem described.]
As you may already have heard via the popular press, First Virtual
Holdings has developed and demonstrated a program which completely
undermines the security of every known credit-card encryption mechanism
for Internet commerce.  This is a very serious matter, and we want to
make sure that the Internet community is properly informed about the
nature of the problem that we have uncovered, and the manner in which we
have made the information known.  In this (unavoidably lengthy) post, I
will try to explain the nature of the problem and its implications for
Internet commerce.  In deference to those who are not technically
oriented, the detailed explanation of how the attack works will be the
LAST part of this message.
First of all, let me be perfectly clear about the nature of the problem
we have exposed.  It is NOT a bug in a single program, and it is
therefore NOT something that can be fixed with a "patch" or any other
kind of software upgrade.  Instead, we have demonstrated a very general
attack that undermines ALL programs that ask users to type a credit card
number into their home computer.  We have tested the program and
confirmed that it undermines the security of the credit card encryption
software from Netscape and Cybercash, and we expect that it will work
similarly for ANY future software based on the encryption of credit card
numbers on the desktop.  Quite simply, we believe that this program
demonstrates a FATAL flaw in one whole approach to Internet commerce,
and that the use of software to encrypt credit card numbers can NEVER be
made safe.  For consumers, we recommend the following simple rule:
NEVER TYPE YOUR CREDIT CARD NUMBER INTO A COMPUTER.
We should also be clear about the Internet commerce mechanisms that are
NOT affected by this problem.  First Virtual is unaffected because we
never ask the user to put a credit card number at risk by typing it into
a computer.  Hardware-based solutions can also be devised that are
immune to this attack, including solutions based on smart cards and
solutions based on "card swipe" machines in the home.  We believe that
current digital cash solutions are also not vulnerable to this attack,
although some variants of digital cash may be vulnerable to a similar
form of attack.  Commerce mechanisms based on the use of telephones or
fax machines to transmit credit card numbers are also unaffected by this
kind of attack.  Other proposed commerce mechanisms should, from now on,
be evaluated with this kind of attack in mind.  The bottom line: INTERNET COMMERCE CAN BE VERY SAFE, WITH SEVERAL DIFFERENT MECHANISMS,
BUT ENCRYPTING CREDIT CARDS ON THE DESKTOP IS NOT ONE OF THE SAFE
It's important to understand why we have taken this step.  Obviously, as
the long-time leaders in Internet commerce, the last thing we would want
to do is to undermine general confidence in Internet commerce.  However,
we realized that many people believed that credit card encryption was a
safe and easy path to Internet commerce, and that very few people
understood how easily it could be undermined.  Upon investigation, we
were frankly startled to realize just how easy it was -- a single
programmer got the first version of our program running in about a week.
 Aside from our obvious interest in promoting our own commerce
mechanism, we felt that we had an ethical obligation to bring this
problem to the attention of the consumers, banks, and other financial
institutions who could conceivably suffer catastrophic losses if
software encryption of credit card numbers became widespread.
We also realize that we have an obligation to do everything possible to
avoid helping any unscrupulous people who might seek to utilize this
flaw for malicious purposes.  We have accordingly been extremely
responsible in how we have handled our discovery.  We first demonstrated
and explained our program to vital organizations such as CERT (the
Computer Emergency Response Team) and the ABA (American Banking
Association).  Only after many such private disclosures, none of which
revealed any defense against our technique, did we publicly disclose the
existence of this program.
In addition, we have taken several steps to "cripple" our demonstration
program, all of which will be discussed below.    Furthermore, we have
NOT made the program itself generally available.  We are currently
demonstrating it to selected financial institutions and government
agencies, and will provide copies of the program only to CERT and a few
other independent security-minded organizations.  We have also alerted
Netscape to the problem as part of their "bugs bounty" program.  At some
future date, we might conceivably distribute the program, in binary form
on CD ROM, to selected financial institutions.  The source code will
always be very closely guarded.  Unfortunately, however, the general
method of attack is extremely easy to duplicate, and we don't know of
any good way to alert the public to the problem without explaining it.
THE TECHNIQUE
Our basic approach was to write a computer program that runs undetected
while it monitors your  computer system. A sophisticated version of such
a program can intercept and analyze every  keystroke, mouse-click, and
even messages sent to your screen, but all we needed was the keystrokes.
Selectively intercepted information can be immediately and secretly
transmitted via  Internet protocols, or stored for later use.  First Virtual's research team has built and demonstrated a particular
implementation of such a program, which only watches for credit card
numbers.  Whenever you type a credit card number into your computer --
even if you are talking to "secure" encryption software -- it captures
your card number.  Our program doesn't do anything harmful with your
credit card number, but merely announces that it has captured it.  A
malicious program of this type could quietly transmit your credit card
number to criminals without your knowledge.
The underlying problem is that the desktop -- the consumer's computer --
is not secure.  There is no way of ensuring that all software installed
on the consumer's machine can be trusted.  Given this fact, it is unwise
to trust ANY software such as a "secure" browser, because malicious
software could have easily been interposed between the user and the
trusted software.  The bottom line for consumers is that, on personal computers,
INFORMATION IS INSECURE THE MOMENT YOU TOUCH A KEY.  We  have
dramatically proven that security  ends the moment you type sensitive
information into your computer. The vulnerability lies in the fact that
information must travel from your  keyboard, into your computer's
operating system, and then to your "secure" application. It can be
easily intercepted along the way.
This kind of insecurity is very frightening, and has implications far
beyond credit card theft.  However, credit cards embody and demonstrate
the kind of information that is MOST vulnerable to this kind of attack. Credit card numbers are far more vulnerable to this kind of attack than
most other forms of information because of the following particular
characteristics of credit card numbers:

@_date: 1996-03-10 01:37:06
@_author: Tatu Ylonen 
@_subject: A brief comparison of email encryption protocols 
The current PGP keyring model does not scale anyway.  Suppose one day
every user on the Internet will have a key...  It is not relevant
whether the space per key is 100 bytes, 1000 bytes, or 10000 bytes.
All of these sizes are small enough for it to be quick to transfer a
single key.  There will soon be no way to transfer and store the
entire key ring.  In the long run, the problem must be solved using an
entirely different, distributed architecture.
    Tatu
