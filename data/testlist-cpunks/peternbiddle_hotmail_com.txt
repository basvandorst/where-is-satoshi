
@_date: 2002-08-05 16:35:46
@_author: Peter N. Biddle 
@_subject: No subject 
There are a lot of misconceptions about TCPA and Palladium. I am not going
to address TCPA per se, but I do want to try to clear up differences and
misconceptions around what Pd does.
Comments are in-line:
----- Original Message -----
Sent: Sunday, August 04, 2002 10:00 PM
Like anonymous and Adam, I have also been reading lots on Palladium lately.
I have also been working on Pd since 1997.
I agree, and from my perspective this is a problem. We have a great deal of
information we need to get out there.
We have done technical reviews of Palladium, as shown by Seth Schoen's notes
(a), which I think talk directly about many of the things discussed in this
thread. I suggest anyone who wants to start to understand Pd read these
You don't cite the MS whitepaper. This is not a technical paper but it does
set precedent and declare intent. See (b).
The suggestions for TCPA responses that William Arbaugh raises seem quite
good (c). 1 and 2 are already true for Pd, I believe that 3 is true but I
would need to talk with him about what he means here to confirm it, 4 is
covered in Eric Norlin's blog (d), and 5 is something we should do.
The current TPM (version 1.1) doesn't have the primitives which we need to
support Palladium, and the privacy model is different. We are working within
TCPA to get the instruction set aligned so that Palladium and TCPA could use
future silicon for attestation, sealing, and authentication, but as things
stand today the approaches to the two of them are different enough so that
TCPA 1.1 can't support Pd.
Pd is an OS feature set based on new hardware. Pd requires changes to the
CPU, chipset and/or memory controller, graphics and USB, as well as new
silicon (we call an SCP or SSP), . Microsoft currently has no announced
plans to support TCPA directly, and as things stand today there is no SW or
HW compatibility between the two.
This is not how Palladium works. Palladium loads a small piece of code
called the TOR after the OS has booted and is running (this could be days
later). Pd treats the BIOS, firmware, and privileged Windows OS code as
untrusted. Pd doesn't care if the SW is certified or not - that is a
question left to users.
In Palladium, SW can actually know that it is running on a given platform
and not being lied to by software. In 1, you say that SW virtualization
doesn't work, and that is part of the design. (Pd can always be lied to by
HW - we move the problem to HW, but we can't make it go away completely).
As SW is capable of knowing its own state, it can attest this state to
others - users, services, other apps, etc. It can't lie when it uses Pd to
say what it is. It's up to third parties (again, the user of the machine, or
an app, or service) to decide if it likes the answer and trusts the
application. Disclosure of the apps identity is up to the user and no one
Note that in Pd no one but the user can find out the totality of what SW is
running except for the nub (aka TOR, or trusted operating root) and any
required trusted services. So a service could say "I will only communicate
with this app" and it will know that the app is what it says it is and
hasn't been perverted. The service cannot say "I won't communicate with this
app if this other app is running" because it has no way of knowing for sure
if the other app isn't running.
Confusion. The memory isn't encrypted, nor are the apps nor the TOR when
they are on the hard drive. Encrypting the apps wouldn't make them more
secure, so they aren't encrypted. The CPU uses HW protections to wall new
running programs from the rest of the system and from each other. No one but
the app itself, named third parties, and the TOR can see into this apps
space. In fact, no one (should the app desire) can even know that the app is
running at all except the TOR, and the TOR won't report this information to
anyone without the apps permission. You can know this to be true because the
TOR will be made available for review and thus you can read the source and
decide for yourself if it behaves this way.
Correct enough for this thread; it is actually the TOR that will manage the
keys for the apps, as this makes the concept of migration and data roaming
far more manageable. (Yes, we have thought about this.)
Comparing xBox and Pd isn't particularly fruitful - they are different
problems and thus very different solutions. (Also note that xBox doesn't use
the PID or any other unique HW key.)
Palladium mostly doesn't care about the BIOS and considers it to be an
untrusted system component. In Pd the BIOS can load any OS it wants, just
like today, and in Pd the OS can load any TOR specified by the user. The MS
TOR will run any app, as specified by the user. The security model doesn't
depend on some apps being prevented from running.
I believe that there isn't a single thing you can do with your PC today
which is prevented on a Palladium PC. I am open to being challenged on this,
so please let me know what you think you won't be able to do on a Pd PC that
you can do today.
I think you mean CSS, not DSS.
I don't want people snooping my passwords from the keyboard buffer, nor my
account info from the frame buffer, and HW protections in those HW areas
prevent that.
Palladium doesn't boot strap the OS. Pd loads a secure piece of SW, called
the TOR, which runs in a secure space and loads other apps that want
security. Anyone can load an app into this environment and get the full
protections Pd offers. MS doesn't require that you show them the SW first -
you wanna run, you get to run - provided the user wants you to run. If a
user doesn't like the looks of your app, then you (the developer) have a
problem with that user.
The privacy model in Pd is different from TCPA. I could go on for a long
time about it, but the key difference is that the public key is only
revealed to named third parties which a user trusts. You are right in
thinking that you need to trust them, but you don't have to show anyone your
key if you don't trust them, so you (the user) are always in control of
Pd is not about user authentication - it is about machine and SW
authentication. User auth can be better solved on a Pd platform than on a PC
today, but it isn't required. Pd doesn't need to know who you are to work.
I don't know where to begin on this one. It deserves a long, well thought
out response, and I don't have the time to do it at the moment. I will
follow up on this. Let me state that I think that much of the energy around
DRM and HW is misplaced, and that Pd is designed to enable seamless
distribution of encrypted information, not to disable distribution of clear
text information.
MS will not have the root keys to the world's computers. The TOR won't have
access to the private keys either. No one but the HW does. The TOR isn't
"MS" per se - it is a piece of SW written by users but vetted and examined
by hopefully thousands of parties and found to do nothing other than manage
the local security model upon which Pd depends. You can read it and know it
doesn't do anything but effectively manage keys and applications. And if you
don't trust it, you won't run it.
If you don't trust the TOR, you don't trust Palladium. Trust is the *only*
feature we are attempting to achieve, so every decision we make will be made
with trust and security in mind.
I am confused as to how this would work in Pd. Anyone can write apps to the
Pd API. Zero restrictions. (API's are full of restrictions - by their nature
they limit things to a protocol, and potentially HW, both of which have
understood limitations; I am dodging this concept in saying there are no
This is a problem anyone who wants to compete in the security and trust
space will need to overcome. I don't think that it is particularly new or
different in a world with Pd. Writing a TOR is going to be really hard and
will require processes and methods that are alien to many SW developers. One
example (of many) is that we are generating our header files from specs. You
don't change the header file, you change the spec and then gen the header.
This process is required for the highest degrees of predictability, and
those are cornerstones for the highest degree of trust. Unpredictable things
are hard to trust.
Everything in the TCB (Trusted Computing Base) for Pd will be made available
for review to anyone who wants to review it;  this includes software which
the MS TOR mandates must be loaded.
This doesn't happen in Pd. There is no secure boot strap feature in Pd. The
BIOS boots up the PC the same way it does today. Root control is held by the
owner of the machine. There is no certification master key in Pd.
One of the beauties of Pd is that if there is any SW backdoor, you will know
about it. HW robustness will be something for manufacturers to work out. For
most systems, I think that extensive HW tamper resistance will be a waste of
time, but for some (e.g. highly secure govt systems) it will be a necessity
and one that works well in Pd.
I know that we aren't using undocumented API's and that we will strive for
the highest degree of interoperability and user control possible. Pd
represents massive de-centralization of trust, not the centralization of it.
I think that time is going to have to tell on this one. I know that this
isn't true. You think that it is. I doubt that my saying it isn't true is
going to change your mind; I know that the technology won't do much of what
you are saying it does do, but I also know that some of these things boil
down to suspicion around intent, and only time will show if my intent is
aligned with my stated goals.
Pd does not give root control of your machine to someone else. It puts it
into your hands, to do with as you so desire, including hacking away at it
to your hearts content.
I think that Pd represents an enhancement to personal freedoms and user
control over their machines. I hope that over time I will be able to explain
Pd sufficiently well so that you have all the facts you need to understand
how and why I say this.
(a) Seth Schoens Blog (b) MS Paper
(c) William Arbaugh on TCPA
(d) Eric Norlin's blog
majordomo at wasabisystems.com

@_date: 2002-08-06 19:08:25
@_author: Peter N. Biddle 
@_subject: Privacy-enhancing uses for TCPA 
Neither of us really had the time to clearly articulate things last time, so
I am glad you brought it up. My perspective is primarily from an
architectural one, and it boils down to this:
Platform security shouldn't choose favorites.
I don't want there to be any second class data citizens, as the
determination of who is a "first class" citizen and who isn't seems
arbitrary and unfair, especially if you happen to be second class. The
technology should be egalitarian and should be capable of treating all data
the same. If a user wants data to be secure, or an application wants it's
execution to be secure, they should be able to ask for and get the highest
level of security that the platform can offer.
You point out that legal and societal policy likes to lump some kinds of
data together and then protect those lumps of data in certain ways from
certain things. Policy may also leave the same data open for some kinds of
usage and or exploitation in some circumstances. This is a fine and
wonderful thing from a policy perspective. This kind of rich policy is only
possible in a PC if that machine is capable of exerting the highest degrees
of security to every object seeking it. You can't water the security up; you
can only water it down.
I don't think that the platform security functions should have to decide
that some data looks like copyrighted information and so it must be treated
in one way, while other data looks like national secrets and so should be
treated differently. The platform shouldn't be able to make that choice on
it's own. The platform needs someone else (eg the user) to tell it what
policies to enforce. (Of course the policy engine required to automatically
enforce policy judgement on arbitrary data would be impossible to manage. It
would vary from country to country, and most importantly (from my
architectural perspective) it's impossible to implement becuase the only SW
with access to all data must be explicitly non-judgemental about what good
or bad policy is.)
More in-line:
----- Original Message -----
Sent: Tuesday, August 06, 2002 12:11 PM
You say above that you don't agree the the problems are the same, but you
don't specify in what domain - policy, technical, legal, all of the above,
something else? The examples you give below are not technical examples - I
think that they are policy examples. What about from the technical
The term I use is "a blob is a blob"...
Isn't copyright a legal protection, and not a technical one? The efficacy of
copyright has certainly benefited greatly from the limitations of the
mediums it generally protects (eg books are hard and expensive to copy;
ideas, quotes, reviews and satires are allowed and also (not coincidentally)
don't suffer from the physical limitations imposed by the medium) and so
those limitations can look like technical protections, but really they
I agree that copyrighted material is subject to different policy from other
kinds of information. What I disagree on is that the TOR should arbitrarily
enforce a different policy for it becuase it thinks that it is copyrighted.
The platform should enforce policy based on an external (user, application,
service, whatever) policy assertion around a given piece of data.
Note that data can enter into Pd completely encrypted and unable to be
viewed by anything but a user-written app and the TOR. At that point the
policy is that the app, and thus the user, decides what can be done with the
data. The TOR simply enforces the protections. No one but the app and the
TOR can see the data to attempt to exert policy.
I swear that *I* was arguing this very point last time, and you were saying
something else! Hmmm. Maybe we agree or something.
The platform should treat this kind of data with the highest degree of
security and integrity available, and the level of security available should
support local policy like "no SW can have access to this data without my
explicit consent". The fact that the data is small makes it particularly
sensitive as it is so highly portable, so there must be law to allow the
legal assertion of policy independently from the technical exertion of
policy, and there has to be some rationalization between the two approaches.
While bandwidth limits the re-distribution of many kinds of content, it
doesn't with this kind of info. (And of course bandwidth limitations aren't
really technical protections and are subject to the vagaries of increased
bandwidth. Not a good security model.)
Not only should the platform be able to exert the highest degrees of control
over this information on behalf of a user, it should also allow the user to
make smart choices about who gets the info and what the policy is around the
usage of this info remotely. This must be in a context where lying is both
extremely difficult and onerous.
Common sense dictates that the unlawful usage of some kinds of data is far
more damaging (to society, individuals, groups, companies) than other kinds
of data, and that some kinds of unlawful uses are worse than others, but
common sense is not something that can be exercised by a computer program.
This will need to be figured out by society and then the policy can be
exerted accordingly.
I am not sure I understand the dichotomy; technical enforcement of user
defined policies around access to, and usage of, their local data would seem
to be the right place to start in securing privacy. (Some annoying cliche
about cleaning your own room first is nipping at the dark recesses of my
brain ; I can't seem to place it.) When you have control over privacy
sensitive information on your own machine you should be able to use similiar
mechanisms to achieve similiar protections on other machines which are
capable of exerting the same policy. You should also have an infrastructure
which makes that policy portable and renewable.
This is, of course, another technical / architectural argument. The actual
policy around data like "X is gay" must come from society, but controls on
the information itself originates with the user X, and thus the control on
the data that represents this information must start in user X's platform.
The platform should be capable of exerting the entire spectrum of possible
majordomo at wasabisystems.com

@_date: 2002-08-06 20:06:04
@_author: Peter N. Biddle 
@_subject: more TCPA stuff (Re: "trust me" pseudonyms in TCPA) 
----- Original Message -----
"Adam Back" Sent: Monday, August 05, 2002 2:26 PM
The Pd SCP isn't extensible or programable. I wouldn't say that it is
"general purpose" either, but I am not sure what you mean by this. It is
soldered to your motherboard. It provides a limited (smaller than a TPM)
feature set. Pd does not create a a centralised point belonging to
Microsoft. There are no root certs from MS except those to certify our own
nub and SW, and these are SW certs. How others do this for their SW is up to
them. I expect that we will want to get third party certification for our Pd
software as well as certing it ourselves. HW is assumed to be certified by
whomever built it, based on whatever criteria they want to use for whatever
the solution and cost dictate, and they too can get third-party certs as
they see fit.
It is entirely possible to run Pd and get it's benefits without telling MS
Inc. anything about your machine. For Pd to work you have to tell the MS TOR
(unless you are using a different TOR) about your machine, and so we have to
prove to everyone that telling the TOR something is very different from
telling MS Inc. something. Pd doesn't phone home on it's own.
There isn't centralized control in Pd. Users are in control. It is up to
whomever cares about the trust on a given system to decide if they trust it,
and this obviously must start with the user.

@_date: 2002-08-06 20:42:12
@_author: Peter N. Biddle 
@_subject: USENIX Security TCPA/Palladium Panel Wednesday 
I consider it a Bad Thing that we don't have more clearly organized
technical documentaion to show right now, and I can only say that we are
working on providing this post haste. I certainly am not happy to be
pointing you to blogs as primary sources. I apologize for this, and I will
send stuff out to this alias when we have it.
----- Original Message -----
Sent: Tuesday, August 06, 2002 4:57 PM
majordomo at wasabisystems.com

@_date: 2002-09-18 09:15:12
@_author: Peter 
@_subject: but _is_ the pentium securely virtualizable? (Re: Cryptogram: Palladium Only for DRM) 
The issue isn't whether or not the architeture as it existed in the past is
or isn't able to securely isolate user and kernel mode processes in an OS
which may not exist. If an OS can be written to securely isolate user and
kernel mode processes then I am sure that someone clever will find a way to
use it to do such a thing and may have an excellent security solution for
that OS which runs on current chips. I wish whomever tries to do this the
best of luck.
[Moderator's Obnoxious Note: I believe such an operating system is
called "Unix"...]
In Windows there are a number of reasons we can't use the current isolation
model for absolute enforcement of isolation. The biggest business reasons
are backwards compatibility for applications and kernel mode drivers, both
of which count on the current architecture and all of it's strengths (and
quirks). As we have stated before, we designed Pd with the assumption that
we couldn't break apps and we couldn't break device drivers.
Arbitrary Windows code which runs today must continue to run and function in
Pd as it does today, and yet Pd must still be able to provde protection.
Someone with a niche OS who doesn't care about breaking things may use a
different approach - they don't have gazllions of lines of 3rd party code
counting on version to version compatibility. We do.
current isolation models don't work My guess is that a hard core Linux or
Unix kernel dev could probably explain this just as well as MS could,
however I will see if I can get someone on our end to outline the issues as
we see them.
I think that you are talking about separating user-mode processes in VMWare
(right?). What about SCSI controllers? The BIOS? Option ROMS? Kernel mode
device drivers? DMA devices? Random kernel foo.sys? What if the attack uses
SMM to attack VMWare itself? How does VMWare prove that the environment it
inherited when it booted is valid?
Lastly - Pd is only partially about process isolation. Nothing in the
current architecture even attmempts to address SW attestation, delegated
evaluation, authentication, or the sealing of data.
----- Original Message -----
Sent: Tuesday, September 17, 2002 3:01 PM
majordomo at wasabisystems.com
