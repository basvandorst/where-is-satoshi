
@_date: 2000-12-10 19:21:59
@_author: Joseph Ashwood 
@_subject: The US mis-election - an oportunity for e-voting.. 
I guess it's time once again to dig back into the little bag o tricks to
destroy an accurate e-vote. Let's take the most recent election in the US,
where the highest office actually hinges on the swing of less than 500
voters. Consider if at the same time we had instated a vote from your home
initiative. It was publicly known that the race would be close, so activists
were out in force.
All it would take is 1000 strong willed people, with equally strong willed
guns, standing behind terrified voters who would gladly cast their vote
anyway the nice man with the big gun told them to. With public voting
locations we can certify that this did not happen, with vote from home, the
US has laws specifically for the privacy of what happens behind those doors,
as long as it's not illegal. Through this those 1000 vote would have gone
for  and he would have won
the election. In spite of the fact that 1000 felonies were committed to get
him elected, the voting structure of the united states is such that those
votes MUST be counted. I honestly don't care how good the voting system is.
I don't care how many people actually vote their conscience. I don't care
how many people were simply killed because they lived in an area that
favored . A private system is more
corruptible than a public one.
                    Joe
----- Original Message -----
Sent: Sunday, December 10, 2000 9:23 AM

@_date: 2000-12-19 20:44:46
@_author: Joseph Ashwood 
@_subject: Crypto questions 
Honestly, it's pretty easy to take care of everything you need. Since you're
using SMTP you obviously know how long the message is so you can use fairly
well anything. Also because it's going over SMTP you need to be aware that
you should base-64 encode everything, and the other issues. However what you
need is simply:
a random number generator
an implementation of RSA-OAEP
a good block cipher with a good chaining method (Rijndael, CBC is great)
a signature scheme
do the following
generate a 128-bit number K
D = RSA-OAEP(K)
B = data | signature(data)
S = D | RijndaelCBC(K, B)
Toss in some markers, something along the lines of "---Begin PGP encrypted
message---" and it should work wonderfully. The reverse should be obvious,
but just to make sure
T = receive()
S = base-64Decode(T)
(D, B)= Parse(S)BasedOnMarking
K = RSA-OAEPDecrypt(D)
data = RijndaelCBCDecrypt(K, B)
You can send anything you want this way. You can also add compression to the
data before encryption, and decompress after decryption. It's not bleeding
edge, but it's dependable, it's fast, it's secure, and if you're really
paranoid about security, move to SHA-256 with RSA-OAEP, and use a 256-bit
Rijndael key. You'll also need to make sure you use properly sized RSA keys.
If you want something closer to bleeding edge, go with XTR in place of RSA,
and well Rijndael is just an all around great cipher. If you want to strive
for exotic, use XTR and Serpent. Of course if you want the tried and true
use 3DES instead of Rijndael. If you want the most buzzwords for you
condition use half-ephemeral ECC like this:
do the following
generate a random private key
generate the public key to go with it, P
Compute the shared secret, K
B = data | signature(data)
S = P | RijndaelCBC(K, B)
Decryption is left as an exercise. If you'd like more help there are plenty
of people on the cypherpunks list (myself included) that are capable of
consulting to determine what parameters you need to use.
                        Joe
----- Original Message -----
Sent: Tuesday, December 19, 2000 9:35 AM

@_date: 2001-08-07 21:05:40
@_author: Joseph Ashwood 
@_subject: Remailer Phases 
Actually it is absolutely necessary. If all operators are willing to
collude, then your precious anonymity is completely lost. A simple tracing
methodology can establish this. The first remailer operator tracks the exact
outgoing message to the next collusion, the second tracks to the third, etc
until the message escapes, then the colluding operators track back through
the list of remailers, linking based on the intermediate value being sent,
until it reaches operator 1 who knows the sending address. This assumes a
best case of the sender determining the path taken through encryption. Worst
case the first operator can reveal the information to everyone.
                        Joe

@_date: 2001-08-08 15:16:44
@_author: Joseph Ashwood 
@_subject: Remailer Phases 
Sent: Wednesday, August 08, 2001 5:40 AM
I don't trust any of them. I don't personally use remailers, I don't tend to
do things that are illegal, but if I did there are other methods that I'd
                    Joe

@_date: 2001-08-08 17:26:59
@_author: Joseph Ashwood 
@_subject: Mixmaster Message Drops 
Sent: Wednesday, August 08, 2001 7:05 PM
That matters from a correction view but not from a usage view, which I
assume we're taking. Basically we don't care what technology the remailer
uses as long as it is correct technology and trustable. From there we care
only what remailers are disfunctional and which are useful.
Well assuming that the remailers are under attack, we start using digital
signatures with initiation information stored in them. Mallet can introduce
duplicates, but the likelihood of a duplicate being detected rises very
quickly, (i.e at a rate of 1-(1/20)^M for M duplicate messages assuming a
drop rate of 1 in 20). This gives us the ability to discount the vast
majority of what Mallet does and get very close to accurate values. The
bigger risk is for Mallet to identify our queries and force the proper
functioning of the node exclusively for the query. Correcting this is much
more difficult, but would only take the use of digital signatures and
encryption on all the messages traversing the network. Since the remailer
user inherently a more developed user than Joe (l)User this is much more
reasonable. But still approaches impossible because the remailer users is a
finite set so Mallet could store all the remailer user keys, and treat them
differently from the query keys. This becomes extremely difficult as long
term keys are defeated as well as ephemeral keys. Instead the remailer users
will have to maintain statistics, or at least a large unknown portion of
them. If users upload to say freenet once a month the number of anonymous
messages they have sent and recieved (without mention of timeframe except
implicitly month) we could get an overall droprate, and the users wouldn't
have to reveal who they are.
Agreed I was beginning to adress this above, it still has some major
through a set of trusted remailers, if those remailers are trusted and are
used for test initiation, then the exact droprate from that entry point will
be known. This will build a reputation for those remailers making it
desirable for trustable remailer operators to be in that set by increasing
the number of messages, leading to better security by initiating from the
trusted list.
You count the messages in and the messages out, you don't care what they
say, where they're from etc, the operator doesn'tr even need to know you're
doing it. Of course this is a rather difficult task, the better option would
be to test the network as a whole, by colluding of users to collect
statistics on their own messages going through, this would defeat much of
what Mallet could do because the test messages would be real messages that
are being propogated through.
Wouldn't be that bad. Treating the network as a function of it's entry-point
seems easiest. Then it's just a simple fraction which can be published raw
or you can waste 4 seconds on a 1GHz machine and compute the values. Either
way it's not compute intensive, most of the work needs to be done by
legitimate users with legitimate messages (to prevent Mallet from playing
with the messages).
We are trying to determine the best entry-point for anonymous remailer use
as measured by percentage of messages that reach their destination, as
filtered by being "trusted".
Then we only trust the servers on the "trusted" list, and we use the best
remailer from the list in terms of delivery. This will encourage individuals
that run worthless remailers to improve their systems, eventually leading to
the dropping of only a handful of messages a year.
                        Joe

@_date: 2001-08-08 17:46:04
@_author: Joseph Ashwood 
@_subject: Remailer Phases 
Sent: Wednesday, August 08, 2001 4:48 PM
As has been pointed out it's not latency but latency/messages that matters.
If there are 2 messages a day going through the system then 5 minutes is
simply not enough, it will be completely traceable. OTOH if there are
5000/sec going through the system then 5 minutes is arguably overkill. I
think that with the current relatively low level of usage 24 hours is the
minimum average latency that should be used. Of course this is across the
entire Mixmaster net where messages could be dispersed enter at any location
and leave at any location. Based on this I believe that each node should
maintain a list l of known other nodes. It should of course select a delay
time at random say up to t time. Assuming that the server will choose a new
exit point at perfect random from itself (where it will exit immediately on
timer expiration) and l this gives an equation for t in hours f(t) =
necessaryDelay; f(t) = t + ((|l|-1)/|l|)f(t), by finding the solution for t
you will find the necessary average t. I don't have the time to solve this
right now but given a list l of magnitude 100 the value of t will be
significantly greater than 5 minutes.
So the remaining question is what value to use for necessary delay? This is
also dependent on the number of known nodes. All nodes must be equally
likely for the transfer for obvious reasons. Based on this I believe that
necessaryDelay needs to be greater than the time needed to receive |l|
messages. The reason for this is fairly simple, at the extreme we have only
one possible message going through the system at once, this is obviously
bad, an observer simply watches the output of the system, and what comes out
is what they are looking for. with at least |l| messages going through a
node and |l| necessary delay time (note that as the magnitude of l increases
the entire system slows, this could be bad, I'm sure I'm missing something
that will dominate on scaling) each message can be mistaken for other
messages. Since it is expectable that the usage of remailers will increase
at least as fast as the size of l the latency will likely decrease over
If there is sufficient demand it is entirely reasonable to reduce from |l|
to a value of at least 2, but I don't believe this is reasonable at 100 or
even 1000 remailers. If the amount of remailer usage increases to the point
where > 20% of email traffic goes through remailers it may become feasible
to lower this limit, but probably unnecessary because this scaling would
result in lowered delays as a matter of recomputation.
What is surprising is that this can be automatically calculated in a rather
interesting way. If each still maintains l it is entirely possible for a
remailer to create a message pool of size |l| and when a new message arrives
if the pool is full randomly select 1 entry to be flushed towards it's
destination _prior_ to the insertion of the new message, with an autoflush
happening every sqrt(|l|) hours (perhaps by insertion of null message). This
would cause a ripple effect each time a message was sent which could be seen
as a problem by the uninitiated because there would be a decided pattern of
travel with each message entering the system causing activity along a random
walk. To an amateur this would appear to be a flaw in the system, except
that the message being sent by the ith node is not the message sent by the
i-1th node, so the risk is non-existent, and since the average path length
is going to be k=2((|l|-1)/|l|), and the random walk is going to choose from
growth rate to block tracing. If this growth rate is unacceptable we can
also add a minimumHops value to the protocol increasing the number of paths
to |l|^minimumHops + |l|^2, minimumHops should be chosen to be a suitable
number, based on current assumptions I would recommend minimumHops =
logbase|l|(2^128), making the |l|^2 only a footnote as the total would be
greater than 2^128 giving an enormous difficulty in even selecting a
duplicate path.
Mitigating factors are present however, because each message can only exist
in one of |l| locations, so the maximum difficulty in guessing is still
bounded in that fashion, leaving the reasonable values for minimumHops at
around 10 for a 100 node network.
                        Joe

@_date: 2001-08-09 12:52:36
@_author: Joseph Ashwood 
@_subject: Remailer Phases 
Let me see if I've got your personal fantasy correct, because that's all it
is. You believe that it is best to design a protocol that is somewhat
resistant, and simply ignore it's faults. I strongly disagree, doing that is
to put it bluntly stupid. The real game is to design a protocol and make
it's strengths and weaknesses known. Anything else is just more stupidity.
And for the record, I'm far from new.
                        Joe
----- Original Message -----
Sent: Thursday, August 09, 2001 12:01 PM

@_date: 2001-08-09 13:17:31
@_author: Joseph Ashwood 
@_subject: Remailer Phases and Joseph Ashwood's criminal behavior 
There you go confusing things again, I really wish you'd get that fixed.
Anonymity is strictly for the purpose of doing something that would
otherwise be troublesome. Perhaps you are confusing anonymity with
pseudonymity? Confidentiality is another matter entirely, and strictly
seperate from anonymity or pseudonymity. I think once you deal with your
confusion (which I understand, you are most likely a product of the american
school system which for quite some time confused me also) you will see
things much more reasonably.
And do you always wear a black plastic tarp that's inflated to the size of a
small car to hide your identity in public?
You are failing to grasp the difference between anonymity and not forcibly
revealing information.
And you put your return address either on the envelope or in the letter
don't you? You seem to be having a very hard time grasping the difference
between functional anonymity and forced anonymity, let alone the often more
difficult to grasp pseudonymity versus anonymity. Let me give you an
example. If you go outside right now and walk down the street, you will
likely pass a person. Do you care who that person is? Do you know that
person? Does the person care who you are? Does the person know you? If the
answer is no, then the two of you have functional anonymity.
I see the less intelligent are crawling out of the woodwork on this one
(I've got a few more in my inbox). I said I don't use anonymous remailers,
that is far different from not making use of anonymity (which I occasionally
do require), or pseudonymity which I quite often use. There seems to be a
significant number of people that have little to no grasp of the differences
between anonymity, functional anonymity, and pseudonymity. Anonymity is
where no matter how hard the attacker works they cannot find out who sent
(or in some cases received) the message, this level of anonymity is simply
not available through remailers (see my message where I dictated the
collusion to break the entire remailer network). Functional anonymity is
much more possible, that's where it will take more work than the attacker is
willing to put in, this level is attainable and is probably available from
various sources. Pseudonymity is what most people really want when they say
anonymity, that's where the attacker will not/is unable to expend the work
effort to find out who actually sent the message, but any future messages
sent can be linked, hushmail is quite often used in this way. I think once
people get these straight the level of uninformedness on the subject will
very quickly lower itself.
                        Joe

@_date: 2001-08-09 14:09:41
@_author: Joseph Ashwood 
@_subject: peer to peer wireless WAN? 
Sent: Thursday, August 09, 2001 3:53 PM
Easier solution. Screw the 802.11 security. Run TCP/IP over it and use
IPsec. Magically the thing is now reasonably secure, without having to mess
around with redesign, just leave everything as is.
                        Joe

@_date: 2001-08-10 15:26:32
@_author: Joseph Ashwood 
@_subject: Why Joseph Asherwood Should Use Remailers 
Yet another failure on the part of the reader. Anonymity is strictly to
avoid having something linked to your identity. Since what I do requires
trust, and trust cannot be built into an anonymous system. I can't do what I
want to do through anonymity.
Additionally you have apparently failed to grasp exactly where the reality
and perceived reality of anonymous remailers actually lies. What you have
taken to calling anonymity is far from it. Not only does it have a threshold
of workeffort that can be applied to trace a message, but it also comes from
a finite set of users, a potentially very small finite set of users.
For example, how many people make use of remailer.xganon.com? Certainly no
more than a few thousand. A few thousand investigations would turn of any
facts surrounding statements made through the "anonymous" remailer. To
narrow the possibilities even further, how many of them read cypherpunks
through lne.com?
Whether you have realized it or not, your anonymity is heavily flawed. So
what point is there to using "anonymity" that is heavily flawed, slow, drops
messages, and through not being pseudonymous does not allow the building of
trust? The only remaining reason would be doing something illegal where the
flaws become benefits, slowness is immaterial, dropped messages don't
matter, and you don't want trust. If you can give me an example of another
use of a generally horrible technology that is legal I will gladly point out
to you why there are still better technologies for that as well.
                                Joe
----- Original Message -----
Sent: Friday, August 10, 2001 2:19 PM

@_date: 2001-08-11 16:46:28
@_author: Joseph Ashwood 
@_subject: Mixmaster Message Drops 
First let me say those were not my numbers, those numbers were supplied by
another source, I simply reiterated them.
----- Original Message -----
Sent: Saturday, August 11, 2001 6:19 PM
Actually if you are simply testing the number of messages that come in
versus the number that go out, duplicates are a worry. If we are ignoring
the content then a message stream of
1,2,3,4,5,6,7,8,9,10,11,121,13,14,15,16,17,18,19,20 looks identical to
1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1 so that failure mode needs to be
addressed, the individual signatures addresses that issue, which means that
we can distinguish between the two message streams. This allow us to detect
that numbers 13 and 17 for example) got dropped, and to cover the
seperatation in the stream Mallet duplicated messages 7 and 11. This gives
us a level of tracability that we can enforce ourselves outside of the
network system. I believe that detecting and eliminating duplicates
eliminates a very important activity that Mallet could perform to throw off
our measurements.
Actually you can start with just one trusted remailer. If you can get in an
personally inspect 1 remailer, or run it yourself, you can trust a single
one. Once the single trust location has been established you begin routing
information through that single entry point, and make use of that entry
point to measure to depth 2. Once you have built trust in a depth 2 entry
point, you can then test it as a depth 1, making sure that mallet doesn't
allow just a single entry point proper passthrough. From there you will have
2+ entry points to begin more depth 2 tests, from 2+ locations to begin
with, repeat until the trust base has reached the necessary levels. Of
course this testing has to be maintained continually, but the ability to
send a couple dozen messages through each remailer each day should provide
enough maintenance power.
                                Joe

@_date: 2001-08-11 17:14:34
@_author: Joseph Ashwood 
@_subject: Mixmaster Message Drops 
Sent: Saturday, August 11, 2001 7:07 PM
Well technically you begin by granting trust to someone, the easiest being
yourself. Then you build trust in something else, use trust in one thing to
build trust in another. Regardless you have to establish trust in a single
location, before you can build trust in multiple.
I'm only worrying about future behavior, the past being something that will
never matter again in the behavior of the system, especially since we can't
plan on testing "in the past." In this particular case we are only concerned
with whether or not it will forward every message sent to it within some
very tight bounds so it is unlikely that a non-malicious entity would change
the configuration, if the configuration is changed in that respect then it
will be detected later on during the testing phase. The issue being that it
will represent other remailers as undependable, possibly while making itself
look flawless. This is a very difficult problem to solve.
But we can build trust in it's ability to forward messages in a way that is
for some definition anonymous, in particular the project is to eliminate
it's dropping of messages either at random or maliciously, or at least
determine which remailers perform the dropping and with what rate.
Which won't matter because the particular behavior we are trying to detect
will be reported on if they change, simply by the fact that the messages
will be dropped. All we're trying to do is build trust in the ability of a
single location to begin an anonymous analysis of the abilities of the
others to forward messages without dropping any (save a small ratio).
                    Joe

@_date: 2001-08-25 16:24:11
@_author: Joseph Ashwood 
@_subject: Gnutella scanning instead of service providers. 
A scan enabled Gnutella would reveal itself immediately to a somewhat
intelligent legal team who would simply set up a cheap system that would
recieve the pings, making the users _easier_ to locate instead of your
assumed harder, since the legal team would not even necessarily have to ping
the world themselves.
It's a fairly simple problem, under IPv4 there are 2^32 ip addresses. A fast
ping is a few milliseconds each ping, and can be mounted from a large
connection at a large number simultaneously, so lets say 8192 attempts per
second. A fast ping machine will take 2^32/8192 seconds which comes out to
524288 seconds or about 4 days. So the ping set itself would take too long.
The internet clogging comes from the quantity of these pings. Let's say
there are 1 million Gnutella pingers, they all of course first hit AOL
because it's a prime candidate for pretty much anything. AOL has let's say
65536 addresses, receiving 1 million pings per second (approximately) which
will fully occupy several T1 lines which means that the ping messages will
be blocked at every router disabling the scan portion of gnutella putting us
back where we are now, but with more time, more code, and more bloat
dedicated to it. So we'd slow down every major network until they all block
the gnutella ping messages some how, costing everyone more time, more money,
more hassle, more headaches. Like it or not, the ping idea for Gnutella is a
very bad idea.
                                Joe

@_date: 2001-12-18 19:35:35
@_author: Joseph Ashwood 
@_subject: Speech May Not Be Free, but It's Refundable 
Sent: Tuesday, December 18, 2001 7:14 PM
Actually that's not true. Take for example the nearly nationwide ban on
committing suicide. This clearly only impacts a single person directly, and
is by it's very act clearly consensual. But it has even been proposed that
attempted suicide should be tried as attempted murder 1. Oddly enough this
would in some areas make attempted suicide punishable by death. There are
other examples of similar, but not as ludicrous, legal situations where the
only person being impacted is the person acting, but it's still illegal.
                            Joe

@_date: 2001-02-19 22:31:34
@_author: Joseph Ashwood 
@_subject: FAQ? how to set up a cross-platform encrypted mailing list/forum 
Well it's not an easy way, but it's a functional way. Take the code for GPG
and the code for your favorite open-source list server, integrate them so
that each mailing list has it's own GPG key, decrypts, reencrypts to the
targets (individually so as to avoid having insane sizes for each message)
sends, repeat. If you need higher speeds you can compromise of the encrypt
to targets by choosing a key periodically, encrypting it to all the targets
and holding it to be combined with the header.
I would recommend against an S/MIME solution. S/MIME has a great many weak
solutions, and take a lot of work before it's really ready for secure
                            Joe

@_date: 2001-02-20 19:23:09
@_author: Joseph Ashwood 
@_subject: FAQ? how to set up a cross-platform encrypted mailing list/forum 
Sent: Tuesday, February 20, 2001 10:27 AM
So you write it so that unencrypted e-mail simply will be bounced. Simple
enough to do.
                        Joe

@_date: 2001-02-20 22:29:01
@_author: Joseph Ashwood 
@_subject: FAQ? how to set up a cross-platform encrypted mailing list/forum 
Actually considering that the reference was for PGP encrypted messages,
which have known characteristics. They decrypt, and since we also have
complete control over the requirements, you can require signatures. What are
the odds that an arbitrary message will have a verifiable PGP signature
attached? As well as appropriate formatting for decryption? That's one of
the beauties of formatting, we know exactly what it should look like, give
or take the middle parts.
                        Joe

@_date: 2001-07-17 15:16:21
@_author: Joseph Ashwood 
@_subject: Computer detective talks about Levy evidence 
I can tell you it is not true. I have a less than reputable brother (drugs),
and he was recently accused of running a small meth lab within whatever the
distance is of 2 school, so it's is a very serious accusation. The computer
that sits not 20 feet from the alleged location of the alleged meth lab
(sorry I honestly don't know the truth, all reports I get are at least
second hand) has not been notably touched by law enforcement officials of
any kind. So while I'm sure it is commonplace and perhaps standard protocol
to confiscate the computer with most crimes, there are exceptions.
                            Joe

@_date: 2001-07-19 19:26:49
@_author: Joseph Ashwood 
@_subject: HushMail 2.0 released, supports OpenPGP standard 
What probably happened is that you didn't see the other windows come up
where it was gathering entropy and needed your mouse input. If you don't see
that window I can see where you wouldn't be able to upgrade.
                    Joe
----- Original Message -----
Sent: Thursday, July 19, 2001 8:37 PM

@_date: 2001-06-16 15:47:19
@_author: Joseph Ashwood 
@_subject: eBay: Burn DVD movies onto CD? 
Sounds to me like it's just a plain binary copy. The software takes the data
off the DVD, and puts it on the media, when it runs out of space it asks for
more media.
                        Joe
----- Original Message -----
Sent: Saturday, June 16, 2001 2:37 AM

@_date: 2001-06-26 19:42:43
@_author: Joseph Ashwood 
@_subject: digital watermark/XML books 
Well XML Signature is easy enough to come across, all the information is
available from  .
                    Joe
----- Original Message -----
Sent: Tuesday, June 26, 2001 1:56 AM

@_date: 2001-03-15 20:18:45
@_author: Joseph Ashwood 
@_subject: content owners vs. ISPs 
I'm sorry, there will be no war _between_ security of any kind, and privacy
of any kind. One implies the other.
If something is private, it must be secure
If something is secure, there is obviously something worth keeping
Now how exactly are they going to start monitoring people at the ISP level,
when we are all free to make use of things such as IPSec, Freedom, etc? If
we don't exercise our rights, they can of course take them away.
                        Joe
----- Original Message -----
Sent: Thursday, March 15, 2001 1:54 PM

@_date: 2001-11-09 14:46:56
@_author: Joseph Ashwood 
@_subject: Security-by-credential or security-by-inspection 
Sent: Friday, November 09, 2001 3:12 PM
Then let's make proper use of technology. We want to make sure the ID card
is issued by the correct authority, that's almost exactly what digital
signatures were designed for. Just create some uniform way of computing the
data from the card (easiest would be to just use a plain old-fashioned
smartcard), and check the signature against a publicly known public key.
It's really quite simple.
That is the far bigger problem. Identifying these people simply won't make
any difference. If a person is intent on being a suicide bomber, they will
blow other people up with them, no matter how well we can make an
Couldn't get the thumbprint idea going that quick, but smartcards and
smartcard readers are already in mass production making my idea not easy,
but possible to get underway in 60 days. Completion though would be a matter
of approximately a decade.
Very good examples of how not to go about it. My idea (while far from
perfect or fully developed) lacks the same bottleneck points, the only
information that needs to be accessed millions of times remains static
across years, with a retrieval rate like that it would be more than possible
to simply broadcast the key over a public broadcasting station along side
the current time, since nobody is watching anyway you could easily take over
the closed captioning for a few seconds to send out the key. I'm clearly not
addressing certification of the key as correct but having the president read
back a hash of it at the state of the union address (couldn't be any more
boring than the rest) would certainly provide some evidence.
I agree, no matter what method is chosen, the possibilities for abuse are
excessive (some of these people can't even be trusted not to use a phone
book improperly, give then some real power and who knows what will happen),
and the value of the target is too great. Let's pretend that my idea is
used. Let's say each card costs $10 to issue. How much is impersonation
worth? Well for something of the impact of Sept 11 it could easily be
estimated at billions of dollars. That will buy a massive amount of computer
power, a large quantity of the world's best mathematicians, and a
significant amount of time. I don't like the odds of DSA against that, it's
too close to the wire right now, supplying a target of this size could be
devastating. That leaves RSA varients, but for billions of dollars and a
significant amount of time 2^80 work (SHA1) isn't that much, some less fully
examined algorithm would have to be used, that presents it's own problems.
Basically the target is simply too big for current standards, once SHA-512
is fully examined there may be a chance, but until then I just don't think
the card everyone idea is cryptographicly feasible. The non-cryptographic
methods would pose additional problems because anything that can be
phyisically made by one person can be physically made by another.
That's true, we certainly lose more people to far more mundane things every
day than the WTC tragedy caused. But at the same time you have to realize
that most people don't think about bee stings as a cause of death, they
don't even think about bed they sleep in as a cause of death (look up the
statistics it's hilarious), and both of those cause vastly more deaths each
year as terrorism on average. The problem is that the media has hyped this
up, the president's handlers have told him that this is a big deal, as a
result of this the general populus wants blood. Thinking people know taht we
will never eliminate terrorism, well I guess on a technicality we could, but
it would require extermination of all but 1 human.
I think that line says it all.
                        Joe

@_date: 2002-08-10 21:46:12
@_author: Joseph Ashwood 
@_subject: Seth on TCPA at Defcon/Usenix 
[brief description of Document Revocation List]
Actually it does, in order to make it valuable. Without a hardware assist,
the attack works like this:
Hack your software (which is in many ways almost trivial) to reveal it's
private key.
Watch the protocol.
Decrypt protocol
Grab decryption key
use decryption key
problem solved
With hardware assist, trusted software, and a trusted execution environment
it (doesn't) work like this:
Hack you software.
DOH!!!!! the software won't run
revert back to the stored software.
Hack the hardware (extremely difficult).
Virtualize the hardware at a second layer, using the grabbed private key
Hack the software
Watch the protocol.
Decrypt protocol
Grab decryption key
use decryption key
Once the file is released the server revokes all trust in your client,
effectively removing all files from your computer that you have not
decrypted yet
problem solved? only for valuable files
Of course if you could find some way to disguise which source was hacked,
things change.
Now about the claim that MS Word would not have this "feature." It almost
certainly would. The reason being that business customers are of particular
interest to MS, since they supply a large portion of the money for Word (and
everything else). Businesses would want to be able to configure their
network in such a way that critical business information couldn't be leaked
to the outside world. Of course this removes the advertising path of
conveniently leaking carefully constructed documents to the world, but for
many companies that is a trivial loss.
                Joe

@_date: 2002-08-13 22:58:58
@_author: Joseph Ashwood 
@_subject: Overcoming the potential downside of TCPA 
Lately on both of these lists there has been quite some discussion about
TCPA and Palladium, the good, the bad, the ugly, and the anonymous. :)
However there is something that is very much worth noting, at least about
There is nothing stopping a virtualized version being created.
There is nothing that stops say VMWare from synthesizing a system view that
includes a virtual TCPA component. This makes it possible to (if desired)
remove all cryptographic protection.
Of course such a software would need to be sold as a "development tool" but
we all know what would happen. Tools like VMWare have been developed by
others, and as I recall didn't take all that long to do. As such they can be
anonymously distributed, and can almost certainly be stored entirely on a
boot CD, using the floppy drive to store the keys (although floppy drives
are no longer a "cool" thing to have in a system), boot from the CD, it runs
a small kernel that virtualizes and allows debugging of the TPM/TSS which
allows the viewing, copying and replacement of private keys on demand.
Of course this is likely to quickly become illegal, or may already, but that
doesn't stop the possibility of creating such a system. For details on how
to create this virtualized TCPA please refer to the TCPA spec.
                Joe

@_date: 2002-08-14 15:10:44
@_author: Joseph Ashwood 
@_subject: Overcoming the potential downside of TCPA 
<3D5A4A38.20609
----- Original Message -----
Actually that does nothing to stop it. Because of the construction of TCPA,
the private keys are registered _after_ the owner receives the computer,
this is the window of opportunity against that as well. The worst case for
cost of this is to purchase an additional motherboard (IIRC Fry's has them
as low as $50), giving the ability to present a purchase. The
virtual-private key is then created, and registered using the credentials
borrowed from the second motherboard. Since TCPA doesn't allow for direct
remote queries against the hardware, the virtual system will actually have
first shot at the incoming data. That's the worst case. The expected case;
you pay a small registration fee claiming that you "accidentally" wiped your
TCPA. The best case, you claim you "accidentally" wiped your TCPA, they
charge you nothing to remove the record of your old TCPA, and replace it
with your new (virtualized) TCPA. So at worst this will cost $50. Once
you've got a virtual setup, that virtual setup (with all its associated
purchased rights) can be replicated across an unlimited number of computers.
The important part for this, is that TCPA has no key until it has an owner,
and the owner can wipe the TCPA at any time. From what I can tell this was
designed for resale of components, but is perfectly suitable as a point of
                    Joe

@_date: 2002-08-14 19:23:29
@_author: Joseph Ashwood 
@_subject: Overcoming the potential downside of TCPA 
I certainly don't believe many people to believe me simply because I say it
is so. Instead I'll supply a link to the authority of TCPA, the 1.1b
specification, it is available at
 . There are other
documents, unfortunately the main spec gives substantial leeway, and I
haven't had time to read the others (I haven't fully digested the main spec
yet either). From that spec, all 332 pages of it, I encourage everyone that
wants to decide for themselves to read the spec. If you reach different
conclusions than I have, feel free to comment, I'm sure there are many
people on these lists that would be interested in justification for either
Personally, I believe I've processed enough of the spec to state that TCPA
is a tool, and like any tool it has both positive and negative aspects.
Provided the requirement to be able to turn it off (and for my preference
they should add a requirement that the motherboard continue functioning even
under the condition that the TCPA module(s) is/are physically removed from
the board). The current spec though does seem to have a bend towards being
as advertised, being primarily a tool for the user. Whether this will remain
in the version 2.0 that is in the works, I cannot say as I have no access to
it, although if someone is listening with an NDA nearby, I'd be more than
happy to review it.
                    Joe

@_date: 2002-08-15 13:06:26
@_author: Joseph Ashwood 
@_subject: TCPA not virtualizable during ownership change (Re: 
References: <01da01c24357$f8e3f560$6601a8c0
  <3D5A4A38.20609
  <00de01c243df$d357a660$6601a8c0
  <20020815070604.A935125
This is going to be a very long, and very boring message. But it should
highlight why we have differing opinions about so very many capabilities of
the TCPA system. For the sake of attempting to avoid supplying too little
information, I have simply searched for the term and will make comments on
each location that it appears.
----- Original Message -----
I wholeheartedly agree. 332 pages to say 5 pages worth of real information
is not helpful.
[Search criteria "endorsement"]
While I haven't found any solid evidence either way, they seem to almost
deliberately avoid that discussion on Page 22 I found a fatal errorcode
TCPA_NO_ENDORSEMENT at TCPA_BASE+35 "The TPM does not a EK installed"
attempting to interpret the bad grammar, I believe this should state "The
TPM does not [have] an [Endorsement Key] installed" which seems to indicate
that the platform may ship without one.
On page 35 the endorsement key is listed as persistent data. Which at first
would indicate that the endorsement key happens before shipping, but since
there is also an RNG state variable stored persistently, my confidence in
this is undermined. Adding to the complications, down near the end of the
page, in the table it says "This is the TPM's endorsement key pair. See 9.2.
The default value is manufacturer-specific" which indicates that it does
ship with an endorsement key, but that the key can be changed by the owner.
Page 38, the existance of the CBKPUsed flag hints that the endorsement key
pair need not always be present. Unfortunately the spec goes on to say
"NOTE: This flag has no default value as the key pair MUST be created by one
or the other mechanism." Which certainly confuses things.
Page 41 "TPM_CreateEndorsementKey may be called before TPM_Startup. This is
necessary because TPM_Startup will fail unless an endorsement key exists" is
of no help either way. As with all the others, it states that there may
exist conditions where the EK may not exist, but does not give any hints
whether this is before or after the TPM leaves the plant.
On page 79, the EK is metioned twice. The first time if useless for our
purpose. The second time states "This SHALL be the TPM endorsement
credential" which indicates that an endorsement credential must exist. Other
locations though seem to hint that a void endorsement credential may be
Starting on Page 84 is section 4.32.1, which seems to be as close to an
authority on the EK as possible, but lacks a statement of whether the EK is
shipped with or added later. It does however clearly indicate that the
creation of the EK occurs before the Privacy CA is contacted, which was
already agreed on.
[somewhere around here I stopped addressing everyt occurance of the word
"endorsement" because most of them are frivolous]
Page 135, Section 5.11.1, clearly states "The new owner MUST encrypt the
Owner authorization data and the SRK authorization data using the PUBEK."
Which clearly indicates that the EK must exist before ownership can be
taken. Other places have hinted that ownership may be taken and then the EK
updated, which completely contradicts the one-timeness, or this statement.
Page 135 "If no EK is present the TPM MUST return TCPA_NO_ENDORSEMENT" which
indicates that one can at least attempt to take ownership before an EK is
present, which would contradict the requirement that the EK come from the
Page 178, Section 7.3 I am only mentioning because it presents a rather
interesting possibility. It hints that under some circumstances it may be
acceptable for a manufacturer to copy the data from one TCPA to another.
This portion begins with "The manufacturer takes the maintainance blob . .
." This may however only be to update an existing one to address flaws or
meet new capabilities.
Page 183, hints that even the manufacturer is not allowed to known EK public
key, which complicates things no end, because the Privacy CA certainly
cannot at that point be permitted to view it. This would indicate that even
if the EK is shipped with the system, it can never leave the system. This
would limit the ability of the EK to simply certifying the owner, if that is
true then it confuses me even further.
Page 213 section 8.10 clearly states that if the owner clears the TCPA,
everthing is cleared "except the endorsement key pair." Which would indicate
that this is truly a one-shot deal.
Page 240, states "This is a dead TPM. It has failed it's startup smoke test.
It should not leave the factory floor." This indicates that the EK must be
created before the TPM leaves the factory.
Section 9.2, page 261, states that TPM_CreateEndorsementKeyPair can only be
called once, but does not state if this is done by the owner, or by the
plant. Later on the page is a hint that it may be shipped with it. "The
PRIVEK and PUBEK MAY be created by a process other than the use of
TPM_CreateEndorsementKeyPair" and related statements, which indicate rather
well that the endorsement key created before shipping. It also states that
the credential could be stored after "an Owner has taken ownership of the
platform," confusing the matter even more. Of course at the end of this
section they change the mandatory return value for
TPM_CreateEndorsementKeyPair (beginning TCPA_FAIL, end TCPA_DISABLED_CMD).
Page 268, "The TPM creates an identity-binding signature (the value of a
signature over the
TCPA_IDENTITY_CONTENTS structure). Among other things, this proves
possession of the new private key, which does the signing of the
TCPA_IDENTITY_CONTENTS structure." Indicates that the endorsement key (they
only possible plant binding factor) is never used outside the box. The
TCPA_IDENTITY_CONTENTS structure contains only TCPA_VERSION, ordinal, label
PrivCADigest, and identityPubKey. Note the absence of anything identifying
the TPM and binding it to the manufacturing. On the next page they state
that there is some method used of certifying that it came from a genuine
TPM, but I can't immediately find such evidence.
I suppose just to make life more interesting for everyone involved, they
included the following "The form of the following certificates is out of
scope for this version of the TPM specification:
. TPM endorsement entity certificate"
Section 9.5.1 page 283 states "If the data structure
 is stored on a platform after an Owner has taken
ownership of that platform, it SHALL exist only in storage to which access
is controlled and is available to authorized entities." Which indicates very
strongly that the EK can be created after shipping. More interesting though
is the TPM_ENDORSEMENT_CREDENTIAL itself, which includes only the
information to gaurantee that the cert was grabbed from a valid instance of
a TPM, but does not from what I can tell actually certify that the current
request comes from a valid TPM. Although they do take the odd step of making
certain that CRLs are not used, under CRL dictribution points "If present
and marked critical, then reject" which of course means that CRLs will not
be used for this.
The other credentials have the same interesting quirks.
Page 311, Section 10.8 gives an interesting view of the situation, which
indicates that the EK may be produced later discussing power-on self-tests
"If an endorsement key has not yet been generated the TPM action is
manufacturer specific."
In the Glossary I think is the clearest statement about the EK in the entire
document, the problem is the sentence is too long, shortening it for the
valid point "Endorsement Key [-] A term used ambiguously" is entirely
accurate enough for our purposes.
Result: I have no idea whatsoever about where/when the EK is created, there
are a number of conflicting statements regarding it, and at least once where
they even change the return value of a function.
Agreed. After almost two hours of picking through poorly written
specification, I'm just going to state that I agree.
Probably correct, but I'm not sure.
Only to the owner, the manufacturer is not supposed to have a copy
The privacy CA never recieves a copy of the PUBEK, the PUBEK is only to be
seen by the owner.
I agree with everything except the "during manufacture" which while
seemingly is the correct way to do it to meet the majority of the spec,
isn't necessarily the only way.
c) the
I believe this is incorrect. The complete contents of the endorsement
credentials can be found starting on page 283, and does not include the
endorsement key in any way. Which is good because knowledge of the
endorsement public key, would allow someone to claim ownership of the
It is not encrypted with the endorsement public key, it can't be since the
Privacy CA does not receive a copy of it. I'm not sure whether or not it is
encrypted at all, but at the very least it cannot be encrypted by the EK.
Certainly correct, but I don't believe the Privacy CA necessarily has to be
in on it.
This would probably be easier done with a court order forcing the Privacy CA
to perform the operation under the guise of law enforcement.
But it could be easily combined with a smaller, easier to get court order
along the lines of B where a company is required to allow the FBI to work
under the company's name to perform the necessary work. This eliminates the
phony Privacy CA from existance.
I didn't notice that, but it does seem to be true, as with you I can see no
reason for that decision. Perhaps it was a non-decision that made it into
the spec?
It is discussed surprisingly briefly for this spec, it occupies a mere 10
pages, beginning on page 178. The reliance on trust here stems from the
manufacturer's ability to replicate TPMs (although the EK does not appear to
be changable this way).
Thinking about it more, I don't like that this spec assumes there is 1
primary user of a machine, and that this user is also the owner. A better
design would use something akin to a smartcard, and a large chunk of
non-volatile RAM. Making properly use of this would allow for authentication
of families of people, simply plugging in your card, and entering your
passphrase would allow users to authenticate to the system easily, allowing
the home system to separate the identities of Mom, Dad, Son, Daughter, Aunt,
Uncle. And could still bind people to a specific machine.
I think everyone who reads the spec, or even our commentary on it can safely
conclude that all the chips supporting this should have a "WTF" somewhere in
their identifier.
Now leaving the topic.
Fortunately in my current state of (lack of) employment I have plenty of
time to do this kind of examination, otherwise I wouldn't bother. With that
said, if anyone knows of a company currently hiring software
engineer/cryptanalyst/etc I'd appreciate any information.
                    Joe

@_date: 2002-07-01 18:46:29
@_author: Joseph Ashwood 
@_subject: maximize best case, worst case, or average case? (TCPA 
The same argument can be applied to just about any tool.
A knife has a high likelihood of being used in such a manner that it causes
physical damage to an individual (e.g. you cut yourself while slicing your
dinner) at some point in its useful lifetime. Do we declare knives evil?
A hammer has a high likelihood of at some point in its useful life causing
physical damage to both an individual and property. Do we declare hammers
DRM is a tool. Tools can be used for good, and tools can be used for evil,
but that does not make a tool inherently good or evil. DRM has a place where
it is a suitable tool, but one should not declare a tool evil simply because
an individual or group uses the tool for purposes that have been declared
                        Joe

@_date: 2002-07-06 18:31:23
@_author: Joseph Ashwood 
@_subject: Closed source more secure than open source 
Ross Andseron's approximate statements:
Closed Source:
Open Source:
bugs remain equally easy to find.
Anonymous's Statements:
I disagree, actually I agree and disagree with both, due in part to the
magnitudes involved. It is certainly true that once Beta testing (or some
semblance of it) begins there will be users that cannot make use of source
code, but what Anonymous fails to realize is that there will be beta testers
that can make use of the source code.
Additionally there are certain tendencies in the open and closed source
communities that Anonymous and Anderson have not addressed in their models.
The most important tendencies are that in closed source beta testing is
generally handed off to a separate division and the original author does
little if any testing, and in open source the authors have a much stronger
connection with the testing, with the authors' duty extending through the
entire testing cycle. These tendencies lead to two very different positions
than generally realized.
First, closed source testing, beginning in the late Alpha testing stage, is
generally done without any assistance from source code, by _anyone_, this
significantly hampers the testing. This has led to observed situations where
QA engineers sign off on products that don't even function, let alone have
close to 0 bugs. With the software engineers believing that because the code
was signed off, it must be bug-free. This is a rather substantial problem.
To address this problem one must actually correct the number of testers for
the ones that are effectively doing nothing. So while L is the extra
difficulty in finding bugs without source code, it is magnified by something
approximating (testers)/(testers not doing anything). It's worth noting that
(testers) > (testers not doing anything) causing the result K =
L*(testers)/(testers not doing anything), to tend towards infinite values.
In open source we have very much the opposite situation. The authors are
involved in all stages of testing, giving another value. This value is used
to adjust L as before, but the quantities involved are substantially
different. It must be observed, as was done by Anonymous, that there are
testers that have no concept what source code is, and certainly no idea how
to read it, call these harassers. In addition though there are also testers
who read source code, and even the authors themselves are doing testing,
call these coders. So in this case K = L*(harassers)/(harassers+coders).
Where it's worth noting that K will now tend towards 0.
It is also very much the case that different projects have different
quantities of testers. In fact as the number of beta testers grows, the
MTBD(iscovery) of a bug must not increase, and will almost certainly
decrease. In this case each project must be treated separately, since
obviously WindowsXP will have more people testing it (thanks to bug
reporting features) than QFighter3
( the lest active development on
sourceforge). This certainly leads to problems in comparison. It is also
worth noting that it is likely that actual difficulty in locating bugs is
probably related to the maximum of (K/testers) and the (testers root of K).
Meaning that WindowsXP is likely to have a higher ratio of bugs uncovered in
a given time period T than QFighter3. However due to the complexity of the
comparisons, QFighter3 is likely to have fewer bugs than WindowsXP, simply
because WindowsXP is several orders of magnitude more complex.
So while the belief that source code makes bug hunting easier on everyone,
is certainly not purely the case (Anonymous's observation), it is also not
the case that the tasks are equivalent (Anonymous's claim), with the
multiplier in closed source approaching infinite, and open source towards 0.
Additionally the quantity of testers appears to have more of an impact on
bug-finding than the discussion of open or closed source. However as always
complexity plays an enormous role in the number of bugs available to find,
anybody with a few days programming experience can write a bug free Hello
World program, but it takes significantly more effort to write something the
complexity of Windows or Linux bug-free. That is where open source receives
the biggest boost, where Microsoft has a limited number of software
engineers, and the Linux development process which has fewer testers but
many more developers. This will reduce the number of bugs simply due to the
effort put into the process by developers.
Which is better? That depends entirely on your situation.
                    Joe

@_date: 2002-06-02 18:24:20
@_author: Joseph Ashwood 
@_subject: FC: Hollywood wants to plug "analog hole," regulate A-D  
Everything I'm about to say should be taken purely as an analytical
discussion of possible solutions in light of the possibilities for the
future. For various reasons I discourage performing the analyzed alterations
to any electronic device, it will damage certain parts of the functionality
of the device, and may cause varying amounts of physical, psychological,
monetary and legal damages to a wide variety of things.
There seems to be a rather siginficant point that is being missed by a large
portion of this conversation.
The MPAA has not asked that all ADCs be forced to comply, only that those in
a position to be used for video/audio be controlled by a cop-chip. While the
initial concept for this is certainly to bloat the ADC to include the
watermark detection on chip, there are alternatives, and at least one that
is much simpler to create, as well as more benficial for most involved
(although not for the MPAA). Since I'm writing this in text I cannot supply
a wonderful diagram, but I will attempt anyway. The idea looks somewhat like
analog source ------>ADC------>CopGate----->digital
Where the ADC is the same ADC that many of us have seen in undergrad
electrical engineering, or any suitable replacement. The CopGate is the new
part, and will not be normally as much of a commodity as the ADC. The
purpose of the CopGate is to search for watermarks, and if found, disable
the bus that the information is flowing across, this bus disabling is again
something that is commonly seen in undergrad EE courses, the complexity is
in the watermark detection itself.
The simplest design for the copgate looks somewhat like this (again bad
        ----CopChip-----|
Where the buffer gates are simply standard buffer gates.
This overall design is beneficial for the manufacturer because the ADC does
not require redesign, and may already include the buffergates. In the event
that the buffer needs to be offchip the gate design is well understood and
commodity parts are already available that are suitable. For the consumer
there are two advantages to this design; 1) the device will be cheaper, 2)
the CopChip can be disabled easily. In fact disabling the CopChip can be
done by simply removing the chip itself, and tying the output bit to either
PWR or GND. As an added bonus for manufacturing this leaves only a very
small deviation in the production lines for inside and outside the US. This
seems to be a reasonable way to design to fit the requirements, without
allowing for software disablement (since it is purely hardware).
            Joe

@_date: 2002-06-03 14:50:47
@_author: Joseph Ashwood 
@_subject: FC: Hollywood wants to plug "analog hole," regulate A-D 
Sent: Friday, May 31, 2002 6:59 PM
To quote myself "the CopChip can be disabled easily," last paragraph
sentence begins with "For the consumer . . . " as has been pointed out by
numerous people, there is no solution to this. With a minimal amount of
electrical engineering knowledge it is possible for individuals to easily
construct a new ADC anyway.
Which can and should be done after conversion.
You seem to be of the mistaken opinion that I believe this to be a good
thing, when the design I presented was designed to minimize cost, of design,
manufacture, and removal. I am of the fundamental opinion that this is not a
legal problem, this is a problem of the MPAA and anyone else that requires a
law like this to remain profitable is advertising incorrectly. The Hollywood
studios have already found the basic solution, sell advertising space
_within_ the program. In fact some movies are almost completely subsidized
by the ad space within the movie. By moving to that model for primary
revenue it is easy to accept that a massive number of copies will be made
since that improves the value of the ad space in your next movie/episode. Of
course I'm not involved with any studio so they don't ask my opinion.
                    Joe

@_date: 2002-06-05 17:44:34
@_author: Joseph Ashwood 
@_subject: Degrees of Freedom vs. Hollywood Control Freaks 
They stopped selling them to the general public, but you only have to stop
by a DJ record shop (as opposed to the consumer shops) to see a wide
selection of vinyl albums. DJs prefer vinyl primarily because it allows beat
matching by hand, scratching, etc. The only disadvantage I know of for vinyl
is that it degrades as it is played, for a DJ this isn't much of a problem
since tracks have a lifespan that's measured in days or weeks the vinyl
becomes useless after a few weeks, which is how long it lasts at good
                    Joe

@_date: 2002-06-15 13:51:21
@_author: Joseph Ashwood 
@_subject: Harry Potter released unprotected  
I believe it's a test. The studio has determined that Harry Potter has
already made a (sizable) profit, so using it for an experiment is
acceptable. By testing on a big budget target they can now determine if
copy-protection costs exceed value.
I don't think so, not yet at least. This looks like just a pilot program.
Watch the normal piracy channels though, if Harry Potter shows up stronger
than other releases Macrovision will be around a while. But if Harry Potter
isn't substantially hit by piracy, then you might want to start shorting
Macrovision, they'll start losing customers.
                    Joe

@_date: 2002-06-18 01:18:06
@_author: Joseph Ashwood 
@_subject: Harry Potter released unprotected  
In it's realistic form, Macrovision has nothing to do with any of it.
However since it is current industry protocol to use Macrovision
copy-protection, Macrovision is of interest. In truth, this isn't even a
question of copy-protection, there's plenty of evidence that none of that
works. Instead this is about a technology, and a company, the technology is
the Macrovision copy-protection technology, and the company explicitly
involved is Macrovision. Macrovision makes the bulk of their profits from
this copy-protection technology, and since it is a copy-protection
technology it is of general interest to many cypherpunks, even if not in any
real way. (see the other reply regarding picture corrections). Because of
Macrovision's heavy reliance on the copy-protection technology for profits,
an undermining of that critical asset will greatly diminish the value of the
company, and so diminish the stock price. For any other purpose, there's
basically no reason for this thread at all. Hope this helped a bit.
                    Joe

@_date: 2002-06-25 14:08:49
@_author: Joseph Ashwood 
@_subject: Ross's TCPA paper 
I disagree that these are entirely contradictory. The first is a statement
in the realm of logic; that if Bob is prepared to deal with whatever
consequences will occur because of the publication of the conversation,
there is nothing Alice can do to stop him (short of killing him before he
The second is a statement in the realm of law, that companies will try to
rid themselves of any requirements that are painful to their bottomline. In
the current case there is a perceived threat of this nature (right or wrong)
regarding the TCPA, that corporations are using their monetary power to
revoke rights that are currently enjoyed.
These statements are not contradictory; Alice can still do as she pleases to
the TCPA devices in her computer, the problem arises that she may have to
deal with substantial civil and criminal penalties for doing so. This is
similar to the question "Is Alice free to walk into a bank carrying a
full-automatic weapon, kill everyone inside, and steal all the money?" yes
she is, but she has to be prepared to deal with the (wo)manhunt that will
begin the moment something like that happens, and the inevitable that she
simply won't live to stand trial (with high probability).
                Joe

@_date: 2002-06-25 14:23:08
@_author: Joseph Ashwood 
@_subject: "Terror Reading" 
I can tell you that at least in some areas that is simply not the case. I
have personal experience with the San Jose City library and know this for a
fact to be incorrect. They store information since the last upgrade of the
central database, currently the better part of a decade, but coming up on a
cycle point. Although it is very difficult to get the information, and large
portions of even that have been lost through various issues.
That is just a single area, but it seems reasonable that most
cities/counties/schools would follow the same general principle. Of course
with the lax way the information is kept it takes nearly a week to recover
the list of books you've checked out in the last month that have been
returned (unless there are penalties), so there is some saving grace to the
                    Joe

@_date: 2002-06-29 17:09:20
@_author: Joseph Ashwood 
@_subject: Piracy is wrong 
Which is a correct statement, but an incorrect line of thinking. Piracy is
an illegitimate use of a designed in hole in the security, the ability to
copy. This right to copy for personal use is well founded, and there are
even supreme court cases to support it. DRM removes this right, without due
representation, and it is thinking like yours that leads down this poorly
chosen path. The other much more harsh reality involved is that DRM cannot
work, all it can do is inconvenience legitimate consumers. There is massive
evidence of this, and you are free to examine them in any way you choose.
These include the expectation that the artist will be paid according to
whatever deal they have signed with their label. Inherent in this deal is
the consumer's right to copy for personal use, and to resell their purchased
copy, as long as all copies that the consumer has made are destroyed. DRM
attempts to revoke this right to personal copying, and resale.
And if the artist cannot accept the fundamental rights specifically granted,
they should not produce art.
Last time I checked the giver is supposed to remove the pricetag from the
gift before giving it. By a similar argument, everyone should be happy that
the WTC flying occured, after all they were kind enough not to kill anyone
that's still alive. The logic simply doesn't hold.
In fact one of the specifically granted rights is the right to share the
music with friends and family, so this has nothing to do with being "a liar
and a cheat" it has to do with excercising not just rights, but rights that
have been specifically granted.
Because of that specifically granted right, that copies can be made for
friends and family, it is also a specifically granted right to accept those
copies. So it is merely excercising a specifically granted right. You
clearly have not read or understood the implications and complexities of
your statements, with regard to either logic or the law.
Apparently it is too complicated for you.
It's just basic rights and excercising of those rights.
If the record companies were prepared to trust, why do they employ a
substantial army of lawyers? Why do they pursue every p2p network? Why are
they pushing for DRM? Trust is not a one-way street. The recording labels
have demonstrated that they cannot be trusted in any form, what delusion
makes you think they can be trusted now?
Exactly, I refuse to accept a DRM -limited environment which does not allow
me full ownership of something I purchased.
No, that's a fundamental misunderstanding of everything involved, from law
to basic logic you have misunderstood it all.
                        Joe

@_date: 2002-05-31 11:38:01
@_author: Joseph Ashwood 
@_subject: How can i check the authenticity of a private key 
Sent: Friday, May 31, 2002 5:30 AM
Sure, and it's fairly easy too. Choose some random data, encrypt with the
public key, decrypt with the private key, if the data isn't corrupted, then
they match. Of course this isn't a perfect way of telling, but with any
given potential key pair it's steep odds. If you want to really be sure,
pass it through a few times.
                    Joe

@_date: 2003-04-27 15:05:01
@_author: Joseph Ashwood 
@_subject: How convenient... 
<20030427112844.GH25990
----- Original Message ----- Indeed it is appearing more so, I found a link to this on CNN.com's front
page  .
First non-bolded sentence "As far as I'm concerned, we do not need to find
any weapons of mass destruction to justify this war." It's becoming clear
that many people in the US simply wanted to beat someone up. The
justification for this war simply hasn't materialized and now at least one
reporter has clearly stated that it doesn't matter.
                    Joe
Trust Laboratories
Changing Software Development

@_date: 2003-06-10 19:20:27
@_author: Joseph Ashwood 
@_subject: An attack on paypal --> secure UI for browsers 
Ok what flavor of crack are you smoking? Because I can tell from here that's
some strong stuff. Downloading random DLLs that are given complete access to
private information is one of the worst concepts that anyone has ever come
up with, even if they are signed by a "trusted" source. Just look at the
horrifically long list of issues with ActiveX, even with WindowsXP (which
hasn't been around that long) you're already looking at more than half a
dozen, and IIRC win95 had about 50. This has less to do with "windows is
bad" than with "secure programming is hard." Arbitrarily trusting anyone to
write a secure program simply doesn't work, especially when it's something
Now for the much more fundamental issue of your statement. Palladium will
never "download site-specific" anything. Palladium is a hardware technology,
not a web browser.
I will refrain from saying Paladium is a bad idea, simply because I see some
potentially very lucrative (for me) options for it's use.
                    Joe

@_date: 2003-06-11 11:52:38
@_author: Joseph Ashwood 
@_subject: An attack on paypal --> secure UI for browsers 
Actually, properly designed Palladium would be little more than a smart card
welded to the motherboard. As currently designed it is a complete second
system that is allowed to take over the main processor. It has a few aspects
of what it should be, but not many. It does include the various aspects of
the smart card, but it also makes room for those aspects to take over the
main system, properly designed this would not be an option, of course
properly designed it could also be a permanently attached $1 smart card that
internally hangs off the USB controller instead of a mammoth undertaking.
I still stand by, "Arbitrarily trusting anyone to write a secure program
simply doesn't work" regardless of how many times MS says "trust us" any
substantially educated person should as well be prepared to either trust a
preponderance of evidence, or perform their own examination, neither of
these options is available. The information available does not cover the
technical information, in fact their "Technical FAQ" about it actually has
the following:
"Q: Does this technology require an online connection to be used?
A: No. "
That is just sooooo enlightening, and is about as far from a useful answer
as possible.
This applies only under the condition that the software in Palladium is
perfectly secure. Again I point to the issues with ActiveX, where a wide
variety of hoels have been found, I point to the newest MS operating system
which has it even been out a month yet? and already has a security patch
available, in spite of their "secure by default" process. Again I don't
believe this is because MS is inherently bad, it is because writing secure
programs is extremely difficult, MS just has the most feature bloat so they
have the most problems. If the Palladium software is actually secure
(unlikely), then there is the issue of how the (foolishly trusted) NCAs are
determined to be the same, this is an easy problem to solve if no one ever
added features, but a hard one to solve where the program evolves, once MS
shows the solution for this, I will point to the same information and show
you a security hole.
After running unattended on your computer, a brilliant
idea, hasn't anyone learned?
Not in the slightest, a single compromise of a single ecommerce site
(remember they're "trusted") will remove all this pretend security. Let's
use a particularly popular example on here right now  they
could easily apply to be an ecommerce site, they collect money, they offer a
service, clearly they are an ecommerce site. Are you really gullible enough
to believe that they won't do everything in their power to exploit the data
transfer problem above, as well as any other holes in Palladium? I should
hope not.
Yes I have, in fact at this point I think it is safe to say that you have
not, or you didn't understand the implications of the small amount of
information it actually contains.
                    Joe

@_date: 2003-06-16 13:00:19
@_author: Joseph Ashwood 
@_subject: An attack on paypal --> secure UI for browsers 
I think there was some substantial miscommunication here (probably my fault
for snipping too much). Even assuming that MS's implementation is perfect,
the NCAs (as suggested by anonymous) would be downloaded from a variety of
sources, assumedly without source code. These are at least as big of a
threat, as the ActiveX saga demonstrates. The problem with ActiveX was never
that the core technology was itself causing problems, the problem was that
the supplemental technologies (the signature verification, the sandboxing
when applicable, etc) were continually being attacked by rogue ActiveX
components (I would consider everything form Gator to be such an attck) that
did undesirable things. Since the can I buy a vowel technology once called
Palladium "protects" these NCAs the result is a new ActiveX-type saga, even
if MS gets everything perfect. Assuming general software rules where bugs
will be present, we're looking at something potentially worse.
But what about everything outside of the micro-kernel? It's still
If that were true, the Palladium would be useless. The LHS, MUST be able to
touch the RHS, otherwise the LHS would be a completely seperate system, with
no software to run on it, performing no computation, and simply taking up
board space and processor time. However, there is a connection, and no
person has any control over what is run in the the LHS. That is in itself
problematic, and leads to a perfect avenue for massive abuse of power. And
as we all know the ability to abuse power, quite quickly leads to the abuse
of power.
Except for the fact that the buggy everything can contact it, and give it a
new NCA, that NCA can do as it pleases.
If you look I never suggested that the nexus itself would necessarily be
insecure, I've said that the supporting technology (everything MS has not
agreed to release) is open to massive abuse, and that the likelihood that it
will have numerous insecurities found is very high.
But what about the rogue NCA? the one that decides to consume all the
processor, store inordinate amounts of information, spy on the user,
provided of course by the "buggy" software that put it there.
I guess I missed where you mentioned NCA at all before this, in fact I went
back and did a text search, the only ways that you mentioned NCAs so far
were "The Nexus, which is the micro-kernel for the trusted components
(NCAs), will be published for review," "and the NCAs acting as the
applications" "whose only job is to service the NCAs, " some rant about
manifest (which is another avenue for attack, but does not present what you
believe, and this one I'm responding to.. All you've done is rant on about
how the nexus this and the nexus that, completely ignoring the fact that
most of the problems with any operating system are not in the kernel
(micro-kernels are relatively easy), the problems continue to stem from the
same source they had in ActiveX, everything around the core component, the
loading, the verification, the scheduling, the stopping them from doing as
they please. These you have not even made an effort to address. You have
ranted around and around, pretending that spouting all these worthless words
actually justify claiming that can I buy a vowel, and all the NCAs that will
be produced by every company, all the hackers, various individuals, etc can
all be trusted. To quote myself yet again "Arbitrarily trusting anyone to
write a secure program simply doesn't work" and that is exactly what is
being claimed about can I buy a vowel.
What proposed attack? I never stated that the information protected by can I
buy a vowel would be compromised, I claimed that it will almost certainly
have gaping security holes, and that it's design makes some very bad
decisions. If each website has it's own NCA, then each website is free to do
as they please on your computer (including read the still encrypted
information form others), "Arbitrarily trusting anyone to write a secure
program" is a bad idea.
(remember when you advocated ECB mode for the XML
Actually yes I do, and I still believe that ECB mode has its advantages in
very specific targets, if you had bothered to actually read the statements I
have made you would understand that. The goal I had for the possibility of
adding ECB was not to provide a common resource, but to push the issue of
clarity, by pushing the issue of ECB even slightly, I succeeded in having
clarity added to the document, both the spec and the XML. If you would
actually care to debate whether or not ECB is a valid mode for inclusion in
a wide usage standard, I suggest first you take it up with NIST, they have
perhaps the oldest standard for it, once you succeed in that, I suggest that
you consider all the border cases, especially the ones where a single block
is encrypted (other modes double the size), random data being encrypted
(which is equivalent security to all other modes), and where a limited
subset of the possible block space is being used (especially useful when
there is an attack that requires a large volume of text pairs). In each of
these cases ECB is at least as secure as any other mode, and in the last it
can be argued that under some circumstances it may actually be more secure.
I remember very well my advocation of ECB for XML Enc, it appears once again
to be you that has faield to grasp realities of the situation.
you could do worse than spending a few hours
Here you go assuming things that simply aren't true. I don't see can I buy a
vowel becoming the "central techonology" for anything for very long. We're
already seeing the X86 processor being replaced at the high end, and slowly
being displaced at the mid-range, can I buy a vowel only has a few years
before it will need to be completely replaced, and since it will take a few
years for it to be adopted it's almost certainly a dead-end technology.
Now the other assumption, you assume that I intend to do security
consulting, how mistaken you are. I am actually the CEO of Trust
Laboratories, and security consulting would almost certainly be a pay cut.
There is a chance that eventually you can make something of yourself "But
you'll have to do your homework"
                    Joe
Trust Laboratories
Changing Software Development

@_date: 2003-05-12 16:27:13
@_author: Joseph Ashwood 
@_subject: New seccure commercial messaging site? 
Try again, this time seriously. I didn't even bother reading most of the
front page where it says you support auto-destruct features. Ummm, bull,
_your_ implementation may support auto-destruct, but you cannot verify the
auto-destruct on the other side. Ok I take it back I did eventually go and
read the rest of the page, it's just as laughable, your entire "product" is
write a message, upload it to our servers, and we'll tell your friends.
Let's assume for now that someone will actually be stupid enough to trust
you. Just in case you hadn't noticed very few systems actually attached to
the internet remain unhacked for a full year. The direct result of this is
most likely someone placing snooping software on your server to read every
message. The next problem, your claim "messages are untraceable," completely
incorrect. Again let's look at the snooping bug, it sees where the data is
coming from, what the data is, and who the data is being sent to, doesn't
sound at all like the untracability I know. Even without the snooping bug,
let's assume your system can't be hacked for whatever reason. I own a
business (Trust Laboratories), we do software assurance, and of course we
have a few secrets, if those secrets are being leaked you can bet the first
thing I'm gonna do is start digging through all the email, web, phone, ftp,
etc. logs I keep on my network, you haven't even begun to address the
capabilities of a real system, all you've created is a fun toy that a few
AOL (or MSN) users* can use to get themselves in even more trouble. Please
do the world a favor and quit wasting bandwidth, oh and BTW your going throu
gh hushmail doesn't deceive us into believing that you don't work for the
piece of sh*t in question.
                        Joe
* side note: While my email address is at msn.com, I don't consider myself a
user of MSN, I haven't visited one of their internal sites since the last
time I had to change my credit card info. I simply keep it because so many
people know this address.

@_date: 2003-05-12 17:09:55
@_author: Joseph Ashwood 
@_subject: A Trial Balloon to Ban Email?  
<018801c31807$1f991670$0b01a8c0
----- Original Message ----- That one's easy. Use a problem that is not in P but is in NP. To make it
clearer to most people, use a problem that can be verified cheaply, but that
can't be solved cheaply. Since it's only everyone's computer Minesweeper is
an example of such a problem. Once a solution has been found it is easy
enough to verify that it is correct (all bombs marked, all non-bomb places
revealed), but it can be prohibitively expensive to compute a large grid.
Other common examples include jigsaw puzzles, digits of pi, etc. More
functional puzzles for this purpose are NP-complete problems; e.g. traveling
salesman, Hamiltonian cycle, SAT, etc. Right now another couple of good
examples would be discrete logarithm, and integer factoring. In all these
cases verifying the solution is cheap (generally travelling the path in the
NP-complete problems, or computing the values in the DL and IF). Verifying
that the puzzle is valid is only slightly more difficult, but retaining an
active list of problems would solve the issue (but open up the possibility
of DOS attacks). Basically it's a fairly easily solved problem.
                        Joe

@_date: 2003-05-13 12:09:56
@_author: Joseph Ashwood 
@_subject: A Trial Balloon to Ban Email? 
<018801c31807$1f991670$0b01a8c0
----- Original Message ----- So you're expecting that everyone will be honest about cashing
micropayments? That seems rather silly, if such a mechanism were to become
required on the internet I'd simply retire today, sign my email accounts
(all except 1) up on every spam list, every mailing list, everything that
would get me thousands of tokens a day, have an automated script cash all
the tokens for me, and I'm generally considered fairly scrupulous.
Additionally there is one major flaw in your design, what's to stop the
spammers from using fake micropayments? The fact that people who believe it
is spam will be unable to cash them? Like they really care about the people
who delete their email. Or were you planning on every intermediate mail
forwarder (all 14 of them between your sending and my recieving on this
list) taking the time out of their busy schedule to verify the
It won't work, the micropayment will be widely reused anyway, the spammers
depending on the bulk of the sends reaching their targets before the
micropayment is cashed. This will in turn increase the burden on the
intermediate servers; because the spammers obviously have to send out far
more now (because so many of their messages never reach the servers), and
the servers need to verify the payments (otherwise the payments mean
nothing). The entire solution only raises the backlog of spam, raises the
requirements for intermediate servers, raises the requriements for end
servers, and introduces new methods of mass abuse. Doesn't exactly sound
like something I want sitting on my network.
                    Joe

@_date: 2003-05-13 19:01:40
@_author: Joseph Ashwood 
@_subject: Email send 
Well I guess that settles the debate over how much spammers pay for their
list of emails doesn't it.
                    Joe

@_date: 2003-05-13 20:21:16
@_author: Joseph Ashwood 
@_subject: A Trial Balloon to Ban Email?  
<5.2.0.9.0.20030512183116.044db8d0
----- Original Message ----- I disagree. If you assume that the entire internet will eventually take up
on the process, start with a rule that says "if it has a hashcash token
don't process the other rules." Obviously at first this rule would be hit
rarely, but a big PR campaign surrounding it would get to people, as would
implementing it in Outlook. Eventually your other rules would be rarely hit,
and you could change them to simply discard. Once it's everywhere you can
begin culling the bad ones. I just don't see where the necessary overhead
bult into the servers will take place, or be justified.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-05-14 16:58:29
@_author: Joseph Ashwood 
@_subject: A Trial Balloon to Ban Email? 
<5.2.1.1.0.20030514102219.03968110
----- Original Message ----- [double-spending problem, and associated unacceptable costs]
The biggest cost I see isn't in the bandwidth or cpu, the cost I see is in
the memory. First let's look at the load incurred on these systems. For a
single email, the email must be held in RAM for the time necessary to verify
the coin (otherwise double spending occurs, filters fail, etc). Obviously
each of these messages (real or fake doesn't matter) will incur a toll on
the memory of the system. Let's assume for the sake of argument that 1 token
costs 1 second to verify (under heavy loads this would be an expectable
number), this is not the CPU-time necessary, but the roundtrip bank time,
where the token enters a queue on each side and slowly makes it's way
through. So every message must be held an additional 1 second for
verification. Now let's look at the impact this would have on say AOL's mail
system. Estimating that they have 35 million members (I believe this is
close to accurate), each of them receiving on average 16 emails a day, and
each email averaging 100kb (AOL appears to use strictly HTMLified email so
this number is not as off as it sounds), this will result in an additional
load of 663 MB of RAM at the very minimum. Since these emails come in
batches and there is additional overhead beyond simply holding the message
in RAM, what you're looking at is probably approaching 50 GB of RAM, last
time I checked a fully loaded Itanium could handle 68 GB, so this is pushing
dangerously close. Now let's assume something like hat happened at Telewest
over the last week or so happens
( where an enormous
quantity of email is sent in, brute force style, now you start requiring
increasing amounts of RAM because although you filter as fast as you can,
you can't send out. This places increaseing load on your resources,
enormously accelerated and aggravated by the necessity of verifying each
coin (you can't contact the bank under such load so the round trip time
starts increasing, reaching several minutes, and eventually stops). For all
it matters you could have 0% cpu load during this, the cpu isn't the
problem. The problem is the full incoming pipe, normally this can be dealt
with by spooling to disk, but the necessity of contacting the bank creates a
rippled effect of this. The net result is that a single short-term attack
can conceivably bring down a mailsystem for days. The net effect of this is
that the small spam problem (those companies tht have a small list of people
they occassionally send unwanted mail to) pretty much goes away, the bigger
fish though are unaffected. Personally I don't much mind some of the small
fish, right now I have several unwanted emails from various conference
companies, but I only get 20 a year, that's handlable, but from cypherpunks
alone I received several times that in really dumb ones today. If we can
find a solution to the big problem (which this doesn't solve at all), I
think the little problem won't seem like a problem at all.
I agree, the problem with the proposal is that it very quickly opens the
door to spammers sending "sufficient quantities" to cripple the entire
payment claim system. The net result is that under the best of conditions
the bank is under a sporadic DDoS from people trying to claim tokens before
anyone else, and at worst the bank is fine because no one can speak from all
the spam being shoved in their mouths, crippling the entire system. Doesn't
sound like much of a winning situation for the "good" side.
                Joe

@_date: 2003-05-23 16:25:40
@_author: Joseph Ashwood 
@_subject: RSA/DSA questions 
Sent: Friday, May 23, 2003 9:30 AM
It depends on what is meant by RSA signatures, 9796 is effectively dead, RSA
PKCS 1 v1.5 is certainly no longer competitive securitywise, PSS is
exceptional, and those are the first 3 that come to mind. Going from this I
would recommend DSA above 9796, and PKCS  v1.5. DSA vs PSS though is
significantly more complicated. Both DSA and PSS rely on the randomness of
the RNG (contrary to popular belief Windows is not inherently bad at RNGs
it's just that it doesn't come with a good one). Collisions in PSS are less
critical than in DSA (an output collision reveals only that the RNG and hash
spit out the same values twice), but PSS suffers from IFPs weakness versus
DLP, this stems from several solid proofs that IFP (integer factoring) can
be no harder than DLP (integer discrete logarithm), and may be mitigated if
you believe that DLP and IFP will reduce to the same problem (the current
algorithms indicate this may in deed be the case), but in the immediate
future DLP is inherently more difficult than IFP. PSS gains though in that
without breaking any standard that I'm aware of the modulus can be extended
indefinitely whereas DSA1 (don't recall DSA2 immediately having such an
issue, but I don't recall DSA2 specifics immediately) has a standard limit
of 1024-bit (the maths scales indefinitely though). The other thing to
consider is speed, since you're using this for SSH, it may be important that
the server be capable of more connections per time, in which case DSA is the
clear winner (RSA wins for verification though for a typcial
neither one is inherently more secure than the other. Personally I have an
affinity for DSA, but that is a personal preference without any fundamental
reason. Pointes to the information itself is out of my immediate reach, I
just upgraded my computer and have yet to completely restore the crypto
                Joseph Ashwood
Trust Laboratories
Changing Software Development

@_date: 2003-05-24 15:12:14
@_author: Joseph Ashwood 
@_subject: CNN.com - Proposed law: $500 per unwanted spam - May. 24, 
References: ----- Original Message ----- Sent: Saturday, May 24, 2003 2:53 PM
In a somewhat similar vein, I'm currently assembling a number of contracts
for my company (Trust Laboratories), and I'm looing for truly effective
anti-spam clauses. So I'm looking for any examples (preferrably examples
tested in court). The value of $500 per spam reported in the CNN article
would certainly be enough deterent, and I'm already planning out a spam
reporting mechanism. If anyone has any examples of such clauses I would
greatly appreciate receiving them, private email to ashwood at msn.com would
probably be the best way (and it would avoid adding to the cypherpunks
bandwidth). I greatly appreciate all replies.
                Joseph Ashwood
Trust Laboratories
Changing Software Development

@_date: 2003-05-30 13:08:01
@_author: Joseph Ashwood 
@_subject: Nullsoft's WASTE communication system 
It should've been pulled for several reasons. The primary one being that it
is basically worthless securitywise. It uses RSA PKCS v1.5 (the one
everyone seems to pick on, and always seems to find a way to be insecure),
Blowfish which supplied a maximum of 150-some gigabytes before insecurity
(birthday paradox), used PCBC which only serves one function and that's
having the longest name. MD5 which should be retired. In short
cryptographically it simply wasn't any good. Now if it was pulled bacause
AOL decided to pull it, I don't have a problem with that.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-08 16:51:04
@_author: Joseph Ashwood 
@_subject: Digital cash and campaign finance reform 
[anonymous funding of politicians]
Simple attack: Bob talks to soon to be bought politician. "Tomorrow you'll
recieve a donation of $50k, you'll know where it came from."
Next day, buyer makes 500 $100 donations (remember you can't link him to any
transaction), 50k arrives through the mix. Politician knows where it came
from, but no one can prove it.
By implementing this we'll see a backwards trend. It will be harder to prove
the buyout (actually impossible), but the involved parties will know exactly
who did the paying. Right now you can actually see a similar usage in the
Bustamante (spelling?) campaign in the California Recall Election, the
Native Americans donated $2M to him in spite of a limit of ~22k by donating
from several people. Same method only now we know who did the paying.
                Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-08 22:32:51
@_author: Joseph Ashwood 
@_subject: Digital cash and campaign finance reform 
You act like they aren't already used to addressing that "problem." I'll go
back to the Bustamante, simply because it is convenient right now.
Bustamante recieved a multi-million dollar donation from the Native
Americans, this was not done through a single check, that would be illegal,
instead it was done through multiple smaller checks, each of which ends up
randomized and delayed in processing (USPS is wonderful source of
randomness), so the actual occurance of the donations is scattered acros
several days, from several accounts, by several people, and I'm sure
Bustamante never even looked to see who the donations were actually from,
just that the full amount arrived. The "problem" that you found, is already
addressed, and already not a problem.
            Joe
Trust Laboratories
Changing Software Development

@_date: 2003-09-09 17:07:56
@_author: Joseph Ashwood 
@_subject: [cdr] Re: Digital cash and campaign finance reform 
I think that's the smartest thing any one of us has said on this topic.             Joe

@_date: 2003-09-22 00:20:19
@_author: Joseph Ashwood 
@_subject: Encrypted search? 
Sent: Sunday, September 21, 2003 3:45 PM
Actually sending the list in encrypted form will create holes, the key is to
not send the list, but to send the information that allows a member to see
that they are on the list.
Correct that won't work. A smarter idea would be to use the user ID and
password to key encryption of a quantity (see Unix password system which is
very similar, but lacked the presence of the user ID).
I believe the answer is no, at least not without leaking large quantities of
information. It is possible I am wrong, but there are simpler, more straight
forward solutions.
Based on my interpretation of the problem there are a number of solutions.
One fairly straight forward one is:
Assuming you have a single file to protect (or a single group of files), and
don't need to protect the number of people who have access, the simple
solution is to use a method very similar to PGP, but without the key
identifiers. While this is just a quick sketch the file undergoes
approximately the following:
establish public keys for each member of the group (e.g. hash(passphrase,
username) = priv, use priv as private key in ECC, everyone can use the same
Choose 2 random keys (K1, K2), and 2 random IVs for CBC (IV1, IV2)
MAC the plaintext file using CBC-MAC, key is K2, IV = IV2
postpend the MAC to the file
Encrypt the file using K1 in CBC mode, IV = IV1
For each member in the group take their public key, and construct a shared
secret (your choice how), that shared secret is used as the key to encrypt
K1 and K2, this (known length) encrypted value is PersonalText[i]
The new file format is:
number of members of group (n)
encrypted file
On each access the accessing person iterates through the PersonalText list,
for each decrypted value (remember there is no authenticator on the value to
save room, and raise the cost of determining membership in bulk) perform a
full decrypt and MAC verification, if the MAC verifies the decryption is
This is rather similar to what PGP and others use for multi-target
encryption, but to speed the process they include key ids of some kind, that
is effectively the only change (assuming proper choices for omissions).
Proving the security of this is more difficult as there is a possibility
that the correlations given by PersonalText[0,n] may provide improved
methods of breakage, this can be addressed using hashes and random numbers
in the PersonalText. However assuming ECC is equivalent to DH key agreement
(almost certainly), the determination of whether a given PersonalText[i] is
for User U is a simple variation of the Decision Diffie-Hellman problem.
Probably not, until they realize that it can be solved with a reuse of
PGP-type messages without key ids, at which point it should be well within
their knowledge.
                Joseph Ashwood
Trust Laboratories
Changing Software Development

@_date: 2004-12-09 19:47:50
@_author: Joseph Ashwood 
@_subject: punkly current events 
Well besides the misinterprettaion of the ruling, which I will ignore, what makes you think MixMaster isn't already dead?
MixMaster is only being used by a small percentage of individuals. Those individuals like to claim that everyone should send everything anonymously, when in truth communication cannot happen with anonymity, and trust cannot be built anonymously. This leaves MixMaster as only being useful for a small percentage of normal people, and those using it to prevent being identified as they communicate with other known individuals.
The result of this is rather the opposite of what MixMaster is supposed to create. A small group to investigate for any actions which are illegal, or deemed worth investigating. In fact it is arguable that for a new face in action it is probably easier to get away with the actions in question to send the information in the clear to their compatriots than it is to use MixMaster, simply because being a part of the group using MixMaster immediately flags them, as potential problems.
In short, except for those few people who have some use for MixMaster, MixMaster was stillborn. I'm not arguing whether such a situation should be the correct way things happened, but that is the way things happened.
                Joe

@_date: 2004-12-10 21:47:25
@_author: Joseph Ashwood 
@_subject: Mixmaster is dead, long live wardriving 
Wardriving is also basically dead. Sure there are a handful of people that do it, but the number is so small as to be irrelevant. Checking the logs for my network (which does run WEP so the number of attacks may be reduced from unprotected) in the last 2 years someone (other than those authorized) has attempted to connect about 1000 times, of those only 4 made repeated attempts, 2 succeeded and hit the outside of the IPSec server (I run WEP as a courtesy to the rest of the connection attempts). That means that in the last 2 years there have been at most 4 attempts at wardriving my network, and I live in a population dense part of San Jose. Wardriving can also be declared dead. Glancing at the wireless networks visible from my computer I currently see 6, all using at least WEP (earlier there were 7, still all encrypted). I regularly drive down through Los Angeles, when I have stopped for gas or food and checked I rarely see an unprotected network. The WEP message has gotten out, and the higher security versions are getting the message out as well. Now all it will take is a small court ruling that whatever comes out of your network you are responsible for, and the available wardriving targets will quickly drop to almost 0.
Wardriving is either dead or dying.
Now we're back to the MixMaster argument. Mixmaster was meant to be a "Napster-level popular app" for emailing, but people just don't care about anonymity. Such an app would need to have a seperate primary purpose. The problem with this is that, as we've seen with Freenet, the extra security layering can actually undermine the usability, leading to a functional collapse. If a proper medium can be struck then such an application can become popular, I don't expect this to happen any time soon.
                Joe

@_date: 2004-06-18 13:57:42
@_author: Joseph Ashwood 
@_subject: A National ID: AAMVA's Unique ID 
<065701c44946$272f34c0$6401a8c0   <200406171731.i5HHV6d9020276
----- Original Message ----- ; Sent: Thursday, June 17, 2004 10:31 AM
I think you misunderstood my point. My point was that it is actually
_easier_, _cheaper_, and more _secure_ to eliminate all the silos. There is
no reason for the various silos, and there is less reason to tie them
together. My entire point was to put my entire record on my card, this
allows faster look-up (O(1) time versus O(lg(n))), greater security (I
control access to my record), it's cheaper (the cards have to be bought
anyway), it's easier (I've already done most of the work on defining them),
and administration is easier (no one has to care about duplication).
I think they are drawing the line a bit finer than either of us would like.
They don't call it a national ID because it being a national ID means that
it would be run by the federal government, being instead run by state
governments, it is a state ID, linked nationally.
As I said in the prior one, I disagree with any efforts to create forced ID.
Well then create a High-Security ID card company, build it on the technology
I've talked about. It's fairly simple, file the paperwork to create an LLC
with you and Robyn, the LLC acquires a website, it can be co-located at your
current office location, the website talks about my technology, how it
allows the unique and secure identification of every individual, blah, blah,
blah, get a credit card issued in the correct name. They'll almost certainly
let you in, you'll look and smell like a valid alternative (without lying
because you could certainly offer the technology), if you really want to
make it look really good I'm even willing to work with you on filing a
patent, something that they'd almost certainly appreciate.
Of course it won't, their "mission and service" is to offer the strongest
identity link possible in the ID cards issued nation-wide, as such the
citizen's course of action has to be to govern the states issuing these
identication papers. However, if you offer them technology to actually make
their "mission and service" cheaper, more effective, and as a side-benefit
better for their voters. Besides, if you can't beat them (you won't stop
them, no matter what you do) at least improve the situation, you could
easily become a far wealthier individual and improve our general security
versus the alternatives.
Very much so, but I also realize that there are far more people who are more
than willing to be "ear-tagged" than those of us willing to fight, as such
what we need to do is fight on a fundamental basis, the most fundamental
benefit offered by good technology in doing this is the cost savings
(regardless of the improved security). As such we need to wage a war on two
fronts, on one front we work to destroy the basis on which they can enstate
these measures, this will work to scale-back the deployment. The second
front is to make it more secure as it does get rolled out, and to build the
technology in such a way that their invasive tactics can be thrown out by
the voting population without destroying the core usefulness of the system
(e.g. it can still be a driver's license).
Such a two-front war is complex and difficult, but if the first front is
completely successful we have gained our desires, the second-front is only
there to erode the invasiveness and provide an abort-path for getting rid of
the technology.
I guess my further point is that sometimes disruptive activities only
results in them hiding from you while they work, but delicate adjustments
can result in real changes.
                Joe

@_date: 2005-02-04 01:54:03
@_author: Joseph Ashwood 
@_subject: Dell to Add Security Chip to PCs 
<1107495901.4338.5.camel
----- Original Message ----- That issue has been dealt with. They do this by initializing the chip at the production plant, and generating the certs there, thus the process of making your software TCPA work actually involves faking out the production facility for some chips. This prevents the re-init that I think I saw mentioned a few messages ago (unless there's some re-signing process within the chip to allow back-registering, entirely possible, but unlikely). It even gets worse from there because the TCPA chip actually verifies the operating system on load, and then the OS verifies the drivers, solid chain of verification. Honestly Kaminsky has the correct idea about how to get into the chip and break the security, one small unchecked buffer and all the security disappears forever.
                    Joe
Trust Laboratories
Changing Software Development

@_date: 2005-02-16 21:06:44
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
I believe you are incorrect in this statement. It is a matter of public record that RSA Security's DES Challenge II was broken in 72 hours by $250,000 worth of semi-custom machine, for the sake of solidity let's assume they used 2^55 work to break it. Now moving to a completely custom design, bumping up the cost to $500,000, and moving forward 7 years, delivers ~2^70 work in 72 hours (give or take a couple orders of magnitude). This puts the 2^69 work well within the realm of realizable breaks, assuming your attackers are smallish businesses, and if your attackers are large businesses with substantial resources the break can be assumed in minutes if not seconds.
2^69 is completely breakable.
                Joe

@_date: 2005-02-18 03:11:30
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
<421476B9.9040206
----- Original Message ----- Sent: Thursday, February 17, 2005 2:49 AM
I believe you substantially misunderstood my statements, 2^69 work is doable _now_. 2^55 work was performed in 72 hours in 1998, scaling forward the 7 years to the present (and hence through known data) leads to a situation where the 2^69 work is achievable today in a reasonable timeframe (3 days), assuming reasonable quantities of available money ($500,000US). There is no guessing about what the future holds for this, the 2^69 work is NOW.
----- Original Message ----- ; "Cryptography" What you're missing in this is that Deep Crack was already a year old at the time it was used for this, I was assuming that the most recent technologies would be used, so the 1998 point for Deep Crack was the critical point. Also if you check the real statistics for RC5-64 you will find that Distributed.net suffered from a major lack of optimization on the workhorse of the DES cracking effort (DEC Alpha processor) even to the point where running the X86 code in emulation was faster than the native code. Since an Alpha Processor had been the breaking force for DES Challenge I and a factor of > 1/3  for III this crippled the performance resulting in the Alphas running at only ~2% of their optimal speed, and the x86 systems were running at only about 50%. Based on just this 2^64 should have taken only 1.5 years. Additionally add in that virtually the entire Alpha community pulled out because we had better things to do with our processors (e.g. IIRC the same systems rendered Titanic) and Distributed.net was effectively sucked dry of workhorse systems, so a timeframe of 4-6 months is more likely, without any custom hardware and rather sad software optimization. Assuming that the new attacks can be pipelined (the biggest problem with the RC5-64 optimizations was pipeline breaking) it is entirely possible to use modern technology along with GaAs substrate to generate chips in the 10-20 GHz range, or about 10x the speed available to Distributed.net. Add targetted hardware to the mix, deep pipelining, and massively multiprocessors and my numbers still hold, give or take a few orders of magnitude (the 8% of III done by Deep Crack in 23 hours is only a little over 2 orders of magnitude off, so within acceptable bounds).
2^69 is achievable, it may not be pretty, and it certainly isn't kind to the security of the vast majority of "secure" infrastructure, but it is achievable and while the cost bounds may have to be shifted, that is achievable as well.
It is still my view that everyone needs to keep a close eye on their hashes, make sure the numbers add up correctly, it is simply my view now that SHA-1 needs to be put out to pasture, and the rest of the SHA line needs to be heavily reconsidered because of their close relation to SHA-1.
The biggest unknown surrounding this is the actual amount of work necessary to perform the 2^69, if the workload is all XOR then the costs and timeframe I gave are reasonably pessimistic, but if the required operations are dynamically sized mulitplies then the time*cost is off by some very large Even simple bulk computation assuming full pipelining says that 4700 4 GHz to complete 2^69 operations in 1 year, even assuming using full 3.8 GHz pentium 4s instead of a more optimal package only leads to a processor cost of 3.1 million for a 1 year 2^69, dropping that down to 2.4GHz celerons requires 7800 of them, but only $538,000. Moving to DSPs and FPGAs the costs will drop substantially, but I don't feel like looking it up, and as the costs drop the number of processors that can be used increases linearly additionally as the individual speeds drop the purchase cost drops better than linearly. I am quite confident that with careful engineering a custom box could be produced for the $500,000 mark that would do 2^69 operations in the proper timeframe. With deep pipelining any complexity of 2^69 operations could be done in the timeframe, but will scale the price. I suppose I should also point out an unspoken qualifier, I am assuming a large number of these machines will be built reducing the engineering overhead to miniscule, for a one-off project this will likely be the dominant cost.
2^69 work is achievable, the cost multiplier associated will be the determining factor.
                Joe

@_date: 2005-02-18 22:46:34
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
<421476B9.9040206
----- Original Message ----- Sent: Friday, February 18, 2005 3:11 AM
[the attack is reasonable]
Reading through the summary I found a bit of information that means my estimates of workload have to be re-evaluated. Page 1 "Based on our estimation, we expect that real collisions of SHA1 reduced to 70-steps can be found using todays supercomputers." This is a very important statement for estimating the real workload, assuming there is an implicit "in one year" in there, and assuming BlueGene (Top 500 list slot 1) this represents 22937.6 GHz*years, or slightly over 2^69 clock cycles, I am obviously still using gigahertz because information gives us nothing better to work from. This clearly indicates that the operations used for the workload span multiple processor clocks, and performing a gross estimation based on pure guesswork I'm guessing that my numbers are actually off by a factor of between 50 and 500, this factor will likely work cleanly in either adjusting the timeframe or production cost.
My suggestion though to make a switch away from SHA-1 as soon as reasonable, and to prepare to switch hashes very quickly in the future remains the same, the march of processor progress is not going to halt, and the advance of cryptographic attacks will not halt which will inevitably squeeze SHA-1 to broken. I would actually argue that the 2^80 strength it should have is enough to begin its retirement, 2^80 has been "strong enough" for a decade in spite of the march of technology. Under the processor speed enhancements that have happened over the last decade we should have increased the keylength already to accomodate for dual core chips running at 20 times the speed for a total of 40 times the prior speed (I was going to use Spec data for a better calculation but I couldn'd immediately find specs for a Pentium Pro 200) by adding at least 5 bits preferrably 8 to our necessary protection                 Joe

@_date: 2005-02-20 18:41:18
@_author: Joseph Ashwood 
@_subject: SHA1 broken? 
<421476B9.9040206    <42176111.7050501 <20050219165341.GR1404
  <4217AE53.4060006
----- Original Message ----- That is only misreading my statements and missing a very large portion where I specifically stated that the new machine would need to be custom instead of semi-custom. The proposed system was not based on FPGAs, instead it would need to be based on ASICs engineered using modern technology, much more along the lines of a DSP. The primary gains available are actually from the larger wafers in use now, along with the transistor shrinkage. Combined these have approximately kept the cost in line with Moore's law, and the benefits of custom engineering account for the rest. So for exact details about how I did the calculations I assumed Moore's law for speed, and an additional 4x improvement from custom chips instead of of the shelf. In order to verify the calculations I also redid them assuming DSPs which should be capable of processing the data (specifically from TI), I came to a cost within a couple orders of magnitude although the power consumption would be substantially higher.
                Joe

@_date: 2005-10-23 16:17:38
@_author: Joseph Ashwood 
@_subject: [smb@cs.columbia.edu: Skype security evaluation] 
Tom Berson's conclusion is incorrect. One needs only to take a look at the
publicly available information. I couldn't find an immediate reference
directly from the Skype website, but it uses 1024-bit RSA keys, the coverage
of breaking of 1024-bit RSA has been substantial. The end, the security is flawed. Of course I told them this now years ago, when I told them that 1024-bit RSA should be retired in favor of larger keys, and several other people as well told them.
                    Joe

@_date: 2006-03-08 21:30:18
@_author: Joseph Ashwood 
@_subject: Re: 
<4ef5fec60603082022q6f7260eeq668d9c97737a7aba
  <20060308222922.O52713
----- Original Message ----- Sent: Wednesday, March 08, 2006 8:39 PM
I'm still around. Cypherpunks are like roaches, if you see one there are hundreds more.
            Joe

@_date: 2006-03-13 18:19:34
@_author: Joseph Ashwood 
@_subject: On being a cypherpunk 
<20060312184941.A79019   <20060313222236.GS75666   <20060313225123.GT75666   <4918801a0603131515k53b2adb8h8b8e9232eed2ae7b
  <20060314000948.GV75666
My 2 cents.
----- Original Message ----- I write code because someone has to redeem my birthday after 1945 ushered in the atomic age.
I write code because I live by "Judge a man by his works, not by his worth"
                Joe

@_date: 2006-10-19 15:25:30
@_author: Joseph Ashwood 
@_subject: Regarding Windows Vista Disk Encryption Algorithm. 
Sent: Thursday, October 19, 2006 5:55 AM
The short answer: they do.
If you can't trust the hardware vendor there are worse things they can do to you. But in essence you either trust them or you don't.
This changes the rules some, but generally speaking with modern encryption, if the key is not available you're screwed.
You missed the part where it can only be done with the administrator There is no reason to hide the boot block, but too many uneducated users would go "But they can find the boot block" and complain about how the security MUST be weak, based on a gross misunderstanding of the situation.
I don't know.
Without the introduction of another key it is impoosible to improve on the security proof of CBC, so what they've done is introduce a method of obfuscation that they hope will not be broken, but breaking it will not affect the security of CBC mode in any way, simply because if it did break AES-CB, an attacker could apply it themself quite cheaply. The proof basically boils down to: it's CBC, attacker loses.
                    Joe

@_date: 2006-10-24 14:42:46
@_author: Joseph Ashwood 
@_subject: Mixmaster? 
<453C8BF1.9080701
  <570521b20610241321x1d213282q6ae00f95b2cfba3e
----- Original Message ----- Start with the obvious: What purpose would it serve? You say you "need" one, why? Why do you need to run a server? The client for Mixmaster is an email client. As a result there is no need for a billion mixmaster servers out there, since the high bandwidth big mail systems run on UNIX, and those are the primary target for Mixmaster, I really don't see the purpose in supporting anything else.
                    Joe

@_date: 2010-02-23 05:27:04
@_author: Joseph Ashwood 
@_subject: Markets are efficient if and only if P = NP 
References: <4F221964-482B-4C31-8DA6-27757917238B
  <4B83CBD9.6030908
I disagree, its actually useful, in a particularly odd way. The profitability of hedge funds says that the markets are not efficient (one of their founding principles). So the inability of a massive multiprocessing system to build proper market efficiency indicates that no one currently has a solution to prove P=NP. There is also far more money in economic research than in algorithmic research, so anyone who manages to bring the market to being efficient will in fact have proven P=NP long before the algorithm is known. From a security standpoint this is extremely valuable information. Maybe Maymin should apply the same processes and become a hedge fund manager, they make a lot more money than mere engineers.
                Joe
