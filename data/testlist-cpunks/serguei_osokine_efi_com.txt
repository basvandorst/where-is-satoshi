
@_date: 2005-12-12 11:24:33
@_author: Serguei Osokine 
@_subject: [p2p-hackers] p2p in some place or other 
not sure if their results directly apply to P2P nets. They are talking
about six nines and replication factor of 20 to 80. They would likely
commit suicide if they would try to actually use Gnutella for rare
content. Any improvement would be nice - and forget about six nines.
results are very hard to verify and to reproduce, which is absolutely
necessary if one would want to repeat their calculations with some
different assumptions about the system requirements.
trace from April of 2003. Back then Gnutella was more than an order of
magnitude smaller, and it would be interesting to repeat the
calculations for today's situation. But the properties of this trace
are not explicitly listed anywhere, being hidden in multiple charts
and obscure statements like "only 5,000 of the 33,000 Gnutella hosts
were usually available" (This, by the way, is a total mystery to me,
since in April of 2003 Slyck's stats archive lists Gnutella at about
90,000 simultaneous nodes, so I have no idea where these 5,000 or
33,000 came from and what their meaning might have been.)
I do not trust any one of their conclusions, as far as the caching
in P2P file-sharing network is concerned. All their reasonings
should be repeated for the reliable network statistical data, and
with the set of requirements that reflects the needs of P2P users,
not the need for a six nines-reliable data storage. I suspect that
then the conclusions might prove to be a bit different.
Eugen* Leitl leitl ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2005-12-13 11:50:31
@_author: Serguei Osokine 
@_subject: [p2p-hackers] p2p in some place or other 
saying. Yes, at some point I used to place hight hopes on this method,
basically thinking that the transfer rates for the rare content can
be improved at the expense of the popular one. Popular content can be
found at lots of places anyway, so penalizing it should not hurt all
that much; for me the goal was to equalize the download rates for
all content regardless of its popularity. So if improving the rare
content download speed would make the widely distributed content
transfers a bit slower (because the systemwide cumulative uplink
bandwidth is a scarce resource, after all), so be it.
(the one that I've already quoted in this thread) shows that from
the uploader standpoint the prioritization of rare vs popular content
does not cover a very significant percentage of all upload situations.
rare, so give the rare more bandwidth". Just as widespread is "many
rare uploads from one node", in which case changing their relative
priorities is pointless, and also "rare upload from a single node",
in which case no matter what this node does, the speed is going to
be substandard.
very common. Essentially the download speed for the rare content is
limited by the uplink rates of the nodes with rare content, even if
all the nodes are always on and spend just a small percantage of their
online time downloading. For popular content, you can have very fast
downloads in such a case; you can even saturate your downlink if you
wish. But for rare content, you're still stuck with whatever is the
uplink rate of a single node that has this file.
becomes more and more pronounced no matter how you prioritize the
uploads. And seeing this causes the user frustration on a significant
percentage of all downloads (on everything that is in the long tail).
Eugen* Leitl leitl ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]
