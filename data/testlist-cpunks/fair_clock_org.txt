
@_date: 1995-11-22 07:24:04
@_author: Erik E. Fair (Time Keeper 
@_subject: "The Right To Privacy" by Caroline Kennedy, et. al. 
Did anyone here go to this event, or has read the book yet? There is a
review of it in TIME magazine at this URL:
Erik Fair

@_date: 1995-10-05 09:26:42
@_author: Erik E. Fair (Time Keeper 
@_subject: Rethinking the utility of netnews "cancel" control messages 
The day of unauthenticated netnews control messages of any kind is
basically over. We gotta:
1. turn off all automated system-wide control of netnews, or
2. properly authenticate all such messages (newgroup, rmgroup, cancel, etc.).
I think we also ought to think carefully about continuing to have a
"cancel" control message (and the Supercedes: header) any more,
authenticated or not - as useful as this mechanism occasionally is to
remove unsightly spams (and other Officially Troublesome Material), isn't
this kind of casual revisionism something that is, historically, to be
If you had a netnews system which simply marked a message as cancelled in
some way, would you set your netnews reader to seek out cancelled messages?
Or ignore them?
If you were a librarian or historian operating The Official USENET Archive
of Everything, would you accept and process cancel control messages?
        "Backbone cabal? What's that?"
        Erik Fair

@_date: 1995-10-05 10:19:05
@_author: Erik E. Fair (Time Keeper 
@_subject: Rethinking the utility of netnews "cancel" control messages 
Is it? The principal effects of not having the mechanism is a slightly
higher disk storage requirement for netnews - something completely unheard
of in the annals of USENET.
The downsides of having the mechanism (especially unauthenticated) we see
now: official and unofficial squelching of articles that someone doesn't
like for whatever arbitrary or situational reason.
In the long run, which is the more detrimental effect? It isn't desireable
for systems to be perfectly efficient, if they generate imperfect results;
as I understand it, the ponderousness of our federal legislative system was
designed in for precisely this reason: they were optimizing for long term
correctness, instead of efficiency.
Frankly, I think that if the question were posed correctly, I'm sure that
Brad Templeton (President of Clarinet) would think carefully about
answering it, since it has quite a few aspects.
I'm just trying to stimulate a little more careful thought about this as a
philosopical issue, before you go whack on INN again...
Erik Fair

@_date: 1995-10-05 12:14:32
@_author: Erik E. Fair (Time Keeper 
@_subject: Rethinking the utility of netnews "cancel" control messages 
This, presumably, is a lawyer, discovery, lawsuit thing, right?

@_date: 1995-10-06 00:36:09
@_author: Erik E. Fair (Time Keeper 
@_subject: FORGED CANCELS of posts on n.a.n-a.m 
An NNTP v2 protocol specification effort is underway in the IETF now.
Strictly to clean up existing extended practice - not to do new things.
Erik Fair

@_date: 1995-09-14 11:18:54
@_author: Erik E. Fair (Time Keeper 
@_subject: Mixmaster posting poll 
Rich Salz wrote mail2news, based on some ugly stuff I wrote, ages ago.
Erik Fair

@_date: 1995-09-15 09:04:16
@_author: Erik E. Fair (Time Keeper 
@_subject: NSA on GAK 
Um, not really. I dunno about you, but I don't bother to get new ROMs from
cisco for each new software release - I FTP the code over the net, and
write it into flash RAM in the routers (or net boot it). So, in fact, there
are many sites (and backbones) for which that statement is literally true.
Erik Fair
P.S.    And yes, cisco does publish MD5 hashes of their binaries.

@_date: 1995-09-19 09:04:52
@_author: Erik E. Fair (Time Keeper 
@_subject: Verification of Random Number Generators 
Just an idle thought: it might be possible to do a probabalistic
verification of a RNG by sampling it over some number of samples, and
statistically analyzing the sample space. This would be analysis under the
model of "RNG as black box" as opposed to (or rather, if you're smart, in
addition to) code inspection & review. Any statisticians among us?
Erik Fair

@_date: 1995-09-20 04:36:04
@_author: Erik E. Fair (Time Keeper 
@_subject: Please send me SSL problems... 
Jeff, the SSL specification has a severe *architectural* problem - it
assumes that Internet Protocols are APIs - interface standards, and that
you can just slide a "layer" underneath without anyone noticing. Such is
not the case - all the Internet Protocols are real protocol standards, in
that they specify the syntax, order, and semantics of the actual bits on
the wire. The IETF quite explicitly doesn't care about APIs - that's a host
software issue, and it doesn't matter what the host software looks like (or
even what the machine looks like), so long as it gets the bits on the wire
right, according to the protocol spec. This is how the Internet can make
very strong guarantees about interoperability.
You can't fiddle with a communication protocol without getting agreement
from everyone about the change, or extend it in a way that is compatible
with the protocol you're modifying, on a per-protocol basis (e.g. adding a
TELNET negotiation option to TELNET for encryption, an FTP command to FTP,
etc). Otherwise, all you've done is made a private, non-interoperable
change to an existing protocol that guarantees interoperability *failures*
between systems that implement the existing specification, versus your own
version of HTTP, or TELNET, or whatever. In short, the SSL specification,
as written, proposes to change all Internet application protocols, globally
- "slide in a layer." That's not how it's done, and it's not the right
place to do it, even if it appears to work in an enclave of systems.
About the SSL protocol, encryption algorithms, or the SQA that went into
'em, I think other people have expounded on those issues eloquently, and so
I have nothing to add to that.
Erik Fair

@_date: 1995-09-20 10:09:50
@_author: Erik E. Fair (Time Keeper 
@_subject: Please send me SSL problems... 
And we see how far *that* effort has gotten...
There was some discussion in Toronto last summer about APIs for the basic
transports (i.e. standardizing "sockets", or TLI, or whatever), which got
backed off to a list of "required service elements" that a good stack
vendor should make available to the app programmers, and then the whole
notion got killed off for the reasons I cited.
GSSAPI was an attempt to make it easy to slide in authentication &
encryption into existing software - lay a foundation for real security in
the applications. A fine goal, but a bad plan for achieving the goal. I
think they were also trying to avoid blessing any particular crypto scheme,
to avoid both the export morass, and the patent morass - "we'll drop in
whatever we can get on good terms, later."
API and interface standards are to be avoided in part because of the
reasons I cited previously, in part because they're hard to do right for
all platforms (not everyone uses function-call style system/library calls),
and in part because they do not guarantee you interoperability - classic
case in point is the Microsoft Mail API (MAPI): you can put *anything*
under MAPI: Novell MHS, cc:Mail, QuickMail, or SMTP, just to name a few. If
you are not speaking the same wire protocol as your intended correspondent
(or peer), you lose, regardless of the fact that your software and hers are
both using the same API - you cannot interoperate.
What really annoys me is the fuss you see in the trade rags about
"middleware" these days; they've missed this entire point about interfaces
versus protocols, and they're propagating the misconception that interfaces
give interoperability to the general marketplace. And the vendors are
laughing all the way to the bank.
Erik Fair

@_date: 1995-09-20 10:51:33
@_author: Erik E. Fair (Time Keeper 
@_subject: RSA Prevails In Arbitration Against Cylink 
Is there electronic copy of the Arbitration Panel's precise ruling
available? No quicker way to end the PR confusion than to read the Real

@_date: 1995-09-25 10:37:25
@_author: Erik E. Fair (Time Keeper 
@_subject: Persistent Services Needed 
One way to establish persistent services is to use the DNS for indirection:
register a name for a service (or a set of services), which then point to
servers of those services by a DNS name. If the service needs to move
(hosts, net connections, etc), the only thing that changes is the DNS zone
file and the references to the service through the name stay exactly the
same. Hell, if your service requires no state information or can have
replicated data (e.g. DNS, FTP, WWW), you can use "round robin" techniques
with very low DNS RR TTL's to spread the service load over a bunch of
widely distributed hosts.
The NetBSD gang understand this principle: netbsd.org has several servers
all over the place:
E-mail to netbsd.org is handled at MIT.
 is served up by WWU.EDU
ftp.netbsd.org is at CMU.
Perhaps Eric Hughes can be prevailed upon to permit "privacy.net" to be
used in this manner.
Erik Fair

@_date: 1995-09-27 22:40:30
@_author: Erik E. Fair (Time Keeper 
@_subject: "Notes" to be Eclipsed by "Netscape" 
Netnews was the old "msgs" program on serious steroids - the thing everyone
was supposed to run in their .login (or .profile) scripts to get
system-wide announcements. My bet is that msgs was inspired by the
TOPS-20/ITS equivalents at MIT. Netnews subsequently underwent relatively
rapid forced evolution in its early days to meet the scaling demands of the
UUCP network, and the Internet of that time (~1983).
The "notesfiles" system from UIUC that Rob Kolstad and Ray Essick wrote was
not so much a distant cousin of NetNews as it was a similar system designed
to solve the same problem (distributed message-based computer
conferencing); I would argue that NetNews had the better transports and
backends, but notesfiles was one or two up on NetNews in UI features
(message threads, etc). The two were sufficiently close that (bad) gateways
were written to move messages from one system to the other.
With any luck, the next round of NetNews user interfaces will remove all of
the UI advantages of notesfiles - the hooks have always been there, but
writing good UI's hard work, and most NetNews hackers (me included) have
had more fun/luck/interest in hacking the transport level to be ever more
slightly efficient.
I had the impression from what I read that this was going to be an IPX WAN,
and that after announcing this Brave New Service, the partners discovered
just how poorly IPX behaves on a WAN, and so have backed out to Notes on IP
for this thing. I haven't heard much about it since, but I'd be surprised
to find AT&T being foolish enough to try and operate an IPX WAN.
Lotus is indeed one of RSA's licensees; I remember reading that in the WSJ
at about the same time that Apple became one.
I still place my message-based distributed collaboration bets on NetNews
technology, or some obvious derivative of it.
Erik Fair

@_date: 1996-02-03 04:35:21
@_author: Erik E. Fair (Time Keeper 
@_subject: Espionage-enabled Greed 
This scenario has one problem: the providers have determined that large
public peering points like the CIX, NAPs, MAEs, and FIXs do not scale well,
and that for the continued health and growth of the Internet, there are
going to have to be more small, private interconnects between providers.
Put another way: if the equipment you're working with has certain limits
(let's say 100Mb/s FDDI or 45Mb/s T3/DS3 interfaces), it's better to have
more interconnects with fewer peers at each interconnect point when your
traffic potentially or actually will exceed those interface limits in
aggregate. This is being driven by the incredible growth of the Internet,
and by the fact that the customers can (and do) buy the same size pipes
into the providers that the providers themselves use for their backbones -
i.e. any such customer can potentially fill your backbone around the
section of your backbone where he connects to you. Ooops.
If you want to have fewer, large interconnects, which, incidentally, you
can monitor all the traffic passing through, you've gotta have monstrous
point-to-point bit pipes and/or LANs, and the Router/Switch From Hell to
make the traffic move. There are people trying to build such things - it's
called Asynchronous Transfer Mode (ATM), but it doesn't really work in
practice yet at high enough speeds - best you can get at the moment is OC3
(155Mb/s), which is only a trifle faster than FDDI, and the stuff is more
expensive than conventional LAN/WAN technology, so it's only being used in
small areas to prove the technology (with the hope that it really does
scale as promised, and gets cheaper). There are working examples of a fast
LAN switch in use at the public peering points: the DEC GIGAswitch (3.2Gb/s
aggregate - 16 100Mb/s FDDI ports).
Of course, you also have to build a pretty fast computer to suck down all
this traffic and analyze it, too. And we all have the ultimate laugh on
would-be eavesdroppers: IP security (read: end-to-end encryption of the
data payload of IP packets on a per peer basis), drafts for which are in
implementation phase as of the Stockholm IETF meeting (July 1995). This
leaves 'em with just traffic analysis to use on us.
Erik Fair

@_date: 1996-01-25 13:54:51
@_author: Erik E. Fair (Time Keeper 
@_subject: another thought about random numbers 
While musing over a roulette table, and noticing the preponderence of
electronic games in the various Casinos in Stateline, NV, a thought
occurred: does anyone know what sorts of random number generators those
electronic games use, and how (if at all) they are measured and regulated
by the Nevada Gaming Commission? They might have something to teach us.
Erik Fair

@_date: 1996-07-04 14:03:48
@_author: Erik E. Fair (Time Keeper 
@_subject: blocking software & brock meeks 
I agree with you in general, however, Brock and Declan have a point
to make too: these companies need to differentiate themselves based on two
1. basic philosophy of filtering (why they filter what they filter).
2. diligence in keeping up their databases.
Brock & Declan are right to expose the basic filtering philosophies of the
different companies, so that those of us who may wish to avail ourselves
their services know exactly what we're getting (or rather, not getting).
In the end, the market will choose between the simple "no porn" philosophy
(for whatever your definition is of that), and the "christian family values
approved by the christian coalition" philosophy (with, one hopes, a whole
lot of other points on the spectrum in the middle). However, the consumers
cannot make this choice absent the information; Brock & Declan have done
everyone a service by shining some light on this.
What I'm surprised about is that these companies apparently aren't already
trumpeting their philosophies of filtering themselves. The principle
differentiator for this market is not the software - there really aren't
that many ways to filter this stuff, and these companies ought to share
their techniques in that area so that they can all be more effective and
thus serve their customers better. The real differentiator is what's in
their databases, which (one presumes) is driven by each of their
philosophies of what is "harmful" to minors.
One wonders if these companies might be embarassed to actually take a
public position on this burning issue: just exactly what *is* "harmful" to
minors? Personally, I fail to see how they can avoid it - it is the essence
of their entire business.
        Erik Fair

@_date: 1996-07-05 04:48:27
@_author: Erik E. Fair (Time Keeper 
@_subject: Word lists for passphrases 
You could just snarf up a week's worth of netnews...
        Erik

@_date: 1996-03-21 20:28:01
@_author: Erik E. Fair (Time Keeper 
@_subject: Dorothy Denning attacks Leahy's crypto bill 
I feel a bit stupid right now. I read Leahy's bill from top to bottom
shortly after it was submitted, and aside from the provision which
separately criminalizes the use of encryption in a the comission of a crime
or to obstruct justice, I fail to see the flaws that you see.
Could you spend a little time enumerating the flaws in this bill as you see
them, with reference to the particular wording in Leahy's bill?
thanks for your time & trouble,
Erik Fair

@_date: 1996-05-07 09:06:48
@_author: Erik E. Fair (Time Keeper 
@_subject: [History] USPS tried to monopolize email? (c. 1981) 
The U.S. Postal Service's first attempt at E-mail was called "E-COM" (ca.
1984), and it amounted to an electronic submission system for mail that
would then be printed, stuffed into envelopes, and delivered in the usual
way - but done so at the regional centers. It was geared toward 3rd class
mass mailings, and was a dismal failure. While it was cheaper than standard
3rd class mailings, the mailings were output on Printronix dot-matrix line
printers, and they looked terrible. Who knows? If they'd invested in laser
printers instead...
Some of you who were on the UUCP/USENET at the time may remember a small
company on the UUCP network in Rockville, MD called "netword", which would
accept E-mail for E-COM and deliver it for free; the deal was that the
input batches to E-COM had to be of a certain size, and the "netword" folks
rounded out their batches with the stuff from the net.
Eventually, E-COM was sold (I seem to recall the Netword people bidding on
it), and it disappeared shortly thereafter. I know about this story because
Netword was a customer of another company which has also since disappeared:
Dual Systems of Berkeley, California, makers of a Motorola 68000-based,
Version 7 (and later System V) UNIX system on the S-100 (IEEE-696) bus. I
worked for Dual from March '83 to June '85 - my first job out of college.
        Erik Fair       fair at clock.org

@_date: 1996-05-07 16:27:45
@_author: Erik E. Fair (Time Keeper 
@_subject: Is the network layer geodesic? 
The principle problem is that public exchange points do not scale beyond
current LAN technology (i.e. half-duplex 100 Mb/s FDDI or Ethernet), and
how many DS3 (T3; 45Mb/s full-duplex!) pipes does it take to fill that up?
Now, drop a DEC GIGAswitch in there (16 FDDI ports, 3.2Gb/s backplane), and
now you can have sixteen peers on the exchange. Last count I saw, there are
1,800 ISPs operating in the USA alone, and *everyone* want to be at the
exchange points. Oops. How many exchange points are there? Well:
NSF Network Access Points (NAPs): New York (well, Pennsauken, NJ; Sprint),
Chicago (Ameritech), San Francisco (Pac*Bell)
MAE-EAST (D.C.), MAE-WEST (Mountain View-San Jose), MAE-LA, CIX (San Jose)
FIX-EAST (D.C.), FIX-WEST (Mountain View; just for the Feds)
SWAB (D.C., but almost no one left there).
There are probably a few new ones that are forming that I am unaware of as
yet, but the point is that they're small-fry. There are also probably
exchange points outside the USA, but I bet they're being held up with PTT
The Internet is amorphous. It ain't a star, exactly, but it still not too
far from that. However, to get away from this situation into the rich and
more fully amorphous connectivity we used to take for granted in the UUCP
network, we're going to have to see a lot more cooperation on the part of
the small ISPs in agreeing to talk *directly* to each other to exchange
traffic, and more small exchange points, instead of the small number of
large ones.
Of course, this means that you, Mr. or Ms. Discriminating Internet
Consumer, must educate yourself a little, and ask interesting questions
like, "why do my packets have to go to California to get across town to the
ISP my friend uses?" If the customers ask, the ISPs will serve. They just
gotta know what you want (and you have to be willing, of course, to pay for
Erik Fair

@_date: 1996-11-06 12:14:22
@_author: Erik E. Fair (Time Keeper 
@_subject: "censorship in cyberspace"??? 
Simple enough to solve:
1. all E-mail messages must be crypto-signed by the author's private key.
2. the list exploder verifies the key against the list membership, and only
forwards the message to the distribution list if the signature matches a
member of the list.
3. New members are added through invitation or introduction (hand wave).
There are, of course, some technology integration and ease-of-use issues
here, given that no commonly used commercial E-mail software will do

@_date: 1996-10-21 17:27:14
@_author: Erik E. Fair (Time Keeper 
@_subject: Reno at FCBA 
Perhaps C-SPAN can be convinced to cover this talk? Congress is not in
session, after all...
