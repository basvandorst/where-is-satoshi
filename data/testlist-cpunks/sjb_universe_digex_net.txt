
@_date: 1995-12-02 07:14:24
@_author: Scott Brickner 
@_subject: ecash lottery (Was: ecash casino) 
You still have the problem that the lottery agency gets to hold the
money until after the drawing.  It's reasonable to expect that they'll
eventually take advantage of their anonymity to just skip the drawing
and abscond with the bucks.  There's some incentive for them to *not*
do this on the first few drawings, but only so they can get a good
enough reputation so when they *do* skip, they'll get a lot more
The problem with this whole scheme is that there has to be some
*non-*anonymous party to enforce the contract.

@_date: 1995-12-02 09:07:27
@_author: Scott Brickner 
@_subject: Barring access to Netscape 
Just make all the URLs on the server point to a CGI script.  The script
would check the requesting browser's id and return the real data or the
"sorry" message.  This would allow the process to be done with *no*
server changes.
Since "Cypherpunks write code", one can easily imagine a hack to the
Apache or NCSD or CERN servers that did the same thing without having
the overhead of a CGI script for each access.

@_date: 1995-12-04 10:10:48
@_author: Scott Brickner 
@_subject: towards a theory of reputation 
Analytically, using an escrow agent doesn't change the utility
function.  It replaces the trading partner's honesty reputation
estimate with the escrow agent's (which is presumably higher, or why
use them?).  This is just a parameter substitution.
Whence comes the intractability?

@_date: 1995-12-04 12:26:45
@_author: Scott Brickner 
@_subject: INTERNET SECURITY RISKS FOR CONSUMERS OVERBLOWN 
Duh.  The point of the article the original poster quoted was that
there's little risk to individual *consumers*.  If someone sniffs
thirty thousand credit cards from a poorly secured web-site, the
consumers are still only liable for $50.  Of course, the card company
gets a big bill, and probably will try to sue the site to recover, and
both will pass those costs back to the consumer, assuming they
survive.  The total cost is still pretty small to the individual.

@_date: 1995-12-04 14:58:50
@_author: Scott Brickner 
@_subject: Netscape gives in to key escrow 
This is a poor argument.
I'd consider myself "anti-war", though I've done nothing more than
argue against it and behave peacefully myself.  I've even heard
"pro-war" arguments and considered some of them valid, though not
enough to change my opinion.
Netscape has pretty clearly said that they don't like the idea of GAK,
and that in fora where such things are discussed, they'll argue against
it.  They've also said that they won't let mandatory GAK put them out
of business.  That *doesn't* make them pro-GAK.
Jim Clark hasn't made any statements to the effect that *Netscape*
supports GAK (quite the contrary), but he *has* noted the government
position --- "GAK is necessary for law enforcement".

@_date: 1995-12-05 14:22:26
@_author: Scott Brickner 
@_subject: towards a theory of reputation 
I had in mind that the utility function was being used by some agent to
determine its course of action.  Imagine the agent trying to determine
which of several services to use.  It may reasonably be expected to
evaluate the utility function for each one, and choose the one with the
highest utility.  "Reputation for honesty" is one parameter to the
function.  Price, turnaround, and reputation for quality are others.  A
smarter agent could consider "metaservices" which bundle the given
service with an escrow agent.  The net effect is to permit the agent to
replace the service's honesty with the escrow agent's for the
evaluation --- regardless of the internals of the model.
The strategy for estimating h(t) should be wholly independent of the
utility model.  Otherwise you'd be effectively unable to make efficient
use of rating services, which do such evaluations as their business.

@_date: 1995-12-06 13:24:47
@_author: Scott Brickner 
@_subject: NIST GAK export meeting, sv 
Whoa.  Time-out.  Having a SECRET clearance does not imply that one is
answerable to the government.  You don't get a clearance independent of
a job.  You have to be hired for the job, then the investigators look
for anything that might disqualify you, then you get the clearance.
A key-escrow company could hire anyone they want.  Assuming that
they're approved for a SECRET billet when they're approved as an escrow
agent, the *company* designates the individual the government is to
The only leverage the government really has is the right to take the
clearance away.  The person (and probably the company) could sue for
its return if it was really done as a pressure tactic.  The guidelines
for approving or denying such clearances are pretty specific.

@_date: 1995-12-06 14:28:05
@_author: Scott Brickner 
@_subject: Solution for US/Foreign Software? 
I agree.  It does bring to mind an idea, though.  Netscape builds an
exportable system by choosing a random 128 bit number and then just
including 88 bits of it in plaintext.
This means one of two things.  Either there's a field which holds the
"key", but the export version stores 88 bits plain + 40 bits cipher,
and knows this structure, or there's a field which holds the 128 bit
enciphered key, and a second field which holds the 88 bits of plaintext
In the latter case, a patch which modifies the code which stores the
88 bit plaintext field to write all zeros would be almost trivial.
Just over-write the store instructions with noops, most likely.
In the former, the patch would be more significant, but still possible.
You'd disable the "write the plain" part and extend the "decode the
cipher" part to decode all 128 bits --- probably just a loop test.
Either patch for a given system should require less than a page of
I wonder how the ITAR would view this.

@_date: 1995-12-28 12:44:44
@_author: Scott Brickner 
@_subject: DOS - MD5 - Thanks 
I have source code to a program called "md5sum".  The comments indicate
that it works on DOS, as well as unix.  It's three source files, a
"main", a ".h", and "md5.c", which implements the md5 hash.
Want it?
P.S.  There's also source code for MD5 in the RFC1321, available
from ftp.internic.net.

@_date: 1995-12-28 13:26:59
@_author: Scott Brickner 
@_subject: Employer Probing Precedents? 
I have to agree with David.
I don't think that "property rights" are quite as clear-cut as Tim
claims.  By granting use of certain equipment to a single employee,
such as a desk, a uniform, or personal computer, the employer has
invested that employee with a vague sort of limited ownership of the
The notion that, simply because you're wearing a uniform owned by your
employer, you're subject to physical search at the employer's
discretion is laughable.  The difference between this and searching the
computer on one's desk differ only in degree, IMO.
Property rights *are* fundamental to many other human rights, but they
aren't the exclusive basis of them.  The right of self-determination
isn't based in property (except to the extent that one may be said to
inalienably own oneself, but this is really an analogy), and is equally
fundamental to human rights.
Many of the issues related to workplace privacy concerns exhibit
conflicts between these two.

@_date: 1995-12-30 06:51:13
@_author: Scott Brickner 
@_subject: Proxy/Representation? 
Why bother with the shared key?  You need a message from Helen describing
the powers with which you are invested, signed by her key.  The wonderful
thing about data is that copying it is virtually free.  When you issue an order on her behalf, include a copy of the signed PoA, and sign the whole
thing with your key.

@_date: 1995-11-04 05:43:58
@_author: Scott Brickner 
@_subject: FBI seeks huge wiretapping system 
No it isn't.  If someone builds a "cone of silence", traditional audio
surveillance becomes impossible.  Why should wiretaps be any
different?  Strong crypto is the "cone of silence" for digital

@_date: 1995-11-04 12:12:48
@_author: Scott Brickner 
@_subject: /dev/random for FreeBSD [was: Re: /dev/random for Linux] 
You need a similar "mind bending".  "Feeding in non-random data"
doesn't lead to the driver thinking it has "plenty of randomness" left,
since it doesn't increase the entropy level to counteract the decrease
from the entropy-sucker.
The hard part would be having the driver figure out how much entropy
it's getting from the input.  "Non-random" implies some sort of
correlation between the bits.  I can't think of any way of measuring
that which doesn't make some sort of "horizon" that a malicious user
can confuse.
The simple mechanism would be to assume that input from untrusted users
adds no entropy, forcing entropy estimates to represent a lower bound.

@_date: 1995-11-04 21:16:47
@_author: Scott Brickner 
@_subject: Info needed on observers 
With ecash it's possible to duplicate a coin and spend it twice ---
it's not really any different than copying a file and printing each
copy, after all.  In most ecash schemes, the double-spender can be
detected and identified when the coins are deposited, but this could be
long enough for him to disappear, leaving the bank or the payee holding
the bag.
An "observer" is designed to create a prior restraint to
double-spending.  The observer is a tamper-resistant smart card that
must be involved in all coin transfers.  When the coin is withdrawn
from the bank, the observer's signature is needed to validate it.  When
the coin is spent, the *same* observer's signature is again needed to
validate it.  The observer won't sign the same coin twice during the
spend protocol.
To double-spend with an observer present, you'd need to bypass the
tamper resistance.

@_date: 1995-11-04 21:17:46
@_author: Scott Brickner 
@_subject: censored? corrected [Steve Pizzo cited in The Spotlight] 
It isn't correct.  First, your host is immediately looking for a
namserver for c2.org, by querying it's configured default server (say,
piaget.mpd.tandem.com) for it.  If the server already has the answer
cached, it's returned immediately.  If not, a bit in the query tells it
whether the client wants it to find the answer or return an "I don't
know" answer -- most want it to find an answer.
Piaget.mpd.tandem.com probably already knows enough to bypass queries
to the tandem.com and com domains, since it's probably already resolved
at least one org query.  It can then go directly to a server for org to
get the c2.org information the client requested.
The other confused point you have is that there isn't just *one* server
for org.  There are at least a dozen interchangeable root nameservers
which handle all of com, org, edu, net, mil, gov, and the country
domains (us, uk, de, etc).
It's been a matter of policy for quite some time now that to register a
sub-domain under one of the top level domains (i.e., to register c2.org
under org) you must demonstrate two accessible nameservers for the new
domain.  I note, for example, that mpd.tandem.com has *four*
To eliminate "tandem.com" from the DNS, all of the dozen or more root
nameservers, which are in different jurisdictions, must be
compromised.  Even then, sub-domains of the top level generally offer
very long expiration periods for cached data.  It could be years before
the data left the cache from some of the second level servers, assuming
they stayed up that long.
It would almost certainly be long enough to get a judge to slap an
injunction against the action.
Once again, the net interprets censorship as damage and routes around it.

@_date: 1995-11-08 05:35:20
@_author: Scott Brickner 
@_subject: lp ? 
OTOH, the ITAR explicitly permits "temporarily imported" munitions to
be re-exported.  Those clauses should entirely eliminate the issue of
whether UUNET could be held liable under ITAR.  ITAR says otherwise --
see section 120.18.

@_date: 1995-11-08 05:42:03
@_author: Scott Brickner 
@_subject: So much for free speech...[noise?] 
As a child, we all knew the maxim, "Sticks and stones can break my
bones, but names will never hurt me."  Why have we forgotten it as

@_date: 1995-11-11 06:16:21
@_author: Scott Brickner 
@_subject: ecash speed 
A system like Stephen Brands' should significantly improve the
situation.  There's no benefit in double-spending micropayments since
you'll be identified after the fact.  For small enough payments, you
can skip the on-line validation and handle them in batch-mode later.
It becomes a matter of risk management, then.

@_date: 1995-11-15 04:55:41
@_author: Scott Brickner 
@_subject: NSA, ITAR, NCSA and plug-in hooks. 
I think it's 121.1, Category XIII paragraph (b) item (5):
"Ancillary equipment specifically designed or modified for paragraphs (b)
(1), (2), (3), (4) and (5) of this category;"

@_date: 1995-11-15 05:29:36
@_author: Scott Brickner 
@_subject: NSA, ITAR, NCSA and plug-in hooks. 
If the ban *is* due to Category XIII (b) (5), the wording would
indicate that the "hole" must be "specifically designed or modified" to
support crypto.  One that was specifically designed to support some
sort of block compression library should be exempt under that
paragraph, even if someone else were to write and distribute a crypto
library with an identical interface.
'Course, IANAL, and the interpreters of the ITAR don't really seem to
care what it *says*, anyway.

@_date: 1995-11-16 03:32:09
@_author: Scott Brickner 
@_subject: NSA, ITAR, NCSA and plug-in hooks. 
The referenced paragraphsdescribe cryptographic hardware, software and
technical data.  Computers in general are not "specifically designed"
as cryptographic equipment.

@_date: 1995-11-17 08:29:40
@_author: Scott Brickner 
@_subject: NSA, ITAR, NCSA and plug-in hooks. 
The problem is that the non-encryption program must use the same
interface as the encryption program.  Text compression is often cited
as an example of a non-encryption program that can use the same hooks
as a compression program, but there's a key difference:  the text
compressor *doesn't* need a key.
The encryption tool would have an interface like
    Boolean (*)( DataSource, DataSink, void*);
A compressor written to the same interface would never need to touch
that third argument.  Therefore, the second argument is "specifically
designed" to permit an encryption tool to be used.
You'd need a program which not only *accepted* the additional parameter,
but also *needed* the second parameter.  I confess I have some difficulty
thinking of one.

@_date: 1995-11-23 05:43:33
@_author: Scott Brickner 
@_subject: Repeated Words/characters in Password/Phrase 
I don't buy this argument.  The only reason "foofoo" could have less
entropy than "foobar" is if the attacker had some reason to know that
the user tends to choose doubled passwords, or something like that.
If the user has historically chosen passwords with roughly six bits of
entropy per character, then "foofoo" is exactly as likely as "foobar",
and is no "weaker" from an information-theoretic perspective.
In fact, information theory would generally note that discarding the
"foofoo" choice slightly reduces the entropy in the password.
It is also worth noting that any good password algorithm doesn't permit
one to determine if the password is _partly_ right, so entropy
measurements can't really meaningfully be made on a per-character
basis, only on the password as a whole.
It is because the attacker knows that many (if not most) users tend to
prefer passwords that are "easier to remember" that leads him to try
the more memorable combinations *first*.  The information-theoretic
interpretation of this is that such memorable passwords have less
entropy than the others, because the probability that the next account
an attacker tries to guess uses a memorable password is higher than the
probability that it doesn't.
"foobar" occurs as a password less frequently than "foofoo", so it has
more entropy.  The extra entropy didn't come from the use of more
characters, it came from all the more lazy users who like "foofoo"
To use a variant of Tim's example, "7f is not
measurably better than "7f even though the
latter uses the " character much more frequently than the first.
Both passwords are so far down the list that they probably have never
occurred as passwords.  Both contain effectively the same entropy.
To address the original question:
Probably not.  It may even increase security, as "the quick brown fox"
is more frequently used than "the quick quick brown fox" as someone's
password, and should, therefore, be tried first.
Again, probably not.  If the letters are generally chosen at random,
then "abafraa" is just as likely to occur as "abifryu".  If the letters
are chosen less randomly, like from a name, then "anna" is more likely
than "xavier", but less likely than "john".

@_date: 1995-11-23 05:53:42
@_author: Scott Brickner 
@_subject: MED_vac 
Actually, Bell Labs outlines a system which can preserve anonymity
under these circumstances in "The Use of Communications Networks to
Increase Personal Privacy In a Health Insurance Architecture" at
It's based on their anonymous credit card protocol, which is really a
sort of identity escrow service managed by a remailer.  You might find
it interesting.

@_date: 1995-10-05 17:52:22
@_author: Scott Brickner 
@_subject: subjective names and MITM 
A public key *is* "very probably unique".  A "randomly selected" 1024 bit
prime number has a specific amount of entropy in it.  The likelihood of
two users world wide "randomly" choosing the same such prime may be
precisely determined (assuming you can figure the entropy).
Who needs a KCA to certify it?
The real benefit of the KCA is as a means of linking the key with a unique
person.  As I've commented before, anonyms have no meaningful "credit rating".

@_date: 1995-10-06 09:03:28
@_author: Scott Brickner 
@_subject: subjective names and MITM 
Hence the suggestion to use a hash of the key instead of the key
itself.  Someone pointed out that a uniformly distributed 1024 bit
prime has something like 1014 bits of entropy.  An md5 hash of the key
should have about 128 bits of entropy, with the probability of a
collision among 2^33 keys (one per person, worldwide) being about
1 in 2^95, or about 1 in 10^29.  Sounds like we're safe, even without
straining our databases.

@_date: 1995-10-06 15:29:03
@_author: Scott Brickner 
@_subject: subjective names and MITM 
I think you understand my usage (anonym = untraceable pseudonym).  But
I still disagree with you.
I brought this subject up a couple of weeks ago (as the "inheritance
problem"), but was unable to really participate in the discussion as
things got a little busy.
A quick note on terminology:  I'll use "name" to mean a symbol which
may be easily traced to a physical entity, "pseudonym" to mean a symbol
which is traceable only under certain conditions, and "anonym" to mean
a symbol which is not traceable.
The reputation of an anonym is fundamentally different from that of a
name or pseudonym.  When a named or pseudonymous entity fails to
perform a contract, the pseudonym is exposed (becoming a name), and the
reputations of all names associated with the entity suffer.  Further,
creating a new name doesn't help, as it gets the reputation of the
other names.
There are basically two kinds of pseudonyms that I can see:  I'll call
them escrowed identities and encrypted identities.  With an escrowed
identity, an escrow agent knows who I am and part of our contract says
they're permitted to reveal the name under enumerated circumstances.
An encrypted identity is one which is revealed by the act of violating
the contract - like the double-spending protections in e-cash - and the
KCA merely certifies that the tokens were created correctly.
Escrowed identities are vulnerable to "rubber-hose cryptanalysis" and
other forms of social engineering, but there are many sorts of
transactions which don't permit encrypted identities for technical
In both circumstances, an involved KCA may attest to more than the
traceability of the identity.  The KCA may also certify that, at the
time of certification, the identity's reputation was "clean" with
respect to some standard.  Note that the KCA knows a *name* for the
entity, and thus imbues the newly created pseudonym with the reputation
of the name.  The entity still hasn't escaped a poor reputation with a
new pseudonym (if the KCA's reputation is trustworthy, that is).
In effect, names are two-way links between reputations and entities,
pseudonyms are one-way links from reputations to entities, and anonyms
are broken links between them.  Reputation credit will flow from a name
to its entity, and then flow back out to all the entity's other names.
Reputation debit will flow from a name *or pseudonym* to the entity and
then back to all the other names (but not pseudonyms).  Anonyms don't
transmit their reputation to anything.
The upper limit of credit worthiness in an anonym lies in the cost of
replacing it.  If I can create a new reputation for $1000, and you've
loaned me $1500, then I can abandon the old one at a profit of $500.
Clearly you can't extend me more credit than it will cost me to create
a new anonym.
Given the dearth of anonymity (or even pseudonymity) today, it seems
that the average entity doesn't value anonymity particularly highly.
How many people do you know that use credit cards for virtually
*everything*, simply because they value the convenience of a single
monthly statement and the security of not carrying cash more than they
value anonymity?  I know several.
This implies that the cost of creating an anonym must be fairly low if
they are to become commonplace, which further implies that the credit
worthiness of anonyms must be correspondingly low.
The next question is whether a low-cost anonym can ever expect to be
considered an "expensive" (and therefore credit worthy) anonym.  Let's
consider anonyms like Pr0duct Cypher and Black Unicorn, since you
always bring them up as examples of anonyms with reputation.
Certainly the entities behind these anonyms have a certain amount of
time and energy invested in them.  They *do* have a reputation, but
it's a reputation regarding the quality of their products.  If you were
to advance one of them $500 in consideration for writing some software,
and they took the money and didn't produce the software, how would that
hurt them?  Certainly they wouldn't be likely to get another such
contract --- it would be cash on the barrelhead from there on out, just
like any cheap anonym.  Their reputation for quality information and
freeware wouldn't change a bit.
There's clearly a large risk involved in loaning money to an anonym
with no reputation for paying back loans.  Only completed contracts
with named entities improve the anonym's reputation.  Contracts with
other anonyms or pseudonyms are unreliable indicators --- otherwise I
could create a hundred anonyms (or encrypted-identity pseudonyms) and
have ninety-nine of them report successful transactions to create an
artificial reputation.  Multiple contracts with the same named entity
are also unreliable --- I can falsely report successful transactions
with my own anonyms, too.  A small number of named entities may even
act in collusion to create an artificial reputation.
The credit reputation of the anonym is thus reliable only in proportion
to the number of named entities with which it successfully transacts.
Furthermore, a transaction with a traceable entity is implicitly
secured by the other assets of the entity, so the amount risked in a
loan is not the entire amount of the loan.  There's no reason for an
anonymous entity to hold any assets --- anything they need may simply
be held by another entity, and thus protected from seizure.  The amount
risked in a loan to an anonymous entity is the full amount of the loan,
so the credit limits for an anonym will grow *much* more slowly than
for traceable entities.
Assuming the existence of a reputable escrow agent for pseudonyms, the
cost to establish a given credit rating for a pseudonym is *much* less
than that for an anonym while the risk of undesirable disclosure is
only slightly more.  If the anonym is sought only for privacy, then
a pseudonym is a much better buy.  It's when the cost of disclosure
is very high that the anonym becomes desirable.  So what sort of entity
has so much to lose by disclosure that it's unwilling to accept the risk
involved with a pseudonym escrowed with a reputable agent? (Remembering
that we're talking about a world that's sufficiently changed as to
permit anonyms at all --- something I don't think can happen in America
today, for instance.)  Would you want to do business with them?

@_date: 1995-10-06 15:36:40
@_author: Scott Brickner 
@_subject: subjective names and MITM 
I'm not an expert here, but I understand the "well-known methods" to
essentially use some formula that "tends" to generate prime numbers from
uniformly distributed numbers, feed it a "good" random number, and then
check to see if it's really prime.  If it's not, pick another "good"
random number and try again.  The entropy in the prime is the same
as in the random number generator.

@_date: 1995-10-09 16:03:45
@_author: Scott Brickner 
@_subject: Certificate proposal 
This whole issue is a philosophical one.  The issue is the "ontology"
of electronic relationships.  The argument presented is analogous to
the "Turing test" for artificial intelligence.  The MITM is relevant
only where two commuicating parties share no channels which the MITM
doesn't control, otherwise they exchange one secret over such a channel
and Mitch is hosed (with probability 1/2^h, where h is the entropy of
the secret).
Now, if Alice communicates with an entity she knows as "Bob", which in
"reality" is Bob filtered by Mitch, I think we can readily agree that
Alice probably cannot communicate securely with Bob.  She *can*,
however, communicate in perfect secrecy with "Bob" -- the amalgamation
of Bob and Mitch.  The ontological issue comes about when we ask who it
is with whom Alice *wants* to communicate.  I'd maintain that Bob has
no ontological status with Alice.  She knows nothing of Bob, only of
"Bob".  Therefore, she must be intending to communicate with "Bob", and
her communication is secure.
An entity cannot have a meaningful ontological status until some
communication occurs.  The status which results from the communication
is "the entity, calling itself Bob, with whom I communicated over
channel X".  When a second communication occurs, we may have "the
entity, calling itself Bob, with whom I communicated over channel Y".
If the second communication contains an authenticating transaction,
then we can note that the two entities are the same.  This is what we
really mean by authentication, anyway.
As long as Mitch is successful in his MITM attack, then Bob is not an
entity with respect to Alice.
If Alice finds a key that purports to belong to Bob, about whom she
previously knows nothing, what possible relevance can it have whether
it really belongs to Bob or to "Bob" --- there is nothing in Alice's
mind to distinguish the two.
If Alice finds a key that purports to belong to Carol, about whom she
knows something, then she must execute an authentication protocol with
the new key to verify that the entity with whom it permits
communication is actually Carol, and not "Carol".
Identifying the key with the person is entirely reasonable, if the key
is what introduced the person to you (and thus ontologically created
the entity).  If the introduction happens prior to receiving the key,
then authentication becomes necessary to avoid MITM.

@_date: 1995-10-09 16:52:11
@_author: Scott Brickner 
@_subject: Certificate proposal 
I disagree.  The MITM is foiled by one successful communication.  The
reason for certificates is to isolate and limit the number of
authentication transactions which are not automated.
When you get your key certified you go through some sort of
very-hard-to- subvert process.  The exact process is irrelevant, as it
merely affects the trustworthiness of the certifier.  Let's assume for
the sake of argument that the key is certified by the same organization
(DMV/MVA/DPS/whatever) that issues drivers licences, and on the same
identification criteria.  When you have your key certified, you also
get a copy of the KCA's key.  You now have enough information to
authenticate to roughly the same level as presentation of a state
issued ID card.  After the first transaction, you can accept any
key *signed* by the KCA under the same circumstances you'd accept
the id card.  But you can get KCA signed keys from almost *anywhere*,
without the overhead associated with that level of authentication.
The expensive authentication happens once, followed by a nearly
unlimited number of cheap ones.
I think we're still "arguing past each other."
One side seems to argue "people have keys, and we need a way to
authenticate them".  The other seems to argue "there are situations
where we don't care about the people behind the keys."
Both are correct.  As I said before, authentication is the correlation
of entities with whom you've communicated over different channels.  The
notion that "people have keys" sort of implies that you know something
about the "people".  This really means you've communicated with them
out-of-band --- even if you've just heard about them, it's a few bits
of information.  When you finally communicate in-band, you need an
authentication protocol to correlate the entity on the other end of the
current channel with the entity you have in mind.

@_date: 1995-10-10 08:33:35
@_author: Scott Brickner 
@_subject: Certificate proposal 
Mike McNally writes
By "successful" I mean communicating without the MITM *interfering*.
Either the parties need to exchange a symmetric key without the MITM
eavesdropping, or exchange asymmetric keys without the MITM modifying
The chance of failure is minimized by diversity in the channels used to
try to bypass the MITM.  The issue becomes one of risk management.  If
you can't afford a failure, you *do* need a channel over which you have
nearly complete control.  The simplest such channel is a physical
meeting, during which you exchange public keys.  If the MITM threat is
from your ISP, you are likely to bypass his control with the telephone
network.  Any single success is adequate.

@_date: 1995-10-11 12:01:42
@_author: Scott Brickner 
@_subject: MITM garbage 
I agree.  The whole of the post to which you responded was directed to
the point that MITM is virtually impossible in the real world.  Since
as little as one successful communication can reveal his presence,
Mitch must cover *all* avenues his victims may use.
Once again.  That's what I said.  "Going around Mitch" is another way
of saying "using (yet) another channel", one which you haven't tried
before, meaning more diversity in the channels.
This goes back to the issue of why you care about the identity of the
key owner.  Presumably you have some knowledge of Alice which may be
verified by physical presence, or Alice carries some credentials which
are sufficiently difficult to forge.  Barring something like this,
though, you *can't* know whether there's a MITM --- but barring prior
knowledge of Alice, you don't care who's behind the key.

@_date: 1995-10-11 15:22:40
@_author: Scott Brickner 
@_subject: MITM evasion MITM evasion 
I see two general categories of MITM attacks.  In one case, Mitch wants
to eavesdrop on Alice and Bob, but doesn't really care about other
communication they do.  In the other, Mitch wants to know about all of
Alice's communications, regardless of with whom they are.
Public key cryptography turns the first case into two instances of the
second.  If Mitch doesn't control all of both Alice and Bob's
communications with everyone, the will eventually discover that the key
they're using for the other isn't the same one everyone else uses.
In the second MITM model, Mitch has an unbelievable task.  Any public
key that goes from Alice to anyone else, or vice versa, must be
substituted with one Mitch holds.  Any messages *about* public keys
must be transformed into messages about the corresponding MITM keys.
This includes telephone conversations where Alice and Bob exchange
keyids, the business card Eve has printed with her keyid and gives
to Alice at Interop, the Betsi key Alice can read in the newspaper,
WWW pages, files FTP'd, and face-to-face meetings.
Anything short of total control gives Alice an opportunity to learn
about Mitch's presence.  If Alice can exploit the hole enough to get
one good key, Mitch must change his tactics to denial of service
with respect to that key, or Alice can ask the key owner for other
good keys.
If Mitch can successfully surround Alice in such a cloud, I submit
at least one of the following statements is true:
1. Alice is such a non-entity that no one really wants to communicate
with her.
2. Bob can safely assume that the new key he just got isn't really from
Alice, because an Alice-with-a-life surrounded by a nearly successful
Mitch-cloud wouldn't be sending out keys --- she'd be sending out
messages saying "HELP ME!!  I'M LOCKED IN MITCH'S SECRET BOMB

@_date: 1995-10-11 15:43:53
@_author: Scott Brickner 
@_subject: Banque des Cypherpunks 
I don't rember if any key-splitting schemes currently allow it, but how
about this:  the escrow agencies would be the courts, requiring one
assent from each judge on the appeals chain.  As each judge rules
against the defendant or denies the appeal, he adds his piece of the
key to the ruling.  When you reach the top of the chain, then *and only
then* can you be traced.
I'm not really sure if this would apply in the ecash situation, since
you don't have a defendant until you've done the trace, but it sounds
like a legitimate safest solution in the case of GAK.  One can hardly
argue that the government has illegally revealed the keys when the
whole legal system has approved it.
NB:  I'm *strongly* opposed to GAK in principle.  I don't personally
think there's any such thing as a "legitimate need for law enforcment"
to listen in on private individuals.  A free man shouldn't have to
arrange for his life to be convenient for his servants --- private or
civil, it should be the other way around.  I'm just nothing that,
working from the common notion of "legal", this system would make
illegal key seizure unlikely.

@_date: 1995-10-13 14:44:08
@_author: Scott Brickner 
@_subject: Anguilla Cypherpunks Meeting 
The violation is in the *export*, not the sale.  You're suggestion is
that the US person developing software outside the country evades ITAR.
Mr Froomkin suggests otherwise.  His credentials seem sounder, but I,
for one, would like to hear his reasoning.
If it's true that ITAR makes such behavior illegal, then ITAR not only
inhibits freedom of speech, it inhibits freedom of *thought*.

@_date: 1995-10-17 17:07:33
@_author: Scott Brickner 
@_subject: DalSemi: Add-Only Memory for Storage of Digital Cash 
Some enterprising cypherpunks can buy a bunch and resell 'em for cash.

@_date: 1995-10-17 17:30:19
@_author: Scott Brickner 
@_subject: java flaw 
It is.  Java scans the applet to make sure it doesn't try to cheat
the interpreter into violating the object access rules.  The scanning
has nothing to do with viruses.
Java doesn't try to prevent viruses (viri?).  It doesn't even claim
such.  It *does* make claims that imply limits on what the virus can
do, though.  If the virus does no more than eat up CPU cycles, it's
fairly benign.  Java is supposed to prevent viruses that destroy files
and damage equipment.  This feature isn't restricted to viruses, though.
Even non-replicating programs aren't supposed to be able to hurt anything.
Whether they achieve this goal or not is a matter of some debate.

@_date: 1995-10-18 08:32:30
@_author: Scott Brickner 
@_subject: DalSemi: Add-Only Memory for Storage of Digital Cash 
Big deal.  NSA now knows "someone bought address X".
That's the great thing about cash you know... anonymity.

@_date: 1995-10-18 09:37:16
@_author: Scott Brickner 
@_subject: I want to learn! [NOISE] 
Anyone else thinking those AOL ads that end "Ma!  Pa done shot up the
'merica on-line agin!" seem to be aimed dead-center for their

@_date: 1995-10-18 09:52:37
@_author: Scott Brickner 
@_subject: Anonymity: A Modest Proposal 
This doesn't really help.  The only information that's different in
this approach is in fields that are removed by the remailer before it
goes to the folks who get upset.  I suppose it might improve the
traffic analysis situation somewhat, though, by making it harder
for the analyst to collect all the data.
Why bother?  It means all the remailers need to share the same key,
making it impossible to add a new remailer without verifying that it
isn't a CoS/NSA/FBI/whatever tentacle.
A vastly simpler solution would be to have all the remailers scanning
all the time, and only forwarding those messages encrypted with its
This also significantly overestimates the efficiency of news propagation.
Two remailers at distant parts of the net see news messages arrive
in different orders --- often a message received at one point won't
reach the other for up to a day.

@_date: 1995-10-19 09:29:46
@_author: Scott Brickner 
@_subject: digital cash and identity disclosure 
You're right.  Tim's wrong.  Bob can't spend the money Alice gave him
without depositing it in the bank and getting new money issued.  Each
coin has "This money was issued to Alice" as an invisible imprint which
only shows up when two coins with the same serial number are together.

@_date: 1995-10-23 16:34:41
@_author: Scott Brickner 
@_subject: A secure cryptosystem with a 40-bit key? 
I've been reading a bit recently on constructed languages like
Esperanto.  I came across one that developed out of something called
"LOGLAN" that was published in Scientific American in the early
sixties.  The current active project is called "Lojban".  It has one
really curious property that gave me an idea for an interesting
symmetric-key cryptosystem.
All "native" Lojban words are of entirely predictable forms.  "Root"
words are all five characters containing three consonants and two
vowels in one of two patters (CCVCV and CVCCV).  "Structure" words have
four forms (VV, CV, CVV, and CV'V).  "Combining forms" have two forms
(CVC and CV'C).  All other words are not "native" words (being either
proper names or borrowed words).  The upshot of this is that there is a
fixed limit on the size of the Lojban dictionary of 249500 words (given
17 consonants and five vowels).
The grammar of the language is *so* regularized that they are able to
give a YACC description for it.
A message written entirely using native Lojban words can be encrypted
in a codebook fashion where the particular codebook to be used is a
permutation of the dictionary represented by an 40-bit number (18 bits
to permut the "root word" list, 10 bits for the "structure word" list,
and 12 bits for the "combining form" list).
This system has the interesting property that *any* plaintext with the
same grammatical structure is a potential encryption of a given
cyphertext.  This is similar to some more usual cryptosystems which
operate at the lexical level but which are designed to create this
effect, but has the curious side effect that it is *very* easy to
determine a false-key which makes the transmitted message say nearly
anything you want, thus making mandatory key escrow systems
When you want to send the message "attack at dawn", you devise a
grammatically identical message, "party 'til you puke" (which is
grammatically identical in Lojban), generate a random key, as well as
the key representing a similar permutation, but with "attack" and
"party" exchanged, "puke" and "morning" exchanged, and so forth.
Transmit the message with the false key in the LEAF field (or report it
to your government-approved escrow agency) and government eavesdroppers
get the wrong message.  Other eavesdroppers get a grammatically
correct, but apparrently nonsensical message ("drink by brick").
There's still the problem of borrowed words and proper names, which
remain problems in any codebook approach, but represents a small
portion of the language, and the words which represent individual
letters are part of the "structure words" category, and could be sent
This works well in Lojban because it never changes word forms based on
grammatical usage.  Most natural language declensions and conjugations
would make the encrypted message ungrammatical, and make it *much* more
difficult to determine a false key for the LEAF field.  The irregularity
of word forms makes the dictionary much more complicated, too.

@_date: 1995-10-23 17:08:45
@_author: Scott Brickner 
@_subject: Don't Kill the Messenger--A New Slant on Remailers 
This is an interesting notion, but one I don't think is quite right.
The anonymous remailer is not merely a courier.  It actively modifies
the message envelope by removing any indication of its origin.  The
main issue in Hal's quoted complaints are that the receiver isn't able
to contact the sender.  This fact is a direct result of the action of
the remailer.
Consider what would happen if a remailer were set up that *didn't*
remove the "From:" data.  Anonymizing remailer operators could attempt
to limit complaints by forwarding everything through the non-anonymizer
to make it the last link.  Who do you think would get the complaints?
The last anonymizer.
I had a similar idea that I mentioned to Hal in a private message.  How
about a POP server that authenticates with crypto, and accepts and
holds email addressed to the keyid of a PGP key?  You send email to
4466A801 at keymail.com it holds them for 30 days (or whatever) and
discards them.  When I connect to the server to retrieve my mail, it
asks for my public key, encrypts a random challenge with it, and I tell
it the decrypted version.  Having proved that I can read messages
encrypted to the key, it delivers messages addressed to the hash of the
key.  It might also allow me to configure an address where
notifications of new messages should be sent.
It's an interesting twist on the anon.penet.fi system, since you
needn't bother tracking all the nym/email mappings, and *can't* give
CoS any incriminating information.

@_date: 1995-10-23 18:33:51
@_author: Scott Brickner 
@_subject: Reducing the Flames, Attacks, and Nit-Pickings 
Not always.  A couple of months ago someone was asking what the fuss
was about in making sure random number generators were secure.  In
describing potential problems with poor RNG seeds I "idly" speculated
that if Netscape has a lousy RNG that it might be *lots* easier to
attack that than the (then current) brute force attack was.
A week or to later, Ian posted a reverse engineered copy of the
Netscape RNG stuff, and a week or so after that announced his big
Occasionally, idle speculation sparks good ideas.

@_date: 1995-10-24 10:59:53
@_author: Scott Brickner 
@_subject: Don't Kill the Messenger--A New Slant on Remailers 
Two reasons.  One, it cuts down on traffic.  Why bother to waste the
server's bandwidth on something the client can't read anyway.  The only
possible reason someone could be asking for the data is because they're
trying to compromise the key or do traffic analysis.  Why help bad
Second, there's no reason the messages need to be encrypted.  The
server can accept messages addressed to *any* string of eight hex
digits, and doesn't care about the content.  The server needn't limit
the kinds of encryption used in the actual message.  It only cares that
the recipient is "really" (in some sense) the right reciever.
The original mental prompt for the idea came from the discussion of
the "key-is-the-person" model.  I was trying to devise a scenario where
it was possible to know of an entity only through his key, and came up
with this.  I also included the idea that messages signed by the key
would be forwarded by the server after being pseudonymized to the
keyid.  That way, the user could participate in mailing lists purely
identified by the key.

@_date: 1995-10-24 12:10:25
@_author: Scott Brickner 
@_subject: Verification of Registration 
Mine too.
C|NET Central is a cable TV show which reports on the 'net.  The
service is their on-line address where you can get more details
on stuff they report.
Doesn't seem to be especially c-punks related, but I guess someone
didn't like the idea of having to reveal his identity just to look
at a web page.  You can probably assume that the password for the
"cypherpunks" userid on the server is either "cypherpunks" or

@_date: 1995-10-25 09:50:48
@_author: Scott Brickner 
@_subject: A secure cryptosystem with a 40-bit key? 
Well, I was definitely oversimplifying things.
I'm sure that one could be done on one page, but I doubt it would have
the expressive power of a natural language *and* the lack of ambiguity
of Lojban.
And there's feedback between MEX and the non-MEX grammar since there
are cmavo which covert MEX into sumti and selbri and vice versa.
To achieve the goal of the cryptosystem it may not be necessary to
encode the cmavo, since they have no real meaning on their own, just
the gismu and rafsi.  The goal is to hide the *meaning*, not the
The selma'o that only have one member are especially meaning-free, as
they're typically elidable terminators and such.
In a natural language this might be true, but in Lojban the grammar's
regularity eliminates much of this information.  In English it's
"strange" to say "the red big dog", while "the big red dog" is fine.
Lojban doesn't have these features.  Lojban bridi are essentially
the same as function calls in a programming language, from a grammatical
perspective.  The only distinguishing feature of a selbri is the number
of sumti that it takes, and it's unusual for all of them to be specified,
and extra ones may be added using the BAI selma'o.
Irregularities make this nearly impossible for computers, though.
There are also problems due to ambiguity.
The even bigger inconvenience with natural laguages comes in defining
the codebook.  The limited forms of Lojban gismu and rafsi makes the
whole dictionary a well-defined list, permitting the codebook to be
specified as a single number that anyone could use --- even without
prior exchange of the wordlist.

@_date: 1995-10-25 10:59:24
@_author: Scott Brickner 
@_subject: Reformated: How secure.... 
and then...
Well, Perry, if it's really such a threat, isn't it worth someone's
time to combat it?
Wasn't that point driven home by the Netscape PRNG problems?

@_date: 1995-10-27 11:36:14
@_author: Scott Brickner 
@_subject: A secure cryptosystem with a 40-bit key? 
I agree.  There are some cmavo with enough meaning to warrant some encoding.
Ok, for numbers we take our original 40 bit (or whatever) key and, by
convention, run it through a md5 to produce the key used for the next
digit.  By writing all numbers with 20 digits (padding zeros on the
front or back as desired) magnitude (and/or precision) is hidden.
I'm not sure it gives anything away.  The cryptanalyst would only know
that there's a location, not anything important about the location, since
it could be padded with "null" direction and temporal operators.
And the tense selma'o could probably be treated as a single group for
encoding.  They're only semantically different, not syntactically.  You'd
end up with more confusing cyphertext, but that's no real problem unless
you're trying to hide the fact that you're encoding --- for which you
could stego the cyphertext in a rant generator.
There are enough that encoding them as a group hides their meaning.
I think you're underestimating because you're discarding the effect of
having a cypher which can arbitrarily substitute gismu and rafsi.  With
my original examples of "Attack at dawn" vs "Party 'til you puke", there's
nothing to relate the items.  A long sequence of similarly simple statements
wouldn't add anything.  Increasing the complexity of the statements makes
it more difficult to find a false key, but the regularity of Lojban should
give you enough leeway to do it.
You'd probably want to avoid really complicated bridi, but these sorts of
things tend to appear more in literary works than in ordinary communication.
I had in mind the sort of ambiguity that comes from "Time flies like an
arrow", in which any of the first three words could be the verb.  A
computer translator would have to know which to conjugate and which to
Yep, but it would have to be part of the cryptosystem's definition, as
opposed to the language's.
.o'anai mi na cusku fi la .esperantos.

@_date: 1995-10-28 04:52:06
@_author: Scott Brickner 
@_subject: New release of CFS Unix encrypting file system available 
What happens to hard links?
mkdir foo bar
CFS_set_directory_key -directory ./foo -key foo-key
CFS_set_directory_key -directory ./bar -key bar-key
cp /etc/passwd ./foo/test1
ln ./foo/footest ./bar/bartest
cmp ./foo/footest ./bar/bartest

@_date: 1995-11-01 07:25:54
@_author: Scott Brickner 
@_subject: InfoWar 
Didn't you read the post?  The whole point was that the constraints
*don't* cover many *new* technology.  Sure, your local video store
can't release the data, but your *cable* company is under no such
constraint with regard to pay-per-view.  Ditto with Hughes DirecTV.

@_date: 1995-11-01 07:37:18
@_author: Scott Brickner 
@_subject: /dev/random for FreeBSD [was: Re: /dev/random for Linux] 
As Perry pointed out in the last round on hardware noise generators, they
may not be able to predict it, but they *may* be able to generate a field
which will *influence* it.
It's difficult to know for sure if your noise source is really random,
and to what degree.

@_date: 1995-11-01 08:02:46
@_author: Scott Brickner 
@_subject: CJR returned to sender 
IANAL, but this seems implausible.  If MIT has received assurances
(written or oral) from the DoJ that indicate that their scheme is
adequate, then another organization prosecuted while following an
identical scheme can admit this as evidence.
There isn't, to my knowledge, a specific law which defines the act of
export over the 'net.  The DoJ, in effect, determines the definition by
their actions.  Failure to prosecute MIT should lead a responsible
judge to dismiss actions against a subsequent defendant that follows
the same practice.
I agree that things would be different in cases like traffic laws:  the
fact that millions of people exceed legal speed limits every day
doesn't make speeding laws invalid, but this is a matter where there is
no question whether the act broke the law.  Where the line is drawn by
the legislature, failure of the executive does not invalidate the law
--- it merely tarnishes the reputation of the executive.  Where the
line is drawn by the executive, failure to prosecute moves the line,

@_date: 1996-04-10 12:08:38
@_author: Scott Brickner 
@_subject: Enforcing the CDA improperly may pervert Internet architecture 
Wait a second.  I don't know that it's really as impossible as you
think.  Given the CDA advocates' hypothesis that anonymity is a Bad
Thing (tm), it's reasonable for them to assume that the ISP can arrange
to have a policy requiring that it know who's making the SLIP/PPP
connection.  It's not too hard to have *every* packet generated by a
given connection flagged with an IP option indicating "adult" or
"minor".  It's not that different from the "Security Classification"
option that's already in the IP spec.  Incoming connections to a
server are then already marked, leaving no excuses for servers that
deliver contraband to such connections.
The only technical problem comes when the SLIP/PPP link serves a mixed
group of users, as described in Dr Reed's paper.  In this case, I'd
think the ISP would be responsible for verifying that the person
requesting the "adult-flagged" service is really an adult, and *that*
person is responsible for what happens to the data after it's
delivered.  It'd be no different from the case where an adult goes into
an adult bookstore, buys contraband, and gives it to a minor.  The
bookstore isn't accountable.
The argument that this is technically infeasible is hooey.  This
doesn't address the issue of whether it's a Good Thing (tm), though.
Dr Reed argues that such end-to-end policies are best left out of the
network layer, but admits that adding support to the network layer may
reduce the implementation cost.  It's still expensive, though, since
all providers of indecent material and all participating ISPs have to
upgrade their software.
What possibilities does it leave for anonymity?  ISPs that don't
participate in the packet flagging might permit anonymous connections,
since it's entirely up to the information provider whether to deliver
the requested data.  Adult content providers who deliver contraband to
unflagged connections are asking for trouble.
Given your position, io.com is only accessible to adults in the world
of the CDA advocates.  Just upgrade your IP software to refuse
connections from minors.
My response to the censors' position that too much stuff on the 'net is
unsuitable for children is:  "Keep 'em off the net, then."  I'd rather
have internet access by minors generally forbidden than have

@_date: 1996-04-11 02:58:14
@_author: Scott Brickner 
@_subject: Enforcing the CDA improperly may pervert Internet architecture 
Not necessarily.  The ISP could provide a configuration mechanism for
"self ratings" which the IP software would recognize.  Mislabeling would
be punishable the same way showing nekkid pictures of your wife to your
neighbor's kid is.
Too true.  I wish they'd grow up and realize that information is inherently
harmless.  "Sticks and stones may break my bones..."

@_date: 1996-04-11 16:06:27
@_author: Scott Brickner 
@_subject: Enforcing the CDA improperly may pervert Internet architecture 
Actually, the IP layer specifies "options", but doesn't use all of
them.  I think undefined options aren't interpreted by the router,
except to observe the "copy on fragment" bit's setting.  Even if they
are, using the existing "security compartment" instead of defining a
new option could do the same thing.  Using security compartment might
permit the use of existing equipment everywhere, making the transition
to this scheme require only reconfiguration of a subset of existing
routers.  IPv4 is so stable now that adopting a new option is *very*
unlikely to break anything in existing routers.  Let's say that option
class 1 (currently unused) is used for the information.  Option number
1 means "adult", option number 2 means "not adult".  Neither option
requires parameters, so they only mean one more octet per packet (13 if
security compartment is used).  The "copy on fragment" bit is set in
Now, let's assume the worst:  the CDA is upheld through a few of these
court cases.  The IETF's raison d'etre is to facilitate usage of the
Internet, privacy isn't a goal per se.  With all the US members
scrambling to figure out how to cope with CDA, *many* of the members
might consider something like this to be a relatively easy protocol
fix.  Routers that don't accept packets directly from customers will
already work fine.  At the borders of autonomous systems, system owners
may categorize each link as "adult", "non adult", or "unspecified".
"Unspecified" means they can use an existing router, and assumes that
the other end bears responsibility for having the right "adulthood"
option.  For "adult" or "non adult", they need a router with software
modified to put the right option in all packets.  For switched
connections, like SLIP or PPP, the router needs to know who's on the
other end and put the appropriate options in the packets.
Ultimately, a relatively small number of network components need to be
changed, and almost all of them may be changed through fairly simple
software updates.  Still think the IETF would refuse?

@_date: 1996-04-12 18:22:35
@_author: Scott Brickner 
@_subject: questions about bits and bytes 
. . .
Which is precisely the reason the IETF always refers to "bytes" as
"octets".  "Octet" is defined to be eight bits, regardless of local
word sizes.

@_date: 1996-04-13 01:04:49
@_author: Scott Brickner 
@_subject: Protocols at the Point of a Gun 
Yikes!  Don't lend it the credibility of calling it "proposed".
Someone might think you're serious.  "Suggested" is as far as I'd go.
Anyway, you computer creates the IP packet, but then sends it to your
ISP's router.  That router *always* makes changes to the packet header
because it must decrement the time-to-live field and recompute the
header checksum.  The ISP's router software would (in the scenario I
suggested, but deplore), based on to whom it's connected, set the
drivers licence flag as it sees fit.  When a PPP account of a "minor"
sends a packet, the router always inserts "minor".  When the account of
an adult sends it, it inserts "adult".  When the account of a partner
who has contractually accepted liability for the flag's setting sends a
packet, it leaves it alone.

@_date: 1996-04-13 05:55:45
@_author: Scott Brickner 
@_subject: Protocols at the Point of a Gun 
As I pointed out in a private note to Perry, it's not the high-speed
routers that have to change the packets.  They typically are between
the sort of ISPs that would get "network common carrier" status, and
could rely on the options added (or not) by the other side.  It's only
when the packet crosses the border from outside the "common carrier"
net to inside that the header needs changed, and that's usually at a
terminal server, not a "very high speed router".

@_date: 1996-04-13 08:19:18
@_author: Scott Brickner 
@_subject: Protocols at the Point of a Gun 
Few of the following are really *technical* problems.
This is more of an economic problem than a technical one.  By the
"standard API" we usually mean "BSD sockets", which already has a
"getsockopt()" and "setsockopt()" interface for the application to
communicate this sort of thing.  Adding a SO_SECCLASS to change the
setting from the system default would be pretty straightforward,
Actually, this is a bit of an "ivory tower" picture of the Internet.
Conceptually, the protocols are purely peer-to-peer, but in the real
world, those end-user Linux boxes go through an ISP.  The User to ISP
link is governed by a contrac, which may specify filtering done at the
ISP.  If you want to access "adult" material, but don't want your kid
to be able to, you should get a separate filtered PPP account for the
Not so.  If one end of the connection is discarding "adult" packets,
the SYN packet attempting to establish the session will *also* be
dropped, probably resulting in a "connection refused" (from a RST by
the other side) or "destination unreachable" (from the IP module that
discarded it).
Dropped connections don't dump you out of the browser.  You just get
a popup.  (If it *does* dump you out, get a new browser.)
Again, these aren't technical issues, they're social.  A European
company who sends a dirty magazine to a sixteen year old American is
violating existing non-CDA decency laws.
True, but this isn't a problem.  The "validation" is simply a matter of
checking the "information level" in the packet with the "authorization
level" of the user.  If the ISP is filtering adult packets, the "authorization
level" is a constant per PPP connection.  If the ISP is inserting "information
levels", it's still constant per PPP connection, but now the content provider
needs to check if the request is permitted to be fulfilled.  In either case
the test is trivial.  It's not like there has to be a key exchange and
RSA exponentiation for each packet.
Agreed.  But the current discussion is about adding features to the network
Regardless of whether information is added at the network layer to
communicate the "adult/minor" information, knowingly sending web pages
with adult material to a minor is illegal.  With current implementations,
providers have the excuse that they have no way of knowing that the requester
is a minor.  If the information is added to the protocols, they lose the
excuse.  Again, this is a social matter, not a technical one.
The point about breaking protocol boundaries is an interesting one, and
as far as I am concerned, the *only* technical issue you raise.  I note
that the IP layer's "Security Compartment" option, which is one I've
suggested might be used to implement the censorship, already provides
exactly this "violation".  The "Stream ID" option and "Type of Service"
field are similar "violations".  The TCP layer gives the application
layer the "Urgent" and "Push" flags, which are arguably similar
Again, I'd like to emphasize that I think implementing this suggestion
would be censorship, and do more harm than good.  I really hope someone
can come up with a solid technical reason why doing this won't work,
but the more I think about it, the more I think it *will* work.  I
maintain that the CDA is bad socially, but that support for it at the
network layer is technically possible.

@_date: 1996-04-13 15:15:34
@_author: Scott Brickner 
@_subject: Digital Cash Escrow 
Well, the earliest RFC is dated 4/7/69.  That's not really "middle 1960's".
The term "byte" seems to date from the mid-to-late 1950's.  Try again.

@_date: 1996-04-13 21:31:28
@_author: Scott Brickner 
@_subject: Protocols at the Point of a Gun 
The way I envision it (in my nightmares), you'd have two options:  have
the account configured as "kid safe", and live in a cyberspace playground,
or have it configured as "adult", and accept responsibility for your kids'
use.  As I see it, with the censorship support at the network layer that
I outlined, the ISP can have "common carrier" status.  They sold the account
to an adult, so all packets delivered to the account are delivered to that
adult, as owner of the ISDN router.  If the adult chooses to then deliver
that packet to a child, it's no different than if the adult buys a copy
of "Debbie Does Dallas" and shows it to the kid.
Well, I don't know that the CDA supporters are thinking of.  I just
responded to the charge of "technically infeasible" with an outlined
technical solution.
I *do* think that the separation between ISP account and email address
isn't quite as black and white as you seem to think.
Actually, I expect configurations like yours to become more widespread
in the near future.  There are a lot of cable-modem designs that
basically put an ethernet port on your cable box.  There's little
practical difference (from a network topology perspective) between that
and your ISDN setup.

@_date: 1996-04-17 10:22:24
@_author: Scott Brickner 
@_subject: Protocols at the Point of a Gun 
Izzat so?  So explain to me what the difference between the PICS type
ratings and security classifications is.  If something is labelled "Top
Secret" with some compartments, it means "do not deliver this to a
principal which hasn't been authorized to receive it".  If something is
labelled "Not suitable for minors", it means "do not deliver this to a
minor".  "Age of majority" is really no different than a security
clearance to receive certain information in the CDA context.
Clearly the IETF believed that the network layer was an appropriate
place for general classification when they developed IPv4.  I haven't
verified it, but I suspect that IPv6 has (or will have) an appropriate
mechanism for indicating security classification.  The identical
mechanism may be used for packet labelling, with the broad
classification indicating the distinctions between "G", "PG", "PG-13",
"R", and "NC-17", and the compartments available for such things as
"violence", "nudity", "adult language", "sexual content",
"advertising", and so forth.
Of course, putting it at the application layer is like requiring that
every driver create his own license plate and hold it out the window
while driving.

@_date: 1996-04-17 17:50:03
@_author: Scott Brickner 
@_subject: PICS required by laws 
This is a bit naive.  The "packet bits" I've discussed are added by the
content provider (since he doesn't want to open himself to charges of
"contributing to the delinquency of a minor", which exist regardless of
the CDA) and packets with the "bits" are never delivered to the
minors.  To think that someone along that path would subvert the system
is ridiculous.  As an example, the path for packets from playboy.com to
me is entirely controlled by two entities:  MCI (Playboy's provider)
and DigEx (my provider).  This will generally be true, and though the
number of entities may be larger, the "kinds" of entities will be the
same.  Even if we're discussing a mom & pop porno shop instead of
playboy, the general picture is the same:  the content provider will
hand off the labelled data to someone with "network common carrier"
status, who will not jeopardize that status by delivering the packets
to a minor's connection.
The sorts of organizations that form the core of the internet, and are
involved in this network layer censorship scheme, just *aren't* the
sort of "subversives" (or "patriots", take your pick) that would try to
bypass the system.

@_date: 1996-04-18 09:54:59
@_author: Scott Brickner 
@_subject: Protocols at the Point of a Gun 
Nevertheless, security labels *already* exist in IPv4.
Actually, we're talking about the network level.  The transport level
is where TCP and UDP reside, not IP, which has the security labels.
I'm beginning to agree with the CDA supporter who claimed that "you're
just trying to protect your pornography by saying it's impossible when
we all know otherwise."  Of course, that person really didn't know
otherwise, but I do.  The abstract model of the Internet network layer
thinks of all transport entities as equivalent, as are all link
entities.  In the real world, such mixed user bases are unusual.  If my
scheme were implemented, service providers would probably have to
segregate shell account access onto "childproof" and "adult" machines,
or acquire a TCSEC B level system.  Either approach works, and most
would likely choose the former, since its cheaper.  It's still not
really that many machines.
I don't want the labelling to exist at all.  But I note that even PICS
labelling is not strictly voluntary.  A content provider who fails to
label adult material as "unsuitable for minors" is fully liable for
legal penalties should such material be transmitted to a minor.  The CDA
has nothing to do with it.  It's the same situation as when a bookstore
sells Playboy to a minor or a liquor store sells him beer.
As I outlined the scheme, network layer labels are just as "voluntary".
They really are in the DoD security world, too.  If you create a file in
an editor, you're responsible for making sure the right classification
goes on it, and *you're* going to be held accountable if the information
is leaked because you put the wrong label on it.

@_date: 1996-04-18 15:42:45
@_author: Scott Brickner 
@_subject: why compression doesn't perfectly even out entropy 
Then I propose the following compression algorithm to compress your
"random" one-time pad of 2 million bits with value k.  The algorithm
will decompress the input bit "1" to k, and decompress the input bit
"0" to the bit-string "10101010".  Therefore your "random" pad is
compressible to exactly one bit, and is not "random" as you supposed.
"Smaller representation" indeed.  The decompression *algorithm* must be
accounted for in the "representation" of the compressed text, otherwise
an arbitrary amount of information may be stored in the algorithm
Hamming codes offer a way to compress any bit stream.  They move
whatever patterns they can find in independent 8-bit segments into the
coding alphabet, and replace them with shorter strings.  If you don't
save the alphabet, you can't decompress the stream, and have lost
information that was originally in the stream.
If an OTP generator accidentally chooses "Hamlet", big deal.  As long
as your opponent believes that you have a good OTP generator he has no
reason to try "Hamlet" before any other pad, so Hamlet's
compressibility as english text is irrelevant.

@_date: 1996-04-24 12:37:57
@_author: Scott Brickner 
@_subject: Bernstein ruling meets the virus law 
O.W. Holmes suggested out in "The Common Law" that the law delineates a
certain minimum level of competence in forseeing the outcomes of our
actions which all members of society are expected to attain.  We'll
hold you responsible for actions a "reasonable person" should have
avoided because of their danger.  As such, persons with limited
training in manipulating biological viruses are expected to avoid doing
so.  Individuals *with* training are expected to take adequate
precautions to avoid their spread.  I see no reason why electronic
viruses shouldn't be treated similarly.  If you're going to write them,
you *better* take steps to prevent their release, or you are liable for
the damages.

@_date: 1996-04-24 14:38:07
@_author: Scott Brickner 
@_subject: arbiter/escrow agent for hire 
Wouldn't it be more reasonable for the fee to be something like 2%?  It
seems odd that to have a $2 bet settled you'd need to pay $5.  And
since the ante is required before the bet is formalized, why not just
take your cut out of the winnings?

@_date: 1996-04-24 14:41:57
@_author: Scott Brickner 
@_subject: [NOISE] Reasonable people 
I know.  I've no formal legal training, and picked up "The Common Law"
to try to get an understaning of "lawyer-think", not to learn the law.
You use what you know, though.
I'd argue that I'm holding everyone to the same standard:  either know
the safe ways of handling viruses and follow them, or don't handle them
at all.  You seem to imply that I'd hold the untrained virus writer
harmless.  No way.  He's reckless and *should* be liable.  When one has
training, it's no longer reckless to simply handle (or write) the
virus, but disregarding safe procedures is negligent.
I assume that a canonical example of the lower-standard case is the
"Good Samaritan" laws which reduce the liability of a trained person
performing rescue activities (e.g., administering CPR).
It seems to me that the "reasonable person" isn't the real issue
there.  Someone with training ought to be expected to do the "right"
thing.  If you're trained to administer CPR, and you do it *wrong*, you
shouldn't be absolved of liability -- you're negligent.  If you don't
know anything about CPR (except what you've seen on "Baywatch"), then
we're back to what a "reasonable person" should do.  If you're trained
and you do it right, but the person is still injured by your actions,
limiting your liability is society's way of encouraging you to use
your training for the common good.
In my mind, the difference between the objective standard and the
subjective one marks the difference between recklessness and
negligence.  If an objective "reasonable person" wouldn't do it, it's
reckless.  If a subjective "reasonable person" wouldn't, it's
Perhaps these aren't the "legalese" usages of the terms, but it seems
reasonable to me.
I'd be interested.

@_date: 1996-04-24 15:43:40
@_author: Scott Brickner 
@_subject: [NOISE- Legal Theory] Reasonable people 
I don't agree with this.  I expect everyone who handles viruses to know
what they're doing and take precautions.  By handling the virus at all
you are effectively claiming such expertise, as I see it.  The court
needn't consider formal training at all.  A "reasonable person" ought
to know if his training is adequate, after all.  The court may choose
to examine this claim, and find it to be in error, thus making the
handling of the virus reckless.  If the court accepts the claim, then
it should examine the actual procedures.  If the procedures are found
wanting, there is negligence (though I suspect my "non-legalese" usage
of these terms has them reversed --- negligence is a worse fault, in my
estimation: you had the knowledge but failed to act in accordance with
it; recklessness means you acted without fully appreciating the
consequences, and thus didn't know better.)
I guess "trained" may have been inappropriate.  How about "knowledgable"?
Formal training implies that one is knowledgable, but such knowledge
may be acquired without formal training (or new fields would never come
about).  Certain actions are clearly acceptable for knowledgable people
but are dangerous for those without the knowledge --- handling a
biological virus is one of them.
The court need to nothing more than determine whether the precautions
were adequate.
I'm not sure I'm imposing stricter negligence on trained CPR types, see
my comments below.  What I *am* doing is imposing a stricter
recklessness standard on untrained types.
Doing the wrong thing willfully is reckless or even malicious.
But is it actionable?  Doesn't the law have a sort of "no harm, no
foul" interpretation?  According to Holmes, if I believe that an enemy
is trying to kill me, and I arrange things so that when he thinks he's
shooting me, he's really shooting a mannekin, he has *not* committed
attempted murder.  Similarly, if a pickpocket puts his hand in my
pocket, but there's nothing there, he hasn't committed a crime.
The expert shouldn't get reduced liability for this.  The 'idiot' is
effectively a tool in the expert's hands.  Too, the 'idiot' has no
way of assuring himself that the supposed expert is, in fact, qualified.
It's no more appropriate for him to administer CPR under the guidance
of a stranger than to do it on his own judgement.
Dunno.  Is it "reasonable" for an untrained person to attempt CPR?  That's
for a court to decide.
Actually, I'd say the error in this abortion argument is that there's
a presumption of guilt, which runs counter to a basic tenet of common
In the virus case, I'd expect the plaintiff/prosecutor to prove that
the precautions were inadequate.  Not merely that they were ineffective
in the specific case, but that a "reasonable person" would have known
the activity to be dangerous without adequate precautions, and that a
"resonable expert" would have considered the precautions taken
inadequate.  Without such proof, the defendant need only indicate
what precautions were taken, and claim that they are adequate.
I see it the other way around.  The "objective" reasonable standard
says "don't handle the virus unless you're and expert".  Handling the
virus and being found incompetent to do so (the idiot case) means
you're reckless and subject to those punitive damages.  Being found
competent to handle them and found not to have taken adequate steps
leaves you at least negligent, but reckless if it wasn't accidental.
Competent with adequate precautions means you weren't even negligent.
Precautions don't necessarily eliminate danger, they simply reduce it
to acceptable levels.  Licensed drivers are, in some sense, driving
experts.  Why do they get in accidents?  Often because of liability,
but often there are merely unpredictable circumstances --- junk in the
road, sudden ice storms, etc.  The burden of proving negligence must
remain with the one claiming injury.
It's a faulty assumption, and a common law court ought to stick to its
philosophical origins --- innocent until proven guilty.

@_date: 1996-04-27 14:44:09
@_author: Scott Brickner 
@_subject: The Joy of Java 
Unfortunately, this last statement isn't really true.  To quote from the
"Java Security" paper from some Princeton researchers:
    The Java language has neighter a formal semantics nor a formal
    description of its type system.  We do not know what a Java program
    means, in any formal sense, so we cannot reason formally about Java
    and the security properties of the Java libraries written in Java.
    Java lacks a formal description of its type system, yet the security
    of Java relies on the soundness of its type system.
And later:
    The Java bytecode is where the security properties must ultimately
    be verified . . . .  Unfortunately, it is rather difficult to verify
    the bytecode. . . .  The present type verifier cannot be proven
    correct, because there is not a formal description of the type
    system.  Object-oriented type systems are a current research topic;
    it seems unwise for the system's security to rely on such a
    mechanism without a strong theoretical foundation.  It is not
    certain that an informally specified system as large and complicated
    as Java bytecode is consistent.
And in the conclusions:
    We conclude that the Java system in its current form cannot easily
    be made secure.  Significant redesign of the language, the bytecode
    format, and the runtime system appear to be necessary steps toward
    building a higher-assurance system. . . . Execution of remotely-
    loaded code is a relatively new phenomenon, and more work is
    required to make it safe.
I do think that the ideas embodied in Java are very important, and will
significantly shape the future of computing, but Java itself may be just
a stepping stone on the way.

@_date: 1996-04-27 14:55:22
@_author: Scott Brickner 
@_subject: The Joy of Java 
True.  It's still lacking a couple of (non-language) features.  The
most important (and most cpunks relevant) is a mechanism to pay people
to run programs for you.  This sort of thing is dangerous without a
safe environment.  If it was safe to do so, I can see about two hundred
PowerPC systems from where I sit that are idle 90% or more.  As more
users become permanently connected to the net (cable modems and such),
there will be *millions* of computers with a little processing power
each that are available for distributed tasks.
The next generation of "Toy Story" just might be done in near real-

@_date: 1996-04-30 15:29:41
@_author: Scott Brickner 
@_subject: The Joy of Java 
The speed can be significantly addressed by compiling the byte-code to
local machine instructions, but given the sheer number of junk cycles
that are made available by letting a Java interpreter sell them, it
doesn't much matter for some applications.
I agree that Java is currently too unsafe.  The current Java model may
not even be salvageable (that being where I got in on this thread).
It's the concept embodied by Java (and it's many conceptual cousins,
Scheme, Safe-TCL, E, etc.) that I was talking about.
I don't understand what you mean by "insufficiently powerful".  It's as
expressively powerful as most high-level languages, and computationally
Turing equivalent.  It's lack of power seems entirely in the performance
arena, which may be solved, eventually.

@_date: 1996-05-01 12:03:35
@_author: Scott Brickner 
@_subject: The Joy of Java 
It is false that Java applications "can't" save files to disk.  Java
has no I/O facilities, exactly like C and C++ have none.  Any I/O
capability must be provided in external functions.  The applet
environment doesn't include file I/O functions, but it can be easily
added in a reasonably safe way (filesystem object only allocates a fixed
region of real disk space, applets are charged to use it, after the
"rent" is gone, the blocks are freed, etc.)
Java applications may also send checkpoint data or intermediate results
back "home", even in the current environment.

@_date: 1996-02-02 16:40:38
@_author: Scott Brickner 
@_subject: noise levels 
I have an expansion on this.  Why not generalize the problem to create
a group rating system?  Anyone who wants to can send ratings messages
(rating each message on a scale of one to five, one meaning "what total
crap" and five meaning "what a useful piece of information") to the
ratings server.  The server maintains the ratings for each message by
sender.  Client software can retrieve the ratings added since a
given time and use this information with the ratings assigned by the
user to generate compatibility profiles indicating with which raters
the user tends to agree, and provide ratings on all messages based on
it.  The user can then have anything lower than his tolerance threshold
automatically deleted.
This is patterned after a newsgroup collaborative filtering tool I
read a paper on not too long ago.  I can't find that reference, but
 has an open
architecture design for a ratings server.
Ideally, one would modify MUAs to recognize an "X-Ratings-To:" header
to tell where ratings messages should be sent, and the list server
would add that to all outgoing messages.  The MUA would present
the ratings buttons when displaying messages containing "X-Ratings-To:"
headers and automatically generate and send the rating when the user
pushed a button.
The beauty of this is that it works for *any* mailing list that has
an associated ratings server.  It allows anyone with the appropriate
MUA to ignore those conspiracypunks boneheads almost transparently.
Necessary coding:
    modifications to majordomo:
    modifications to MUAs:

@_date: 1996-02-14 13:38:10
@_author: Scott Brickner 
@_subject: Secrecy of NSA Affiliation 
This sounds about right.  When I was an NSA employee ('83), our
introductory briefing included the suggestion (I don't recall it being
phrased as a command command) that we identify ourselves as DoD.  It
was suggested that this might lessen our visibility as espionage
At the time, the brass would likely have been identified as NSA, as
were the lesser brass in the National Computer Security Center, but
regular employees weren't.
Yikes.  Sometimes age creeps up on you when you aren't looking.  I
guess I'd be an "oldtimer" if I were still working there?  An
"oldtimer" at thirty-three... hmm.

@_date: 1996-01-04 07:08:14
@_author: Scott Brickner 
@_subject: Guerilla Internet Service Providers (fwd) 
Key management problems?  With someone across the street?  You gotta be
kidding.  If you can't memorize the key (say with the S/Key key-to-
phrase algorithm) and walk it across the street, write it on the back
of an envelope, walk it over, re-key, and burn it.

@_date: 1996-01-05 03:22:32
@_author: Scott Brickner 
@_subject: Will the real Anonynous please stand up 
Get a grip.  Those monarchs didn't make those names.  Others did.
You're free to make up your own relative clauses to attach to
"Anonymous" --- if they're good enough maybe others'll start using
Meanwhile, *you* need to consider what reputation statements from
anonymous sources are worth.  One needs some degree of reputation to
make a useful comment on another's.

@_date: 1996-01-05 03:52:57
@_author: Scott Brickner 
@_subject: Starting an e-cash bank 
Wait a minute.  I can see how one needn't be a bank to convert ecash
into pcash, but going the other way requires that the cash be
transferrable in ways that Digicash isn't.
If I withdraw ecash from the bank, it's marked so I'm the one who's
identified if it's double-spent.  If I give the cash to someone else
(different from paying it to them, which requires they have an account)
they're free to double-spend with (relative) impugnity.
What'd I miss?

@_date: 1996-01-15 14:05:17
@_author: Scott Brickner 
@_subject: (none) [httpd finding your identity] 
Or you can use 'ftp://anonymous:password at ftp.netscape.com/', and
skip the prompt.  Not really less secure (assuming you can prevent
shoulder surfers) as FTP sends the password in the clear, anyway.

@_date: 1996-01-16 12:55:21
@_author: Scott Brickner 
@_subject: Respect for privacy != Re: exposure=deterence? 
I agree with Charlie.  These government employees claim to be working
for the american taxpayers, of which group I am a member.  Government
agents must, therefore, expect to be accountable to the citizens, while
accountability in the other direction is virtually the definition of

@_date: 1996-06-05 13:02:34
@_author: Scott Brickner 
@_subject: Multiple Remailers at a site? 
Wait a minute.  More traffic should make analysis easier, since traffic
analysis is mostly statistical work on the source and destination (not
necessarily "from" and "to").  A bigger sample makes more reliable
For traffic analysis, I don't know *who* sent the message (it was,
after all, anonymized), but I do know a site which transmitted it and
one which received it, the time it was transmitted, and maybe its
size.  Multiply this times a whole bunch of messages, and I can infer
information about "common interests" between those sources and
The delays and mixing done by remailers make it harder by
disassociating the true sender from the true receiver.  If a remailer
were to ignore this step, the analyst can deduce from the two data
    "message a, source A, destination RemailerX, time t, size s"
    "message b, source RemailerX, destination B, time t+0.001s, size s"
that there's some connection between A and B.  The more such evidence,
the stronger the connection.  If the remailer does a good job with
the delays and shuffling, then it becomes difficult for the analyst
to match message a with message b, leaving him with what he already
knew (that A and RemailerX have a common interest, as to B and RemailerX,
but the interests may be wholly unrelated).
Multiple remailers on the same machine increases the resolution of
the address information, at best, improving the analysts ability to
make connections.  The same traffic load going to a single remailer
at the site makes the analyst's job harder.
Hmm.  Nothing really stops the machine owner from creating a personal
anonymous account to run the remailer.  When someone complains, shut it
down and create a new one.  There isn't yet a law which requires that
the owner be able to identify the user.  This affords the same
protection that multiple users does.

@_date: 1996-06-07 12:34:11
@_author: Scott Brickner 
@_subject: Multiple Remailers at a site? 
Aside from the fact that your point doesn't address mine, it doesn't
address the issue.  The "to" and "from" values that the traffic analyst
will be using are the IP addresses in the packets.  It doesn't matter
whether mixmaster, cypherpunks, or penet remailers are used, they still
use IP addresses.
Retransmission delays slightly reduce the analyst's ability to
correlate inbound and outbound messages.  Mixmaster significantly
reduces it, since all messages are the same size.  Chaining (and
mixmaster's inter-host mixing) means that the analyst needs to target
more machines to get meaningful correlations.
The discussion was about multiple remailers from multiple accounts on
the same machine.  The very existence of the remailer, independent of
issues like shuffling and chaining, is supposed to eliminate
identifying the originator by the content of the message.  Message
shuffling, delays, and chaining are entirely for the purpose of
reducing the information available to the traffic analyst.  If several
remailers are running on the same machine, they may be treated as if
there were only one remailer, for the purpose of traffic analysis.
Getting more traffic going through them just makes the analysts job
easier, because his statistical conclusions are stronger.

@_date: 1996-06-11 10:50:24
@_author: Scott Brickner 
@_subject: Multiple Remailers at a site? 
The TA isn't just looking at your messages.  All traffic through the
remailer represents data.  The S/N ratio is constant whether the
machine has a single remailer or a dozen.  The total traffic through a
machine with a dozen remailers is likely to be higher, since the total
number of remailers world-wide is so small, and users are looking for
fairly random and fairly long chains.  This means that the TA's
statistical sample is a larger fraction of the population (of total
remailer traffic), so correlations identified are stronger.
I agree totally.  The whole point is that multiple remailers on one
machine are a bad thing.  If it weren't for traffic analysis, we would
be happy even if there were only one remailer world-wide that we felt
was safe from subversion.  Adding more remailers to the same machine
doesn't improve protection from traffic analysis, and may slightly
weaken it (by attracting more traffic).
Therefore, multiple remailers on a single machine are a bad thing.

@_date: 1996-05-07 12:49:02
@_author: Scott Brickner 
@_subject: PICS required by laws 
The problem is that it requires the cooperation of both of your ISPs.
You'll never receive packets to route from either of them unless you
have some sort of contract in place.  In the scenario I outlined, the
"common carrier" status of the ISPs is contingent on their following
the censorship protocol, so their contract will require that you, too,
follow it.
The network layer isn't the geodesic Bob H likes to talk about.  That
doesn't happen until the transport layer (one higher).  It's a
heirarchical star, with a relatively small number of big ISPs acting as
the hub, several groups of regional ISPs acting as local arms, and many
local ISPs acting as the end-points.
Even in the face of a "digital silk road", this isn't likely to
change.  The cost of operating a router is proportional to the number
of connections it has.  The vast majority of traffic doesn't have
stringent enough delay requirements that it'll be willing to pay the
additional cost of going through a very highly connected router.
Therefore the hierarchical star configuration is near-optimal for
normal traffic (and pretty much all of the stuff that they claim they
want to censor).

@_date: 1996-06-01 11:39:05
@_author: Scott Brickner 
@_subject: Remailer chain length? 
Because it's not strictly true.  Implicit in traffic analysis is looking
at the "envelopes" of the traffic.  Since this means intercepting those
envelopes, once you've put your monitor on the first remailer at a site,
you've probably gotten all the rest at the site for free.
I don't think multiple remailers at the same site help anything.
