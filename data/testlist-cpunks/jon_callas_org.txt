
@_date: 2001-09-29 01:50:45
@_author: Jon Callas 
@_subject: Sudan Bank Hacked, Bin Laden Info Found 
Sudan Bank Hacked, Bin Laden Info Found - Hacker
E-Mail This Article
Printer-Friendly Version
By Ned Stafford, Newsbytes
MUNICH, GERMANY,
27 Sep 2001, 2:46 PM CST
 A group of U.K.-based hackers has cracked computers at the AlShamal
Islamic Bank in Sudan and collected data on the accounts of the Al Qaeda
terrorist organization and its leader Osama bin Laden, Kim Schmitz, a
flamboyant German hacker/businessman, has claimed.
Schmitz, who has offered a $10 million reward for the capture of bin Laden,
told Newsbytes that the information has been turned over to the FBI. Bin
Laden, a millionaire Saudi exile whose base is now Afghanistan, is
suspected of being the driving force behind the deadly Sept. 11 attacks on
the World Trade Center and the Pentagon with hijacked planes.
Newsbytes could not confirm Schmitz's claim. An FBI spokesman in Washington
declined to confirm or deny the story, saying that the agency's policy is
not to comment on information and leads it is receiving.
"We have received a lot of information on this case," he told Newsbytes.
"Of course we appreciate the leads we are receiving from the public, but we
cannot confirm what specific information has been provided to us or by
Schmitz, 27, a former teen hacking prodigy who spent time behind bars
before starting a successful data security business, has been accused of
being press hungry. He says his recent strong anti-terrorism pronouncements
are not a PR prank, but stem from his strong desire to wipe out terrorism.
He says he has received death threats from the Middle East.
The bank Schmitz claimed was hacked was mentioned Wednesday by Sen. Carl
Levin, D-Mich. during a Senate Banking Committee hearing. According to CNN,
Levin referred to a 1996 State Department report that said bin Laden had
provided the AlShamal Islamic Bank with $50 million in start-up capital.
Schmitz told Newsbytes that he could not provide details about what hackers
found in AlShamal Islamic Bank's computers or about the hackers themselves.
Nonetheless, he provided Newsbytes with the following outline of what he
says happened.
Last week, Schmitz, who lives in Munich, posted letters on his Web site
rallying politicians to the cause of fighting terrorism and offering his
hacking expertise.
"I received plenty of e-mails from hackers around the world offering their
services," he said.
Schmitz founded a group that numbers around 23 hackers called "Young
Intelligent Hackers Against Terror." He calls the group YIHAT, which is
similar to the word Jihad, which is Arabic for Holy War.
Schmitz said that last Friday, a Sudanese banker sent the group an e-mail
after reading about the $10 million reward, informing the group that Al
Qaeda and bin Laden have accounts at AlShamal Islamic Bank.
A team of U.K.-based hackers sprang into action, and hacked the nameserver
of AlShamal Islamic Bank, he said. They were able to gain access to the
bank's intranet by exploiting a "checkpoint firewall 1 vulnerability," he
After bypassing the firewall, the hackers achieved "superuser" status on
the server, and "sniffed" eight valid user IDs, and then were able to
collect information on accounts of Al Qaeda and bin Laden.
"This information was sent to the authorities in the USA," Schmitz said.
Schmitz sent the following e-mail to the Webmaster at the AlShamal Islamic
"dear webmaster of sudans shamalbank,
"your bank has been hacked. information regarding Al Qaeda's and bin Ladens
accounts have been captured. all information reached the US authorities.
thanks for using products from checkpoint (firewall1).
"have a nice day, Kim "Kimble" Schmitz, Founder of "Young Intelligent
Hackers Against Terror" YIHAT"
When asked which authorities received the information from the hackers,
Schmitz said: "FBI."
He said that authorities had not given the hackers a "green light" to
undertake the hacking, that the group had done so on its own initiative.
Schmitz said the U.K.-based hackers wish to remain anonymous.
"I am the spokesperson of the group," he said. "They don't want to be
involved with the press. What they do is illegal, unless we find a
government that legalizes our activities."
And somewhat mysteriously, he added: "If I would deliver details about the
hacks, I could get arrested. I am not hacking myself, because that is
illegal. I must keep myself out of the details to make sure that I am still
able to offer them the communication platform they need to fight as united
hackers of the world against terror."
AlShamal Islamic Bank Home Page: Kim Schmitz's Personal Web Site: Schmitz Letter to governments around the world:
Schmitz Reward For Osama Bin Laden: Reported by Newsbytes.com,  .
14:46 CST
Reposted 15:03 CST
(20010927/WIRES TOP, ONLINE, LEGAL, BUSINESS/)
ï¿½ 2001 The Washington Post Company
--- end forwarded text

@_date: 2002-08-01 20:47:05
@_author: Jon Callas 
@_subject: Challenge to David Wagner on TCPA 
Is this a tacit way to suggest that the only people who need anonymity or
pseudonymity are those with something to hide?
    Jon

@_date: 2002-06-25 20:57:09
@_author: Jon Callas 
@_subject: Ross's TCPA paper  
I think it even goes further than that.
I was giving one of my DMCA-vs-Security talks while l'affaire Sklyarov was
roiling, and noted that while that was going on, the US was being testy with
China over alleged espionage by US nationals while in China. At a high
level, each of infringement and espionage can be described as:
Alice gives Bob some information. Bob is careless with it, disclosing it to
someone that Alice would rather not see it. Alice has a non-linear response.
You can call it infringement or you can call it espionage, but at the bottom
of it, Alice believes that a private communication has been inappropriately
disclosed. She thinks her privacy has been compromised and she's stomping
angry about it.
At the risk of creating a derivative work, you say pr-eye-vacy, I say
pr-ih-vacy. Infringement, espionage, let's call the whole thing off.
    Jon

@_date: 2005-11-09 14:27:46
@_author: Jon Callas 
@_subject: gonzo cryptography; how would you improve existing 
References: But OpenPGP does. Here's an extract fro RFC 2440:
5.1. Public-Key Encrypted Session Key Packets (Tag 1)
    An implementation MAY accept or use a Key ID of zero as a "wild      or "speculative" Key ID. In this case, the receiving implementation
    would try all available private keys, checking for a valid decrypted
    session key. This format helps reduce traffic analysis of messages.
Now, there has been much discussion about how useful this is, and  there are other related issues like how you do the UI for such a  thing. But the *protocol* handles it.
You might also want to look at the PFS extensions for OpenPGP:
and even OTR, which is very cool in its own right (and is designed to  take care of the sort of edge conditions all of these other things

@_date: 2006-05-03 00:42:09
@_author: Jon Callas 
@_subject: pgp.com DOA? 
References: <20060502200600.H17898
It's nice to be thought well of, so thanks. But it's just network

@_date: 2011-09-23 18:34:39
@_author: Jon Callas 
@_subject: [cryptography] Nirvana 
Much better to counter with TPM-protected malware.
cryptography mailing list
cryptography at randombit.net

@_date: 2012-12-26 13:38:35
@_author: Jon Callas 
@_subject: [cryptography] Tigerspike claims world first with Karacell for 
Hash: SHA1
I took a look at it. Amusing. I didn't spend a lot of time on it. Probably not more than twice what it took me to write this.
It has an obvious problem with known plaintext. You can work backward from known plaintext to get a piece of their "tumbler" and since the tumbler is just a big bitstring, work from there to pull out the whole thing.
The encrypted Karacell file format has 64 bits that must decrypt to zero. Since encryption is an XOR onto a pseudo-one-time-pad, this leaks 64 bits of the tumbler. Similarly, the "checksum" at the end is a bunch of hash blocks of their special hash all XORed together. This doesn't work against malicious modificationp; you can cut-and-paste through XOR, etc.
There are obvious vulnerabilities to linear and differential cryptanalysis. It is a lot of XORing on large-ish fixed longterm secrets with only bit-rotating through the secrets, and between the vulnerabilities of known plaintext as well as the leaks in it, I don't see a lot of long-term strength. I bet that you can use known structure of plaintext (like that it's ASCII/UTF8, let alone things like known headers on XML files) to start prying bits out of the tumblers and you just work backwards. But beyond that, it isn't even particularly fast. Since it needs a lot of bit extraction and rotations, I doubt it would be as fast as AES on a processor with AES-NI instructions. The whole thing is based on doing 16-bit calculations and some bit sliding; I don't expect it to be as fast as RC4 or some of the fast estream ciphers.
Obviously, I could be missing something, but there are other errors of art that lead me to think there isn't a lot here. For example, if your basic encryption system is to take a one-time-pad and try to expand that out to more uses, zero constants are errors of art. You should know better. There are similar errors like easily deducible parameters that give more known plaintext. The author discusses using a text string directly as a key, which is very bad with his expansion system. He invented his own "message digest" functions, and they look like complete linear functions to me. They're in uncommented C that's light on indenting and whitespace. Confirmation bias might be making me miss something, but it's not like he made it easy for me.
cryptography mailing list
cryptography at randombit.net

@_date: 2012-02-24 23:50:39
@_author: Jon Callas 
@_subject: [cryptography] US Appeals Court upholds right not to decrypt a 
There is no such thing as plausible deniability in a legal context.
Plausible deniability is a term that comes from conspiracy theorists (and like many things contains a kernel of truth) to describe a political technique where everyone knows what happened but the people who did it just assert that it can't be proven, along with a wink and a nudge.
But to get to the specifics here, I've spoken to law enforcement and border control people in a country that is not the US, who told me that yeah, they know all about TrueCrypt and their assumption is that *everyone* who has TrueCrypt has a hidden volume and if they find TrueCrypt they just get straight to getting the second password. They said, "We know about that trick, and we're not stupid."
I asked them about the case where someone has TrueCrypt but doesn't have a hidden volume, what would happen to someone doesn't have one? Their response was, "Why would you do a dumb thing like that? The whole point of TrueCrypt is to have a hidden volume, and I suppose if you don't have one, you'll be sitting in a room by yourself for a long time. We're not *stupid*."
cryptography mailing list
cryptography at randombit.net

@_date: 2013-08-16 23:04:38
@_author: Jon Callas 
@_subject: [cryptography] Reply to Zooko (in Markdown) 
Hash: SHA1
Also at # Reply to Zooko
(My friend and colleague, [Zooko Wilcox-O'Hearn]( wrote an open letter to me and Phil [on his blog at LeastAuthority.com]( Despite this appearing on Silent Circle's blog, I am speaking mostly for myself, only slightly for Silent Circle, and not at all for Phil.)
Thank you for writing and your kind words. Thank you even more for being a customer. We're a startup and without customers, we'll be out of business. I think that everyone who believes in privacy should support with their pocketbook every privacy-friendly service they can afford to. It means a lot to me that you're voting with your pocketbook for my service.
Congratulations on your new release of [LeastAuthority's S4]( and [Tahoe-LAFS]( Just as you are a fan of my work, I am an admirer of your work on Tahoe-LAFS and consider it one of the best security innovations on the planet.
I understand your concerns, and share them. One of the highest priority tasks that we're working on is to get our source releases better organized so that they can effectively be built from [what we have on GitHub]( It's suboptimal now. Getting the source releases is harder than one might think. We're a startup and are pulled in many directions. We're overworked and understaffed. Even in the old days at PGP, producing effective source releases took years of effort to get down pat. It often took us four to six weeks to get the sources out even when delivering one or two releases per year.
The world of app development makes this harder. We're trying to streamline our processes so that we can get a release out about every six weeks. We're not there, either.
However, even when we have source code to be an automated part of our software releases, I'm afraid you're going to be disappointed about how verifiable they are. It's very hard, even with controlled releases, to get an exact byte-for-byte recompile of an app. Some compilers make this impossible because they randomize the branch prediction and other parts of code generation. Even when the compiler isn't making it literally impossible, without an exact copy of the exact tool chain with the same linkers, libraries, and system, the code won't be byte-for-byte the same. Worst of all, smart development shops use the *oldest* possible tool chain, not the newest one because tool sets are designed for forwards-compatibility (apps built with old tools run on the newest OS) rather than backwards-compatibility (apps built with the new tools run on older OSes). Code reliability almost requires using tool chains that are trailing-edge.
The problems run even deeper than the raw practicality. Twenty-nine years ago this month, in the August 1984 issue of "Communications of the ACM" (Vol. 27, No. 8) Ken Thompson's famous Turing Award lecture, "Reflections on Trusting Trust" was published. You can find a facsimile of the magazine article at  and a text-searchable copy on Thompson's own site, .
For those unfamiliar with the Turing Award, it is the most prestigious award a computer scientist can win, sometimes called the "Nobel Prize" of computing. The site for the award is at .
In Thompson's lecture, he describes a hack that he and Dennis Ritchie did in a version of UNIX in which they created a backdoor to UNIX login that allowed them to get access to any UNIX system. They also created a self-replicating program that would compile their backdoor into new versions of UNIX portably. Quite possibly, their hack existed in the wild until UNIX was recoded from the ground up with BSD and GCC.
In his summation, Thompson says:
    The moral is obvious. You can't trust code that you did not totally
    create yourself. (Especially code from companies that employ people
    like me.) No amount of source-level verification or scrutiny will
    protect you from using untrusted code. In demonstrating the
    possibility of this kind of attack, I picked on the C compiler. I
    could have picked on any program-handling program such as an
    assembler, a loader, or even hardware microcode. As the level of
    program gets lower, these bugs will be harder and harder to detect.
    A well installed microcode bug will be almost impossible to detect.
Thompson's words reach out across three decades of computer science, and yet they echo Descartes from three centuries prior to Thompson. In Descartes's 1641 "Meditations," he proposes the thought experiment of an "evil demon" who deceives us by simulating the universe, our senses, and perhaps even mathematics itself. In his meditation, Descartes decides that the one thing that he knows is that he, himself, exists, and the evil demon cannot deceive him about his own existence. This is where the famous saying, "*I think, therefore I am*" (*Cogito ergo sum* in Latin) comes from.
(There are useful Descartes links at:  and  and .)
When discussing thorny security problems, I often avoid security ratholes by pointing out Descartes by way of Futurama and saying, "I can't prove I'm not a head in a jar, but it's a useful assumption that I'm not." Descartes's conundrum even finds its way into modern physics. It is presently a debatable, yet legitimate theory that our entire universe is a software simulation of a universe . Martin Savage of University of Washington  has an interesting paper from last November on ArXiV .
You can find an amusing video at   in which Savage even opines that our descendants are simulating us to understand where they came from. I suppose this means we should be nice to our kids because they might have root.
Savage tries to devise an experiment to show that you're actually in a simulation, and as a mathematical logician I think he's ignoring things like math. The problem is isomorphic to writing code that can detect it's on a virtual machine. If the virtual machine isn't trying to evade, then it's certainly possible (if not probable -- after all, the simulators might want us to figure out that we're in a simulation). Unless, of course, they don't, in which case we're back not only to Descartes, but Godel's two Incompleteness Theorems and their cousin, The Halting Problem.
While I'm at it, I highly, highly recommend Scott Aaronson's new book, "Quantum Computing Since Democritus"  which I believe is so important a book that I bought the Dead Tree Edition of it. ([Jenny Lawson]( has already autographed my Kindle.)
Popping the stack back to security, the bottom line is that you're asking for something very, very hard and asking for a solution to an old philosophical problem as well as suggesting I should prove Godel wrong. I'm flattered by the confidence in my abilities, but I believe you're asking for the impossible. Or perhaps I'm programmed to think that.
This limitation doesn't apply to just *my* code. It applies to *your* code, and it applies to all of us. (Tahoe's architecture makes it amazingly resilient, but it's not immune.) It isn't just mind-blowing philosophy mixed up with Ken Thompson's Greatest Hack.
Whenever we run an app, we're trusting it. We're also trusting the operating system that it runs on, the random number generator, the entropy sources, and so on. You're trusting the CPU and its microcode. You're trusting the bootloader, be it EFI or whatever as well as [SMM]( on Intel processors -- which could have completely undetectable code running, doing things that are scarily like Descartes's evil demon. The platform-level threats are so broad that I could bore people for another paragraph or two just enumerating them.
You're perhaps trusting really daft things like [modders who slow down entropy gathering]( and [outright bugs]( Ironically, the attack vector you suggest (a hacked application) is one of the harder ways for an attacker to feed you bad code. On mobile devices, apps are digitally signed and delivered by app stores. Those app stores have a vetting process that makes *targeted* code delivery hard. Yes, someone could hack us, hack Google or Apple, or all of us, but it's very, very hard to deliver bad code to a *specific* person through this vector, and even harder if you want to do it undetectably.
In contrast, targeted malware is easy to deploy. Exploits are sold openly in exploit markets, and can be bundled up in targeted advertising. Moreover, this *has* happened, and is known to be a mechanism that's been used by the FBI, German Federal Police, the Countries Starting With the Letter 'I' (as a friend puts it), and everyone's favorite The People's Liberation Army. During Arab Spring, a now-defunct government just procured some Javascript malware and dropped it in some browsers to send them passwords on non-SSL sites.
Thus, I think that while your concern does remind me to polish up my source code deployment, if we assume an attacker like a state actor that targets people and systems, there are smarter ways for them to act.
I spend a lot of time thinking, "*If I were them, what would I do?*" If you think about what's possible, you spend too much time on low-probability events. Give yourself that thought experiment. Ask yourself what you'd do if you were the PLA, or NSA, or a country starting with an 'I.' Give yourself a budget in several orders of magnitude. A grand, ten grand, a hundred grand, a million bucks. What would you do to hack yourself? What would you do to hack your users without hacking you? That's what I think about.
Over the years, I've become a radical on usability. I believe that usability is all. It's easy to forget it now, but PGP was a triumph because you didn't have to be a cryptographer, you only had to be a techie. We progressed PGP so that you could be non-technical and get by, and then we created PGP Universal which was designed to allow complete ease of use with a trusted staff. That trusted staff was the fly in the ointment of Silent Mail and the crux of why we shut it down -- we created it because of usability concerns and killed it because of security concerns. Things that were okay ideas in May 2013 were suddenly not good ideas in August. I'm sure you've noted when using our service our belief in usability. Without usability that is similar to the non-secure equivalent, we are nothing because the users will just not be secure.
I also stress Silent Circle is a *service*, not an app. This is hard to remember and even we are not as good at it as we need to be. The service is there to provide its users with a secure analogue of the phone and texting apps they're used to. The difference is that instead of having utterly no security, they have a very high degree of it.
Moreover, our design is such to minimize the trust you need to place in us. Our network includes ourselves as a threat, which is unusual. You're one of the very few other people who do something similar. We have technology and policy that makes an attack on *us* to be unattractive to the adversary. You will soon see some improvements to the service that improve our resistance to traffic analysis.
The flip side of that, however, is that it means that the device is the most attractive attack point. We can't help but trust the OS (from RNG to sandbox), bootloader, hardware, etc.
Improvements in our transparently (like code releases) compete with tight resources for improvements in the service and apps. My decisions in deploying those resources reflect my bias that I'd rather have an A grade in the service with a B grade in code releases than an A in code releases and a B service. Yes, it makes it harder for you and others, but I have to look at myself in the mirror and my emphasis is on service quality first, reporting just after that. Over time, we'll get better. We've not yet been running for a year. Continuous improvement works.
I'm going to sum up with the subtitle of the ACM article of Ken Thompson's speech. It's not on his site, but it is on the facsimile article:
    To what extent should one trust a statement that a program is free
    of Trojan horses? Perhaps it is more important to trust the people
    who wrote the software.
Thank you very much for your trust in us, the people. Earning and deserving your trust is something we do every day.
cryptography mailing list
cryptography at randombit.net

@_date: 2013-08-30 16:12:41
@_author: Jon Callas 
@_subject: Who bought off Zimmermann? 
References: <20130825235403.BDDC4EAABC
That's precisely what we mean.
The crypto is the easy part. The hard part is the traffic analysis, of which the worst part is the Received headers. Everyone should look at their own headers -- especially people on this list and at least comprehend that your email geotracks you forever, as it's all in the Mailman archive.
There are plenty of other leaks like Message-ID, Mime-Version, X-Mailer, the actual separators in MIME part breaks, and so on. It's absolutely correct that some combination of VPNs, Tor, remailers of whatever stripe, and so on can help with this, but we're all lazy and we don't do it all the time.
What we're learning from Snowden is that they're doing traffic analysis -- analyzing movements, social graphs, and so on and so forth. The irony here is that this tells us that the crypto works. That's where I've been thinking for quite some time.
Imagine that you're a SIGINT group trying to deal with the inevitability of crypto that works being deployed everywhere. What do you do? You just be patient and start filling in scatter plots of traffic analysis.
The problem isn't the crypto, it's SMTP.

@_date: 2013-08-31 00:13:28
@_author: Jon Callas 
@_subject: Who bought off Zimmermann? 
References: <20130825235403.BDDC4EAABC
I consider delivering a zero-day to be a form of cryptanalysis. I believe that they do, too. I've been harping on that for some time.
I recognize that I have a tendency to be glib in one sentence and then rigorous in another and that's a character flaw. It's glib to say both "the crypto works" and "zero days are cryptanalysis" in many respects.
When I say, "the crypto works" I mean the basic structures. We know how to build block ciphers. We figured out hash functions a few years ago. We understand integer-based public-key cryptography well enough that it gives us the creeps. We kinda sorta understand ECC, but not as well as we think we do. I think our understanding of ECC is like our understanding of hash functions in 2003. Meow.
The protocols mostly work, except when they don't. The software is crap. It's been nearly fifteen years since Drew Gross enlightened me by saying, "I love crypto; it tells me what part of the system not to bother attacking."
Look at it anthropicly. We know the crypto works because the adversary says they're looking at metadata. To phrase that differently, they're looking at metadata because the crypto works! Look at things like Fishbowl, even. It's easy to get dazzled by the fact that Fishbowl is double encryption to miss that it's really double *implementations*.
The crypto works. The software is crap.
Think like the adversary. Put yourself in their shoes. What's cheaper, buying a 'sploit or cracking a cipher? Once you start buying 'sploits, why not build your own team to do them yourself, and cut out the middleman? Every other part of the tech world has seen disintermediation, what makes you think this is different.
On the other end of things, there's traffic analysis. We have seen -- stuff -- from them over the last decade. Papers on social graph analysis, pattern analysis. Emphasis on malware, validation, and so on. Here's another analogy. Imagine that you're looking at a huge, fantastically complex marching band. You're trying to figure out who all is doing what to what parts of the music and it's horribly complex. And then accidentally one day, you lose the audio feed and then realize that it's *easier* to tell what the band is doing when the sound is off.
Aphasiacs are (so I am told) good at telling truth from lies because they look at the face rather than listen to the voice. They analyze the metadata, because they can't hear the data and it works *better*.
Traffic analysis is what you do if your feed from the marching band loses its audio. It's what you do if you're aphasiac -- which is *exactly* what happens when the crypto works, by the way.
Thus with a large budget, you do both. With one hand, you crack the crypto by cracking the software. When it works it works. When it doesn't, it doesn't. Stop stressing. With the other hand, you revel in the glory of silence. In silence you can think. You watch the band, you watch square dance. You just watch who is pairing with whom, where the lines cross and the beats are. Sometimes you can even guess the tune by watching the dance (which is also cryptanalysis).
And all of that is why the problem in email isn't the crypto, it's SMTP.

@_date: 2013-08-31 20:15:25
@_author: Jon Callas 
@_subject: why not disable external mail, 
References: <20130825235403.BDDC4EAABC
 <34A48E2C-9760-4007-ACC1-0A8D6E0D2193
 <20130831080507.GA7052
I believe that when one is on a team, the more senior one is on the team, the more one has the responsibility to discuss the *team* decision even when one's opinion was different. Actually, *especially* when one's personal decision was different. Every decision has reasons for and reasons against. One's job as a senior team member is to talk about the way one came to the decision for, and not about the reasons against.
I just had a short conversation with Mike Janke about this issue and this discussion, and with his leave I'm going to go against my normal beliefs.
Silent Circle is Mike's vision. He did physical security in a variety of countries and saw that people who are expats from anywhere in anywhere else have a lot of issues they have to face that are all secure communications. Moreover, these people are told "no" all the time (don't use Skype, don't use Gmail, don't trust SMS, don't use cell phones, landlines) and never "yes." The initial vision of Silent Circle was to give those people a "yes." There are (were) three pillars of that vision to give people yesses -- voice/video/etc., texting etc., and email etc.
When I wrote that the email was "something of a quandary," that means that Mike was always for it and I was always against it. I see the other side of it and believe that something that's email-like is essential. We have an architecture for how we're going to grow texting into "messaging" and that will be email-like with true end-to-end security for internal mail. It is a ways off. There are lots of things to work on, from user experience to syncing across devices -- each with real security.
In the meantime, what do the users do? We did a lot of talking to end users, and what they want and need is more than just internal email. They need it to be connected to the Internet. Part of the use case includes that someone wants to send a subscriber a PDF of an insurance form, rental agreement, or so on that the subscriber needs to print out, sign, scan, and send back. A number of them said that what they really wanted as much as anything was an email system run by people who give a damn about security as much as the crypto itself. Whatever that means.
Mike was (and is) a happy customer of one of the existing secure email systems for years, understood its limitations and thought that a useful system could be made out of a conventional email infrastructure augmented by PGP Universal. I was on the other side. PGP Universal is designed for a different use case, a different threat model, blah, blah, blah. You've heard me say it, so I won't repeat it.
When I rationally looked at the facts of the situation, Silent Mail's proposed security was *different* than other secure email systems, but similar. If someone uses it "securely" then it's very good, and when they use it "conveniently" it isn't worse than any of the other convenience-minded secure email systems. Moreover, and getting to the real brass tacks here, Mike's the boss. It's his dream and his money funding it. As an interim system to have, it isn't that bad.
Additionally, one of my bugaboos about security is something I call "security arrogance." Security arrogance is when the security person tells the users what their threat model should be. It's closely related to another thing I talked about a decade ago that I called "the security cliff" -- you start with no security and to get to security, you have to climb a cliff rather than ascend a ramp in that you can't stop halfway up. I believe that one of the ways we security people shoot our clients in the foot is to focus on the ways that security is imperfect and thus argue that less-than-perfect security is worse than no security.
Okay, fine. Hoist by my own petard. Silent Mail, ho!
I'll also add that other team members were of course, spread all over the essential quandary here from thinking it was wonderful to being conflicted to thinking that Silent Mail was worse than nothing.
Development-wise, we had some plans to improve Silent Mail -- specifically, one of the tasks was to make a network widget that would scrape offending headers out of SMTP. However, note that we're a startup. Life is not a zero-sum game, but development is. Every iota of effort that's spent propping up SMTP is an iota that's not going to making its replacement. This ended up being a different sort of quandary. The people who accepted Silent Mail warts and all (or shock, horror liked it) like the idea of the new "messaging" system even better. Thus, propping up SMTP didn't really have any champions, and it's not like we have people sitting around doing nothing. We all considered Silent Mail to be a stop-gap.
Let me fast-forward up to the day before we shut Silent Mail down. One of the major requests that we had was to split the suite of products up. We were working on precisely that. (And it should go live next week.) In fact, we were *discussing* a breakup of the suite even before Silent Mail went live, and we noted that it became a legacy product after being up for about a week.
As there was more and more news about state-sponsored espionage (China, Countries Starting With The Letter 'I', etc.), we got more "business" customers and they were as a rule not interested in secure email that was not under the direct control of their own IT. Post-Snowden, the people who thought, "It's good enough" became fewer. The proportion of users who were using Silent Mail was about 5% of the total.
Every account has a page where you set up your devices, and there's a link to click to set up Silent Mail. Only people who clicked that link got set up, and the 5% number is the people who set it up, so that's obviously an upper bound of people using it.
We had been discussing shutting it down -- that 5% figure is either an argument for why it just isn't succeeding as a product, or an argument why the people who are using it understand it and its limitations. It was a discussion item for our September BoD meeting. My plan was to suggest we stop taking new orders and subscription renewals as part of the suite break-up, and then just let it fade away. I was, in fact, lobbying hard for that. I believe I would have prevailed at the board meeting, but of course I'd think that.
Your suggestion about making it be internal-only was something I'd be willing to compromise on. However, remember that much of the whole *point* of Silent Mail is that it's a well-run Internet Email System.
Now let's get to the day we shut it down. I had been at the VoIP conference, ClueCon, in Chicago. As luck would have it, I finished up early and failed to get standby on an early flight home. Others of us were scattered with other travel. One of my major thoughts was what if there's paperwork on its way, and that paperwork doesn't know I'm in an airport lounge? When I finally got Mike on the phone, he said, "You did the right thing. I'm glad you're my partner." Interestingly, the guys who work for me told me after that they had decided that they would delete things themselves if things went on for another couple hours.
I know this has been long, so let me sum up answers to your questions:
* Silent Mail was always a debate between perfect and good enough. It was even a debate over what it means to be good enough.
* The people who thought it was good enough don't think like you and me, and I think their point of view has it's own validity.
* The people who wanted it wanted it to be an Internet Email System above all. Even in the design of the new thing, it has to be connected to the Internet so that someone on the Internet can send you an email. Pulling back to being internal-only would not meet the goals of the people who wanted it.
* We're a startup. We only have so many resources, and no one was the champion of making Silent Mail better. The people who thought it was good enough didn't see the point in making it better, and the people who thought it wasn't good enough didn't see the point either.
I hope this helps explain.

@_date: 2013-12-16 15:28:56
@_author: Jon Callas 
@_subject: Gmail's receiving mostly authenticated email 
References: Zero. DKIM requires at least a 1024-bit key. Whatever you might want to say about those is a different discussion.
SPF is non-cryptographic authentication.

@_date: 2013-02-08 11:26:23
@_author: Jon Callas 
@_subject: [cryptography] "Meet the groundbreaking new encryption app set 
Hash: SHA1
Thanks for your comments, Ian. I think they're spot on.
At the time that the so-called Arab Spring was going on, I was invited to a confab where there were a bunch of activists and it's always interesting to talk to people who are on the ground. One of the things that struck me was their commentary on how we can help them.
A thing that struck me was one person who said, "Don't patronize us. We know what we're doing, we're the ones risking our lives." Actually, I lied. That person said, "don't fucking patronize us" so as to make the point stronger. One example this person gave was that they talked to people providing some social meet-up service and they wanted that service to use SSL. They got a lecture how SSL was flawed and that's why they weren't doing it. In my opinion, this was just an excuse -- they didn't want to do SSL for whatever reason (very likely just the cost and annoyance of the certs), and the imperfection was an excuse. The activists saw it as being patronizing and were very, very angry. They had people using this service, and it would be safer with SSL. Period.
This resonates with me because of a number of my own peeves. I have called this the "the security cliff" at times. The gist is that it's a long way from no security to the top -- what we'd all agree on as adequate security. The cliff is the attitude that you can't stop in the middle. If you're not going to go all the way to the top, then you might as well not bother. So people don't bother.
This effect is also the same thing as the best being the enemy of the good, and so on. We're all guilty of it. It's one of my major peeves about security, and I sometimes fall into the trap of effectively arguing against security because something isn't perfect. Every one of us has at one time said that some imperfect security is worse than nothing because it might lull people into thinking it's perfect -- or something like that. It's a great rhetorical flourish when one is arguing against some bit of snake oil or cargo-cult security. Those things really exist and we have to argue against them. However, this is precisely being patronizing to the people who really use them to protect themselves.
Note how post-Diginotar, no one is arguing any more for SSL Everywhere. Nothing helps the surveillance state more than blunting security everywhere.
cryptography mailing list
cryptography at randombit.net

@_date: 2013-02-08 23:06:55
@_author: Jon Callas 
@_subject: [cryptography] "Meet the groundbreaking new encryption app set 
Hash: SHA1
I am separating this from my previous as I went into a rant.
As we were designing Silent Text, we talked to a lot of people about what they
needed. I don't remember who told me this anecdote, but this person went over
to a colleague's office after they'd been texting to just talk. They walked
into the colleagues office and noticed their phone open with a conversation
plainly visible with someone else. A third party who was their mutual
colleague was texting about that meeting.
In short: Alice goes to Bob's office for a meeting and sees texts from Charlie
about that meeting, including comments about Alice.
There wasn't anything untoward about the texting. No insults about Alice or
anything, but there was an obvious privacy loss here. What if it *had* been
included an intemperate comment about our Alice? Alice said nothing about it
to Bob, but I got an earful. That earful included the opinion that the threat
of accidental disclosure of messages within a group of people is greater than
either the messages "being plucked out of the air" or seizure and forensic
groveling over the device. Alice's opinion was that when people have a secure
communications channel, they loosen up and say things that are more dramatic
than they would be otherwise. It's not that they're more honest, they're less
honest. They're exaggerated to the point of hyperbolic at times. Alice said
that she knew that she'd texted some things to Bob that she really wouldn't
want the person she'd said them about to see them. They were said quickly, in
frustration, and so on. It's not that they'd be taken out of context, it's
 that they'd be taken *in* context.
It's interesting underlying the story, Alice suddenly saw Bob not as an ally
in snark, but a threat -- the sort of person who leaves their phone unlocked
on their desk. Bob, of course, would say something like that if the texts had
been potentially offensive, he'd have locked his phone. This explanation would
thus convince Alice that Bob is *really* not to be trusted with snark.
This is incredibly perceptive, that the greatest security threat is not the
threat from outside, it's the threat from inside. It is exactly Douglas
Adams's point about the babelfish that by removing barriers to communication,
it created more and bloodier wars than anything else.
That's where "Burn Notice" came from. It's a safety net so that when Charlie
texts Bob, "I'm tired of Alice always..." it goes away.
What I find amusing is the reaction to it all around. There's a huge
manic-depressive, bimodal reaction. Lots of people get ahold of this and
they're like girls who've gotten ahold of makeup for the first time. ZOMG! You
mean my eyelids can be PURPLE and SPARKLY? This is the same thing that happens
when people discover font libraries or text-to-speech systems. For a couple of
days that someone gets the new app, there's nothing but text messages that are
self-destructing, purple, sparkly eyelids with font-laden Tourette's Syndrome
with the Mission Impossible theme song playing in the background. (Note, if
you are using Silent Text, you can't actually make the text purple, nor
sparkly, nor change fonts. You need to put all of that in a PDF or an animated
GIF -- and you will. This is a metaphor, not a requirements document.)
The next thing that happens is that they are so impressed with some
particularly inspired bit self-desctructing childishness that they take a
screen shot. As they gaze at the screen shot, or sometimes just as they take
the screen shot, light dawns. Oh. You mean.... Oh. Then the depressive phase
kicks in.
Back in the dark ages, PGP had the "For Your Eyes Only" feature. This is
pretty much the ancestor of Burn Notice. Simultaneously useful and worthless.
It's useful because it signals to your partner that this is not only secret
but sensitive and does something to stop accidental disclosure. It is utterly
ineffective against a hostile partner for many of the same reasons. We did all
sorts of silly things with FYEO that included an anti-TEMPEST/Van Eck font,
and other things. Silent Text actually has an FYEO feature that isn't exposed,
thank heavens.
I mention all of that because once you're in the depressive phase, it's easy
to go down the same rathole we did with FYEO. I spent time researching if you
can prevent screen shots on iOS (you can't). I did this while telling people
that it was dumb because I can take a picture of my iPhone with my iPad. I
held up my phone to video chat and said, "Here, see this? This is what you can
Sanity prevailed, but I think that fifteen years of FYEO helped a lot. When
you stare into self-destructing messages, trying to figure out how make them
really go away flawlessly, they stare back. You will end up trying to figure
out how to do a destructive two-phase commit, what class libraries need to be
patched so those that non-mutable strings inherit from mutable strings (not
the other way around), all while a nagging voice whispers in the back of your
head about how brave freedom fighters are gonna die because of this.
After the depressive phase comes the patronizing, retributive phase in which
it's clear that letting people delete potentially embarrassing messages is
bad, because it's imperfect. Imperfect security is worse than plaintext.
People have to learn self-control. Cue the Kalil Gibran quotes. People can't
just say any old thing on a secure chat program because that leads to purple
eyeshadow and thus inevitably to brave freedom fighters having their phones
seized at borders, and then people will die -- all because we let them delete
their incriminating messages. This phase makes so little sense that it's hard
for me even to mock it. But the gist of that objection really is that it's bad
to let people delete sensitive things because that will cause seizure of
sensitive things. Otherwise sane people have said this to me, and they don't
seem to see how funny they are.
Nonetheless, there's two things that happen. On the one hand, there are people
who think this cute, simple feature is the second coming of sliced bread. The
other hand is the people who insist it must be impossible (because they've
over-thought it) or evil (because security shouldn't be fun, let alone
purple). There is a small point to the dour, greyfaced side of this, I admit.
You cannot solve human problems with technology. Technology often just
shuffles around the brilliance that humans have at shooting themselves in the
foot. I'm well aware of Laotse's snarky comment that the invention of locks
created burglary, and I often agree with him.
But I think there has to be fun with security. We talk a lot about how
security has to be usable, but I think fun is up there, too. If it's fun,
people will use it. They make their mistakes cheaply, and in a reasonably safe
environment. Most of all, they'll actually use it. That's been the challenge
of the last couple decades, getting people to use it. People use things that
they play with. I think thus that play is part of security, too. What's
"groundbreaking" in what we're doing is that we're having fun and encouraging
others to do so, too.
cryptography mailing list
cryptography at randombit.net
Eugen* Leitl leitl ICBM: 48.07100, 11.36820  8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE

@_date: 2013-07-23 11:39:52
@_author: Jon Callas 
@_subject: Python Random Number Generator for OTP 
References:  <820323d3dd0dd4e28f5fc3deba096903.squirrel
 <20130723154517.GA29309
Actually, you want to whiten it before output, not before input. Whitening before input is a problem, because you can't run an estimator on the input -- because it's been whitened.
If you want to know the unbiased entropy of a source, you want the raw inputs. If you don't care about the unbiased entropy, then you don't.

@_date: 2013-07-30 10:28:46
@_author: Jon Callas 
@_subject: Python Random Number Generator for OTP 
References:  <20130722214614.GF25759 <51EE318C.9070309
 <20130723222446.98325D061
 <20130724172706.GL27178
 <20130724215618.BIAM3897.eastrmfepo102.cox.net
Some time ago, I ended up being a mentor in some coding thing. Vagueness is there to protect the guilty.
The project in question was for some program to communicate using one-time pads. That the pad in a one-time-pad must be full-entropy is why it's relevant. The question came up of how you distribute the pads, because that's the key problem (nyuck, nyuck) in doing a one-time-pad system.
The solution the person came up with was to encrypt them with PGP using a 4K-bit RSA key. I leave commentary on this system to the reader, and won't spoil the thought experiment with my own, at the moment.
This entropy broker strikes me as exactly the same sort of understanding.

@_date: 2013-10-03 14:31:12
@_author: Jon Callas 
@_subject: [cryptography] the spell is broken 
Hash: SHA1
You might call it "security theatre," but I call it (among other things) "protest." I have also called it "trust," "conscience," and other things including "emotional." I'm willing to call it "marketing" in the sense that marketing often means non-technical. I disagree with "security theatre" because in my opinion security theatre is *empty* or *mere* trust-building, but I don't fault you for being upset. I don't blame you for venting in my direction, either. I will, however, repeat that I believe this is something gentlepersons can disagree on. A decision that's right for me might not be right for you and vice-versa.
Since the AES competition, NIST has been taking a world-wide role in crypto standards leadership. Overall, it's been a good thing, but one could have one's disagreements with a number of things (and I do), but it's been a good *standards* process.
A good standard, however, is not necessarily the *best*, it's merely agreed upon. A standard that is everyone's second choice is better than a standard that is anyone's first choice. I don't think there are any problems with AES, but I think Twofish is a better choice. During the AES competition, the OpenPGP community as a whole, and I and my PGP colleagues put Twofish into OpenPGP *independently* of the then-unselected AES. It was thus our vote for it. When Phil, Alan, and I were putting ZRTP together, we put in Twofish as an option (RFC 6189, section 5.1.3). Thus in my opinion, if you know my long-standing opinions on ciphers, this shouldn't be a surprise. I think Twofish is a better algorithm than Rijndael.
ZRTP also has in it an option for using Skein's one-pass MAC instead of HMAC-SHA1. Why? Because we think it's more secure in addition to being a lot faster, which is important in an isochronous protocol. Silent Phone already has Twofish in it, and is already using Skein-MAC.
In Silent Text, we went far more to the "one true ciphersuite" philosophy. I think that Iang's writings on that are brilliant. As a cryptographer, I agree, but as an engineer, I want options. I view those options as a form of preparedness. One True Suite works until that suite is no longer true, and then you're left hanging.
To be fair, there are few options in ZRTP -- it's only AES or Twofish and SHA1-HMAC or Skein-MAC, so the selection matrix is small when compared to OpenPGP. We have One True Elliptic Curve -- P-384, and options for AES-CCM in either 128 or 256 bits and paired with SHA-256 or SHA-512 as hash and HMAC as appropriate. There's a third option, AES-256 paired with Skein/Skein-MAC, which I don't think is in the code, merely defined as a cipher suite. I can't remember. So we have to add Twofish there, but it's in Silent Phone now.
Now let me go back to my comment about standards. Standards are not about what's *best*, they're about what's *agreed*, and part of what's agreed on is that they're good enough. When one is part of a standards regime, one sublimates one's personal opinions to the collective good of the standard. That collective good of the standard is also "security theatre" in the sense that one uses it because it's the thing uses to be part of the community.
I think Twofish is better than AES. I believe that Skein is better than SHA-2. I also believe in the value of standards.
The problem one faces with the BULLRUN documents gives a decision tree. The first question is whether you think they're credible. If you don't think BULLRUN is credible, then there's an easy conclusion -- stay the course. If you think it is credible, then the next decision is whether you think that the NIST standards are flawed, either intentionally or unintentionally; in short, was BULLRUN *successful*. If you think they're flawed, it's easy; you move away from them.
The hard decision is the one that comes next -- I can state it dramatically as "Do you stand with the NSA or not?" which is an obnoxious way to put it, as there are few of us who would say, "Yes, I stand with the NSA." You can phrase less dramatically it as standing with NIST, or even less dramatically as standing with "the standard." You can even state it as whether you believe BULLRUN was successful, or lots of other ways.
Moreover, it's not all-or-nothing. Bernstein and Lange have been arguing that the NIST curves are flawed since before Snowden. Lots of people have been advocating moving to curve 25519. I want a 384-or-better curve because my One True Curve has been P-384.
If I'm going to move away from the NIST/NSA curve (which seems wise), what about everything else? Conveniently, I happen to have alternates for AES and SHA-2 in my back pocket, where they've been *alternates* in my crypto going back years. They're even in part of the software, sublimated to the goodness of the standard. The work is merely pulling them to the forefront and tying a bow around it.
And absolutely, this is an emotional response. It's protest. Intellectually, I believe that AES and SHA2 are not compromised. Emotionally, I am angry and I want to distance myself from even the suggestion that I am standing with the NSA. As Coderman and Iang put it, I want to *signal* my fury. I am so pissed off about this stuff that I don't *care* about baby and bathwater, wheat and chaff, or whatever else. I also want to signal reassurance to the people who use my system that yes, I actually give a damn about this issue.
I am fortunate enough to have a completely good cipher and completely good hash function in my back pocket. So I'm going to use them. If it turns out that there's a good explanation, that BULLRUN is wrong, it's just software. Your situation is different, as is everyone else's. I admire your cool head, but I have to stand over there. I apologize for angering you, but I'm not sorry.
If I'm wrong, I'll have to eat my words. I would rather eat my words in this direction -- moving away -- than the other direction -- standing pat.
cryptography mailing list
cryptography at randombit.net

@_date: 2013-10-17 14:39:01
@_author: Jon Callas 
@_subject: Curious RNG stalemate [was: use of cpunks] 
References:  <20131017204727.B32E8EB07
 <20131017221211.3b81105d
Be aware in all of this of the Heisenberg-Schödinger Credulity Effect. That effect is that the word "quantum" sucks people's brains out, and otherwise sensible people suffer from impaired reasoning.
It is certainly true that radioactivity is a random effect, and is quantum in nature. That does not mean that in order for a random sampling to be quantum, it must be based on radioactivity; there are other quantum sources of randomness. Noisy diodes, resister noise, CCD noise, etc. are all quantum. If you want to get picky, *all* physical effects are quantum, even ones that aren't usefully random. There is nothing magic about one physical source or other that makes it more suited for crypto. Thinking that a hardware source should be radioactive is affirming the consequence, as well.
Not does it mean that a radioactive (or other) source is suitable for cryptography without some sort of conditioning. Hardware sources are often biased in distribution, or have other numeric flaws that can be fixed with a whitening function.
In short, radioactivity is neither necessary nor sufficient for cryptographic use. If you want to use a source for crypto, you want to run it through a system like /dev/random or at the very least a DRBG to give clean outputs.
Furthermore, what we really want in crypto is what I call "unguessability." This is both weaker than true randomness and stronger. It's stronger in that the numbers have to remain secret. A completely random process that everyone knows is completely unsuitable for crypto, but a weakly entropic input can be jiggered into suitability.
To sum up -- don't get wrapped around the axle about radioactivity. It's not the only random process in the universe, and you have to do a lot of work once you have it. The sort of work that you need to do is precisely what a well-done OSRNG does.

@_date: 2013-10-18 10:11:15
@_author: Jon Callas 
@_subject: Curious RNG stalemate [was: use of cpunks] 
References:  <20131017204727.B32E8EB07
 <20131017221211.3b81105d
 <3b006fcb-dffd-4fae-98b3-d8461d4e8f71
Because people think that over-the-top is necessary.
Perhaps more to the point, people start gilding the lily, and then worrying about how pure the gold is on the lily, and then deciding that the gilt on the lily needs to be mono-atomic and to form a single crystal.
Even more to the point, they start thinking in their heads that they will be criticized for not having a single-crystal structure on the gilt on their lily, and give up. After that, they criticize other people who grow lilies because -- heck, anyone can do that, and years ago, they gave up on lilies because of how hard it is to get mono-crystalline gilt. Go look it up in the cypherpunks archives, for pete's sake. Nicholas Bourbaki discussed it to death there back in '92.
Building a good RNG is both simpler than you think and harder. You need:
* An unguessability source. It doesn't have to be as good as you think it does. If it's crap, you just need more. It just has to be unguessable. The deterministic process going on on my LAN might be good enough. It might not. What matters is the work factor of guessing.
Here's an example of a source I have seen that is plenty good enough, but not what most people think:
- Take an array of unsigned ints; 16 or 32 in length is fine.
- On every interrupt, read the "cpu clock." Lots of CPUs have something good enough. Tick counters, high-speed uptime, etc. It almost doesn't matter.
- XOR that into the current array element. - Rotate that element by an odd number of bits. Mathematically, all that's required is that you pick an odd number that's relatively prime with the word size so you get maximum mixing over time intra-element. Most people can pick a suitable odd number.
- Increment/wrap your element pointer.
Poof, you're done. Incidentally, this is *also* a quantum process, it's just quantized fatter than other quantum processes. That's why you want to do it on every interrupt. When I first saw this, my jaw dropped at its elegance.
* A pool and distiller. Hash functions are great here, as are other things. The thing to remember is only that you might get a megabyte of input that has only a single bit of unguessability in it, and you need to cope. That's why hash functions are great. Entropy estimation is highly desirable, but not necessary. This is a long discussion. It's possible to build one with no estimator that works well, and one with an estimator that works poorly.
* An output function. Ciphers and hash functions are your friends. The SP 800-90 DRBGs are all designed by committee, but work (with the obvious exception). Far more important is to have your output function stir back into the pool. Something as simple as feeding back the length of the request is fine. Even better is to feed in something like a cpu tick counter. If you do something mildly reasonable here in stir-back, then you can completely forget about entropy depletion. This is another long discussion, and that's why I'm simply asserting it. There are gentlepersons who disagree with me. That's it. Yeah, the devil's in the details, and my sketch is kinda like saying oh, all you have to do to land on the moon is get some rockets and life support. But this is a lot easier than landing on the moon, despite many people thinking it's harder.
And yes, yes, there are other other considerations like rebooting, suspend, hibernate, restoring VMs, and of course initial boot.

@_date: 2013-09-06 17:58:33
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
I'd be happy to give a different answer, like -- almost certainly not.
We know as a mathematical theorem that a block cipher with a back door *is* a public-key system. It is a very, very, very valuable thing, and suggests other mathematical secrets about hitherto unknown ways to make fast, secure public key systems. To me, it's like getting a cheap supply of gold and then deciding you'll make bullets out of it instead of lead. To riff on that analogy, it feels like you're suggesting that they would shoot themselves in the foot because they know that the bullet fragments will hurt their opponent.
That's why I say almost certainly not. It suggests irrationality beyond my personal ken. It's something I classify colloquially as "too stupid to live."
My assumptions about the NSA are that they're smart, clever, and practical. Conjectures about their behavior that deviate from any of those axes ring false to the degree that they deviate from that.
My conjectures start with assuming they're at least as smart as me, and I start with "what would I do if I were them?" I think they're smart enough not to attack the strong points of the system, but the weak points. I think they're smart enough to prefer operating in stealth.
Yeah, yeah, sure, if with those resources I stumbled into a fundamental mathematical advantage, I'd use it. But I would use it to maximize my gain, not to be gratuitously sneaky.
The math we know about block ciphers suggests (not proves, suggests) that a back door in a cipher is impractical, because it would imply the holy grail of public key systems -- fast, secure, public key crypto. It suggests secure trapdoor functions that can be made out of very simple components.
If I found one, it would be great, but I'd devote my resources to places where I technology is on my side. Those include network security and software security, along with traffic analysis.
If I wanted to devote research resources, I'd be looking closely at language-theoretic security. I'd be paying close attention to the fantastic things that have come out of there.
The stuff that Bangert, Bratus, Shapiro, and Smith did on turning an MMU into a Turing machine is where I'd devote research, as well as their related work on "weird machines."
I apologize for repeating myself, but I'd fight the next war, not the last one.
The cryptography mailing list
cryptography at metzdowd.com

@_date: 2013-09-11 08:17:28
@_author: Jon Callas 
@_subject: hardware RNG 
References: <5.2.1.1.1.20130909162233.01aede10
Yes. If you took noise off of a diode or even a resister and just threw it into Yarrow, you'd have a very nice thing.
The biggest problem with building good random number generators is that it's harder than you think on first glance and easier than you think on third glance.

@_date: 2013-09-11 08:19:02
@_author: Jon Callas 
@_subject: [guardian-dev] pgp, nsa, rsa 
References: <20130911083842.GK10405
LTC is my preferred place to start with a crypto library. It's just brilliant in design.

@_date: 2014-01-31 13:13:10
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
References:  <2A1FFA4B-9768-48A7-8838-4AB6729CB8B5
 <7DFE3277-2ED4-492F-B5DF-F00F01A32B13
We *are* using some of the Guardian Project's software. Also software that we're building for Blackphone will be available for other people to use on their own ROMs. And heck, you can go to Github, get the Silent Circle apps and put them on your own device. We're finally to the point that we've QA'ed people who aren't us building them and using them. (And if you can't, it's a bug.)
Let me answer your question with a question.
What's the difference between going to a restaurant as opposed to going to the grocery store and buying a bunch of ingredients and making the same meal? There are groups devoted to making food the way the Child or Keller might. You can't have a meal by Child because she's gone, but you could make a Keller meal as well as Keller's people can. Why go to the restaurant?
Now to comment on that line of both our questions, we all have a set time in this existence and some people might like to write their own compilers so they can write their own software, just as some people grow their own food so they can make their own meals. But some people don't want to do that, and every single one of us trades off the things we want to do against things we're happy to pay other people to do. No offense taken. I may be a smartass, but I like tough questions. If/when they do, I'd love to see it. I don't have time to make an open, secure baseband, but want to include one. The world needs one. Maybe we can arrange some sort of trade.

@_date: 2015-02-20 14:36:55
@_author: Jon Callas 
@_subject: [Cryptography] trojans in the firmware 
References:  <54E2B04C.9080707
 <54E436FB.9000709
NAND memory runs faster when the hamming weight of the data is approximately even between zeroes and ones. You can speed up NAND flash by running the data through a suitable whitening function.
AES is a great whitening function. If you then go to the extra effort to do key management, you have security. It's a simple matter of architecture and programming. :)

@_date: 2015-05-27 22:01:13
@_author: Jon Callas 
@_subject: Apple At-Rest Encryption 
References: <556381ED.4050900
You should turn it on. The battery effect on the CPU is negligible; it’s using AES-NI in the processor and that’s running at less than one clock per byte. But if you’re on a computer that has flash – like any of the Air/Retina machines – the write time and power requirements of NAND flash are much better when you use a whitening function, of which AES makes a great one.
But in any event, it’s all going to be not worth worrying about in the costs. You might even benefit. You are also gaining in the security end. We can certainly debate whatever the operational security benefits are from encrypting your disk, but the real benefit comes from when you inevitably decommission that machine and storage. You are vastly, vastly better off with encrypted storage then, and better off for having encrypted it all along.
