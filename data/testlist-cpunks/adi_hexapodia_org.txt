
@_date: 2013-08-11 08:55:42
@_author: Andy Isaacson 
@_subject: Lavabit and End-point Security 
References: <20130809144310.GA29800
 <20130811143955.GA17288
Security doesn't work that way.  Keeping your system secure is like
walking a tightrope across a gorge filled with ravenous tigers every
morning.  There are a billion ways to fuck up and get owned/eaten by the
tigers, and asking someone who's successfully walked the tightrope every
day for 40 years "tell me your secret?" completely misses the point.
The expert can share advice and point out when you're about to step off
the tightrope, but no kind of advice can substitute for your own caution
and experience.  Pretending that a magic balance bar, or a magic
technique that can be applied without careful thought, or a magic shoe
that will make you stick to the rope, will save you is the kind of thing
that works in a fairy tale but not in real life.
The analogy breaks down, though, because in fact you can get totally
owned, through and through; exfiltrated, impersonated, and strung up
by a prosecutor before a secret grand jury before you even learn that
your security has failed.  At least the tiger has the courtesy of giving
you pain when you fail.

@_date: 2013-08-11 11:50:45
@_author: Andy Isaacson 
@_subject: Moonlet 
References: <20130810181310.GA9105 <20130811113029.GB14031
Are you referring to the google group
or is there another, less cloudy, mailing list?

@_date: 2013-12-09 18:55:21
@_author: Andy Isaacson 
@_subject: Open phones for privacy/anonymity applications, Guardian 
References: <78A88087-DAFD-4FE9-84C7-E17B2E1D82FF
 <20131209233218.816EE2280C6
I believe that there's no law or regulation preventing anyone from
writing open source implementations of GSM in the US.
However there definitely are regulations preventing the sale or
operation of unlicensed intentional emitters, and the FCC definitely
cares about the GSM bands.  Getting a license for an open source
implementation of GSM would likely be a large expense, which AFAIK no
open source implementor has even started to try to undertake.
There is also federal law prohibiting the sale of equipment which can
intercept wireless telephony communications.  Many scanners have
filters or programming/configuration jumpers which prevent reception of
the specific frequencies covered by the law.  Depending on the reading
and the zealousness of the prosecutor, such a law might be seen to be
relevant to an open source GSM platform.

@_date: 2013-12-11 21:15:38
@_author: Andy Isaacson 
@_subject: [cryptography] Which encryption chips are compromised? 
References: <527F865A.7020703
 <20131211001137.892EA228094
I believe it's just a guess based on fit.
Suppose I'm the manager writing this document, reporting the expected
accomplishments of my group.  We do cryptanalysis.
If we're projecting success against FooBarCo chips' encryption sub-core,
and everybody knows FooBarCo chips are used in both encryption and
non-encryption products, it makes sense to cite the specific
applications where FooBarCo chips are used.  So "for FooBarCo chips used
in VPN and SSL" makes sense, even if FooBarCo chips are not *solely* VPN
and SSL.
However, in "for FooBarCo encryption chips used in VPN", the
"encryption" seems to me to denote a special purpose chip, rather than a
general purpose chip with an encryption sub-core.  I've seen worse
manglings of language in similar documents, though, so I would not put
it past said middle manager to write "for Intel Ivy Bridge encryption
chips used in VPN and SSL", even though that's a bit of word salad to
anyone who knows the technology.
"Cavium Networks" or "Cavium Nitrox" are approximately the right length
to fit.  Other vendors that might be interesting include F5, Barracuda,
Riverbed, Cisco SCA 11000, Radware (an Israeli/American company), and
everybody listed on The document looks like Word and appears to be fully justified; anyone
with that software want to match the fonts and try out various
substitutions to see what fits best?
Note that
seems to have been digitally processed and redacted; the font baselines
are perfectly aligned, to the sub-pixel antialiasing limit; while
appears to have gone out to paper and then been scanned in on a
non-flatbed scanner; there is significant vertical slew across the line
of text in question.  Since the source document appears to be the same
for both, an enterprising DTP jockey could use -clean-1.pdf to tune the
document settings precisely, and then use -project.pdf to search for
better unredaction matches.

@_date: 2013-12-12 13:24:37
@_author: Andy Isaacson 
@_subject: [cryptography] Which encryption chips are compromised? 
References: <527F865A.7020703
 <20131211001137.892EA228094
 <20131212051537.GA20782
In reply to Declan tweeting about this discussion (shame on you, Declan,
if you're reading this and trying to take the discussion to the public),
Kevin Poulsen points out
that the Times' comment on this redaction appears to imply that the
redacted text names two chips:
    Large Internet companies use dedicated hardware to scramble traffic
    before it is sent. In 2013, the agency planned to be able to decode
    traffic that was encoded by one of these two encryption chips,
    either by working with the manufacturers of the chips to insert back
    doors or by exploiting a security flaw in the chips' design.
VPN gear lasts in the field for 2-5 years post roll-out.  Design wins
into large provider's hardware will often see the same chip being
manufactured for 2-5 years after it ceases being available at retail.
(ark.intel.com has an "embedded option available?" field to denote the
chips they support this for.)
"Complete Enablement" is jargon with a specific meaning.  I'm not
certain I understand it, but I *think* it means "we have plaintext
access on any targeted session".  I don't think it means "we can get
plaintext for an arbitrary previously recorded session" and I don't
think it means "we automatically get plaintext for every session we can
hear".  Suppose a NSA chip backdoor receives its triggering command by a
specific sequence of TCP retransmits (dropped packets) and after being
triggered, leaks the key by varying the timing or ordering of outbound
packets.  By my reading, this would count as "complete enablement" even
though a session which was not triggered would not be eavesdroppable.
To specifically respond to your point, "Complete enablement" is also
time dependent.  Productionizing a timing side channel attack could
result in complete enablement only for new flows and would still be
complete even though there was no enablement before the attack was
Especially taking the NYT commentary into account, I'm even more
convinced you're right.  "Intel and AMD" is about the right length...

@_date: 2013-12-16 11:54:55
@_author: Andy Isaacson 
@_subject: Bruce Schneier to leave BT 
References: Unlikely; not impossible, but I doubt it was actual retaliation.  Bruce
denies it:
    Schneier told The Register this evening of his departure: "This has
    nothing to do with the NSA. No, they [BT] weren't happy with me, but
    they knew that I am an independent thinker and they didn't try to
    muzzle me in any way. It's just time. I spent seven years at BT, and
    seven years at Counterpane Internet Security, Inc before BT bought
    us. It's past time for something new.
    As to the future: Answer, cloudy; ask again later." Circa 2009 Bruce had some amusing off-the-record stories of previous
interactions with the BT hierarchy.  Bruce at BT was never a clean fit,
but BT profited greatly by, as they say in the article's leaked email,
having someone who can challenge their expectations.
Staying at a single gig for 14 years is quite remarkable in the tech
All that said, there are still things that one Cannot Say when one has
$CORP on one's business card.  Just as a simple example, the doctrine
of "judicial notice" means that if an officer of a corporation makes a
public statement about something and the corporation does not publish a
correction, a court can assume that the statement is true in a later
civil proceeding.  The company can counter the presumption or clarify
the necessary context by presenting evidence, but it's expensive and
means you're burning minutes/pages on refuting nonsense.  So execs need
to be careful not to say, for example, that "$CORP has poor network
security" because that's food for a nuisance lawsuit.
I hope Bruce can use his newfound free time to do additional reporting
on the docs.  Good news!

@_date: 2013-12-16 19:39:30
@_author: Andy Isaacson 
@_subject: Aqua - a high bandwidth anonymity system that resists traffic 
References:  <1510892.DPo3hhB2I8
MS Research is trying fairly hard to be a Real Research Lab in the vein
of DEC SRC, Xerox PARC, IBM Almaden.  They released a NetBSD kernel for
their experimental CPU a few years back.
Strange days we live in.

@_date: 2013-02-07 11:15:16
@_author: Andy Isaacson 
@_subject: [liberationtech] Cryptography super-group creates unbreakable 
Silent Circle has some significant credibility gaps.  They repeatedly
claimed, and AFAIK continue to claim, to be "open source", but the
source isn't even available for inspection under restrictive license,
much less actually open source per OSI or DFSG or common sense
guidelines.  They haven't justified or explained this gap in any of
their statements I've seen.
They're trading very heavily on the excellent pedigrees of their
principals, while making outlandish and unsupported claims to credulous
mainstream journalists.
Your participation in their marketing interviews makes you complicit in
this problematic enterprise, Chris.
I think it's incredibly unfair of you to attack Nadim for pointing out
the flaws in this system without addressing your role in those flaws.
We all want privacy and security for users.  Silent Circle's
misappropriation of the "open source" label and hagiographic mainstream
press treatment in advance of actual public review, abetted by a wide
variety of experts and public voices, is deeply problematic for the
liberation technology community's role in civil society.
Silent Circle may be an excellent privacy app.  It might not have any
significant security problems.  It might even do a good job of
mitigating important platform-based attacks and supporting important new
use cases (the "burn after reading" feature).  When it's actually open
source I'll take a look and if it is good, I'll recommend it to users.
Until that open review happens, I think it's inappropriate for voices in
our community to commend or recommend such a proprietary system.  Each
person makes their own choices, of course, and nobody should base their
actions solely on what *I* think is right, but I hope you can hear my
concerns and consider the outcomes of your actions.
Unsubscribe, change to digest, or change password at:

@_date: 2013-07-22 14:46:15
@_author: Andy Isaacson 
@_subject: Python Random Number Generator for OTP 
References: Why not simply use /dev/urandom (after ensuring you have enough entropy,
etc, etc).  If you don't have systemic entropy collection, Python is not
going to be able to help.
Of course any entropy pool measurement is merely computationally
feasible randomness; you'll need to measure a physically
nondeterministic process directly if you want true information theoretic
entropy.  Something like an entropykey should do the trick, if you trust
their design and that they haven't included backdoors.

@_date: 2013-07-23 01:34:54
@_author: Andy Isaacson 
@_subject: Python Random Number Generator for OTP 
References:  <20130722214614.GF25759
If Bob requires *really* *great* entropy, why would he trust a network
link (secured with a non information theoretically secure cipher such as
AES) to transmit his entropy securely?
If Bob is willing to trust merely computationally secure methods such as
private key cryptography, he should gather "less high quality" entropy
locally, using a pool implementation with good mixing, and trust that.
In short -- asking someone else to generate your random numbers is, of
course, a state of sin.

@_date: 2013-07-24 09:16:27
@_author: Andy Isaacson 
@_subject: Gnu PG is more Safe ? 
References: <51EF7B99.90207
Of course open source isn't magic pixie dust, but neither is most
commercial software very well analyzed.  There are exceptions, but most
commercial software that I have direct experience with is lacking the
"active analysis" by people who are qualified and motivated to find

@_date: 2013-07-24 10:27:06
@_author: Andy Isaacson 
@_subject: Python Random Number Generator for OTP 
References:  <20130722214614.GF25759
 <51EE318C.9070309
 <20130723222446.98325D061
My /dev/random generates a few hundred kilobytes a day.  I exchange OTPs
on a SD card to a friend sitting across the table.  I need to be able to
make a bigger pad than allowed by the horrifically overly conservative
entropy estimates provided by /dev/random.

@_date: 2013-07-24 12:46:24
@_author: Andy Isaacson 
@_subject: Python Random Number Generator for OTP 
References:  <20130722214614.GF25759
 <51EE318C.9070309
 <20130723222446.98325D061
 <20130724172706.GL27178
I don't use it for anything real, because among other issues there's no
message integrity, but: onetime.

@_date: 2013-07-26 05:27:44
@_author: Andy Isaacson 
@_subject: SSLegance 
References: I've run my primary browser with no trusted CAs, manually TOFUing
certificates for sites, for months on end.  It's slightly easier than
"view source" to use control-shift-K (in Firefox) and reload the page,
then watch for resource load errors in the console.  Some fairly small
adjustments to browser UIs would make this use case much easier.  The
biggest problem is that Firefox's SSL exception implementation only
allows a single certificate per hostname, so load-balanced hosts such as
p.twimg.com which toggle between multiple valid certificates are
(I also VPN this browser through a fairly trusted datacenter, so I'm not
TOFUing over the local WLAN of course.)
It's fairly helpful to use SSL errors as a firewall to help me avoid
accidentally loading sites whose TOS I refuse to accept, such as G+ and
It also functions as a primitive adblock for some sites since you don't
have to accept the certificates for doubleclick.net et al.

@_date: 2013-11-07 17:38:25
@_author: Andy Isaacson 
@_subject: sidebands of great =?utf-8?Q?justice_?= 
=?utf-8?B?4oCd?= the mysterious Mac and PC malware that jumps airgaps]
References: Or, you know, just look on github.

@_date: 2013-11-08 11:45:11
@_author: Andy Isaacson 
@_subject: sidebands of great =?utf-8?Q?justice_?= 
=?utf-8?B?4oCd?= the mysterious Mac and PC malware that jumps airgaps]
References:  <20131108013824.GI18544
 <20131108044449.GB27852
Ettus has some new lower-cost SDR boards that are getting approving
glances from several of my radio-savvy friends:
And the new kid on the block is Nuand BladeRF, with a half-the-cost
design that seems pretty promising:
Both of these theoretically support MIMO, with clock distribution
available as an added-cost option to sync up multiple boards' Tx/Rx.

@_date: 2013-11-08 11:50:40
@_author: Andy Isaacson 
@_subject: good clocks (not using GPS) and multi-channel hw [was: 
References: You don't need a global clock like GPS for MIMO, just a local high
quality clock pulse to build a time base across multiple transceivers.
... unless I'm missing something?
Nuand already has 2x2 MIMO by slaving one board to another's clock, and
is developing a 4x4 MIMO clock distribution board.

@_date: 2013-11-10 00:54:01
@_author: Andy Isaacson 
@_subject: NIST Randomness Beacon 
References: <527F0B61.7060700
    WARNING:
    DO NOT USE BEACON GENERATED
    VALUES AS SECRET
    CRYPTOGRAPHIC KEYS.
The Beacon is a potentially useful service.  Folks have implemented
similar semantics by, for example, hashing the DJIA closing value of a
given date (see NIST's implementation, of course, makes them a trusted third party to
any security critical applications of this oracle.  I'd be more
comfortable with a cryptographic hash of an unpredictable but publicly
determined value; however, it's hard to find one that has as much
entropy as the Beacon.
For example, suppose you use the low bits of the bitcoin blockchain
hash.  An attacker with 10% of the hash power could probabilistically
attack such a system by chosing blocks with a specific value in those
bits; furthermore, the miners might know the relevant value earlier than
other users of the system.

@_date: 2013-11-12 16:10:51
@_author: Andy Isaacson 
@_subject: (Times of Israel) Stuxnet, gone rogue, hit Russian nuke plant, 
References: The final payload was specific to the Natanz turbine controllers.  The
Windows malware delivery mechanism, though, could in theory infect any
Windows host it came in contact with (that didn't have the 0days fixed).
The intermediate stage attacked the Siemens Step7 software, which runs
on Windows and which could potentially be used in space applications
(although it seems somewhat unlikely that it would have been used *on*
the ISS).  The intermediate stage was designed to be inactive unless the
specific configuration of hardware found at Natanz was detected, so in
theory it should be "safe" even if Step7 were found on an ISS system,
but that theory seems risky to depend on.
Reading the reports charitably, I would suspect that the Windows malware
delivery mechanism might have been transported to the ISS, but would
have been inactive there in the absence of a Step7 installation for the
intermediate stage to infect.

@_date: 2013-11-23 11:43:27
@_author: Andy Isaacson 
@_subject: whoah 
References: <20131123114848.GI5661
Tim Lee cites Sarah Meiklejohn:
    While she says she can't be sure, Meiklejohn says that that
    194,993-bitcoin transaction was probably done by Bitstamp, the
    world's second-largest exchange for trading dollars for bitcoins:

@_date: 2013-11-25 16:32:04
@_author: Andy Isaacson 
@_subject: bitcoin as a global medium of exchange (was Re: Interesting take 
References: <1385370567.90180.YahooMailNeo
 <5293740D.4030506
 <1385417238.14632.YahooMailNeo
 <1385421475.40988.YahooMailNeo
Fractional bitcoins work just fine (down to 1/100,000,000, per
 ).  If BTC goes to 100,000 USD we'll
just start pricing things in "thous" or "mils" or something similar.
In other words, there are already 12 quadrillion Satoshis in
circulation, plenty to absorb any further deflation.

@_date: 2013-11-25 18:43:07
@_author: Andy Isaacson 
@_subject: bitcoin as a global medium of exchange (was Re: Interesting 
References: <1385417238.14632.YahooMailNeo
 <1385421475.40988.YahooMailNeo
 <20131126003204.GK7523
The critical feature of the BTC PoW block chain is that the work is
applied to a believed-computationally-hard problem that is a function of
the block under consideration.  This precludes the "work" being a
function of any other property.
How quickly we forget ... Bitcoin did 4 impossible things before
breakfast, and now we're whining that it didn't do 5. :)
In 2008 nobody in the open research community would have proposed that a
peer-to-peer (1) autoscaling (2) computational PoW (3) deflationary (4)
space-conserving cryptocurrency was even theoretically possible.  Then
Nakamoto dropped working code and the paper.
Adding a "useful work" unit to the mining PoW has been considered; it's
extremely hard to do and puts the "useful work" project (whatever it is)
squarely in the line of fire for fraudsters and attacks.

@_date: 2013-10-04 01:16:52
@_author: Andy Isaacson 
@_subject: Silk Road founder arrested ... 
References: <20131002123743.GA14320
 <524C41F5.5020105
 <1380734343.30026.10.camel
 <1380740444.30026.18.camel
 <20131002193108.GA11783
 <1380742664.5216.3.camel
 <20131004002225.AB0F4DFB9
The FBI malware didn't attack the Tor network, it just caused vulnerable
endpoints to connect (outside of Tor) to a tattle-tale network server.
Not email, but rather, private messages on the Silk Road platform.
Which apparently stored more or less all messages, forever.

@_date: 2013-10-04 11:49:52
@_author: Andy Isaacson 
@_subject: how to use Tor securely (Re: Silk Road founder arrested ...) 
References:  <1380734343.30026.10.camel
 <1380740444.30026.18.camel
 <20131002193108.GA11783
 <1380742664.5216.3.camel
 <20131004002225.AB0F4DFB9
 <20131004081652.GK15039
 <20131004090126.GA2045
 <524E9590.702
Note that this thread has meandered around, discussed several different
security failures, and you seem to be returning to the Silk Road one.
I don't see any evidence or claim that the investigation touched,
investigated, or influenced any Tor relays in the published documents
about the Silk Road arrest.  Do you have any basis for this
(BTW, it's *very* easy to "find a bunch of Tor machines", most of the
Tor relays' IPs are listed in the public "consensus".)
Again, I see no published claim that any malware was used in this
investigation, nor that the investigators had to lean on anyone (much
less torture them, as the phrase "rubber hose" indicates) to install
The complaint and the indictment are stunningly silent on that part of
the investigation, and the press coverage I've seen also doesn't shed
much light on exactly how the machine in "a certain foreign country" was
located.  A few possibilities have been raised:
 - an investigator exploited the Silk Road software stack via its public
   web UI and caused the server to disclose its IP by connecting to a
   service outside of Tor.  This seems quite plausible, to me.
 - the investigation already had Ulbricht targeted, but without a
   smoking gun, and watched his SSH traffic using a standard wiretapping
   warrant.  This should have shown up in the arrest complaint if so.
 - a NSA/GCHQ capture was used to locate the server, and the public
   disclosure so far is an example of "parallel construction".
 - a vulnerability in the Tor network let the investigators find the
   server, possibly assisted by the investigators running some number of
   Tor relays.
 - the IP was known to any of the several criminal elements known to be
   interested in Silk Road, and the investigators got it as part of a
   deal (to drop another investigation, or harass someone's enemy, or
   similar).
Given the shoddy quality of the rest of Ulbricht's security posture, I
strongly suspect that a "phone home" vuln in the SR server was the
trigger.  "Never trust anyone who's programming language of choice is

@_date: 2013-10-05 00:17:11
@_author: Andy Isaacson 
@_subject: [liberationtech] 49 Page NSA analysis of Tor 
References: <524F2D93.1010605
This is the output of a student Summer Program project, as advertised
    Cryptanalysis and Exploitation Services Summer Program (CES SP)
    (formerly MSEP)
    The Cryptanalysis and Exploitation Services Summer Program (CES SP)
    is open to undergraduate students majoring in mathematics, computer
    science, or a major with a strong background in math and computer
    science.
Here's one interesting story about a summer program invitation:
The 2006 CES SP Tor paper is pretty superficial; they make several
claims that don't bear up under the slightest analysis ("we might be
able to MITM a Tor node because the certificates are self-signed") and
don't seem to have developed any significant analysis or attacks on the
This document doesn't give much insight into capabilities the IC has
developed against Tor.  It's apparently quite common to run multiple
research teams (either known or unknown to each other) against a single
target, and a few summer students with a dozen lab machines is a pretty
small investment.  I'd expect there are other programs with more
sophisticated attacks, especially now 7 years later.
In fact the most enlightening fact about this paper might be that the
NSA thought Tor was worth attacking *at all* in 2006.
I wonder if tor.eff.org has any referer logs from 2006 showing inbound
traffic from  or similar.

@_date: 2013-10-05 13:25:20
@_author: Andy Isaacson 
@_subject: [liberationtech] 49 Page NSA analysis of Tor 
References: <524F2D93.1010605
 <20131005071711.GN15039
 <9DB4FA45-E882-48DB-9602-30AEBBD9106D
 <525031FB.4090304
Yep, I was specifically referring to Referer: headers.  I know I've
worked at places with an internal wiki, with revealing page titles, with
outbound links to our competitor's webpages.  *Hopefully* NSA/GCHQ are
more clueful than that, but I wouldn't put anything past them at this

@_date: 2013-10-11 09:35:42
@_author: Andy Isaacson 
@_subject: who are the service operators here? 
References:  <20131011114213.GY10405
Hmmm, I hadn't heard of YaCy before, thanks for the mention!
OVF is a dead end AFAICS.
It's not perfect, but the combination of Chef/Puppet (to specify +
install + configure the software stack) plus Vagrant (to specify +
install + configure the base VM) seems like a more fruitful path
forward.  There are some missing pieces; for example, it's regrettably
common in current Cookbooks and Vagrantfiles to download
unsigned-and-unhashed code from the network and trust it.  But that's
fixable with more hashing and content addressed storage.

@_date: 2013-10-22 00:07:15
@_author: Andy Isaacson 
@_subject: Curious RNG stalemate [was: use of cpunks] 
References: It's super frustrating that Turbid assumes you are going to
reverse-engineer the amplifier stage of your sound card in order to set
some difficult-to-understand parameters which apparently can completely
break it's ability to extract entropy if set incorrectly.  (See the
installation instructions in section 12 of the paper linked above.)
It would be much better for it to have a default set of parameters (or
an autotuned parameter engine) that have a very high likelihood of
giving acceptable results upon "apt-get install turbid" on some
arbitrary hardware.
I mean, seriously.  The Turbid authors appear to assume that every
person who installs Turbid is going to build a custom Y-audio cable and
put a voltmeter (set to the correct mode of course!) on the outputs of
their sound card.  WTF?
It's fine if conservative, default settings result in Turbid getting
only 100 bits of entropy per second rather than 100 Kbit/sec.  Mix it
into /dev/urandom and call it a day.

@_date: 2013-10-22 11:27:16
@_author: Andy Isaacson 
@_subject: DIY RNGs [was: Re: Curious RNG stalemate [was: use of cpunks]] 
References:  <20131022070715.GW15525
 <1382454909.16118.32.camel
 <1475165977.20131022184740
It seems that rnd_wavein uses a small window (you document 256 samples
as the default).  One common silent-failure mode of video capture
interfaces is to intermittently provide the same frame (around 1 MiB of
data) twice!
If your whitener doesn't chain blocks and you use the output directly as
random data (worst case, as an OTP) then a long-term repeat like that is
completely catastrophic, giving you a modern reprise of the Venona
If you do chain, it's merely reducing the entropy of the stream
significantly.  Also it's entirely possible that an attacker can
influence the behavior of the system; depending on your threat model
either through direct physical access or by causing CPU starvation
through a network or algorithmic DoS to trigger misbehavior in the
It would be much better to implement a multi-stage entropy pool design
with catastrophic mixing, such as Schneier et al's Fortuna:

@_date: 2013-10-22 12:17:51
@_author: Andy Isaacson 
@_subject: Linux Kodachi 
References: <201310212043.25539.envite
 <20131022021550.GC9206
 <52668E74.2090005 <20131022220952.044acd80
 <5266C72B.70106
A new project that was foolish enough to select sourceforge as their
hosting?  That, alone, is disqualifyingly stupid IMO.

@_date: 2013-10-23 23:18:26
@_author: Andy Isaacson 
@_subject: Curious RNG stalemate [was: use of cpunks] 
References:  <20131017204727.B32E8EB07
 <20131017221211.3b81105d
 <20131023100131.GA21168
I'm not at all sure "uniquely insensitive to tampering" is true against
an attacker who can influence the RNGs physical environment.  Suppose you're
timing alpha particles, using a clock accurate to microseconds, and the
attacker puts a microgram of 210-Polonium a few centimeters from your
detector; you'll have an event to measure every microsecond and your
detector saturates, resulting in an unending stream of 1s.
A similar attack (saturating a detector which is supposed to be secure
based on a "physical principle") defeats some "quantum key distribution"
systems (which seem to be snake oil for the most part); for example,
Certainly it's possible to add complexity to the system to ensure that
"everything is as it should be" and "nothing odd is going on"; this
complexity negates the putatively "simple" nature of systems that are
"uniquely immune" or whatever.

@_date: 2013-10-23 23:46:26
@_author: Andy Isaacson 
@_subject: Curious RNG stalemate [was: use of cpunks] 
References:  <20131022070715.GW15525
Six discrete (ISA and PCI) sound cards manufactured before 2008, plus
generic "intel-hda" and "usbaudio" profiles.
That might cover as much as 20% of systems shipped in 2013.
Also, AFAICS the .ctl files do not contain the Q, R, B, and K values
computed in sections 12.1 - 12.8.  There are sample values for a few
(circa 2005) devices in table 4.
The turbid.tgz download is unversioned and unsigned.  Something between
60% and 90% of PCs sold today are not covered, since only one device
that's included is still on the market (intel-hda).
The Turbid paper seems focused on generating a few KiB/sec of
physical randomness, continuously.  The actual problem facing users
today is getting 100 bits of randomness, *ever*, to seed urandom.
This seems like a classic example of engineering building a system
that's far beyond spec for the problem it's actually supposed to solve,
and incapable of adressing the actual problem due to overengineered
Turbid fails the first rule:  build systems for people to actually use.

@_date: 2013-09-05 11:58:02
@_author: Andy Isaacson 
@_subject: PayPal freezes MailPile's account 
References: <2780551.exnjNUlKQP
 <20130905111933.GC29404
Your first and second sentences don't go very well together, Eugen. :)
Using Paypal is not a bad idea; they're pretty efficient in terms of
fee percentage, their servers don't go down very often, and they have a
very large market presence in the US and a few other markets.  I
personally don't use any other standalone online payment system.
But, keeping a significant fraction of your capital in Paypal is a
terrible idea.  They have very little patience for innovative business
models, they have a poor track record of customer service, and their
default response to any surprise is to freeze the account balance.
Noisebridge automatically transfers our Paypal balance to our bank
account on a regular schedule (nightly, I think, but don't quote me.)

@_date: 2013-09-05 18:25:42
@_author: Andy Isaacson 
@_subject: what has the NSA broken? 
Tinfoil hat time ...
With today's disclosures, the question turns to -- what has the NSA
broken?  Unfortunately the journalists bowed to pressure from the
espionage-industrial complex and decided not to publish specific
details of what's broken; and the Snowden documents don't include all
the compartmentalized details anyways.  So all we can do is speculate
based on what is already known and the high level overview provided.
I don't believe that NSA has a complete AES break.  Call me foolish if
you must, but it's just not consistent with what we know so far.  I
believe that a correctly implemented, truly randomly keyed AES-256-CBC
or -CTR cipher is robust against cryptanalysis.  It seems just barely
possible that AES-128 has a complete break, since I suspect NSA can do
2^80 work on 2^60 bytes if it gives them decrypts of all the AES-128
they can sniff.
However, virtually nobody properly keys their ciphers with physical
entropy.  I suspect that correlated key PRNG attacks are almost
certainly a significant part of the NSA/GCHQ crypto break.  Many
deployed systems expose a significant amount of correlated output of
and serveral TFLOPs of built-to-spec supercomputers [1], this seems like
an obvious place for a hidden advance.
Also, retrieving key material from endpoints is a high return activity.
Nearly nobody uses PFS ciphersuites, many HTTPS privatekeys are used for
multiple years, and a single 1 KiB leak of key material is sufficient to
decrypt all traffic under that key.  (You don't even need the whole key,
just half the bits are plenty to reconstruct RSA keys using attacks in
the open literature.)  Insiders copying privatekey files after hours,
DRAM remanence after "hardware failure" in SSL offload boxes, bugdoors
leaking key bits in subtly biased entropy from crypto accelerator
hardware, on-disk encrypted keys decrypted due to low entropy
passphrases, etc.  Any key stored on a US-based VPS is obviously
compromised.  (Doubly so if your VPS is linode.)  Radio emissions from
colocated boxes are a nearly completely unexplored area of research.
Server-class IPMI baseboard coprocesssors have undisclosed access to
host RAM at runtime, and often unaudited access via provider
management-plane Ethernets.  If I had to get the keys out deniably, I'd
be scanning RAM for high entropy key schedules and leaking key bits in
the timing of heartbeat messages.
It seems fairly likely that NSA is at least a decade ahead of academic
RSA factoring.  I've heard second-hand stories of $10M machines of
custom ASICs built to attack RSA before 2005, and third-hand stories of
machines far weirder than that.  RSA-1024 I'd treat as dead, RSA-2048 is
probably robust enough that if NSA have an attack it would be too
valuable to risk exposing under anything but an existential threat
Non-AES legacy/proprietary ciphers are probably toast.  People switching
to RC4, stahp!  A5/2, lulz.  Maybe GOST and twofish and Salsa20 are
secure; I've met djb and all my checks for NSA minders came up negative.
[1] Cray is still in business, building 10,000 CPU with attached FPGA
    and 1Âµs interconnect megaclusters for "undisclosed government
    customers".  The systems listed as "Government" in the latest top500
    list are just the tip of the iceberg; larger systems are built and
    installed without any public disclosure.

@_date: 2013-09-06 12:34:54
@_author: Andy Isaacson 
@_subject: [liberationtech] Random number generation being influenced - rumors 
I agree with some of your premises, but disagree with the conclusion you
seem to be drawing.
Yes, it's just a fear of uncertainty.  We do not have evidence, nor even
a claim based on knowledge, that HWRNG backdooring has occurred.
However, I claim that the fear is well founded and should be taken into
account by all threat models.
HWRNG is a nearly-uniquely difficult security problem to crack.  By
definition it is impossible to prove that a black-box HWRNG is safe.
This is different from the security properties of a blackbox AES or
MODMUL accelerator, which can be demonstrated to conform to a known
specification.  If your AES instructions don't do AES, then testing
against a software implementation will show it!  The AES logic unit
will have a hard time leaking the AES keybits since there's nowhere
nondeterministic to put them.  etc.
By contrast, a properly functioning HWRNG cannot be tested in a way that
distinguishes it from the output of a stream cipher seeded with a
backdoor key.  And there's no way to test the behavior of HWRNG on an
ongoing basis; even if you had a test to run, it might switch to "stream
cipher mode" under the covers.
This is not to say that RdRand is completely unusable.  Putting RdRand
entropy into a software pool implementation like /dev/urandom (or
preferably, a higher-assurance multipool design like Fortuna) is a cheap
way to prevent a putative backdoor from compromising your system state.
Now, there is a way that we can learn that a backdoor was included; if
someone does a tear-down of a HWRNG and finds circuitry that has no
purpose other than being a backdoor, that would be conclusive.  AFAIK
nobody has tried that experiment.
Weighing towards distrusting HWRNG we have the fact that NSA is reported
(yesterday) to have intentionally backdoored Dual_EC_DRBG, and to have
spent significant amounts of money to backdoor chip implementations,
with enough success that they brag about it in administrative summaries.
So, I put a lot of credence in distrusting HWRNG black box
implementations.  But unfortunately we need a lot more reliable entropy.
A fully open source, nothing up my sleeve hardware entropy source would
be a huge improvement.

@_date: 2013-09-06 22:24:00
@_author: Andy Isaacson 
@_subject: [liberationtech] Random number generation being influenced - rumors 
That's the claimed design, yes.  I see no particular reason to believe
that the hardware in my server implements the design.  I can't even test
that the AES whitening does what it is documented to do, because Intel
refused to provide access to the prewhitened input.
Providing accessible "test points" (software interfaces to the innards
of the implementation, with documentation of expected behavior between
the components) would be the absolute minimum to provide believable
assurance of the absence of a backdoor.  Better would be documents from
Intel of how the chip is designed at the mask level, and a third party
mill-and-microphotograph of a retail chip showing that the shipped
implementation matches the design.
Intel will never go for that, of course, since their chip masks are
their jealously guarded IP.  Since they can't provide evidence of a lack
of a backdoor, any reasonably cautious user should avoid depending on
Intel's implementation.

@_date: 2013-09-09 21:38:13
@_author: Andy Isaacson 
@_subject: hardware RNG 
References: <5.2.1.1.1.20130909162233.01aede10
 <20130909195833.1E0F5E0CD
If you go down this path, you'll want to review the math at
Diehard is very good at what it does ... but what it does is not very
useful for validating a HWRNG.  There's a long but very clear
explanation of why, including examples, in the Turbid paper, section 7:

@_date: 2013-09-10 16:37:36
@_author: Andy Isaacson 
@_subject: [liberationtech] iPhone 5S Fingerprint and Records (Was: iPhone5S and 5th amendment) 
Printing a fingerprint is pretty easy:
No word yet if this technique works on the iPhone reader.

@_date: 2013-09-12 02:06:13
@_author: Andy Isaacson 
@_subject: [tor-relays] Getting max bandwidth out of a relay 
There are three main sinks of CPU usage in a well-configured large Tor
1. doing AES and SHA.  This scales with the network bandwidth used.
2. doing Montgomery multiplication for circuit creation requests.
3. bookkeeping.
(4. kernel TCP overhead etc.)
Until the August botnet hit,  was the primary user of CPU on most
relays.  A single Xeon core can do about 150 MB/sec of AES, or with
AES-NI around 700 MB/sec.
With the vastly increased circuit creation load currently in progress,
 and  have become a larger problem.  The bookkeeping, in particular,
has grown significantly.  On noisetor right now, 17% of all CPU cycles
are being spent in a single bookkeeping routine,
circuit_unlink_all_from_channel, according to "perf top".
This increased circuit-create-and-destroy CPU load reduces the CPU
available to do useful AES, so as a result, currently many Tor relays
are showing increased CPU usage with decreased bandwidth usage.
You'll have trouble getting a single Xeon core to run much more than 300
Mbps even with AES-NI, even without the botnet increasing CPU load
without increasing throughput usage.  In the current state, with so much
extra bignum work and bookkeeping, a single daemon will have even more
trouble pushing much bandwidth.
Best practice for maximum bandwidth is to run one Tor daemon per
physical core, each on a distinct IP address.  Plan for each daemon to
push about 15 MByte/sec.  They can do more like 20 or 30, but planning
for lower leaves some headroom.
Your boxes, with 12 cores and 70 GB of RAM, are quite a bit overpowered
for running 500 Mbps of Tor.  If you ran a Tor daemon per core, you'd be
able to push around 2 Gbps of Tor traffic, easily.
tor-relays mailing list
tor-relays at lists.torproject.org

@_date: 2013-09-19 14:19:48
@_author: Andy Isaacson 
@_subject: Linus Torvalds admits he was asked to insert a backdoor into 
References: Because it's funny, and one of the ways that humans deal with stressful
situations is through humor.
Having seen Linus give many talks through the years, I can assure you
that he might make such a joke regardless of the true situation.
Or it might indicate that he knew it would get a laugh from the
audience.  It's invalid to read detail into a single such comment.
It would seem logical, yes.  It would also seem logical for Them (tm) to
game-theory two more stages, and specifically avoid asking the most
prominent maintainers in favor of pressuring or encouraging lower-level
contributors to insert (or avoid fixing) bugdoors.  I don't think we
have enough information to make an informed judgement which scenario has

@_date: 2013-09-21 15:04:15
@_author: Andy Isaacson 
@_subject: [liberationtech] News from Eric Hughes 
References: <660D47F6-23CA-485F-8A9C-6B5A12F2A414
Why on earth do you route through t.co.

@_date: 2013-09-21 15:44:13
@_author: Andy Isaacson 
@_subject: Mailing list format with Subject Tagging 
References: <523E1058.9050105
The List-ID header is perfect for filtering, and I am always sad to lose
space in the often-limited subject field display to [tag]s.
So please don't add subject tags.

@_date: 2013-09-22 16:58:47
@_author: Andy Isaacson 
@_subject: [8] code observatory 
References:  <20130922223905.GP15094
No, I'm not.
Your threats and crude language don't make me think that I should bother
reading anything else you post.
Also, it's impolite to re-CC the list when I specifically didn't post my
reply to the list.  Doing so merely to threaten me is ... amusingly

@_date: 2013-09-29 17:10:31
@_author: Andy Isaacson 
@_subject: NSA IDA Cryptological Research Centers 
References: As is common in math, they define what they mean in the first paragraph.
To paraphrase, they're considering ways to arrange a large number of
sets of s so that a minimum number of "blocks" is used to enclose
all of the sets.
I'm not a mathematician but that looks like set theory to me.  It's the
kind of fundamental mathematical research that frequently arises when
considering some more applied problem space.  Such fundamental
approaches frequently have applications in wide-ranging fields; to
compare to a more well-documented example, the "4-color problem" first
solved in the 70s generated techniques which ended up being critical to
optimizing C compiler designs for RISC processors in the 90s.
I doubt that much can be concluded about the activities at the research
site based on their publishing one database in such a rarefied field.

@_date: 2014-04-06 15:14:49
@_author: Andy Isaacson 
@_subject: [tor-talk] How safe is smartphones today? 
References: <533C96AF.2050004
 <20140403013535.GA9002 <533D33AD.3040509
Agreed that free-software SDR is better in the long run, but there are
blob-free WiFi cards available:
(they even let you pay with bitcoin.)

@_date: 2014-04-11 15:44:19
@_author: Andy Isaacson 
@_subject: NSA alleged to have known & used Heartbleed for 2 years 
References: <53484395.9090208
 <53485E00.5050500
The only weasel room I can see is if the exploitation capabilities are
in DoD Cyber Command, rather than NSA.

@_date: 2014-08-17 15:06:58
@_author: Andy Isaacson 
@_subject: [cryptography] Question About Best Practices for Personal File 
References:  <1408308993.4145919.153686805.74F2CD04
I'm a significant proponent of open source, and the benefits you
enumerate here are definitely true.  Open source can be helpful in
reviewing code, in grokking developer intent, in providing a hash-chain
guarantee of code lineage, in providing change history and justification
when reviewing new releases of a previously audited program, and in
fostering positive engineering practices.
However --
Your "proprietary program" strawman is full of holes.
The intellectual labor of decompiling a program delivered as a binary is
not especially large compared to the labor required to do a thorough
systematic review.  Given IDA Pro and a non-obfuscated Win32 or Linux
app, people I trust say the decompilation process is on the order of
10%-20% of the total effort of a review.
Binary patches are not great by any means, but they are definitely a
feasible method of deploying fixes, and this method works and is well
tested in the real world.  Some kinds of deployments basically require
binary patching, no matter what the underlying source management
technology.  (The Linux Ksplice project provides one prominent example.)
Backdoors are an enormous problem for both open source and
binary-distribution codebases, and claiming that open source will save
you from backdoors ignores the reality of the situation.  Just to start,
"Building Reliable Voting Machine Software", Ka-Ping Yee
page 148 of  provides a sobering
assessment of the difficulty of finding intentionally inserted bugs in
open source software.

@_date: 2014-08-17 15:44:51
@_author: Andy Isaacson 
@_subject: How does the Hacking Team network malware work? How bad is it? 
References: As an attacker, you want your attack to be targeted to a single user.
(It's not the end of the world if you QUANTUMINSERT a few extra
machines, but byspray increases the likelihood of a sample escaping to a
non-colluding AV vendor [if there are any of those] or a curious
So you want to target a HTTP session that you have high assurance
belongs to the targeted user.
For targets who have a Google account, Google has helpfully assigned
cookies which associate the user's account with the HTTP stream.  These
cookies are initially established over https, but are linked to YouTube
cookies for unencrypted http so that YouTube can provide valuable
advertising services at lower cost than serving over HTTPS.
The general technique is flexible to any targeted vulnerable network
software, but to make my description more concrete, the Flash one is
very understandable.  The Flash plugin has hundreds of unpatched RCE
vulns.  The exploit for these vulns is generally a sequence of a few
hundred ActionScript bytecode instructions.  The YouTube webpage serves
a very standardized Flash .flv file which is nearly identical for every
video.  So as the QUANTUMINSERT vendor, you code up a module which takes
the .flv off the wire from the server, patches it to include the
exploit, and puts a new .flv on the wire to the user.  Then you wire
that up to an inline capture appliance which uses an FPGA running at
10GigE wire speed to match HTTP responses that contain the desired
cookie, and just wait for your target to desire a fuzzy kitten video.
I don't know any details of any fielded systems, so the following is
just my description of a sensible way to build this product.  (I have
some experience building products with these technologies.)
The injection is extremely low level, at the Ethernet frame processing
layer.  The FPGA has the responsibility of running at line rate and
forwarding all frames except those belonging to a stream marked as a
When the regex engine running in the FPGA sees the Cookie: header of the
target, it notes the IP-4-tuple (srcIP, srcPort, dstIP, dstPort) as an
intercepted stream, and forwards those packets to a higher-level
software layer.  Every other stream on the network keeps running as
At the higher layer, QUANTUMINSERT wants to keep youtube.com happy, so
it forges the ACK packets that the browser would have been sending.
QUANTUMINSERT also can see what packets the browser has received so far,
and the data that youtube.com was sending.  QUANTUMINSERT then edits the
FLV, on the fly, so that it contains the exploit code *and* the fluffy
kitten video.  The resulting FLV is the same size or maybe a few hundred
bytes longer, but that's peanuts compared to the size of the MPEG video
stream, so nobody notices.
it just arrived 150 milliseconds later than expected (due to the
QUANTUMINSERT flv editing software).  From the point of view of the
user, the fluffy cat video pranced as expected.  From the point of view
of the Mukhabarat, they've got a backdoor into the PC of another
dangerous twitter user.
(OK the Bahraini Mukhabarat don't use QUANTUMINSERT, they are a Hacking
Team customer.  Same difference.)
Any specific attack is platform+browser dependent, but the general
technique works everywhere where you have 0day RCE vulns.  (ie
everywhere.)  Browser and OS vulnerability mitigation techniques like
ASLR and (especially) sandboxing highly vulnerable components like Flash
can help enormously by raising the complexity of exploits, by reducing
the supply of known+weaponized 0day, and by requiring more complicated
multistage exploits.
If you haven't read it already, the WaPo spy story is *incredible*:

@_date: 2014-12-02 10:52:54
@_author: Andy Isaacson 
@_subject: [qubes-users] First Mention of Qubes in US Court Documents 
References: <20141202155906.GA10743
The mention of Qubes is interesting, but the rest of this document is
even more amazing!
First amazing assertion: use NLP to automatically redact classified
documents.  (page 3.)
Second amazing reference: links to a tor2web URL. (page 4, footnote 6)

@_date: 2014-06-03 15:53:02
@_author: Andy Isaacson 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
References: <1800350.DuBgtkdSDz
My mitigation would be to make auditing a default-deny rule, rather than
a default-allow.
Security auditing needs to be a holistic analysis, starting by
re-engaging with the requirements, verifying that the design is a
sensible and minimal approach to addressing the requirements, and
verifying that the implementation is a sensible, safe, auditable,
version controlled, approach to the design.
If the auditor at any point says "Well, I wouldn't have *recommended*
that you implement your JSON parsing in ad-hoc C with pointer arithmetic
and poor and misleading comments, but I can't find any *bugs* so I guess
it must be OK" then that is an immediate fail.
This is the default deny: we default to assuming the system is insecure,
and any sign that this might be true results in a failure.
Versus the current auditing method of default-allow: we run the audit,
and if no *concrete* exploits or bugs are found before the auditors run
out of time, then we trumpet that the system "has passed its audit".
Only if the design is sane, the implementation is sane, the development
team is following best practices and defensive coding strategies, with a
cryptographically and procedurally audited edit trail (immutable git
commit logs signed and committed to W/O media) in a development
environment that is safe by default rather than risky by default ...
... then you *might* have a chance of catching the intentional backdoor
inserted by the APT malware on your team member's workstation.
Current efforts in this direction fall *very* far short of the utopia I

@_date: 2014-06-03 18:32:52
@_author: Andy Isaacson 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
References: <1800350.DuBgtkdSDz <20140603225302.GJ10586
 <1462819215.136856.1401844030733.JavaMail.www
That seems like a feature...
(note that I don't think most software should be audited as security
critical.  We can reduce the Trusted Computing Base and audit only those
I like to compare our current situation to the Steam Age.  There was an
enormous amount of innovation in steam power, heating, etc in the 1800s.
There was a concomitant lack of standardized safety measures, and
occasionally boilers exploded taking entire apartment buildings with
Over time the rate of innovation decreased, standardization set in,
safety measures were instituted, and now we have boring steam radiators
in apartment buildings rather than exciting steam-powered Difference
Engines in our pockets.

@_date: 2014-06-04 11:22:52
@_author: Andy Isaacson 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
References: <1800350.DuBgtkdSDz <20140603225302.GJ10586
 <538EB484.7040405
I strongly disagree.  There are implementations that are Just Too
Complicated and are Impossible To Audit.  Such implementation choices
*do*, empirically, provide cover for bugs; and as we as a society build
more and more software into the fabric of our life-critical systems it's
imperative that "the implementor liked this complexity and refuses to
change it" gives way to the larger goals at stake.  The auditor
absolutely must have leeway to say "no you don't get to write your own
string processing, you are going to use the standard ones."
This kind of feedback is precisely what happens in the higher quality
audits that are becoming standard practice for security-critical
As a satisifed iSec customer (at a previous job), I have a bit of
insight here.  iSec is a leader in this space and definitely leads by
example.  Across the industry, the average quality of discourse in the
source auditing business is pretty good in my experience; only the
bottom-skimming truly awful auditors reduce their customer-facing
feedback to just a binary pass/fail.
However, inevitably, in the societal analysis of software quality for
practical purposes, reductive reasoning happens.  (This is not a bad
thing, it's absolutely necessary -- we humans don't have the cognitive
capacity to hold a complete decision tree in our head while doing this
reasoning.)  Thus statements like "you should use $OSS_CRYPTO_PACKAGE,
it has passed its audits" end up playing a role in the discourse.
We as domain experts have an obligation to ensure that our contribution
is given appropriate weight in the debate and decisions -- in both
directions.  For example if an auditor sees their results being
mis-interpreted in customer marketing material or media coverage, the
auditor has a moral obligation to correct that and insist that the
mischaracterization stop.  (And yes, I believe that this moral
obligation would override an NDA between the customer and the auditor;
the contract should be structured to recognize this fact.)

@_date: 2014-06-21 05:14:52
@_author: Andy Isaacson 
@_subject: dm-crypt+LUKS 
References: <53A0EC7C.6020208
The design and implementation of dm-crypt + LUKS appears to be solid.
The code is well maintained and your use case is well supported by
distributions and other software.
I haven't given it a thorough audit but at least dm-crypt has avoided
many of the OMG WTF bugs and design flaws that have plagued other disk
encryption systems (looking at you, ecryptfs).

@_date: 2014-05-28 18:11:08
@_author: Andy Isaacson 
@_subject: is truecrypt dead? 
References:  <1401310124.13661.122629289.65499863
ecryptfs is a complete joke.  It intentionally does not encrypt *ANY*
metadata execpt the filename, leaking modification times, filesizes
(rounded to the block), write patterns, file ownership, permissions,
etc.  Because it's design is such a joke, it hasn't gotten any serious
crypto review, so I'd be surprised if it doesn't have critical
implementation bugs in the parts that aren't broken by design.
Please don't use ecryptfs.  It's not even better than nothing.

@_date: 2014-11-13 10:29:05
@_author: Andy Isaacson 
@_subject: Lantern: One Device, Free Data From Space Forever 
References: <20141113085235.GR10467
 <1994186.yFoYsa0xCB
Outernet, at least, appears to be a real thing:
Appears to be a different project.

@_date: 2014-11-19 13:35:33
@_author: Andy Isaacson 
@_subject: WhisperSystems + WhatsApp 
References: <2429476.9jgn6LQJC8
 <9AF6C350-E5C5-4753-BEDF-AA106E3B4321
Not meaningless, although of course open source would be preferable from
a trustability standpoint.  I've got the executable code for the
proprietary WhatsApp apk installed on my phone, and can reverse engineer
it if I so choose.  (I'm running CM11 so extracting the APKs is fairly
straightforward.)  I also have automatic app updates turned off, so I
know when the code is supposed to change.
Of course it would be Best (TM) if everyone could use a completely
free operating system and had complete freedom to inspect all the code
we depend on.  But given the world we live in, 600M users with access to
E2E encrypted messaging is better than 600M users without such access.

@_date: 2014-11-19 15:40:30
@_author: Andy Isaacson 
@_subject: WhisperSystems + WhatsApp 
References: <2429476.9jgn6LQJC8
 <9AF6C350-E5C5-4753-BEDF-AA106E3B4321
 <20141119213533.GE5226
Have you heard of the phrase "harm reduction"?  You can't solve a
social/technical problem by insisting that only perfect solutions are
acceptable.  You must provide incremental solutions that can be part of
a broad based move from the horrible place where we are now, towards a
more safe future.
I mean, *you* can do whatever you want, but users are going to ignore
solutions that don't connect to where they are today.  "Incremental
steps with continuous improvement" is a model for advice that actually
works in improving outcomes for real populations.  "Burn everything to
the ground and start over" is a model for advice that lets activists
maintain ideological purity without dirtying their hands with actual
people's actual problems.

@_date: 2014-11-28 20:00:10
@_author: Andy Isaacson 
@_subject: WhisperSystems + WhatsApp 
References: <2429476.9jgn6LQJC8 <20141119234030.GH5226
 <7747702.P3C0aqJKYt
Agreed on the first point, disagree on the second.  Any system that
claims to be secure will attract uses that are inappropriate to its
assumptions.  Documentation is not enough to dissuade this.
A colleague and I, both interested in modern cryptographic systems,
started to collaborate on a new project, using Pond.  Months later, we
realized that we had communicated useful information early on, over Pond
exclusively, and the "social norm that communications are deleted after
a few days" resulted in us losing important notes about the early days
of our project.
Even though it was clearly documented and I had simultaneously advocated
Pond to other experimental users for exactly this feature, I didn't
think through the consequences of this design feature for my use case.
I didn't even realize that I *had* a use case, until much later.
For this scenario, it turns out we wanted a modern secure communication
system more like Prate,  .  Except
perhaps with email-sized-message semantics rather than chat semantics
(or email in addition to chat?).
Generalizing from this specific example, you can find many other
examples of a security system being used outside of its designed
ssh is widely used for login to ephemeral hosts, reducing
TOFU to single session duration.
ssh is used with github as merely a bidirectionally-key-authenticated
transport layer ("git clone git at github.com:kragen/prate") rather than
its original remote shell purpose.
HTTPS x509 DV certificates have the mostly verstigal x.500 (iirc?)
Location/organization/etc naming support, the CN/sAN fields being nearly
the only operative ones.
HTTPS virtually never uses the many varied client authentication
mechanisms supported in TLS (client certificates, SRP, etc), instead
Rails and the many other web-app frameworks implement user
authentication over the top using passwords and cookies etc.

@_date: 2014-10-27 14:14:27
@_author: Andy Isaacson 
@_subject: CITIZENFOUR 
References:  <544C5587.10700
You know, the people who are involved in these stories are actually
humans, with real lives, families, and friendships.  If you watched
CITIZENFOUR without realizing that, I am terribly sorry for your
inability to releate to others on a human level, and I'd recommend that
you talk to someone with a more mainstream level of emotional
intelligence about these issues.
While Snowden is the object of significant official pressure, the rule
of law is still respected at least occasionally in the USA, and Lindsay
is not accused of any crimes.  There's no reason to suppose that she
would be prevented by USG from traveling, and any such restriction would
be front-page news.  (I'm not disputing that surreptitious tracking of
her and others is quite likely to be occurring, of course!)
Your insinuation that Snowden could not have had a girlfriend before his
trip to Hong Kong is baffling and inexplicable; he was leading a
perfectly normal life for someone in his position.  Both social and
economic documentation of their relationship exists.
(And your bringing up of her hobbies in this context seems to betray a
kind of naiive mistrust on your part; a majority of my friends who are
atheletic and in the 25-35 age bracket have tried out pole or other
circus arts.  It's fun!)

@_date: 2014-09-18 13:49:26
@_author: Andy Isaacson 
@_subject: killing RC4 in Chrome 
References:  <541AB34F.7080908
 <1411053055.21331.17.camel <3763193.Z2n6Afzyi6
 <1411071413.21331.20.camel
Plenty of sites switched *to* RC4 during the BEAST attack mitigation.
Some may not have switched back.

@_date: 2014-09-18 17:23:06
@_author: Andy Isaacson 
@_subject: killing RC4 in Chrome [now with certificate data!] 
References:  <541AB34F.7080908
 <1411053055.21331.17.camel <3763193.Z2n6Afzyi6
 <1411071413.21331.20.camel
 <20140918204926.GH12685
 <732e0075cfbdb38fe83701cbe151d189
Note that the BEAST mitigation consists of moving RC4 to the front of
the list.  RC4 was always a valid option on most server implementations.
So if you're "checking for RC4" by looking at the preference list,
you're overcounting.  Instead you need to look at what the existing
client implementations will choose when connecting to the given server
preference list.

@_date: 2015-04-17 11:16:00
@_author: Andy Isaacson 
@_subject: Raspberry pi safe? 
References:  <9A0AED52-625F-4A5E-957B-081D60BCB10C
I contest this claim.  BRCM SoCs are probably not the *worst* SoCs in
the market (that distinction probably belongs to Mediatek or a chinese
vendor we've never heard of) but they are almost certainly not in the
first ranks.
Unfortunately I can't make a strong argument as to which SoCs are in the
first rank.  I'd give some of the TI chips a higher chance, but
brand-hunting is not the route to safety -- some of the TI chips are
almost certainly as bad or worse.
I am concerned about the following --
1. existing SoCs CPUs certainly have errata (known errors or
undocumented "features") that are not disclosed to the public, and never
fixed in patched chip releases.  Some of these are likely to cause
security issues.  Previous SoCs (circa 2008) have had undisclosed bugs
in instruction decode allowing privilege elevation, for example.  Even
Intel and AMD, who have a *much* larger team working on these systems
than places like Broadcom and Mediatek, still manage to ship security
bugs from time to time... I don't give BRCM much of a chance of shipping
bug-free silicon.
2. SoCs contain a multitude of "Intellectual Property Blocks" such as a
DRAM controller, an Ethernet controller, USB, SATA, AC97 Audio, etc.
These are all connected together by an interconnect bus.  Each block
comes from a different development group, often purchased from a
different company.  The company that sells you the SoC often doesn't
even know what the features and bugs of their purchased IP cores
*are*... and that undocumented ethernet core may well have a "feature"
that would allow arbitrary access to the interconnect.
3. SoC interconnects don't have much in the way of security.  When the
Ethernet controller bug lets a Evil Packet onto the interconnect, it's
probably just a hop skip and a jump to main memory.

@_date: 2015-01-13 17:56:32
@_author: Andy Isaacson 
@_subject: [Cryptography] open hardware as a defence against state-level 
References: <54ACE91A.3050808
 <54B3A5F7.8020608
The LowRISC project aims to build a complete open SoC based on the
RISC-V architecture and get chips fabbed.

@_date: 2016-01-07 11:41:18
@_author: Andy Isaacson 
@_subject: Chaum Fathers Bastard Child To RubberHose ... PrivaTegrity cMix 
References: The privaTegrity (PT) backdoor is significantly more malignant than the
Tor dirauth issue.
If you pwn the Tor dirauths, you can sign and publish a false
"consensns" to clients that will cause them to use only your relays for
new connections, thus breaking anonymity for new connections.  Doing so
leaves a trail of bits showing that this was done (mostly just on the
target system).  Tor is actively seeking solutions to make their system
more privacy-preserving and if a better option shows up in research,
they will likely adopt it.
If you pwn the PT overlords, you can retrospectively deanonymize
connections that you recorded in the past.  If PT were deployed at scale
with a, say, 12-month deanonymization window [1] then every connection
during that interval would be silently deanonymized by APT0 who has
stealthily exfiltrated the overlord private material.
[1] the whole point of the PT backdoor and its claim to "break the
    crypto war stalemate" is that a lawful investigation could go back
    and ask "who sent this bomb threat".
If PT were deployed at scale and a vulnerability were found that used
the backdoor, the developers are left with an uncomfortable choice --
fix the vuln and thereby break the backdoor, or leave users vulnerable
and preserve the so-called "lawful" access?  This is not a conflict that
I want my privacy technologists to have to navigate.
Now, cMix seems like an interesting technology (much like the tech bits
of eCash were interesting back in the 90s, a previous   I
chatted with one of the coauthors yesterday and there's clearly an
interesting performance improvement to existing mix networks; read the
paper[2] for more details.  But the PT system built on it is predicated
on an unrealistic model of datacenter security, international
geopolitics, network economics, cyberwar, and network reliability.
[2] [3]
