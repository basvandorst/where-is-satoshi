
@_date: 2003-11-16 20:32:51
@_author: Zooko Wooko 
@_subject: [mnet-devel] Mac OS X binary package up 
I put up a Mac OS X binary package which is actually just a tarball with the
Mnet.sh script as the way to launch it.  I know that this doesn't make users
want to lick the screen, so it is only a temporary measure.
R. A. Hettinga The Internet Bearer Underwriting Corporation 44 Farquhar Street, Boston, MA 02131 USA
"... however it may deserve respect for its usefulness and antiquity,
[predicting the end of the world] has not been found agreeable to
experience." -- Edward Gibbon, 'Decline and Fall of the Roman Empire'

@_date: 2003-11-25 09:37:31
@_author: Zooko O'Whielacronx 
@_subject: [mnet-devel] Grid Of Trust -- pre-design 
take a peek and
for modification, but
grid.  This provides
Here is the text of that web page, for the sake of future generations who are
required to read the mnet-devel archives as part of their education:
                                 Grid Of Trust
   This isn't a design document.  It's more of a proposal for what could be
   to make sure all the attacks are really covered.
   Nodes are distributed randomly into cells of a hypercube with d
   dimensionals.  Each axis is broken down into g pieces such that the
   hypercube contains gd cells.
   Nodes in a cell can connect to all nodes in cells whose coordinates vary
   in at most one dimension.  This is different than CAN where only dirrect
   neighbors have links.  A DHT based on this type of a network should only
   require d hops to handle a request.  The cost is of coarse a higher
   neihbor count.
   We will define x as the expected population or a cell and N as the total
   number of nodes.
   x = N/(gd)
   We will define m as the expected neighbor count.
   m = x(d*(g-1)+1)
   So let us visualize a small network with g=8 and d=2 which looks like a
   chess board.  Let N=256 so x=4.  Messages can move like rooks on this
   chess board, so requests can be handled in 2 hops.  So we can caculate the
   neihbor count as:
   m = 4*(15) = 60
   --todo add pretty picture
   Now let us consider a larger network g=10 d=6 and x=4, so N=4,000,000.
   Messages should be routable in 6 hops and for the niehbor count we get:
   m = 4*(6*9+1) = 220
   This still would be acceptable for freenet like applications.
   When x and g are held constant m is O(log(N)) just like CAN.
   For large N the probability a cell is vacant is e-x.  So at x=4 this
   chance is about 1.83%.  This should be acceptable if some redundancy is
   used.  Not that when a message has serveral dimensions to move along, the
   chance that there is no dirrect way becomes very low.
Node Placement
   Nodes must be placed randomly into the network to prevent, serveral types
   of attacks which involve an adversary selecting the locations of the grid
   he would like to control.  In order to do this a node is forced to pay for
   it's identity with a large hash cash calculation.  The hash cash then
   determines the node's location in the grid.
   One space optimization, is to store all the identification in the public
   key.  The first time user picks an N=pq.  He then tries to make an e such
   that it's first 32 bits contain the julian date, it is relatively prime to
   (p-1)(q-1), and SHA-1() has the first h bits zero.  We can then set h
   to be so high it takes a 1CPU-day to calculate the key.
   We then use some globaly defined salt per dimension Si, which we encript
   with the  to give us the identities location L in the grid.
   Li = (Si emod N) mod g
Boot Strapping
   Boot strapping is pretty straight forward.  A new node must have contact
   with one node in the system.  It uses this node to route a message to the
   target cell, a node there sends its IP:Port encripted with the public key
   of the identity back.  The node the decrypts this to get the IP out.  It
   never has dirrect contact to any of the other nodes along the routing
   path.  Once the new node connects to the node in its home cell, it can get
   all the other IPs and keys from it.
   It could be the case that the cell is vacant, or that one doesn't want to
   trust the single node from the home cell.  It is easy for such a boot
   strap message to be routed to find a node from a cell connected to the
   target cell.  The same protocol can be used.  So one might have to repeat
   the process d times to get nodes from all the dimensions.
   Note that this type of bootstrapping will probably only have to happen
   once for a node.  Once a strip of g*x nodes all know each other, some of
   them can leave and join with out having to worry too much about not being
   able to find anyone when they come back.
DHT Functionality
   Similar to the way in which a node must be kept from deciding what cell it
   wants to serve in, we must ensure that the cells to which keys for
   requests and inserts are mapped can not be selected by an advesary.  Based
   on the application key of which there may be many types a routing key is
   calculated based on a small hash cash proportional to the some
   requirements of the key like:
     * size of the data
     * priority of messages
     * possiblity and maximum rate of rewrite
     * expiration rate of the data
     * expiration date of the key
   The hash cash which is calculated will then be given back to the
   application to use for the a pointer.  The actual routing key is the hash
   of the hash cash and other key data that went into the hash cash
   calculation.  This ensures that a hard hash cash calculation must be done
   before determining what node(s) will be responsible for handling the key.
   For example you want to make a pretty picture freesite.  For your picture
   you know it's not going to change or at least nobody is going to link
   dirrectly to the picture, so you calculate a cheap on time hash cash for
   it, which is included in your HTML.  For your HTML you decide you want to
   be able to change it once a day so you buy a more expensive hash cash for
   it.  This is given to others who want to see or link to your site.
   Requests with substandard hash cash don't have to be dropped they can just
   be treated with lower priority.
   File sharing data should ooz slowly through a network, while browsing
   requests should skip over them.
Premix Routing
   Tarzan provides a very good model for premix routing.  One problem is that
   they rely on IP subnets as a resource to limit adversaries and they
   require all nodes to contact all other nodes periodically in order to
   verify identities.  The Grid solves this problem.
   Since an advesary can only create a certain amount of valid identities and
   those identities are spread around evenly, there is no way for an
   advesarial node to present a large list of faulty neighbors.  If the
   advesary provides the tunnel user with a very small list this would be
   obvious too.
   If that weren't enough, we could demand certificates from multiple nodes
   in a cell to verify neighbor lists.
   It should also be noted that the DHT traffic can provide perfect cover
   traffic.
Resource Accounting
   Since the relationships between nodes are long term and identity isn't
   free, barder is a great economic model.  Over the long term two neihbor
   nodes can add up how much work they did for each other, and do some simple
   statistics to see if anyone is being a "leach".  No centralized authority
   is required.
   Premix bandwidth is always charged back the dirrection the tunnel
   originates from.
Topology Aware Routing
   Since routing can go in multiple dimensions and mutliple nodes inhabit the
   same cells, there is great flexibility in which way messages can be
   routed.  Messages can be forwarded along the path with the lowest latency
   and to nodes with low load.  This is similare to how you navigate American
   streets with a car avoiding the streets and avenues with too much traffic.
Attack Models
  Cancer Node Censorship Attack
   In this model an adversary runs a few nodes where particular requests are
   likely to be handled in order to drop or mishandle them.
   The Grid prevents this by randomly distributing expensive identities
   throughout.  In that large 4,000,000 node network only 1 in a million
   identities would be in the target cell.  If each identity cost 1 CPU-day,
   an adversary would have to buy lots of hardware and work really long.
  Spy and Flood
   There is a slightly better chance of getting a node to wind up connected
   to the target cell: 54/1,000,000.  From here the advesary could lauch
   classic SYN floods or try to hack nodes in the target cell, since he would
   know thier IPs.
   This still is a prohibitive amount of  work. A little bit of redundacy
   will also dramatically increase resistance further.
  DHT Flood
   In this attack model the advesary tries to generate messages, which all
   must be handeled by nodes in the target area.  The hash cash should
   prevent this from happening for most types of keys.  Since you have to pay
   first and then you know the route.  Messages for things with more
   expensive hash cash could be processed first, to avoid shortcoming of
   cheaper key types.
   boconstrictor attack --naa
Other Wacky Ideas
   Clusters
   Big Clusters
   Using levels to deal with heterogenity
   Baby Mode - Slower bootstrap
   setstats 1
This SF.net email is sponsored by: SF.net Giveback Program.
Does SourceForge.net help you be more productive?  Does it
help you create better code?  SHARE THE LOVE, and help us help
YOU!  Click Here: mnet-devel mailing list
mnet-devel at lists.sourceforge.net

@_date: 2003-09-28 19:12:19
@_author: Zooko 
@_subject: [mnet-devel] progress implementing emergent networks 
Myers reported (I think) that his twisted Chord network passes a unit test
where you start with two separate Chord networks and introduce one node from
the first net to one node from the other, and then the two nets merge.
Meanwhile, I've finished implementing a version of ent (based on Kademlia)
which keeps only one node per k-bucket, and fixed several bugs, but there
remains some bug that I haven't investigated (I'm out of time) which causes it
to fail the basic "contruct a network, publish a block, fetch the block" unit
test.  (Please, someone fix it, as I'm probably busy this week.)
I just wanted to comment that there is no way known (to me at least) for
Kademlia to pass the unit test that Myers is using on his Chord net -- merging
two separate nets into one.  Kademlia can't do that AFAIK.  (This is one way
of observing the "Kademlia doesn't self-heal" problem.)
I also wanted to mention that Chord can sometimes fail, too, if the nets
happen to line up so that the resulting merged Chord net is "loopy".  The
Liben-Nowell paper [1] explains how to fix that.
[1] This sf.net email is sponsored by:ThinkGeek
Welcome to geek heaven.
mnet-devel mailing list
mnet-devel at lists.sourceforge.net
[demime 0.97c removed an attachment of type application/pgp-signature]

@_date: 2004-09-10 12:55:04
@_author: Zooko O'Whielcronx 
@_subject: potential new IETF WG on anonymous IPSec 
References: <20040909195729.4798957E2B
I believe that in the context of e-mail [1, 2, 3, 4] and FreeSWAN this is called "opportunistic encryption".
[1] [2] [3] [4]

@_date: 2005-12-02 11:45:57
@_author: zooko at zooko.com 
@_subject: [p2p-hackers] darknet ~= (blacknet, f2f net) 
Ian, p2p-hackers:
It's not my goal to quibble about etymology (except inasmuch as it is useful
preserve the historical record).  My goals are:
1.  Avoid ambiguity -- where some people think that word X denotes concept 1,
    and others think that word X denotes concept 2.  Especially if concepts 1
    and 2 are related but not identical.  Especially if one of them is
    politically incendiary.
2.  Make sure we have names for our useful concepts.
However, before I get to that I am going to go through the history one last
time in order to cast light on the current problem.  I turned up some
interesting details.
Let's start with a Venn diagram:
         _______      _______
        /       \    /       \
       /         \  /         \
      /           \/           \
     /            /\            \
    /            /  \            \
            |    |            |
        1   |1^2 |   2        |
            |    |            |
            |    |            |
    \            \  /            /
     \            \/            /
      \           /\           /
       \         /  \         /
        \_______/    \_______/
Let 1 be the set of networks which are used for illegal transmission of
information, and 2 be the set of networks which are built on f2f connections,
and 1^2 be the intersection -- the set of networks which are used for illegal
transmission of information and which are built on f2f connections.
[bepw2002] introduces "darknet" to mean concept 1.  In their words darknet is
"a collection of networks and technologies used to share digital content",
they use it consistently within that meaning.  They refer to concept 2,
starting in section 2.1, using the term "small-world nets", and they clearly
distinguish between what they call "small-world darknets" and
However nowadays some people in the mass media seem to think that a "darknet"
means primarily a network which is "invitation-only", i.e. a "small-world" or
"f2f" net [globe].  When did the meaning shift?
Ooh -- how interesting to examine the evolution of this word on [wikipedia]!
The original definition on wikipedia was written on 2004-09-30.  It read in
full: "Darknet is a broad term to denote the networks and technologies that
enable users to copy and share digital material.  The term was coined in a
paper from four Microsoft Research authors.".
The next change was that two months later someone redirected the "Darknet"
to just be a link to the "Filesharing page", with the comment "Just another
word for filesharing".
The next change was that on 2005-04-14 someone from IP 81.178.83.245 wrote a
definition beginning with this sentence: "A Darknet is a private file sharing
network where users only connect to people they trust.".
By the way, I should point out that I have a personal interest in this
because between 2001 and 2003 I tried to promulgate concept 2, using Lucas
Gonze's coinage: "friendnet" [zooko2001, zooko2002, zooko2003, gonze2002].
I would like to know for my own satisfaction if my ideas were a direct
inspiration for some of this modern stuff, such as the Freenet v0.7 design.
So much for etymology.
Now the problem is that in the current parlance of the media, the word
"darknet" is used to mean vaguely 1 or 2 or 1^2.  The reason that this is a
problem isn't that it breaks with some etymological tradition, but that it is
ambiguous and that it deprives us of useful words to refer to 1 or 2
specifically.  The ambiguity has nasty political consequences -- see for
example these f2f network operators struggling to persuade newspaper readers
that they are not primarily for illegal purposes: [globe].
My proposal to rectify the lack-of-words problem is to use "blacknet" to
to 1 specifically and "f2f net" to refer to 2 specifically.  I don't know if
there is any way to rectify the ambiguity problem.
So you think of "darknet" as meaning 1^2.
That's an interesting remark -- that you regard concealment as one of the
motivations.  I personally regard concealment as one of the lesser

@_date: 2005-12-08 10:48:25
@_author: zooko at zooko.com 
@_subject: [p2p-hackers] darknet ~= (blacknet, f2f net) 
The concept of a networking technology or a network which is specifically
for illegal information is an interesting concept, for example Tim May
"blacknet" [1, 2, 3] and Biddle, et al. "darknet" [4].  If you would like to
use "darknet" to mean something else then I can't stop you, but I would like
talk about that concept so I need a word for it.
P.S.  The most salient difference between blacknet [1] and darknet [2] in my
opinion is that blacknet is a market, in which participants are motivated by
economic gain, and darknet is a more general concept, in which the
of participants may be various -- including but not limited to friendship.
[1] [2] [3] [4] p2p-hackers mailing list
p2p-hackers at zgp.org
Here is a web page listing P2P Conferences:
Eugen* Leitl leitl ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2005-12-08 11:08:39
@_author: zooko at zooko.com 
@_subject: [p2p-hackers] f2f for purposes other than privacy 
I would love to learn more.  Is there a white-paper or design document beyond
these slides from DefCon [1]?
Well, I think of the links between two friends in f2f to be not solely
communication channels but also to have other meaning.  For example, if
transmit music files to one another, then in addition to any privacy
that the network may have, it also serves as a decentralized,
recommendation engine for music.
Honestly, this area of research is ripe for exploration, but I can give you
least a couple of examples.  Doceur set it up with a claimed general negative
result in "The Sybil Attack" in 2002 [2].  But his general negative result
isn't quite true, as disproven by e.g. Advogato, 2000 [3, 4, 5].  Recently
George Danezis, Chris Lesniewski-Laas, M. Frans Kaashoek, and Ross Anderson
smashed these two ideas together and mixed in some DHT routing: [6].
[6] is an excellent paper, which proposes a concrete DHT design and which
really nails the fact that the introduction graph or "bootstrap graph"
information which can defeat the allegedly undefeatable Sybil Attack.  [6]
references some related work which looks interesting, but I haven't followed
those links yet myself.  I guess [6] is somewhat relevant to the Freenet v0.7
So, uh, anyway, this shows that there is interest in the notion of using
friendship networks for purposes other than privacy, namely attack resistance
of DHT routing and attack resistance of metadata [7 (self-citation)].
I think there's a lot more value to be mined from this concept, and I'm
glad that it has finally gotten the attention of some p2p researchers.
Oh, and here's another perspective on this idea -- a post I wrote to my blog
few years ago suggesting that all sorts of DHT innovations which were
to improve network performance could be applied to attack resistance: "trust
just another topology" [8].
[1] [2] [3] [4] [5] [7] p2p-hackers mailing list
p2p-hackers at zgp.org
Here is a web page listing P2P Conferences:
Eugen* Leitl leitl ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2005-03-08 17:04:46
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] good-bye, Mnet, 
contents of this message: "good-bye, Mnet", "The Fully Connected
Topology", "Rate-Limited Structured Flood", "A Nice Slow Network", "Did
you say 'BROADCAST SEARCH'?" "Persistent Tit-for-Tat == Bilateral
Dear mnet-devel and p2p-hackers:
I am about to accept an exciting job that will preclude me from
contributing to open source projects in the distributed file-system
I will miss the Mnet project!  Good luck without me!
I'm writing the following as a record of the most advanced design that
I have thought of for Mnet.  (See Acknowledgements section below.)
Most or all of the design written below has previously been published
in different web pages, e-mail messages, and IRC transcripts, and a
brief presentation I made at Privacy Enhancing Technologies Workshop
The design described below is almost but not quite what is currently
implemented, by myself and others, in Mnet v0.7, available at [1].
 *** Design of Mnet v0.7+:
I.  Network connectivity -- the Fully Connected Topology
I.A.  Local peer database.  Each node remembers the nodeId of for each
node that it has ever heard of or received a message from.  There is a
maximum number of nodeIds, in deference to memory and computation costs
(your local memory and your local computation).  I don't know what that
maximum number should be.  If you have more than the maximum number of
nodeIds in your database, you can flush some of the
least-recently-alive ones.
I.B.  Exponential Backoff.  With every peer in your db is stored a
"deadness level".  When the deadness level is equal to 0, that means
that the most recent thing that happened is that you receive a message
from that peer -- whether it was a reponse to a request of yours or if
it were a request from him to you.  We say that the peers with deadness
level 0 are "the live peers".  If a peer has deadness level 1, then
that means that the most recent time that you sent a request to that
peer, he didn't write back.  Now, whenever you want to choose from a
set of peers in order to send a request to one of them, the set you
choose from is all of the deadness level 0 peers, plus with 50%
probability all of the deadness level 1 peers, plus with 25%
probability all of the deadness level 2 peers, and so on.  Deadness
level 2 means that the peer was in deadness level 1, and you chose to
query him (with 50% probability), and he didn't write back again.
I.C.  Lookup of Peer Contact Info.  When you want to find the current
contact info (i.e. current IP address+port number, or current Relay
Server) for a peer, you send a query to a certain number of other peers
(called "MetaTrackers" when they are serving this purpose) -- a "lookup
contact info" query.  How many peers?  Approximately log(N) where N is
the number of peers in your local peer database.  Which peers?  You use
the Chord distribution -- you query the peer closest to your target,
the one closest to the point halfway around the circle from your
target, the one closest to the point a quarter of the way around the
circle, and cetera.
Obviously, you need to publish your current contact info to *your*
MetaTrackers whenever you join the network or whenever your contact
info changes.  Your MetaTrackers are the peer in your db which is
closest to you, the peer in your db which is closest to the point
halfway around the circle, etc.
I.D.  Discovery of New Peers.  Rate-Limited Structured Flood.  Every 60
minutes, you send an update to each of your MetaTrackers.  That update
contains the list of the peerIds of every new peer.  A "new peer" for
this purpose is defined as follows: you've never before announced this
peer to MetaTrackers, and this peer currently has deadness level 0 in
your local db.  If the complete (compressed) message containing the
information about the new peers would exceed 256 KB, then select a
random subset of the new peers to announce in this announcement so that
the message doesn't exceed 256 KB.  This is a rate-limited structured
flood.  The flooding nature of it means that eventually you will find
out about the arrival of every new peer.  The structured nature of it
means that it will take only log(N) time intervals before you find out,
and you will receive only (;-)) log(N) separate notifications of the
arrival of a new peer.  (And you will send log(N) separate
announcements of new peers -- one announcement to each of your
MetaTrackers.)  The rate-limited nature of it means that if new peers
are arriving so fast  that these notifications would take more than
log(N)*256 KB per hour bandwidth, that instead they take up only
log(N)*256 KB per hour and it takes longer for you to find out about
the arrival of every new peer.
I.E.  Relay.  You choose some peer which you can make a TCP connection
to and appoint it to be your RelayServer.  How you choose it, and
dynamically update your choice, is complicated -- see the
implementation in RelayClient.py.  Whenever someone wants to send a
message to you and they find it impossible to open a TCP connection to
you (which is all the time when you are behind a NAT or firewall that
prevents incoming TCP connections) then they send the message to your
RelayServer instead.  See also [2, 3, 4].
II.  Filesystem.
II.A.  Encoding of a file.  This is already described fully and
succinctly in [5].  Here is a capsule summary:  1.  Erasure code the
file, 2.  Encrypt the blocks, 3.  Put the list of the Ids of the
encrypted blocks into a new file named the inode, 4.  Encrypt the
inode, 5.  The Id of the inode, combined with the secret key used for
encryption are the mnetURI of the file.
II.B.  Push each block to the BlockServer which has an nodeId closest
to the blockId (in the Chord metric).
II.C.  In order to download a file, given its mnetURI, you first have
to download the inode, and then you have to download a sufficient
number of the erasure coded blocks.  For each block that you want to
download, you query the BlockServer whose Id is closest to that block's
Id.  If he doesn't have it, then you ask the BlockServer whose Id is
the next-closest.  Etc.  If the file is not actually reconstructable at
all because there are not enough blocks present at all on the network,
then this search algorithm devolves to a broadcast search.
This concludes the basic description of the design of Mnet v0.7+.
There are other aspects of the design that are not included here,
including a metadata search facility invented by Myers Carpenter.  In
addition, there are many specifics or added features in the core
network+filestore that are excluded from this description for brevity.
 *** Discussion:
III.  A Nice Slow Network.  The deliberate pace of announcement of new
peers is a feature, not a bug.  For starters it is convenient to
implement, because for various reasons it is easier to efficiently
manage 256 KB every hour than 72 bytes every second even though they
amount to the same bandwidth over the long term.
However, beyond it being convenient, it is also useful for
discriminating among our peers, because we don't want to start using
new peers until they've been around for a while anyway.  Fresh peers
are more likely to disappear than older ones.  In earlier versions of
this system, we heard about new peers more quickly, and then we had to
add logic to avoid using them until time had passed.
However, beyond it being convenient and useful, it is also desirable to
me, because I wanted Mnet to be more amenable to deliberate and
long-term use than to urgent and impulsive use.  Brad Templeton argued
to me in the year 2002 that if a distributed filesystem were primarily
for acquiring novelty videos, pop songs, and porn, then instant
gratification would be important, but if it were primarily for
acquiring classic works of literature, political and historical
documents, or backup copies of your own data, then instant
gratification isn't so important.  His argument made an impression on
IV.  Scalability Issues and other problems.
IV.A.  Size of the local Peer DB.  One limitation on scale is the size
of the local peer db.  For a modernish desktop machine, the local peer
db could probably be on the order of 2^20 entries with no noticeable
problem.  The limit could probably be made much higher by optimizing
the db.  The current db is trivial -- it consists of a Python dict in
memory which gets serialized by Python pickle and then written to a
file every hour or so.
IV.A.i.  In practice, almost all peers which fall into high deadness
levels will never revive.  If you purge a peer from your local db and
then that peer does revive and reconnect to the network, and if he
sends you a request, then you will re-add him to your db just as if he
were a brand new peer.  So if your local peer db is too big, you could
purge peers, starting with the most dead ones.
IV.B.  Announcement of New Peers.  Each newly arrived peer will be
announced to each current peer.  If the rate of arrivals of new peers
is sufficiently small, then it will take log(N) time intervals for all
peers to learn about the new peer.  My estimate is that "sufficiently
small" is about 2000 new peers per hour, with the current
implementation.  It could doubled or quadrupled by better compression
of the announcement messages.  If the rate of arrival of new peers
exceeds this, then the time before all peers have heard about the new
peer increases proportionally to the rate.
IV.C.  Block store churn.  There is currently no way for a block server
to indicate that its store of blocks is full and that it refuses to
accept new blocks being pushed into it, so you should give them to
someone else.  Absent this "back pressure", the stores of nodes with
small storage capacity are likely to get flushed out by new
publications even while the stores of nodes with large  capacity are
underutilized.  This inefficient distribution reduces the half-life of
files by some unknown constant.
IV.D.  Did you say "BROADCAST SEARCH"?  Well, consider what would
happen if you stopped searching for a block after your first query.
Then we would similar likelihood of finding a block as in e.g. the
Chord File System, with minimal message complexity -- a single query.
The problem is that maybe the block isn't on that node but is on a
nearby node (either because that node joined the network after the
block was published, or because the publisher of the block wasn't yet
aware of that node when he published the block).  This problem can be
solved in one of three ways:
1.  When a new node arrives, it requests copies of all of the relevant
blocks from the node(s) that it shadows.  This is a bad idea for this
2.  As nodes arrive and leave, they keep track of which other nodes
they shadow, then they forward requests which might be satisfied by
other nodes.  Interesting possibility, but I couldn't figure out how to
do it in a way that wouldn't be very complicated and still end up
falling in the worst case to the third approach:
3.  When a downloader doesn't find the desired block on the server
closest to the blockId, it chooses whether to broaden the search and
query other servers that are further from the blockId.
The interesting thing about 3 is that the decision about how broad to
make the search is made by the agent who has the most information about
its importance (such as whether other erasure coded blocks have been
found that can replace the missing block, or whether the user considers
downloading this file important enough to impose additional burden on
the network).  This same agent that has the most information is also
the one that has the incentive to want the download to succeed, so it
is this agent who should choose whether or not to impose the additional
burden on the network of a broader search.  See also section V --
"Persistent Tit-for-Tat == Bilateral Accounting".
V.  Persistent Tit-for-Tat == Bilateral Accounting.  Mnet v0.7+ lacks
something -- an incentive mechanism to limit excessive costs imposed on
the network and simultaneously to motivate users to contribute
resources to the network.  I was always deeply impressed with the
simplicity and robustness of Bram Cohen's "Tit-for-tat" incentive
mechanism for BitTorrent.  Suppose we wanted a similar "tit-for-tat"
mechanism for Mnet v0.7++, but we wanted the peer relationships to
extend through time and across multiple files and multiple user
operations.  Then we would probably invent a bilateral accounting
scheme for each node to keep track of how much goodness each of its
peers has done for it, and to reward helpful peers.  This would then
turn out to be more or less identical to the "bilateral accounting"
scheme that was originally invented by Jim McCoy and Doug Barnes in
Mojo Nation [footnote *].
 *** Acknowledgements
I know that I am doomed before I start to accidentally exclude
important and deserving people from this section.  Sorry.  This is in
roughly chronological order of their earliest contributions to this
design, as far as I can remember.
Obviously this design owes a great debt to the original designers of
its direct ancestor Mojo Nation: Jim McCoy and Doug Barnes, as well as
to the Evil Geniuses -- especially Greg Smith, Bram Cohen, and Drue
Lowenstern.  Also: Raph Levien, Sergei Osokine, the Freenet folks --
Ian Clarke, Oskar Sandberg, and Adam Langley among others -- Justin
Chapweske of Swarmcast, Brandon Wiley, Martin Peck, the Chord folks,
the Pastry folks, the CAN ("Content-Addressable Network") folks, the
OceanStore folks, The Mnet Hackers [7] -- Hauke Johannknecht, Jukka
Santala, Myers Carpenter, Oscar Haegar, Arno Waschk, Luke Nelson --
Mark S. Miller, Bram Cohen again (and Drue Lowenstern again) with
BitTorrent, the Kademlia folks, numerous contributors to the
p2p-hackers mailing list and the  IRC channel.  Like I said

@_date: 2005-11-29 10:03:13
@_author: zooko at zooko.com 
@_subject: [p2p-hackers] darknet ~= (blacknet, f2f net) 
It's a shame that the distinct concepts of "friend-to-friend net" [1] and
"blacknet" [2, 3, 4, 5] are being munged together in the media under the
The word "darknet" was coined, as far as I know, by Biddle, England, Peinado,
Willman [6].  Last time I read their paper, it appeared to me to describe a
system like Tim May's Blacknet -- an anonymous, secure, decentralized network
which is used to transfer information illegally.  It didn't mention anything
about using friend-to-friend techniques to build such a network.
However, the media seems to have started using the word "Darknet" to mean a
friend-to-friend net and/or a blacknet [7, 8], thus simultaneously making it
harder for people to think about blacknets which are based on other than
friend-to-friend architectures and making it harder for people to think about
friend-to-friend networks which are used for other than illegal information
I place some of the blame for this development on the Freenet folks, who may
the first to promulgate this munging, and if they aren't the first they're
certainly the most effective.
Of course, courting controversy in the mass media is part of the Freenet
strategy, and I'm not saying it's a bad strategy.
But oh well.  It is too late to change media usage, and it isn't a good idea
maintain technical jargon which is related to but subtly different from media
terminology, so how about us technical folks, when we wish to denote a
network-used-for-illegal-information-trading, use the original term
and when we wish to denote a network-built-on-friend-to-friend, use
"friend-to-friend net" or "f2f", and when we wish to refer to both of them
together or to confuse visiting reporters, we use "darknet".
[1] [2] [3] [4] [5] [6] [7] [8] p2p-hackers mailing list
p2p-hackers at zgp.org
Here is a web page listing P2P Conferences:
Eugen* Leitl leitl
ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2008-07-16 10:42:23
@_author: zooko 
@_subject: how bad is IPETEE? 
Oh, then they should learn about Adam Langley's Obfuscated TCP:
One of the design constraints for Obfuscated TCP was that an  Obfuscated TCP connection is required to take zero more round trips  to set up and use than a normal TCP connection.  Way to go, Adam!

@_date: 2008-11-07 05:10:31
@_author: zooko 
@_subject: ADMIN: no money politics, please 
Hey folks: you are welcome to discuss money politics over at the p2p- hackers mailing list:
I'm extremely interested in the subject myself, having taken part in  two notable failed attempts to deploy Chaumian digital cash and  currently being involved in a project that might lead to a third   -- Tahoe, the Least-Authority Filesystem
 -- back up all your files for $10/month

@_date: 2009-04-14 10:46:50
@_author: zooko 
@_subject: [tahoe-dev] ANNOUNCING Tahoe-LAFS v1.4 
ANNOUNCING Tahoe, the Least-Authority Filesystem, v1.4
The allmydata.org team is pleased to announce the release of version
1.4.1 of "Tahoe", the Lightweight-Authorization Filesystem. This is the
first release of Tahoe-LAFS which was created solely as a labor of love
by volunteers -- it is no longer funded by allmydata.com (see [1] for
Tahoe-LAFS is a secure, decentralized, fault-tolerant cloud storage
system.  All of the source code is publicly available under Free
Software, Open Source licences.
This filesystem is distributed over multiple servers in such a way the
filesystem continues to operate correctly even when some of the servers
are unavailable, malfunctioning, or malicious. Here is the one-page
explanation of Tahoe's unique security and fault-tolerance properties:
This is the successor to Tahoe-LAFS v1.3, which was released February
13, 2009 [2].  This is a major new release, adding garbage collection,
improved diagnostics and error-reporting, and fixing a critical
performance problem when downloading large (many GB) files.
See the NEWS file [3] and the known_issues.txt file [4] for more
Besides the Tahoe core, a crop of related projects have sprung up,
including frontends for Windows and Macintosh, two front-ends written in
JavaScript, a Ruby interface, a plugin for duplicity, a plugin for
TiddlyWiki, a new backup tool named "GridBackup", CIFS/SMB integration,
an iPhone app, and three incomplete frontends for FUSE. See the Related
Projects page on the wiki: [5].
Tahoe v1.4 is fully compatible with the version 1 series of Tahoe. Files
written by v1.4 clients can be read by clients of all versions back to
v1.0. v1.4 clients can read files produced by clients of all versions  v1.0.  v1.4 servers can serve clients of all versions back to v1.0  and v1.4
clients can use servers of all versions back to v1.0.
This is the fifth release in the version 1 series. The version 1 series
of Tahoe will be actively supported and maintained for the forseeable
future, and future versions of Tahoe will retain the ability to read
files and directories produced by Tahoe v1 for the forseeable future.
The version 1 branch of Tahoe is the basis of the consumer backup
product from Allmydata, Inc. --  .
WHAT IS IT GOOD FOR?
With Tahoe, you can distribute your filesystem across a set of servers,
such that if some of them fail or even turn out to be malicious, the
entire filesystem continues to be available. You can share your files
with other users, using a simple and flexible access control scheme.
Because this software is new, we do not categorically recommend it as
the sole repository of data which is extremely confidential or
precious.  However, we believe that erasure coding, strong encryption,
Free/Open Source Software and careful engineering make Tahoe safer than
common alternatives, such as RAID, removable drive, tape, "on-line
storage" or "Cloud storage" systems.
This software comes with extensive tests, and there are no known
security flaws which would compromise confidentiality or data integrity.
(For all currently known issues please see the known_issues.txt file
This release of Tahoe is suitable for the "friendnet" use case [6] --
it is easy to create a filesystem spread over the computers of you and
your friends so that you can share disk space and files.
You may use this package under the GNU General Public License, version
2 or, at your option, any later version.  See the file "COPYING.GPL"
[7] for the terms of the GNU General Public License, version 2.
You may use this package under the Transitive Grace Period Public
Licence, version 1 or, at your option, any later version.  (The
Transitive Grace Period Public Licence has requirements similar to the
GPL except that it allows you to wait for up to twelve months after you
redistribute a derived work before releasing the source code of your
derived work.) See the file "COPYING.TGPPL.html" [8] for the terms of
the Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either licence,
at your option.)
Tahoe works on Linux, Mac OS X, Windows, Cygwin, and Solaris, and
probably most other systems.  Start with "docs/install.html" [9].
HACKING AND COMMUNITY
Please join us on the mailing list [10].  Patches are gratefully
accepted -- the RoadMap page [11] shows the next improvements that we
plan to make and CREDITS [12] lists the names of people who've
contributed to the project.  The wiki Dev page [13] contains resources
for hackers.
Tahoe was originally developed thanks to the sponsorship of Allmydata,
Inc. [14], a provider of commercial backup services.  Allmydata,
Inc. created the Tahoe project, and contributed hardware, software,
ideas, bug reports, suggestions, demands, and money (employing several
Tahoe hackers and instructing them to spend part of their work time on
this Free Software project).  Also they awarded customized t-shirts to
hackers who find security flaws in Tahoe (see ). After discontinuing funding of Tahoe R&D in early 2009, Allmydata,
Inc. has continued to provide servers, co-lo space and bandwidth to the
open source project. Thank you to Allmydata, Inc. for their generous and
public-spirited support.
Zooko Wilcox-O'Hearn
on behalf of the allmydata.org team
Special acknowledgment goes to Brian Warner, whose superb engineering
skills and dedication are primarily responsible for the Tahoe
implementation, and significantly responsible for the Tahoe design as
well, not to mention most of the docs and tests and many other things
April 13, 2009
Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] Tahoe, the Least-Authority Filesystem -- store your data: $10/month -- I am available for work -- tahoe-dev mailing list
tahoe-dev at allmydata.org

@_date: 2009-04-14 12:46:51
@_author: zooko 
@_subject: No subject 
ANNOUNCING Tahoe, the Least-Authority Filesystem, v1.4
The allmydata.org team is pleased to announce the release of version
1.4.1 of "Tahoe", the Lightweight-Authorization Filesystem. This is the
first release of Tahoe-LAFS which was created solely as a labor of love
by volunteers -- it is no longer funded by allmydata.com (see [1] for
Tahoe-LAFS is a secure, decentralized, fault-tolerant cloud storage
system.  All of the source code is publicly available under Free
Software, Open Source licences.
This filesystem is distributed over multiple servers in such a way the
filesystem continues to operate correctly even when some of the servers
are unavailable, malfunctioning, or malicious. Here is the one-page
explanation of Tahoe's unique security and fault-tolerance properties:
This is the successor to Tahoe-LAFS v1.3, which was released February
13, 2009 [2].  This is a major new release, adding garbage collection,
improved diagnostics and error-reporting, and fixing a critical
performance problem when downloading large (many GB) files.
See the NEWS file [3] and the known_issues.txt file [4] for more
Besides the Tahoe core, a crop of related projects have sprung up,
including frontends for Windows and Macintosh, two front-ends written in
JavaScript, a Ruby interface, a plugin for duplicity, a plugin for
TiddlyWiki, a new backup tool named "GridBackup", CIFS/SMB integration,
an iPhone app, and three incomplete frontends for FUSE. See the Related
Projects page on the wiki: [5].
Tahoe v1.4 is fully compatible with the version 1 series of Tahoe. Files
written by v1.4 clients can be read by clients of all versions back to
v1.0. v1.4 clients can read files produced by clients of all versions
v1.0.  v1.4 servers can serve clients of all versions back to v1.0
and v1.4
clients can use servers of all versions back to v1.0.
This is the fifth release in the version 1 series. The version 1 series
of Tahoe will be actively supported and maintained for the forseeable
future, and future versions of Tahoe will retain the ability to read
files and directories produced by Tahoe v1 for the forseeable future.
The version 1 branch of Tahoe is the basis of the consumer backup
product from Allmydata, Inc. --  .
WHAT IS IT GOOD FOR?
With Tahoe, you can distribute your filesystem across a set of servers,
such that if some of them fail or even turn out to be malicious, the
entire filesystem continues to be available. You can share your files
with other users, using a simple and flexible access control scheme.
Because this software is new, we do not categorically recommend it as
the sole repository of data which is extremely confidential or
precious.  However, we believe that erasure coding, strong encryption,
Free/Open Source Software and careful engineering make Tahoe safer than
common alternatives, such as RAID, removable drive, tape, "on-line
storage" or "Cloud storage" systems.
This software comes with extensive tests, and there are no known
security flaws which would compromise confidentiality or data integrity.
(For all currently known issues please see the known_issues.txt file
This release of Tahoe is suitable for the "friendnet" use case [6] --
it is easy to create a filesystem spread over the computers of you and
your friends so that you can share disk space and files.
You may use this package under the GNU General Public License, version
2 or, at your option, any later version.  See the file "COPYING.GPL"
[7] for the terms of the GNU General Public License, version 2.
You may use this package under the Transitive Grace Period Public
Licence, version 1 or, at your option, any later version.  (The
Transitive Grace Period Public Licence has requirements similar to the
GPL except that it allows you to wait for up to twelve months after you
redistribute a derived work before releasing the source code of your
derived work.) See the file "COPYING.TGPPL.html" [8] for the terms of
the Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either licence,
at your option.)
Tahoe works on Linux, Mac OS X, Windows, Cygwin, and Solaris, and
probably most other systems.  Start with "docs/install.html" [9].
HACKING AND COMMUNITY
Please join us on the mailing list [10].  Patches are gratefully
accepted -- the RoadMap page [11] shows the next improvements that we
plan to make and CREDITS [12] lists the names of people who've
contributed to the project.  The wiki Dev page [13] contains resources
for hackers.
Tahoe was originally developed thanks to the sponsorship of Allmydata,
Inc. [14], a provider of commercial backup services.  Allmydata,
Inc. created the Tahoe project, and contributed hardware, software,
ideas, bug reports, suggestions, demands, and money (employing several
Tahoe hackers and instructing them to spend part of their work time on
this Free Software project).  Also they awarded customized t-shirts to
hackers who find security flaws in Tahoe (see ). After discontinuing funding of Tahoe R&D in early 2009, Allmydata,
Inc. has continued to provide servers, co-lo space and bandwidth to the
open source project. Thank you to Allmydata, Inc. for their generous and
public-spirited support.
Zooko Wilcox-O'Hearn
on behalf of the allmydata.org team
Special acknowledgment goes to Brian Warner, whose superb engineering
skills and dedication are primarily responsible for the Tahoe
implementation, and significantly responsible for the Tahoe design as
well, not to mention most of the docs and tests and many other things
April 13, 2009
Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] Tahoe, the Least-Authority Filesystem -- store your data: $10/month -- I am available for work -- p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2009-04-16 07:51:21
@_author: zooko 
@_subject: [tahoe-dev] CodeCon! (and can I borrow your laptop and give it back 
Hey folks:
If you are in San Francisco this weekend, come to CodeCon!
I'll be presenting Tahoe at 13:15 on Saturday, April 18.
By the way, I was originally planning to borrow an old Pentium 133  laptop from Brian, show it operating as one of the storage servers in  the live demo (live demos are a requirement of presentations at  CodeCon), and then smash it to pieces with an axe.
The demo would then proceed to show how the files can still be  accessed by using the remaining storage servers (which are also  laptops, sitting visible on stage with me).
However, Brian can't get his P133 laptop to boot because the CMOS  battery is dead so it can't find its hard drive at boot time.   Hopefully he can get it to boot from cd and we can install Tahoe on it.
If that doesn't work, does anyone else have a computer which is  powerful enough to run Tahoe, unloved enough to be sacrificed for a  greater cause, and flimsy enough to satisfyingly splinter under assault?
tahoe-dev mailing list
tahoe-dev at allmydata.org

@_date: 2009-04-22 12:14:45
@_author: Zooko O'Whielacronx 
@_subject: [tahoe-dev] questions about development priorities for Tahoe-LAFS, 
Here are just a few scattered notes about how I'm not sure who is  going to move Tahoe forward in which ways next.  Your opinions about  what should be done, and your offers to do them, would be gratefully  First of all, about me.  I'm interviewing for new jobs (if you were  considering offering to hire me, now is the time!).  As part of this  process I'm asking potential employers if they will give me a degree  of moral support to work on Tahoe in my "spare time".
I'm also encouraging Peter Secor in his efforts to find renewed  funding for Tahoe development, either on a commercial basis from  someone who intends to make money with this technology, or on a  philanthropic basis from someone who thinks that this technology  could improve people's lives.
It could turn out that within a week or two from writing this letter  I will take on some exciting new job which is not directly related to  Tahoe and my contributions to Tahoe will lessen dramatically.
Now about Brian: it looks like he is still planning to do lots of  hacking on Tahoe this summer, and he's an awesome hacker, so you can  expect top-quality work from him.
Kevin Reid is not going to work on a Tahoe GUI this summer after all

@_date: 2009-04-29 15:59:05
@_author: Zooko O'Whielacronx 
@_subject: [tahoe-dev] SHA-1 broken! (was: Request for hash-dependency in 
Wow!  These slides say that they discovered a way to find collisions  in SHA-1 at a cost of only 2^52 computations.  If this turns out to  be right (and the authors are respected cryptographers -- the kind of  people who really hate to be wrong about something like this) then it  is very exciting!  SHA-1 was already known to be vulnerable to attack  by a moderately well-funded organization such as a national security  agency, national military, corporation, or organized criminal group.   Now it turns out that finding SHA-1 collisions is in the reach of a  dedicated hobbyist or an eccentric genius [1].  Let's put a rough  number on it.  I might be a little bit off, but you can build a  COPACOBANA machine for about $10,000 [2], and it can brute-force a 56- bit DES key in about six and a half days.  2^52 SHA-1 operations  should take roughly the same amount of time and money.  As another  example I guess that distributed computation engines [3] and botnets  [4] might be able to generate a SHA-1 collision in seconds.
Plus of course the amplifying effects of birthday attacks and rainbow  tables and so on mean that the longer you keep your COPACOBANA or  your botnet generating SHA-1 collisions, the more SHA-1 users around  the world become vulnerable to you. So basically, if these slides are  right then relying on SHA-1 collision-resistance has been revealed as  a major vulnerability!
Almost all hash functions in civilian, open use are either MD5 or  SHA-1.  For example, decentralized revision control tools such as  monotone, git, and hg rely on SHA-1.  Interesting times!
As Shawn already correctly pointed out (and as Nathan probably  already knew), Tahoe doesn't use SHA-1, so we're not affected by this  new discovery.  Tahoe-LAFS uses SHA-256 (in the "double-hashing" mode  suggested by Ferguson and Schneier and named "SHA-256d").  We also  add our own tagging and salting prefix to avoid certain problems.  We  aren't currently vulnerable to hash collision attacks, and we plan  never to get into that position (about which more below).
Nonetheless, it would be a very good exercise to spell out what sorts  of problems could result if attackers could violate what sorts of  properties of the hash function(s) used in Tahoe.  The basic uses of  secure hashes in Tahoe are for integrity-checking of immutable files  and for digital signatures on mutable files and directories.
If an attacker could generate two different inputs which yielded the  same hash output (that is, to find a "hash collision"), then they  could give you a single immutable file cap that produced two (or  more) different files when you used it to download the file.  We  believe that nobody is currently able to do that, so currently if  someone gives you an immutable file cap, you can rely on there being  at most one file which can be downloaded using that cap.
For mutable files it is even safer.  If an attacker could find an  input which yielded the same output as someone *else*'s input (that  is, to find a "second pre-image"), then that attacker could write  changes to a mutable file or directory that they were not authorized  to write to.  Finding a second pre-image is probably much harder than  finding a collision -- for example nobody has yet figured out how to  find a second pre-image in SHA-1.  That's why I say it is even  safer.  You already assume that the person who can write to a mutable  file can make it so that two or more different file contents would be  downloaded from using the same mutable-file read cap, but for  immutable files we hold them to a higher standard and prevent even  the original uploader of the immutable file from being able to make  more than one file that matches the immutable-file read cap.
There are a lot more details of how Tahoe uses hash functions that I  would be happy to work out when I have time, but those are the most  important ones, and the immutable file caps are the most likely to  turn out to be vulnerable.  (Although, as I've said, even the  immutable file caps are extremely unlikely to be vulnerable.)
(Hm, this puts an interesting twist on Vincent and Nathan's idea of  layering Mercurial-or-Bazaar on top of Tahoe.  Tahoe uses stronger  cryptography (and also more flexible cryptography, by the way), so if  you have uploaded your Mercurial repository to Tahoe then even when  SHA-1 turns out to be weak (as it has), you can still rely on the  integrity of your repository.)
Sure -- a Merkle Tree has collision-resistance if the underlying hash  has collision-resistance, and a Merkle Tree has second-preimage- resistance if the underlying hash has second-preimage resistance.  So  if the underlying has doesn't have collision-resistance but does have  second-preimage-resistance (as we currently suspect that SHA-1  might), then your Merkle Tree would stil have second-preimage- resistance.  Also a Merkle Tree might be stronger than its underlying  hash function in a few ways, even if the underlying hash is somewhat  It is certainly possible to preserve all the data.
The obvious way is to download your files and re-upload them in the  new format.  I suspect that will probably end up being the best way,  too.  I would like to emphasize that it is extremely unlikely that  anyone will need to do this due to a weakness in the hashing  algorithm in Tahoe in a hurry.  The people who are suffering from the  collisions in MD5 and SHA-1 are suffering, not because MD5 or SHA-1  were suddenly revealed to be insecure, but because they ignored the  warning messages from cryptographers for many years.  (I'm a tad  irritated about this, since "I tried to tell them" [5] and "They  wouldn't listen!" [6].)
By the time that SHA-256 (plus our tagging and salting) is vulnerable  to collisions, which could be anywhere from five years to a hundred  years from now, we will have already upgraded Tahoe to use a stronger  hash function (SHA-3 or a SHA-3 candidate) and gracefully upgraded  pre-existing files.
Now the actual details of securely upgrading extant files to new  integrity check mechanisms could be interesting.  We've thought a bit  about how to facilitate future graceful upgrades and this will no  doubt prompt us to think about it some more.  The stickiest bits are  in the capability itself.  Let's put it this way: suppose you upload  a file to a Tahoe grid today and get an immutable read cap in  return.  Then suppose a few years from now someone does some  unspecified operation which adds stronger hashes to the file as it  exists out there on the servers.  Now, how do you as the holder of  the original immutable read cap know that those new stronger hashes  are correct?  You don't, because your read-cap wasn't generated from  those new stronger hashes.  This isn't a weakness in the Tahoe  capability-oriented design, it's more of a fundamental problem which  is just thrown into sharper light by the cap design.  You can, of  course, choose to delegate your decision about whether or not the  file is correct to someone else (using Tahoe as well as using any  other scheme), but if you want to actually have certainty *yourself*  that the file is correct using the new hashes, then you're going to  have to do some sort of download and computation on the file  yourself, using the new hash algorithm.
[1] [2] [3] [4] [5] [6] tahoe-dev mailing list
tahoe-dev at allmydata.org

@_date: 2009-08-01 21:19:43
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-announce] ANNOUNCING Tahoe, the Lofty-Atmospheric Filesystem, 
ANNOUNCING Tahoe, the Lofty-Atmospheric Filesystem, v1.5.
The Tahoe-LAFS team is pleased to announce the immediate availability of
version 1.5 of Tahoe, the Lofty Atmospheric File System.
Tahoe-LAFS is the first cloud storage technology which offers security
and privacy in the sense that the cloud storage service provider itself
can't read or alter your data. Here is the one-page explanation of
its unique security and fault-tolerance properties:
This release is the successor to v1.4.1, which was released April 13,
2009 [1]. This is a major new release, improving the user interface and
performance and fixing a few bugs, and adding ports to OpenBSD, NetBSD,
ArchLinux, NixOS, and embedded systems built on ARM CPUs. See the NEWS
file [2] for more information.
In addition to the functionality of Tahoe-LAFS itself, a crop of related
projects have sprung up to extend it and to integrate it into operating
systems and applications.  These include frontends for Windows,
Macintosh, JavaScript, and iPhone, and plugins for duplicity, bzr,
Hadoop, and TiddlyWiki, and more. See the Related Projects page on the
wiki [3].
Version 1.5 is fully compatible with the version 1 series of
Tahoe-LAFS. Files written by v1.5 clients can be read by clients of all
versions back to v1.0. v1.5 clients can read files produced by clients
of all versions since v1.0.  v1.5 servers can serve clients of all
versions back to v1.0 and v1.5 clients can use servers of all versions
back to v1.0.
This is the sixth release in the version 1 series. The version 1 series
of Tahoe-LAFS will be actively supported and maintained for the
forseeable future, and future versions of Tahoe-LAFS will retain the
ability to read and write files compatible with Tahoe-LAFS v1.
The version 1 series of Tahoe-LAFS is the basis of the consumer backup
product from Allmydata, Inc. --  .
WHAT IS IT GOOD FOR?
With Tahoe-LAFS, you can distribute your filesystem across a set of
servers, such that if some of them fail or even turn out to be
malicious, the entire filesystem continues to be available. You can
share your files with other users, using a simple and flexible access
control scheme.
We believe that the combination of erasure coding, strong encryption,
Free/Open Source Software and careful engineering make Tahoe-LAFS safer
than RAID, removable drive, tape, on-line backup or other Cloud storage
This software comes with extensive tests, and there are no known
security flaws which would compromise confidentiality or data integrity
in typical use.  (For all currently known issues please see the
known_issues.txt file [4].)
You may use this package under the GNU General Public License, version 2
or, at your option, any later version.  See the file "COPYING.GPL" [5]
for the terms of the GNU General Public License, version 2.
You may use this package under the Transitive Grace Period Public
Licence, version 1 or, at your option, any later version.  (The
Transitive Grace Period Public Licence has requirements similar to the
GPL except that it allows you to wait for up to twelve months after you
redistribute a derived work before releasing the source code of your
derived work.) See the file "COPYING.TGPPL.html" [6] for the terms of
the Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either licence,
at your option.)
Tahoe-LAFS works on Linux, Mac OS X, Windows, Cygwin, Solaris, *BSD, and
probably most other systems.  Start with "docs/install.html" [7].
HACKING AND COMMUNITY
Please join us on the mailing list [8].  Patches are gratefully accepted

@_date: 2009-08-05 09:28:59
@_author: Zooko Wilcox-O'Hearn 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
[cross-posted to tahoe-dev at allmydata.org and cryptography at metzdowd.com]
It doesn't look like I'm going to get time to write a long post about  this bundle of issues, comparing Cleversafe with Tahoe-LAFS (both use  erasure coding and encryption, and the encryption and key-management  part differs), and arguing against the ill-advised Fear, Uncertainty,  and Doubt that the Cleversafe folks have posted.  So, I'm going to  try to throw out a few short pieces which hopefully each make sense.
First, the most important issue in all of this is the one that my  programming partner Brian Warner already thoroughly addressed in [1]  (see also the reply by Jason Resch [2]).  That is the issue of access  control, which is intertwined with the issues of key management.  The  other issues are cryptographic details which are important to get  right, but the access control and key management issues are the ones  that directly impact every user and that make or break the security  and usefulness of the system.
Second, the Cleversafe documents seem to indicate that the security  of their system does not rely on encryption, but it does.  The data  in Cleversafe is encrypted with AES-256 before being erasure-coded  and each share stored on a different server (exactly the same as in  Tahoe-LAFS).  If AES-256 is crackable, then a storage server can  learn information about the file (exactly as in Tahoe-LAFS).  The  difference is that Cleversafe also stores the decryption key on the  storage servers, encoded in such a way that  any K of the storage  servers must cooperate to recover it.  In contrast, Tahoe-LAFS  manages the decryption key separately.  This added step of including  a secret-shared copy of the decryption key on the storage servers  does not make the data less vulnerable to weaknesses in AES-256, as  their documents claim.  (If anything, it makes it more vulnerable,  but probably it has no effect and it is just as vulnerable to  weaknesses in AES-256 as Tahoe-LAFS is.)
Third, I don't understand why Cleversafe documents claim that public  key cryptosystems whose security is based on "math" are more likely  to fall to future advances in cryptanalysis.  I think most  cryptographers have the opposite belief -- that encryption based on  bit-twiddling such as block ciphers or stream ciphers is much more  likely to fall to future cryptanalysis.  Certainly the history of  modern cryptography seems to fit with this -- of the original crop of  public key cryptosystems founded on a math problem, some are still  regarded as secure today (RSA, DH, McEliece), but there has been a  long succession of symmetric crypto primitives based on bit twiddling  which have then turned out to be insecure.  (Including, ominously  enough, AES-256, which was regarded as a gold standard until a few  months ago.)
Fourth, it seems like the same access control/key management model  that Cleversafe currently offers could be achieved by encrypting the  data with a random AES key and then using secret sharing to split the  key and store on share of the key with each server.  I *think* that  this would have the same cryptographic properties as the current  Cleversafe approach of using an All-Or-Nothing-Transform followed by  erasure coding.  Both would qualify as "computation secret sharing"  schemes as opposed to "information-theoretic secret sharing"  schemes.  I would be curious if there are any significant differences  between these two constructions.
I don't think there is any basis to the claims that Cleversafe makes  that their erasure-coding ("Information Dispersal")-based system is  fundamentally safer, e.g. these claims from [3]: "a malicious party  cannot recreate data from a slice, or two, or three, no matter what  the advances in processing power." ... "Maybe encryption alone is  'good enough' in some cases now  - but Dispersal is 'good always' and  represents the future."
Fifth, as I've already mentioned, the emphasis on cryptography being  defeated due to advances in processing power e.g. reference to  Moore's Law is confused.  Advances in processing power would not be  sufficient to crack modern cryptosystems and in many cases would not  be necessary either.
Okay I think that's it.  I hope these notes are not so terse as to be  confusing or inflammatory.
Zooko Wilcox-O'Hearn
[1] [2] [3]

@_date: 2009-08-09 14:48:20
@_author: Zooko Wilcox-O'Hearn 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
[dropped tahoe-dev from Cc:]
This is true for information-theoretically secure secret sharing, but  not true for Cleversafe's technique of composing an All-Or-Nothing- Transform with Reed-Solomon erasure coding.
Hey, let's be nice.  Cleversafe has implemented a storage system  which integrates encryption in the attempt to make it safer.  They  GPL at least some of their work [*], and they publish their ideas and  engage in discussion about them.  These are all good things.  My  remaining disagreements with them are like this:
1.  (The important one.)  I don't think the access control policy of  "whoever can access at least K of the N volumes of data" is the  access control policy that I want.  For one thing, it immediately  leads to the questions that James Hughes was asking, about who is  authorized to access what servers.  For another thing, I would really  like my access control policy to be fine-grained, flexible, and  dynamic.  So for example, I'd like to be able to give you access two  three of my files but not all my other files, and I'd like you to  then be able to give your friend access to two of those files but not  the third.  See Brian Warner's and Jason Resch's discussion of these  issues: [1, 2].
2.  Cleversafe seems to think that their scheme gives better-than- computational security, i.e. that it guarantees security even if  AES-256 is crackable.  This is wrong, but it is an easy mistake to  make!  Both Ben Laurie and James Hughes have jumped to the conclusion  (in this thread) that the Cleversafe K-out-of-N encoding has the same  information-theoretic security that secret-sharing K-out-of-N  encoding has.
3.  Cleversafe should really tone down the Fear Uncertainty and Doubt  about today's encryption being mincemeat for tomorrow's  cryptanalysts.  It might turn out to be true, but if so it will be  due to cryptanalytic innovations more than due to Moore's Law.  And  it might not turn out like that -- perhaps AES-256 will remain safe  for centuries.  Also, Cleversafe's product is not more secure than  any other product against this threat.
It is hard to explain to non-cryptographers how much they can rely on  the security of cryptographic schemes.  It's very complicated, and  most schemes deployed have failed due to flaws in the surrounding  system, engineering errors or key management (i.e. access control)  problems.  Nobody knows what cryptanalytic techniques will be  invented in the future.  My opinion is that relying on well- engineered strong encryption to protect your data is at least as safe  alternatives such as keeping the data on your home computer or on  your corporate server.  The Cleversafe FUD doesn't help people  understand the issues better.
[1] [2] [*] Somebody stated on a mailing list somewhere that Cleversafe has  applied for patents.  Therefore, if you want to use their work under  the terms of the GPL, you should also be aware that if their patents  are granted then some of what you do may be subject to the patents.   Of course, this is always true of any software (the techniques might  be patented), but I thought it was worth mentioning since in this  case the company authoring the software is also the company applying  for patents.

@_date: 2009-08-19 09:28:45
@_author: Zooko Wilcox-O'Hearn 
@_subject: Tahoe-LAFS key management, part 2: Tahoe-LAFS is like encrypted git 
Okay, in today's installment I'll reply to my friend Kris Nuttycombe,  who read yesterday's installment and then asked how the storage  service provider could provide access to the files without being able  to see their filehandles and thus decrypt them.
I replied that the handle could be stored in another file on the  server, and therefore encrypted so that the server couldn't see it.   You could imagine taking a bunch of these handles -- capabilities to  read an immutable file -- and putting them into a new file and  uploading to the Tahoe-LAFS grid.  Uploading it would encrypt it and  give you a capability to that new file.  The storage service provider  wouldn't be able to read the contents of that file, so it wouldn't be  able to read the files that it references.  This forms a  "Cryptographic Hash Function Directed Acyclic Graph" structure, which  should be familiar to many readers as the underlying structure in git  [*].  Git uses this same technique of combining identification and  integrity-checking into one handle.
the handle for encryption in addition to integrity-checking and  (There are many other differences.  For starters git has a high- performance compression scheme and it has a decentralized revision  control tool built on top.  Tahoe-LAFS has erasure-coding and a  distributed key-value store for a backend.)
Okay, the bus is arriving at work.
Oh, so then Kris asked "But what about the root of that tree?".  The  answer is that the capability to the root of that tree is not stored  on the servers.  It is held by the client, and never transmitted to  the storage servers.  It turns out that storage servers don't need  the capability to the file in order to serve up the ciphertext.   (Technically, they *do* need an identifier, and ideally they would  also have the integrity-checking part so that they could perform  integrity checks on the file contents (in addition to clients  performing that integrity check for themselves).  So the capability  gets split into its component parts during the download protocol,  when the client sends the identification and integrity-checking bits  to the server but not the decryption key, and receives the ciphertext  in reply.)
Therefore the next layer up, whether another program or a human user,  needs to manage this single capability to the root of a tree.  Here  the abstraction-piercing problem of availability versus  confidentiality remains in force, and different programs and  different human users have different ways to manage their caps.  I  personally keep mine in my bookmarks in my web browser.  This is  risky -- they could be stolen by malicious Javascript (probably) or I  might accidentally leak them in an HTTP Referer header.  But it is  very convenient.  For the files in question I value that convenience  more than an extra degree of safety.  I know of other people who keep  their Tahoe-LAFS caps more securely, on Unix filesystems, on  encrypted USB keys, etc..
[*] Linus Torvalds got the idea of a Cryptographic Hash Function  Directed Acyclic Graph structure from an earlier distributed revision  control tool named Monotone.  He didn't go out of his way to give  credit to Monotone, and many people mistakenly think that he invented  the idea.

@_date: 2009-02-13 18:37:14
@_author: zooko 
@_subject: [p2p-hackers] ANNOUNCING allmydata.org "Tahoe", 
I'm pretty excited about this relese of tahoe-lafs.  Please try it out.
ANNOUNCING allmydata.org "Tahoe", the Least-Authority Filesystem, v1.3
We are pleased to announce the release of version 1.3.0 of "Tahoe", the
Least Authority Filesystem.
Tahoe-LAFS is a secure, decentralized, fault-tolerant filesystem.  All
of the source code is available under a choice of two Free Software,
Open Source licences.
This filesystem is encrypted and distributed over multiple peers in
such a way it continues to function even when some of the peers are
unavailable, malfunctioning, or malicious.
Here is the one-page explanation of the security and fault-tolerance
properties that it offers:
This is the successor to v1.2, which was released July 21, 2008 [1].
This is a major new release, adding a repairer, an efficient backup
command, support for large files, an (S)FTP server, and much more.
See the NEWS file [2] and the known_issues.txt file [3] for more
In addition to the many new features of Tahoe itself, a crop of related
projects have sprung up, including Tahoe frontends for Windows and
Macintosh, two front-ends written in JavaScript, a Tahoe plugin for
duplicity, a Tahoe plugin for TiddlyWiki, a project to create a new
backup tool, CIFS/SMB integration, an iPhone app, and three incomplete
Tahoe frontends for FUSE. See Related Projects on the wiki: [4].
The version 1 branch of Tahoe is the basis of the consumer backup
product from Allmydata, Inc. --  .
Tahoe v1.3 is fully compatible with the version 1 branch of Tahoe.
Files written by v1.3 clients can be read by clients of all versions
back to v1.0 unless the file is too large -- files greater than about
12 GiB (depending on the configuration) can't be read by older clients.
v1.3 clients can read files produced by clients of all versions since
v1.0.  v1.3 servers can serve clients of all versions back to v1.0 and
v1.3 clients can use servers of all versions back to v1.0 (but can't
upload large files to them).
This is the fourth release in the version 1 series.  We believe that
this version of Tahoe is stable enough to rely on as a permanent store
of valuable data.  The version 1 branch of Tahoe will be actively
supported and maintained for the forseeable future, and future versions
of Tahoe will retain the ability to read files and directories produced
by Tahoe v1 for the forseeable future.
WHAT IS IT GOOD FOR?
With Tahoe, you can distribute your filesystem across a set of
computers, such that if some of the computers fail or turn out to be
malicious, the entire filesystem continues to be available, thanks to
the remaining computers.  You can also share your files with other
users, using a simple and flexible access control scheme.
Because this software is new, we do not categorically recommend it as
the sole repository of data which is extremely confidential or
precious.  However, we believe that erasure coding, strong encryption,
Free/Open Source Software and careful engineering make Tahoe safer than
common alternatives, such as RAID, removable drive, tape, or "on-line
storage" or "Cloud storage" systems.
This software comes with extensive unit tests [5], and there are no
known security flaws which would compromise confidentiality or data
integrity.  (For all currently known issues please see the
known_issues.txt file [2].)
This release of Tahoe is suitable for the "friendnet" use case [6] --
it is easy to create a filesystem spread over the computers of you and
your friends so that you can share disk space and files.
You may use this package under the GNU General Public License, version
2 or, at your option, any later version.  See the file "COPYING.GPL"
[7] for the terms of the GNU General Public License, version 2.
You may use this package under the Transitive Grace Period Public
Licence, version 1.0.  The Transitive Grace Period Public Licence has
requirements similar to the GPL except that it allows you to wait for
up to twelve months after you redistribute a derived work before
releasing the source code of your derived work. See the file
"COPYING.TGPPL.html" [8] for the terms of the Transitive Grace Period
Public Licence, version 1.0.
(You may choose to use this package under the terms of either licence,
at your option.)
Tahoe works on Linux, Mac OS X, Windows, Cygwin, and Solaris, and
probably most other systems.  Start with "docs/install.html" [9].
HACKING AND COMMUNITY
Please join us on the mailing list [10].  Patches that extend and
improve Tahoe are gratefully accepted -- the RoadMap page [11] shows
the next improvements that we plan to make and CREDITS [12] lists the
names of people who've contributed to the project.  The wiki Dev page
[13] contains resources for hackers.
Tahoe is sponsored by Allmydata, Inc. [14], a provider of commercial
backup services.  Allmydata, Inc. created the Tahoe project, and
contributes hardware, software, ideas, bug reports, suggestions,
demands, and money (employing several Tahoe hackers and instructing
them to spend part of their work time on this Free Software project).
Also they award customized t-shirts to hackers who find security flaws
in Tahoe (see  ).  Thank you to Allmydata, Inc. for
their generous and public-spirited support.
Zooko Wilcox-O'Hearn
on behalf of the allmydata.org team
Special acknowledgment goes to Brian Warner, whose superb engineering
skills and dedication are primarily responsible for the Tahoe
implementation, and largely responsible for the Tahoe design as well,
not to mention most of the docs and many other things besides.
February 13, 2009
Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2009-07-18 11:03:45
@_author: Zooko Wilcox-O'Hearn 
@_subject: why hyperelliptic curves? 
Oh, and by the way the way that TahoeLAFS uses public key  cryptography highlights some of the weaknesses of current public key  techniques and some of the strengths of possible future techniques  such as hyperelliptic curves.  (I know that Tanja Lange has done a  lot of work on those.)
TahoeLAFS generates a unique public-private key pair for each mutable  file and each directory.  (Immutable files don't use public key  cryptography at all -- they are secured solely with a stream cipher  and secure hashes.)  The "file handle" or "capability" to a mutable  file or directory contains the actual public key (if it is a read- only capability) or the actual private key (if it is a read-write  capability).  Therefore some of our most important measures of  performance are public key size and keypair generation time.   Unfortunately, we blundered into using one of the worst possible  public key signature algorithms for such requirements: RSA!  Our  current project is replacing RSA with ECDSA.  TahoeLAFS v2.0 will  support ECDSA-based capabilities (in addition to RSA-based ones for  backward compatibility).
TahoeLAFS also requires more than two levels of privilege.  With  traditional public/private keys there are exactly two levels: you  either know the private key or you don't.  We need to have an  intermediate level of privilege -- someone who doesn't know the  private key but who does know something that not everyone knows.   (Everyone knows the public key.)  We use these three levels of  privilege to create read-write capabilities, read-only capabilities  and verify capabilities.   (A verify capability gives the ability to  check integrity of the ciphertext, which everyone has, because  everyone knows the public key).  If this doesn't make sense to you  then see if my longer explanation in lafs.pdf makes any more sense.
Anyway, if it is true that hyperelliptic curves have a security level  commensurate with the number of bits in the public key, then  hyperelliptic curves with semi-private keys would be the ideal public  key crypto signature scheme for TahoeLAFS.  Unfortunately, semi- private keys aren't proven secure nor properly peer-reviewed, and  hyperelliptic curves aren't well implemented or widely appreciated.   Hopefully someday TahoeLAFS v3.0 will support semi-private- hyperelliptic-curve-based capabilities (in addition to RSA and ECDSA  for backward compatibility).
Zooko Wilcox-O'Hearn
P.S.  Oh, I told a lie in the interests of brevity when I said that  file handles contain actual public keys or actual private keys.  RSA  keys are way too big for that.  So instead we go through interesting  contortions to make a "surrogate" value which can be used to check  the correctness of the RSA key (i.e. the surrogate value is derived  from the RSA key by secure hashing) as well as can be used to control  access to the RSA key (the RSA key is encrypted with a stream cipher  using the surrogate value as the symmetric encryption key).  The  surrogate value therefore offers the same integrity and access  control properties as the RSA key itself (when the user also has  access to the encrypted RSA key itself), but it is sufficiently short  to embed directly into the file handles a.k.a. capabilities.  This  too is explained more fully in lafs.pdf.
[1] [2]

@_date: 2009-07-24 07:33:29
@_author: Zooko Wilcox-O'Hearn 
@_subject: cleversafe says: 3 Reasons Why Encryption is Overrated 
[cross-posted to tahoe-dev at allmydata.org and cryptography at metzdowd.com]
Disclosure:  Cleversafe is to some degree a competitor of my Tahoe- LAFS project.  On the other hand, I tend to feel positive towards  them because they open-source much of their work.  Our "Related  Projects" page has included a link to cleversafe for years now, I  briefly collaborated with some of them on a paper about erasure  coding last year, and I even spoke briefly with them about the idea  of becoming an employee of their company this year.  I am tempted to  ignore this idea that they are pushing about encryption being  overrated, because they are wrong and it is embarassing.  But I've  decided not to ignore it, because people who publicly spread this  kind of misinformation need to be publicly contradicted, lest they  confuse others.
Cleversafe has posted a series of blog entries entitled "3 Reasons  Why Encryption is Overrated".
 # 3 Reasons Why Encryption is   # Response Part 1: Future  Processing Power
 # Response Part 2:  Complexities of Key Management
 # Response Part 3: Disclosure  It begins like this:
When it comes to storage and security, discussions traditionally  center on encryption.  The reason encryption b or the use of a  complex algorithm to encode information b is accepted as a best  practice rests on the premise that while itbs possible to crack  encrypted information, most malicious hackers donbt have access to  the amount of computer processing power they would need to decrypt  But not so fast.  Letbs take a look at three reasons why encryption  is overrated.
The first claim -- the today's encryption is vulnerable to tomorrow's  processing power -- is a common goof, which is easy to make by  conflating historical failures of cryptosystems due to having too  small of a crypto value with failures due to weak algorithms.   Examples of the former are DES, which failed because its 56-bit key  was small enough to fall to brute force, and the bizarre "40-bit  security" policies of the U.S. Federal Government in the 90's.  An  example of the latter is SHA1, whose hash output size is *not* small  enough to brute-force, but which is insecure because, as it turns  out, the SHA1 algorithm allows the generation of colliding inputs  much quicker than a brute force search would.
Oh boy, I see that in the discussion following the article "Future  I donbt think symmetric ciphers such as AES-256 are under any threat  of being at risk to brute force attacks any time this century.
What?  Then why is he spreading this Fear, Uncertainty, and Doubt?   Oh and then it gets *really* interesting: it turns out that  cleversafe uses AES-256 in an All-or-Nothing Transform as part of  their "Information Dispersal" algorithm.  Okay, I would like to  understand better the cryptographic effects of that (and in  particular, whether this means that the cleversafe architecture is  just as susceptible to AES-256 failing as an encryption scheme such  as is used in the Tahoe-LAFS architecture).
But, it is time for me to stop reading about cryptography and get  ready to go to work.  :-)
Tahoe, the Least-Authority Filesystem -- store your data: $10/month -- I am available for work --

@_date: 2009-11-08 03:30:47
@_author: Zooko Wilcox-O'Hearn 
@_subject: hedging our bets -- in case SHA-256 turns out to be insecure  
We're going to be deploying a new crypto scheme in Tahoe-LAFS next  year -- the year 2010.  Tahoe-LAFS is used for long-term storage, and  I won't be surprised if people store files on Tahoe-LAFS in 2010 and  then rely on the confidentiality and integrity of those files for  many years or even decades to come.  (People started storing files on  Tahoe-LAFS in 2008 and so far they show no signs of losing interest  in the integrity and confidentiality of those files.)
This long-term focus makes Tahoe-LAFS's job harder than the job of  protecting transient network packets.  If someone figures out in 2020  or 2030 how to spoof a network transaction that you sent in 2010 (see  [1]), it'll be far too late to do you any harm, but if they figure  out in 2030 how to alter a file that you uploaded to a Tahoe-LAFS  grid in 2010, that might harm you.
Therefore I've been thinking about how to make Tahoe-LAFS robust  against the possibility that SHA-256 will turn out to be insecure.
A very good way to do this is to design Tahoe-LAFS so that it relies  as little as possible on SHA-256's security properties.  The property  that seems to be the hardest for a secure hash function to provide is  collision-resistance.  We are analyzing new crypto schemes to see how  many security properties of Tahoe-LAFS we can continue to guarantee  even if the collision-resistance of the underlying secure hash  function fails, and similarly for the other properties of the secure  hash function which might fail [2].
This note is not about that design process, though, but about how to  maximize the chance that the underlying hash function does provide  the desired security properties.
We could use a different hash function than SHA-256 -- there are many  alternatives.  SHA-512 would probably be safer, but it is extremely  expensive on the cheap, low-power 32-bit ARM CPUs that are one of our  design targets [3], and the output size of 512 bits is too large to  fit into Tahoe-LAFS capabilities.  There are fourteen candidates left  in the SHA-3 contest at the moment.  Several of them have  conservative designs and good performance, but there is always the  risk that they will be found to have catastrophic design flaws or  that a great advance in hash function cryptanalysis will suddenly  show how to crack them.  Of course, a similar risk applies to SHA-256!
So I turn to the question of how to combine multiple hash functions  to build a hash function which is secure even if one or more of the  underlying hash functions turns out to be weak.
I've read several interesting papers on the subject -- such as [4, 5]  and especially "Robust Multi-Property Combiners for Hash Functions  Revisited" by Marc Fischlin, Anja Lehmann, and Krzysztof Pietrzak  [6].  The good news is that it turns out to be doable!  The latter  two papers show nice strong theoretical results -- ways to combine  hash functions so that the resulting combination is as strong or  stronger than the two underlying hash functions.  The bad news is  that the proposal in [6] builds a combined function whose output is  twice the size of the output of a single hash function.  There is a  good theoretical reason for this [4], but it won't work for our  practical engineering requirements -- we need hash function outputs  as small as possible (partially due to usability issues)
The other bad news is that the construction proposed in [6] is  complicated, underspecified, and for the strongest version of it, it  imposes a limit on the length of the inputs that you can feed to your  hash function.  It grows to such complexity and incurs such  limitations because it is, if I may call it this, "too theoretical".   It is designed to guarantee certain output properties predicated on  minimal theoretical assumptions about the properties of the  underlying hash functions.  This is a fine goal, but in practice we  don't want to pay such a high cost in complexity and performance in  order to gain such abstract improvement.  We should be able to "hedge  our bets" and achieve a comfortable margin of safety with a very  simple and efficient scheme by making stronger, less formal, but very  plausible assumptions about the underlying hash functions.  Read on.
I propose the following combined hash function C, built out of two  hash functions H1 and H2:
C(x) = H1(H1(x) || H2(x))
The first observation is that if H1 is collision-resistant then so is  C.  In practice I would expect to use SHA-256 for H1, so the  resulting combiner C[SHA-256, H2] will be at least as strong as  SHA-256.  (One could even think of this combiner C as just being a  tricky way to strengthen SHA-256 by using the output of H2(x) as a  randomized salt -- see [7].)
The next observation is that finding a pair of inputs x1, x2 which  collide in *both* H1 and in H2 is likely to be much harder than  finding a pair of inputs that collide in H1 and finding a pair of  inputs that collide in H2 (see [5]).
Now the reason that a combiner like this one is not published in  theoretical crypto literature is that it obviously could fail if the  outer hash function H1 fails.  For example, even if H2 is collision- resistant, if H1 turns out to be susceptible to collisions, then  theoretically speaking C[H1, H2] might be susceptible to collisions.   However, in real life C[H1, H2] would most likely still be collision  All practical attacks on real hash functions so far (if I understand  correctly) are multi-block attacks in which the attacker is able to  feed a sufficiently long and unconstrained input to the hash  functions that the effects of the later parts of his inputs are able  to manipulate the state generated by the earlier parts of his  inputs.  My combiner C uses H1 in its outer invocation on a single- block-sized input, which means no such multi-block attacks are  possible on the outer invocation.  In addition, the inputs that the  attacker gets to feed to the outer invocation of H1 are highly  constrained.  Basically, he would already have to be very good at  manipulating the inner invocations H1 and H2 in ways that he isn't  supposed to before he can even begin to manipulate the outer  invocation of H1.
A measure of the practical security of a combiner like this one would  be "how safe would it be if it were instantiated using broken  practical hash functions such as MD5 and SHA1?".  It appears to me  (from an admittedly cursory analysis) that there is no realistic way  to find collisions in C[MD5, SHA1] even though there are realistic  ways to find collisions in MD5 and in SHA1.  Of course, I'm not  proposing to use C[MD5, SHA1]!  I'm proposing to use C[SHA-256, _]  where _ is some other hash function which is believed to be strong.   The example of instantiating C with MD5 and SHA1 just goes to show  that C is a hash function which is stronger than either of its two  underlying hash functions.
The other desirable security properties such as second-preimage  resistance and pre-image resistance seem to follow the same pattern  as collision-resistance -- C[H1, H2] seems to be much stronger than  H1 or H2 alone.
[1] [2] [3] [4] Krzysztof Pietrzak: "Non-Trivial Black-Box Combiners for  Collision-Resistant Hash-Functions don't Exist"
[5] Jonathan J. Hoch, Adi Shamir: "On the Strength of the  Concatenated Hash Combiner when All the Hash Functions are Weak"
[6] Marc Fischlin, Anja Lehmann, Krzysztof Pietrzak: "Robust Multi- Property Combiners for Hash Functions Revisited"
[7]

@_date: 2009-09-02 13:50:21
@_author: Zooko Wilcox-O'Hearn 
@_subject: so how do *you* manage your keys, then? part 3 
So How Do You Manage Your Keys Then, part 3 of 5
In part one of this series [1] I described how Tahoe-LAFS combines  decryption, integrity-checking, identification, and access into one  bitstring, called an "immutable file read-cap" (short for  "capability").  In part two [2] I described how users can build tree- like structures of files which contain caps pointing to other files,  and how the cap pointing to the root of such a structure can reside  on a different computer than the ciphertext.  (Which is necessary if  you want someone to store the ciphertext for you but you don't want  to give them the ability to read the file contents.)
In this installment, consider the question of whether you can give  someone a cap (which acts as a file handle) and then change the  contents of the file that the cap points to, while preserving their  ability to read with the original cap.
This would be impossible with the immutable file read-caps that we  have been using so far, because each immutable file read cap uses a  secure hash function to identify and integrity-check exactly one  file's contents -- one unique byte pattern.  Any change to the file  contents will cause the immutable file read-cap to no longer match.   This can be a desirable property if what you want is a permanent  identifier of one specific, immutable file.  With this property  nobody -- not even the person who wrote the file in the first place

@_date: 2009-09-14 10:22:16
@_author: Zooko Wilcox-O'Hearn 
@_subject: how to encrypt and integrity-check with only one key 
I had an idea about how to use a single key to accomplish both  encryption and integrity checking on an immutable file.  I posted it  to the tahoe-dev list [1], and David-Sarah Hopwood followed up with  an interesting new crypto cap design [2].  Here is the basic crypto  trick, which may be useful in other contexts than Tahoe-LAFS.
Suppose you have some data and you want to control who gets to see  it, and you also want anyone who sees it to be able to verify its  integrity.  So far, these requirements are familiar to  cryptographers.  The obvious answer is to encrypt the data and then  to MAC (Message Authentication Code) the ciphertext.  There would be  one key for the encryption and one key for the MAC.  However, this  has the wrong semantics for our purposes -- anyone who is given the  ability to check the integrity (by being given the MAC key) is also  given the ability to create new texts which would verify.  Likewise,  whoever creates the initial MAC tag can also create other MAC tags  which would cause others files to also verify.  Instead, we want a  single file that can pass the integrity check, and nobody -- not a  reader who is able to verify integrity nor even the writer who  initially created the file -- is able to make a different file which  would also pass the integrity check.
Therefore, we want the integrity check value to be the secure hash of  the file itself.  That's what we currently have in Tahoe-LAFS.  The  immutable file read cap is a concatenation of two values: the  decryption key and the secure hash.  The latter is solely for  integrity-checking.  Actually in Tahoe-LAFS, the integrity check  value is not just a flat hash of the plaintext, but instead it is the  hash of the roots of a pair of Merkle Trees, one for verifying the  correctness of the shares and the other for verifying the correctness  of the ciphertext (see [3]).
Now, convergent encryption could do both jobs with one value!  If you  let the symmetric key be the secure hash of the plaintext, then the  reader could use the symmetric key to decrypt, then verify that the  key was the hash of the plaintext.  However, you can't always use  convergent encryption.  Not only because of the security issues [4],  and not only because it requires two passes over the file which  prevents "on-line" processing, but also because you might need to  generate the symmetric key and/or the integrity check value in a  different way.  For example, the Tahoe-LAFS integrity-check value  isn't just a secure hash of the plaintext.  It would be inefficient  to generate the full Tahoe-LAFS integrity check value before  beginning to encrypt, and we want to be able to give someone the  integrity check value (in a verify cap) without thus giving them the  decryption key (i.e. the read cap).
So here is my idea to use a single value to accomplish both  decryption and integrity checking even when you can't set the  symmetric key to be the secure hash of the plaintext.  You use the  encryption key K1 to encrypt the plaintext to produce the ciphertext,  and in the same pass you compute the integrity-check value V.  Then  you compute the secure hash of the combination of K1 and V, let's  call the result R = H(K1, V).  Then you encrypt K1 using R and store  the encrypted K1_enc with the ciphertext.  Now R is the real key --  the read cap.  If someone gives you R, the ciphertext, and the  encrypted K1_enc, then you first use R to decrypt K1, check that R = H (K1, V), then perform the decryption and integrity-checking of the  Here is a diagram: [5] (also attached).
David-Sarah Hopwood suggested the improvement that the integrity- check value "V" could be computed as an integrity check (i.e. a  secure hash) on the K1_enc in addition to the file contents.
[1] [2] [3] [4] [5]

@_date: 2010-08-03 11:05:37
@_author: Zooko O'Whielacronx 
@_subject: [tahoe-announce] ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0N2 
Please try this out! I think this version is reliable and performs
well and will become the v1.8.0 final release in less than two weeks'
Send your results to tahoe-dev at tahoe-lafs.org or if you find an issue
add it to the issue tracker at  .
Thanks! :-)
ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0N2
The Tahoe-LAFS team is pleased to announce the immediate
availability of version 1.8.0N2 of Tahoe-LAFS, an extremely
reliable distributed storage system.
Tahoe-LAFS is the first distributed storage system which
offers "provider-independent security"bmeaning that not even
the operators of your storage servers can read or alter your
data without your consent. Here is the one-page explanation of
its unique security and fault-tolerance properties:
The current stable release of Tahoe-LAFS is v1.7.1, which was
released July 18, 2010 [1].
v1.8.0N2 is the beta release of major new improvements, including fast
and fault-tolerant downloads and better Windows support. See the NEWS
file [2] for details. If no regressions or critical bugs are
discovered in this 1.8.0N2 version then we will release a functionally
identical 1.8.0 final version on approximately August 15, 2010.
WHAT IS IT GOOD FOR?
With Tahoe-LAFS, you distribute your filesystem across
multiple servers, and even if some of the servers are
compromised by by an attacker, the entire filesystem continues
to work correctly, and continues to preserve your privacy and
security. You can easily share specific files and directories
with other people.
In addition to the core storage system itself, volunteers have
built other projects on top of Tahoe-LAFS and have integrated
Tahoe-LAFS with existing systems.
These include frontends for Windows, Macintosh, JavaScript,
iPhone, and Android, and plugins for Hadoop, bzr, mercurial,
duplicity, TiddlyWiki, and more. See the Related Projects page
on the wiki [3].
We believe that strong cryptography, Free and Open Source
Software, erasure coding, and principled engineering practices
make Tahoe-LAFS safer than RAID, removable drive, tape,
on-line backup or "cloud storage" systems.
This software is developed under test-driven development, and
there are no known bugs or security flaws which would
compromise confidentiality or data integrity under recommended
use. (For all currently known issues please see the
known_issues.txt file [4].)
This release is fully compatible with the version 1 series of
Tahoe-LAFS. Clients from this release can write files and
directories in the format used by clients of all versions back
to v1.0 (which was released March 25, 2008). Clients from this
release can read files and directories produced by clients of
all versions since v1.0. Servers from this release can serve
clients of all versions back to v1.0 and clients from this
release can use servers of all versions back to v1.0.
This is the eleventh release in the version 1 series. This
series of Tahoe-LAFS will be actively supported and maintained
for the forseeable future, and future versions of Tahoe-LAFS
will retain the ability to read and write files compatible
with this series.
You may use this package under the GNU General Public License,
version 2 or, at your option, any later version. See the file
"COPYING.GPL" [5] for the terms of the GNU General Public
License, version 2.
You may use this package under the Transitive Grace Period
Public Licence, version 1 or, at your option, any later
version. (The Transitive Grace Period Public Licence has
requirements similar to the GPL except that it allows you to
delay for up to twelve months after you redistribute a derived
work before releasing the source code of your derived work.)
See the file "COPYING.TGPPL.html" [6] for the terms of the
Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either
licence, at your option.)
Tahoe-LAFS works on Linux, Mac OS X, Windows, Cygwin, Solaris,
*BSD, and probably most other systems. Start with
"docs/quickstart.html" [7].
HACKING AND COMMUNITY
Please join us on the mailing list [8]. Patches are gratefully
accepted -- the RoadMap page [9] shows the next improvements
that we plan to make and CREDITS [10] lists the names of people
who've contributed to the project. The Dev page [11] contains
resources for hackers.
Tahoe-LAFS was originally developed by Allmydata, Inc., a
provider of commercial backup services. After discontinuing
funding of Tahoe-LAFS R&D in early 2009, they have continued
to provide servers, bandwidth, small personal gifts as tokens
of appreciation, and bug reports. Thank you to Allmydata,
Inc. for their generous and public-spirited support.
Google, Inc. is sponsoring Tahoe-LAFS development as part of
the Google Summer of Code 2010. Google suggested that we
should apply for the Summer of Code program, and when we did
they generously awarded four sponsorships to students from
around the world to hack on Tahoe-LAFS this summer. Thank you
to Google, Inc. for their generous and public-spirited
HACK TAHOE-LAFS!
If you can find a security flaw in Tahoe-LAFS which is serious
enough that feel compelled to warn our users and issue a fix,
then we will award you with a customized t-shirts with your
exploit printed on it and add you to the "Hack Tahoe-LAFS Hall
Of Fame" [12].
This is the fifth release of Tahoe-LAFS to be created solely
as a labor of love by volunteers. Thank you very much to the
team of "hackers in the public interest" who make Tahoe-LAFS
David-Sarah Hopwood and Zooko Wilcox-O'Hearn
on behalf of the Tahoe-LAFS team
August 3, 2010
Rainhill, Merseyside, UK and Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] tahoe-announce mailing list
tahoe-announce at tahoe-lafs.org

@_date: 2010-07-16 09:35:48
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] Alpha testers needed 
Thanks for the history lessons, IanG and JimM! And thanks for the new
p2p research references, Dan Menasche and Vijay Gurbani. And thanks
for the valuable skepticism, DavidB and SergueiO. ;-)
Something that I would like to append to IanG's story about
cypherpunks thinking prices are the answer and p2p-punks thinking
decentralization is the answer:
That there is also a new generation of interesting payment systems
including The Love Machine and Flattr.
I think Flattr is very interesting:
Founded by founders of The Pirate Bay, they do several things that are
very promising:
1. The marginal cost to you of clicking on someone's "flattr me"
button is zero. This is due to the scheme of subscribing to Flattr.com
with a monthly fee and then at the end of the month your money gets
split among everyone whom your clicked on. This is the most promising
solution to the problem of "mental transaction costs" [1].
2. The pitch is that this is a way to express love to people. b% > $
3. Look: content!  It is very easy
to find things to love on the flattr.com web site.
This has a lot in common with the "tipping" feature that we advertised
as a future feature of Mojo Nation (e.g. it features prominently in
the write-up of Mojo Nation in The Economist). (Inside Evil Geniuses
For A Better Tomorrow we called that feature the "Pay Lars" button, in
honor of a certain musician who had publicly criticized Napster for
depriving him of well-earned income.)
evolution of ideas. The founders of The Pirate Bay are probably
intimately familiar with BitTorrent, but as far as I know, they are
unfamiliar with anonymous Chaumian digital cash. I wouldn't be
surprised if they got the idea for Flattr from their experience with
BitTorrent and basically observing that there was a "hole" in
BitTorrent where micropayments could go. :-) Does anyone know the
inside story on how they got the idea for Flattr?
[1] p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2010-07-19 08:31:09
@_author: Zooko O'Whielacronx 
@_subject: [tahoe-announce] ANNOUNCING Tahoe, the Least-Authority File System, 
ANNOUNCING Tahoe, the Least-Authority File System, v1.7.1
The Tahoe-LAFS team is pleased to announce the immediate
availability of version 1.7.1 of Tahoe-LAFS, an extremely
reliable distributed storage system.
Tahoe-LAFS is the first distributed storage system which
offers "provider-independent security"bmeaning that not even
the operators of your storage servers can read or alter your
data without your consent. Here is the one-page explanation of
its unique security and fault-tolerance properties:
Tahoe-LAFS v1.7.1 is the successor to v1.7.0, which was
released June 18, 2010 [1].
v1.7.1 is a stable minor release which adds several bugfixes
and improvements in security, forward-compatibility,
packaging, usability, and portability. See the NEWS file [2]
for details.
WHAT IS IT GOOD FOR?
With Tahoe-LAFS, you distribute your filesystem across
multiple servers, and even if some of the servers are
compromised by by an attacker, the entire filesystem continues
to work correctly, and continues to preserve your privacy and
security. You can easily share specific files and directories
with other people.
In addition to the core storage system itself, volunteers have
built other projects on top of Tahoe-LAFS and have integrated
Tahoe-LAFS with existing systems.
These include frontends for Windows, Macintosh, JavaScript,
iPhone, and Android, and plugins for Hadoop, bzr, mercurial,
duplicity, TiddlyWiki, and more. See the Related Projects page
on the wiki [3].
We believe that strong cryptography, Free and Open Source
Software, erasure coding, and principled engineering practices
make Tahoe-LAFS safer than RAID, removable drive, tape,
on-line backup or "cloud storage" systems.
This software is developed under test-driven development, and
there are no known bugs or security flaws which would
compromise confidentiality or data integrity under recommended
use. (For all currently known issues please see the
known_issues.txt file [4].)
This release is fully compatible with the version 1 series of
Tahoe-LAFS. Clients from this release can write files and
directories in the format used by clients of all versions back
to v1.0 (which was released March 25, 2008). Clients from this
release can read files and directories produced by clients of
all versions since v1.0. Servers from this release can serve
clients of all versions back to v1.0 and clients from this
release can use servers of all versions back to v1.0.
This is the tenth release in the version 1 series. This series
of Tahoe-LAFS will be actively supported and maintained for
the forseeable future, and future versions of Tahoe-LAFS will
retain the ability to read and write files compatible with
this series.
You may use this package under the GNU General Public License,
version 2 or, at your option, any later version. See the file
"COPYING.GPL" [5] for the terms of the GNU General Public
License, version 2.
You may use this package under the Transitive Grace Period
Public Licence, version 1 or, at your option, any later
version. (The Transitive Grace Period Public Licence has
requirements similar to the GPL except that it allows you to
delay for up to twelve months after you redistribute a derived
work before releasing the source code of your derived work.)
See the file "COPYING.TGPPL.html" [6] for the terms of the
Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either
licence, at your option.)
Tahoe-LAFS works on Linux, Mac OS X, Windows, Cygwin, Solaris,
*BSD, and probably most other systems. Start with
"docs/quickstart.html" [7].
HACKING AND COMMUNITY
Please join us on the mailing list [8]. Patches are gratefully
accepted -- the RoadMap page [9] shows the next improvements
that we plan to make and CREDITS [10] lists the names of people
who've contributed to the project. The Dev page [11] contains
resources for hackers.
Tahoe-LAFS was originally developed by Allmydata, Inc., a
provider of commercial backup services. After discontinuing
funding of Tahoe-LAFS R&D in early 2009, they have continued
to provide servers, bandwidth, small personal gifts as tokens
of appreciation, and bug reports. Thank you to Allmydata,
Inc. for their generous and public-spirited support.
Google, Inc. is sponsoring Tahoe-LAFS development as part of
the Google Summer of Code 2010. Google suggested that we
should apply for the Summer of Code program, and when we did
they generously awarded four sponsorships to students from
around the world to hack on Tahoe-LAFS this summer. Thank you
to Google, Inc. for their generous and public-spirited
HACK TAHOE-LAFS!
If you can find a security flaw in Tahoe-LAFS which is serious
enough that feel compelled to warn our users and issue a fix,
then we will award you with a customized t-shirts with your
exploit printed on it and add you to the "Hack Tahoe-LAFS Hall
Of Fame" [12].
This is the fifth release of Tahoe-LAFS to be created solely
as a labor of love by volunteers. Thank you very much to the
team of "hackers in the public interest" who make Tahoe-LAFS
David-Sarah Hopwood and Zooko Wilcox-O'Hearn
on behalf of the Tahoe-LAFS team
July 18, 2010
Rainhill, Merseyside, UK and Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] tahoe-announce mailing list
tahoe-announce at tahoe-lafs.org

@_date: 2010-11-10 08:19:53
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] how do I tell these Chinese people "You're doing it 
Dear people of p2p-hackers, tahoe-dev, and liberationtech:
I think I confused the issue when I said in [1] "some people in China
might be relying on using the Tahoe-LAFS public demo over unencrypted
HTTP and thinking that it provides security properties like they would
get if they ran their own copy of Tahoe-LAFS locally".
Encryption of the HTTP connection isn't very important, so it was
confusing when I mentioned "over unencrypted HTTP". I should have just
said "some people in China might be relying on using the Tahoe-LAFS
public demo and thinking that it provides security properties like
they would get if they ran their own copy of Tahoe-LAFS
Look at this diagram:
Using an unencrypted connection (HTTP or FTP) between the Tahoe-LAFS
client and the Tahoe-LAFS gateway means that the link between those
two objects on the diagram is red, meaning that you are vulnerable to
anyone who controls that link. If you instead used an encrypted
connection (HTTPS or SFTP) between those two objects then that link
would be black, meaning that you are not vulnerable to someone just
because they control that link. But you are still vulnerable to
whoever controls the Tahoe-LAFS gateway which the link goes to!
The right way to do it is to run the Tahoe-LAFS gateway yourself on a
computer that you control. The Tahoe-LAFS gateway object is red on
that diagram, meaning that you rely on it for your security, which is
why you should run it on a computer that you control.
You could run it on the same laptop or desktop that you are running
your web browser (which is acting as the Tahoe-LAFS client), in which
case it doesn't matter whether you use HTTP or HTTPS because the
connection is only running over the loopback interface anyway.
Or you could run it on some other computer that you control, in which
case you need to use HTTPS so that you aren't vulnerable to anyone who
controls the link between your local computer running your web browser
on and your remote computer running your Tahoe-LAFS gateway.
So, how do we explain to these Chinese users (and everyone else) that
if they want good security, they must run a Tahoe-LAFS gateway (which
is a web server) on a computer they control? Perhaps it would help to
draw one variant of this diagram showing a user using a gateway on a
remote server and being vulnerable to the people who control that
server (which may include more people than the server's legal owner
thinks), and another picture showing a user using a gateway on his
local machine and being safe against the threat of the server operator
betraying him.
Does anyone have design skills (and Chinese!) and could try to explain this?
Here is the source code for the current version of the diagram:
[1] p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2010-10-05 23:15:03
@_author: Zooko O'Whielacronx 
@_subject: [tahoe-announce] statement on backdoors 
Statement on Backdoors
October 5, 2010
The New York Times has recently reported that the current U.S.
administration is proposing a bill that would apparently, if passed,
require communication systems to facilitate government wiretapping and
access to encrypted data:
  (login required;
username/password pairs available at
Commentary by the  Electronic Frontier Foundation
( ),  Peter
Suderman / Reason
( ),
Julian Sanchez / Cato Institute
( ).
The core Tahoe developers promise never to change Tahoe-LAFS to
facilitate government access to data stored or transmitted by it. Even
if it were desirable to facilitate such accessbwhich it is notbwe
believe it would not be technically feasible to do so without severely
compromising Tahoe-LAFS' security against other attackers. There have
been many examples in which backdoors intended for use by government
have introduced vulnerabilities exploitable by other parties (a
notable example being the Greek cellphone eavesdropping scandal in
2004/5). RFCs  1984 and  2804 elaborate on the security case against
such backdoors.
Note that since Tahoe-LAFS is open-source software, forks by people
other than the current core developers are possible. In that event, we
would try to persuade any such forks to adopt a similar policy.
The following Tahoe-LAFS developers agree with this statement:
David-Sarah Hopwood
Zooko Wilcox-O'Hearn
Brian Warner
Kevan Carstensen
FrC)dC)ric Marti
Jack Lloyd
FranC'ois Deppierraz
Yu Xue
Marc Tooley
tahoe-announce mailing list
tahoe-announce at tahoe-lafs.org

@_date: 2010-09-12 01:31:22
@_author: Zooko O'Whielacronx 
@_subject: [tahoe-announce] ANNOUNCING Tahoe, the Least-Authority File System, 
ANNOUNCING Tahoe, the Least-Authority File System, v1.8.0c4
The Tahoe-LAFS team is pleased to announce the immediate
availability of version 1.8.0c4 of Tahoe-LAFS, an extremely
reliable distributed storage system. Get it here:
Tahoe-LAFS is the first distributed storage system which
offers "provider-independent security"bmeaning that not even
the operators of your storage servers can read or alter your
data without your consent. Here is the one-page explanation of
its unique security and fault-tolerance properties:
The current stable release of Tahoe-LAFS is v1.7.1, which was
released July 18, 2010 [1].
v1.8.0 offers greatly improved performance and fault-tolerance
of downloads and improved Windows support. See the NEWS file
[2] for details.
v1.8.0c4 is the release candidate for v1.8.0. v1.8.0c4 fixed a
unit test failure and added some documentation and build
system improvements. v1.8.0c3 fixed performance regressions
for some server layouts. v.1.80c2 corrected some minor
regressions in download status pages.  v1.8.0c1 corrected
regressions in compatibility with other versions, and in
downloads using the Range header, that were introduced in
v1.8.0N2. If no further regressions or critical bugs are
discovered, we will release v1.8.0 final on September 18,
WHAT IS IT GOOD FOR?
With Tahoe-LAFS, you distribute your filesystem across
multiple servers, and even if some of the servers fail or are
taken over by an attacker, the entire filesystem continues to
work correctly, and continues to preserve your privacy and
security. You can easily share specific files and directories
with other people.
In addition to the core storage system itself, volunteers
have built other projects on top of Tahoe-LAFS and have
integrated Tahoe-LAFS with existing systems, including
Windows, JavaScript, iPhone, Android, Hadoop, Flume, Django,
Puppet, bzr, mercurial, perforce, duplicity, TiddlyWiki, and
more. See the Related Projects page on the wiki [3].
We believe that strong cryptography, Free and Open Source
Software, erasure coding, and principled engineering practices
make Tahoe-LAFS safer than RAID, removable drive, tape,
on-line backup or cloud storage.
This software is developed under test-driven development, and
there are no known bugs or security flaws which would
compromise confidentiality or data integrity under recommended
use. (For all important issues that we are currently aware of
please see the known_issues.txt file [4].)
This release is compatible with the version 1 series of
Tahoe-LAFS. Clients from this release can write files and
directories in the format used by clients of all versions back
to v1.0 (which was released March 25, 2008). Clients from this
release can read files and directories produced by clients of
all versions since v1.0. Servers from this release can serve
clients of all versions back to v1.0 and clients from this
release can use servers of all versions back to v1.0.
This is the eleventh release in the version 1 series. This
series of Tahoe-LAFS will be actively supported and maintained
for the forseeable future, and future versions of Tahoe-LAFS
will retain the ability to read and write files compatible
with this series.
You may use this package under the GNU General Public License,
version 2 or, at your option, any later version. See the file
"COPYING.GPL" [5] for the terms of the GNU General Public
License, version 2.
You may use this package under the Transitive Grace Period
Public Licence, version 1 or, at your option, any later
version. (The Transitive Grace Period Public Licence has
requirements similar to the GPL except that it allows you to
delay for up to twelve months after you redistribute a derived
work before releasing the source code of your derived work.)
See the file "COPYING.TGPPL.html" [6] for the terms of the
Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either
licence, at your option.)
Tahoe-LAFS works on Linux, Mac OS X, Windows, Cygwin, Solaris,
*BSD, and probably most other systems. Start with
"docs/quickstart.html" [7].
HACKING AND COMMUNITY
Please join us on the mailing list [8]. Patches are gratefully
accepted -- the RoadMap page [9] shows the next improvements
that we plan to make and CREDITS [10] lists the names of people
who've contributed to the project. The Dev page [11] contains
resources for hackers.
Tahoe-LAFS was originally developed by Allmydata, Inc., a
provider of commercial backup services. After discontinuing
funding of Tahoe-LAFS R&D in early 2009, they continued
to provide servers, bandwidth, small personal gifts as tokens
of appreciation, and bug reports.
Google, Inc. sponsored Tahoe-LAFS development as part of the
Google Summer of Code 2010. They awarded four sponsorships to
students from around the world to hack on Tahoe-LAFS that
Thank you to Allmydata and Google for their generous and
public-spirited support.
HACK TAHOE-LAFS!
If you can find a security flaw in Tahoe-LAFS which is serious
enough that feel compelled to warn our users and issue a fix,
then we will award you with a customized t-shirts with your
exploit printed on it and add you to the "Hack Tahoe-LAFS Hall
Of Fame" [12].
This is the fifth release of Tahoe-LAFS to be created solely
as a labor of love by volunteers. Thank you very much to the
team of "hackers in the public interest" who make Tahoe-LAFS
David-Sarah Hopwood and Zooko Wilcox-O'Hearn
on behalf of the Tahoe-LAFS team
September 11, 2010
Rainhill, Merseyside, UK and Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] tahoe-announce mailing list
tahoe-announce at tahoe-lafs.org

@_date: 2011-02-21 15:04:55
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] P2P file storage systems 
Dear Michael Militzer:
Thanks for your ideas about secure P2P storage, and thanks for your
interest in Tahoe-LAFS in particular.
Here are a couple of quick responses. I'm an architect and developer
of Tahoe-LAFS.
I also like that about Octavia. I'd like to see Octavia developed and
used more so I can learn more about the consequences of its
Since it is so simple it should be relatively easy for a volunteer to
implement it or to contribute part of an implementation of it.
Somebody out there do that! :-)
Replication costs more bandwidth on upload than erasure coding does
(for a similar degree of fault-tolerance) as well as costing more
That's right, We have kicked around some ideas about how to do a more
scalable-DHT-like routing instead of keeping a connection open from
each client to each server, but even if we had such a thing Tahoe-LAFS
grids would still be comprised exclusively of servers whose owners
gave you some reason to believe that they were reliable. Scalable
routing a la DHT wouldn't be sufficient to allow you to safely rely on
strangers for storage, for various reasons that you touched on next:
Hm, but then you say something that I don't quite follow:
I think the word "trust" often causes confusion, because it bundles
together a lot of concepts into one word. I find that rephrasing
things in terms of "reliance" often makes things clearer.
So: Tahoe-LAFS users absolutely do *not* rely on the storage servers
for confidentiality and integrity. Confidentiality and integrity are
guaranteed by the user's client software, using math (cryptography).
Even if *all* of the storage servers that you are using turn out to be
controlled by a single malicious entity who will stop at nothing to
harm you, this doesn't threaten the confidentiality of the data in
your files nor its integrity.
But, Tahoe-LAFS users *do* rely on the storage servers for the
longevity and availability of their data. If the malicious entity that
controls all the servers decides to delete all of the ciphertext that
they are holding for you, then no mathematical magic will help you get
the data back. :-)
That is why Tahoe-LAFS users typically limit the set of storage
servers that they will entrust their ciphertext. They choose only
servers which are operated by friends of theirs, or by a company that
they pay for service, or servers operated by members of a group that
has collectively agreed to trade storage for storage with each other.
I wrote more on this topic in a letter to the tahoe-dev mailing list last night:
   "BitTorrent for storage" is a bad idea
Why do you think these parameters would need to be changed?
It is plenty efficient for k and n up to about 256. It is also
probably efficient enough for k and n up to about 2^16, although I'm
skeptical that anyone actually needs k and n that size.
There is a Merkle Tree in Tahoe-LAFS which is computed over the
identifiers of the n shares, so that Merkle Tree would grow in size as
n grew. However that is a small cost that probably wouldn't need much
if any optimization.
Hm, so if I understand correctly, Tahoe-LAFS currently doesn't have
*scalability* in terms of the number of servers, but it does have
nearly optimal *censorship resistance* at a given scale. For example,
suppose there are 200 servers which are all joined in the conspiracy
to host a repository of Free and Open Source Software, and some evil
attacker is expending resources attempting to disrupt that hosting or
deny users access to it. If those 200 servers are organized into a
traditional scalable DHT like Chord, then a client would have
approximately a logarithmic number of connections to servers, say to
perhaps eight of them. An attacker who wants to deny that client
access to the Free and Open Source software repository would have to
take down only eight servers or prevent the client from establishing
working connections to them, right? Whereas with a full bipartite
graph topology like Tahoe-LAFS the attacker would have to take down or
deny access to a substantial constant fraction of all 200 of them
(depending on the ratio of k to n).
(Note: is assuming that the erasure coding parameter n is turned up to
200, which is already supported in Tahoe-LAFS -- you can configure it
in the tahoe.cfg configuration file.)
(Note: this is about attacking the storage layer, not the introduction
layer. Those are separate in Tahoe-LAFS and while the latter does need
some work, it is probably easier to defend the introduction layer than
the storage layer since introducers are stateless and have minimal
ability to do damage if they act maliciously. Multiple redundant
introducers were implemented by MO Faruque Sarker as part of the
Google Summer of Code 2010 but it hasn't been merged into trunk yet.
You can help! We need code-review, testing, documentation, etc.
 :-) )
You may be interested in Tahoe-LAFS-over-Tor and Tahoe-LAFS-over-i2p.
:-) I'm sure both of those projects would be grateful for bug reports,
patches, etc.
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2011-06-11 07:59:43
@_author: Zooko O'Whielacronx 
@_subject: scalable flooding information distribution 
Possibly related:
The description refers to other Tahoe-LAFS terminology, but basically
the proposal amounts to using the Chord structure for broadcast
instead of routing.

@_date: 2011-03-08 21:55:46
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] how do I tell these Chinese people "You're doing 
(carbon-copied to the p2p-hackers mailing list and the liberationtech
mailing lists)
As previously discussed on these lists (e.g. on 2010-11-10), we at the
Tahoe-LAFS project have now posted a prominent tutorial on the
security properties of our architecture:
Thanks to David Triendl, Yu Xue, David Andersen, Rujia Liu, Weihan
Wang, David-Sarah Hopwood, and Jonathan M.
I have (still) never had a successful two-way conversation with the
Chinese users that we originally noticed last year, so I don't know if
this warning label is still needed or if there are any Chinese people
using it. If you hear of any Chinese people using Tahoe-LAFS, please
let us know! :-)
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2011-05-03 12:59:35
@_author: Zooko O'Whielacronx 
@_subject: [cryptography] Point compression prior art? 
Have you seen DJB's "Irrelevant patents on elliptic-curve cryptography"
The section on "Point Compression" says:
Miller in 1986, in the paper that introduced elliptic-curve
cryptography, suggested compressing a public key (x,y) to simply x:
``Finally, it should be remarked, that even though we have phrased
everything in terms of points on an elliptic curve, that, for the key
exchange protocol (and other uses as one-way functions), that only the
x-coordinate needs to be transmitted. The formulas for multiples of a
point cited in the first section make it clear that the x-coordinate
of a multiple depends only on the x-coordinate of the original
point.'' This is exactly the compression method that I use.
Popular rumor states that point compression is covered by a subsequent
Vanstone-Mullin-Agnew patent: US patent 6141420, filed 1994.07.29,
granted 2000.10.31. What the patent actually claims are (1--28)
encryption using an elliptic curve over a finite field of
characteristic 2 with elements represented on a normal basis; (29, 36)
communicating (x,y) on a curve by communicating x and having the
receiver somehow compute y; (30--35, 37--41) communicating x and
``identifying information'' of y, such as one bit; and (42--52) some
secret-key encryption mechanisms.
My Curve25519 software never computes y, so it is not covered by the
patent. It should, in any case, be obvious to the reader that a patent
cannot cover compression mechanisms published seven years before the
patent was filed.
DJB also has this page, which goes into more detail about 6141420:
Contrary to the "filed 1994.07.29" above, the patent was actually
filed January 29, 1997:
Which means it expires January 29, 2017.
cryptography mailing list
cryptography at randombit.net

@_date: 2011-05-28 06:12:12
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] announcing bThe Tahoe-LAFS Weekly Newsb 
With the help of new volunteer Patrick McDonald, we're going to start
publishing the Tahoe-LAFS Weekly News. You can see the inaugural
edition in HTML form (attached), in .rst source form (appended), or on
the web at  .
We will *not* be sending the Tahoe-LAFS Weekly News to
p2p-hackers every week. Instead we'll set up a site
with an RSS feed that you can subscribe to if you want.
Oh, in fact, maybe we should create a new mailing list named
tahoe-lafs-weekly-news at tahoe-lafs.org that people can subscribe to if
they want the Tahoe-LAFS Weekly News in their inbox. Is anyone out
there interested? Let us know: Patrick McDonald
, Zooko , or
tahoe-dev at tahoe-lafs.org.
o;?.. -*- coding: utf-8 -*-
Tahoe-LAFS Weekly News, issue  number 1
Welcome to the inaugural edition of the Tahoe-LAFS Weekly News,
brought to you by Patrick McDonald and Zooko Wilcox-O'Hearn, scribes.
Announcements and News
Brian Warner is the release manager for 1.9.0_.  The plan for release
is late July with Kevan's Medium-Sized Distributed Mutable Files
(MDMF_) being the major new change. MDMF will implement almost the
same protocol and format as the current SDMF files. However it
provides the following new features:
* efficient random-access reads of arbitrary spans
* fairly efficient random-access writes to arbitrary spans
* allow more than one segment's worth of data
.. _1.9.0: .. _MDMF: Tahoe-LAFS needs people to do code reviews.  There is worry that the
mandatory code review process may be slowly down the project too much.
There are currently fifteen_ tickets which need review. Please help the
project out by reviewing a patch.  This page_ covers how to review a
.. _fifteen:  .. _page: Planning has begun for the First International Tahoe-LAFS Summit, to
be held June 27 to June 30, 2011 in San Francisco.
The volunteergrid2_ is looking for inaugural members who meet their
requirements of uptime and capacity.
.. _volunteergrid2: Peter Secor on behalf of AllMyData released_ the code for webdrive.
Webdrive_ is a JavaScript UI for Tahoe-LAFS that was developed by
.. _released: .. _WebDrive: Peter Secor on behalf of AllMyData released the source code for the
Tahoe-LAFS `iPhone client`_.
.. _iPhone client: Patrick McDonald volunteered to provide the Tahoe-LAFS Weekly News and
help with documentation.
Tahoe-LAFS is now one of the backends that git-annex_ can store files
in. git-annex is a tool that uses all of git's machinery for
identifying and tracking files, but stores the actual complete
contents of the files in a separate location instead of in the git
repo itself. See `The RelatedProjects Page`_ on the Tahoe-LAFS wiki
for other cool hacks.
.. _git-annex: .. _The RelatedProjects Page:
.. Please change this version number whenever you edit this file!
.. version: 2.1
Tahoe-LAFS Weekly News, issue number 1
   Welcome to the inaugural edition of the Tahoe-LAFS Weekly News, brought
   to you by Patrick McDonald and Zooko Wilcox-O'Hearn, scribes.
Announcements and News
   Brian Warner is the release manager for [1]1.9.0. The plan for release
   is late July with Kevan's Medium-Sized Distributed Mutable Files
   ([2]MDMF) being the major new change. MDMF will implement almost the
   same protocol and format as the current SDMF files. However it provides
   the following new features:
     * efficient random-access reads of arbitrary spans
     * fairly efficient random-access writes to arbitrary spans
     * allow more than one segment's worth of data
   Tahoe-LAFS needs people to do code reviews. There is worry that the
   mandatory code review process may be slowly down the project too much.
   There are currently [3]fifteen tickets which need review. Please help
   the project out by reviewing a patch. This [4]page covers how to review
   a patch.
   Planning has begun for the First International Tahoe-LAFS Summit, to be
   held June 27 to June 30, 2011 in San Francisco.
   The [5]volunteergrid2 is looking for inaugural members who meet their
   requirements of uptime and capacity.
   Peter Secor on behalf of AllMyData [6]released the code for webdrive.
   [7]Webdrive is a JavaScript UI for Tahoe-LAFS that was developed by
   AllMyData.
   Peter Secor on behalf of AllMyData released the source code for the
   Tahoe-LAFS [8]iPhone client.
   Patrick McDonald volunteered to provide the Tahoe-LAFS Weekly News and
   help with documentation.
   Tahoe-LAFS is now one of the backends that [9]git-annex can store files
   in. git-annex is a tool that uses all of git's machinery for
   identifying and tracking files, but stores the actual complete contents
   of the files in a separate location instead of in the git repo itself.
   See [10]The RelatedProjects Page on the Tahoe-LAFS wiki for other cool
   hacks.
   1.    2.    3.    4.    5.    6.    7.    8.    9.   10. p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2011-11-16 13:01:57
@_author: Zooko O'Whielacronx 
@_subject: [Freedombox-discuss] Tahoe-LAFS is not a filesystem 
I'm a developer on the Tahoe-LAFS project ( )
and I would be happy to see it used in Freedombox. The projects have
many goals in common. I'm also the founder of a startup that sells
Tahoe-LAFS ciphertext storage service:  .
I'm just writing this brief note to try to explain something -- Tahoe,
the Least-Authority File System fits better for file sharing,
streaming or network-attached-storage than it does for a normal, local
It should probably be moved from here, where it is currently listed
next to MooseFS:
To here:
or maybe here or here or here:
Where it can be listed next to things like ownCloud, SparkleShare,
BitTorrent, Tin Can Jukebox, etc.
The confusion is that Tahoe-LAFS *is* a file system, in a couple of
senses. It has arbitrarily nestable directories and files (unlike a
lot of newfangled storage systems), and it has an SFTP server, which
means you can point sshfs at it and have a reasonably good access to
it directly in your kernel VFS. You could also use PyFilesystem to
integrate it into your local filesystem.
*But*, while the compatibility and correctness of Tahoe-LAFS and its
SFTP server are pretty good, its performance characteristics as a
distributed, secure, fault-tolerant storage system are sometimes a bad
match for the expectations of user-space programs that access their
storage through the POSIX filesystem API. Some things work fine -- it
depends on your specific use case, but some access patterns that are
normal and efficient on a local filesystem like ext4 are painfully
slow or even infeasible on Tahoe-LAFS. For example, opening a file for
append, appending a few bytes, and then closing it again, and then
doing this over and over 10,000 times, will take about O(N) time and
O(N) space on ext4 (about 10,000 disk operations and about 10,000
units of storage on disk), but take about O(N**2) time and O(N**2)
space on Tahoe-LAFS (about 100,000,000 network operations and about
100,000,000 units of storage on the backend), which means the program
that does something like that will never finish and might use up a lot
of your secure cloud storage space.
Everybody wants to use Tahoe-LAFS through FUSE as though it were a
local filesystem, but nobody actually uses Tahoe-LAFS through FUSE, to
my knowledge. All the people who actually Tahoe-LAFS end up using it
less like it is a "filesystem" like ext4 and more like it is an
"application" for backing up, sharing, or hosting files.
(N.B. the tahoe-lafs developers actively try to support whatever it is
that our users demand, and so we continue to support and improve its
behavior when it is treated as though it were a local filesystem, even
though I don't exactly recommend it.)
Freedombox-discuss mailing list
Freedombox-discuss at lists.alioth.debian.org

@_date: 2011-09-02 00:24:56
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] A globally distributed peer-to-peer data archive 
I like the way you're thinking.
Of the ones you listed only Freenet and Tahoe-LAFS are things you can
use and are being actively developed, right? Also there is GNUnet. On
the Tahoe-LAFS wiki we have a page of links to related projects [1].
This is something we're doing in the Tahoe-LAFS project, for example
on our ticket  "introductory docs are confusing and off-putting".
You would be more than welcome to jump in and help. Try following the
"quick start" instructions [2] and tell us how it works for you.
Heh heh.
We have plans to do something along those lines, too:
Again, we'd love help! You can get major kudos points (which are
redeemable for gratitude tokens and awesomeness levels) by doing some
fairly simple hacks like "collect server capacities and put them on
the welcome page" (ticket  On the other hand if Ostromism and
warm fuzzy social encouragement isn't your thing, you might be able to
get cold hard cash (U.S. Dollars or BitCoins) by contributing patches
to Tahoe-LAFS in return for bounties. :-)
[1] [2] tickets mentioned in this letter:
 introductory docs
are confusing and off-putting
 collect server
capacities and put them on the welcome page
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2011-09-14 00:51:45
@_author: Zooko O'Whielacronx 
@_subject: [p2p-hackers] announcing Tahoe-LAFS v1.8.3, fixing a security issue 
We found a security vulnerability in Tahoe-LAFS (all versions from
v1.3.0 to v1.8.2 inclusive) that might allow an attacker to delete
files. This vulnerability does not enable anyone to read file contents
without authorization (confidentiality), nor to change the contents of
a file (integrity). How exploitable this vulnerability is depends upon
some details of how you use Tahoe-LAFS. If you upgrade your Tahoe-LAFS
storage server to v1.8.3, this fixes the vulnerability.
We've written detailed docs about the issue and how to manage it in
the "Known Issues" document:
I am sorry that we introduced this bug into Tahoe-LAFS and allowed it
to go undetected until now. We aim for a high standard of security and
reliability in Tahoe-LAFS, and we're not satisfied until our users are
safe from threats to their data.
We've been working with the packagers who maintain packages of
Tahoe-LAFS in various operating systems, so if you get your Tahoe-LAFS
through your operating system there may already be a fixed version
Please contact us through the tahoe-dev mailing list if you have
further questions.
Zooko Wilcox-O'Hearn
ANNOUNCING Tahoe, the Least-Authority File System, v1.8.3
The Tahoe-LAFS team announces the immediate availability of version 1.8.3 of
Tahoe-LAFS, an extremely reliable distributed storage system. Get it here:
Tahoe-LAFS is the first distributed storage system to offer
"provider-independent security" b meaning that not even the
operators of your storage servers can read or alter your data
without your consent. Here is the one-page explanation of its
unique security and fault-tolerance properties:
The previous stable release of Tahoe-LAFS was v1.8.2, which was
released January 30, 2011 [1].
v1.8.3 is a stable bugfix release which fixes a security issue. See the file
[2] and known_issues.rst [3] file for details.
WHAT IS IT GOOD FOR?
With Tahoe-LAFS, you distribute your filesystem across
multiple servers, and even if some of the servers fail or are
taken over by an attacker, the entire filesystem continues to
work correctly, and continues to preserve your privacy and
security. You can easily share specific files and directories
with other people.
In addition to the core storage system itself, volunteers
have built other projects on top of Tahoe-LAFS and have
integrated Tahoe-LAFS with existing systems, including
Windows, JavaScript, iPhone, Android, Hadoop, Flume, Django,
Puppet, bzr, mercurial, perforce, duplicity, TiddlyWiki, and
more. See the Related Projects page on the wiki [4].
We believe that strong cryptography, Free and Open Source
Software, erasure coding, and principled engineering practices
make Tahoe-LAFS safer than RAID, removable drive, tape,
on-line backup or cloud storage.
This software is developed under test-driven development, and
there are no known bugs or security flaws which would
compromise confidentiality or data integrity under recommended
use. (For all important issues that we are currently aware of
please see the known_issues.rst file [3].)
This release is compatible with the version 1 series of
Tahoe-LAFS. Clients from this release can write files and
directories in the format used by clients of all versions back
to v1.0 (which was released March 25, 2008). Clients from this
release can read files and directories produced by clients of
all versions since v1.0. Servers from this release can serve
clients of all versions back to v1.0 and clients from this
release can use servers of all versions back to v1.0.
This is the fourteenth release in the version 1 series. This
series of Tahoe-LAFS will be actively supported and maintained
for the forseeable future, and future versions of Tahoe-LAFS
will retain the ability to read and write files compatible
with this series.
You may use this package under the GNU General Public License,
version 2 or, at your option, any later version. See the file
"COPYING.GPL" [5] for the terms of the GNU General Public
License, version 2.
You may use this package under the Transitive Grace Period
Public Licence, version 1 or, at your option, any later
version. (The Transitive Grace Period Public Licence has
requirements similar to the GPL except that it allows you to
delay for up to twelve months after you redistribute a derived
work before releasing the source code of your derived work.)
See the file "COPYING.TGPPL.html" [6] for the terms of the
Transitive Grace Period Public Licence, version 1.
(You may choose to use this package under the terms of either
licence, at your option.)
Tahoe-LAFS works on Linux, Mac OS X, Windows, Cygwin, Solaris,
*BSD, and probably most other systems. Start with
"docs/quickstart.html" [7].
HACKING AND COMMUNITY
Please join us on the mailing list [8]. Patches are gratefully
accepted -- the RoadMap page [9] shows the next improvements
that we plan to make and CREDITS [10] lists the names of people
who've contributed to the project. The Dev page [11] contains
resources for hackers.
Atlas Networks has contributed several hosted servers for performance
testing. Thank you to Atlas Networks for their generous and public-spirited
HACK TAHOE-LAFS!
If you can find a security flaw in Tahoe-LAFS which is serious
enough that we feel compelled to warn our users and issue a fix,
then we will award you with a customized t-shirts with your
exploit printed on it and add you to the "Hack Tahoe-LAFS Hall
Of Fame" [12].
This is the eighth release of Tahoe-LAFS to be created solely
as a labor of love by volunteers. Thank you very much to the
team of "hackers in the public interest" who make Tahoe-LAFS
Zooko Wilcox-O'Hearn
on behalf of the Tahoe-LAFS team
September 13, 2011
Boulder, Colorado, USA
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2012-04-24 01:37:26
@_author: Zooko Wilcox-O'Hearn 
@_subject: [cryptography] what do you get when you combine Phil Zimmermann, 
Continually nowadays I think I'm living in one of the science fiction
novels of my youth. This one is by Neal Stephenson, I think.
cryptography mailing list
cryptography at randombit.net

@_date: 2012-08-21 16:08:25
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Tahoe-LAFS Weekly Call notes 
We had another Tahoe-LAFS Weekly Call. Here are my notes, which are
patchy and could be inaccurate. You could check this publicly editable
notepad for updates from the other attendees of the call.
In attendance: Brian, David-Sarah, Zooko
About leasedb schema and Python code:
b" Use ascii-encoded or binary blobs in the leasedb (sqlite db)?
b" Use ascii-encoded or binary strings to hold things like shareids
inside the Python interpreter?
About accounting:
Server admin should be able to see aggregate usage per account.
We have read-only admin WUIs because we don't yet have a good
technology to authorize the administrator to access a WUI without
exposing the same access to CSRF attacks. (Brian has a prototype
solution in his "toolbed" project.)
Use cases of sharing:
b" Club
b" For-profit service (e.g. Least Authority)
Individual pairwise storage relationships:
b" Social (as long as there is visibility and control, Bob doesn't need
to feel like he needs something explicit specific in exchange)
b" Money
b" Tit for Tat
b" 3-way: Bob is running a server, Alice is running a client. She is
also, separately running a server (or hiring someone else to run a
server that Bob can use). So, Bob wants to let Alice's client use his
server because he knows that she is responsible for that other server
that he can use.
Somewhere along the line there is going to be a graph of who likes
whom -- who has accepted storage obligations for whom.
Eventually Brian wants to provide tools at *least* to visualize, and
ideally to manage, this network of social relationships.
People could, perhaps configure their node to give anybody storage
space as long as that person is giving you at least X% as much storage
space (Tit for Tat).
The three messages are:
1. I will accept shares from this other person.
2. I'm willing to send shares to this other person.
3. I'm working for this other person (as a storage server).
Next week:
b" Try some alternate tech such as Google Hangouts or Skype? POTS
quality is bad enough to interfere with communication.
b" More about accounting relationship management -- present the
higher-altitude picture of the roadmap from Brian's mind.
The short-term decision that we have to make is whether to have one
key or two keys -- a separate key for the client and for the server.
After David-Sarah rang off for dinner, Zooko made the following
proposal to Brian as a "baby step". Zooko's motivation is that this
would be simple to understand (especially for non-Brian people), and
useful (e.g. to volunteergrid2 folks), and hopefully
forward-compatible with the better "invitation protocol" design that
Brian has in mind.
Baby Step Proposal:
There is a file named "clients.txt" which is edited by a human and is
treated as read-only by the Tahoe-LAFS storage server (and is ignored
entirely by Tahoe-LAFS the storage client). It is a text file with one
record per line. Each record is a complete ascii-encoded public key
followed by an optional whitespace and pet name.
If your client's public key appears in the server's "clients.txt"
file, then your storage usage gets accounted for and displayed to the
storage server owner with his petname for you. If your client's public
key does not appear in that clients.txt, then your storage usage goes
into the "open, anonymous" accounting bucket (or else maybe gets
tracked under your public key?).
Or, the storage server operator can turn on the mode where if your
client's public key does not appear in his "clients.txt" file, then it
refuses to let you store data there at all.
There is an analogous file named "servers.txt" which is read but not
written by Tahoe-LAFS storage clients and is ignored entirely by
Tahoe-LAFS storage servers. This file contains a list of public keys
and optional petnames for storage servers. In "backwards compatibility
mode", it tracks which of those servers you store how much data with.
In "strict mode", it refuses to store data with storage servers that
don't have public keys in that file.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-08-29 13:58:57
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Tahoe-LAFS Weekly Call report 2012-08-29 
Here is your traditional, once-a-week, cryptic, and potentially
inaccurate summary of the developer conference call. In what follows I
make no attempt to explain the context. Sorry -- don't have time right
now! However, we do try to explain contexts on tickets, so if this
sounds interesting to you, you can probably figure out what we're
talking about by reading       and/or
The ultimate goal is to allow you to view arbitrary content when
loaded from your Tahoe-LAFS gateway without allowing it to spy on or
interfere with other content also loaded from your Tahoe-LAFS gateway.
(And by "view" content I mean execute that content as a program.) This
is good to prevent exploits of "decentralized web apps" (such as my
blog: B9). Note that decentralized web apps hosted on Tahoe-LAFS are
_already_ pretty well-defended from one another. The unguessable URIs
go a long way to preventing a lot of attacks. This work is just about
closing off that one weird attack (which requires the user to navigate
*from* the prize content *to* the malicious content for it to work)
and also about providing Defense In Depth so that other unforeseen
attacks will be defeated before we even imagine them.
B9  Cap URLs leaked via
HTTP Referer header
 Can JavaScript
loaded from Tahoe access all your content which is loaded from Tahoe?
 A script in a file
viewed through the WUI can obtain the file's read cap
 Put file download
links ('?save=true') in WUI directory listings
 WUI: ambiently
accessible pages should framebust in order to prevent UI redressing
 WUI: view content
in an HTML5 sandboxed iframe
 Segregate gateway
HTTP ports: one for raw bytes and one for generated WUI pages
Tahoe-LAFS Developer Conference Call
In attendance: Zooko, David-Sarah, Brian
scribe: Zooko
 defense against malicious javascript
Using html5 iframe sandboxing, each request to the gateway to load the
content of a cap "$CAP" actually returns a generated HTML page
containing a sandboxed iframe tag with src=SERVER:BYTESPORT/cap/$CAP.
b" Kaminsky back-jacking
b" an additional authority when the attacker knows the readcap, but we
want to prevent him from causing your browser to execute the contents
in certain context? / David-Sarah's weird idea of an extra key per
b" backward compatibility with scripts/apps that use the WAPI to load
raw bytes of caps?
What authority to I intend to extend to target content when I click on
its link in source content?
proposal: I intend for the target content to receive *no more
authority* when I click on its link than if I had typed in its URL.
Ways that this is not what the web currently does:
b" Click on a little question mark next to a field, it pops open a very
small explanation. If you go back to the original window and click the
question mark again, it will pop open the same explanation window
instead of a new one.
b" Generate a virtual history when everything is really /app
possible threat models:
Game 1: There is a secret prize cap, and you -- the attacker -- win if
any code of yours learns the secret cap. I will run your code, and
navigate around however you like, as a user. (The Kaminsky
back-jacking attack is a successful attack in this sort of game. It
requires that the user will navigate *from* the secret prize document
*to* attacker-controlled content.)
Game 2: There is a secret prize cap, and the user has it loaded *from
the barenakedcap port* in a window. Then the user navigates from that
content to an attacker-provided link. The link can, at the attacker's
discretion, point to attacker-controlled content stored in LAFS. So,
if the link goes to a barenakedcap containing attacker-controlled
content, then the Kaminsky back-jacking attack will work, letting that
attacker-controlled content gain access to the prize cap. But if the
link goes to an HTML5 sandbox wrappedcap, our use of HTML5 sandboxing
will prevent the resulting attacker-controlled content from gaining
the prize cap.
David-Sarah had an idea for a defense that might help in this Game 2
or in a related game, involving another encryption key which is known
to this gateway and unknown to the attacker. Brian argued that the
attacker-controlled content, that gets loaded in this gateway and
given access to that content, will eventually learn any such key or
any such transformation of the URL. Zooko proposed one-time barenaked
URLs -- when the gateway receives a request to load a wrappedcap, then
it generates an unguessable nonce, serves up the HTML5 sandbox wrapper
which includes , and
over on the barenakeddata port, it will answer *only* the first
request for $NONCE by serving up the content. (Any subsequent requests
will get some kind of error.)
After the call was over, I, Zooko, began to wonder what's the point of
the barenakeddata port at all, and if the sandbox port could, instead
of including a src= link pointing to the one-time-URL on the
barenakeddata port,  instead include the actual content in the script
tag. I think Brian mentioned something like that on the call, about
having the gateway put all of the content into a data URL to put into
the script tag, or something. One possible motivation to have the
barenakeddata port at all is to support applications that need access
to the pristine, bitwise exact data, such as in order to download it
to disk. It is kind of funny that we want to have an HTTP server to
serve up pure data, but we never, never want a full-fledged HTTP
client (which interprets Javascript and HTML and all that) to look at
it! Full-fledged HTTP clients should only look at the other HTTP
server -- the one that serves up sandboxed content. Only custom
scripts should look at the one that serves up pure bytes.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-12-13 14:03:40
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Weekly Dev Chat notes, 2012-12-13 
We decided not to do the hangout "on air" this time, as some people
feel like they would be inhibited in their conversation if it were
recorded and published like that. Maybe we could try an experiment in
putting it "on air" at some point.
in attendance: Zooko (scribe), Marlowe, CodesInChaos, David-Sarah, Brian
On internationalization: marlowe is following up with runa to get
instruction in how the workflow works for the Tor project and set up
the same thing for the Tahoe-LAFS project; a man whose name I didn't
catch will hopefully volunteer to translate it into Arabic. Brian will
ask around Mozilla how they do it and Brian and Marlowe can compare
notes. (Tor and Mozilla are two projects that do this sort of thing
quite successfully.)
* Zooko wants to get  closed. It is a critical issue for those
that it strikes (which includes LeastAuthority.com customers), and
there is a patch. We just need a unit test. Zooko started writing unit
tests for NodeMaker during the call. Brian found some extants tests
for NodeMaker, in test_client.py.
* marlowe is working on documentation improvements, hopefully to go
into the 1.10 release.
* marlowe is starting a glossary. He'll maintain it on the wiki, but
then we'll copy a snapshot of it into the source release. Marlowe
figured out that you can put restructured-text into trac wiki with
"{{{ Ideally, we maintain restructured-text formatted glossary
in the wiki, and then copy exactly the same restructured-text file
into the docs/ directory before making a source release.
Wiki vs. revctrl ? We've traditionally wanted to put docs in revision
control to replicate them to users, make them linked to the version of
the source code that they come with, and maintain a useful history of
the docs. But we've also traditionally wanted to put docs on the wiki
in order to make it easier for people to edit them. Maybe github will
satisfy both goals! Brian pointed out that github is introducing an
"edit right here on this page" feature so that it is even easier for
people to contribute patches.
Brian showed up late, but we wanted to talk to him, so we had an extra
long call.
* CoolNewHashFunction to be announced soon! The pitch is that it is a
modern, secure hash, comparably secure to SHA-3, but it is faster than
MD5. On the best-suited platform, with parallelized computation and
the wind at your back, it might be up to 10 times as fast as SHA-256!
Brian pointed out that this might not make any difference -- secure
hashing is probably not the bottleneck in Tahoe-LAFS. Zooko laughingly
countered that this is because our network protocol is so inefficient.
Brian said we have some measurements of encryption and erasure coding
(see "Recent Uploads and Downloads" in the WUI) but not of hashing
because hashing gets interleaved with other operations so it isn't
that easy to measure. Zooko said there was a cool Master's Thesis by
Eirik Haver and Pel Ruud in which they attempted to measure the effect
of the secure hash function on Tahoe-LAFS's performance:
Brian volunteers to be Release Manager for Tahoe-LAFS v1.10.0! Hooray!
There was much rejoicing!
* There are a few other tickets that we *really* want to get in --
  and  -- either because they are already done and
just need to be merged or because they threaten forward-compatibility
issues if we don't fix them before the release.
Next week's Tahoe-LAFS Weekly Dev Chat will be about Tahoe-LAFS v1.10.
Be there or be square!
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-02-10 16:42:47
@_author: Zooko Wilcox-O'Hearn 
@_subject: [Freedombox-discuss] Freedombox For Cloud Services 
Hi! I'm a developer on the Tahoe-LAFS project.
I don't understand this, and can't say whether I think these two
differences correctly apply to Tahoe-LAFS or not.
Thank you! We certainly work hard at making it reliable and secure.
We're also open to accepting patches that make Tahoe-LAFS better for
your use-case, as long as it doesn't complicate or destabilize the
use-cases of others.
Also, I love the idea of the FreedomBox and would like to see it
become useful to a large number of people.
The basic idea of serving storage from a FreedomBox doesn't sound
incompatible with Tahoe-LAFS architecture to me. One thing that I
would be careful of is not to try to use adhoc storage servers whose
only qualification is "they managed to turn it on and run the
software". This sort of principle of inclusion works great for
transfer, e.g. Bittorrent, but not for storage. Unqualified nodes are
too numerous and too unreliable to be any use for storage. They do
more damage than good. I speak from long and hard experience, having
been an architect of several successive notable failed projects, each
of which attempted to take advantage of unqualified, random nodes to
provide storage.
But, FreedomBox storage servers do not have to be adhoc, unqualified
storage servers. Anything which serves to filter out most of the 99%
of bad servers would do. Possible ways to filter which storage servers
you'll upload your ciphertext to:
1. The storage client has to manually add the storage server ID to a
list of storage servers that she is willing to entrust ciphertext to.
(This is basically what Tahoe-LAFS already offers, although it is
currently mediated by a centralized thing called the "introducer".)
2. The storage server has to be blessed by some trusted authority
before the storage clients will use it. (Again, the current introducer
could sort of be pressed into service for this, but we're actively
working on upgrading that to a more decentralized, flexible, and
secure solution.)
3. The storage server has to have some aggregate "good reputation"
among some collection of users.
4. The storage server is owned by someone in your social network. :-)
I like this idea. When you befriend someone, that automatically grants
them a certain amount of storage space on your FreedomBox.
6. The storage server has to demonstrate 30 consecutive days of better
than 95% uptime before the storage client will entrust storage to it.
7. The storage server has to give a micropayment, let's say one dollar
or one Bitcoin, to the client before the client will start using it.
Then, the client will start paying the server for service. Within a
few weeks or months the server will have earned back its dollar plus
more. But, the initial requirement of a payment in the wrong direction
serves to filter out most of the 99% of servers that are completely
useless. I just came up with this crazy notion just now.
I would encourage people interested in hacking on FreedomBox to try
installing Tahoe-LAFS on it and give it a spin. There's nothing like
practical, hands-on experience to inform your ideas.
However, while you're waiting for it to run its extensive and
time-consuming unit tests, you could also browse some hifalutin ideas:
Freedombox-discuss mailing list
Freedombox-discuss at lists.alioth.debian.org

@_date: 2012-07-11 10:09:56
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] weekly Tahoe dev call report: 2012-07-10 
b" 1.9.2 release! Yay David-Sarah, Release Manager. Thanks to everyone
who contributed bug reports, patches, testing, packaging, etc.
b" Add-only sets: Can servers exercise "editorial power" over add-only
sets, remixing different legitimate adder-signed sets to form new
sets? Zooko thinks this could be a problem, and that add-only sets
should be designed against it, but he can't remember why he thinks
that. Brian thinks that it is hardly a problem because the presence of
other servers giving answers renders any one server's ability to
select among legitimate answers almost moot. Andrew and David-Sarah
both think that the notion of a *set* as opposed to a fully serialized
sequence must surely require that readers accept unions. We agreed to
drop the subject for now and move on to lease database.
b" lease database
   b" keep information about leases in some separate location instead
of bundled with each share
   b" let's use a sqlite db through the pysqlite API, like we do with backupdb
   b" for the cloud backend (that Least Authority Enterprises is
building), the leasedb will be stored on persistent storage e.g Amazon
Elastic Block Store (EBS), while the shares are stored on cloud
storage, e.g. Amazon Simple Storage Service (S3).
   b" people can manually add shares, such as by just dropping a share
file into a disk backend filesystem, or uploading a share object to
Amazon S3, and the lease system will eventually discover them and
maintain them as long as they are leased, and then delete them when
they are no longer leased.
   b" people can manually delete shares, such as by just rm'ing a share
file from a disk backend filesystem, or deleting a share object to
Amazon S3, and the lease system will not break when it discovers that
it is gone.
   b" There can be race conditions between such external actions and
the progress of the crawler which is inspecting leases. A state
machine must be carefully analyzed to see that in handles all such
possible sequences of events. See
for initial notes about that. A state transition diagram would be a
good way to analyze and communicate that.
   b" Brian was about to write a new lease database as the next step in
his Accounting work, and Least Authority Enterprises is about to write
a new lease database as the next step in our DARPA research grant
contract, with a deadline of July 26. So, let's cooperate. We need to
agree on separation of responsibilities.
   b" The crawler can be a "background task" that doesn't take up
resources (CPU) for too long at a time, so there's a configurable knob
for "how many seconds in a row do I run" and "how many seconds do I
idle in between runs", and another knob for "how soon should I start a
new pass after I've already finished the last pass".
   b" Should "account ids" or "lease-owner ids" be public keys or
things derived from symmetric secrets? Brian wisely suggests
decoupling that question from the rest of the lease db design. But,
then Brian and David-Sarah agreed that the lease-owner ids should be
small integers. Zooko disagrees, but nobody asked him.
   b" Least Authority Enterprises might someday want to store their
lease databases in funky cloud databases things like Amazon's Cloud
SQL DB or Microsoft's Cloud SQL DB. But for now we're just going to
use pysqlite and local storage such as Amazon EBS.
Brian will review his branch and write up some stuff about how the
accounting branch ought to go. Brian and David-Sarah will
synchronously work on it Tuesday and Thursday.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-07-24 19:56:44
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] notes from the Tahoe-LAFS Weekly Call 
notes from Tahoe-LAFS Weekly Call, 2012-07-24
In attendance: David-Sarah, Brian, Zooko
(appending a copy below)
Summary of the summary: Brian and David-Sarah are hard at work on the
accounting and lease database development, part of which goes to
satisfy Least Authority Enterprises's obligations to DARPA, and we're
planning to start the process of making Tahoe-LAFS v1.10 soon
notes from Tahoe-LAFS Weekly Call, 2012-07-24
In attendance: David-Sarah, Brian, Zooko (first scribe -- pinkish
purplish words)
David-Sarah updated (merged) cloud-backend to current trunk. It would
be helpful if Brian would bring accounting-2 up to date with trunk.
David-Sarah needs to thoroughly go through Brian's code.
David-Sarah has a better mental model of the statemachine of the lease
db than Brian has, so David-Sarah will start writing that code.
One of the next things to do is to make the backend fully asynchronous
without yet making the other changes.
Brian will take David-Sarah's patches to asynchrify the local storage
and merge those to trunk and review them and finish them as needed.
A ShareSet is a set of shares from the same file, stored on a single
server. (This was formerly known in the code, but inconsistently, as a
The order we're doing things in:
1. some combination of:
 1a: Make the storage/crawler tests asynchronous.
 1b: split the storage server into frontend (Referenceable) and
backend (disk/S3) pieces
 1c: make the backend interfaces async (return Deferred from everything)
 1d: make all the tests work again
2: split frontend into even-more-fronty Accounts (Referenceable,
implements RIStorageServer) and slightly-less-fronty Server (not
Referenceable, takes Account object as arg of each request)
3: create leasedb, crawler + share-state-machine
4: publish a single shared "Anonymous" account to clients
5: DARPA milestone: all accesses use the Anonymous account, external
behavior is same as before
6: add Account-acquisition (furlification) process, client keys. New
clients will now use non-anonymous accounts (old clients, if
permitted, will still use the anonymous account).
7: add some kind of UI to display per-account usage
8: add UI to control per-account usage (enable/disable)
Currently mutable files are exempt from storage limits.
The lease db will store the size of each mutable share. We don't mind
if the leasedb trusts its stored share size instead of inspecting the
filesystem to see if that changed out from under it.
Q: What does the cloud backend do when you modify a mutable share?
A: Modification of each chunk is atomic (thanks to S3's semantics) but
the change of multiple chunks at once in a given share is not atomic.
D-S and Zooko, at least, prefer not to try to write changes to mutable
share atomically for now, but just to overwrite them simply and
efficiently and non-atomically. (Ultimately we want end-to-end
two-phase commit!)
Mutable-share storage backend writev() method will be changed to
return the new share size (backend bytes consumed) in the Deferred
that fires when the write finishes. The server will then update the
leasedb with the new size. So out-of-band share *modifications* (not
creations/deletions) will result in inaccurate accounting data until
the share is next modified, at which point it will be fixed. We're ok
with that inaccuracy. (the share-crawler will make sure that
creation/deletion is noticed sooner).
What about the 1.10 release?
Brian needs to look at the current trunk and see if he's content with
the introducer changes. In particular, changing server key to node
Need to follow-up on the bigger discussion about peer-to-peer vs.
client-server, and to what extent the changes we're doing now interact
with that.
For the most part, the changes we're deploying now don't constrain
those future design decisions.
The question of reciprocal accounting agreement: my server will hold
data for your client, *because* your server is holding data for my
client. Should we implement that specifically, by making your client
and server use the same private key, and my server uses that fact to
decide whether to give your client storage? Or should we implement the
more general case?
David-Sarah and Zooko are a bit skeptical of the utility of that
specific reciprocal case. Brian, too.
When I last dove into accounting about 6-12 months ago, I got stuck
trying to figure out how servers would express "please give any
reciprocal benefit for the data I'm holding for you to that client
over there, because I'm working for him". Something like client1 pays
server2 to hold data (in the "rent-a-friend" case), client3 runs
server4, server2 is holding data for client3, so client1 ought to be
able to store data on server4. Each server needs a number of "credit
goes to client X" pointers, and clients probably need something
similar. It got too hairy to think about. The simpler
every-node-is-both-client-and-server case was easier to handle. But
this may be a simplification that we can't afford to make.. maybe we
just have to figure out the full case.
Maybe at the next Summit we could look at accounting from the user
experience perspective -- how do people turn it on, how do they
express their intent about which of their friends to give service to.
Brian told a story about an economist for a MMORPG, who investigated
why some trades had prices way out of market range. He decided that
some of them were people giving items to their friends. The economist
said that money can't pay all debts. Suppose Grandma hosts
Thanksgiving dinner and puts on a feast, and she expects the kids to
do the dishes and cleanup. There is no polite way to ask how much
money would you have to pay to get out of doing the dishes. Similarly,
there are at least two major use cases that we're already supporting:
Friendnets and pay-for-usage services like LAE. If we made a new thing
in which the question of "Why does my server give you service?" can
*only* be answered with money then that would probably exclude Tahoe
from being used in those friendnets. (Zooko joked that he has already
tried that and it didn't work -- alluding to Mojo Nation.)
 (I'm trying to find the blog post this story came from, it probably
came across my twitter stream in the last two weeks, maybe from
amiller or another economics fan. The point was that "money is a
universal exchange medium" isn't actually true).
So, Brian went on, the first steps into the new world of accounting
should include providing visibility to the user about what resources
are being offered by who to whom and so on.
 . Second step is
coarse control over usage (accept/deny/delete). Then comes
finer-grained control (gradations of sanctions), more public
information about usage, more social visibility+control. *then*
In the last couple of minutes of the call there was a brief discussion
of XSalsa20 in Tahoe-LAFS v1.10.0. Zooko is keen to get XSalsa20b
in ASAP, but David-Sarah (Release Manager for v1.10.0) says that v1.10
is supposed to be only things that are already ready and in trunk when
we are ready to begin making a new release (i.e., after the Milestone
3 delivery to DARPA). Zooko conceded that XSalsa20b
AES is not already
ready and in trunk...
notes from Tahoe-LAFS Weekly Call, 2012-07-24
In attendance: David-Sarah, Brian, Zooko (first scribe -- pinkish
purplish words)
David-Sarah updated (merged) cloud-backend to current trunk. It would
be helpful if Brian would bring accounting-2 up to date with trunk.
David-Sarah needs to thoroughly go through Brian's code.
David-Sarah has a better mental model of the statemachine of the lease
db than Brian has, so David-Sarah will start writing that code.
One of the next things to do is to make the backend fully asynchronous
without yet making the other changes.
Brian will take David-Sarah's patches to asynchrify the local storage
and merge those to trunk and review them and finish them as needed.
A ShareSet is a set of shares from the same file, stored on a single
server. (This was formerly known in the code, but inconsistently, as a
The order we're doing things in:
1. some combination of:
 1a: Make the storage/crawler tests asynchronous.
 1b: split the storage server into frontend (Referenceable) and
backend (disk/S3) pieces
 1c: make the backend interfaces async (return Deferred from everything)
 1d: make all the tests work again
2: split frontend into even-more-fronty Accounts (Referenceable,
implements RIStorageServer) and slightly-less-fronty Server (not
Referenceable, takes Account object as arg of each request)
3: create leasedb, crawler + share-state-machine
4: publish a single shared "Anonymous" account to clients
5: DARPA milestone: all accesses use the Anonymous account, external
behavior is same as before
6: add Account-acquisition (furlification) process, client keys. New
clients will now use non-anonymous accounts (old clients, if
permitted, will still use the anonymous account).
7: add some kind of UI to display per-account usage
8: add UI to control per-account usage (enable/disable)
Currently mutable files are exempt from storage limits.
The lease db will store the size of each mutable share. We don't mind
if the leasedb trusts its stored share size instead of inspecting the
filesystem to see if that changed out from under it.
Q: What does the cloud backend do when you modify a mutable share?
A: Modification of each chunk is atomic (thanks to S3's semantics) but
the change of multiple chunks at once in a given share is not atomic.
D-S and Zooko, at least, prefer not to try to write changes to mutable
share atomically for now, but just to overwrite them simply and
efficiently and non-atomically. (Ultimately we want end-to-end
two-phase commit!)
Mutable-share storage backend writev() method will be changed to
return the new share size (backend bytes consumed) in the Deferred
that fires when the write finishes. The server will then update the
leasedb with the new size. So out-of-band share *modifications* (not
creations/deletions) will result in inaccurate accounting data until
the share is next modified, at which point it will be fixed. We're ok
with that inaccuracy. (the share-crawler will make sure that
creation/deletion is noticed sooner).
What about the 1.10 release?
Brian needs to look at the current trunk and see if he's content with
the introducer changes. In particular, changing server key to node
Need to follow-up on the bigger discussion about peer-to-peer vs.
client-server, and to what extent the changes we're doing now interact
with that.
For the most part, the changes we're deploying now don't constrain
those future design decisions.
The question of reciprocal accounting agreement: my server will hold
data for your client, *because* your server is holding data for my
client. Should we implement that specifically, by making your client
and server use the same private key, and my server uses that fact to
decide whether to give your client storage? Or should we implement the
more general case?
David-Sarah and Zooko are a bit skeptical of the utility of that
specific reciprocal case. Brian, too.
When I last dove into accounting about 6-12 months ago, I got stuck
trying to figure out how servers would express "please give any
reciprocal benefit for the data I'm holding for you to that client
over there, because I'm working for him". Something like client1 pays
server2 to hold data (in the "rent-a-friend" case), client3 runs
server4, server2 is holding data for client3, so client1 ought to be
able to store data on server4. Each server needs a number of "credit
goes to client X" pointers, and clients probably need something
similar. It got too hairy to think about. The simpler
every-node-is-both-client-and-server case was easier to handle. But
this may be a simplification that we can't afford to make.. maybe we
just have to figure out the full case.
Maybe at the next Summit we could look at accounting from the user
experience perspective -- how do people turn it on, how do they
express their intent about which of their friends to give service to.
Brian told a story about an economist for a MMORPG, who investigated
why some trades had prices way out of market range. He decided that
some of them were people giving items to their friends. The economist
said that money can't pay all debts. Suppose Grandma hosts
Thanksgiving dinner and puts on a feast, and she expects the kids to
do the dishes and cleanup. There is no polite way to ask how much
money would you have to pay to get out of doing the dishes. Similarly,
there are at least two major use cases that we're already supporting:
Friendnets and pay-for-usage services like LAE. If we made a new thing
in which the question of "Why does my server give you service?" can
*only* be answered with money then that would probably exclude Tahoe
from being used in those friendnets. (Zooko joked that he has already
tried that and it didn't work -- alluding to Mojo Nation.)
 (I'm trying to find the blog post this story came from, it probably
came across my twitter stream in the last two weeks, maybe from
amiller or another economics fan. The point was that "money is a
universal exchange medium" isn't actually true).
So, Brian went on, the first steps into the new world of accounting
should include providing visibility to the user about what resources
are being offered by who to whom and so on.
 . Second step is
coarse control over usage (accept/deny/delete). Then comes
finer-grained control (gradations of sanctions), more public
information about usage, more social visibility+control. *then*
In the last couple of minutes of the call there was a brief discussion
of XSalsa20 in Tahoe-LAFS v1.10.0. Zooko is keen to get XSalsa20b
in ASAP, but David-Sarah (Release Manager for v1.10.0) says that v1.10
is supposed to be only things that are already ready and in trunk when
we are ready to begin making a new release (i.e., after the Milestone
3 delivery to DARPA). Zooko conceded that XSalsa20b
AES is not already
ready and in trunk...
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-06-07 22:30:04
@_author: Zooko Wilcox-O'Hearn 
@_subject: [Freedombox-discuss] Tahoe-LAFS is like a Bittorrent with which 
Hm, I'm not exactly sure. In general, people tend to allocate large
amounts of disk space to one Tahoe-LAFS storage server, like at least
0.5 TB, and then not worry about the few KB or MB of overhead that
Tahoe-LAFS adds.
The storage server process is the one that stores ciphertext on disk
and sends and receives ciphertext over TCP on behalf of a storage
client. It stores each piece of ciphertext in a separate file in its
local filesystem, and it organizes those files into 2-level hierarchy
of directories. That's a source of overhead, since it takes a little
more than X bytes of disk space to store an X byte file in your file
system. That overhead is proportional to the number of files stored
and the exact amount is determined by the filesystem, but I would
estimate 2 KiB per file.
Another source of overhead is that the storage server maintains a
couple of small files of metadata about which files it has processed.
That is a small fixed amount that doesn't grow with the number or size
of files. It's probably around 10 KB.
Another source of overhead is per-file metadata -- leases (markings
showing that someone said "I'm interested in this file -- please don't
garbage-collect it for at least 30 more days!"), and
integrity-checking hashes. The integrity checks are proportional to
the size of each file, but small. It's about 64 bytes of hashes for
each 40 KB of file.
My instinct would be not to try to make use of such scarce storage
resources, but instead say "Want to contribute to the great FreedomBox
Cloud Storage grid? Then plug in a 1 TB hard drive to the USB port!".
That would actually make the resulting storage much more reliable,
because only people who felt a certain level of willingness to
contribute would run a Tahoe-LAFS storage server at all. By excluding
all the people who turned it on at some point and then got bored a few
hours later and turned it off again, we would greatly reduce the churn
and unreliability.
Freedombox-discuss mailing list
Freedombox-discuss at lists.alioth.debian.org

@_date: 2012-06-13 02:57:59
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] verification of subset of file == proof of 
Over on the Bitcoin discussion forums (warning: wretched hive of scum
and villainy), someone was asserting that they wanted a "proof of
retrievability" protocol and saying that, while they hadn't looked,
they were pretty sure Tahoe-LAFS didn't do it right:
I was mildly annoyed by this, because actually we have some extremely
strong features along those lines.
However, when I wrote a reply explaining exactly what we do have, I
was forced to admit that it isn't fully there yet. We already have
verification of complete files (although see  but we don't have
verification of a randomly-chosen subset of a file, which would be a
"Proof of Retrievability". See below for the message I posted to the
Bitcoin forum.
See also an old rant of mine complaining that academic cryptographers
have failed to study the papers and documentation of Tahoe-LAFS
closely enough to realize that there is most of a
proof-of-retrievability in there:
 make immutable
check/verify/repair and mutable check/verify work given only a verify
------- message I posted to the Bitcoin forum
Downloading only a subset of a file is already implemented, but there
isn't a command implemented that says "pick a random segment of this
file, download it from that server, and let me know if it passed
integrity checks". You can approximate it with the current Tahoe-LAFS
client, like this (the following lines that begin with "$" is me
typing in stuff as though I were using a bash prompt):
1. Pick a random spot in the file. Let's say the file size is 10 MB:
$ FILESIZE=29153345
$ python -c "import random;print random.randrange(0, $FILESIZE)"
2. Fetch the segment that contains that point. Segments are (unless
you've tweaked the configuration in a way that nobody does) 128 KiB in
size, so this will download the 128 KiB of the file that contain byte
number 2451799 and check the integrity of all 128 KiB:
$ curl --range 2451799-2451799
Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100     1    0     1    0     0      0      0 --:--:--  0:00:01 --:--:--     0
00000000  a9                                                |.|
As you can see it took only a second to download and emitted only one
byte to stdout, but it downloaded and verified the integrity of the
128 KiB segment that contained that byte.
If you are using multiple servers, using Tahoe-LAFS's awesome erasure
coding feature to spread out the data among multiple servers, then
this will download the data from the 3 fastest servers (unless you've
changed the default setting from "3" to some other number). There is
no good way to force it to download the data from specific servers in
order to test them -- it always picks the fastest servers. You can see
which server(s) it used by looking at the "Recent Uploads and
Downloads" page on the web user interface, which will also tell you a
bunch of performance statistics about this download.
In short, this feature is *almost* there. We just need someone to
write some code to do this automatically in the client (which is
written in Python) instead of as a series of bash commands. Also this
code should download one (randomly chosen) block from every server it
can find instead of from just the three fastest servers, and it should
print out a useful summary of what it tried and which servers had good
Oh, there is a different function which does print out a useful
summary of results -- the "verify" feature. But, that downloads and
tests every block instead of just one randomly chosen block. Another
way to implement this is to add an option to that to indicate how many
blocks it should try:
$ time tahoe check --verify
Summary: Healthy
 storage index: 7qhuoagk4z4ugsjkjgjcre6sx4
 good-shares: 1 (encoding is 1-of-1)
 wrong-shares: 0
real    1m2.705s
user    0m0.570s
sys     0m0.060s
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-06-14 15:03:52
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Proposed short description of tahoe-LAFS for 
How about:
Tahoe-LAFS is not just a backup tool, but rather a distributed file
system. It also comes with an integrated backup tool.
Nicely written.
This is fine and I don't think you need to change it, but for your
information whenever I see the word "trust" a little warning flag goes
up in my brain, and I go back and mentally rewrite the sentence
without the word "trust". This is because that word combines two
things: 1. Whether you think a person or tool is going to fail or
betray you, and 2. Whether your system relies on that person or tool
operating correctly and loyally.
A lot of the architecture of Tahoe-LAFS is focused on question 2 and
the interference of question 1 just confuses everyone. If you think
about question 1 then you'll sometimes end up choosing the wrong
answer for question 2.
For example: think of Least Authority Enterprises. As a company, we
don't want our users to think that we are weak or malicious -- that we
are likely to fail or to betray our customers to someone else. But, we
very much want our customers to be *invulnerable* to us, so that *if*
we were to fail or to betray our customers to someone else, there
would be very little damage that we would be able to do.
If you use the word "trust", then it is hard to explain why you don't
want to give LAE your encryption keys. Does that mean you think we are
dishonest? Don't you trust us?
If you force yourself not to use the word trust, then you can usually
rewrite the sentences to be in terms of "reliance" or "vulnerability"
instead of trust, and suddenly the confusion about question 1 vs.
question 2 disappears. Why do you withhold your capabilities from LAE?
Because you don't wish to be vulnerable to a failure or betrayal at
LAE. Because you don't want the safety of your backups to *rely on*
the continued security of LAE's servers and the continued loyalty of
its employees.
Doing that transformation on your sentence above would give something like:
I like its "paranoid" approach. The idea is that no one (not even your
online storage provider) should have read or write access to your
By the way, zfec is an alternative to PAR2.
zfec is much more efficient than PAR2 for some settings. (See the
benchmarks in the README.rst.)
I don't know of anyone who is actively using zfec's command-line tool
in the way that one uses PAR2's command-line tool, though. There are
lots of people using zfec as a library inside other tools.
Looking forward to reading your article! Maybe I'll painfully struggle
through the first 10 words in French and then get my Francophone wife
to read it to me. :-)
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-06-15 16:53:59
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Announcing RentaNode.nl: Tahoe-LAFS storage nodes 
Welcome to the world, rentanode.nl!
I'm delighted to hear that there is another company focused on helping
customers use Tahoe-LAFS.
I've already had two potential customers ask (on twitter) if it would
be possible for someone to lease service from both rentanode.nl *and*
leastauthority.com and spread the ciphertext across both services.
Because of the architecture of Tahoe-LAFS, it would be easy for us to
make that possible. Let's talk about making sure it can work. With our
current setup, leastauthority.com always provides the introducer, and
since you said the customer can provide their own introducer for
rentanode.nl, then the easiest way to do it would be to use LAE's
Note: decentralized introduction, either by running multiple redundant
introducers per  or by some more advanced gossip protocol, will
probably be coming out in a future version of Tahoe-LAFS. Presumably
that would make it even easier for customers to combine our services.
P.S. I think the announcement of rentanode.nl should be a headline in
the next edition of the Tahoe-LAFS Weekly News!
 implement
distributed introduction, remove Introducer as a single point of
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-06-22 12:51:59
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] tahoe-lafs 1.9.1 shipped in the next Debian stable 
Dear Bertagaz:
We're almost ready to release Tahoe-LAFS v1.9.2, and I wonder if it is
okay to take a couple more days for extra testing or if we should
hurry and finalize it so you can package it for Debian?
We've fixed all but one of the issues that we intended to fix. You can
see the issue tickets here:
The one serious issue that isn't fixed yet is
 KeyError in mutable retrieve
We know of a bug in the code (dict instead of tuple being returned
from get_verinfo() of MDMFs), but we're not sure if that bug is
causing that failure. There is no unit test currently demonstrating
that failure.
We prefer not to fix bugs when we don't have a unit test that goes
from red to green by that fix, so if it won't make us miss the Debian
deadline, I'd like to take a couple more days to figure out how to
write a unit test of  Then we'll find out if the
dict-instead-of-tuple bug is causing  or not.
Also I'm doing an experiment of converting all of the existing unit
tests from using SDMFs to using MDMFs as a way to find any bugs in
MDMFs. So far I haven't found any more bugs in MDMFs that way.
(Because there were already quite a few tests of MDMFs.)
Also, it would be great to have some actual users try out 1.9.2 before
we finalize it. Users! You can do this right now if you're willing to
use darcs to fetch the source. This is actually painless -- you just
acquire darcs ( and then you run
darcs get --lazy Then you never have to touch darcs again.
That command takes about 60 seconds to run.
Oh, but it might fail if the darcs binary you got is incapable of
handling HTTPS. In that case, you give up and complain to us and we'll
post some tar.gz files for you to try out...
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-03-10 16:55:19
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Boulder Hack Fest! 
Let's have a Hack Fest in Boulder, Colorado, and on the Internet on
the weekend of March 30! David-Sarah will be in town, visiting LAE
World Headquarters. Marlowe and possibly other members of the Marlowe
family will drive over from Kansas City. If anybody else wants to
physically attend the Hack Fest, please let me know so I can make sure
to get access to a big enough conference room, and map out crash space
on our floor. I know Jean Lorchat wants to be linked in over the
Internet. If anybody else wants that, let me know.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-03-13 14:32:56
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] monitoring/visualization (was: What about this?) 
What sort of monitoring functionality are you interested in, Ted? What
data do you want to see?
There is the "Download Performance Graphs" in the Tahoe client. Check
it out! It contains lots of information.
There is also the "/statistics" page served up by the Tahoe client
that shows a few statistics about uploads and downloads.
There are munin plugins in the source tree:
I'm not sure, but I think that those plugins are the source of these
graphs of volunteergrid2:
Least Authority Enterprises has set up some basic monitoring of our
services with zenoss:
(username: guest, password: guest)
We haven't yet published the configuration and scripts that we use to
run that monitoring server. We intend to do so. If you care about such
details let us know on this list and this will encourage us to
prioritize publishing it.
A couple of tahoe-lafs contributors, Nejucomo Wilcox and Leif Ryge,
work at StatMover --  -- which is making a
database and JavaScript visualization tool:
 I'd like to see
Tahoe-LAFS statistics graphed in that thing. The demos of it make it
seem fun to explore data in it.
So the first question from your perspective, Ted, is probably "What do
I want to know?".
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-03-22 10:09:19
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Logging howto / sftp with hashed passwords 
Oh yeah, and then there is this:
I'm really glad that we have so many people contributing docs
nowadays, but we really do need some kind of improvement to helping
people find the docs they seek. I'm really not sure how to do that.
This was my attempt to do so:
 , and I'm not sure if
it is sufficient.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-03-27 13:06:37
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] erasure coding makes files more fragile, not less 
I've heard many stories of people losing their files from a Tahoe-LAFS
grid even though they had erasure coding parameters that provide
massive fault tolerance such as 3-of-10 or 4-of-8. In fact, I think
approximately 90% of all files that have ever been stored on a
Tahoe-LAFS grid have died. (That's excluding all of the files of all
of the customers of allmydata.com, which went out of business.)
I've been musing on this, and I just read this excellent blog rant by
the original author of VoldemortbJay Kreps. I came up with this
provocative slogan (I know Brian loves my provocative slogans):
"erasure coding makes files more fragile, not less".
The idea behind that is that erasure coding lulls people into a false
sense of security. If K=N=1, or even if K=1 and N=2 (which is the same
fault tolerance as RAID-1), then people understand that they need to
constantly monitor and repair problems as they arise. But if K=3 and
N=10, then the beautiful combinatorial math tells you that your file
has lots of "9's" of reliability. The beautiful combinatorial math
lies! That's because it is assuming each server has some fixed and
independent chance of surviving, which is always false. ("90%" is
always a good number to use for that fixed and independent chance.
Plug in "90%" into the beautiful combinatorial math with K=3 and N=10
and you'll get more "9's" than you can shake a stick at!)
Here's the excellent blog rant:
Where is the flaw in the reasoning?
The problem is the assumption that failures are independent.
Surely no belief could possibly be more counter to our own experience
or just common sense than believing that there is no correlation
between failures of machines in a cluster.
The actual reliability of your system depends largely on how bug free
it is, how good you are at monitoring it, and how well you have
protected against the myriad issues and problems it has. This isnbt
any different from traditional systems, except that the new software
is far less mature.
Now let's apply this idea to my empirical observations about the
longevity of files stored in Tahoe-LAFS. If almost all of the files
that have ever been stored on Tahoe-LAFS have died, this implies one
of two things:
1. The "reliability" of the storage servers must have been below K/N.
I.e. if a file was stored with 3-of-10 encoding, but if each storage
server had a 75% chance of dying, then the file would be *more* likely
to die due to the erasure coding, rather than less likely to die,
because a 75% chance of dying, a.k.a. a 25% chance of staying alive,
is worse than the 30% number of shares required to recover the file.
2. The behavior of storage servers must not have been *independent*.
I.e. if enough of the servers failed *at once*, then the file died,
even if the chance of any individual server failing was lower than the
erasure coding ratio.
My conclusion: if you care about the longevity of your files, forget
about erasure coding and concentrate on monitoring. (Go ahead and use
3-of-10 because everyone does, and it adds a reasonably low level of
storage overhead.)
Not coincidentally, Least Authority Enterprises (our startup company)
has been spending most of our engineering effort on monitoring,
measurements, and fault detection for the last couple of months. Our
service is still not functional enough to advertise it as non-alpha.
This monitoring and operations engineering is a lot of work!
P.S. But if you want to help us alpha-test our service, by all means
let us know! :-)
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-03-28 23:59:22
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] raise your hand if you have a patch for Tahoe-LAFS that 
As I mentioned on:
 , I've
heard of several patches for Tahoe-LAFS that are complete enough to be
useful but can't be committed to trunk because they don't have tests.
This includes the multi-introducer patch by Faruq that killyourtv is
talking about on that ticket. If I recall correctly Faruq wrote a
basic test of subscribing to multiple introducers 9, but we don't have
tests of other interesting cases, like what happens when one
introducer is connected and you subscribe to it and hear about a
server, and then that introducer goes offline and a different
introducer appears and you connect to it and hear a different (newer)
announcement about that server.
Anyway, I was wondering how many people know of patches that you would
like to see in Tahoe-LAFS trunk, and the reason it is not already
committed to trunk is that it doesn't have tests, and you're not sure
what sort of thing we expect in tests or how to write tests for
Twisted code, etc. You can find such patches by querying the trac for
the keyword 'test-needed': 2.
If that describes you, let me know when you would be available for a
tutorial session on IRC where we pick one such patch and walk through
the process of writing tests for it. Brian Warner and David-Sarah
Hopwood are two of the best engineers in the universe, and they are
both (not coincidentally) two of the best unit-test-writers in the
universe, so I would find a time when at least one and hopefully both
of them would be willing to help.
(I would definitely recommend hanging out with these folks on IRC for
a couple of hours and working on a real patch for a real project. You
could pay some famous person or company big bucks for software
engineering training and not improve your skills as much.)
This coming weekend, when Andrew Miller and Zancas and I will be
hacking in person in Boulder, might be a good time to get on IRC and
help us write unit tests.
9 2 tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-03-31 14:15:16
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Hack Fest Day One report 
Here are some notes about Hack Fest Day One.
b" David-Sarah did a lot of trac ticket gardening. You can see their
changes by browsing the Trac Timeline B9 (which, by the way, is one of
my favorite ways of keeping up with Tahoe-LAFS development). I haven't
looked at all of their updates to tickets yet.
b" Andrew Miller wrote a unit test for his path to make "tahoe backup"
follow a limited number of symlinks, and updated his patch on  He
didn't set the "review-needed" flag. I'm not sure why not -- perhaps
that means he doesn't think the patch is (yet?) suitable for trunk.
We have a conversation about how to handle symlinks. There are three
possible features:
1. Traverse symlinks when doing a "tahoe backup". That's what
amiller's patch does, and it raises the question of how to handle
cycles encountered while traversing the local filesystem to do a
backup. The main use case for this is amiller's desire to use symlinks
as a way to specify which directories should be backed up -- he can
put symlinks into one directory pointing to several other directories,
then ask tahoe to backup that one directory.
2. Have a special file to notify "tahoe backup" to skip the current
directory. It could be named ".tahoeignore" (analogous to .gitignore)
and "tahoe backup" would refuse to backup any directory containing a
.tahoeignore file, nor traverse through that one to others.
3. A way to restore filesystem details other than the names and
contents of files, such as symlinks, ownership, permissions,
timestamps, etc. This is the topic of ticket b" There was some discussion of whether people wanted "tahoe backup"
(which stores your files and directories individually so that you can
browse, restore, and share access to them separately) to handle more
and more of the local filesystem metadata ( or if instead people
who want that should just switch to using a more "archive oriented"
backup tool like duplicity, which bundles all of your files and
directories, along with their metadata, into a tarball before storing
it on Tahoe-LAFS. I tend to "not want to go there" and send people off
to use duplicity or Shawn Willden's Grid Backup or something if that's
what they want, but everyone else seemed to think it would be a good
idea to add more of these features to "tahoe backup".
The reason I gave at the time for not wanting to go there is that
there is such a mismatch between the Tahoe-LAFS model, with immutable
files, capability access control, metadata on edges rather than on
nodes, a full graph instead of a tree, etc., and the traditional Unix
filesystem with its mutable files, ownership and permission bits,
metadata on nodes instead of on edges, tree-plus-symlinks, etc.
I later thought that there is another reason I don't like the idea
extending "tahoe backup" in that direction, which is that I like for
the data which lives in Tahoe-LAFS to be meaningful independent of any
given "source machine" (for example things like "UID of owner" are
either meaningless or incorrect unless you decide to interpret them in
the context of a "source system" which happens to have the right
meaning for that UID), and also independent of any given source
operating system type. For example a long-standing bug that really
annoys me (someone fix it during Hack Fest!) is that "tahoe backup"
reads the "metadata-change-time" if on Unix, or else reads the
"file-creation-time" if on Windows, and then sticks the resulting
timestamp into an field named "ctime" in Tahoe-LAFS ( The result
is meaningless unless you choose to interpret in terms of being either
"in Unix" or "in Windows". Annoying! The thing is in the cloud -- it
isn't in either Unix or Windows while it is there.
I guess in general, I think of the data stored in Tahoe-LAFS as being
shareable among multiple people, and meaningful within different
contexts -- different "source machines" and even different "source
operating system types", and being meaningful outside of the context
of any source machine at all -- just purely "in the cloud". It is
perhaps not impossible, but certainly more work, to handle extended
filesystem metadata correctly, compared to making a traditional
"backup and restore" app. In a traditional "backup and restore" app
like duplicity, the data "in the cloud" can't be shared or used other
than by first restoring it to a system that is of a sufficiently
similar type to its source system.
b" On the topic of issue 1., above, I suggested an alternative way to
prevent cycling endlessly through the local filesystem. The primary
proposal -- already partially implemented by amiller -- is to have a
limit on how many symlinks you'll cross over before you stop. If you
stop, you should probably print out a complete listing of how you got
to where you are, and which steps were traversing symlinks, so that
the user can figure out if this was really a cycle and if so how they
want to break the cycle.
My alternative proposal is that for each directory that you visit,
learn its device id and inode number and use those as a unique
identifier for this directory in the context of this run of "tahoe
backup", and if you encounter a directory for the second time then use
the same within-tahoe-lafs-directory that you created for it the first
That's way better! It is almost elegant. But as Brian then pointed
out, it can't work. Because all directories (except for the root dir)
that are produced by a "tahoe backup" are immutable. You can't have a
cycle of immutable directories, because it is impossible to get a link
to an immutable directory before providing the initial and final
contents of it.
b" This led me to off-handedly comment that such a thing would be
possible with Petrification a.k.a. revocation of write authority
( which inspired David-Sarah to announce that they were going to
implement  during Hack Fest. That would be very ambitious, but
David-Sarah often says that they will do ambitious things and then
does them. I remember that the "Drop-Upload" feature was implemented
in a single afternoon by David-Sarah at The Second Tahoe-LAFS Summit.
Anyway, I'm pretty excited about the possibility of getting  soon.
It would be the first big addition to the vocabulary of what you can
express with Tahoe-LAFS caps.
b" I started this tutorial on "How To Write Tests": B2, because of the
increasing number of patches that are ready except for tests B3. The
tutorial is already enough to get you started, but you'll probably
come to a spot where you say "Hey NOW WHAT?". Please do that, so that
I -- or someone -- will extend the tutorial.
b" I did a lot of tinkering with packaging and build system and docs. I
care a lot about this stuff, and I hope you do too, and I could use
help, but I'm not going to explain it all in this letter because
currently new work is being done at Hack Fest faster than I'm
finishing this letter describing the work that is being done. :-) You
can find out all about it by reading the trac Timeline. One thing I
definitely need help with is building binary packages of dependencies
on various platforms, especially with Python 2.7. Please see the table
of precompiled packages hosted on tahoe-lafs.org: b4.
b" Lebek did some wiki gardening and fixed a bug in FTP -- b" Brian posted a fix for  which is good to see as  was one
of those regressions in handling of mutables that we shipped in
b" Marlowe reviewed some docs patches and resumed working on the
Tahoe-LAFS Weekly News. Yay!
b" John Dougherty posted his first review. b:
Okay, that's at least part of what we've done so far. There are still
approximately 48 hours of Hack Fest to go. Jump in!
B9 B2 B3 b4 tickets mentioned in this email:
 tahoe backup should
be able to backup symlinks
 revocable write authority
 "tahoe backup"
thinks "ctime" means "creation time"
 make `tahoe
backup` keep more filesystem metadata
 ftpd returns 0 for
all timestamps
 assertion failure
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-05-11 13:20:48
@_author: Zooko Wilcox-O'Hearn 
@_subject: [cryptography] Bitcoin-mining Botnets observed in the wild? 
Here's a copy of a post I just made to my Google+ account about this
alleged Botnet herder who has been answering questions about his
operation on reddit:
=== introduction ===
Someone is posting to reddit claiming to be a malware author, botnet
operator, and that they use their Botnet to mine Bitcoin: B9.
I asked them a question about the economics of using a Botnet for the
Bitcoin distributed transaction-verification service ("Bitcoin
mining"): B2.
They haven't provided any proof of their claims, but on the other hand
what they write and how they write it sounds plausible to me.
=== details ===
Here are my notes where I try to double-check their numbers and see if
they make sense.
They in their initial post B9 that they do 13-20 gigahashes/sec of work
on the Bitcoin distributed transaction verification service.
The screenshot they provided B3 shows 10.6 gigahashes/sec (GH/s) in
progress, and that they're using a mining pool named BTCGuild.
According to this chart of mining pools b4, BTCGuild currently totals
about 12.5% of all known hashing power, and according to b5 the current
total hashing power on the network is about 12.5 terahashes/sec
(TH/s), so BTCGuild probably accounts for about 1.5 TH/s.
They say that their Botnet has about 10,000 bots. The screen shot
shows a count of "total bots" = 12,000 and "connected in the last 24
hours" = 3500. This ratio of total bots to bots connected in the last
24 hours is consistent with other reports I've read of Botnets b6, and
also consistent with my experience in p2p networking. The number of
"live bots" available at any one time for this Botnet herder should
probably average out to somewhere between 350 and 550. Let's pick 500
as an easy number to work with. Does it makes sense that 500 bots
could generate 10 GH/s? That's 20 MH/s per live bot. According to the
Bitcoin wiki's page on mining hardware b7, a typical widely-available
GPU should provide about 200 MH/s. Hm, so they are claiming only 1/10
the total hashpower that our back-of-the-envelope estimates would
assign to them. Here is an answer they give to another person's
question that sheds light on this: b8.
Q: "Isn't Bitcoin mining pretty resource intensive on a computer? Like
to the point someone would notice something is up on their system form
it slowing eveyrthing down?"
A: "My Botnet only mines if the computer is unused for 2 minutes and
if the owner gets back it stops mining immidiatly, so it doesn't suck
your fps at MW3. Also it mines as low priority so movies don't lag. I
also set up a very safe threshold, the cards work at around 60% so
they don't get overheated and the fans don't spin as crazy."
It sounds plausible to me that those stealth measures could cut the
throughput by 10 compared to running flat-out 24/7. Also it isn't
clear if the botnet counts computers that don't have a GPU at all, or
don't have a usable one. Maybe such computers are rare nowadays?
Anyway if they are counted in there then that would be another reason
why the hashing throughput per bot is lower than I calculated.
In answer to another question B9b0, they said they get a steady $40/day
from running the Bitcoin transaction-confirmation ("mining") service.
According to this chart B9B9 from B9B2, the current U.S. Dollar value of
Bitcoin mining is (or was a couple of days ago when they wrote that)
about $0.33 per day for 100 MH/s. Multiplying that out by their claim
of 10.6 GH/s results in $35/day. So that adds up, too.
(Note that it sounds like their primary business is stealing and
selling credit card numbers, and the Bitcoin transaction-verification
service is a sideline.)
I don't see a reason to doubt that they really generate about 10.6
GH/s of the Bitcoin distributed transaction verification service.
My primary question is: if this is profitable on a per-bot basis, then
why don't they scale up their operation? Of course, the answer to this
presumably sheds light on the related question of why competitors of
theirs don't launch similar operations. Perhaps one limiting factor is
that the larger your Botnet, the more likely you'll be arrested by
police or extorted by competitors. That may be a limiting factor that
this person doesn't yet know about or doesn't like to think about.
They mentioned b9 that most of their fellow cybercriminals are "too
inexperienced to accept Bitcoin", so it may be that this person is
just ahead of the curve and more people will launch operations like
this in the future.
That's the question that I asked them on redditbwhy don't they scale
up? They haven't yet replied to my question, but they earlier
mentioned in response to a different question b9:
Q: "How many botted machines do you typically gain per month or per campaign."
A: "about 500-1000 a day, weekends more. I'm thinking about just
buying them in bulks and milking them for bitcoins. Asian installs are
very cheap, 15$/1000 installs and have good GPUs."
If they're really gaining 500 to 1000 new bots per day but they have a
total of only 12,000 then either their operation is rapidly expanding,
or the attrition rate is similarly high as the acquisition rate.
=== the bottom line and my take ===
I don't see any reason to doubt that this is real, and that this
person with their Botnet is responsible for about 0.08% (i.e. less
than 1/1000 b not 8%!) of the total Bitcoin distributed
transaction-verification service, and that they profit for it at the
rate of about $35 per day.
There's one open question in my mind about whether this particular
operator is currently rapidly expanding (adding 1000 bots per day to
their network of 12,000 bots) or if the attrition rate of bots
departing from the 12,000-node network is close to 1000 per day.
If the $35/day revenue is really mostly profit (i.e., they don't have
to spend so much time maintaining their 12,000-node Botnet that they
forego more profitable activities, like stealing more credit cards or
finishing their homework), I would expect them and others like them to
turn more and more bots to this purpose.
However, the nature of Bitcoin is that all providers of distributed
transaction-confirmation service are in competition with one another.
In the two weeks since this post went up on reddit, people around the
globe deployed about 2 TH/s more hash power (see this graph of
aggregate Bitcoin hash power b5), which cut the profitability of this
one person's operation from $35.00/day to $30.00/day. If more and more
Botnet operators get into the Bitcoin mining game, they will reduce
the profitability of Bitcoin mining. (As well as competing with each
other for access to victim computers, which has got to be a limited
resources, right? Right? Or is there just a practically infinite
supply of vulnerable computers waiting to be tapped if only someone
can find a way to profit from them?)
In parallel, the legitimate Bitcoin miners appear to be continuing to
roll out new distributed transaction-verification service on their own
hardware. Here is a recent post by "Bitcoinminer" about commercial
Bitcoin farms based on GPU: B9B3. The operation spotlighted in that post
apparently delivers 100 GH/s (about 10X that of our Botnet herder). At
the same time, sales of FPGA-based Bitcoin devices appear to be
booming. I wrote a post about that: B9b4. You'll have to scroll down
through extensive discussion to find where I summarized the numbers,
but in summary it appears that people are in the process of investing
half a million USD in Bitcoin FPGA which, when all deployed, will
deliver around 430 GH/s.
I think there may be a kind of "Game of Chicken" going on: if someone
makes a convincing show of investing in Bitcoin mining then they may
deter other people from getting into the game and dividing up the
profits. That may be the subtext of Bitcoinminer's blog postbhe may be
trying to discourage competitors. An interesting thing about "Game of
Chicken" is that large upfront costs can actually be an advantage
because they demonstrate your commitment! If people are spending half
a million dollars on FPGA Bitcoin miners, then their competitors had
better believe they're really going to keep running them, even if
competition drives down profitability.
See, to deploy 10 GH/s of Bitcoin hash power using FPGA would require
you to purchase about $10,000 worth of hardware which has no resell
value except to other Bitcoin miners. To deploy 10 GH/s using GPU
would require an outlay on about $5000 of hardware, which you could
later resell for gaming (or whatever other uses GPUs have nowadays --
CAD/CAM?). To deploy 10 GH/s using a Botnet requires an unknown-to-me
outlay of time, money, skill, or risk of personal harm, but at least
the marginal cost of adding another few MH/s seems much lower than in
the hardware-based approach. Our Botnet herder on reddit said he could
buy access to Asian PCs with good GPUs for $15 for 1000 PCs. If that's
true then it should cost a piddling $180 to set up a new network as
big as his current 10 GH/s network.
However, if he is considering doing something else with his time and
money, then the fact that people have convincingly committed to
large-scale FGPA mining may deter him, because no matter how well he
does at competing with them, they've already paid a sunk cost, and
their marginal cost for electricity is low, so they won't quit.
(Unless competition swells to such a level that it drives revenue
below their cost of electricity, which seems like a distant prospect
at this point.) Thus they might win at the Game of Chicken and
persuade him to spend his time and money on different projects (such
stealing more credit cards or doing a better job on his homework).
(There are also several organizations who are loudly proclaiming that
they're developing custom ASIC chips for Bitcoin mining. I haven't yet
seen hard evidence of any of them having really spent substantial
money on it or having demonstrable progress on the engineering and
=== last word ===
I'm delighted to see such vigorous and varied competition for
contributing to the distributed, planet-wide transaction-confirmation
service. I especially like the "sunk-cost" people such as the FPGA
miners with their low electricity requirements, because they seem
likely to be long-term, always-on contributors.
B9 B2 B3 b4 b5 b6 b7 b8 b9 B9b0 B9B9 B9B2 B9B3 B9b4 cryptography mailing list
cryptography at randombit.net

@_date: 2012-05-28 13:52:05
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] programmer looking for a distributed filesystem chooses 
"Looking for the ultimate distributed filesystem, take 2"
It ends with:
In summary, I guess the replies I got didn't cause me to change my
mind too much. Tahoe-LAFS still seems to be, if not the best solution,
then at least the bleast badb. Hopefully the drawbacks will be fixed
soonish; the main sticking point (at least from my point of view)
still seems to be the lack of focus on proper filesystem integration.
I'll try setting it all up at some point; I may report further if I
end up finding interesting things.
I think I may have patronized the author, whose name I think is
"Roland", in my email in reply to his previous post. I think I told
him something to the effect that he probably wouldn't use "integration
into his kernel as a filesystem" as much as he thought he would. I
should probably work on a less patronizing way to argue about this
stuff. Probably getting into more specific technical details and less
general value statements is the way to do that.
(I intend to reply to Greg Troxel's recent discussion of related
topics on this list soonish...)
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-11-02 15:33:13
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] hello 
Tahoe-LAFS doesn't offer any API reachable from other processes
(command-line, kernel, or remote-procedure-call) which *doesn't* route
through the webapi. The diagram "network-and-reliance-toplogy.svg" 9
shows this architecture. Everything goes through the "Tahoe-LAFS
gateway", and the only API that the Tahoe-LAFS gateway exports is the
9 Han Zheng: why do you want to upload a local file to the tahoe grid
not using the web server?
The way to accomplish that is to write some Python code that runs in
the same Python process as the Tahoe-LAFS gateway. The way that I find
easiest to do such things is to look at other code that already does
it and copy and modify that.
So, here is the code that gets run when someone makes a PUT request to
the webapi (as described in webapi.rst 2):
2 web/root.py parses the HTTP request and decides what sort of upload
this is (mutable or immutable):
Then it calls web/unlinked.py which constructs a FileHandle object.
That object is provides the interface that the uploader expects, and
it has a handle (open file descriptor) to the file on disk from which
it will read the data while the data is being uploaded.
Then (after an unnecessary layer of indirection that I'm skipping),
immutable/upload.py starts doing some real work: setting the encoding
parameters, deciding whether to literalize this immutable file, etc:
So, if you write some Python code that invokes immutable/upload.py's
"upload()" method, and passes an "uploadable" as the argument (note
that in case shown above the "uploadable" is the FileHandle object
constructed by web/unlinked.py), then you'll upload a file directly to
the grid.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-11-03 10:49:04
@_author: Zooko Wilcox-O'Hearn 
@_subject: [p2p-hackers] is economics off-topic? 
Dear all members of p2p-hackers:
Speaking as the moderator of p2p-hackers mailing list, I don't
consider this to be too "off-topic". I don't think I'll have to ask
anyone to quiet down just because they are going on about economics.
Honestly, I have always kind of thought that p2p theory and practice
was incomplete without the addition of a lot of economics, so I
actually kind of regard economics as pretty much right on topic for
this list!
Now, even if I'm not going to ask you to pipe down, you might still
decide to limit your posting on the topic because you suspect that a
lot of the readers are bored or annoyed by it, or because you're just
expressing your opinion about politics and nobody is really going to
learn anything new or change their minds from reading what you've
said. (They're just going to express *their* opinion about politics,
thus leaving both of you no wiser and wasting twice as much of
everyone else's time scrolling through it.)
If you, oh Members of this List, really hate to see such conversation
and feel that it is detracting from your opportunity to have a better
conversation on a different topic, then please post publicly or email
me privately and say so. Maybe if there's an upwelling of strong
difference of opinion on that then I could create another list called
"p2p-econ" or something. My guess is that there won't be enough people
who complain vociferously enough to motivate me to make a new list for
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2012-11-03 10:49:45
@_author: Zooko Wilcox-O'Hearn 
@_subject: [p2p-hackers] Bitcoin incentive on Kademlia networks 
Speaking as a list member rather than as the moderator,
changaco at changaco.net's statements that "money has to be based on
something", that Bitcoin is "based on" proof-of-work and that people
would need to waste CPU cycles in order to trade files (under
danimoth's proposal) are all incorrect. b:
Money, to be useful as money, only has to be acceptable and valuable
to enough people. It doesn't have to be "based on something".
Bitcoin isn't really "based on" proof-of-work. It's mostly "based on"
digital signatures. The proof-of-work part is really just to make it
difficult (but not impossible) for attackers to perform a rewind
attack. There are designs floating around which replace the
proof-of-work with other mechanisms intended to deter rewind attack,
and the properties of the resulting systems are almost the same as the
properties of Bitcoin.
People would not have to burn CPU cycles in order to trade files in
danimoth's proposal. Only the transaction-verification-servers (also
called "miners" in Bitcoin) need to do any proof-of-work (in order to
deter rewind attack). Normal users who want to send or receive Bitcoin
do not need to do any proof-of-work.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2012-11-04 09:58:46
@_author: Zooko Wilcox-O'Hearn 
@_subject: [p2p-hackers] economics and networking/storage services (was: 
My own p2p project,  is carefully building up
toward automation to support social/economic incentives in distributed
If we finally get there, we will then have circled back around:
 b" from Tahoe-LAFS's roots (its direct ancestor was Mojo Nation),
 b" to a stripped-down version that had no automation to help with
social/economic problems and required the users to arrange all that
themselves (today's Tahoe-LAFS),
 b" and back to a version that has automation to help people find new
trading partners on the Net, track and control their transactions with
them, extend special offers to their friends, and so forth (tomorrow's
If our plans work out, then the resulting thing will be reliable and
functional b unlike the original Mojo Nation, which never worked well
enough to be valuable to users.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2012-11-20 17:32:27
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] two-phase-commit for Tahoe-LAFS, 
I posted some thoughts to  If you're interested in distributed
systems, please read and comment!
I posted an argument for why distributed, end-to-end, two-phase commit
will probably work fine for LAFS's purposes even though it has gained
a well-deserved reputation for "not scaling up to the Internet" in
other contexts. (Hint: the answer is, of course, that we're demanding
less of it than most systems do.)
Unfortunately I didn't yet get around to explaining what we actually
want it for. I remember there being at least two different reasons why
I really wanted end-to-end two-phase-commit in the LAFS storage
protocol. One reason has to do with uploading large mutables and
making modifications to large mutables, without asking any computer to
"buffer up" all the changes so that it can apply them all quickly, and
without opening a large window of time in which a failure in any of
several places will leave a corrupted mutable share. The other reason,
which I remember less precisely, has to do with multiple writers
sharing write-access to the same mutable file or directory. LAFS
currently handles that use case very badly. I think e2e 2PC can do
better, handling write-collisions with a clean failure ("no can do!")
instead of, as it currently does, with potential data loss. At least
in almost all cases.
But even so, multiple uncoordinated writes to the same resource still
have to be held down to a low frequency and a small number of
uncoordinated writers.
A key insight into all this is that you should use shared access to
LAFS's mutables as sparingly as possible, and instead manage almost
all of your state with immutables and with single-writer mutables. A
great example of this design pattern is the new design we came up with
for Dropbox-like functionality on top of Tahoe-LAFS. That synthesizes
a "magic folder" like Dropbox from the perspective of the user, but
does so without *any* concurrent writes to a shared mutable. Instead,
every writer has their own single-writer mutable and the client is
responsible for reading all the mutables and synthesizing the result
as those mutables get changed by their respective writers.
Unfortunately the details of that design are sitting in a queue of
"Notes From the Tahoe-LAFS Weekly Dev Call" that I am supposed to
write up and post to this list ASAP...
 2-phase commit
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-11-26 10:12:23
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Hello & Questions 
Dear til:
Yes, please! I would be very interested in new and/or improved
documentation about why and how to install and use Tahoe-LAFS. Here
are some existing resources:
quickstart, which you mentioned:
The "Installation" page on the wiki:
An issue ticket "introductory docs are confusing and off-putting":
Some good editing improvements offered by Dan Connolly:
I think the next step is for til, or anyone else who cares about this
issue, to write up and post a new "starter document" which is easier
for users to follow.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-11-28 03:00:12
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] notes from the LAFS Weekly Dev Chat for 2012-11-01 
I apparently never posted these notes from the LAFS Weekly Dev Chat of
November 1. I'm sorry about that. I was probably planning to flesh
them out with more context and explanation, but I haven't done that,
either. So, here's a dump of my notes.
=== 2012-11-01 ===
* we don't have consensus on the long-term strategy for caching of
filenodes; options:
  * aggressively cache filenodes, make downloaders be indefatiguable,
so that they never cease their labors unless cancelled
  * aggressively cache filenodes, make downloaders get a fresh burst
of energy whenever a new use of the downloader is begun
  * don't cache filenodes of any kind, implement a separate
mutable-write-serializer (which looks a lot like a cache)
  * cache mutables but not immutables
* but we do have consensus on what to do right now:
  * we're going to write a unit test for the patch attached to and commit it to trunk; That means Tahoe-LAFS v1.10.0 will cache
mutables but not immutables.
* tests of corruption both before and after servermap-construction
don't apply to some parts of the data
* document which are which and why we test some of them both before
and after servermap-construction; Andrew will write, Brian will
* indefinite (or long-term) but cancellable leases
* we discussed two protocols that could implement   * the one shown on the initial description of   * one in which the client builds a manifest and delivers it to the
server in one (potentially big) message
* if I have multiple clients, they could have separate accounts
* but how to get aggregated accounting information
concurrent garbage collection
notification so you can add leases
in the leasedb schema, indefinite leases are indicated by having an
expiration time of null
encrypted timestamps
add a storage api which says "give me back something which the server
will recognize as a timestamp", and another api which says "you are
allowed to clobber everything that hasn't been created or renewed
since $THIS_TIMESTAMP"
should each lease renewal method come with an explicit
$THIS_TIMESTAMP, or should it be able to do an explicit "when you
receive this message"; the latter would unnecessarily require
limited-time-leases to do a round trip first.
we'll need separate account ids on separate leases to prevent one user
from clobbering
add a requested-duration to lease-renewal methods; if we don't have a
negotiation protocol for that, maybe make it server-side-config for
now. (
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-11-29 10:51:25
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] LAFS Weekly Dev Chat notes, 2012-11-29 
legend: "b" means Action Item! (If your name is mentioned after "b"
and you aren't really going to do that thing, then please let us
know!); "b" means Action Item that is already done.
  // I don't have time to write
explanations of all this, and I may have forgotten parts.
in attendance: Zooko (scribe), Marcus, Brian, Evgeny, David-Sarah,
Kevan (briefly b technical difficulties), CiC (briefly)
Marcus found the quickstart doc to be fine b it was easy to get
Tahoe-LAFS set up. He had been using Linux for a while and so he was
just looking for "What's my 'make'? What's my './configure'
Marcus says there's a parallel set of getting-started docs maintained
by the I2P people.
Marcus wonders why the multi-introducer patch hasn't been merged into
main Tahoe-LAFS yet, as it has been in use in I2P for a long time and
is quite stable for them. The I2P setup include a cron job that
downloads a new set of introducers and also downloads news. Answer:
the multi-introducer patch may conflict with the accounting project.
The accounting project is going to add the ability to control which
servers your client will use or which clients your server will serve.
In that case, access to the introducer will no longer be sufficient to
allow you to use a set of servers. In that world, we might want to
replace "introducers" with a "gossip" strategy.
We're reconsidering, because when/if we *do* replace the introduction
process, we could change this behavior without a great deal of
backward-compatibility problems. In fact, Brian thinks that even after
it does the sort of gossip he wants, he might still want a
distinguished set of introducers to serve as "seeds" for gossip, so we
might want to *keep* multi-introducers as is.
Things to do to about the multi-introduction patch:
b" See if it merges with current trunk (which has the
signed-announcements patch, which included significant refactoring of
introduction). b [[Who?]]
b" Let kytv know that we're interested. b [[Zooko]]
b" Write tests that actually exercise the behavior of using multiple
introducers. b [[kytv?]]
b" Write up a plan for forward-compatibility with gossip. b [[Brian,
because nobody else knows what sort of gossip Brian wants,
Brian says there may be differences of opinion about design strategy
here. He favors a "fully distributed" approach where no node is
distinguished in terms of the introduction service that it provides to
its peers. Zooko wonders if this relates to the "Are We Client-Server
or Peer-To-Peer?" issue. David-Sarah says that this may not make any
difference *in terms of security* for our current use cases:
friendgrids, commercial services, volunteergrids, ... It isn't like
we're currently trying to implement the One Grid To Rule Them All use
[[Added by Zooko ex post facto: I long ago posted a design for
"gossip/distributed-introduction" based on a Chord-like ring which is,
IMHO, simple and scalable, and which is described in sufficient detail
to be implemented. Here it is, it is ticket 68, comment 11:
 . I don't
think it matters much whether the set of introducers that are
participating in that Chord-like ring are a specially selected set a
la the current I2P branch or whether "everyone is an introducer" a la
Zooko says that the thing he is concerned about isn't the introduction
part b how clients learn about servers b but instead the authorization
part b how clients choose whether or not to use the servers that they
know about. Current Tahoe-LAFS combines those two issues, which is a
Zooko says that Tahoe-LAFS has very weak security against rollback
attack or DoS, but that if we had server selection (which is a part of
the accounting project) then we would have very strong security
against those. Zooko tells an illustration from Bruce Schneier: if
you're at a coffeeshop and you need to go to the bathroom, you can ask
a random stranger sitting nearby to watch your laptop. If you're
paranoid, you can ask three different random strangers to watch your
laptop and each other. But if three random strangers approach *you*
and offer to watch your laptop, that's different! Zooko says server
selection b where you choose the 1 or 3 servers that you're going to
rely on instead of accepting the offer from 1 or 3 random servers b
makes all the difference in the world in terms of security against DoS
and rollback. Brian wonders what the user experience for that is going
to be like.
Zooko veers the discussion onto Raph Levien's Trust Metric, and thence
onto Brian's cool "Not Tahoe" project.
discussion of Brian's Cool "Not Tahoe" Project. Zooko urges Brian to
make BCNTP be even more "Not-Tahoe-Like" by not having storage
servers. Not having storage servers means you don't have to worry
about how much different people's perspectives on the network overlap
with one another. Brian might use an S3-style hosted approach. He
might let a client publish which set of servers it uses, for the
benefit of its file-sharers. Also, file references can be fat in
Not-Tahoe, since they don't often get sent out of band, so they could
include information about servers.
back to docs: Evgeny's introductory doc is good! It is incomplete. It
is aimed at less sophisticated users. Zooko is unsure if it would
"fit" on the Download page of  or if it should
go somewhere else. It might make a good magazine article. We might end
up with too many started docs (that would be a good problem to have).
There is audience segmentation, for example the I2P starter docs have
been read by many I2P users but not by other users. Zooko wants better
docs for less sophisticated new users, so Zooko is inclined to use
Evgeny's doc. What's the next step? 1. LAFS developers answer some of
Evgeny's questions. b [[Marcus and David-Sarah?]] 2. Test out the doc
on some lab rats^W^Wnew users. b [[Evgeny?]] Evgeny has some users who
want to use Tahoe-LAFS who might beta-test his doc.
Tune in next time for:
 b" Proof-of-Retrievability; Zooko will post it to tahoe-dev in advance
of the meeting so you can read it before the meeting! Warning: it is
long. You might have to turn your phone off or something in order to
get through it. (David-Sarah quips that it probably isn't as long as
the Security Argument For Rainhill.)
 b" Report from Marlowe about translation efforts.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-10-09 12:07:46
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Tahoe-LAFS Weekly Dev Chat, 
I forgot to take notes during, so this is ex post facto. Other
participants, please feel free to reply and add your recollections!
In attendance: Zooko, CodesInChaos, Andrew, Brian, Ali who popped in
briefly because the hangout was posted as a public chat on G+,
The topic was mostly dynamic Merkle Trees and LDMFs again. (For those
following along at home, Dynamic Merkle Trees are this idea of using
something like a Red-Black Tree instead of a static Binary Tree as the
structure for your secure-hash-function based "authenticated data
structure". Andrew Miller claims that this is a unifying abstraction
that describes both git and Bitcoin, as well as any future Tahoe-LAFS
Large Distributed Mutable File. See previous week's notes 9.)
I said that I had been looking at using Tahoe-LAFS as the backend for
other systems, to give them decentralization, fault-tolerance,
integrity-checking, access control, and encryption. Those systems that
I've looked at a little bit include Ward Cunningham's Smallest
Federated Wiki, Dan Whaley's Hypothesis, and Jeff Garzik's
gleam-in-the-eye of a fault-tolerant form to make the Bitcoin forums
DoS-resistant. In each of these cases as well as in others, I have the
feeling that programmers want some sort of search or query language,
and when they find that LAFS doesn't natively offer that, they give up
on using it.
Brian replied that what you have to do is maintain your own index. He
called it "voluntary search", which is a turn of phrase that I like.
MK_FG's recent attempt to make a Skydrive plugin for the Cloud backend
is another example where voluntary search is needed.
Andrew asked for a specific use case for LDMF. How and why anyone
would use LDMFs even if we had them? After all, there's no way to use
the features of an LDMF, such as efficiently inserting bytes into the
middle of a file, through the standard POSIX file API.
I groped around for a while trying to answer that. There are a few
half-formed ideas about things that LDMFs *might* turn out to be
useful for, such as "filesystem in a file", which is basically what
the authors of virtual machines are doing to store their virtual
machine images in POSIX filesystems. Other half-formed ideas are to
use LDMFs as the backend storage for git.
But I finally got a hold of a use case that seems clear enough:
scalable directories. Directories ought to be able to hold arbitrarily
many entries, support efficiently adding or removing children, ought
to be able to maintain a sort order on the children, and support range
queries on the children. That *is* something that you can express
through the POSIX filesystem API, it seems clear that it could be
useful, and very importantly for the purposes of Andrew's research, it
is easy to evaluate how well your solution satisfies it.
Such directories might make a good building block for search. I
remember thinking when I unsuccessfully pitched LAFS as a possible
backend for Singly/LockerProject that if only directories had been
scalable, sorted, and range-queriable then LAFS might have sufficed
for their needs.
Along the way I pointed people at the performance.rst file:
Only to see that it appears to be describing the performance of old
SDMFs, not of MDMFs! For example it says:
Downloading B bytes of an A-byte mutable file
cpu: ~A
network: A
memory footprint: A
notes: As currently implemented, mutable files must be downloaded in
their entirety before any part of them can be read. We are exploring
fixes for this; see ticket  for more information.
Which is very stale information. We've closed  and made it so that
you only need to download about B bytes to read B bytes of an A-byte
mutable file. I thought we had updated performance.rst to reflect
that. What gives? Do I misremember or did we somehow regress
There are a few tickets about updating performance.rst to reflect the
existence of MDMF:  Andrew mentioned that since last week he implemented a new Red-Black
Merkle Tree in C++, and it is 400X faster than the first Python-based
prototype. I questioned why the language would make that much
difference and suggested he try PyPy.
 update
docs/performance.rst to explain the performance of MDMFs
 update docs to include MDMF
9 tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-10-20 12:39:41
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Tahoe-LAFS Sixth Birthday Party! Sat. Oct 27 
Details will follow as I work them out, but this is just to remind you
that yes, there *will* be a Tahoe-LAFS Sixth Birthday Celebration next
Saturday, October 27, 2012, in Boulder, Colorado and possibly
I'm going to ask the Boulder Hackerspace if we can use their space. It
is apparently populated with 3-D printers and computer-controlled
machining tools, which is awesome. b:
Shawn suggested using Google Hangouts to connect party locations. I
think that is a great idea! Everyone get Google Hangouts working
before Saturday.
I also strongly recommend the use of projectors to make the screen
real-estate be shared state among all the participants (who are
physically in one room). In contrast, laptop screens are not good for
physical sharing, and parties with laptop screens tend to result in
people reading their private email and stuff, where parties with
projector screens tend to result in people doing things that are more
interesting to everyone else present.
So, I really want to borrow at least two projectors for the Boulder
location. Anybody in Boulder have projectors we can use?
I am thinking of having LeastAuthority.com pay to make t-shirts that
say Tahoe-LAFS on them, and maybe "6th Birthday Party, Oct 27, 2012"
in small print somewhere. Want one? I'll pay the cost to manufacture
and deliver such a t-shirt to the first few [*] people who ask for
one! To get one: 1. sign up for  service
(which actually costs you *only* for the space you use, so it isn't a
big commitment), and 2. agree to show up at a Physical Tahoe-LAFS
Birthday Party Location to receive your t-shirt.
Zooko Wilcox-O'Hearn
Founder, CEO, and Customer Support Rep
[*] "The first few" = until I run out of t-shirts. I haven't yet
decided how many to make.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-10-27 08:08:49
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] where is meta data store in tahoe? 
Hi. In typical Distributed File Systems like gluster, the contents of
the files are distributed across many storage servers, and the
"metadata" is managed by a single metadata server or a small cluster
of metadata servers.
By "metadata", Distributed File Systems folks mean the directory
structure -- what files are under what paths -- and ownership and
permission bits and timestamps, and also maybe some information about
which storage servers are holding which files.
In Tahoe-LAFS, the directory structure part of that is encrypted by
the storage client and then uploaded to the same storage servers that
hold the file data. Those storage servers can't tell whether a given
block of ciphertext that they've been asked to hold contains encrypted
file data or encrypted directory data.
In Tahoe-LAFS, the "which storage server holds which file" question is
answered by the client *searching* the servers for the file when it
wants to read or write it. This means no metadata anywhere needs to be
updated when storage servers come and go, but it does mean more
network operations are needed to begin uploading or downloading a
file. We use a consistent hashing scheme, with a "tweakable" extension
invented by Brian Warner, to make it so that the client usually looks
in the right place to find the file with its first guess.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-09-11 10:59:55
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] notes from the Tahoe-LAFS Weekly Dev Call, 2012-09-11 
As usual, I'm not taking the time to contextualize and vet all these
notes. Caveat lector! Also, I've maintained my tradition of adding
some of my own thoughts that weren't actually expressed out loud in
the discussion. (In particular the advocacy for adding padding and the
ideas about how to do so.)
in attendance: Brian, David-Sarah, Andrew, Zooko (scribe)
b" topic: The compression attack on HTTPS.
(Not really relevant to Tahoe-LAFS, but interesting.)
Brian says one possible defense against this is to move your secrets from
cookies to the URL. This makes the attack impossible unless your secrets are
sharing a compression context with the attack.
Does anyone actually use TLS compression? Is it turned off by default? Does
anyone configure it on? A similar attack is possible on HTTP encryption, if the
higher-layer protocol includes attacker-controlled data in the payload.
This would be relevant to Tahoe-LAFS if we added compression. However,
even if we added compression, we would never mix attacker-controlled
data with attacker-unknown data in the same file. However, a higher-layer
protocol might mix them.
One caveat: convergent encryption could allow compression between
attacker-controlled and attacker-unknown data! In fact, there is a deep
connection between the adaptive-chosen-plaintext-compression-
violates-confidentiality ("CRIME") attack and the drewp "learn the
remaining information" attack!
The drewp defense -- the Added Convergence Secret -- is exactly the thing
that creates independent compression contexts in order to limit the scope for
Zooko and (perhaps to a lesser degree) Brian are uncomfortable with the fact
that LAFS currently exposes the length of your plaintext, to the byte level
of precision. Zooko wants to add padding. That would probably help against
the convergent-encryption-based CRIME attack which currently exists in hazy
nascent form in Zooko's mind. Also, it enhances general privacy, for example
an attacker might be able to recognize what files you are storing and reading
just from the lengths of the files. Padding could help against that. Padding
out to fixed boundary (e.g. to the next 16 bytes, or to the next 4096 bytes,
or whatever) helps but the information can still leak if there are a number
of files. For example, suppose you're browsing or downloading a directory
containing hundreds of files of varying lengths. The attacker knows the
lengths of some files that he suspects you might be browsing. Even if the
ciphertext is padded out to fixed sizes, thus "coarsening" or discretizing
the information, he might still be able to recognize the pattern. A better
defense is to add a random amount of padding, where the random amount is
determined by the (possibly convergently generated) encryption key.
b" topic: engineering tools and practices
We talked about usage of git. David-Sarah likes git-gui. Andrew asked if he
should rebase patches when submitting pull requests and Brian said yes. Brian
said always first rebase -- bring everything up to trunk -- and then rerecord
it as a set of logical commits. It is not necessary to squash it all down
into a single commit, unless that's what makes sense. Definitely squash out
ephemeral stuff like "Oops, made a typo, oops test didn't pass, let me go
back and fix that.". It often makes sense to make four patches: First
refactor the code so that there is no actual functional changes, second
cosmetic changes like whitespace, third update the unit tests, fourth the new
Ideally, the revision history tells a story.
b" topic: When do we kill off darcs?
David-Sarah still has some patches that need to be darcs pushed. But they
could diff-and-patch those to git.
At some point soon we'll all agree to stop pushing patches into darcs.
LeastAuthority.com's Cloud Backend is currently in darcs. We are scheduled to
merge the cloud backend to trunk within three weeks.
There is still the question of how to handle hyperlinks into
that point at darcs patches and history.
b" topic: Will Cloud Backend, leasedb, and accounting go into Tahoe-LAFS v1.11?
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2012-09-27 09:27:22
@_author: Zooko Wilcox-O'Hearn 
@_subject: [tahoe-dev] Tahoe-LAFS Weekly Conference report, 2012-09-25 
caveat lector
2012-09-25 -- "The Science Fair episode"
in attendance: Zooko (scribe), CodesInChaos, amiller, David-Sarah, elb
Topic: proof-of-storage/proof-of-retrievability
CiC suggested a pass-through "Chess Grandmaster" style attack of
storage server which doesn't hold the data but queries other servers
to answer challenges.
amiller suggested that perhaps not knowing the verify cap would
prevent a storage server from doing that.
CiC pointed out that if you are a malicious storage server who wants
to defect, you won't do that when there are K other, non-malicious,
storage servers online. You might as well wait until there are not
enough non-malicious storage servers left, so that your defection can
accomplish some real harm.
CiC mentioned his standard merkle tree design, but didn't get much
opportunity to say much about it.
There was extensive discussion about the very notion of
Proof-of-Storage and Proof-of-Retrievability, and how they could be
applied to LAFS. I (Zooko) intend to write notes to tahoe-dev about it
soon. David-Sarah had simulation results about the idea of
Proof-of-Storage by adding erasure-coding redundancy to each share
stored on an individual server.
After CiC disconnected, amiller and davidsarah proposed standardizing
a hash-dag instead of a hash-tree, with the tree being a special case
of the dag. There was a bit about having a tweak to make hash
collision attacks harder.
Andrew Miller talked about the Bitcoin blockchain and a git repository
are similar data structures.
Zooko told Andrew Miller that digital signatures built out of secure
hash functions normally use hash trees, but that one design, due to
Bleichenbacher and Maurer, uses hash-dags instead.
The full sub-graph of a LAFS filesystem which is  reachable starting
from an immutable dir constitutes a hash dag. Not so for mutable dirs.
tahoe-dev mailing list
tahoe-dev at tahoe-lafs.org

@_date: 2013-08-16 23:05:24
@_author: zooko 
@_subject: [cryptography] LeastAuthority.com announces PRISM-proof storage service 
I'm not sure what your question is. The available interfaces to the gateway -- i.e. the cleartext side that is marked in red on [1] -- are:
* the "tahoe" command-line tool [2]
* your unadorned web browser, even with JavaScript turned off, pointed at the gateway over localhost (or over SSL to a remote host, or whatever you want)
* your FTP or SFTP client
* FUSE (although in a Rube Goldberg-esque setup where FUSE is chained to the aforementioned SFTP server through the "sshfs" tool; Like a Rube Goldberg device, it actually does work once you get all the pieces set up next to each other.)
The semantics of what you can do with this are described in summary here:
And in much more detail in the documentation pages linked from there.
Does that answer your question?
[1] [2] P.S. This is a test of charset handling through GNU screen, mutt, and GNU mailman: 
(That should be a superscript "1".)
cryptography mailing list
cryptography at randombit.net

@_date: 2013-08-16 23:11:22
@_author: zooko 
@_subject: [cryptography] LeastAuthority.com announces PRISM-proof storage service 
I agree that compromise of the client is relevant. My current belief is that
nobody is doing this on a mass scale, pwning entire populations at once, and
that if they do, we will find out about it.
My goal with the S4 product is not primarily to help people who are being
targeted by their enemies, but to increase the cost of indiscriminately
surveilling entire populations.
Now maybe it was a mistake to label it as "PRISM-Proof" in our press release
and media interviews! I said that because to me "PRISM" means mass surveillance
of innocents. Perhaps to other people it doesn't mean that. Oops!
cryptography mailing list
cryptography at randombit.net

@_date: 2013-10-01 15:45:27
@_author: zooko 
@_subject: On 128-bit security 
Here are my personal opinions about these issues. I'm not expert at
cryptanalysis. Disclosure: I'm one of the authors of BLAKE2 (but not
one of the authors of BLAKE).
I personally do not believe that there is any secret agenda behind
this proposal, even though I believe that there was a secret agenda
behind Dual EC DRBG.
One reason that I believe that the motivation behind this proposal is
the stated motivation of improving performance, is that Joan Daemen
told me in person in January of 2013 that the Keccak team had
considered defining a reduced Keccak to compete with BLAKE2, but had
decided against it because they didn't want to disrupt the SHA-3
standardization process.
Apparently they changed their minds, and apparently their fears of
disruption turned out to be prescient!
I also do not think that a "security level" of 2^256 is necessarily
better than a "security level" of 2^128. *Maybe* it is better, but I'm
not aware of any examples where that sort of distinction has turned
out to matter in practice, and I can't really judge if it is likely to
matter in the future (except, of course, if you forget to take into
account multi-target issues). I suspect nobody else can, either.
However, even though I *personally* would have confidence that a
Keccak with a 256-bit capacity would be safe and would be free of
maliciously induced weakness, I want a standard to be widely accepted
in addition to being safe.
This is the "Caesar's wife must be above suspicion" argument. It isn't
enough to make a secure standard, but also we need other people to
have confidence in it.
And, I don't know if we can persuade people that "no it isn't actually
backdoored/weakened". It may be the kind of thing where if that's the
conversation we're having then we've already lost.
Would it make sense to go ahead and standardize
SHA3-as-a-replacement-for-SHA2 by standardizing the form of Keccak
which is most widely accepted by cryptographers and which is closest
to what was studied during the contest, and then separately offer
SHAKE and reduced-for-speed-Keccak as additional new things?
A lot of uses of secure hash functions don't need to be particularly
efficient. In my slides about BLAKE2
( I argue that there are use-cases
where efficiency is critical, but it is equally true that there are
common and important use cases where a 576-bit capacity Keccak would
be fine, e.g. public key certificates.
Joan Daemen, one of inventors of AES and one of the inventors of
Keccak (SHA-3), replied to my mailing list post as follows:

@_date: 2013-10-04 21:44:23
@_author: zooko 
@_subject: how to use Tor securely (Re: Silk Road founder arrested ...) 
References:  <1380740444.30026.18.camel
 <20131002193108.GA11783
 <1380742664.5216.3.camel
 <20131004002225.AB0F4DFB9
 <20131004081652.GK15039
 <20131004090126.GA2045
 <524E9590.702 <20131004142202.GA19027
 <20131004144038.GA9295
Thanks for mentioning Tahoe-LAFS, Adam. I think combining Tahoe-LAFS with Tor
is a good idea. It is already almost there. It is usable, but it doesn't
yet protect your anonymity correctly. There is a recent burst of work to
improve usability, security, and performance, and we need help.
Also, below, I'll talk about the different, but complementary idea of
"decentralized web apps" (i.e. Javascript apps hosted on Tahoe-LAFS).
But before we get into decentralized web apps, here's the status of
Tahoe-LAFS+Tor: it is currently working, by using a socks proxy that routes
through Tor and configuring your Tahoe-LAFS instance to use it.  Here are some
open issue tickets about tweaks to the Tahoe-LAFS software or documentation
which are needed:
 use only 127.0.0.1 as local address
 Improve docs about Tahoe-LAFS+Tor
 make tahoe Tor- and I2P-friendly
Note that some of these tickets refer to I2P, which is another re-routing
network sort of like Tor, but the ones I mentioned above are just as applicable
to Tor as to I2P. The tickets mention I2P because I2P developers, in addition
to Tor developers, are contributing bug reports and patches.
There is a recent move to a better approach which doesn't require the user to
configure a socks proxy. That approach is to switch Tahoe-LAFS to using a new
network abstraction provided by the "Twisted" library which Tahoe-LAFS uses.
That abstraction is named "Endpoints". The idea is to switch Tahoe-LAFS from
IPv4 to "Endpoints", and then implement Tor and I2P routing as implementations
of the "Endpoints" abstraction. This approach would also probably work with a
cjdns transport, too. (Also it would help Tahoe-LAFS work over IPv6.)
This approach would also allow other Twisted-based applications (besides
Tahoe-LAFS) to use those interesting new transport layers.
We could use help! If you know Python, please do code-review of these patches:
 switch to using Endpoints
How to review patches:
By the way, we LAFS hackers are well aware that anonymity is very hard. In
fact, low-latency anonymity against a modern "global surveillance" threat model
may be impossible. But even the anonymity properties that *are* possible, and
the ones that are currently provided by Tor, might get ruined by some mistake
that Tahoe-LAFS makes, so I wouldn't rely on the Tahoe-LAFS+Tor for anonymity
until it has had a lot more study and testing. (Which we need help with!)
I don't quite follow this sentence. You can write code that uses Tahoe-LAFS
from Javascript if you want. I think that is a *great* idea, and I think that
it is inevitable that in the future "decentralized web apps" will be written in
either Javascript + LAFS, or else Javascript + some-other-decentralized-
However, if you are not ready to accept the inevitable and start running
Javascript in your web browser, you can also poke at Tahoe-LAFS from plain old
HTML forms, or use Tahoe-LAFS from code (written in any programming language)
running on your local machine.
Here's a live demo of a "decentralized web app" in which all storage in an
encrypted, decentralized, fault-tolerant storage network, and all computation
is in the client -- in fact in the web browser. The demo is my blog:
That link gives you read-only access to my blog. If you interact with it, for
example by clicking on "Tags" or using the search box, then you're interacting
with Javascript running in your web browser. When *I* interact with it (I have
read-write access to my blog), for example by creating new entries or editing
existing entries, then I too am interacting Javascript running in my browser.
There is no server anywhere that has code for the functionality of my blog. All
code that implements functionalit is in the client. The storage server does
nothing but store ciphertext to which it doesn't have the decryption key.
Now, there's a subtlety here that will probably confuse some people. To be
truly *decentralized*, the URL you put into your browser has to start with
" rather than with "
right? So when you look at the URL above, you aren't actually *using* a
decentralized web app, you're looking at a demo of a decentralized web app.
To put it another way, when *I* use  I'm using a
decentralized web app, because I control my own node in the network. When I
allow *you* to use  then you are not controlling your own
node in the network -- you are relying on me and on my node.
But if you install Tahoe-LAFS yourself and connect to the Public Test Grid,
then you can join us in playing with true decentralized web apps. :-)
Start with the newest version of TiddlyWiki, which comes with a Tahoe-LAFS
(The demo above -- my blog -- is a much older and more kludgey combination of
TiddlyWiki and Tahoe-LAFS.)
