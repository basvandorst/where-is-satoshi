
@_date: 2002-07-05 03:52:52
@_author: Seth David Schoen 
@_subject: Ross's TCPA paper 
References:  <5.1.1.6.2.20020703225147.0b94ecd0 <20020704205410.GA8220
The latter is closest to what's intended in Palladium.  Individual
programs using Palladium features are able to prevent one another from
reading their executing or stored state.  You can write your own
programs, but somebody else can also write programs which can process
data in a way that your programs can't interact with.
The Palladium security model and features are different from Unix, but
you can imagine by rough analogy a Unix implementation on a system
with protected memory.  Every process can have its own virtual memory
space, read and write files, interact with the user, etc.  But
normally a program can't read another program's memory without the
other program's permission.
The analogy starts to break down, though: in Unix a process running as
the superuser or code running in kernel mode may be able to ignore
memory protection and monitor or control an arbitrary process.  In
Palladium, if a system is started in a trusted mode, not even the OS
kernel will have access to all system resources.  That limitation
doesn't stop you from writing your own application software or scripts.
Interestingly, Palladium and TCPA both allow you to modify any part of
the software installed on your system (though not your hardware).  The
worst thing which can happen to you as a result is that the system
will know that it is no longer "trusted", or will otherwise be able to
recognize or take account of the changes you made.  In principle,
there's nothing wrong with running "untrusted"; particular applications
or services which relied on a trusted feature, including sealed
storage (see below), may fail to operate.
Palladium and TCPA both allow an application to make use of
hardware-based encryption and decryption in a scheme called "sealed
storage" which uses a hash of the running system's software as part of
the key.  One result of this is that, if you change relevant parts of
the software, the hardware will no longer be able to perform the
decryption step.  To oversimplify slightly, you could imagine that the
hardware uses the currently-running OS kernel's hash as part of this
key.  Then, if you change the kernel in any way (which you're
permitted to do), applications running under it will find that they're
no longer able to decrypt "sealed" files which were created under the
original kernel.  Rebooting with the original kernel will restore the
ability to decrypt, because the hash will again match the original
kernel's hash.
(I've been reading TCPA specs and recently met with some Microsoft
Palladium team members.  But I'm still learning about both systems and
may well have made some mistakes in my description.)

@_date: 2005-09-05 06:10:02
@_author: Seth David Schoen 
@_subject: [E-PRV] Internet phone wiretapping ("Psst! The FBI is 
Having Trouble on the Line", Aug. 15)
The original article is at
(subscription required)
Here's the letter we sent:
    Your account of FBI efforts to embed wiretapping into the design of
    new Internet communication technologies ("Psst! The FBI is Having
    Trouble on the Line," Notebook, August 15) is in error.
    You claim that police "can't tap into [Internet] conversations or
    identify the location of callers, even with court orders."
    That is false. Internet service providers and VoIP companies have
    consistently responded to such orders and turned over information
    in their possession. There is no evidence that law enforcement is
    having any trouble obtaining compliance.
    But more disturbingly, you omit entirely any reference to the
    grave threat these FBI initiatives pose to the personal privacy
    and security of innocent Americans. The technologies currently
    used to create wiretap-friendly computer networks make the people
    on those networks more pregnable to attackers who want to steal
    their data or personal information. And at a time when many of our
    most fundamental consititutional rights are being stripped away in
    the name of fighting terrorism, you implicitly endorse opening yet
    another channel for potential government abuse.
    The legislative history of the Communications Assistance for Law
    Enforcement Act (CALEA) shows that Congress recognized the danger
    of giving law enforcement this kind of surveillance power "in the
    face of increasingly powerful and personally revealing
    (H.R. Rep. No. 103-827, 1994 U.S.C.C.A.N. 3489, 3493 [1994] [House
    Report]). The law explicitly exempts so-called information
    law enforcement repeatedly assured civil libertarians that the
    Internet would be excluded. Yet the FBI and FCC have now betrayed
    that promise and stepped beyond the law, demanding that Internet
    software be redesigned to facilitate eavesdropping. In the coming
    months, we expect the federal courts to rein in these dangerously
    expansive legal intepretations.
Seth Schoen
Staff Technologist                                schoen at eff.org
Electronic Frontier Foundation                    454 Shotwell Street, San Francisco, CA  94110     1 415 436 9333 x107
You are subscribed as eugen at leitl.org
To manage your subscription, go to
 Archives at: Eugen* Leitl leitl
ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2006-05-19 11:51:54
@_author: Seth David Schoen 
@_subject: Threats to anonymity set at and above the application layer; HTTP 
User-Agent: Mutt/1.5.11
Reply-To: or-talk at freehaven.net
It's pretty well understood that anonymity can be lost at higher protocol
layers even when it's well protected at lower layers.
One eye-opening paper on this point is "Can Pseudonymity Really Guarantee
Privacy?" by Rao and Rohatgi (in the Freehaven Anonymity Bibliography):
This is a philosophically interesting problem; it prompts the question "if
pseudonymity can't guarantee privacy, what _can_?".  (Rao and Rohatgi
remind us that the authors of the Federalist Papers used pseudonyms and
were still identified solely from the evidence of their writing.)
There is also the scary field of timing attacks on users' typing:
(The Tygar paper is not really relevant for network surveillance, but
it shows the scariness of statistical methods for figuring out what
users are doing based on seemingly irrelevant information.)
In a sense, there are many privacy-threatening features in and above the
application layer (some of them depending on the nature and latency of a
* timing of access (what time zone are you in, when do you usually do
something?) -- for communications with non-randomized latency < 1 day
* typing patterns (cf. Cliff Stoll's _Cuckoo's Egg_ and the Song et al.
* typing speed
* language comprehension and selection
* language proficiency
* idiosyncratic language use
* idiosyncratic language errors (cf. Rao and Rohatgi)
* cookies and their equivalents (cf. Martin Pool's "meantime", a cookie
equivalent using client-side information that was intended for a
totally different purpose -- cache control)
* unique browser or other application headers or behavior (distinguishing
MSIE from Firefox from Opera? not just based on User-agent but based on
request patterns, e.g. for inline images, and different interpretations
of HTTP standards and perhaps CSS and JavaScript standards)
* different user-agent versions (including leaked information about the
* different privoxy versions and configurations
I'm not sure what to do to mitigate these things.  The Rao paper alone
strongly suggests that providing privacy up to the application layer
will not always make communications unlinkable (and then there is the
problem of insulating what pseudonymous personae are supposed to know
about or not know about, and the likelihood of correlations between
things they mention).
These problems are alluded to in on the Tor web site:
   Tor can't solve all anonymity problems. It focuses only on protecting
   the transport of data. You need to use protocol-specific support
   software if you don't want the sites you visit to see your identifying
   information. For example, you can use web proxies such as Privoxy
   while web browsing to block cookies and withhold information about
   your browser type.
   Also, to protect your anonymity, be smart. Don't provide your name
   or other revealing information in web forms. Be aware that, like
   all anonymizing networks that are fast enough for web browsing, Tor
   does not provide protection against end-to-end timing attacks: If
   your attacker can watch the traffic coming out of your computer,
   and also the traffic arriving at your chosen destination, he can
   use statistical analysis to discover that they are part of the same
   circuit.
However, the recommendation to use Privoxy, by itself, is far from
solving the problem of correlations between user and user sessions.
I think a low-hanging target is the uniqueness of HTTP headers sent by
particular users of HTTP and HTTPS over Tor.  Accept-Language, User-Agent,
and a few browser-specific features are likely to reveal locale and OS
and browser version -- sometimes relatively uniquely, as when someone
uses a Linux distribution that ships with a highly specific build of
Firefox -- and this combination may serve to make people linkable or
distinguishable in particular contexts.  Privoxy does _not_, depending on
its configuration, necessarily remove or rewrite all of the potentially
relevant HTTP protocol headers.  Worse, different Privoxy configurations
may actually introduce _new_ headers or behaviors that further serve to
differentiate users from one another.
One example is that some Privoxy configurations insert headers specifically
identifying the user as a Privoxy user and taunting the server operator;
but if some users do this and other users don't, the anonymity set is
chopped up into lots of little bitty anonymity sets.  For instance:
+add-header{X-User-Tracking: sucks}
User tracking does suck, but adding an optional header saying so has the
obvious effect of splitting the anonymity set in some circumstances into
people who send the X-User-Tracking: sucks header and people who don't.
Any variation in practice here is potentially bad for the size of the
anonymity set.
A remedy for this would be to try to create a standardized Privoxy
configuration and set of browser headers, and then try to convince as
many Tor users as possible to use that particular configuration.  (One
way to do this is to try to convince everyone who makes a Tor+Privoxy
distribution or product to use the agreed-upon default configuration.)
The goal is not to prevent people from controlling their own Privoxy
configurations or doing more things to protect their privacy; rather,
it is to try to reduce the variety in headers and behaviors seen by
web servers contacted by Tor users on different platforms.
Seth Schoen
Staff Technologist                                schoen at eff.org
Electronic Frontier Foundation                    454 Shotwell Street, San Francisco, CA  94110     1 415 436 9333 x107
Eugen* Leitl leitl ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2008-12-05 01:22:07
@_author: Seth David Schoen 
@_subject: No data retention in germany for donated services 
I'm not a lawyer in Germany or any jurisdiction and I don't have any
knowledge or opinion of the convincingness or legal well-foundedness
of this article.  I encourage anyone who might want to rely on it to
seek the expert opinion of a German lawyer.  But I do read German, so
I've translated Karsten's note and (most of) the text of the article
below for the benefit of anyone interested in this material who doesn't
read German.
NO DATA RETENTION FOR FREE-OF-CHARGE SERVICES
  Original German text of this article "Keine Vorratsdatenspeicherung fCleitl ICBM: 48.07100, 11.36820  8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE

@_date: 2011-02-02 10:24:08
@_author: Seth David Schoen 
@_subject: What are email risks? 
Hi Bjarni,
There is a stylometry item in the anonbib where they do statistical
analysis of features of writing style:
I bet these techniques have gotten more powerful as the field of
machine learning has developed, although I don't know if there are
more recent studies of what this means for anonymity.

@_date: 2012-08-04 19:17:58
@_author: Seth David Schoen 
@_subject: [liberationtech] Revised Liberationtech Mailing List Guidelines 
This is totally true in general, and of course these zero days have been
used in real attacks, and of course Google can't necessarily recognize
zero-day vulnerabilities.
In the particular case of text documents shared through Google Docs -- as
opposed to Word files hosted for download with some sort of file sharing
site! -- I think malware is a comparatively minor risk.  The reason is that
when you upload a document to Google Docs, Google imports the content of
the document into Google's own internal format.  When you then download a
document from Google Docs, Google is generating _a new document from
scratch_ with the same text and formatting content as the original, but
the result is not the same file that was originally uploaded.
If someone mails you an attachment, or hosts a document file of their own
creation on a web site, your word processor could be compromised if there
are software vulnerabilities that the document exploits, like a buffer
overflow.  And this is also true of, say, a PDF document that you're going
to open in a PDF reader; we know that there have been exploits used in the
wild against PDF readers.
By contrast, if you were to import some Microsoft Word file into Google
Docs and then export the resulting Google Docs document in Microsoft Word
format, what you'd get back would _not_ be the original file or any
modified form of the original file.  Instead, you would get a completely
new Microsoft Word file, generated from scratch by Google, with essentially
the same textual content as the original.  (And if you were to export the
Google Docs document as a PDF, what you'd get would be a PDF that Google
generated from scratch.)
Since these documents are being generated by Google in this way, using
its own internally-developed software, Google will presumably create safe
and valid documents for its users, not ones that contain exploits and
We might still worry that someone could _upload_ a malicious document to
Google in order to attack Google's import process (and perhaps attack the
Google Docs servers in various ways, whether to disable other security
features or access private information), but I presume Google's security
folks have been very cautious about this aspect and Google Docs import
is probably much less vulnerable to malware and exploits than the file
import features in popular desktop word processors like Microsoft Word,
OpenOffice, and LibreOffice.  (Also, attackers can study the binary code
of Microsoft Word -- as well as Microsoft's security patches to it! --
or the source code of OpenOffice and LibreOffice -- as well as their
developers' security patches to them! -- in order to try to find specific
vulnerabilities.  It's harder for attackers to speculate usefully about
what vulnerabilities may exist in Google Docs import functionality because
the attackers probably don't have access to any of the Google Docs code,
whether source or binary.  So even if there are exploitable vulnerabilities
in the way Google Docs parses documents, it will be much harder for
attackers to find and exploit them than it would be for published desktop
(How do I square this with my observation that "Google can't necessarily
recognize vulnerabilities"?  I think the main point is that the zero-day
vulnerabilities we're likely to encounter are vulnerabilities in
desktop software.  Google may not be able to detect these, but it may not
be vulnerable to them either!  And with cautious programming, it can also
default to rejecting files that are suspicious in some general ways, even
if it doesn't know exactly what's bad about them.  For instance, Andreas
Bogk gave a talk last year at the CCC Camp about a PDF security scanner
he's been developing which is able to reject several kinds of invalid PDFs
automatically.  Some of those invalid PDFs may be innocent and not contain
any malware or exploits, but Google could still use a scanner like this to
reject them and refuse to import them out of an abundance of caution.)

@_date: 2012-12-21 19:07:14
@_author: Seth David Schoen 
@_subject: [liberationtech] Google Hangout the new, 
I sympathize with your frustration about Google and other companies'
unwillingness to talk about their interception capabilities.  In the
particular case of Hangouts, it seems clear that the Hangout data is
encrypted only between the user and Google, and not end-to-end.  If
so, intercepting Hangouts is even easier for Google than intercepting
Skype calls is for Microsoft, since they don't even have to tamper
with the key exchange process.  They can just program their servers
to passively record cleartext data already in their possession.
It's disconcerting to see what a low priority secure end-to-end
encryption continues to be for most designers of communications
systems.  (There might be technical reasons, too -- like wanting to
transcode video, translate it, add captions, etc., but if people won't
talk about the subject at all, we might never know the exact balance
of factors that led to their decisions.)
Two challenges for end-to-end encryption which have been discussed on
this list are that many people want to access particular communications
systems from multiple devices, and they may expect to use some services
with a web browser instead of by installing a native client.  The former
means they might expect to access a service from a device where their
private key isn't available (and, if they manage to copy the private key
onto many devices, the risk of key compromise goes way up); the latter
means that they're at risk of receiving a fresh backdoored version every
time they connect.  But we may be able to solve both of these things to
some extent.
A thornier challenge is that articulated demand for end-to-end crypto
is very low, and arguably _falling_.  So even though many of us have
strongly criticized Skype's security model for years, they've felt no
obvious embarrassment or need to change it, and others have felt no
compunction about introducing new products with even lower levels of
cryptographic protection, or even with explicit backdoors (like current
work at ETSI on next-generation GSM voice encryption)!  If Google
_were_ willing to comment, they might say that very few users had
voiced any objection to the Hangout security model, and that the
product continues to be adopted on a huge scale, providing incremental
security benefits relative to using a telephone.

@_date: 2012-06-27 22:28:20
@_author: Seth David Schoen 
@_subject: [tor-talk] possible to identify tor user via hardware DRM? 
I find this message misleading in various ways.  The basic thing that
I've been telling people is that there are few situations in which
either PSN or TPM uniqueness makes things qualitatively worse.
There are lots of hardware unique IDs.  On Linux, try "sudo lshw" and
be surprised at all the things that have unique serial numbers.  There
are also things that are unique about your machine that are not
hardware serial numbers, like filesystem serial numbers and observed
combinations of software configurations.
These can be bad for privacy because software can tell which computer
it's running on.  If the software has an adversarial relationship with
you, it can then use that information in a way that you don't like.
We would be better off in some regards if operating systems let us
hide local uniqueness from software so that the software couldn't tell
what machine it was running on, or set fake values for these unique
Some proprietary software including Microsoft Windows already makes
a sophisticated profile of the local machine, including many kinds of
observations, to tie a copy of the software (or an "activation") to a
particular device (!).
The only substantive difference with the TPM uniqueness is that the TPM
uniqueness lets you prove (like a smartcard) to a remote system that
you're running some software on the same machine as before.  Even if
the OS did let you set fake values when software tried to examine the
system it was running on, the remote system could see that the
TPM-related values were fake.  That's useful for some applications,
including but not limited to DRM-like ones.
I've argued that this is bad in some ways, but at least you can still
turn off the TPM.  Then your system can't attempt to offer that kind
of proof.  As far as I know, turning off the TPM is pretty robust:
it really is turned off.
All of these things are anonymity problems in particular when some
software on your computer is actively _trying_ to tell someone else
what machine you're running on, either because it's programmed to do
so or because someone has broken into your computer and installed
spyware and is trying to use it to monitor you.  If you're not in
that situation, there is nothing especially magical about having
unique hardware IDs in your machine, because everyone's machine has
some uniqueness, and (for the most part) that uniqueness isn't part
of standard network protocols like TCP/IP and doesn't automatically
leak out to anyone and everyone you communicate with over the
Internet.  (There is a possible exception about clock skew, which you
can read about in Steven Murdoch's paper from 2006.)
Similarly, having a GPS receiver in your phone does not mean that
everyone you send an SMS to or everyone you call will learn your
exact physical location.  However, it does mean that if there's
spyware on your phone, that spyware is able to use the GPS to learn
your location and leak it.  If you're worried about spyware threats
on your phone, which can be quite a realistic concern, the GPS
itself isn't necessarily the unique core of the threat, because
there are also lots of other things in the phone that can be read to
help physically locate you (like wifi base station MAC addresses,
taking photographs of your surroundings with the phone's camera,
recording the identities and signal strengths of the GSM base
stations your phone sees...).  So a more fundamental question might
be whether your phone operating system is able to either prevent
you from getting malware or prevent the malware from accessing the
sensors on your phone.
In the case of a desktop PC, the hardware uniqueness is _there to
be read by software_, and if it's in a TPM it _may be able to give
the software remotely verifiable cryptographic proof that the
software is really running on the machine containing that particular
TPM_.  In neither case does the hardware uniqueness directly
broadcast itself to other machines, and in neither case does the
hardware uniqueness prevent the operating system from preventing
other software from reading it.
If you do have some kind of software running on your machine that's
trying to track you or trying to help other people track you,
hardware uniqueness is one thing that the software might look at.
But if you're a Tor user, a more basic thing for the software to
try to do is make network connections to leak your real IP address
in order to associate your Tor-based network activity with your
non-Tor-based network activity.  That might be even easier because
the tracking software could just try to make a direct network
If you're not using Tor, at least not at a particular moment, but
are still concerned about tracking, there's another problem, which
is that all existing browsers _already_ reveal a great deal of
software-based uniqueness to any interested web site, usually enough
to make your browser unique.  See
This is important because it doesn't require there to be any
malicious software on your computer, just a traditional web browser.
One of the defenses people have talked about against hardware
fingerprinting is running inside a virtual machine.  Normally,
software inside the virtual machine, even if it's malicious,
doesn't learn much about the physical machine that hosts the VM.
If you always use Tor inside a VM, then even if there's a bug
that lets someone take over your computer (or if they trick you
into installing spyware), the malicious software won't be able
to read much real uniqueness from the host hardware, unless
there's also a bug in the VM software.
Running in a VM isn't exactly a defense against software
fingerprinting (like browser fingerprinting) if you use the VM
for various non-Tor activities that you don't want to be linked
to one another, because the software configuration inside the VM
might be, or become, sufficiently different from others that it
can be recognized.  There's probably more research to be done
about the conditions under which VMs can be uniquely identified
both "from the inside" by malware, and remotely by remote
software fingerprinting, absent VM bugs that give unintended
access to the host.

@_date: 2012-10-04 17:06:27
@_author: Seth David Schoen 
@_subject: [liberationtech] CryptoParty Handbook 
I'm grateful to people for doing this (and happy that it built upon some
prior sprints that I was part of!) but I'm a bit worried about errors.
Starting from the end of the book I fairly quickly came upon two things
that concerned me:
"Quantum cryptography is the term used to describe the type of cryptography
that is now necessary to deal with the speed at which we now process
information and the related security measures that are necessary.  Essentially
it deals with how we use quantum communication to securely exchange a key
and its associated distribution.  As the machines we use become faster the
possible combinations of public-key encryptions and digital signatures
becomes easier to break and quantum cryptography deals with the types of
algorithms that are necessary to keep pace with more advanced networks."
I think the first and third sentences of this paragraph are completely
mistaken.  (The second sentence is right to assert that quantum cryptography
deals with key-exchange mechanisms.)  First, quantum cryptography for key
exchange is unrelated to "the speed at which we now process information".
In fact, conventional encryption has scaled well and more than kept pace
with increases in communications data rates, particularly since our CPUs
have gotten faster much faster than our communications links have.  That's
one reason that it's now much more feasible to use HTTPS routinely for web
services -- current CPUs can handle the ciphers involved efficiently, and
some CPUs even have hardware acceleration for AES.  It's also not clear that
using quantum cryptography is "necessary" for anyone today.  QKD still
requires strong authentication
so although it could reduce the need to make assumptions about the difficulty
of solving math problems that are used in other forms of key distribution,
it does _not_ make the authentication problem go away.  The authentication
problem is the logistically difficult thing about using all distributed
cryptosystems, so when you use QKD you still encounter these logistical
difficulties, in addition to (in most existing implementations) the extra
major logistical difficulty of needing a physically directly connected
fiber optic cable (!) between the parties who are trying to establish a key.
The book's suggestion that "[a]s the machines we use become faster the
combinations of public-key encryptions and digital signatures becomes easier
to break" is also not an argument for using quantum cryptography, just
appropriate key lengths.  See
NIST and others have thought about what appropriate cryptographic key lengths
are to respond to the phenomenon of computers getting faster.  That's why
current NIST recommendations call for using 2048-bit RSA instead of 1024-bit
RSA -- not a quantum cryptosystem, just a stronger key length.
I was also concerned by the "Securely Destroying Data" section.  Although it
acknowledges some situations under which erased data (or even overwritten
data) could be recovered, it seems to treat these situations as exceptional
and multiple-overwrite tools generally reliable.  It doesn't mention that
these tools are potentially quite untrustworthy on current filesystems even
under normal conditions, because of data journaling.  (I first learned about
this problem from John Gilmore.)  In fact, even the man page for shred gives
a warning about this:
       CAUTION: Note that shred relies on a very important assumption:
       that the file system overwrites data in place. This is the
       traditional way to do things, but many modern file system designs
       do not satisfy this assumption. The following are examples
       of file systems on which shred is not effective, or is not
       guaranteed to be effective in all file sysb tem modes:
       * log-structured or journaled file systems, such as those
       supplied with AIX and Solaris (and JFS, ReiserFS, XFS, Ext3,
       etc.)
       * file systems that write redundant data and carry on even if
       some writes fail, such as RAID-based file systems
       * file systems that make snapshots, such as Network Appliance's
       NFS server
       * file systems that cache in temporary locations, such as NFS
       version 3 clients
       * compressed file systems
       In the case of ext3 file systems, the above disclaimer applies
       (and shred is thus of limited effectiveness) only in data=journal
       mode, which journals file data in addition to just metadata. In
       both the data=ordered (default) and data=writeback modes, shred
       works as usual. Ext3 journaling modes can be changed by adding
       the data=something option to the mount options for a particular
       file system in the /etc/fstab file, as documented in the mount
       man page (man mount).
The wipe man page says
       Journaling filesystems (such as Ext3 or ReiserFS) are now being
       used by default by most Linux distributions. No secure deletion
       program that does filesystem-level calls can sanitize files
       on such filesystems, because sensitive data and metadata can
       be written to the journal, which cannot be readily accessed.
       Per-file secure deletion is better implemented in the operating
       system.
Some people see this concern as hypothetical, but it's pretty easy to
test with loopback mounting.  I just made a 100 MB file, initialized it
with zeroes, created an ext4 filesystem in it, and loopback mounted the
filesystem.  Then I created several very large text files with repeating,
easy-to-recognize contents, and then deleted the files with shred -u.
It was still possible to find a small number of copies of the text file
contents in the underlying storage file afterward -- probably because of
data journaling in ext4.
The book spends several pages describing how to make GUI interfaces for
wipe and shred under GNOME, remarks that wipe is "a little more secure"
than shred, and doesn't mention that (according to their own official
documentation) neither program can be assumed to work properly on a modern
system! :-(
Things like this make me worry that this book needs some more work.

@_date: 2013-04-03 12:54:07
@_author: Seth David Schoen 
@_subject: [liberationtech] suggestions for a remote wipe software for 
I think Prey is a pretty compelling choice for a lot of cases, but looking
briefly at the documentation it seems that their "remote wipe" functionality
for laptops is currently quite limited.  And that's confirmed by looking at
the "secure" module in the Prey source code.
I've suggested Prey to people before for tracking stolen devices in order to
recover them, but I don't think I could recommend it for remote wipe.  It seems
to mainly use plain rm to delete the contents of a small number of directories,
and to call an API to clear MSIE browser history data.  For many users, this is
a pretty incomplete notion of "wipe", and most of the content deleted this way
will be recoverable by forensics.
A further problem that comes to mind is that sending a signal to a phone (that
uses 3G networks) to wipe itself is going to be easier in a lot of cases than
to a laptop (that uses mainly wifi, and maybe not opportunistically).  The
laptop will likely be offline by default if someone removes it from its normal
environment, so it won't hear the wipe signal.  Solutions like Prey for laptops
mainly work because thieves or downstream purchasers may voluntarily connect
stolen laptops to networks to use them without reinstalling them (at least if
the laptops don't require, or seem not to require, a login password!).
Mike Cardwell actually uses a decoy operating system (with Prey) on his laptop
in order to tempt thieves to use it:
I'm quite impressed with his setup, which took him a great deal of time and
thought.  He relies entirely on encryption to get the equivalent of remote
wiping; his Prey install is there just to increase his chances of finding the
laptop if it's taken by "common thieves".
This is some ways away from the original poster's question about remote wiping
a Windows installation.   I guess I want to agree with Eugen Leitl (and Mike
Cardwell) that disk encryption ultimately does that job better, mainly since a
sophisticated or targeted attacker wouldn't connect the laptop to a network
before making a copy of the hard drive.  For Windows users who've been denied
BitLocker by Microsoft's price discrimination, there's TrueCrypt.
