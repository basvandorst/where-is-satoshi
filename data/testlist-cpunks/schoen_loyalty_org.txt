
@_date: 2001-07-21 09:27:49
@_author: Seth David Schoen 
@_subject: [free-sklyarov] Re: Rallies on Monday 
References:  <20010721121135.B7990
I agree, and I fear that this is why StreamBox settled in the _Real
Networks v. Streambox_ case.  They didn't necessarily feel optimistic
that their commercial speech would be protected in the view of the
courts.  The outcome of that case is certainly troubling for this one.
For those who don't want to protest Adobe on Monday, why don't you go protest Real Networks?  Goodness knows they could use it. :-)
Streambox VCR is useful in some of the same ways as AEBPR, although
there are perhaps more legal uses for AEBPR and more illegal uses for
Streambox VCR.  They are both proprietary commercial software and many
people do argue that there is a free speech right to sell them (as
there is a free speech right to sell books!).  But it would be more
immediately obvious if they were free/open source.
Unfortunately, courts already seem to have a hard enough time
believing that electronic publication of free/open source software is
protected by the first amendment.

@_date: 2001-09-06 23:28:18
@_author: Seth David Schoen 
@_subject: Cypherpunks 9/8/01 - GOLDEN GATE PARK - EFF Music Share-In 
References: <5.0.2.1.1.20010906200542.03153ab0
  <5.0.2.1.1.20010906200542.03153ab0
  <5.0.2.1.1.20010906232215.0316ce70
I'm going to be working at the Share-in, and I'd additionally like to
invite everybody to celebrate a billion seconds of Unix with me
immediately following the event.
The 1,000,000,000th second after the Unix epoch is
Sat Sep  8 18:46:40 PDT 2001
which closely follows this weekend's concert.  I intend to have a
celebratory dinner in honor of Unix history as soon as the concert
clean-up is finished.

@_date: 2002-08-06 12:11:39
@_author: Seth David Schoen 
@_subject: Privacy-enhancing uses for TCPA 
References: I would just like to point out that the view that "the protection of
privacy [is] the same technical problem as the protection of
copyright" is Microsoft's and not mine.  I don't agree that these
problems are the same.
An old WinHEC presentation by Microsoft's Peter Biddle says that
computer security, copyright enforcement, and privacy are the same
problem.  I've argued with Peter about that claim before, and I'm
going to keep arguing about it.
For one thing, facts are not copyrightable -- copyright law in the
U.S. has an "idea/expression dichotomy", which, while it might be
ultimately incoherent, suggests that copyright is not violated when
factual information is reproduced or retransmitted without permission.
So, for example, giving a detailed summary of the plot of a novel or
a movie -- even revealing what happens in the ending! -- is not an
infringement of copyright.  It's also not something a DRM system can
But privacy is frequently violated when "mere" facts are redistributed.
It often doesn't matter that no bits, bytes, words, or sentences were
copied verbatim.  In some cases (sexual orientation, medical history,
criminal history, religious or political belief, substance abuse), the
actual informational content of a "privacy-sensitive" assertion is
extremely tiny, and would probably not be enough to be "copyrightable
subject matter".  Sentences like "X is gay", "Y has had an abortion",
"Z has AIDS", etc., are not even copyrightable, but their dissemination
in certain contexts will have tremendous privacy implications.
"Technical enforcement of policies for the use of a file within a
computer system" is a pretty poor proxy for privacy.
This is not to say that trusted computing systems don't have interesting
advantages (and disadvantages) for privacy.

@_date: 2003-04-02 18:16:18
@_author: Seth David Schoen 
@_subject: Logging of Web Usage 
References: <3E8963CC.9070203  <200303311844.h2VIiZdd021188  I'm skeptical that it will even take "a few hours"; on a 1.5 GHz
desktop machine, using "openssl speed", I see about a million hash
operations per second.  (It depends slightly on which hash you choose.)
This is without compiling OpenSSL with processor-specific optimizations.
That would imply a mean time to reverse the hash of about 2100 seconds,
which we could probably improve with processor-specific optimizations
or by buying a more recent machine.  What's more, we can exclude from our
search parts of the IP address space which haven't been allocated, and
optimize the search by beginning with IP networks which are more
likely to be the source of hits based on prior statistical evidence.  Even
without _any_ of these improvements, it's just about 35 minutes on average.
I used to advocate one-way hashing for logs, but a 35-minute search on
an ordinary desktop PC is not much obstacle.  It might still be
helpful if you used a keyed hash and then threw away the key after a
short time period (perhaps every 6 hours).  Then you can't identify or
link visitors across 6-hour periods.  If the key is very long,
reversing the hash could become very hard.
The logging problem will depend on what server operators are trying to
accomplish.  Some people just want to try to count unique visitors;
strangely enough, they might get more privacy-protective (and comparably
precise) results by issuing short-lived cookies.

@_date: 2003-12-11 23:11:47
@_author: Seth David Schoen 
@_subject: [linux-elitists] Monday 15 Dec: first all-Open Source 
... you might observe that they attained the first of the three
regulatory objectives described in the Content Protection Status
Report last month.
(If you have the ability to read PowerPoint documents, take a look
especially at the MPAA presentations at ARDG, like the introduction to
the analog reconversion problem and the MPAA reference model.)
Seth David Schoen  | Very frankly, I am opposed to people
        | being programmed by others.
          |     -- Fred Rogers (1928-2003),
        464 U.S. 417, 445 (1984)

@_date: 2003-12-11 23:32:31
@_author: Seth David Schoen 
@_subject: [linux-elitists] Monday 15 Dec: first all-Open Source 
I don't think the TCG TPM is the really fast processor you're looking
for; from all accounts I've heard, it's really rather slow.
I don't think "spy on the operating system" is quite the right term.
It might be better to say "prove that the relevant parts of the
operating system, if any, are completely unmodified".  There is no
intrinsic value judgment and there is also no attempt to identify
particular _ways_ of modifying the operating system as "bad".  At the
same time, you can also try to reduce the privilege of the operating
system to affect certain things, so that it may not matter to certain
secure operations whether the operating system is compromised or not.
There is an ingenious thing called a platform configuration register
(PCR) that contains hashes of running code.  The hardware is designed
in a such a way that, if the PCRs are used at all, at least the first
PCR will accurately reflect the code first loaded and to which control
was first transferred.
Because of the way the hardware is set up, if you load different code
(a different or modified nexus, for example), the PCR values are
guaranteed to be different.
To put this a different way, the hardware has (minimal, but
security-relevant) knowledge of what software is running.  If
different software is running -- for whatever reason -- the PCR values
will be different.
The encryption and decryption keys for the seal and unseal operation
are derived by hashing the PCRs, and the PCRs are derived by hashing
running code.  There is supposed to be no way to get arbitrary values
into PCRs (which is actually an oversimplification, but you can
pretend it's true) and so you can choose not to use them at all, but
you can't choose to load values into them that precisely correspond to
the values they would have been loaded with if you had booted an
operating environment other than the operating environment you
actually did boot.
Therefore, the _availability_ of a valid encryption or decryption key
inside the TPM -- to make an unseal operation work properly -- depends
on what software is really running.  You can have many different
operating environments installed on a single PC -- differing by a
little or a while -- and in principle they cannot unseal one another's
sealed data at all, because each one has its own family of PCR values
that results when it's booted.
There is no central CA necessarily implied by NGSCB.  Loading these
applets should require some kind of user decision and should not be
automatic based on presentation of a certificate.
The trusted computing applets are not supposed to have direct access
to any hardware (except that they can take input directly from the
keyboard when no other process is doing so, and they can write output
directly to the video framebuffer).  That is the NGSCB model.  So they
can't themselves directly "poke around your machine and your network";
they would need to have an ordinary (unencrypted) user-space agent
that does that.  In priciple, it should be possible to detect and
interfere with the operation of that agent by standard anti-virus or
IDS techniques.  If there is some reason that the user-space agent
can't be disrupted or detected -- which I don't imagine is the case --
then you have a severe problem.  But note that the trusted computing
applet can't authenticate the identity of the user-space agent code.
This part seems very possible.
Since the worm applet needs to use regular operating system services
to access the network, and since your computer is connected to a
network you can observe, you should be able to see the IP addresses of
the peers -- you can just run netstat, or you can run a sniffer, or a
firewall that monitors connection requests.  You can tell who the
peers are.  Of course, the worm can conceivably conceal where the
_ultimate_ destination is, if the worm network is a store-and-forward
kind of network.  In fact, the attacker who wrote the worm can join
the worm network in a way that looks like just another infected peer

@_date: 2003-10-12 23:44:16
@_author: Seth David Schoen 
@_subject: [linux-elitists] LOCAL Mountain View, California, 
Sorry that didn't happen.  And I still haven't fixed TCPA.
Intel has posted its Policy Statement on LaGrande Technology:
LaGrande is in the interstices between TCG and NGSCB.  TCG has not
specified a secure I/O path or "curtained memory" as required by
NGSCB.  LaGrande does, so it effectively provides the complete
hardware support NGSCB would need.  (AMD has a similar project called
SEM, which I know very little about other than that it is supposed to
do similar things and at least one of the people working on it is
exceptionally honest.)
Anyway, Intel wants your comments on the LT policy.  The thing that
jumps out at me (as the author of "Trusted Computing: Promise and
Risk") is that Intel thinks that opt-out or opt-in can solve the
problems of attestation.  This is the official view of a lot of
trusted computing proponents.  The defects of this view are difficult
to describe and are complicated by the fact that some trusted
computing critics don't believe that LT (or TCG or NGSCB) will
actually provide an opt-out.  (I do believe this.)
The root of the difficulty is that, in the nature of attestation, you
can be _punished_ for opting out (beyond the scope of simply not
enjoying particular features to which what you opted out of is
technically necessary).  For example, if you have a feature with
privacy implications like What's Related in browsers, you can opt of
using What's Related and the only penalty will be that you won't see
what's related to the sites you're looking at.  Or if you don't like
Microsoft's software updates, you can opt out of those and the only
penalty will be that your software won't be patched.  (This is
actually a somewhat thorny issue since no other sources of patches to
Microsoft software have so far arisen.)
But in most other cases with which we're familiar, opting out has a
relatively narrow effect, and there is fairly little leverage to
punish you for having done so.  At least, that's true of opt-out
features in the context of technology choices; it might not be true in
some off-line situations.
In the nature of attestation and its effect on interoperability,
though, opting out of attestation might be ruinous for your hopes of
communicating with others.  If they can be induced to use proprietary
protocols or file formats, opting out may lead to a permanent
inability to exchange data with them.  Opting in, by the same token,
could lead to a permanent loss of software choice (and the effective
inability to reverse engineer or repair your software) at least during
the particular periods of time when you want to communicate with other
people or manipulate what they sent you.
Opt-in can't undo the harmful network effects attestation will produce
for competition and for all computer owners.
Anyway, that's what I plan to tell Intel, in somewhat more detail,
sometime before December 31.
And remember:
   [T]rusted computing systems fundamentally alter trust relationships.
   Legitimate concerns about trusted computing are not limited to one
   area, such as consumer privacy or copyright issues.
Seth David Schoen  | Very frankly, I am opposed to people
        | being programmed by others.
          |     -- Fred Rogers (1928-2003),
        464 U.S. 417, 445 (1984)

@_date: 2004-01-02 12:47:25
@_author: Seth David Schoen 
@_subject: [camram-spam] Re: Microsoft publicly announces Penny Black  
References:  <3FF184C2.9070002
  <$Z2sQuJGYd8$EADj <3FF2A856.50900
  <3FF38101.3080107
It seems like one risk for hashcash is that, when mailing lists are
whitelisted, a spammer can then use the lists to amplify spam (which I
think is what Richard Clayton was suggesting above).  For instance,
you generated a single hashcash stamp for cryptography at metzdowd.com of
the same value as the stamp you generated for richard at highwayman.com.
That stamp would hypothetically induce metzdowd.com to send your
message to _all_ of the cryptography subscribers, all of whom have
hypothetically whitelisted the list.  That means that, if your message
were spam, you delivered it to the whole subscriber base at very low
Or does hashcash only help moderated mailing lists (where it "pays"
the moderator for her time)?  My current impression is that it will
benefit individual e-mail recipients but not help subscribers to large
unmoderated mailing lists.

@_date: 2005-05-05 04:08:54
@_author: Seth David Schoen 
@_subject: [IP] Google's Web Accelerator is a big privacy risk 
A bigger problem is that many Google search users are also Gmail
users, and a cookie is shared between Gmail and Google search (because
they use the same domain, google.com).  Therefore, if a person uses
Gmail and Google search from the same computer, even with a long period
of time in between, Google will know the identity of the person
responsible for those search queries.
Google doesn't need to infer your identity from the content of your
other web searches; it already knows it, if you're a Gmail user.
This identification can be retroactive.  If you used Google search
for 3 years on a particular PC, and then signed up for a Gmail
account, your search cookie from that PC would be sent to Google and
the name you provided for your Gmail account could then be associated
retroactively with your entire saved search history.
Google cookies last as long as possible -- until 2038.  If you've
ever done a Google search on a given computer with a given web
browser, you probably still have a descendant of the original PREF
cookie that Google gave you upon your very first search, with the
very same ID field (a globally unique 256-bit value).
This problem is ubiquitous in the web portal industry, and Google is
right to say that its privacy policy is better than many of its
competitors'.  However, Google is still assembling a treasure trove
of personal information, possibly stretching back for years, that
Google may release in response to any civil subpoena or "governmental
Seth David Schoen  | Very frankly, I am opposed
to people
        | being programmed by others.
          |     -- Fred Rogers
        464 U.S. 417, 445
You are subscribed as eugen at leitl.org
To manage your subscription, go to
 Archives at: Eugen* Leitl leitl
ICBM: 48.07078, 11.61144            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
         [demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2005-10-12 09:49:39
@_author: Seth David Schoen 
@_subject: [IP] more on Location tracking -- for people, products, 
places -- is fast coming into its own / It's 11 o'clock. Do you know
where your _______ is?
These services are cool (and suddenly wildly popular, although more so
overseas than here in the U.S.), but (much like Google Search) they are
presenting a huge target for subpoenas because they typically collect
and retain a tremendous amount of juicy personal information about their
Researchers have worked on location-based services that don't require
giving presence information to a central server; there seem to be two
operational obstacles and one business obstacle to this.  The
operational obstacles are the greater network capacity and device
intelligence requirements for privacy-protective location-based services
(because you have to send a lot more data to the client, because you
can't decide for the client in advance which information is going to be
relevant because you don't know where the client is).  For instance, an
ideally privacy-protective service would tell a client about friends who
are "checked-in" in every city in the world, because the service would
deliberately have avoided learning what city the client was located in
(and indeed deliberately not have interpreted the meaning of the
check-in information).  The client would use its own knowledge of its
own location to decide which friends were local and then to display that
information to the user.  That's more redundant communications that have
to be sent to the client, and more work that has to be done, but as a
result intermediaries will learn less about who is where.
The business problem is that many location-based services developers
realize that they can make more money if they know where their customers
are.  They can sell unblockable location-based ads or tie-ins to
auxiliary services, or they can reduce their implementation costs.  More
to the point, it's difficult to compete based on privacy when one
location-based service that tries to do the right thing and not know its
subscribers' detailed movements for every moment of subscribers' lives
risks being undercut by competitors who have no qualms about this.
Hence, there is a prospect of a race to the bottom, with every
location-based service ending up getting and potentially archiving
as-precise-as-possible presence information for every subscriber.
If people are committed to deploying services that rely on server-side
knowledge of subscriber locations -- because they want to optimize for
something other than privacy -- there are still two practical issues to
First, there's a trade-off between implementation efficiency and
precision of geographical knowledge.  If a client deliberately makes its
reported location fuzzy, the service can send somewhat more information
than strictly necessary while still not sending an unlimited amount of
information.  Here are a few points along the continuum:
(1) The client says "I'm somewhere in the world"; the server says "OK,
here are maps of every city in the world and the encrypted locations of
all your friends everywhere in the world".  The client then picks out
the map and the friends' locations that it concludes are relevant.
(If and when we have the communications capacity, this is the ideal for
subscriber privacy; the intermediary _does not have to know anyone's
location at all_.)
(2) The client says "I'm in New York City"; the server says, "OK, here
is a map of all of New York City, and the locations of all your friends
who told me that they were in New York City".  The client then picks out
the region of the map that's relevant and displays the locations of
friends who appear to be nearby.
(3) The client says "I'm on the Upper West Side in New York"; the server
says, "OK, here is a map of the Upper Wide Side, and the locations
of all your friends in that neighborhood"; the client again displays
the subset that it finds relevant.
(4) The client says "I'm on the east side of Broadway between 93rd
and 94th"; the server says "Your friend Josephine is on Broadway
between 94th and 95th; your friend Sam is on Amsterdam Avenue
between 92nd and 93rd; your friend Kate is headed west from Central
Park; your friend Jim just walked out of the building across the
street, take a look!".
If people developing these applications are willing to go a little more
coarse-grained than what they have the _ability_ to do, privacy will be
better protected.
Second, there's the question of how long information is retained.  If
it's retained as long as possible, it's a greater temptation for
subpoenas, and a virtual certainty that these subpoenas will eventually
become routine -- for law enforcement, divorce, child custody,
employment and worker's compensation litigation, and probably other
things we haven't thought of yet.  Not to mention the traditional risks
that it will be stolen, or that some successor-in-interest, in dire
financial straits, will decide to sell it off to the highest bidder.
It takes an effort to overcome the temptation to keep things forever,
but a data-retention policy would do a lot to protect privacy here.
Seth David Schoen  | This is a new focus for the
        | community. The actual user
of the PC
          | [...] is the enemy.
          -- David Aucsmith,
IDF 1999
You are subscribed as eugen at leitl.org
To manage your subscription, go to
 Archives at: Eugen* Leitl leitl
ICBM: 48.07100, 11.36820            8B29F6BE: 099D 78BA 2FD3 B014 B08A  7779 75B0 2443 8B29 F6BE
[demime 1.01d removed an attachment of type application/pgp-signature which had a name of signature.asc]

@_date: 2012-03-25 18:22:43
@_author: Seth David Schoen 
@_subject: [cryptography] 
Automatically trying to make software incur faults with large amounts of
randomized (potentially invalid) input.
If you get an observable fault you can repeat the process under a
debugger and try to understand why it occurred and whether it is an
exploitable bug.  Here's a pretty detailed overview:
When it was first invented, fuzzing basically just consisted of feeding
random bytes to software, but now it can include sophisticated
understanding of the kinds of data that a program expects to see, with
some model of the internal state of the program.  I believe there are
also fuzzers that examine code coverage, so they can give feedback to the
tester about whether there are parts of the program that the fuzzer isn't
