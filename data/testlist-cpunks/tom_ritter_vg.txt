
@_date: 2011-12-05 20:34:03
@_author: Tom Ritter 
@_subject: [cryptography] really sub-CAs for MitM deep packet 
References: <20111204104757.GD31847
  <1323115836.24525.2.camel
Interesting for me is probably around 65%.  You can judge for
yourself:  I like it, but to be honest, it does get spouts of super-high volume
(like the last week while I was traveling).  I consume it via RSS from
gmane which keeps my inbox cleaner.

@_date: 2011-11-08 19:51:53
@_author: Tom Ritter 
@_subject: [p2p-hackers] Verifying Claims of Full-Disk Encryption in Hard 
Hash: SHA1
After reviewing the FIPs approval document for the drive[1], I've tried to put together a complete threat model outlining the major classes of attack on the hard drive in the interest of being rigorous.  I'd like your input to see if I missed any you can think of.  I've explicitly excluded DriveTrust (the proprietary stuff) from the threat model, and am only focusing on the ATA Standard.
[1] In approximate physical/logical order, this is every attack I can conceive of:
1. The BIOS may have been replaced to record passwords
2. The keyboard or keyboard connection may be tapped/keylogged
3. The physical computer may have been tampered with physically installing hardware in any of its components
4. The Operating System may have been tampered with
5. The application used to interact with the hard drive (hdparm) may have been subverted
6. The SATA connection to the HDD may have been tapped
7. On the Drive
8. AT Password Security Protocol
This groups those attacks together, and notes whether I consider them within the realm of testing for the drive.  I'm not sure what will be doable easily or cheaply, but if I can verify the firmware, I'll try.
Not Considered for evaluation
User Coercion or Cooperation / "Evil Maid" Attacks
Side Channel Attacks
Considered for Evaluation
1. Buggy firmware
2. Key Management
3. Encryption
4. System Area
Again, all comments welcome, but particularly interesting in talking to
  - Anyone familiar with these Seagate drives or DriveTrust.
  - Anyone familiar with BIOS support for the AT Security Spec, who can help me locate a new netbook to work with.
  - Anyone familiar with Data Recovery Services who could provide information on disk unlocking, AT password bypass, or moving platters between disks.
  - Anyone who has done this before.
- -tom
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2011-11-15 08:19:18
@_author: Tom Ritter 
@_subject: [p2p-hackers] Verifying Claims of Full-Disk Encryption in 
References: <20111109121027.GS31847
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Misdirected a reply to Eugen instead of the list a week ago.  I don't think this
will correctly reply, because I wasn't subscribed to this list at the time.
I used whatever documents I could find to get as much information
about the drive as possible.  That was the marketing material (which
obviously didn't help much), and the FIPS-140 document (which did have
some technical information).  If I could use the Common Criteria or
Protection Profile document, I'd love to - but I'm not sure how to get
those or go about requesting them (besides just calling and asking.)
I may be naive, having never dealt with FIPS validation, but I kind of
hoped/assumed that things that were insecure wouldn't be approved.
I'm using insecure casually, basically meaning "If I steal your
laptop, can I recover your data for under a couple thousand dollars?'
If that is possible, and within the reach of a hobbyist (or organized
crime, minor government, etc) - I would expect it not to be approved.
And if it was approved, I'd expect the approval to be in error.
Maybe I'm wrong about the approval process - I've never been involved
with it.  I'm just approaching it from the perspective of 'Should I
trust this?' and using the FIPS-140 approval to gain a little intel
and make a good starting point for a hard drive to start with.
- -tom

@_date: 2012-08-07 09:30:52
@_author: Tom Ritter 
@_subject: [liberationtech] What I've learned from Cryptocat 
I agree with a lot of the points being raised, including all of
Moxie's (especially about Google v Riseup) but also Eleanor's
regarding niche products and irrelevancy.
In particular I want to expand on this bit:
I agree with that position wholeheartedly.
Still, possible does not equate to easy.  Cryptocat is the Jackie
Robinson of Web Crypto Services[0].  And not to fault Nadim, as this
is a volunteer effort, but there is more it can and should do to make
it harder for  - just as Google and Facebook have done.
 - Cryptocat should use DNSSEC - even if validating resolvers are not
deployed, it's another piece.  Maybe down the road when a binary
plugin is developed, it can validate the DNSSEC chain.[1]
 - It should Pin certificates in Chrome.  As soon as the header is
supported in any other browser it should use it.  Same with TACK.
 - It should assert the SSL certificate with both DANE and
DNSSEC-Stapled Certificates
 - It should (it does for the record, just saying) use Strict Transport Security
 - It should (it now does as of Sunday) deploy Content Security Policy
 - It should do all the other security techniques recommended:
x-frame-options, X-Content-Type-Options, etc
 - Where it is possible, plugins should assert the validity of the Code and Keys
 - Controversial: It should use per-request mutated javascript
obfuscation to make it more difficult to inline-middle the application
in realtime[2]
 - It could experiment with browser enhancements to provide signed
javascript files and so on
All of those are not too difficult for an individual to try, and it
makes SSL Interception harder.  It's not any less possible, but it
raises the bar.  If you think these techniques aren't effective - I
challenge you to do a live MITM of the gmail interface and have it
still function seemlessly.  It just flat-out won't work in Chrome of
course, but even in Firefox - it is not trivial.[4]  That's what I
mean by being Jackie Robinson - you just have to be 'better'.  Above
and beyond.
Trying to fix  is way, way harder because it's just impractical to
tell someone "Oh, compile everything from source, run a perfectly
secure Linux box with PaX and grsecurity, etc etc"  Ideally, that's
what we'd have.  I suppose the important part is to acknowledge the
risk of server compromise, and keep the bars approximately equal.  If
all the above measures were employed, but the server left the way it
was - then rooting the box is absolutely easier.  Google and Facebook
have a huge advantage here.
[0] If you don't get the reference, Jackie Robinson broke the
segegration barrier in US baseball - he was spit, cursed, threatened
with murder, had pitches thrown at his head - and through it all, he
just played the game steadfastly. There's an urban legend (I'm unsure
on it's truth) that his contract stated he couldn't complain, even
when fans spit on him.
[1] If your arguement is 'I don't trust DNSSEC' my response is 'Me
neither, but I believe in beaurcracy and turf wars and that it'd be
more difficult for a government to subvert two PKIs than one.'
[2] If your arguement against it is "but then I can't audit it" my
argument is "you don't audit it now." If your argument is still "I
can't trust the code delivered" my response is "well, since you
obviously are doing local SSL interception to read the server
responses, hash the mutated javascript serverside, and send a PGP
signature along with it so you can do the same and trust it that way".
[3] Heck, maybe that's a way to upgrade web services to thick client
services - have an optional thick client someone can run that sits as
a proxy running verification.  Same web interface works with or
without the tool, but the tool provides run-time verification.
[4] Javascript obfuscation makes it difficult to rewrite the code; if
you add code - CSP prevents you from exfiltrating easily; and if you
exfiltrate to another user in the chat the chatters *should see* this
other user in the chat they don't trust.
Yes, yes, yes.  There is a *tremendous* amount of implicit and
unmentioned TRUST in the person operating the service or relying on
the software.  That's why anyone would use RedPhone, TextSecure or
WhisperCore back when it was closed source.  Because people *trusted*
If the EFF hosted an e-mail solution I'd be throwing money at them to
let me sign up.  Because Google is huge and diverse - they are
obligated to respond to legal threats in most of the countries of the
world.  Because Google is huge and opaque - I have little faith in
their ability to notify me/etc in the event of a LE request.  But the
EFF has both juristiction on their side (arguably they'd have more if
they were based in, say, Scandenavia) and they have trust.  I trust
that the EFF will fight harder for me than Google.  And there's
intimidation factor - a LE agency ought to know if they come the the
EFF with a request they need to back it up with a supoena or warrant.
I absolutely think of riseup as a trust project.  I would like to see
many more of them.  Nick Merril's Calyx Institute I think of as a
Trust Project.  He's trying very hard to remove Trust, and make it a
cryptography project - but he literally has to build the
infrastructure for that because it doesn't exist.  (Which is one of
the reasons you can't actually buy any services from them yet.)   It
will be *very* interesting to see how Phil Z's and Jon Callas' Silent
Circle positions itself.  A trust project?  They're aiming to remove
trust also; but to what extent can they?
Trying to improve upon the trust factor is extraordinarily difficult.
I think, in the short term, it relies on linking up with a person or
organization people already trust - and therefore somehow convincing
them to trust you. And in the long term - devoting your life to being
a trustworthy individual.  Not something we can solve with
cryptography - even a thick client.
Anyway, I realize I haven't addressed the issue of 'Should cryptocat
move to this model or that, shut itself down, add warnings, push
forward with users, etc'.  But I wanted to raise the point that it can
do more, today, to make users safers - and if _any_ webapp in this
sphere wants to push the envelope, it should probably do those things
first.*  And since people are already using it, these are options to
improve security besides just shutting it down.
* None of this is meant to be a slight at Nadim, who has certainly
earned my respect for both his effort and his results so far.
liberationtech mailing list
liberationtech at lists.stanford.edu
Should you need to change your subscription options, please go to:
If you would like to receive a daily digest, click "yes" (once you click above) next to "would you like to receive list mail batched in a daily digest?"
You will need the user name and password you receive from the list moderator in monthly reminders. You may ask for a reminder here: Should you need immediate assistance, please contact the list moderator.
Please don't forget to follow us on

@_date: 2012-05-14 23:16:28
@_author: Tom Ritter 
@_subject: [p2p-hackers] Pirate Pay 
Hm.  I'm not sure how to bootstrap when you only want to interact with
people you trust - but it seems like this could be a model for a Web
of Trust that grows over time if you have a way to validate what
people are supposed to be sending.
I am Alice and I interact with 1000 other people, Bobs1-1000, who send
me pieces of a file. They're valid, so I sign their public keys saying
they were good for some number of bytes, send them the signature, but
also keep it locally. I send data to 50 other people, Carols1-50 who
in turn give me signatures.  Over time, I interact with Bob50 again,
and trust him based on my previous signature. I find a Dave, who I
trust a little because he has a signature from Bob600, and Bob600 was
good to me.
The key pieces (and why this couldn't work for distributed DNS) are
that I know that Bob[x] sent me is valid because I have the
information to build a merkle tree and validate his piece.  The more I
download, the more people I 'trust' because they were honest actors
last time. The more I upload, the more people 'trust' me because I
acted honestly in the past.
But I may have reinvented something simple or missed an obvious flaw,
because I'm not well-versed in how DHT works, or anything published in
this area.
p2p-hackers mailing list
p2p-hackers at lists.zooko.com

@_date: 2013-08-12 16:23:51
@_author: Tom Ritter 
@_subject: Information theoretically secure communication networks 
References: <1376317284.3276.24.camel
As Lance said, this is pretty close to what alt.anonymous.messages
evolved into in the 90s and early 00's.
I gave a talk two weeks ago looking at 10 years of messages there and
finding user errors, weak passwords, user-segmenting settings, and
traffic patterns.  Details are over here:

@_date: 2013-08-14 19:12:24
@_author: Tom Ritter 
@_subject: [guardian-dev] An email service that requires GPG/PGP? 
That statement is not correct.  Mix networks require more effort to
trace than normal packets or Onion Routing, but are not even close to
"theoretically completely untraceable".  I'll point to Syverson's
papers (Why I'm not an entropist, and Sleeping dogs lie in a bed of
onions) and Serjantov's "From a Trickle to a Flood."
I had not seen that paper, that's cool thanks.  However, it seems
they're observing data (EFF Observatory and Market Prices) and drawing
conclusions about why companies make decisions.  It would be easier
and more reliable to just... ask the companies why they do what they
do.  They seem to omit that somewhat important step to support their
Guardian-dev mailing list
Post: Guardian-dev at lists.mayfirst.org
List info: To Unsubscribe
        Send email to:  Guardian-dev-unsubscribe at lists.mayfirst.org
        Or visit: You are subscribed as: eugen at leitl.org

@_date: 2013-08-27 20:57:35
@_author: Tom Ritter 
@_subject: Metadata anonymization through time delayed email messaging. 
References:  <20130827122252.GA2761
I don't know - if I'm performing physical or network surveillance of a
target, and I see a Mix message leave - that tells me something very
definite about the timing.  Obviously you wouldn't want to store the
message in plaintext, but if you encrypted it to the first hop, along
with the address, and a time to send (and tried your hardest to lie
about the timestamps on the filesystem); you can increase the
difficulty of learning something definite.  And I think that holds
even if the attacker does a physical intrusion and looks at the
filesystem. (It reminds me Rivest's FlipIp game - the attacker is
allowed to do a physical intrusion and read the filesystem, but
everyone learns that they have and thus distrusts that node.)  Of
course it only holds if there are multiple possible senders, delaying
an email from my home when I live alone doesn't help me.  But if there
are multiple possible senders, it feels like tacking on a
lesser-quality mix node at the beginning.
Another argument to it's utility is there is no easy way to disguise
the fact that you are sending a mix message.  Right now the only ways
I can think of hiding that fact would be to use mix bridges (some
entry remailer node that isn't published, akin to Tor's bridges) with
a protocol that looks as identical to SSL in a webbrowser as you can;
or to send them out over Tor.
I think the user-configurable time is the idea behind Alpha Mixing,
although I hope it's implemented better than in Type 1 Remailers.
Absolutely.  And on the sender end, I can't think of good ways to
obfuscate large messages.  The splitting technique of Mixmaster has
always felt like a bit of a hack (no offense), because someone doing
end to end correlation should be able to link those fairly easily.
For receiving large files, I think a client/server architecture where
you can choose to delete the message on the server, or download chunk
by laborious chunk over time would be advantageous[0].
[0] This might, might, even be an argument of added complexity by
splitting files, before compressing and encrypting them, so you can
download chunks 1-4 and (potentially) get a portion of the file in a
readable albeit incomplete format.

@_date: 2013-12-13 19:31:21
@_author: Tom Ritter 
@_subject: Joke 
References: I doubt it - abuse through Tor is a legitimate problem.  Wikipedia blocks
editing from Tor for the same reason.
There are ideas for solving this though, and it would be cool to see more
ideas, and more fleshing out of them.  Mike Hearn has talked about having
people make a bitcoin deposit for an account, and after so much time of
legitimate use, the deposit is refunded.  Before that, if it's used for
abuse, the deposit is kept by the service.

@_date: 2013-12-14 02:55:13
@_author: Tom Ritter 
@_subject: BlueHat v13 crypto talks - request for leaks ;) 
References: I can answer for Cryptopocalype. :)  I had a follow-up blog post after
Black Hat, but the crux is looking for the next crypto black swan. Joux's
work in optimizing the function field sieve for fields of a small
characteristic has been a significance improvement kind of out of left
field. If he or anyone else made improvements to the FFS for fields of a
large  characteristic or the GNFS - we would be in a bad way. The security
margin on the ECDLP is greater than DL or factoring and while we've got the
algorithms, the implementations are sometimes missing and the ability to
pivot, in software update mechanisms, in CAs, everywhere - is completely
missing. ECC has other attributes that make it attractive too, so let's get
the plumbing ready, so we can support a quick pivot away from RSA and over
to ECC if we have to.
I copied Justin rather than (poorly) summarize his work.
(Just landed, sent from the baggage claim, excuse brevity)

@_date: 2013-12-14 15:23:50
@_author: Tom Ritter 
@_subject: BlueHat v13 crypto talks - request for leaks ;) 
References: This is different from the normal 'repeated/non-random k leads to private
key', is it not?  Is there a paper/reference I can read more about this

@_date: 2013-12-15 14:23:09
@_author: Tom Ritter 
@_subject: Gmail's receiving mostly authenticated email 
References: <20131214213326.56CD5F435
I saw that article too, and thought it was interesting, but I noticed
something odd in their statistics:
91.4% of ***NON-SPAM*** emails sent to Gmail users come from
authenticated senders, which helps Gmail filter billions of
impersonating email messages a year from entering our users’ inboxes.
More specifically, the 91.4% of the authenticated ***NON-SPAM***
emails sent to Gmail users come from senders that have adopted one or
more of the following email authentication standards: DKIM (DomainKey
Identified Email) or SPF (Sender Policy Framework).
""" (emphasis mine)
So first Google runs their pretty-good-but-not-perfect spam filtering,
then they look at what they're categorized as non-spam to generate
those statistics.  The ham (not spam) emails that are miscategorized
are much more likely to be omitting SPF/DKIM, so there's a bit of
selection bias occurring.
Also, for what it's worth, SPF isn't related to crypto at all, and is
ridiculously easy to set up for 'normal' domain admins.  (That is,
domain admins with a couple well-known SMTP servers, and not some
crazy distributed architecture.)  There's a great calculator online
for it here: There's some tricky questions people may not know the answer to, but
omitting answers will only create a more _permissive_ policy, rather
than run the risk of borking your email.

@_date: 2013-12-22 13:14:36
@_author: Tom Ritter 
@_subject: RSA complicity or not in the EC_DBRG backdoor (Re: Human scum: 
References: <52B56924.7030605 <52B56B05.50002
 <20131221111358.GB19555
I'm confused, but maybe missing something?  The article says:
The stakes rose when more technology companies adopted RSA's methods
and Internet use began to soar. The Clinton administration embraced
the Clipper Chip, envisioned as a mandatory component in phones and
computers to enable officials to overcome encryption with a warrant.
RSA led a fierce public campaign against the effort, distributing
posters with a foundering sailing ship and the words "Sink Clipper!"
A key argument against the chip was that overseas buyers would shun
U.S. technology products if they were ready-made for spying. Some
companies say that is just what has happened in the wake of the
Snowden disclosures.
The White House abandoned the Clipper Chip and instead relied on
export controls to prevent the best cryptography from crossing U.S.
borders. RSA once again rallied the industry, and it set up an
Australian division that could ship what it wanted.
"We became the tip of the spear, so to speak, in this fight against
government efforts," Bidzos recalled in an oral history.
RSA, meanwhile, was changing. Bidzos stepped down as CEO in 1999 to
concentrate on VeriSign, a security certificate company that had been
spun out of RSA. The elite lab Bidzos had founded in Silicon Valley
moved east to Massachusetts, and many top engineers left the company,
several former employees said.
It seems like Bidzous was out of RSA long before DUAL EC PRNG was even
proposed, and was in fact campaigning and strategizing against RSA
while he was there.  Where are references to other accusations or

@_date: 2013-02-08 16:35:08
@_author: Tom Ritter 
@_subject: [liberationtech] Bellovin, Blaze, Clark, Landau 
When law enforcement relies on vulnerabilities in the system (be it
protocols, operating systems, applications, or web sites), they are
incentivized to keep it insecure.  If it were secure, how would they
get in?
Would the FBI patch their own systems against the bugs they know
about?  How would they control that information across all their
systems?  (This is an old hackers' puzzle: if you had an OpenSSH 0day,
would you patch yourself against it?)
If I were a communications provider (e.g. Silent Circle), and I found
that the FBI was hacking me to learn customer data... what is my
recourse?  To borrow from the CFAA, the FBI is certainly performing
unauthorized access or exceeding authorized access to a computer
system.  Am I allowed to kick them out? Sue them? What if they
accidently crash a system because they're crappy exploit writers?
Just like when Matt Blaze wrote it in Wired, this feels like a
mistimed April Fools joke.
Unsubscribe, change to digest, or change password at:

@_date: 2013-03-08 13:05:30
@_author: Tom Ritter 
@_subject: Summary of where we are right now 
References: I don't agree that the NRL funded Tor for this purpose, but I do agree
that our tools today (Tor, mixmaster/mixminion, PGP mail, RedPhone,
TextSecure, OTR, etc) are easily distinguishable in traffic streams,
and that this is a problem.  Just as Riseup collects a bunch of people
who care a lot about privacy onto one mailserver - people using these
tools are likely to be interesting.
Skype, Facebook, Gmail - for all their problems, they are ubiquitous,
and don't draw attention.
A friend I talked with recently told me he thought it was easy to set
up an anonymity system that worked great for you and your friends, and
near impossible to build one that worked well for everyone else.  Once
it got popular or you became a target of investigation, people would
put the effort into detecting it.  Otherwise, it would continue along,
looking like another TLS/SSH/Skype/whatever that just a little bit
odd...  Tor faces this problem immensely.
I don't see us as having won, I see us as now knowing how to fight.
We know the devices they will use to easily detect our traffic, and in
most cases we can get access to them.  We must make our protocols
indistinguishable on the wire. We know the ubiquitous services and
protocols that we must work within or disguise ourselves as.
We know (some of? most of?) the statistical attacks adversaries of the
future can conduct - we must make them as difficult and expensive as
possible for them to achieve.
We know how woefully inadequate the user interfaces and requirements
of the first generation of tools were, and we know where we must go:
to browsers, smartphones, tablets, and consumer operating systems.
We have a much better idea of how normal people will react to our
tools, and thus how much effort we must make to make them usable, and
push for ubiquity.
We know what requirements are unreasonable of us to make upon people,
and that we must design systems where those requirements are worked
around, dulled, or the single 'sharp edge' of the system.

@_date: 2013-03-25 11:57:16
@_author: Tom Ritter 
@_subject: [liberationtech] A tool for encrypted laptops 
Hi all - at the risk of shilling, my company has released an Open
Source tool called "You'll Never Take Me Alive".  If your encrypted
laptop has its screen locked, and is plugged into power or ethernet,
the tool will hibernate your laptop if either of those plugs are
removed.  So if you run out for lunch, or leave it unattended (but
plugged in) at starbucks, and someone grabs your laptop and runs,
it'll hibernate to try to thwart memory attacks to retrieve the disk
encryption key. Not foolproof, but something simple and easy.
It the moment it only supports Bitlocker, but support for Truecrypt is
coming[0].  If you have suggestions - add them to the github issues
[0] Too many emails? Unsubscribe, change to digest, or change password by emailing moderator at companys at stanford.edu or changing your settings at

@_date: 2013-10-14 17:30:09
@_author: Tom Ritter 
@_subject: An Interview with Simon Persson of CounterMail 
References: <09cebb934a7f5049e5888a339eda559d.cm1
"You can delete the private key from our server (but we recommend this
only for advanced users, your private key is always encrypted on our
server anyway"
This sounds pretty similar to Lavabit. The server stores your emails
encrypted, but they're decrypted for you when you login, using your
password as the key to decrypt your private key.  The difference (I
think, I never used Lavabit) is that you can retrieve the private key
from Countermail and then ask them to delete it.  It would be even
nicer if they let you upload your public key so they never see the
private key.  You'd still have to trust them not to copy plaintext as
it's coming in, which depending on how you think about it might be
equivalent to them having a private key to your mail in the first
In all these 'secure email' providers, they all have the same problem:
they see incoming plaintext, and could be compelled to store it/record
it. It's not their fault, they do the best they can, it's just how
email works.

@_date: 2013-09-19 15:39:53
@_author: Tom Ritter 
@_subject: Linus Torvalds admits he was asked to insert a backdoor into 
References: Is there any indication he took the question seriously and wasn't just
making a joke?  This is a lot to conclude from a single sentence.

@_date: 2013-09-30 22:08:35
@_author: Tom Ritter 
@_subject: Surveillance 
References: I would say you are incorrect. The UK and the US cooperate very, very
closely. Likewise, the Echelon/Five Eyes program is a publicly
documented SIGINT sharing program

@_date: 2014-08-20 11:10:35
@_author: Tom Ritter 
@_subject: New end to end encrypted IM/VOIP web app focused on ease of use 
References: <147ed6bc188.f5d4b41f116455.5490629446127454680
This is cool!  I love the combined distribution of providing a hosted
version, and encouraging people to host it themselves.
I looked into the code to understand more about how it works.  Is it
fair to say that you use WebRTC with SRTP for the transport
encryption, and then a homebaked AES-GCM-based protocol with RSA
public keys to do the encrypted chat/actions/invites, and also to
distribute/authenticate the WebRTC fingerprints?

@_date: 2014-02-07 23:02:18
@_author: Tom Ritter 
@_subject: FB's Conceal secure-storage API 
References: <52F24DD0.5050408
It's not like preventing root from getting the key is some attribute
they omitted by accident or incompetence - it's a significant design
change that changes the way the application would work.
It seems like everyone criticizing Facebook is angry that they're not
compromising their design principals for added security.  They have
very clear priorities: We are _going_ to benchmark and make sure any
code we add does not increase UI latency beyond an unacceptable limit.
 We are _going_ to cache some large MB of data on the phone, because
it makes the app faster. We are _not_ going to take up more space than
we need. We are _going_ to support old phones that have an SD Card,
and if that's where we cache the data, then so be it. We are _not_
going to require the user to enter a password or PIN on app startup.
We are _not_ going to require the phone to be online to used the
cached data.
With requirements like those, what you get is exactly this library. It
adds some small level of security against a very specific attack: data
stored on the SD Card and accessible to other programs. (It may even
be a way to get the security they need to permit themselves to store
cached data on the SD Card, which is a desirable situation because it
makes the app faster.)
If you relax some of those requirements, you can add security
features. Relax the latency or minimal storage requirement and you can
create an encrypted container, and hide metadata like filenames,
sizes, and times (like IOCipher). Relax the password requirement, and
you can have the user enter a password on app startup and prevent root
from getting the key unless it's in memory or entered.  Relax the
latency and offline requirement, and you can have the server send down
a key to decrypt the data.
Facebook is starting with the User Experience and adding as much
security as it allows.

@_date: 2014-01-23 00:47:48
@_author: Tom Ritter 
@_subject: and not a single Tor hacker was surprised... 
References:  <52DFDFCB.9090003
 <4869289.mJZM0fiGMF
> About
this. Is there a way to serve 2 (or more) certificates for a given HTTPS
that do
There are a lot of things like this, but the big question is: how does the
user indicate to you which cert they want?
If it was via pubca.x.com or privca.x.com - that's easy just put the
different certs in the different sites.
But otherwise, you have to rely on quirks.
TLS allows you to send different certs to different users, but this is
based off the handshake and is for algorithm agility - not cert chaining.
EG I send ECDSA signed certs if I know you can handle them, and RSA if not.
You can also send two leaf certs, two cert chains, a cert and garbage, a
cert and a stego message - whatever. This is the closest to what you want,
but this is undefined behavior. Browsers may build a valid chain off the
public CA, and monkeysphere off the private* and it works perfect... Or the
browser may pop an invalid cert warning. It's undefined behavior. You'll
have to test, see what happens, and hope chrome doesn't break when it
updates every week.
* I realize monkey sphere doesn't use a private CA, just using it as an

@_date: 2014-07-07 08:52:59
@_author: Tom Ritter 
@_subject: US enhanced airport security checks target electronics 
References:  <20140707181058.731411b7b0336d3f0e68399c
The text in the email is satire/commentary and not actual reporting.

@_date: 2014-06-04 08:50:14
@_author: Tom Ritter 
@_subject: "a skilled backdoor-writer can defeat skilled auditors"? 
References: <1800350.DuBgtkdSDz <20140603225302.GJ10586
 <538EB484.7040405
On 4 June 2014 01:54, Stephan Neuhaus Perhaps this is getting too far into nits and wording, but I audit software
for my day job (iSEC Partners).  I'm not speaking for my employer. But,
with very few exceptions (we have a compliance arm for example), one does
not 'Pass' or 'Fail' one of our audits.  (Perhaps they might be better
termed as 'security assessments' then, like we call them internally, but
we're speaking in common english, and people tend to use them synonymously.)
Our customers are (mostly) on board with that too.  They never ask us if
they 'passed' or failed' - I'm certain some of them look at a report where
we failed to 'steal the crown jewels' as a successful audit - but the
expectation we set with them, and they sign on with, is not one of
'Pass/Fail'. And engagements where they want a statement saying they're
secure, we turn down - we're not in the business of rubber stamps*.
Our goal is to review software, identify bugs, and provide recommendations
to fix that issue and prevent it from occurring again. AND, in addition to
the specific bugs, provide general recommendations for the team to make
their application and environment more secure - provide defense in depth.
Maybe I didn't find a bug that let me do X, but if there's a layer of
defense you can put in that would stop someone who did, and you're missing
that layer, I would recommend it.
Examples: I audited an application that had no Mass Assignment bugs - but
no defenses against it either. Blacklists preventing XSS instead of
whitelist approaches, and like Andy said, homebrew C-code parsing JSON. We
'flag'-ed all of that, and told them they should rewrite, rearchitect, or
add layered defenses - even if we couldn't find bugs or bypasses.
So the notion of 'Passing' or 'Failing' an audit is pretty foreign to me.
 Perhaps people mean a different type of work (compliance?) than the one I
* The closest we get is one where we say 'We tested X as of [Date] for Y
amount of time for the following classes of vulnerabilities, reported them,
retested them Z months later, and confirmed they were fixed.'  As we do
this very rarely, very selectively, for clients we've dealt with before.

@_date: 2014-06-29 19:11:31
@_author: Tom Ritter 
@_subject: [liberationtech] Nsa-observer: organising nsa leaks by attack 
References: <1403615736.13581.132987521.471F759E
It is up for me.  The site itself is open source
( and the data ex
exportable (

@_date: 2014-11-22 20:24:19
@_author: Tom Ritter 
@_subject: Microsoft Root Certificate Bundle, where? 
References: I don't know.
But I know some copy of it can be accessed here:
I don't know how it's generated, how complete it is, or how up to date
it is.  Depending on your needs to may be sufficient, or may be

@_date: 2015-02-03 19:52:52
@_author: Tom Ritter 
@_subject: What the fark is "TFC" 
References: <15658830.G1IiJoRcAd
TCB is usually Trusted Computing Base.
Some searching indicates TFC may be Traffic Flow Confidentiality.  (Or
less likely, TinFoil Chat, which appears to be some random chat app
plugin for encrypted messaging.)

@_date: 2015-07-05 17:44:21
@_author: Tom Ritter 
@_subject: progression of technologies 
References: <20150625032613.48665228148
I'm far from certain, but I think what you have wrong is the notion
that wavelength doesn't matter. I think the courts have decided it
does: Specifically, "most of the general public lacks the expertise to
intercept and decode payload data transmitted over a Wi-Fi network."
Therefore the notion that you can point whatever sort of 'camera' you
want at people to capture them isn't accurate.  (The other relevant
case is that the police do need a warrant to point infrared cameras at
people's houses.)

@_date: 2015-07-11 14:19:59
@_author: Tom Ritter 
@_subject: progression of technologies 
References:  <20150710155812.D78CF2281ED
Yes! That's the case I was obliquely referring to. Sorry, I kind of
glazed over that part of your argument in the article.
I guess where we quibble is I'm skeptical that the general public (as
defined by the courts?) will (ever?) adopt the types of tools you
refer to (uniquely identifying individuals based on electromagnetics,
tracking tire pressure sensors.)  I don't think the 'general public'
has adopted thermal imagers.  These will make their way into
industry... (advertisers tracking WiFi probes in malls obviously).
So my wonder now is if industry adopting a technology is sufficient
for the courts to qualify as 'general public'. But this, at best, only
affects exotic technology.  We're already fighting this battle.
Automated license plate readers have never (?) been challenged
(successfully?). They are an extension of "a police officer just
watching a highway" which is legal.  And the courts like extensions of
things that are already done - see bulk collection of metadata!
You're right - collection of this data by personals or corporations,
and selling it, is indeed the right battleground. I'm don't think the
answer is correlation, but the collection, as you say in the last

@_date: 2015-03-24 06:51:44
@_author: Tom Ritter 
@_subject: Firefox 36+ listens on UDP:1900 
References: <20150323131047.GA2520
 <1797970.9VJCTFyvlb
 <20150323141705.GB2520 <55102554.9010509
This is a close-to-but-not-exact recounting. His disclosure of his
employer was required by state law, and was neither a statement of
support by the company nor his attempt to make it so.

@_date: 2015-05-28 07:28:13
@_author: Tom Ritter 
@_subject: Firefox will scan your browsing history to suggest advertiser 
References: <20150526181340.GW8510 <5103510.VcCEB5gWH9
 <2433360.KkkMhQ9YbD
4) The browser fetches all available suggested tiles based on country
and language from Onyx without using cookies or other user tracking
5) User interactions, such as clicks, pins and blocks, are examples of
data that may be measured and processed. View Mozilla’s Privacy Policy
or our Data Privacy Principles for more information.
6) Onyx submits the interaction data to Disco, a restricted access
database for largescale analysis.
7) Disco aggregates all Firefox tiles interactions, anonymizing
personally identifiable data before sending to Redshift for reporting.
8) Charts and reports are pulled from Redshift using Zenko, a Content
Services reporting tool, for analysis by Mozilla.
9) Mozilla sends this report to the partner shortly after the campaign ends.
""" [0]
How do you determine user interests?
For Suggested Tiles, we know whether users are interested in your
market category by matching a list of defined URLs (domains, or
subdomains) with their most frequently and recently visited URLs in
Firefox. In this way, we are able to preserve users’ anonymity while
providing a high level of confidence about their interest in different
site categories.
What input do I have over the interest categories?
We work with all our Suggested Tiles partners to define the most
effective interest categories. Partners may provide suggestions for
what URLs should be include. Mozilla’s Content Services Team will
actually define those categories.
""" [1]
I'm most curious about what 'User Interactions' are reported.  Clicks,
pins, and blocks all reveal which tile a user saw, and therefore
something about their browsing history. But they're also pretty
fundamental to advertising.  I'm more worried about Firefox reporting
"Views" or "Mouseovers" or other things that are not clear,
user-initiated actions.
[0] [1]
