
@_date: 2007-12-01 13:29:23
@_author: Henrik Ingo 
@_subject: [p2p-research] threat of wiki bio deletion 
It's good to be on your toes, but I couldn't find anything newer than
2006. Is there a real threat or not?

@_date: 2007-12-01 16:24:30
@_author: Henrik Ingo 
@_subject: [p2p-research] threat of wiki bio deletion 
Now that we have people here that follow Wikipedia more actively than
me, it seems like a good opportunity to ask the question I've always
wondered about:
Is the issue of creating "editions" of Wikipedia often discussed? I
mean the whole question of deletionism seems to me to be a
misunderstanding of the opportunities created by an internet
wikipedia. The problem deletionists try to solve is that of
quality/trustability, they try to delete all such material that would
not normally make it into a traditional encyclopedia, or at least
material which can be suspected to be wrong or just poorly written.
Yet, the great thing about wikipedia (or the internet in general) is
that it is not a traditional encyclopedia - it can contain an infinite
amount of information (why not have bio's of all of us there?) and be
updated fast, rather than through a slow perfectionist process.
It seems that the antagonists in the deletionist vs inclusionist
debate have forgotten that they are dealing with an infinite medium,
where all solutions ( -> forks) may co-exist. The sensible thing to do
would be to have one "source" Wikipedia, which would strive for
maximum inclusionism, and then have editions which strive for a
certain treshold of quality, certain topics etc... (And there are
mechanisms that can be implemented to make sure the original source is
still efficiently re-used, the editions would be subsets of the
inclusionist full wikipedia.)
This would be similar to how Linux distributions work: Sourceforge and
the internet in general will create an endlees supply of Open Source
Software, and distributions are there to filter out the true gems for
easy access to the greater public. Instead now the different camps in
wikipedia seem to have gotten stuck on the idea of a one true
wikipedia, and battling over how that should be governed.
I'm aware of Citizendium
and there also seems to be various Wikipedia editions for - say - PDA
type of devices. But since the deletionists still seem to be so active
in Wikipedia, it seems to me that this solution is not yet obvious to
the majority of Wikipedians?
Lastly, I should note that this is a solution that could be
implemented by Wikipedia itself (as in, edited.wikipedia.org or
similar) or it could be launched as a "deletionist fork" (which
Citizendium seems to be?)

@_date: 2007-12-06 23:04:57
@_author: Henrik Ingo 
@_subject: [p2p-research] disagreement by deletion/addition 
Few more words on deletionism
This inspired me to write some comments on lifecycle of a wiki or an
Open Source project:
The (first) thing that is remarkable about Wikipedia is that it has
succeeded in creating an encyclopedia that competes with the
traditional ones (Britannica) and almost with no paid labor to write
the articles. As was proven by the first unsuccesful Nupedia projects,
the *only* successful way to do such a project is to go for radical
openness, which Wikipedia did (you are allowed to write even bad
articles, no need to register, etc...).
Once Wikipedia becomes more important it is only natural to see a
shift on focusing more on quality. Not only quality of articles, but
categorisation etc... Inevitably this will also raise the bar for
participating, for instance one justifiable way to exclude somebody
from contributing to Wikipedia is if one contributor writes a really
bad article (grammar mistakes, etc) and another contributor then
replaces that with a better text.
This kind of shift happen in Open Source projects too. It is not for
anybody to get code included in the kernel, apache etc...
So it is good to understand that what has happened with Wikipedia is
part of a very natural development cycle for any p2p project. What is
unfortunate is that deletionism is a very idiotic solution to the
problem. There are multiple better alternatives: I already wrote about
Editions, or if incorporated within Wikipedia itself they could
perhaps be created Views, different rating systems (Slashdot, Digg...)
and so forth.
Common with all these solutions is that everyone is allowed to
contribute, yet you are not guaranteed to actually be included in the
subset of some editions of "better quality". The audience on the other
hand is free to choose between reading a (supposedly) higher quality
subset of the full Wikipedia, or the full "raw version".
Come to think about it, could it be that Wikipedia suffers from a lack
of competent programmers? All of the good solutions would require
technical changes, whereas deletionism is something the administrators
can have adopted as a policy themselves. Even if I'm sure good
programmers could be found, if the leading personalities in Wikipedia
are non-technical "humanists" (as is likely, they are encyclopedia
editors after all) they might not be able to see these solutions in
the first place.
BTW. Is what I'm writing obvious to anyone, or not?
I feel a bit like Sam, I'm used to spending time with people to whom
even basic Open Source concepts are amazing, and the situation of
having peers like you to talk with is kind of new to me.

@_date: 2007-12-25 20:31:55
@_author: Henrik Ingo 
@_subject: [p2p-research] DRM standards 
I think the following short rule of thumb is a good start: For various
"ethics related" reasons one may be opposed to all DRM on principle.
On the other hand, if someone is going to use DRM, it is clearly
better that they'd use an open standard - then at least there is a
chance that the DRM is not a reason to make the content locked into a
proprietary one-vendor/one-platform solution. (Richard Stallman would
disagree with this opinion, saying that DRM is always a form of
oppression and producing a good system of oppression is actually worse
than a bad system.)
In practice even open standards DRM will always have to rely on
something being kept secret from the end user / consumer and therefore
the opennes is of questionable value to the end user, who's role is
restricted to being just a consumer.
Typically an open source content player is not a possibility, or at
least some library file providing the particular decryption functions
would have to be closed source. This because even if the DRM system
would be based on an open standard, at least some cryptographic keys
have to be hidden from the user. Other alternatives are to hide the
decryption component in some hardware, like a smartcard or the
infamous TPM chip on a motherboard. Even so, something is restricted
from the end user, this is just another place to hide it. In addition
to hiding the decryption function, a proper DRM also wants to protect
the path from decryption to output device (so that you couldn't copy
the content anywhere within that path). This is why DVDs will play
with lower resolution on Windows Vista unless you have a new monitor
that will give the proper responses in this game.
cryptographical standards "good" DRM is actually an impossible problem
to solve. While good cryptography always relies on the protocol being
public and only a key being secret, the problem DRM tries to solve
necessarily leads to solutions that by cryptographical standards would
be considered ugly hacks. Hardware based solutions are slightly better
in this regard, since extracting the secret from a hardware chip
really would be practically impossible. Nevertheless from a
cryptographical point of view DRM is like eating the cake (giving user
content) and trying to keep it too (not giving user content).
So in practice an open DRM system will always be like "doing the wrong
thing the right way".

@_date: 2007-12-26 16:41:34
@_author: Henrik Ingo 
@_subject: [p2p-research] DRM standards 
Ah, indeed. So this is actually quite the same for which Creative
Commons already created its own RDF based markup language. Such
standard(s) would indeed be desirable.
Moreover, it is probably best not to call such a system DRM at all.
(It could be a component of a DRM system, but would have valuable
independent uses of its own, of which Creative Commmons is a good
example.) DRM automatically leads me to think of a system where the
content is restricted by some sort of encryption/obfuscation.
I do not know, apart from "Rights Expression Language" what these
tools should be called though.

@_date: 2007-12-26 17:20:50
@_author: Henrik Ingo 
@_subject: [p2p-research] Your Swarm Registration / enterprise p2p 
I believe the the "underlying theme of power" is exactly the right
place to start such a research and will boldly advertise two of my own
links, second of which is from our Nottingham workshop:
And continuing again with a short armchair analysis of mine... From
this it follows that a modern enterprise is not well suited for  a p2p
governance model, because in commercial enterprises aka as companies
the power by definition lies solely within its owners ("hegemony of
the owner"). Therefore the balance from "community voting with its
feet" is lacking. By this logic even if a company was seemingly
governed by a p2p process, in reality it would only be at the grace of
the owners, who at any time could take back the power they had decided
to give to the p2p process. (To be continued below...)
In contrast the problem of an enterprise acting as an agent within a
larger p2p system is more easy to approach and as a rule of thumb I
would suggest that such a situation is not so different from when
individual persons act in a p2p governed system. For instance, a
company such as IBM will decide to voluntarily spend its resources on
Apache or Linux projects, without any contractual guarantee to receive
anything in return. This is completely equivalent to an individual
person choosing to participate in such a project. OTOH even this is
not an easy task for many companies, for instance internal processes
for many companies I know would make it impossible for them to really
participate in the blogosphere, because the company culture is such
that employees writing uncensored statements in public is unthinkable.
(Even if the company allowed it, many employees would not dare to
actually write anything more interesting or useful than the average
company press release.)
Also enterprises other than commercial companies (like a non-profit or
a chess club, say) could be more easily governed by a p2p process,
since they don't have owners in that sense to begin with. In
particular, such an enterprise might incorporate in its bylaws some
clauses which would enable easy forking. Yet even then such an
enterprise will probably govern some scarce material goods which
cannot be forked.
(And now back to our main story of commercial companies...) So to have
truly p2p governed commercial companies we must come up with a way to
shift the balance of power from "hegemony of the owners" to a more
balanced situation where power is distributed with the owners,
employees, customers etc... We could start by questioning the hegemony
of the owners directly, is it a true view of a commercial company, or
is it just something our society is falsely taking for granted? While
it is true that the employer has tremendous power of the employee, in
practice it would be terrible for the company if a majority of its
employees revolted against the employer. Similarly if even a
significant minority of the customers are unhappy with a company. So
maybe there is more balance than we think? On the other hand it is in
our society seen ethically ok currently for the employer to restrict
the power of its employees, in particular "forking" of a company can
be contractually restricted by having employees accept non-competing
agreements. (Whereas a Free Software license for instance, enables and
almost encourages forking.)
As an illustrative example to help me get to my point, consider the
military in a modern western state. Arguably, if the Finnish army
decided to take power in Finland, they would technically be able to do
so. (Although, the US adventure ini Iraq proves it is not as easy as
you think and I might in fact be wrong on this point.) However, in no
established western democracy is this at all likely to ever happen.
This is because the values of the society and the military itself sees
the military as subservient to democracy. So again, the question is:
Could there be a way to start looking at modern commercial companies
in a way where the balance of power is radically different from the
current hegemony of owners? If not, then the second question is, how
should society change its values and legislation on this issue, to
better enable such a situation to come by.
Finally of course there is the possibility of a future where
commercial companies are not the primary vehicle driving society
forward as they are today.

@_date: 2007-12-28 14:20:32
@_author: Henrik Ingo 
@_subject: [p2p-research] creating a policy network 
Youre welcome :-)
Of course, in policymaking/lobbying issues, the FFII.org is your best
ally and they'd have all the MEP contacts you're interested in.

@_date: 2008-04-03 22:38:37
@_author: Henrik Ingo 
@_subject: [p2p-research] Who is contributing to the kernel 
I remember there was a fierce debate in Nottingham over some
statistics related to who is contributing to the Linux Kernel and how
much. The Linux Foundation just published a paper on this issue
Also see:

@_date: 2008-04-28 08:56:42
@_author: Henrik Ingo 
@_subject: [p2p-research] from a us intelligence briefing 
Ah, I recommend reading
Manuel Castells and Pekka Himanen. The Information Society and the
Welfare State: The Finnish Model (2002)
...coupled with a subsequent relocation to Finland of course :-)
My newborn sons mother is at the start of a (up to) 3 year paid
maternity leave. (Which can of course be extended by the simple method
of having more children.)
And personally I'm enjoying my decision to work for a distributed
company such a Sun (former MySQL), where I spend 99% of my days
working from home office. So I'm working, but with my family.
There are various shades to capitalism and Himanen & Castells argue
Finland is an example of having the best of both worlds.

@_date: 2008-02-04 09:58:31
@_author: Henrik Ingo 
@_subject: [p2p-research] [P2P Foundation] From Citizendium To Eduzendium 
Hi Jon, welcome to the list and for taking this discussion here too:
Just as a side comment to the above: When talking about peer
production in general, one should not be too keen on seeing any
equality or egalitarianism in its processes. In fact, the principle of
"benevolent dictator" and meritocracy explicitly state that some
leaders of the project will use their own power above other, and those
leaders are often not elected to their positions in any democratic way
(and this works very well in Free Software). Exclusion again is a very
important ingredient to almost any successful Free Software project,
its users would expect everything but the most high quality
contributions to be excluded, again nothing wrong there.
The problem with Wikipedia simply seems to be that a growing number of
people is disagreeing with the actions of its "leadership", that there
is exclusionism where there shouldn't be, etc. In fact, Wikipedia
would probably gain from a more explicit, yet still undemocratic
leadership. Now it seems that the more stubborn one always wins, this
is a terrible state of affairs. If a "dictatorship" powers where
clearly given to a small set of leaders, who wouldn't be acting behind
pseudonyms, it would be better than the current situation.
Even so, the Citizendium model with more rigid and democratic-like
governing processes is probably a very healthy step at this point, and
it may well be that for an encyclopedia project it is the right thing
to do. (Debian is an example from the Free Software world, where a
demcoratic governance model clearly is the best fit.)

@_date: 2008-02-04 10:33:43
@_author: Henrik Ingo 
@_subject: [p2p-research] [P2P Foundation] From Citizendium To Eduzendium 
Yes and no. I guess we have to be more explicit in how we map aspects
of Wikipedia to Free Software and vice versa. As I see it
 - anyone is free to try = anyone is free to write text or code
 - anyone is free to publish this on the internet, a mailing list,
even try to publish it on wikipedia until it is deleted
 - deletion is an integral part of striving for excellence. Even in an
inclusionist wikipedia you'd have to tidy up spam - at a minimum
So the problem with wikipedia is not that deletion is possible, the
problem is that wikipedia is now run by idiots. And that a lot of
other people don't agree with the choices being made by those who de
fact have the power in wikipedia. As Tere points out, the cure to that
is typically to fork the project.
So 1) wikipedia exists, 2) there are some people that hold the power
in it 3) they've chosen to make it into a very selective encyclopedia
instead of an inclusionist one (which is a choice I wouldn't agree
with but a possible choice for a website to make) and 4) unfortunately
the deletion of articles doesn't seem to sum up to "excellence",
judgement seems to be based on many other factors than the quality of
the article and experts are therefore alienated from contributing at
all, creating a bad spiral effect.
I'll say it again in slightly different terms. The desire to be
"selective" is not the problem, but somehow Wikipedia has failed to
arrive at the "excellence" part.
But also, as I've previously noted, in an online medium there wouldn't
be a need to be this selective at all. A rating system or some other
softer mechanism (like CZ editing process) would be a better
alternative than just rampant deleting.
Does this make sense?

@_date: 2008-02-04 12:11:54
@_author: Henrik Ingo 
@_subject: [p2p-research] [P2P Foundation] From Citizendium To Eduzendium 
Sure, I completely agree that what is going on at wikipedia currently
is crazy. I just wanted to point out that elsewhere we have thriving
Open Source projects that are selective and undemocratic. So those
notions in themselves cannot be at fault.
So I still hold that the only problem with wikipedia is the
incompetence of its current leadership, and whatever are the failures
in its governance process that allowed it to come to that. (Like
pseudonyms, the more vocal ones driving out the more sane ones,
I guess my slightly positivist interpretation of deletionism would be
that you certainly can decide to run a website with only perfect
encyclopedia articles. (The fact that WP editors fail to correctly
identify good and bad articles is a separate issue, I'm just talking
about an objective here.) In that light deletionism is just a
statement from WP that they don't want their encyclopedia to be a
staging ground for less than perfect articles, you should first write
them somewhere else and once perfected copy them into WP. This is of
course ridiculous, since as you say there is no need to play the
scarcity game, it is also contrary to how WP come to be in the first
place and wiki philosophy in general. So I agree it is utter folly,
yet I interpret it as a somewhat deliberate policy of the current WP
leadership. And this of course brings us back to my main argument,
that the main problem is incompetent leadership.

@_date: 2008-02-04 20:44:57
@_author: Henrik Ingo 
@_subject: [p2p-research] [P2P Foundation] From Citizendium To Eduzendium 
Hi Michel, others
After being out for a walk, I thought I should slightly adjust my position :-)
Thinking about it for a day, I still hold that many Open Source
projects are undemocratic, however I think most that are successful
put effort into encouraging new developments, even forks. (For
instance, there are always multiple kernel trees, often 2 versions of
samba, etc, but they are not considered forks since they are
officially encouraged.) So yes, while there still exists a right to be
selective, good projects at least accommodate for alternative
solutions to co-exist, if not always including them.
But still, I hold that the main failure of wikipedia is not
deletionism, that is more like a symptom. The core problem is the
governance process that allowed current leaders to become leaders.

@_date: 2008-02-11 08:26:09
@_author: Henrik Ingo 
@_subject: [p2p-research] mode of production shootout 
It seems to me it would make sense to separate physical and immaterial
peer production?

@_date: 2008-02-13 10:24:21
@_author: Henrik Ingo 
@_subject: [p2p-research] Open letter to Routledge 
As I wrote privately, I'll sign anything (almost:)

@_date: 2008-01-06 18:12:31
@_author: Henrik Ingo 
@_subject: [p2p-research] 
Good work Michel
This is a very good piece. The last point that Wikipedia's founder is
running a related for-profit business shouldn't be given too much
weight, it is kind of common with many Open Source projects. Apart
from that the compilation of criticisms was eyeopening. I read in
particular the case of  If any of the
other links support this is true, it seems the wikipedia
administration should be ousted in a popular revolt - or a fork
(although brand value of wikipedia is so high the cost of forking is
almost prohibitive).
It seems my previous thoughts on Wikipedia administrators simply
misunderstanding the situation and opportunities of an online medium
was way too positive. I now believe Wikipedia has in fact been taken
over by a bunch of incompetent fools, who are drunk on the power of
administering facts on the worlds now most popular source of
This reminds me of what easily happens with large mailing-lists. An
online medium runs the risk of becoming dominated by those who have
most time to spend - such as people who are unemployed because of
mental problems (this was literally the case, I'm not kidding - of
course some of the others were just weird types without a social life
and therefore spending too much time creating controversy on the list)
- and therefore can afford spending so much time producing nonsense
that all the sane people (with day jobs) just have to surrender and go
One would certainly hope something is done to correct this problem,
Wikipedia is too valuable to suffer such a fate. Let's hope a group of
more clueful and responsible people are willing to spend their scarce
time in the battle that seems to be necessary.

@_date: 2008-01-07 10:16:58
@_author: Henrik Ingo 
@_subject: [p2p-research] 
Sure. And as your other email shows, it seems that this might indeed
have happened. Very sad if it is the case. It seems that Wikipedia
governance has not been given the same attention as the actual
producing of material.

@_date: 2008-07-02 03:46:04
@_author: Henrik Ingo 
@_subject: [p2p-research] programming a direct democracy 
Greetings from Dublin, middle of the night...
Yes, so in Open Source it is great that whoever can understand the
code and invest time in programming can participate, those two things
function like a good barrier to enter the system. (Since a project is
interested in exactly the kind of programmers: who understand what
they need to do and have time to do it.) But with legislation or other
situations where the question is about power, I see it as almost a
recipe for disaster to say that those who are the loudest ones and can
afford to spend more time arguing than some other people, will
eventually wear out the opposition and have the decisionmaking power
for themselves. The difference with Open Source is that you can always
choose to ignore a long winded discussion and choose to do productive
work instead. I don't see how political decisionmaking could share
that feature.
That being said, it is obviously true that a good voting system with
no system for debate would be a hilarious thing too! (Random
decisionmaking, almost?) So you need to have a system for good debate
or deliberation too, but the power must not reside in the debate. You
must be able to ignore the debate and still retain your share of
power. Thus, you must be able to delegate your power so it is being
used even if you don't invest your own time.
I couldn't resist browsing the  site a bit and it
seems they have thought about this part quite a lot. For instance, the
time a vote is open fluctuates depending on its activity. It is
designed to make stupid proposals just go away and good ones to get
approved when there are enough votes. Really interesting and there is
an English FAQ at least.
(Unlike myself, where I have thought more about the voting and
delegation of that, assuming that Internet is good at organizing
communication so I was never worried about that, kinda skipped that

@_date: 2008-07-20 10:52:53
@_author: Henrik Ingo 
@_subject: [p2p-research] fyi: interview in Rome, 
Without knowing Kris at all, I'll boldly comment on the 2nd part:
The non-motivation should mostly not be seen as a problem. The thing
is that in a global / internet-wide p2p setting, the mathematics are
just very different than what we are used to. So there are millions of
Linux users but only about 1000-1500 Linux (kernel) developers. And
this is a very high number. There could be millions of users for
something with only a handful of active contributors.
Facebook is a good example of this too: Many FB users are probably
like me, very passive, mostly just accepting friend requests. But
maybe 2-4 times a month I feel inspired to forward something and maybe
once a month I'll write something in my status box. And that's all
that is needed. In the eyes of my connected FB friends that probably
makes me an active FB user, since a couple of times a month something
from me will pop up on their radar.
To give a final example: I maintain a module in the Drupal CMS system
(the Footnotes module). In a year I probably spend 40-60 hours
programming it. If I didn't do anything for Drupal it probably
wouldn't matter much. Even so, many other Drupal modules are just like
this too. When you have hundreds of persons doing the 40-60 hour
effort, it suddenly becomes significant.
All of this is actually an aspect of the long tail phenomenon. So the
fact that you have millions of passers by that don't contribute is
just as it should be.
And finally, there might be projects that really don't get off the
ground for a lack of contributions. That should be seen as a feature
too. It just means nobody cared. You might think your idea was
important, but it wasn't. Or maybe it is, but you failed to convince
anyone. Often for this reason many successful projects start with one
mans heroic efforts for 1-2 years. Therefore the counter question is,
did you care about your own idea enough to make that initial
investment? (Hmm... I've written about this too: See first half of
chapter

@_date: 2008-06-04 22:27:55
@_author: Henrik Ingo 
@_subject: [p2p-research] open source alternative to second life 
To reply to the original question then...
I'm not much of a gamer and never used secondlife either, so I don't
have much to say. I guess the only wise thing to say is I'm glad I
listened to my professors advice and added more to the book title than
just "Open Life".

@_date: 2008-06-29 21:40:14
@_author: Henrik Ingo 
@_subject: [p2p-research] programming a direct democracy 
This was a nice link, though not perhaps for the reason you describe:
I'm packing for a week-long business trip, so the whole article was
too long for me to read now. However, I was glimpsing through it, and
stopped when it talked about sitting in assembly raising hands.
Clearly this is not what I'm interested in with my system. (Academic
writing style gives me headache too!)
Yes, the technique proposed could be modified to a useful tool for
"my" direct democracy society, perhaps... or not. In general I think
"Deliberative Direct Democracy" ideas are misguided, since the
fundamental assumption that people can or should spend all their lives
debating legislation is false. (And since it is false it means that
those who can spend the most time end up with most influence, which I
consider a flaw.)
But, the first comment leads to a very interesting Swedish party, with
analogous ideas to myself. I should be in touch with them sometime.
This is what I wanted to thank you for :-)

@_date: 2008-03-29 23:43:41
@_author: Henrik Ingo 
@_subject: [p2p-research] From the document web, via the data web, 
It's in the middle of the night, but I just feel compelled to add some
points. This is not good enough for a blog, but may be freely
copypasted by anyone...
My impression of the article is that it is written by some academic
non-programmer, who has tried to study the HTTP protocol and some W3C
standardisation efforts, but has no experience in actually producing
web applications. As first impressions go I could of course be very
wrong, I didn't even bother to read some of the middle parts of the
The Document web definition is fine, it is what anyone would consider
"Web 1.0". What I strongly disagree with is the authors criticism or
belittlement of current "Web2.0". In my opinion a significant shift in
the web happened with the maturation of the Firefox browser, which
released an avalanche of web based applications and portals that made
heavy use of JavaScript and CSS. (If someone wouldn't like the term
"Web2.0" it may be better and clearer to call this "The advent of
Before Firefox there where 2 browsers, Internet Explorer and Netscape
that supported advanced JavaScript, but they supported totally
different versions of it (the standardised version today is the IE
one, a testament to the fact that MS indeed employes some very good
programmers, the ones that happened to work on IE from 4.x to 6.x
before 2001). Therefore most pages that tried to do anything with
JavaScript or advanced CSS supported only one of these browsers, or
sometimes tried to support both of them, often with poor results. And
many in the universities or Open Source crowds for instance were still
using text-based browsers - which is notable because at the time this
group had significant mindshire in the web's development. For all of
these reasons use of JavaScript was considered evil by (in my opinion)
a majority of web developers and what was then called "Dynamic HTML"
was mostly a phenomenon of the Microsoft camp. (Even today if you use
the web interface to Microsoft Exchange email, it is very nice on IE
but barely usable on Firefox.)
With the advent of Firefox - which supported the then standardised IE
style of JavaScript - the situation started changing, since there now
was a standard, and a free multiplatform browser to support the
standard. Quite soon very cool web based apps were born, led by Google
maps, Google mail... This was called AJAX programming, as in
Asynchronous JavaScript and XML. Compared to Microsofts DHTML
evangelisation this was much cooler technology than anyone had ever
dreamt of, and availability of an Open Source browser to support it
made also the opposition vanish. So imho this, and not IE4.x with
DHTML support was the de fact next phase of the web.
At the same time we had developed some additional techniques - most
signicant would prehaps  be RSS and the family of XML markups used to
provide blog feeds. This lead to a collaboration between websites
beyond linking: You could provide parts of another blog or newssite on
your own page, for instance. Or to take a very different example,
BookMooch uses Amazon to provide data and cataloguing of books. Yet,
BookMooch is a site for free sharing of old books, you'd think Amazon
wouldn't like "helping out" such a project. Not so, in reality lots of
BookMooch users end up buying books on Amazon. In fact, BookMooch
probably makes most of its income based on money they get from Amazon
for these referrals.
AJAX combined with RSS and some other by then standard tools (wiki is
a significant one) is in my opinion rightly called Web2.0. This is
very different from the original document based web and rightly has
been given its own name.
Web2.0 is NOT the social web (like FaceBook, LinkedIn). The social web
is merely an application of Web2.0, technically it doesn't contribute
anything new. (Well, apart from FaceBooks innovation of letting 3rd
parties develop applications embedded in its own site, that is a great
innovation, but it is not "THE social web".) Why the social web is so
much hyped is in this context in fact a good question, I believe there
is in fact a little pyramid scheme to it all. I mean Facebook is fun
and all, but it isn't THAT fun, I think the effective inviting
mechanism plays a part.
This is the point we are now. Now for my own predictions:
Next we will see the advent of the Single sign-on web, most likely
emodied in the form of OpenID. (SSO means you don't have to create new
logins for every site, you just use one main identity and password to
log in to each site. Obviously the sites you log in to don't get to
know your password, they just accept the referral from your ISP, mail
provider, or other OpenID provider you are using.) This imho will add
further granularity to the web, in that users can come and go more
fluidly than today, where you make a choice to register and join
FaceBook but not something else. This in turn should foster a
development where we can again have smaller sites providing one small
funny little piece of the social web, instead of the monolithic
FaceBooks of today. This would be in line with what Web2.0 was all
about, Facebook et al are in fact a countertrend to the Web2.0 trend
if seen in this light.
Whether a "decentralised social web" will arise from this is a good
question, and whether the Global Giant Graph will emerge from that is
an even better question. It might, but it might end up something
entirely different. The GGG is technically possible today, and how
OpenID works there are some similarities to the RDF used in GGG, so
once OpenID becomes popular, the next step might be to not just
externalise (or decentralise) your login credentials but also your
social connections. But we will know the answer to this in something
like 5 years.
The proposal in the end on new HTTP commands is just pure folly (it is
just the wrong place to do it, period), which underlines that the
author wasn't just slightly off with his Web2.0 comments, but in fact
knows nothing at all about the technology he is talking about. To
implement such functionality by extending HTTP would imho be quite
silly, and in fact a peer-to-peer protocol like SIP would probably be
a better starting point in the first place, and even then you wouldn't
do it by commands like those, but you'd develop an XML based document
language to transmit this kind of information.
So, I guess it turned out to a semi-good commentary after all. OTOH I
think you stole my evening with this link so I'll have to do what I
really was going to do tomorrow. Good night!

@_date: 2008-03-30 23:09:04
@_author: Henrik Ingo 
@_subject: [p2p-research] From the document web, via the data web, 
Athina, I still disagree
On Sun, Mar 30, 2008 at 5:17 AM, Athina Karatzogianni
I still disagree. (Will still not bothering to read all of the article :-)
For instance, to continue using the HTTP protocol as an example, the
purpose of it is to locate a document and deliver it to you. There is
no room for "fuzzy logic" here, nobody would want to use a web where
you get a page that is ALMOST the one you requested.
The fuzzy non-binary part is contained within the documents that are
delivered, and I argue that to some degree we already have made great
advances to a more colored and less black and white reality. Instead
of reading THE political truth from the one major Finnish newspaper,
you can read a lot of blogs on the net with DIFFERENT viewpoints. You
can go to Wikipedia to read an article that is ALMOST factually
correct. etc...
Furthermore, there is computer science that deals with non-binary
logic, in particular Artificial Intelligence practices like Fuzzy
Logic or Neural Networks (which try to emulate how we know our brains
work, really interesting). While this is much more difficult than
"normal" programming, and therefore these techniques are not as
prevalent building blocks of the Internet today, as the "binary based"
technologies are, we do use them today:
 - Spam filters are often based on Bayesian filters or some other NN techniques
 - Google tries to find you pages that you are most likely going to be
interested in (as opposed to pages whose content most exactly match
the keywords you search for)
 - Amazon will send you advertisements on books that they think you
might be interested in, etc...
 and
Again, I see Web2.0 being exactly like this. The blogosphere is not a
customer-client relationship, it is a "networking and building on each
other" phenomenon.
But I'll give you the last part, although it is imho a subjective
judgement. To me as an IT literate, I know of nothing more fun than
the current Internet :-) Of course, the future Internet will be even
more fun!

@_date: 2008-05-15 00:09:06
@_author: Henrik Ingo 
@_subject: [p2p-research] Of possible interest? 
When writing, I too totally enjoy the aspect of not being in the
academia. I never was motivated to add a lot of references just to
prove the reader that I've read some important books but that were not
relevant to the issue I'm writing. I understand when writing an essay
in high school, you do it mostly for the teacher, but after that imho
it should be because you have something to say, not merely to report
that you've read something from some authority.

@_date: 2009-12-13 17:24:36
@_author: Henrik Ingo 
@_subject: [p2p-research] An email today can save MySQL tomorrow 
Since members of this list might be interested in MySQL's future, I'd
like to forward this plea from its creator and my friend (disclaimer:
and my boss), Michael "Monty" Widenius:
In short: email comp-merger-registry at ec.europa.eu and let them know
whether you think Oracle is a good owner for its main open source
competitor. Preferably do this by Monday morning, but don't give up if
you read this later than that. If you're in a hurry, just send a short
one, sometimes quantity is more important than quality.
You can do this whether you're European or not.
