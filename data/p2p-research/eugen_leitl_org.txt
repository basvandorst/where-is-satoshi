
@_date: 2009-08-06 17:54:51
@_author: Eugen Leitl 
@_subject: [p2p-research] The Starbucks API 
Anyone can cook like Bocuse. Just write it all up and distribute it
to McDonaldses everywhere, and hey presto! Instant haute cuisine.
See, this replicating excellence thing is easy.

@_date: 2009-12-01 11:11:32
@_author: Eugen Leitl 
@_subject: [p2p-research] about those old android phones for 3rd world 
A free netbook from Google is much too expensive. I cannot afford it.
In fact most people can't. They just don't know it yet.

@_date: 2009-12-04 08:41:00
@_author: Eugen Leitl 
@_subject: [p2p-research] Google gets into the DNS business 
It is ridiculous that people would willingly give over more and
more of their services to a single commercial entity, which at
worst is a one stop shop for the TLA of your choice, government
subpoena or data mining for marketing purposes.
Running your own caching DNS resolver is a) trivial b) the only way to prevent your ISP monkeying with your DNS queries trivially.
(I reside in a country which must record connection info by law,
and block sites in a blacklist, currently implemented via DNS).
The only way to make sure your ISP is not sniffing or altering
your data in transit is to route all your traffic through a
VPN (e.g. OpenVPN) tunnel to a (V)server, preferably in a different

@_date: 2009-12-06 13:01:05
@_author: Eugen Leitl 
@_subject: [p2p-research] Google gets into the DNS business 
At the very least, they should avoid aggregating all those
services with one vendor, however convenient that may be.
Email yes, though there are advantages in a platform that has
many users (calendar sharing). There are currently no easy
ways to seamlessly to aggregate and integrate individual
feeds. This is something which can and should be fixed.
If you have a dedicated system at home it will cost you about that
much in term of electricity costs. Of course nowadays we have embedded
systems which take 5-10 W and can do most of the work required.
These can be easily findable using services like DynDNS or using
IPv6 tunnels.
VServers of course come with a highly available low-latency connection
and a static IP (or IPv6 subnet), and are very efficient energetically.
Well, there are some enduser products becoming available, e.g.
Of course here you're relying on a company again, though that needs
not to be that way in principle.
I see this more as a business opportunity. If 1-5% of all customers
are interested in securely hosting their own storage and applications at home in a trusted environment provided it's trivial to setup that is a
considerable market. Drobo and PogoPlug are catering to that.
Try setting up a Pogo Plug, just for shits and giggles. What I am missing is awareness in those potential 1-5% target that
they're having a particular problem, and that problem can be solved
painlessly with the right kind of appliance, or commercial service
which is not concentrated, and doesn't try to lock you in.

@_date: 2009-12-06 16:19:36
@_author: Eugen Leitl 
@_subject: [p2p-research] Google gets into the DNS business 
If you're pulling 10 W it's something different from 300 W. As a rule of thumb an embedded platform will pay for itself
within less than a year.
DynDNS is ok. P2P can be in principle completely serverless.
Sharing services can be typically on arbitrary ports.
Not really an issue in most of the world. At worst you have you
deal with ISP-wide NAT.
If your service provider eats your data you've got not even a bricked piece of hardware to show for it. As a matter of fact,
it's not unheard of for people travelling to have two such
boxes, which keep each other in sync.
If you want your data secure, use something like Tahoe.

@_date: 2009-12-06 17:58:03
@_author: Eugen Leitl 
@_subject: [p2p-research] Google gets into the DNS business 
I wasn't refering to SMTP, of course.
There's not much point running your own SMTP server (unless you're
using an external relay) since you're sending from an address pool which is
almost certainly blackholed.
This is the way to go. Yes. You're also completely under an xray screen, since running on
untrusted, shared hardware where the administrator has higher than
superuser access. Of course, if you're routing everything
through a VPN to different entity, especially entity in a different
jurisdiction, so this already adds a layer of protection.
If you want to make it interesting, send in a small low-power tamperproof
system with trusted hardware to your hoster's colo. These days,
it is almost as cheap as a VServer.
Even if you don't use a VPN tunnel, you can use IMAP over SSL, and let
local mail server do StartTLS. Still much better than nothing.
I'm viewing the issue holistically, and not necessarily in terms
of today's technology.
Quite a pity, since this is about as P2P and grassroot as it gets.

@_date: 2009-12-06 20:21:02
@_author: Eugen Leitl 
@_subject: [p2p-research] Google gets into the DNS business 
You can't. Neither a corporate nor a government entity can be trusted.
You can trust other end users a lot more if safeguarded against
snooping and tampering precisely because their capabilities do
not differ markedly from your own. Cryptography is a key way to achieve that.
Cryptography is also a key technology to keep track of trust and reputation.
I find declaring defeat a priori both unwise and dangerous. Also for other people,
not just you. Especially since you cannot solve social/political problems technically. Paranoia is the wrong term, since we do not have to guess under
which level of scrutiny we are living. We know. It's mostly a matter of
public record. Very little if at all has to be extrapolated to fill
in the blanks. The first time I've run into security problems in regards to military
cryptography and was disciplined for it was 1988. Since then, I very
much doubt my profile has gotten any less conspicuous. Or so a little
bird told me a few years ago. Log yes, decrypt no. TLAs can do a lot more, but they have their limitations.
Nobody does brute-force decryption of modern cryptosystems. Brute-force
attack on the passphrase, yes. Sigint and system compromise, yes. Brute-force
the cryptosystem, no.
Regardless of potential weaknesses, if the only alternative is cleartext
whether to do or not to do encryption is a no-brainer.
Not my threat model. You're wrong about systems getting faster, this is not what cooks your goose. Without advances in cryptoattacks faster hardware
is meaningless. Hey, they can log those few TByte/month that passes through my system.
Be my guest, I have no idea what's in there. Please store it, please burn
resources on it, that's precisely the point. Sufficiently widespread
nonclear traffic is indistinguishable from lots of honeypots.
You have a strange model on how cryptoanalysis advances.
Let's say you're organizing a strike or a demonstration today. Of what
interest to you is that somebody *might* (I consider it unlikely, session
keys are ephemeral) break a given OpenVPN session log two decades hence?
Use encryption today to make sure this doesn't happen tomorrow.
There's no guessing about that. It is routine today. You're right now in
violation of probably a dozen laws at least, and can be persecuted for it.
I'm missing the part where they traced his Tor session and cracked his Truecrypt system partition which was secured by a strong passphrase.
Oh wait, because that never happened. He unwittingly ran into a honeypot, and submitted to have his unhardened machine to be analyzed forensically, then pleaded guilty in a fishy bargain in a modern equivalent of a Malleus maleficarum case.
Much more interestingly is that in many countries service providers
must log your location info with time (which is a movement profile)
and record connection info. Plans to log your complete online trail
are in the works, at least in UK. Plans to only allow going online when authenticated with personal ID smartcards ditto. One wonders what plans are in the drawers we haven't heard about yet.
GSM "encryption" was deliberately designed to be weak. Realtime
GSM cracks are over a decade old now. Try the same with another
cryptosystem considered deprecated, such as 3DES. Not so easy now, eh.
Not my SIP session over OpenVPN they won't.
You're describing an existing arrangement.
It depends on your threat model. For most cases you're wrong.
Now you know why I no longer travel to UK.
That is not a very useful assumption. A much better assumption that
everything in cleartext passes through tapping points which can do
nontrivial filtering, and can destil down interesting traffic for further
scrutiny and possible storage, including human analyst eye time.
Already trivial encryption will very effectively disrupt this
large scale passive scrutiny of cleartext. Active MITM takes a lot
more sophistication and can be detected in principle, so it can't
be done on a wide scale. If you think it will be peaceful, legal and non-violent you're bullshitting
yourself. The current developments are not sustainable. Either the current
trend will reverse, or we'll see considerable mayhem ahead. (We already know
that the current national administrations are deploying this technology to early identify and isolate trouble hot spots -- any trouble hot spots).
Precisely. So what are you doing about it?
If you want to assume the worst, you should consider us becoming the
Emergents, in Vernor Vinge's A Deepness In The Sky novel. I do not see how we can recover from that once we're there. This isn't
your grandfather's Evil Empire. Oh, the horror. The horror of it. Imagine if this was the rule. Imagine
the number of data you would have collect, and how many human analysts
you would tie up listening to pillow talk or academic treatises. If
you've read your Bamford, you'll know that this is not a theoretical scenario.
If you are in that situation, it is your civic and human duty to organize
underground resistance. If you don't, you will eventually find yourself at
the receiving end of the system.
Paranoia works best if titrated to functional levels. And paranoia and
naivete don't really mix well. Not many people are that lucky, these days.
If the society does not use the tools for privacy, the society might find
itself in a failure mode quite difficult to impossible to recover from.
"We have no privacy anyway, so we can give up already" is exactly the unconstructive attitude to take. History tends to take a dim view on quislings.
Right now the current political and business process is directed towards
surveillance and control of uprisings in the coming scarcity scenarios.
I am an active member of the Pirate Party and so should you.
Jury is specific to anglosaxon law, IIRC.
I happen to disagree about that too.
The difference between US and EU is that EU does not even have guns to take
away first.
Who's interested in morality when you have a civil war in your hands?
Some severe cognitive dissonance here.
What if this is against the law and will land you in jail?
What if perpetuation of scarcity is exactly the appropriate
control paradigm, and you're the square peg that is in the way?
I prefer technology forms leading to empowerment of individuals, not more control for superpersonal organisation forms. Encryption as a means for
privacy, strong authentication, trust and tamper-hardening are key parts of that of that technology box.

@_date: 2009-12-18 08:52:49
@_author: Eugen Leitl 
@_subject: [p2p-research] Drone hacking 
It's not hacking, since the UAVs were not remotely exploited.
This is a simple tap (sigint), and has been a known problem since 2005.

@_date: 2009-12-19 20:55:38
@_author: Eugen Leitl 
@_subject: [p2p-research] Drone hacking 
If they didn't bother to encrypt the video feed with standard crypto
(e.g. 3DES or AES) despite it being a known problem since 2005 draw your own conclusions, gentlemen.
Unfortunately the drones are still too cheap. And unfortunately the
targets are too low tech to fight back with the same, and teach the real
meaning of asymmetric warfare by hacking a Cessna into a HE UAV crashing
a high-profile dinner, or by placing a primitive 10 kT nuke in a Manhattan penthouse during peak population hours.

@_date: 2009-12-20 10:55:09
@_author: Eugen Leitl 
@_subject: [p2p-research] Drone hacking 
It is definitely heartening that this has not happened yet.
The Curve of Binding Energy is from 1973. If anything, it is
much easier now, some 40 years hence. How our cities and
society would look like in times of high-frequency asymmetric
nuclear warfare is probably none of us is particularly keen
to experience. I don't know who is doing what and why, but you'll have to agree
that in terms of ROI a few kT in a major high-density location
are extremely well invested.
If you cannot increase the rate of body bag shipments (or, better
alive but maimed bodies) only way to demoralize left is to carry mayhem back to soft heartland of the agressor.
Doesn't describe any reality I live in.
The current generation is not really autonomous either.
Encryption is incredibly cheap in terms of logic real estate or CPU cycles. There is a distinct kill rate threshold at which US forces are recalled.
That, too. Money has nearly run out now, though.

@_date: 2009-12-20 17:58:22
@_author: Eugen Leitl 
@_subject: [p2p-research] Drone hacking 
EMP isn't new. Neither is remote control.
EMP has a very narrow range of impact (unless it's NEMP), and it only targets electronics. You can make hundreds of IEDs from one
EMP's HE charge, and this is exactly what you want.
Drones were first used in WWII. Also in Vietnam. I was on the Internet
20 years ago. Packet-switching goes back to 1960s. Technology never comes
out of the blue.

@_date: 2009-12-20 23:16:17
@_author: Eugen Leitl 
@_subject: [p2p-research] Drone hacking 
I hate me-toos, but this point can't be emphasized enough.
Sophistication and resources favor large centralized institutions.
As an example, imagine matching the capabilities of an intelligence
organisation such as NSA with a large number of volunteers. Clearly, this doesn't nearly work.

@_date: 2009-07-05 20:48:33
@_author: Eugen Leitl 
@_subject: [p2p-research] Fwd: More on the Supply and Demand Curve 
It is not out of question we'll get this low, though I hope not.
Arguably many factors are a lot of worse than 80 years ago, though
of course the current situation is hard to compare to then.

@_date: 2009-07-16 12:23:49
@_author: Eugen Leitl 
@_subject: [p2p-research] debate on open agriculture 
There's no doubt that industrial-scale food production can crank
the largest amount of food Joules with the least amount of people.
However, it takes 10 Joules of fossil energy to generate one Joule
of food, destroys the soil, biodiversity, depletes nonrenewable resources (soil minerals and phosphate), pollutes and depletes
the water table, increases erosion, and has a pretty low We're faced with an ageing demographics with people out of work.
A retired couple could occupy themselves quite well with a quarter acre,
or around 0.10 ha (50 x 50 m). In principle they could derive 100% of their calories from that space, using high-intensity organic gardening methods with no to very little mineral/synthetic fertilizer input.
This strikes me as a win/win situation.
Of course closed-loop life support ecosystems based around single-cell
algae photobioreactors could use even less space.

@_date: 2009-06-24 11:25:08
@_author: Eugen Leitl 
@_subject: [p2p-research] [Open Manufacturing] Addressing Post-Scarcity 
You're comparing apples and oranges. Wind doesn't scale well to small installations. Solar collectors do very well in small installations,
and PV does very well across the entire scale. Wind is hypervariable,
solar is moderately to low variable. You still need to buffer for
diurnal occlusion as long as you don't have a global grid, or solar
power satellites in line of sight.
Even current systems can easily produce excess, potentially considerable
excess for suburbia-like density in most locales.
Carnot is never a good idea.
Personal transportation is already included in residential PV, the needs
are negligible. Industry transportation and industry needs is industry's
own problem. They can buy power from small scale producers or large scale
producers, or build own facilities.
No disagreement there.
I disagree. There are multiple reasons why we should phase out old installations,
limit new installations until considerable R&D is sunk into novel reactor types
(unenrichened uranium and thorium in-situ breeders, in-situ fuel processing like
molten salt, etc). Even then, you would limit these to process heat and baseload,
and its fraction would asymptotically approach zero.
Meanwhile, we've got a bad case of zero-sum budget, and time is of essence. Frankly,
we no longer have time to fuck around. We've blown it in 1970s, now we need it yesteryear.
It can shrink considerably short-term, but it will grow long-term.
As a transhumanist, I think in terms of 4 MT/s matterenergy flux as
our mid-term limit to growth.
That is very true. We can and must compensate a log of megawatts with negawatts.

@_date: 2009-06-25 10:32:12
@_author: Eugen Leitl 
@_subject: [p2p-research] [Open Manufacturing] Addressing Post-Scarcity 
Ryan, please try to not top-post.
Unfortunately I missed Stan's email, because I killfiled him due to deliberate top-posting. In practice, there's a very large spread of insulation across the
industrial countries. Especially Japan and Germany lead here. The
real issue is ROI and EROI(E)I, which is a function of energy price,
culture, availability (zero-energy houses require special parts and
expertise), etc.
In practice high, monotonously rising energy prices as well as taxation
and subsidies can be a powerful motivator.
Stan, your objection is effectively nonsense because energy saving measures are quite popular in many countries, and objectively provide a saving
in comparison to those countries that don't. This is more than compensated
by the growth elsewhere, which I see as our challenge, duty and opportunity
to address.
If faced with peak resource, and especially rapidly falling ERO(E)I
the question is not how many extravagances we can afford, but just to stop
us from sliding into the past. Because we do, and our time to switch
to effectively infinite (~4 MT/s as compared to ~0.2 g/s matterenergy flux
we currently use) energy resources is rapidly running out.
Fossil are a limited resource, electrons are 100% recyclable.
Given some 0.2 harvest factor of Wp and panel prices of 1 USD/Wp and falling
I'm sure you can do the math.
Terrestrial solar can't, in absence of buffers. Baseline is some half to one third of daytime peak. Here's an incentive to
further lower it by dynamic electricity pricing.
Thermal collectors have an efficiency of >80%, or even >90%. They are readily
bufferable over diurnal cycles, and with enough ballast even seasonal cycles.
The solar constant is 1.4 kW/m^2. If this is little power to you, be my guest.
Current commercial PV has about 15-20% efficiency, which is a factor of 2-3
from crossover of residential electricity. Existing roofs alone can provide an
effective excess in residential areas. The question is of scaling existing
technology and introducing novel thin-film and power electronics to lower
the price. The sun never stops shining. Above the clouds, or somewhere else. You see a problem, I see a challenge and a business opportunity.
Wind (which is quite big where I sit) is a hypervariable resource,
and complementary in location and duration to PV and solar thermal.
If you see no evidence that the status quo will never change (the past
has proven you wrong already) I guess we'll just wait and see.
All energy saving in Germany has been driven by legislation and regulation.
If it doesn't work where you sit you must be doing something wrong.
Solar thermal absorbers are 80-90%. Current top of the line PV is approaching
50%, and rectennas scaled into VIS/NIR range might reach 80%.
However, it's a red herring, since the issue is of ERO(E)I and ROI over
lifetime, installation grain cost, resource base and recycling rate.
The efficiency isn't really part of the equation. Plants do it quite
nicely with 0.5%-1% efficiency, and they manage to self-replicate
and suffer being harvested.
Legislation for new installations (roof alignment, inclination, solar thermal
designed-in) is effectively cost-free. If you're looking at diesel, then you
already know what you're doing wrong.
US is not that important in the long run, so we should focus on new installations.
Efficiency gains are only suitable until you start running into diminishing
returns. Clearly you must both conserve energy and move to renewable base.
Why should this be necessary? Zoning laws can be readily overruled.
Some of us have been doing it for a long time. It really works.
We're clearly already observing that process in action.

@_date: 2009-11-03 12:36:47
@_author: Eugen Leitl 
@_subject: [p2p-research] organic architecture and post-scarcity 
You don't need fully programmable matter in order to grow. A termite-like
model (macroscale, or microscale) would do nicely. Both fuel and substrate
can come either from natural gas lines, or in situ photovoltaics (enrich
and fixate air CO2/H2O as feedstock).
3d systems for incremental deposition of more or less arbitrary shapes in building construction exist.

@_date: 2009-11-09 14:34:49
@_author: Eugen Leitl 
@_subject: [p2p-research] Developing countries falling into 'broadband gap' 
Apparently, 802.11n (no longer draft, though there is
no real difference) has been recently used for DIY WAN links with
open source drivers on Linux, with very decent performance.
With a pair of aligned sat dishes, the line of sight is
the limit.
Speaking about the sky, no phased array fixed-geometry
antennas yet. These can be retargeted in ms, and can track
anything across the sky, multiple targets simultaneously,
provided you have the ephemerides. Notice you can wire
consumer 802.11x access points as a poor man's phased
array, which can be used to probe for people behind doors
and walls.
4G chips have also gotten very speedy now, to a cut-through LEO constellation from few-kg microsats are within reach, even with current 10 kUSD/kg launch costs. Even few simple maildrops
with few TByte SSD onboard would do very well. Since Cerf has
got DTN into Android recently, I smell opportunities. It's the Internet, and given how things develop the open
Internet is dead, so we'll see a transition to a distributed
cryptographic filestore, with addresses like
which don't resolve to a particular location and
can't be censored.
There are a few unsolved problems still left, but in
general we've been making slow but steady progress since
Xanadu. See This is a job for a Layer 3 switch, not a router. Of course you can let
a random FreeBSD system handle this, but it's not cost effective in price.
Layer 3 switches are cheap, and the ASICs do both IPv4 and IPv6 in hardware.

@_date: 2009-11-09 16:22:37
@_author: Eugen Leitl 
@_subject: [p2p-research] is open source design inferior 
Ubuntu is a tradeoff, though. It's getting prettier, but also
more broken. In general Linux is the Windows of Unices, and Canonical is Linux' Microsoft.
The problem is that FLOSS people and usability don't mix.
At least the current culture doesn't seem to, it might mend
Not holding my breath, though.

@_date: 2009-11-09 16:59:26
@_author: Eugen Leitl 
@_subject: [p2p-research] big does not necessarily innovate: the current 
That's precisely the plan, actually. Not quite habitats as we know them, but habitats nevertheless.

@_date: 2009-11-10 12:57:53
@_author: Eugen Leitl 
@_subject: [p2p-research] a short message on our list culture 
Since Michael is to busy to format his messages right, I'm unfortunately
too busy to spend any of time time deciphering his writings.
Unfortunately, the disease is spreading. Apparently, the intelligence
of many does not reach to the point where they notice that they're
destroying their own communication channel. I've seen it happen too
often to not to care, but I still do.
Simple enough calculation, one should think. Perhaps too obvious.

@_date: 2009-11-10 15:38:28
@_author: Eugen Leitl 
@_subject: [p2p-research] is the mind a computer 
But how does it help us to transcend the human condition?
"You Could Be Immortal Already?"
Bunk. Statistics doesn't apply to perfectly biased samples of one.

@_date: 2009-11-10 17:00:27
@_author: Eugen Leitl 
@_subject: [p2p-research] Yacy search engine 
Ugh. It's Java.
Right. So no full Python port yet? Too bad.
How many nodes are out there, and how large the total index crawled?
What's the average query response time? How is query farmed (and
babby formed)? To how many nodes would it scale?

@_date: 2009-11-11 18:01:03
@_author: Eugen Leitl 
@_subject: [p2p-research] is the mind a computer 
Connectivity has a formal definition. Physical layer and
logic are two different domains, though physics of
computation sets you limits. However, these limits
are currently so far remote you can ignore them. You do not seem very familiar with machines.
That doesn't mean a damn thing. A machine is an abstract
concept, not something which you'll find in your toolshop.
It is good that you limit your scope.
Why must machines be created by humans?

@_date: 2009-11-12 10:58:37
@_author: Eugen Leitl 
@_subject: [p2p-research] Hermit Nation: Does Tech Boost Social Isolation? 
Well, it will be history in less than 5 years. It arguably already is.
Cell phones come with full email and IM clients these days, and
increasingly they're all-purpose computers, with a rich application
How does it feel to be ephemeral? Are you doing it deliberately?
What do you use IRC for?

@_date: 2009-11-12 11:09:02
@_author: Eugen Leitl 
@_subject: [p2p-research] is the mind a computer 
Noncomputability and undecidability are theoretical constraints
(which are not very relevant in practice), those of computational
physics another. None of these bite (with the notable exception of relativistic latency, though even that is a problem of topology
and feature size which constrain functionality concentration) in practice.

@_date: 2009-11-12 15:07:25
@_author: Eugen Leitl 
@_subject: [p2p-research] Hermit Nation: Does Tech Boost Social Isolation? 
That is impressive.
A good point.
I was referring to your mode of communication, not your person.

@_date: 2009-11-19 17:36:41
@_author: Eugen Leitl 
@_subject: [p2p-research] P2P Carbon trading - stage one 
Consider rooftop flue gas scrubbing and human phosphate/nitrate
regeneration, *and* animal feed (e.g. for fish aquaculture). Win/win/win.
You can yield some 15-19 g of chlorella or scenedesmus/m^2/day at least,
arguably more since flue gas has high CO2 concentrations. At 100 m^2
this is some 1-2 kg of quality biomass/day.

@_date: 2009-11-22 22:02:01
@_author: Eugen Leitl 
@_subject: [p2p-research] How (not) to resolve the energy crisis 
Please cite references for your claim.
That is a repetition of above assertion, not an argument.
How do you enforce that, especially among delinquent nations?
Because there are a few billions of new hopefuls to join the rank
of consumers, chomping at the bit?

@_date: 2009-11-22 23:05:30
@_author: Eugen Leitl 
@_subject: [p2p-research] Who Writes Wikipedia? (blog post by Aaron Swartz) 
There's a huge stink about deletion squads in the German Wikipedia
going on. It increasingly looks like Wikipedia will be forked into
a distributed git-backed depository.
Good riddance.

@_date: 2009-11-25 20:25:36
@_author: Eugen Leitl 
@_subject: [p2p-research] the wikipedia decline 
It was impossible to miss, at least in this particular compartment.
 has been covering it quite extensively. The decline
has started years ago, the deletion wars go several years back.

@_date: 2009-11-27 14:05:51
@_author: Eugen Leitl 
@_subject: [p2p-research] Post-Depression first: Americans get more money 
It is useless to argue technical reasons with people who made an
irrational decision that Email Is Dead (or Old-Fashioned).
They Just Don't Get It. Killfile them, and save your breath.

@_date: 2009-11-28 12:57:03
@_author: Eugen Leitl 
@_subject: [p2p-research] Again on mailing lists, 
Here's a suggestion. Let us all filter each other's emails into
the void or a high-noise folder. After we no longer see each
other's messages, let's just disband the list. How's that for
a plan?
You know, I read Marco and you, but I've filtered the people who
think the form doesn't matter. I'm also annoyed that I'm still getting
their regurgitations by proxy. That doesn't really work. The problem is Turing-complete. The only way to make it work currently is to pay a person to do it for you.
Novel and interesting. Why don't you write a book about it.

@_date: 2009-11-29 12:57:02
@_author: Eugen Leitl 
@_subject: [p2p-research] Post-Depression first: Americans get more money 
Right now Wave is basically unusable. It applies both to the
UI (cluttered), speed (abysmal), openneness (still can't run
the entire package on your own servers with full interoperability).
We'll see how things pan out in the coming years. It is not obvious it is going to die, but not obvious it is going to catch on either.

@_date: 2009-10-04 11:08:32
@_author: Eugen Leitl 
@_subject: [p2p-research] Moving to blogging: Wordpress vs. Movable Type 
Both Drupal and Wordpress are poorely written, and are high-maintenance
security-wise. In case it's not hosted with professionals taking good
care and feeding of it, and you don't want to spend most of your time tracking bugs and patching instead of writing, I would consider something else.

@_date: 2009-10-04 11:11:58
@_author: Eugen Leitl 
@_subject: [p2p-research] Pirate Party program 
There's a window of opportunity right now to help formulate the
goals of the emerging global Pirate Parties. I'm working on the
German Pirate Party platform currently, and I'm asking for your Thank you.

@_date: 2009-10-04 18:25:26
@_author: Eugen Leitl 
@_subject: [p2p-research] Moving to blogging: Wordpress vs. Movable Type 
I presume you can find a decent wiki (I use MoinMoin) blog or CMS package
in the favourite (Python or Common Lisp) secure language of your choice
without having to roll your own.
For the minimalistically minded, especially emacs users there are some
useful solutions, e.g. I presume there's something similiar for vim users as well.
I think such hosting plans exist, and you likely can point your existing
domain to your provider's DNS servers (I certainly offer such, though largely
to friends and friends' friends).

@_date: 2009-10-17 13:08:00
@_author: Eugen Leitl 
@_subject: [p2p-research] New Wi-Fi Direct Gets Peer-to-Peer Connections - 
Anyone can roll own your own internet, or Internet these days.
802.11n mesh with satellite dishes goes a long way, but of course
you can light up fiber with cheap GBICs low-Watt with up to 140 km ranges, though monomode splicing takes equipment in 10 kUSD range and
a semiskilled operator (can be rented or contracted out).
If your budget is much higher (though LEO launch costs are
dropping, and hardware is getting cheaper and smaller), a cloud of 3.5/4 G low LEO microsats or even balloons can easily offer global coverage. Even a single multi-TByte SSD maildrop passing overhead shouldn't be underestimated.

@_date: 2009-09-08 15:50:49
@_author: Eugen Leitl 
@_subject: [p2p-research] never mind policy can filtering technology stop 
Easy. Mandatory ID (via smartcards) to go online, packet traceability (against
mix cascades), encryption escrow or outright outlawed encryption, personalized
watermarks, draconian legislation to enforce above.  If they could mandate end to end encryption with DRM into your retina or cochlea (the last analog hole), they would go for it.

@_date: 2009-09-21 16:37:58
@_author: Eugen Leitl 
@_subject: [p2p-research] good summary of anti-transition hypothesis 
It takes less brain cycles to be wasteful with energy. That's not
a value statement, that's just what empiry tells us.
In general, this is the reason why energy conservation in some
locations doesn't seem to matter: what we save, gets cheerfully
wasted elsewhere. As long as cheap energy is available, and ill
effects postponed we will continue to waste.

@_date: 2010-02-21 20:58:53
@_author: Eugen Leitl 
@_subject: [p2p-research] personal server technology 
I studiouly try to avoid using the cringe-indusing terms cloud computing or Web 2.0 myself.
You did operate a 19" rack in your living space? Wasn't that a bit
on the noisy side? The only semi-silent racks are full of musical
ftp is still de rigeur in hardcore filesharing circles.
Usenet alt.binaries is still seeing lots of use.
That's what high availability and failover are there for. In
case of p2p high availability there's very little complexity.
The problem is putting trust in the service provider, and also
in the availability of your data. It's a no-brainer that your
provider will snoop you out and rat on you, and that there's no
accountability is case of total loss of data, or even transient
unavailability (even it's a commercial service).
Home NAS appliances do all these things for you. You can of course
roll your own, using FreeNAS, OpenFiler, and EON Storage.
Admittedly a decent (=fast) NAS will set you back around 600 EUR sans drives.
But baseline NAS (PogoPlug) starts at around 100 EUR. Slow but usable
NAS is around 300 EUR.
We've got already people trying to evade deep packet inspection and Layer 7
traffic throttline, playing cat and mouse with the cheap providers (the
decent kind charge enough/provide decent data plans so that throttling
is unnecessary).
I think it's a reality for many people already. Home NAS is a booming
market. There will be still the Internet as we know it, and even if I have to
roll my own. I run my own "cloud" (uck, ptui), have my own networks,
my ISP is nothing but a (hostile, untrusted) transport layer.
You can do very decent meshes with Level 3 switches and FTTC/FTTH. The
reason it's not being done is mostly because policy is easier to roll
out to the network core. You've got accounting and trust issues. You can't really sandbox code.
Because there's no p2p online currency (despite BitCoin & Co) there's no
way to cash in on services rendered to the community.
Skype is a corporation, hence not really p2p. Check out Tahoe.
Geeks are always 10-20 years ahead of the mainstream. As Gibson said, the future's here, it's just not evenly distributed yet.

@_date: 2010-02-21 21:01:53
@_author: Eugen Leitl 
@_subject: [p2p-research] personal server technology 
I would strongly suggest so.
Of course, it's a design problem. Difficult, but doable in principle.
A decent system hides the complexity for you.
We already know what the components for such systems are, it's just
nobody has actually built them all, nevermind integrated them into
one working grid.

@_date: 2010-02-21 21:58:03
@_author: Eugen Leitl 
@_subject: [p2p-research] What should a Community do with Profit? (was: 
Does community-owned include the last mile infrastructure? I'm aware
of nonprofit-deployed ISPs, which includes the last mile. But municipally
owned infrastructure appears to be a much better match altogether.
Municipally owned infrastructure makes a lot of sense, actually.
The question is why an ISP needs centralized infrastructure. FTTC
infrastructure is distributed to every street corner, and contains
fiber drops/DSLAM in ventilated metal boxes on every street corner. In principle there's no reason why you could
completely dispense with central infrastructure altogether.
The backbone thing is also a bit of a dated metaphor. If you deploy
GBit Ethernet mesh which is municipally owned and connect this by
intercity fiber there's no centralized backbone. It's a bit like a
federal railway (I realize not many in the US are aware that such
do exist), only much less expensive. Actually, frequently fiber
and rail go together.
Redundant networks (meshes qualify) degrade gracefully, so there's
no need to rush out for repairs.
Again, that one absolutely needs carrier-quality systems in centralized
settings strikes me as oldthink. Of course this is how things are still
done today, mostly, but there's no law why this must remain that way.
Actually, I'm wrong, there's a law, only not of the natural variety.
It's typically about laws of accounting, and traffic management, and lawful interception.
That does currently make sense for current settings, but arguably
embeddeds at the edge of the network can take up that slack. The
only reason for a virtual server is energy efficiency, professional
operators and it being close to the network core/backbone. If
there's no backbone, the thing is highly reliable, zero-admin and redundant/can do failover to the other nodes in the network
virtual servers lose a lot of their lustre.
You can rent height units or complete rack(s) as well, including contents.
It's mostly a marketing problem. There's plenty of VPN products for
end customers these days, due to the ISPs snooping on end users, and
being required by law to keep and release logs on demand.
There's no need to use cloud, p2p is plenty enough. Just define
end user and community-operated nodes, and maybe intermediate infrastructure
as well.
Why, invest it into infrastructure operation, maintenance and upgrade.
Earthworks for one are remarkably expensive, especially a lot of them in
cluttered environment.

@_date: 2010-02-21 22:43:13
@_author: Eugen Leitl 
@_subject: [p2p-research] What should a Community do with Profit? (was: 
I realize that the issue is hard to contain. I must admit I wasn't
even trying to.
Technically,  is a server. Something hitherto people
have been rolling out by themselves, using the DynDNS service and home routers
which double as NAS (exporting USB hard drives and flash sticks on the local LAN) to make home resources available abroad. Poor upstream,
yes. Unreliable, yes. Still, a lot better than no access to home
resources at all.
Of course Pogo is still a commercial service, so what you need is a serverless/
headless P2P grid client running on your local embedded. The poor availability
and low upstream is no longer an issue if multiple copies of your content
are dynamically locally coalesced on demand. Wherever you are. As long
as you run the right client, and authenticate properly.
The topic warrants further exploration here, methinks. Anyone agrees?
We're less interested in a particular piece of hardware than the
service it provides. Even few instances of low-end embeddeds on
unreliable, low-upstream domestic broadband can provide a remarkably
resilient, reliable and performant service. In theory, at least.
The proof of the pudding is in the eating.
The probability of complete outage over multiple geographic locations
and multiple providers is indistinguishable from zero.
If I'm using a service, which relies on multiple locationally and
logically diverse servers then the quality of the individual server
and its connectivity does not nearly matter that much.
Of course this is not what most people are familiar with, and making this foolproof zero-administration is rather nontrivial.
Well, embeddeds do around 10-15 W these days. That's around
10-25 EUR/year, 27/7/365 operation. Virtual servers start at about 10 EUR/month,
and decent ones are 20-30 EUR/month. Don't get me wrong, there's a reason why I'm renting a whole rack
at a major local colo provider. There's a very large difference
between 100 MBit/s Ethernet to the nearest core switch, and 6/100 MBit DOCSIS 3.0 cable modem at the periphery.
VPN is a major requirement from end users, simply because they don't
want their ISP to spy on them. And VPNs are a simple way to avoid ISP throttling, at least for now. Won't last, of course. But the
privacy at least will remain.
The word is a fad. Just as P2P was. In another 5-10 years there
will be new buzzwords. The problem set will be mostly static in
that time frame, however. Nothing we have today is fundamentally different in a 10, 15, even 20 year frame. So everybody will be
on 1 or 10 GBit/s symmetric domestic broadband. IPv6 or similar.
That's what I have on the local loop already. Easy enought to benchmark in the lab, or to model. The future's here already. Very few surprises, if you know where
to look.

@_date: 2010-02-22 23:11:13
@_author: Eugen Leitl 
@_subject: [p2p-research] What should a Community do with Profit? (was: 
Not a nonprofit corporation, a nonprofit community. e.V. in my
jurisdiction, I don't know what your equivalent would be.
Doesn't have to be that way.
Again, not necessarily so.
Local governments (county scale) are typically less corrupt than higher
The only reason there's no full user freedom it's because other laws
prevent them.
Your nonprofit. You can outsource the operation to a lowest bidder
for-profit. But the infrastructure ownership should remain in nonprofit's hand.
In case of a nonprofit, the end users who paid for it are the owners.

@_date: 2010-02-23 08:52:15
@_author: Eugen Leitl 
@_subject: [p2p-research] Request: Peer to Peer and Human Evolution 
Very few activities are truly opaque to the state these days,
unfortunately. The trend is up, way up.
It could be only an economy which operates on the slack that
the state has left. Even so the state so far has been extremely suspicious
about e.g. unregulated monetary transactions, and tended to treat them as if they were criminal. Bits yes, atoms no. Since readily inspectable, and difficult
to hide.
So far there are preciously few such institutions. OpenCA comes
to mind, which is pretty obscure.

@_date: 2010-01-13 13:35:53
@_author: Eugen Leitl 
@_subject: [p2p-research] 2010 will be a hot year in the US 
I agree that Kunstler is well worth listening to, up to a point.
His suggestions are a temporary workaround at best, a dead end at worst and definitely not a solution.
The "network culture" so far could well be just a pretty phrase.
The proof of the pudding etc.
A lot of R&D required is definitely out of reach of small shops and communities. You need both bottom up and top down pulling
in the same direction for optimal effects. Of course we're not
exactly getting that, at least not yet.

@_date: 2010-03-01 13:21:06
@_author: Eugen Leitl 
@_subject: [p2p-research] does this correspond to your experience of 
As a contemporary (and top-posting) youth perhaps you're in no
position to compare notes. Where? Even if, Ylajali Hansen is probably not a native English speaker.
E.g. in German the child is sexless, and hence an 'it'.
