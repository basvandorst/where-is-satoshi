@_author: laurentmt
@_date: 2016-08-04 19:48:15
I like it ! :)
Seems very smart if the idea is technically sound. 
Just one question after a quick first read: the article cites 4 or 5 millions utxos but we currently have around 40 millions utxos. What do I miss ? 
@_date: 2016-08-04 23:24:34
Thanks !
On irc, has just pointed out to me this asymptotic behavior and of course you're both right :)
Moreover, it worths remembering that out of 40M utxos, more than 10M were created during the summer 2015 spam attack (with txs having a high number of outputs). And as you wrote, on average each tx adds far less than 1 utxo.
WRT the "total numbers of outputs ever", if I'm correct, it should be around 420M.
@_date: 2019-07-18 22:54:15
Hi  u/Chayyreo
The last hop of your staggered Ricochet was confirmed around 3:00AM GMT (see transaction  [ As far as I can telll, the output of this transaction was spent by the receiver around 11:12AM GMT (see transaction  [ ).
Hope it helps.
@_date: 2018-09-21 09:31:51


If you share your xpub with all people who need to send you recurring payments, you basically give them access to your full transaction history. A workaround would be to use one xpub per counterparty but it implies that users manage this additional complexity.


 I disagree with this claim that tools like stonewall or stowaway don't add any value. For instance, they make much harder (if not impossible) a direct analysis of the transaction graph by a human analyst and they increase the cost of automated analyses. So far, blockchain analysis has been "efficient" because 2 very simple (and not expensive) heuristics are often enough to get a very good idea of users' activity.
Breaking the assumptions at the core of these heuristics and increasing the cost of blockchain analysis is definitely a win.
Saying that stonewall doesn't add any value is like saying that Joinmarket or Zerolink don't add any value because they can't reach the level of privacy provided by Confidential Transactions or ZCash. It's a matter of degrees.


 I agree with the idea that wallets should support the 3 types of addresses (at least during the transition period) and it's definitely the philosophy of Samourai Wallet which supports the 3 types of addresses.


I get your point and I definitely agree that there's a real need (and a real challenge) in terms of user education. That being said, it seems obvious that we'll never be able to transform all users into "privacy experts with perfect opsec". Thus, improving the default behavior of wallets is another important and complementary factor and this is where tools like StoneWall and Stowaway come into play.


Sure. New ideas and suggestions are always welcome! I send you my email address in PM.
@_date: 2018-09-15 11:25:11
Hi Nopara.
No problem. Actually, I think we agree on all points.
With regards to Stonewall, I can confirm that the idea isn't to create strange transactions but to build transactions having the same properties as multiparties coinjoin txs. I guess your point is that there isn't enough services/tools allowing to build such multiparties coinjoin txs right now. I agree with the sentiment and I'm eager to see more wallets adopting schemes like Stonewall-2P or Stowaway/P2EP. That being said I don't think it's correct to say that no such tool exists. IMO, they're just not enough widely distributed but I'm optimistic that things are going to change with the efforts pushed by wasabi, samourai and many others.
With regards to Boltzmann, I agree that we shouldn't consider metrics like entropy scores or link probabilities as our ultimate goal. Such metrics are useful tools but just some tools among many required for improving the privacy of users. IMHO, these scores don't prove that your privacy is intact but they can hightlight that your privacy has been damaged (coinjoin sudoku proposed by kristov was a perfect example of 
this philosophy).
Practically speaking, on our side, we're used to work with 2 kind of entropies that we call "cold" and "hot" entropy:
- Cold entropy is what you get when you consider the tx in isolation (no additional information used beyond the structure of the tx)
- Hot entropy is what you get when you consider the tx as a part of the whole transaction graph (i.e. when you take into account additional information provided by others parts of the tx graph, by misc. heuristics or by humint).
Current version of Boltzmann allows to support these 2 kind of computations.
It means that hot entropy isn't a static score. It may evolve over time when users make new transactions and it can only decrease (or stays constant at best). 
The entropy score displayed by OXT is a "hot" version relying on the whole tx graph and on heuristics used by the OXT platform.
@_date: 2018-09-03 17:39:01
If I'm correct,  [3PtJRj5x.]( is controlled by Bitstamp (see [
@_date: 2015-12-18 06:22:35
WRT 2) made a very good point in the discussion on github. By adding an input to the transaction, you create another privacy leak (b/o merged inputs heuristic used to cluster addresses). May be worse, this leak is written in the blockchain and it doesn't require an active monitoring of the network. 
WRT 3) Expiring a transaction would require a new type of message sent to the network. Actually, we may consider that RBF does 2 things within a single message: it expires the previous transaction (because miners are likely to prefer the new transaction with higher fees) and its embeds the new transaction. Not so bad.
@_date: 2018-09-20 17:45:50


Charities are indeed a good example but aren't the only one. Another user case for these "static addresses" is recurring payments (rents, salaries, etc).


This comment is interesting in the context of our discussion about Stonewall. :)
I think you'll agree that the certainty of your first analyis has now been replaced by probabilistic assessments ('ample evidence', 'most likely') and this is the whole point of Stonewall ! (more about this below).


Actually, it wasn't but it's not very important and I don't want you to lose your time on this game :)


Agreed. There's a real privacy challenge with periods of transition between different address formats (e.g.: legacy addresses vs segwit addresses).


On my side, this question is still open. I suspect that it may depend on user's activity patterns and that the fragmentation may converge to a limit. This is something that I would like to evaluate in the future.


You get it ! It's 100% the philosophy behind these multiple tools:
\- on one side there's a true multiparties mixing tool (whirpool),
\- on the other side, there's a set of obfuscation tools (stonewall, stonewall-2P, stowaway, ricochet, etc).
The goal of these obfuscation tools isn't to provide the same level of privacy as whirlpool (they don't) but to decrease the damages done to user's privacy and to increase the deniabilty of her transactions when a mix isn't possible or when the user doesn't want to mix (because "I have nothing to hide" ™). In a sense, they play the role of an intermediate level between multiparties mixes and plainly transparent payments. My hope is that at some point, we'll all have an even better option directly included at protocol level (CT, etc).
@_date: 2018-09-19 09:12:35






@_date: 2018-09-19 18:49:33


I fairly disagree about this. From my discussions with multiple bitcoiners during meetups or events, I've noticed that many are asking for the availability of static addresses which can be shared. That sadly lead to massive address reuse in the past, by lack of a proper solution. I think that there's a real need here and having solutions providing this feature while avoiding address reuse is a major win (even if these solutions can be improved).


Agreed. I think it's also the reason why the samourai guys want to test a different trade off.


I must say that you seem to have some good skills at blockchain analysis ! :) 
Unfortunately, several points in your analysis aren't correct and that casts a shadow over your conclusions. According to the rules of the "Stonewall game" I'm not supposed to give you additional information but let's make an exception (just once). Here are a few hints:


This is incorrect.


Things are not as simple as they sometimes appear to be.


I think that I get your point but more than 6 times the dust limit (around 0.2$ if I'm correct) isn't what I would call a near-dust output.


Samourai definitely allows to mix different input types.
Additional (general) tip:
If I'm correct you seem to make the assumption that all the txs in the neighbourhood of these 2 txs are the result of some code from a publicly released version of a software (samourai, joinmarket, etc). But it's a very strong assumption. For instance, it's not unusual that the samourai guys test new features on mainnet before a public release (a kind of pre-alpha stage).  Also, it's worth noting that some txs may have been built manually with the intent to deceive analysts. For instance, Samourai has some features that you can combine to manually build txs having a specific fingerprint (if the available utxos allow it).
@_date: 2018-09-19 07:59:11
Concerning BIP47, the issue you describe is real but it's related to a very specific part of the protocol (the notification transaction). It's also worth noting that the Stealth Addresses model had it own trade offs. For instance, the sender had to include an ephemeral PubKey in an aditional op-return output for each payment transaction. That had 2 downsides: it created payment transactions with a specific fingerprint and it required bigger payment transactions (more expensive).
IMHO, beyond the specific issue of the notification transaction, BIP47 is absolutely a net positive because it addresses a set of needs which have existed for years (e.g.: ability to share a public "address", better management of "refund addresses", etc) and these benefits are greater than the downside of the notification transaction.
FWIW, Samourai Wallet is working on an alternative to this notification transaction (based on different \*trade offs\*).
Concerning Stonewall, I propose you a little game. :)
Here are 2 transactions. One is a Stonewall transaction, the other is a coinjoin with 2 participants. Which one is which ?
@_date: 2015-12-18 07:55:32
2) You're right. It's not different from common transactions. My point is that if you need an additional input to create your RBF transaction then one more address can be added to the cluster. The assumption is that you don't reuse addresses. If you reuse addresses, there is no leak. But address reuse per se, leaks a lot of information about your activity, so it isn't a viable solution.
3) That may work if all nodes enforce this policy ("Remove a transaction 72h after its reception"). The main problem is policies are local and "decided" by node owners. The "beauty" of RBF is that it provides a financial incentive for the enforcement of the policy (and the removal of the first transaction).
@_date: 2015-12-18 02:25:21
Here's another one: UXTOs consumed vs UTXOs created 
- July &amp; August: coinwallet.eu spams the blockchain (and the uxto set)
- September, October, November: people (miners ?) clean the mess
@_date: 2015-12-20 19:09:49
It seems these transactions are associated to a peeling chain organized around this address 
The initial stack was 5kbtc and the peeling chain started with this transaction 
kudos to for having spotted the address.
@_date: 2015-06-20 16:55:34
Naively, I would say that increasing the number of relay nodes supporting the relay network might increase the difficulty of a targeted attack but I'm not sure it would be enough. It might also require that mining pools connect to several relay nodes (not sure it's currently the case). And there could be other drawbacks I'm not aware of.
At best that would be a temporary solution (better than nothing) which doesn't solve the fundamental issue of the redundancy of the 2 networks.
Imho, Gavin's proposal increases the technical (and security) debt of the protocol for the sake of adoption. If things go wrong all we'll have achieved is putting more people in a mess...
It may be possible that a higher blocksize limit is still safe even without fast propagation on the P2P network. I doubt this limit is as high as 8Mb. Finding this sweetspot would require some works (like simulations of the network). But considering how the discussions have turned, I fear that rational studies and arguments are not welcome anymore. That's really sad.
@_date: 2015-12-18 08:39:38
2) The problem is related to the "merged input" heuristic. Basically, this heuristic says that addresses associated to inputs of a transaction are likely to be controlled by a same entity. More inputs with different addresses imply a bigger leak. 
@_date: 2015-12-24 12:57:31
It seems that academics don't agree.


nodes  by  doing  so,  are  an  important  indicator  of  an  ongoing unresolved inconsistency
Source: 
Emphasis is mine.
@_date: 2015-12-24 11:10:23
Food for thoughts: Fee/kb VS Transaction size
Aggregated per block (for 10/2/2015)
Intra block (for block 359121)
TL/DR: Small transactions pay higher fee per kb
Intuition: Many wallets use a same formula to compute the fees to be paid ?
Fee = FeePerKb * Math.ceil(size / 1000)
FeePerKb = FeePerKb parameter defined in the wallet
size = size of the transaction 
@_date: 2018-09-14 22:18:54
Hi lawrence,
This is an excellent question. The philosophy followed by Samourai Wallet in terms of privacy is to rely on 2 different sets of tools/methods:
\- mixing tools (like Whirpool/ZeroLink) which aim to provide "stronger" privacy based on multiple participants and pure mathematics (combinatorics). These transactions are usually easy to spot (they have specific fingerprints) but their strength is really in mathematics, not in being stealthy, so that's ok.
\- "deception" tools (like Stonewall, Stonewall-2P, Stowaway, Ricochet) which aim to improve the deniability of transactions by breaking the assumptions of the multiple heuristics used by analytics tools.
In a sense, this is pretty similar to the difference between cryptography and steganography for secret messages.
With regards to your specific question about Stonewall: for sure, analytics platforms are free to make the hypothesis that transactions with a similar fingerprint are specific to Samourai Wallet and identify a single sender but this hypothesis is already wrong (e.g. here's a stonewall-like coinjoin transaction between 2 users which was created before the inception of Samourai Wallet ! =&gt; [ and things will get worse and worse with the implementation of Stonewall-2P (i.e. a stonewall-like coinjoin tx with 2 participants).
As a side note: Stowaway will apply the same logic for regular transactions (a transaction with 2 participants which looks like a simple payment by a single user).
In conclusion, this isn't really "Stonewall" vs "true coinjoin txs". Both types of tools have different goals and advantages and they should be used in conjunction (as in "use mixing tools from time to time to get a fresh start and then use 'deception tools' for an improved deniability on your daily txs"). And obviously, this isn't the end game. Best privacy will be provided by tools included at protocol level (à la Confidential Transaction) but it might take time before we see that implemented in Bitcoin.
Hope it helps.
@_date: 2015-06-05 20:58:14
Food for thought:  (from this paper 
Then, think about the consequences of 20Mb blocks without a mechanism like IBLT implemented in the P2P network. 
What are the consequences for the relay network (used by a lot of miners) ?
Hints: network resilience, single point of failure
@_date: 2015-06-15 20:13:02
For the sake of finding a consensus, I think it would be great to add a few sentences (in the chapter "A concrete Proposal: BIP 100") explaining the choice of the constants used in the model (period of 3 months, growth capped by a factor of 16 / year, consensus at 90%, drop of 20% low-high votes). 
Just a few sentences explaining why you think they're needed and adequate to sustain the growth of bitcoin and how they protect the security &amp; values of bitcoin.
My 2 satoshis
@_date: 2015-06-19 13:23:55
Imho, if bank aren't interested by the truly disruptive properties of bitcoin (decentralization &amp; censorship resistance), they should better invest in permissioned ledgers (like those proposed by Eris, Hyperledger, ...) which provide the db features without the cost of PoW.
@_date: 2015-06-14 15:36:17




Well, I have several objections here:
- First one is a "detail". My feeling is that 3 months is a too short period. Moreover, I don't think that a majority of users lobbying miners every 3 months is a sustainable solution (see principle of least effort  
- Like Meni, I don't think that interests of miners &amp; users are aligned.
- This kind of off chain feedback loop (like lobbying by email) isn't resistant to sybil attacks.
- It may sound very anti-democratic but I believe that one should be allowed to vote if she has sacrificed something (same principle as miners "burning" electricity).
  Investing a minimum (running a full node) to support the values of bitcoin (decentralization &amp; censorship resistance) seems to me like the minimum required.
  Peter Todd has proposed that users vote with their transactions but I'm not fond of this proposal for the same reason.
  The counterparty of this "elitist" point of view is that barriers to entry (running a full node) should remain as low as possible.
  


I agree that your proposal tries to address the fundamental problem while many others just kick the can down the road.
But conservative approaches, like a new static limit, are also a way to buy more time to come up with a solution which isn't decided in emergency. That's not so bad.
All these points might seem quite negative but, once again, I'm very grateful for this document.
I've spent a lot of time on forums, mailing list to better understand this problem of scalability but I know many people can't spend so much time on these matters. 
Having a document summarizing the context and the issues was much needed and should be very useful to have a consensus. So thanks again !
@_date: 2015-06-20 17:17:14
More full nodes is good but it won't solve the issue of networks redundancy. Anyway... To the moon !  ;D
@_date: 2015-12-18 01:29:43
You're right. Full RBF may leak information about the change address and this information may be collected by services monitoring the bitcoin network. On the bright side, unlike many others privacy leaks, this one isn't written in the blockchain... 
A related discussion on the OBPP's github: 
@_date: 2015-06-18 13:33:40


You're right that Hub&amp;Spoke is different from centralized BUT Hub&amp;Spoke is also very different from a random network.
The fact is that the bitcoin network is an expander graph (random graph) for some good reasons.
There's is bunch of academic litterature explaining why (unlike Hub&amp;Spoke) random graph are resilient to targeted attacks but these matters have very practical consequences (gnutella != bitorrent)
At the end, the only question to answer is: Do you think that censorship resistance is a core value of bitcoin ?
@_date: 2015-06-29 16:41:27
Thanls for the link !
@_date: 2015-06-13 22:21:14
I'm very grateful to Jeff for the great and useful work provided with his document.
That being said, I feel uncomfortable with his current proposal relying on miners, for the same reasons stated by Meni. Users lobbying mining pools by emails every 3 months in order to influence the vote doesn't seem a credible scenario. 
Moreover, it's very likely that the increasing population of people using online services will always ask for bigger blocks since they don't support the cost of a full node ( 
IMHO, if there's a "market" which should decide of the blocksize limit, it's the community of full node owners. That would create an incentive to run full nodes, especially for services who expect more transactions on the blockchain. But we're back to the eternal problem: how to validate that a full node is a full node ?
@_date: 2015-06-18 22:54:09


So true ! :D


I think my bigger disagreement with you comes here.
Censorship resistance isn't something that can be recovered once it has been lost/weakened. If it was the case, the banking system would be resistant to censorship in all our democracies.
Moreover, without decentralization &amp; censorship resistance, bitcoin is a terrible and inefficient solution when compared with others (centralized) systems.
Actually, I would say that the world doesn't lack money (dollar, euro, yen, ...). The world lacks good money.
@_date: 2015-06-20 14:49:07
Any plan to include a solution for fast propagation of blocks on the P2P network with this patch ?
If not, is there an ETA ?
@_date: 2015-06-04 04:55:54
As a complement, two charts related to the evolution of the utxo set (2009 - 2015).
utxos with amount &lt;1BTC (linear and logarithmic scale): 
utxos with amount &gt;1BTC (linear and logarithmic scale): 
@_date: 2015-06-18 13:04:36


Here TotalWork/day is the metric at network level, not node level.
My point was just about Mike's comment related to this metric, not about the metric itself.


k*u*n = k*(n*log(n))*n = k*n²*log(n)
@_date: 2015-06-29 17:11:02
Thanks. I think it's important for everybody to know the technical "prerequisites" associated to each proposal.
I fear that my main "objection" to current proposals is the absence of this new network protocol.
What about the proposal to run a "big blocks" testnet ? For sure, that would require some investments (time, machines, ...) but that seems like a good way (the best we have ?) to detect potential risks associated to different block sizes without the new network protocol.
@_date: 2015-06-14 18:36:45




Thanksfully, I don't see miners/mining pools as evils actors :)
But, like Meni, I think they're (rational) economic actors. Per se, their goal is (should be) to maximize their profits.
Of course, user needs are a factor but we all know that there's many ways allowing to trick this "equilibrium" especially when the matters are highly technical. As a consequence, suspicion remains (e.g. suspicions of collusion of mobile operators in Europe).
Actually I don't see how users can voice their preferences with this model. It seems to me that the model is based on the belief that miners will act for the "greater good of the community". That may be true, that may be wrong. It's a belief.




I get your point and I see how your approach allows monitoring/prediction of trends. Actually, this is something I really like in your proposal.
As a conclusion, I acknowledge that this solution is pragmatic and can be implemented today but I can't convince myself that it's a good economic/political solution on the long term. A quick experiment to summarize that point. Let's replace "miners" by "bitcoin devs" in your previous comment:


Why do you think that miners will do a better job on these matters ? 
Actually, the past few years tell me that the bitcoins devs have done a better job to preserve the core values of bitcoin (decentralization &amp; censorship resistance). This point of view might be biased or erroneous and I would be glad to be convinced otherwise.
@_date: 2015-06-19 18:04:51
Unrelated to our previous discussion. If you have time, I would be interested by your opinion on a point related to Gavin's proposal:
EDIT : I've just found a post written by Rusty Russell after some tests on these matters: 
@_date: 2015-06-29 16:46:16
Yep. I guess the value of k depends on several factors (number of nodes connected, persistence of connections, ...) and varies for different nodes.
I've made the same naive estimation about max bandwidth consumed by bigger blocks.
@_date: 2015-12-20 15:09:08
It may be a coincidence but it seems this activity stopped when the addresses having received these micro payments amounted to 50% of the addresses with a positive balance. 
@_date: 2015-12-18 11:20:24
WRT 3) FWIW, someone found an interesting post from satoshi about 0-conf transactions 
TL/DR: Transactions need confirmations.
EDIT: And a very good post by explaining (far better than me) the subtleties of local policies 
@_date: 2015-06-15 20:42:22
Scaling the blockchain (data blocks) is only one part of the problem.
The scalability of bitcoin has many factors (mempool, utxo set, delay required for validation &amp; propagation vs periodicity of blocks mined, ...) all interconnected. 
There's no free lunch.
@_date: 2015-06-17 22:02:40
Imho, the disagreement comes from the fact that your consider u ( as a constant.
Let's say that we have:






If I'm correct, your hypotheses are:




So far, so good. But is it the end of the story ? 
Bitcoin is still in its infancy and we all hope that adoption (u) is going to increase. Therefore, the question is: what is the relation between u and n ?
If you think that bitcoin should be as decentralized as possible (everybody runs a full node) you get: u = n
which gives us: TotalWork/day = O(k*u*n) = O(k*n²) = O(n²)
If you think that human are lazy animals and that users will always prefer convenient SPV wallets to full nodes (and I tend to agree with this hypothesis :D) you may get something like: u = n log(n) 
which gives us: TotalWork/day = O(k*u*n) = O(k*n*log(n)*n) = O(n²log(n))
@_date: 2015-12-18 02:10:17
Here's one for you =&gt; Fee/kb vs Transaction size
- Aggregated per block: 
- Intra block (block 359121):  &amp; 
Spoiler: Fee = feePerKb * Math.ceil(size / 1000)
@_date: 2015-06-20 15:26:35


I still have the same concern regarding this proposal: higher blocksize limit without fast propagation on the P2P network may introduce a new weakness in the protocol by removing the full redundancy of the relay network &amp; the P2P network (ability of the P2P network to propagate blocks in a short delay). 
For more details see 
@_date: 2015-06-29 16:40:22
Correct. My observations were done on nodes running 24/7 for weeks.
@_date: 2015-06-15 17:43:58
Are you serious ?
- What about the relay network ?
- What about headers first synchronization ?
- What about the works done by Sipa on the secp256k1 library ?
- ...
@_date: 2015-06-14 01:04:13
Hi Here are a few useful resources:
-  : has a list of client or server implementations in several languages and frameworks (like wordpress)
-  : draft of a BIP describing the protocol
-  : a complement to the BIP describing the messages exchanged between client and server (might be useful if you want to develop a bitid library from scratch)
-  : technical thread on bitcointalk (don't hesitate to ping us here if you need some help)
-  : a test server hosted by (uses testnet addresses if I'm correct)
-  : another test server (testnet addresses) hosted by myself
-  : a test server using mainnet addresses
-  : a test demo using bitid as a second factor for 2FA
@_date: 2015-06-15 20:19:51
BTW Congratulations for the new version of statoshi.info ! :)
@_date: 2015-06-06 16:14:48
Don't get me wrong. I don't mean that IBLT (or another solution achieving the same result) won't be implemented. Actually, I'm almost sure that something like this will be implemented in the future.
My "concern" is about an increase of the blocksize limit without IBLT deployed *at the same time*, which is the current solution proposed by Gavin on Github.
I think Gavin has addressed this point by saying he would ask mining pools to slowly increase the size of mined blocks (I don't remember where I've read this. Sorry. Discussions have spread on far too many forums, mailing-lists... :D). 
IMHO, having the security of the system built upon the good faith of pseudonymous participants isn't a good move, even if it's for a temporary period. 
Note that these "concerns" are also shared by some people supporting Gavin's proposal. 
@_date: 2015-12-31 02:54:46


Mmm... I guess I should replace the term "directly controlled" by "owned". The hashpower used to compute this metric is, indeed, owned by the mining operators. Currently, more than 51% of the hashpower is owned by 5 entities (it's not hashpower provided to the pools by individual miners).
Anyway, my reaction after having computed the score for R3 was the same :D
But let's think about it one more time. We like to describe bitcoin miners as pseudonymous actors but the truth is that 51% of hashpower is directly controlled by 5 well-known corporations.
The inauguration of the new Bitfury datacentre is an eye-opener ( 185,000 sq.m located in a Technology Park (Tbilisi, Georgia) with an opening featuring the Georgian Prime Minister.
Don't get me wrong, my intent isn't to throw stones at Bitfury (I guess the 4 others mining pools are also well-known actors).  I want to believe that big mining operators are still independent economic actors but I think it's also important to face the reality: almost all exchanges apply some KYC/AML procedures because they're required to by their local regulator or because they don't want to take the risk to be sued in the future.
IMHO, with such a level of centralization, it's just a just a matter of time before a part of the mining ecosystem is regulated / influenced / coerced by state entities.
As bitcoin believers, our natural answer is to say that economic incentive wills prevent this situation but the truth is that bitcoin doesn't live in a vacuum. It exists in a political world. And political decisions aren't all motivated by economic factors. If it was the case, the history of national debts would be very different ;)
Moreover, forcing bitcoin to follow a dead-end isn't a problem for governments. They already have alternative financial solutions. The death of bitcoin wouldn't be a big loss for them.
As a conclusion: 
It's a good point that a metric like this one doesn't capture the current state of relationships between the validators and governments. I guess this kind of characterization would require in-depth analysis and I'm not sure it can be easily summarized in a single metric.
My intent with this metric is less ambitious. It should serve as a tool for comparisons with others systems.
The rationale for this metric is very simple. It's basically an answer to the question: "how many doors do you have to knock in order to coerce a majority of the system ?"
For example, if we consider that a system like R3 (score=22) isn't decentralized, how can we be confident that a system with a lower score can remain decentralized in the long run ?
@_date: 2015-06-18 17:07:14
Great post about big O notation ! 
Anyway, the true question remains: Do we think that censorship resistance is a core value of bitcoin ?
If the answer is 'Yes', we must acknowledge that network topology is of primary importance. And in these matters, hub&amp;spoke networks are very different from expander graph (current topology of bitcoin network). They're far less resilient to targeted attacks.
To make things a bit more difficult, we must also acknowledge that the subject isn't just technical (topology of the IT network) but is also human. Even with an expander graph, the bitcoin network could be centralized if nodes are run by a small number of entities with converging interests. 
This is why keeping the "cost" of running a full node as low as possible is an important factor in order to preserve the current topology (at human and infrastructure levels). 
It doesn't mean that we should stop the growth of Bitcoin for the sake of running full nodes. It means that we have to find the best compromises all along the way.
As a conclusion: the point isn't to know if an IT system can process word scale transactions but is "Do you think that censorship resistance is a core value of bitcoin ?"
@_date: 2018-09-19 08:43:04
You're absolutely right that a scheme like BIP47 isn't a magical solution solving all privacy problems. Its goal is to address some specific issues/needs which have existed for years, like:
\- being able to share a static public "address" without systematically leaking all your past transactional history,
\- having a scheme allowing a better management of refund addresses (this one is a real PITA for exchanges and payment processors).
In a nutshell, BIP47 is just a part of a better solution and the main benefits are for the receivers of transactions.
@_date: 2015-06-19 14:16:49
Considered alone, PoW is not inefficient but expensive. Nothing surprising here, hashcash and PoW were designed with this goal in mind.
PoW is inefficient if, for a given level of trust, its cost is higher than the cost of other solutions. Imho, the cost is justified if bitcoin keeps its core values (decentralized and censorship resistant). Without them, I'm not so sure.
@_date: 2015-12-18 20:56:12


@_date: 2015-06-15 17:30:54
FWIW, the average transaction size isn't 250b but is closer to 500-600b...
@_date: 2014-05-14 14:24:53
Eric has integrated BitId in an android wallet for test purpose.
You can find it here : 
Next step is convincing wallet developers to integrate the protocol in their product. :)
@_date: 2014-07-24 18:32:50
Well, I've just done that for fun. Everybody is welcome to have his own interpretation. Mine would be :
- Core devs : "Bitcoin is an experiment"
- Alt devs : "We can do better than that very quickly"
- VCs : "Interesting. Let's fund some startups building things on top of that and let's get some good money"
- Early adopters: "To da moon..."
- Traders : "Woohoo ! Such a rollercoaster..."
- Paul Krugman : "This thing is broken by design. It will never work"
- Bankers : "This tech seems interesting but first, let's get rid of all these jerks and anarchists"
- Ben Lawsky : "I don't like trees" ;)
- Unbanked : "Bitcoin ? What the hell are you talking about ?"
@_date: 2014-07-19 20:17:10
BTC/Fiat is a real subject but it's not my point. The draft document states that a european (russian, chinese, ...) startup with NY customers must have a NY Bitlicense but is not allowed to invest its earnings in its local legal currency (EUR, RUB, RMB, ...). I'm sure you'll agree that it's a quite funny proposal for a global market. 
@_date: 2014-07-24 18:38:49
WRT Ben Lawsky, my interpretation would be : "I'm ok with bitcoin if you remove this and this and this..."
@_date: 2014-08-11 18:57:11
Anonymity (or at least privacy) is a real subject. A paper, published in 2014, shows how it's possible to deanonymize users by "attacking" the network and blocking TOR users (
In spite of all things written by media, Bitcoin (in its first releases) offered a very weak financial privacy, which is a true problem for consumers security.
Works done by teams like unsystem with darkwallet are very important for all of us and shouldn't be considered only as tools for dark activities. It's much more than this.
WRT a connection to a single trusted node, I think we can consider SPV wallets (electrum, ...) as a good example of this model.
@_date: 2014-06-07 17:55:17
That's a good question and I fear that many people have too much confidence in 2FA. 
It was proposed as a counter-measure to phishing attacks. Basically, a phishing attack is a fake website looking like the original. Very often, you receive an email with a link to access the website and a message which invites you to do something on the website. You access the fake website and input your login/password. Usually, the attacker stores these info and use them later to connect to the original website and do bad things...
Here comes 2FA. Basically, it's an ephemeral token required to authenticate against the website. It's validity is limited to a small time frame (like 30 seconds). It means that even if the attacker gets your credentials and your token, the latter will be refused when used later.
It didn't take a long time to hackers to figure a bypass which is called "real-time phishing". As stated by its name, it's phishing done in real-time. Your credential and your token are used by the attacker as soon as you have input them on the fake website (meaning inside the validity time-frame). When you notice that there's a problem, it's already too late.
Online wallets like blockchain.info are the target of many and many phishing attacks, which let met think that RTPhishing has become a very popular attack vector.
The best protections are: 
- never click a link provided by an email, forum, tweet, ... and which is supposed to bring you to your wallet/exchange
- check the url in the address bar (but sometimes it can be tricky to notice a small difference in the url)
- check if the provided certificate seems legit (for https connections)
[EDIT] An interesting technical article explaining how this kind of attack works 
@_date: 2014-07-19 20:32:01
Well. I see 3 options:
- I missed the fact that NYDFS has a global juridiction,
- NYDFS wants to ban non-american startups from NY market,
- It's an unfortunate mistake and I hope Ben Lawsky has a good sense of humor or some collaborators at NYDFS are about to be fired.
@_date: 2014-07-24 18:49:48
That would be very funny ! :D 
@_date: 2014-07-18 16:26:02
Yep ! Regulation as proposed in the draft document is just an attempt to rebuild the current banking system on top of the blockchain, denying the main innovation supported by bitcoin (a system not relying on trusted 3rd party).
I don't have any problem with regulation of exchanges but regulation of online wallets, as envisioned in the draft document, is a real concern.
Imho, there's only one good solution for the community : proposing better solutions for secured and usable private wallets and get rid of "bitcoin banks". 
@_date: 2014-12-13 15:24:45
Attended a presentation of it, two days ago. They have a clear and smart vision. Very exciting things to come.
Regarding open source/ open specs :
My understanding is that they have some NDA which don't allow them to open source their code for now. The idea of the open specs is that the community can check the wallet behaves as described in the specs. It requires that independent devs/auditors develop these tests. Not sure if it will be done but it's definitely possible.
@_date: 2014-07-19 21:01:23
100% agree. Regulating cryptocurrencies will be tough because they question the statu quo. During a meetup, we had a great talk by a judge, expert in financial crimes, explaining that the main problem with bitcoin is not money laundering but the fact that it challenges all we thought to know about money and financial regulation.
@_date: 2014-05-23 21:00:19
It reminds me a video with an old woman facing an ipad and this brilliant statement: "It's a mirror for our thoughts". The most beautiful definition of computers I've ever heard.
(French speaking video)
@_date: 2014-10-28 21:31:02
Blockr.io : 
REST API ( can also be used by calling  instead of 
@_date: 2014-07-19 17:08:21
Yep. This one is really funny. European startups which would like to be licensed are forbidden to hold their earnings in Euro (our legal currency). :))
@_date: 2014-07-01 23:52:11
IMHO, a more important difference: SIN is about identity. BitId is about authentication of a key pair and allows anonymous authentication if you want it but could also support SIN.
Totally agree that there's nothing new here. All people using Lotus Notes in the 90's were already doing that.
a NKOTB ;)
@_date: 2014-05-27 16:54:01
You're right to say that pubkey auth is not a new thing. I was already using systems with that kind of security 20 years ago. As some others wrote, there's also SQRL which proposes a model similar to BitId. 
Anyway, I could be wrong but I like the implementation proposed by Eric. Here are the main reasons:
- I'm convinced that we, as persons, need more privacy in our everyday digital life. I guess many core devs work on this inside the bitcoin protocol but I think it needs to be extended on a larger perimeter (see the "Motivation" chapter here 
- SSL, X509 and others certificates-based solutions rely on centralized certificate authorities. In the spirit of Bitcoin, we need more decentralized solutions, less prone to privacy loss.
- SQRL is quite similar to BitId and has some good feedbacks. If it gains good traction, it would seem logical to invest in this solution. On my side, I like SQRL but I think it has a flaw: it does not address the spam problem. Nothing prevents a rogue player flooding a website with millions of fake accounts. Of course we can imagine that signup requires a captcha (or anything else) but it sounds like a bad patch, degrading the UX.
With BitId, we can address the spam problem in smarter and innovative ways, better integrated with the bitcoin ecosystem:
  - We can integrate BitId with the payment protocol. Doing so, a signup is validated if and only if it's associated to a payment.
  - We can use addresses used for payments (outside of BIP70) for signups with BitId. Doing so, server can check that he has received a payment from this address before validating the signup. I'm not fond of this solution because it has many constraints but it could work.
  - We can imagine many others models (some proposed a proof of stake to fight trolling in forums, ...)
  - BitId is built upon the same crypto stack as the bitcoin protocol. It makes developers' life easier (less things to maintain, less bugs, ...)
@_date: 2014-10-21 15:11:07
What is your position on the current state of financial privacy in bitcoin ? 
- Perfect
- Good enough
- Can do better
- Can do much better
- "Houston we have a problem"
- ...
@_date: 2014-09-15 21:15:07
I agree with still_unregistered. Transfer of private keys is very insecure for your users and for your business. The natural place for private keys is a secure place (vaults, wallets, ...).
@_date: 2014-10-31 18:19:56
 : did you keep a link to this discussion ? 
@_date: 2014-09-14 18:10:20
Hi ISkiAtAlta,
WRT BitId, a few wallets start to support it:
- DarkWallet alpha 5 (
- onchain.io (
- Mycellium (limited to testnet for now since they're testing it with HD)
Hope it helps
@_date: 2014-07-24 19:22:02
By the way, if you haven't already seen it, here's a real painting made by a real artist : 
It's in the same spirit (the different components of a whole community)
@_date: 2014-07-19 21:05:41
Yep. French bank BNP Paribas discovered that point with a huge fine ($9b) but requiring startups located in others countries to invest in dollars seems a bit...abusive.
@_date: 2014-07-24 18:44:55
Yep! Original comic was a very funny one about software projects but the message is the same: so many participants with so many different visions of a unique process/system.
By the way, if someone knows who is the author of the original cartoon, please tell me. He deserves credits for the idea.
@_date: 2014-07-24 22:09:40
Excellent ! Do you know when it was done ?
@_date: 2015-05-10 16:54:31
My bad... Typo on my side. Please read January 2015.
An interesting post on this subject: 
@_date: 2014-09-16 10:43:20
The white paper contains a strange comment :
"While the blockchain may carry regulatory and economic risk as a long-term store of value (as in the case of Bitcoin), it can be quite revolutionary as a transaction processing tool"
Blockchain and PoW "solve" the Byzantine Generals problem by providing financial incentives to stay honest. Thus, it implies that the blockchain is a store of value. If you remove the incentives (store of value) the system collapses. I can't figure out how they plan to run their system on top of a blockchain without coping with the need to be a store of value.
Anyway, it's a really exciting news.
Thanks for the links.
@_date: 2014-07-19 23:34:55
He is vice-president of "Tribunal de Grande Instance" in Paris, France. Unfortunately, the talk is in french but if someone fluent in english &amp; french is able to translate it, it would be great !
To give you an idea: he talks about the history of money, explains why the true problem with bitcoin is a shift of paradigm and concludes his talk by saying that he's seriously thinking to create a bitcoin startup and that he's looking for developers ! :)
part 1: 
part 2: 
@_date: 2015-05-31 12:48:26


I agree with you on that point !
@_date: 2015-05-04 20:31:50
Do we have some forecasts about the consequences of this modification on the size of the utxo set ?
@_date: 2015-05-30 15:56:53
A few more data:
- I call "Test period" the 8 hrs between 00:56 and 09:00, based on this chart of the mempool  (from statoshi.info)
- 54 blocks were mined during the 8hrs preceding the test (16:56 - 00:56)
- 78 blocks were mined during the 8hrs after the test (09:00 - 17:00).
Once again, it might be a coincidence but the coincidence is intriguing.
@_date: 2015-05-27 20:34:38
Ok. Thanks !
My main "concern" with this simulation is the fact that patterns of activity (and the morphology of transactions) has changed since block 300,000 (May 2014).
For example, the chart of average  ( suggests that we have entered a new "phase" since May 2014 (with an increase in the size of the utxo set).
FWIW, here's an attempt for a high level analysis of these new patterns (
TBH, I'd feel more confortable with a simulation which keeps the same blockchain from block 1 to block 300,000 and simulates several months of 20Mb blocks based on blocks &gt; 300,000.
A simulation extrapolating an increase of some activities (like faucets...) might be even better.
My 2 satoshis.
@_date: 2014-08-05 00:29:14
It's technically impossible. The wallet embeds the challenge in an "envelop" and signs the result. It can't be exploited to make you sign a transaction.
@_date: 2015-05-30 14:46:06
Another point:
- 13 blocks were mined between 23:38 and 03:29 (instead of theoretical 24 blocks)
- on a larger scale, 31 blocks were mined between 00:56 and 09:00 (instead of theoretical 48 blocks)
I see 2 possibilities:
- these results are caused by variance in mining and it's just a strange coincidence.
- beyond the blocksize limit, there are others factors limiting scalability.
@_date: 2015-05-31 10:58:21
I agree with you about all these points.
And anyway, with this unique observation we should assume the null hypothesis (variance in mining). 
But this kind of coincidence always activate the associative part of my brain :D
EDIT: FWIW, I've done a chart ( of  received in the intervals [-30mn, + 30mn] around the reception of a block. So, I'm going to share it with everybody. Note that the central zone is the stress test period. 
@_date: 2015-05-31 11:46:29
You may be right but let's try to analyze the other part of the story.
If keeping the 1Mb blocksize limit destroys bitcoin next year, it's likely that it will be a big issue for the BitSats project because BitSats doesn't make sense if bitcoin is destroyed by the 1Mb limit.
Therefore, it seems to me that Jeff Garzik has a strong incentive to chose what he thinks to be the best option for bitcoin and BitSats. It doesn't mean that it's the "right" choice but it should be an honest choice...
Edit: replaced "has no sense" by "doesn't make sense" 
@_date: 2015-05-10 15:14:37
Actually, I've thought to a very similar idea but with Pow being computed by the user (users with smartphones may use services provided by full nodes to get their Pow computed).
While the idea isn't stupid, I think it has 2 major flaws:
- it seems much more complicated than a pure solution based on fees paid to miners
- it subsidizes the energy industry and not the bitcoin ecosystem. A solution based on fees subsidizes the ecosystem (miners).
@_date: 2015-05-31 01:14:47
Describing the situation as "Gavin listens to the community while the others devs act like autistic guys" seems unfair...
Here's the conclusion of a message sent by Gavin on the dev mailing list: "Because if we can't come to consensus here, the ultimate authority for determining consensus is what code the majority of merchants and exchanges and miners are running."  (
Then think about these 3 points:
Do you see any reference to the choice of the community (SPV wallets users, full nodes owners) ? 
Why is there no reference to these end users ?
What does it say about the role of technical decentralization for "democracy" in bitcoin ?
Hint: I don't suspect Gavin (or any core dev) to be an awful dictator ;)
@_date: 2015-05-27 16:24:04
WRT to these tests ( what is the last block used in order to fill the 20Mb blocks (300k, 333,734...) ? Thks !
@_date: 2015-01-20 17:26:18
Actually, this is what BitId does :)
See the demo video: 
@_date: 2014-08-05 00:09:50
BitId has already been implemented in:
- DarkWallet alpha 5 (
- Mycelium 1.2.15 (
Server side, a few plugins have already been developed for Django, Wordpress, mediawiki, ...
So it's definitely possible to play with BitId !
@_date: 2015-01-03 17:08:40
It was not Hashcoins but another company.
See this thread on BTC Talk for more details (french subforum) : 
@_date: 2015-01-02 22:36:50
Actually, it's one of the use-cases envisioned for the BitId protocol (
There's also a draft for the integration of BitId with BIP70 which would be quite adapted to your scenarii (
@_date: 2016-03-16 18:00:02
WRT double spends, please see my other comment here: 
WRT the status of softfork I agree with you. 
I may be wrong but I think the understanding of the nature of soft/hard forks has changed a lot lately (with the discovery of what can be done with softforks). I don't see any malice here. It's just that everybody is still learning things about the system.
I don't think that anybody is now advocating "its a softfork, dont worry". One may says that softforks are less "radical" than hardforks in terms of deployment and, imho, it's true. 
But I totally agree that now, we know that consequences of soft forks can be radical. So yes, their content matters !
@_date: 2015-01-22 22:09:34
Very good work. Clear &amp; concise. Thanks !
@_date: 2016-01-16 21:06:52
Very simple. 
This code was used to hack cryptsy, a "centralized" exchange living at the edges of the bitcoin network. It's in no way a hack of bitcoin (the protocol), it's just the hack of a server connected to internet.
If your friend doesn't know the difference between hacking the bitcoin protocol and hacking an exchange, he has just demonstrated that he doesn't know how bitcoin works and that he may have a few thing to learn from you.
Done ! ;D
@_date: 2016-03-16 18:40:06
Actually, the answer can be found in the BIP (BIP141 / chapter Examples 
Segwit has 3 possible forms:
- P2WPKH witness program =&gt; -3 bytes compared to a P2PKH tx)
- P2WSH witness program =&gt; +11 bytes compared to a P2SH tx but improves the security of the transaction)
- Witness program nested in P2SH =&gt; +35 bytes
From my understanding "Witness programs nested in P2SH" has been proposed but is not intended to be largely used. The main form promoted should be "P2WSH witness programs" which improves the security of bitcoin...
WRT space consumed, I think focus on the wrong factor (disk space consumed) but forgets another important resource consumed: the bandwidth. Thanks to segwit, a full node doesn't have to transfer the witness to lightweight clients which don't check signatures. And lightweight clients don't have to download these data. As explained by P.Wuille in his presentation of segwit, signatures are a large part of the data composing a transaction. So, yes, there's a real gain and a real improvement here.
@_date: 2015-01-20 15:53:52
I fear there's a misunderstanding about what bitid is or allows (the name is surely one of the causes).
Using BitId to manage an online identity is a use case but isn't the only one. Signing with an address previously used for a payment is also a use case (but not the only one).
IMHO, one of the most exciting use cases allowed by BitId provides better privacy by removing the concept of identity from e-commerce transactions (see this draft:  
@_date: 2015-05-30 18:37:49
Yep. The number of transactions has no impact on hashing &amp; difficulty. But it may have an impact on the time required for validation of transactions by mining pool operators.
I guess that most mining pools use custom softwares. How do these softwares react to an important increase in the number of transactions ?
It would be interesting to get some insights from experts in mining...
@_date: 2016-01-20 12:08:02
Variant: 
@_date: 2016-03-17 11:41:21
This comment was related to propositions made in order to mitigate the effects of the great firewall of China. The initial observation was that the GFC has a noticeable impact on propagation of blocks. Therefore, it was suggested that chinese mining pools should run their full nodes in data centers outside of China.
There's 2 problems with this idea:
- as noted by several chinese actors, defining which side of the firewall is the "right one" and should host the full nodes is very subjective :D
- as noted by Nick Szabo, suggesting that miners &amp; pools without strong control over their full nodes is the way to go is problematic, especially when suggestion is done by engineers working on bitcoin.
@_date: 2016-01-07 18:30:23
I just checked the fee estimated by my bitcoind. 
0.0035 BTC/kb should allow your tx to be confirmed in the next block.
0.00005 BTC/kb should allow your tx to be confirmed in 25 blocks (4 hours on average). 
I fear that minimum fees (0.00001BTC/kb) won't be enough to get a "fast" confirmation.
@_date: 2015-05-10 19:16:18
Actually it can be checked easily by anyone thanks to the stats from bc.i
You just need to check the number of txs and the increase in blockchain size for a given period. Then, you just divide the former by the latter to get the avg size of a tx.
@_date: 2016-03-16 17:52:25
Hi I can imagine one case for this kind of double spend: a old node accepting 0-conf txs which would accept a tx with "segwited" inputs. 
Anyway, I expect that anyone accepting 0-conf uses an up-to-date dedicated service detecting double-spend attempts or is actively developing its own system. As a general rule, I would say that it's a very bad idea to accept a 0-conf tx if you can't fully validate its content.
WRT confirmed txs, they'll be protected by the switchover mechanism (see chapter deployment in 
There may be more subtle attacks allowed by segwit but I fear that you'll have to find a real expert ;)
FWIW, I'm almost sure that you'll reconsider your position about segwit if you take more time to understand the cons&amp;pros and there's 2 reasons:
- My first reaction about segwit was the same the first time I've heard about it. But I've changed my mind after having dug deeper onto the subject.
- I've followed some of your works on supernet and I can't believe that you may be afraid by the implementation of segwit... :D
My 2 satoshis
@_date: 2016-03-16 19:09:27


Well, I think the magic lies in the fact that the classic scriptpubkey ("OP_DUP OP_HASH160 ... OP_EQUALVERIFY OP_CHECKSIG") is replaced by a 20 bytes hash ("0 &lt;20-byte-hash&gt;") with "implicit detection" of the P2WPKH form.
The '0' in scriptPubKey indicates the following push is a version 0 witness program. The length of the witness program indicates that it is a P2WPKH type. The witness must consist of exactly 2 items. The HASH160 of the pubkey in witness must match the witness program. 
(from BIP141)
@_date: 2016-03-16 11:00:39
No. It means that Segwit wallets will send funds to non-Segwit wallets thanks to usual scripts (P2PKH, P2SH..) managed by the non-Segwit wallet.
One has to remember that the payee is the only one defining how he wants to be paid. It wouldn't make sense to ask for a segwit payment if your wallet doesn't manage Segwit (like it doesn't make sense to ask for a payment on a P2SH multisigs script if your wallet doesn't manage P2SH multisigs...)
@_date: 2016-03-26 19:43:02
Not sure but that may be  or 
@_date: 2014-07-18 16:34:14
It is enforceable. In its current state, the draft document proposes to rebuild the current banking system on top of the blockchain, denying the main innovation of bitcoin (getting rid of trusted 3rd party).
With this kind of model, you would be able to send/receive coins only if the other person is also registered in the system (by the same or another licensed company). It would be like having another currency inside bitcoin. 
@_date: 2016-03-16 19:26:08


The main improvement in terms of security is about replacing HASH160 by SHA256 (see 
I don't know if we can call it a peer review but the subject was "challenged" and discussed on the mailing list (see  with some kind of agreement about the fact that it's a good idea (see 
@_date: 2016-03-16 11:20:09
Yes. Validation is the main issue for old nodes because they will consider a "segwit txo" as always spendable. It's somewhat similar to the situation of old nodes when P2SH was introduced.
@_date: 2019-03-06 23:46:21
I like it :)
Just a suggestion for the "possibliy coinjoin transactions". Their detection seems to deactivate others privacy notes like the detection of change outputs which are deterministically linked with some inputs. IMHO, it would be a good idea to display this note when such change outputs exist (in addition to the coinjoin detection).
E.g.: [
@_date: 2015-05-30 19:57:07
While I almost agree with your definition of the "opposite camps", I think you should consider others factors which could explain this opposition.
Several members of blockstream have a background in cryptography and in the cypherpunks group.
I'm sure you know that decentralized money was an old dream among cypherpunks and that current tech was built thanks to this dream and works done by cypherpunks.
My take is that people at blockstream share a common vision for a bitcoin network remaining as decentralized as possible (following the vision initiated by the cypherpunks).
On the other hand, I guess that Gavin Andresen and Mike Hearn are closer to startups entrepreneurs who expect a quick increase of adoption (i.e. increase of activity in the blockchain) and are less concerned by centralization.
The good news: I think they all share a same concern "Avoiding to break bitcoin" and they all agree that increasing the blocksize will have to be done.
The bad news: they don't agree on what could break bitcoin. Blockstream guys seem concerned by an blocksize increase done without enough preparation while Gavin and Mike seem more concerned by what could happen if the 1Mb limit is reached next year.
The very bad news: they don't seem able to have a proper discussion relying upon rational arguments and technical analysis.
So, I would say that financial motivations may be a reason but "ideologies" and characters are also strong motivators. 
My 2 satoshis (from a guy living on the old continent and sometimes wondering what his american cousins are f... doing ;).
@_date: 2016-03-16 19:32:46
WRT to the emphasis on hdd space consumed you may be right that segwit isn't the best solution here. Actually, I would say that if segwit was only about "scalability", I'm not sure I would support it (vs a basic block size increase). But beyond a basic increase of the capacity, segwit comes with many additional benefits which are important for the future of bitcoin. IMHO, that changes everything (but it's just the opinion of a random guy on reddit ;)
@_date: 2016-03-23 09:21:18
You may be interested by these 4 charts provided by OXT ( + select tab "FEES"): total fees, fees/tx, fees/kb and the ratio fee/block reward. You can drill-down and see the metrics for one month or one day.
These metrics can also be compared to additional metrics with scatterplots (
@_date: 2015-05-10 15:19:58
In January ~~2014~~2015, the average size of a transaction was around 500 bytes.
EDIT: Fixed typo
@_date: 2016-03-16 10:39:40
There's an obvious problem at the very beginning of the post
If a bunch of users end up with unspendable funds, then I guess I will  be forced to massively complicate the blockchain handling.
As a user, you can't end up with unspendable funds because of segwit. Segwit will be used by the payer if the payee has provided a segwit address which isn't possible if payee's wallet doesn't manage segwit...
@_date: 2017-03-11 02:03:48
I confirm that the first multisig utxo (1-of-2) was created on January 2012 =&gt; 
@_date: 2016-03-16 18:49:09
My answer would be that in addition to an increased number of txs per block, segwit brings a lots of benefits which are important for bitcoin (tx malleability, improved security, introduction of an incentive to "control" the growth of the utxo set, ...). 
More details here: 
@_date: 2016-01-20 16:26:33
@_date: 2014-07-22 18:06:40
You should use addresses displayed in "Receive" tab to receive coins and only these addresses.
"Details" popup displays all inputs and outputs of a previous transaction. If a transaction has several outputs with only one for you (transactions sent by exchanges, shared wallets, ...) they're all displayed. So, you shouldn't use this popup to select a receiving address. Never !
@_date: 2016-03-16 20:30:42
TBH, I'm not very interested by the marketing side of this "debate" (mostly because I'm a dev and have no experience in marketing). Anyway, it you look closely at the "marketing" page of Segwit ( you'll notice that it isn't presented as a scalability solution but as a solution allowing to increase the *capacity*.
In my understanding, it was made clear that a natural future step is to improve the scalability with works done on the P2P protocol and on propagation of blocks (IBLT, thin blocks or whatever...).
IMHO, it has been a long controversy (more than 12 months ?) and at this point (I would say since a few months) discussions aren't anymore on technical aspects of the solutions but are purely about communication and politics. But I guess its just another side of bitcoin social consensus...
@_date: 2015-05-10 19:07:30
No need to change your OP. My comment was just to highlight that  is even lower than theoretical 7tx/s. But that doesn't invalidate what you wrote (orders of magnitude).
@_date: 2016-03-16 11:55:37
I wouldn't define myself as an expert (far from that) but I'm going to give it a try.
WRT "wtxid wasting precious space in the blockchain": From my understanding, a new id (wtxid) associated to each transaction is used to compute a merkle tree and the only thing stored in the blockchain is the root of this merkle tree stored in the coinbase transaction. "Wasted" space will be around 38 bytes (per block). It doesn't seem dramatic... But may be I've missed something.
WRT Segwit not being backwards compatible: I think it's wrong. The misunderstanding seems caused by the belief that non-segwit wallets might receive "segwit payments".
WRT to Segwit not being a softfork: this point is more interesting. Technically, it's wrong. Segwit is a softfork. But it raises an interesting point about the nature of soft forks and hard forks and more importantly about what can be achieved with a soft fork. I think most of the developers agree now that a lot of things may be implemented with soft forks (even very contentious modifications). 
@_date: 2015-05-27 21:23:42
Fair answer ;)
@_date: 2017-01-13 03:51:12
The spam attack which occurred during summer 2015
@_date: 2017-01-13 20:05:44
Here's something for you :)
This chart seems to shows the ratio (avg  removed/created per blocks in a size interval) is pretty constant (around 0.6).
The anomaly between 750-900kB (related to the spam attack) shows how this ratio plunges to lower values (around 0.25) when the system is "abused" or pushed to its limits.
Segwit aims to counterbalance this negative incentive favoring the "attacker".
@_date: 2017-01-13 04:05:32
@_date: 2017-08-24 14:53:31
 has for 4 charts dedicated to segwit:
- number of segwit transactions
- virtual size / transaction
- fees / vbyte
- types of utxos spent in P2WSH
(OXT - Segwit ready since January 2017 ;)
@_date: 2017-01-13 04:09:01
The horizontal cluster is related to the spam attack which occurred during summer 2015
@_date: 2017-01-13 15:14:31
Then, you might be interested by [OXT Landscapes](
Navigation with the "fly controller" may be a bit tricky at first if you're not a gamer but it becomes fun when you're accustomed to it. I was mostly developed for the fun but it can also be used to observe/detect some behaviors at a macro level.
[demo video](
@_date: 2017-01-13 23:41:42


Yes. They are accounted for the creation side.
@_date: 2017-08-27 21:12:55
This is an interesting thought experiment. Some times ago, I've played with a different (but somewhat close) idea of an attacker leveraging her attack by spamming services with dust outputs forcing these services (or their users) to fund a part of the attack by having them paying the fees for transactions gathering the dust outputs : 
If on any use, here's a chart for the distribution of utxos per amounts  (desktop only)
@_date: 2017-01-31 19:39:18
Good job ! Here's the entity that you've identified as potential source of these transactions with hundred of inputs : 
And here's a screenshot displaying the weekly pattern of activity on January: 
As written by others in the thread, it might be a service aggregating utxos and sending them to a cold wallet. The most "depressing" is that they don't do it on sunday when network activity is low. That would allow them to save on fees paid to the miners. I see no good reason foir doing that during the week...
@_date: 2017-01-13 15:31:53
You're absolutely right. It should be "removed/added".
Incentives in the fee model are broken because they don't take into account a resource used by the bitcoin protocol: the utxo set. 
Worse, the current model encourages abuses of this resource because it's about 4 times cheaper to add an utxo to the set than to remove it. Therefore, it creates an asymmetry of costs giving an advantage to the attacker.
This isn't just a theoretical idea. It happened with the spam attack of summer 2015. More than 10M of utxos were added to the set in a few days. Since then, we're living with these utxos because the cost of cleaning the set is high.
@_date: 2017-01-24 21:21:20
BIP42 is the best BIP ever :D
@_date: 2017-01-13 16:42:34
Agreed. 3D tools seem exciting at first but are often hard to use for a (serious) practical analysis. 
@_date: 2016-11-29 22:27:56
A study case from late 2014 
(sorry for the crappy english :D)
Edit : see last post for a TL/DR and conclusion
@_date: 2017-08-24 11:45:34
 has for 4 charts dedicated to segwit:
- number of segwit transactions
- virtual size / transaction
- fees / vbyte
- types of utxos spent in P2WSH
@_date: 2018-06-18 15:02:58
I don't know the details of your specific problem but the fact that you were able to reproduce the problem twice doesn't mean that the problem isn't related to a rare or specific condition. For example, I've scripts which have run flawlessly for years and one day a bug is triggered multiple times in a raw.
As a dev myself, all I can say is that yelling to a dev is rarely the best way to have your issue fixed quickly, especially when this dev is working for free. 
As wrote, you may try a non free wallet but I'm not even sure that you would get a better service (i.e. a real fix) during week ends.
@_date: 2017-08-24 14:47:36
 has for 4 charts dedicated to segwit:
- number of segwit transactions
- virtual size / transaction
- fees / vbyte
- types of utxos spent in P2WSH
@_date: 2016-12-01 21:53:08
My hope is that we'll see more studies on the dynamics of the utxo set (like which factors explain its growth - market cap, usage, services, ...-).
There's a real decrease of the average size per tx since June 2014 which seems correlated with the decrease in the average number of txos per tx. Check this chart: 
It's true that scripts have become more complex but my intuition is that P2PKH is still the most prevalent template used by transactions. It may explain the correlation.
You're right that creation/consumption is more or less balance. Average number of txos created is slightly higher than the average number of txos consumed. The small difference explains the "slow" growth in "normal mode". Some services (like faucets) put more pressure on the utxo set but it seemed manageable till early 2015.
In rare occasions, the network enters a "spam mode" (e.g. summer 2015) and the utxo set quickly grows (+2 millions utxos during summer 2015).
The growth observed 1 week ago is suspicious (+500k utxos in 5 days). For now, it's difficult to assert with certainty if the cause was malicious or not but we start to have some interesting observations (
@_date: 2018-06-18 14:14:39
If your bug is caused by a problem with OP_RETUN, it means that it's related to BIP47 payment codes. 
If you check github ( you'll see that a fix has been pushed yesterday and that a new version has been tagged today.
Basically, it means that a dev has worked this week end in order to fix a bug in a wallet which is provided for free (and is still in alpha version). 
That's so fucked up /s
@_date: 2018-06-18 14:35:59
Yeah sure. Devs don't fix bugs they know about because it's so much fun to have users complaining about your product..
Life tip: Writing false statements (like "they try to hide bugs and fixes") or yelling to people isn't the best way to get a quick resolution of a problem. My 2 satoshis
@_date: 2016-11-30 20:04:35
All charts are interactive (you can drill-down/drill-up to see details for a given period -all, month, day, block-). See this video  (and its associated description) for a quick intro.
On my hand, the most intriguing observation is that the average number of inputs/output per tx has been decreasing since June 2014 (which implies a decrease in the average size of txs)...
Edit: 
You can also give a try to OXT Landscapes (3D viz of the bitcoin blockchain): 
Navigation may be a bit tricky at first if you're not a gamer but it gets very fun when you're used to it. For the utxos, I recommend 2 prebuilt scenes called "Satoshi's tree" &amp; "Spam in the sky" (the latest provides a good visualization of July 2015 spam attack) 
@_date: 2019-10-22 22:55:26
If not already done, contact the support team at [support
@_date: 2019-06-24 21:56:44
Why do you think that using a mobile wallet is worse for privacy than using a wallet running on a computer?
@_date: 2015-08-10 00:59:06
There's an excellent paper published in 2013 by Kroll, Davey &amp; Felten (The Economics of Bitcoin Mining, or Bitcoin in the Presence of Adversaries - 
In this paper, the authors describe bitcoin as a set of 3 consensus:
- a consensus about the value ("what is the value of bitcoin ?")
- a consensus about the state ("what is the valid blockchain ?") 
- a consensus about the rules ("what are the rules used to define a valid transaction or a valid block ?")
From the authors:
Each of these forms of consensus depends mutually on the other two.  For example, it is hard to agree on the history without agreeing on the rules.  And it is hard to believe in the value of a Bitcoin if participants cannot even agree on who owns which Bitcoin...
Something which isn't stated by the authors but seems obvious to me is that we have decentralized "solutions" for the first and second consensus. Free market (exchanges, ...) is used to find a consensus about the value of bitcoin, and the Nakamoto's consensus allows to find a consensus about the valid chain of blocks.
Remains the consensus about the rules. A decentralized solution has still to be invented but it's likely that it will be a very hard job. Some may think that a voting system (aka democracy) is a good solution but imho it's not. Because the matters are not just about ideologies or personal choices but are also technical. I mean, as a member of the bitcoin community, I'm all in for allowing all kind of transactions without any censorship. I don't care if you pay for a house, for a second of video on streamium or if you're just a spammer. But as a developer, I know this is just a bad choice because it's technically impossible with current (and near future) technologies.
For months, we have all struggled with a same problem and it's not the blocksize limit, it's the absence of this decentralized mechanism allowing the social consensus. May be worse, weeks after weeks, we have started (collectively) to damage the fragile consensus which existed and which is required to keep bitcoin alive (see previous quote from the authors).
Bitcoin has a scalability problem and this problem won't happen in one year, it's happening now. It's the consensus about the rules which doesn't scale today, not the consensus about the state.
It's unlikely that someone will come up tomorrow with a decentralized solution for this problem. Until this "miracle" happens, we should all do our best to preserve the system as a whole. And the system is composed of 3 consensus.
My 2 satoshis
@_date: 2015-08-24 17:59:45


Sorry but I can't agree with this.
Network sciences have well defined metrics for centralization and all these metrics have clear mathematical definitions. There's also well known results showing why centralized networks are less resilient to targeted attacks against hub.
Without even talking of political consequences of centralization, talking of the decentralization of the full nodes network isn't a waste of time. It's totally spot on if we take care of the security of the network.
Some people like Andrew Miller have started to work on these matters (project coinscope  and it's sad that nobody seems interested in what they have to say.
My 2 satoshis
@_date: 2015-08-13 20:54:01
Gavin did simulations with 20 MB block SIZES. OK, may be theyr were not convincing to everybody. 
Gavin has simulated the behavior of a single node with 20M blocks against the past blockchain (till block 300k).
This is a good first test but, indeed, it's not convincing for a distributed consensus system.
Note that increasing the block size LIMIT is not increasing the scale of the system by 20. 
Come on ! Not this argument again... Do you really want me to write that anticipation of worst cases is also the job of a good manager (also known as "risk managements") ?
Shit ! I wrote it. 
For the record, I truly respect Gavin for his work and his dedication to bitcoin. But this is not a blank check and it's a strength of Bitcoin: the need to reach a consensus.  
Blockstream has decided to repurpose bitcoin to be a settlement layer of the Ligtning Network, and limit the block sizes to 1 MB so that the network would become congested, because they decided that a "fee market" would be cool. All that without a simulation, or even a BIP, or even having a minimally clear idea of what the LN will be.
Yep. I know this theory about blockstream opposing the proposals because of a conflict of interest.
How about this other theory ? Some guys have created blockstream because, years ago, they came to the conclusion that the blockchain could not scale to the level expected by the community (aka the "VISA fantasy") while keeping its main property (decentralization) ?
So, sorry, but I cannot feel much remorse for that sentence you quoted...
No remose needed. 
When you write about systems reaching their capacity, I'm happy to read your posts and I'm glad to include your arguments as important factors for the debate.
On the other hand, rants against blockstream just degrade our capacity to reach a consensus.
@_date: 2015-08-23 18:40:56


I guess my point is about your "might" ;)
Many people like to repeat that LN is just an idea on a paper. 
On my hand, I would like to see a roadmap for the integration of IBLT in the P2P protocol.
@_date: 2015-08-15 17:07:34
Interesting thoughts. Thanks for sharing.
@_date: 2015-08-14 15:10:09
Thanks for this detailed and argumented answer. I agree with you on some points and disagree on some others. For the sake of brevity, I'll focus on our main disagreements.
Those recent traffic surges were clearly stress tests, not spam attacks
Recent events were spam attacks. 
A stress tests consumes resources temporarily but leaves a clean system once completed.
You can check the graph of the utxo set on statoshi.info and you'll see that it was spam.
You analysis of bigger blocks against spam is logical and seems sound. But I don't buy it because it's just, imho, a way to hide the problem under the carpet. 
This analysis holds if we consider the problem of a single attacker with malicious intents. But let's be honest and acknowledge that the real problem is something different.
Many of us were attracted to bitcoin because of the "promise" of micropayments. Of course, when you start to dive deeper in the technical details, you understand that it's just impossible with the current tech.
But saying that bitcoin can handle micropayments is sexy and good to attracts investments and it would be unwise to break the cash machine...
A few months ago, I've shared an "analysis" related to the growth of the utxo set ( 
The conclusion is that a small number of services doing micropayments (like faucets) are likely to play a role in this growth.
Don't get me wrong. These services aren't malicious and they don't aim to threaten the existence of bitcoin. 
They just use a resource which is available and are a perfect example of the well known motto "When you have a hammer..."
These use cases would be a perfect fit for payment channels but that didn't happen because the resource was available and doing so was cheaper than investing in the development of payment channels.
Let me play the devil's advocate for a minute.
If we think that these services should continue to transact on-chain, why do we deny this right to others use cases ?
A while ago, 21inc talked about the idea to use 1 satoshi utxos as tokens for IoT. Their use case is legit. Why should we prevent them to do these transactions on-chain ?
So, I propose that we remove the dust limit from the protocol...
In conclusion, I fully agree with Sipa when he says that raising the blocksize limit will just create an incentive for these kinds of transactions and that we will hit the same problem in the future.
What we need are true solutions for this kind of transactions which, I say it again, are legit but not a good fit for the blockchain.
The third possibility is that the natural traffic itself will grow to fill a large fraction of the 8 MB limit. That is not expected to happen for several years. Even if traffic doubles every year (as it did in the past 12 months), with an 8 MB limit it will take 3 years for it to reach 3.6 MB/block. By then, an artificial 1 MB block size limit would be forcing 95% of the potential usage to other alternatives: off-chain "bitcoin banks" like Coinbase and Circle, altcoins, or (much more likely) traditional methods like banks, Paypal, and credit cards.
If demand is unlikely to increase to 8 or 20Mb in the coming years, why is it so important to quickly raise the limit to these levels ? Something doesn't compute... ;)
@_date: 2015-08-15 13:29:11


That would be a real problem if this person travels with Uzbekistan Airways ;D 
@_date: 2015-08-22 16:49:48
There's an excellent paper published in 2013 by Kroll, Davey &amp; Felten (The Economics of Bitcoin Mining, or Bitcoin in the Presence of Adversaries - 
In this paper, the authors describe bitcoin as a set of 3 consensus:
- a consensus about the value ("what is the value of bitcoin ?")
- a consensus about the state ("what is the valid blockchain ?") 
- a consensus about the rules ("what are the rules used to define a valid transaction or a valid block ?")
From the authors:
Each of these forms of consensus depends mutually on the other two.  For example, it is hard to agree on the history without agreeing on the rules.  And it is hard to believe in the value of a Bitcoin if participants cannot even agree on who owns which Bitcoin...
Something which isn't stated by the authors but seems obvious to me is that we have decentralized "solutions" for the first and second consensus. Free market (exchanges, ...) is used to find a consensus about the value of bitcoin, and the Nakamoto's consensus allows to find a consensus about the valid chain of blocks.
Remains the consensus about the rules. A decentralized solution has still to be invented but it's likely that it will be a very hard job. Some may think that a voting system (aka democracy) is a good solution but imho it's not. Because the matters are not just about ideologies or personal choices but are also technical. I mean, as a member of the bitcoin community, I'm all in for allowing all kind of transactions without any censorship. I don't care if you pay for a house, for a second of video on streamium or if you're just a spammer. But as a developer, I know this is just a bad choice because it's technically impossible with current (and near future) technologies.
For months, we have all struggled with a same problem and it's not the blocksize limit, it's the absence of this decentralized mechanism allowing the social consensus. May be worse, weeks after weeks, we have started (collectively) to damage the fragile consensus which existed and which is required to keep bitcoin alive (see previous quote from the authors).
Bitcoin has a scalability problem and this problem won't happen in one year, it's happening now. It's the consensus about the rules which doesn't scale today, not the consensus about the state.
It's unlikely that someone will come up tomorrow with a decentralized solution for this problem. Until this "miracle" happens, we should all do our best to preserve the system as a whole. And the system is composed of 3 consensus.
My 2 satoshis
[Note: this comment was initially written in the context of another discussion ( but seems spot on for this thread]
@_date: 2015-08-23 18:28:26
Actually, I don't want to shutdown Matt's relay network. 
Gavin says that bigger blocks don't take much longer to propagate but this statement goes against all studies published on the subject (see  
Fact is that Gavin can say that because the fast relay network exists. But, as stated by Matt Corallo, the fast relay network isn't supposed to replace the P2P network
I'm organizing a system of peering between nodes in the network by creating a system of high-speed relay nodes for miners and merchants/exchanges. This system will a) act as a fallback in the case that the public Bitcoin network encounters issues and b) decrease block propagation times between miners.
**It is NOT designed to in any way replace or decrease the need for the public Bitcoin P2P network.**
source: 
note: emphasis (bold) is mine
@_date: 2015-08-23 16:18:05
Let's make a deal :D 
Let's increase the block size limit to 8M or 20M or [put your value]M. 
In counterparty, we shutdown Matt's relay network.
Deal ?
Edit: Downvoting is ok but it would be far more constructive if downvoters say why they don't want this deal.
@_date: 2015-08-14 21:59:37
It's becoming interesting ! Good point about pools mining empty blocks.
This sort of discussion is pointless without numbers. What is the natural block size limit now? it is surely more than 1 MB, and I bet that it is more than 8 MB. 
I could not agree more.
According to the papers cited in my previous comment, the time it takes to get to 50% of the network nodes with the P2P network is:
(block size =&gt; time)
1M  =&gt; 69s
2M  =&gt; 137s
3M  =&gt; 204s
4M  =&gt; 272s
5M  =&gt; 339s
6M  =&gt; 407s
7M  =&gt; 475s
8M  =&gt; 542s
9M  =&gt; 610s
10M =&gt; 677s
11M =&gt; 745s
12M =&gt; 813s
13M =&gt; 880s
14M =&gt; 948s
15M =&gt; 1015s
16M =&gt; 1083s
17M =&gt; 1151s
18M =&gt; 1218s
19M =&gt; 1286s
20M =&gt; 1353s
On my side, I would bet that things have improved since this formula has been proposed (the awfull devs from blockstream have optimized a bunch of things ;). For an updated version of this formula, may be the data collected by bitnodes.io may help (if they agree to share their data).
Moreover, this formula doesn't take into account the fact that mining pools are now interconnected via Matt's relay network and are not "anonymous" nodes in the P2P network.
And this point lead us to the conclusion of my previous comment: are we ready to trade the P2P nature of bitcoin for a few additional megabytes ?
@_date: 2015-07-08 21:47:52
I have to strongly disagree with you about the risks associated to this scenario.
Of course, bitcoin was designed to cope with forks but forks are supposed to be very rare and short. Forks like the one observed with BIP66 (6 blocks) are exceptional and considered as an incident.
Long delays for blocks propagation may raise the length and the occurrence of forks and that would have severe implications. The main risk with (even temporary) splits of the network is of course an increased number of double-spending.
We should all be very fearful of these risks because the most important feature of bitcoin (before adoption, decentralization, ...) is its resistance to double-spending and confidence of people in this resistance. If we lose this, bitcoin is dead.
@_date: 2015-08-23 18:59:58


Yep. I know and I fully agree with you :)


I agree but the fast relay network is composed of around 10 servers in the world. The P2P network is around 6000 nodes.  
This is the point !
@_date: 2015-07-09 14:38:56
I fear this subject (full nodes as paid services) will be another hot debate !
There's obvious divergent opinions on the subject but imho the first difficulty is technical. To my knowledge, we still don't know how to validate that a full node is really a full node, with reliable countermeasures to fight sybil attacks. 
@_date: 2015-07-09 14:50:56


Agreed. It's interesting to measure this kind of metric at local and network level. They gives us complementary information about the state of the network.


It's kinda ironic to replace an arbitrary limit by... another arbitrary limit (just kidding ;)
I guess this is just another example illustrating that economic goals (bitcoin as a free market) don't always match with technical goals (network security, mitigation of spam, ddos attacks, ...)
That should encourage everybody to fix the social consensus by accepting that bitcoin is all about tradeoffs. 
@_date: 2015-07-01 15:19:35


I strongly disagree with this way of thinking, calling FUD the "critics" of proposals.
An important part of engineering activity is risk analysis. It implies to analyze the worst outcomes (probabilities of occurrences, conditions of occurrence, cost estimation, mitigation plans, ...)
The point isn't to know if big blocks will appear after the modification of the limit. The point is to enumerate/describe what might happen in the worst cases.
If this kind of work can't be done without raising a wave of ad hominem attacks, let's just stop all discussions. It will save a lot of time and energy for everyone.
To get back to the OP:
- Does anybody know a study/experiment showing that current implementation of the P2P network (without optimizations like the relay network) can cope with 2M, 4M, 8M, ... blocks without any risk for the protocol (consensus about the longest chain)
- Can we all agree on the fact that the relay network isn't a P2P network but that it relies on a small set of relay nodes and that part of its security relies on the fact that some of these nodes are not public (WRT ddos attacks) ?
- Can we all agree that the security model of the relay network is different from the security model of the P2P network ?
- Can we all agree that for now, it doesn't matter because there is a redundancy of the 2 networks ? (the P2P network can cope with 1mb blocks)
- Can we all agree that if current implementation of the P2P network can't cope with big blocks, that should be mentioned explicitly in the proposals, considering that bitcoin is a "Peer-to-Peer Electronic Cash System" ?
WRT to the incentive of mining pools to produce small blocks: Yes. Sure. This is one of the core principles of bitcoin.
- Anyway, what are the risks if a mining pool with significant hashing power and access to the private relay nodes is coerced and goes rogue ? 
- What are the consequences for the global network ? 
- Do we have the same consequences with the current limit and with a 8Mb limit ?
- Can we afford the risks associated to bigger blocks, considering the potential benefits.
Whatever the conclusions, this is the kind of analysis I would expect to see in these discussions.
TL/DR: 
Avoiding any talk about the risks associated to a proposal is a marketing approach.  Marketing has an important role to play for bitcoin but it shouldn't be part of the technical discussions. Technical arguments are only a part of the decision process but they should provide an objective vision of the benefits/risks associated to a proposal.
@_date: 2015-08-13 20:53:39


No problem. I'm an agnostic ;D
@_date: 2015-07-06 01:47:22
Wrong. I'm the "small-blockista" who suggested this scenario and it has nothing to do with fear of miners. 
Miners have invested a lot of money in the network and it's rational that they try to maximize their benefits. It's what I would do if I was a miner.
On the other hand, a significant portion of miners (close to 50%) which doesn't validate previous blocks is a serious problem for bitcoin and for all of us.
I want to be open to all point of views in this discussion but saying with authority that the recent fork isn't a problem or is the result of small blocks, seems to me like a new instance of Job's reality distortion field. 
My point is simple: let's avoid to create bad (worse) incentives for miners (which push them not to do what they're supposed to do).
My 2 satoshis
@_date: 2015-07-08 22:47:14


I think this chart gives a large and aggregated view of forks (which isn't bad but may be misleading)
If I'm correct, forks should be more rare at the level of a single node (see this discusssion 
But more important, I bet that current forks don't live for long and are quickly resolved. 
With bigger blocks, and no improvement of the protocol, that may change. This is why I'm convinced that tests are so important.
As an example, Rusty Russel has published a very interesting post today ( illustrating how 8Mb blocks may create new risks (which don't exist with 1Mb blocks).


Agreed. It was exactly my point yesterday with the paper describing the 3 consensus of bitcoin. 
For now, it's the social consensus which is broken and damaged, not the bitcoin protocol.
@_date: 2015-08-13 19:08:09


No offense intended but this sentence really made me laugh. 
For the record, here's a post you've written 2 days ago: 
Let's note the emotion-free conclusion about the blockstream devs
I am sorry, but I have this strong ingrained dislike for liars and sleazes...
I suppose this kind of remark aims to help in the process of reaching a consensus... :D
Oh and by the way, no competent manager would propose to release a solution increasing the scale of a system by 10 or 20 without prior tests and simulations.
@_date: 2015-07-08 21:13:58


You're talking about downloads but you seem to forget that the main bottleneck for bandwidth is upload. 
And currently, I can observe on my full nodes that quantity uploaded = 20 * quantity downloaded. 
@_date: 2015-07-07 00:36:59


Yep. It's likely that the current implementation of the P2P network can't handle much bigger blocks without increasing the number of forks.
The paper linked in my previous comment ( has a formula allowing to compute the propagation delay required to reach 50% of nodes via the P2P network for a given size.
I guess the constants may be a bit outdated (optimizations have been done on the verification part) but it's straightforward to check that the current implementation of the P2P protocol can't handle very big blocks. Anyway, that's theoretical stuff. Having a 8Mb testnet would be a great way to validate/invalidate this point and others possible threats.
The relay network allows faster propagation for miners but the relay network isn't intended to become a replacement of the P2P network.
In its introduction to the relay network, Matt Corallo has made clear that miners should continue to use the P2P network (as a security if a problem occurs with the relay network):


Related to these matters, the answers I've received so far are:
1- miners will use the relay network
2- miners won't create big blocks
The problem is:
1- even if miners use the relay network, we need to keep the redundancy of the 2 networks and we need to be sure that the P2P network can cope with big blocks. 
2- if everybody is so sure that miners won't create big blocks, the question remains: why is it so urgent to increase the limit to 8Mb or 20Mb ?


That's the most worring part. BIP66 was an easy soft fork. Something very consensual. Announced months ago...
The recent fork is clearly a problem related to processes used by mining pools and is independant of the content of BIP66.
In term of consequences, BIP101 is far more complex than BIP66.
@_date: 2015-07-09 00:08:25


Your argument seems logical and correct but the fact is that if you run a full node 24/7 for a long time, you can observe that quantities of data downloaded and uploaded diverge.
If we exclude SPV nodes (not sure about their cost for a full node), I think that may be explained by some patterns of usage. 
If a node connects to the network only a few hours per day or per week, it must retrieve the missing the blocks &amp; txs and will consume more down. than up. On the other side of the pipe the 24/7 node will consume more up. than down. to send data to these "intermittent" nodes.
 
Too bad that we can't have stats about the entities running full nodes (corps, hobbyists, merchants, ...). Results might be interesting.
@_date: 2015-07-08 20:24:54
I think the linked discussion misses a very important point: propagation of transactions is one thing. Propagation of blocks is a very different thing. 
Propagation of blocks is central because long delays to push a block to more than 50% of the hash power will disrupt the protocol with recurrent forks.
The current stress test will give us interesting insights about the current implementation of the protocol but it doesn't say anything about the consequences of big blocks.
Thus, I fear the debate is far to be killed ;) 
@_date: 2015-07-26 19:45:17


Well, it's not really surprising. A thread starting with "The crowd arguing..." doesn't sound like an invitation to a discussion. 
My 2 satoshis.
@_date: 2015-07-07 18:59:02


TBH, I think it's difficult to predict the outcome of a "contentious" hard fork (whatever our opinions on the matters). 
Anyway, I'm not fond of "aggressive" or unilateral proposals (like checkpoints to eject miners) because they damage the idea of a protocol built by consensus.
They would also create a very bad precedent for investors which are not in the first circle, by damaging confidence in the bitcoin consensus.


I guess it would take a few months (specs, preparation, execution, ...). 
It would be a net benefit to invite academics because some good works have been done lately (Andrew Miller &amp; al.):
- on the topology of the bitcoin network (Coinscope: 
- on full scale simulations (
@_date: 2015-07-06 21:57:53






The fast relay network plays for miners the same role that IBLT could play for the P2P network.
The problem is that the Relay Network has a very different security model, relying on some "private" relay nodes (in order to avoid ddos attacks).
With the current blocksize limit, both networks are redundant. If the relay network is attacked, miners can use the P2P network to push blocks.
With a higher limit, we will lose this redundancy.
My point is 100% technical and unrelated to the economic consequences discussed by others people (fee market, miners incentives, ...).
I'm a software engineer and I prefer to focus on technical matters.


Doing extended tests of big blocks would be a great way to stop these never-ending discussions.
A while ago, P.Todd proposed to run a 8Mb testnet experiment ( 
Gavin's answer was heartbreaking (


My interpretation of this answer: "let's not worry with tests. It's not fun for bitcoin startups. Let's put that in production and when we have problems, we'll see what we can do..."
Can you imagine Boeing or Ford saying: "Testing the security of our product is expensive. We're very busy trying to stay in business by attracting customers to our products. We'll tackle security problems when they occur." ?


I'm not sure to agree with this vision but the metaphor made me laugh. :D
As a side note: how would you call bitcoin businesses pushing for bigger blocks but refusing to invest some money for tests ?
EDIT: I've just found this post by Rusty from Blockstream:  May be one of the best news for big blockistas, lately.
@_date: 2015-08-14 16:03:40
This influence can be shown by an argumentum ad absurdum.
Let's imagine "huge" blocks (10, 100G, ...) propagating on the network with the current P2P network.
Propagation delays will increase significantly, reaching several minutes to flood the network and that will increase the probability of forks.
Papers published on the subject (Decker/Wattenhofer, Sompolinsky/Zohar) suggest a linear dependance between block size and propagation delay (
Things seem better since Matt Corallo's relay network has been launched but it's good to remember how Matt has introduced this network:
This system will a) act as a fallback in the case that the public Bitcoin network encounters issues and b) decrease block propagation times between miners.
It is NOT designed to in any way replace or decrease the need for the public Bitcoin P2P network.
Actually, it's one of my "concerns" with the current proposals. It's never written but they all seem to assume that the relay network is part of the solution before the P2P network is improved.
But the relay network isn't the P2P network. Its infrastructure relies on a couple of servers dispatched in the world (see  and its security model is quite different from the P2P network.
To my knowledge, no one has come up with an estimation of what can be supported by the current P2P network before occurrences of forks start to increase.
Like many in the community, I've invested in a cryptocurrency backed by a P2P network. Having the system relying more and more on the relay network isn't a small change.
@_date: 2017-04-04 11:03:23
Hi u/_chjj &amp; u/josephpoon,
May you provide some details about the rationale behind the choice of the parameters in the "Extented Transaction Cost" section ?
Thanks in advance.
@_date: 2015-07-06 20:55:05


Basically, delays for block propagation and resilience of the network.
Without a mechanism like IBLT for the P2P network, a higher blocksize limit may introduce weaknesses in the protocol. It doesn't mean that horrible things will happen as soon as the limit is increased but that remains a bad move because the value of bitcoin isn't only related to its adoption but also relies on its security.
My point is that we shouldn't raise the limit without fast propagation implemented in the P2P network.
If we really have to increase this limit, let's do proper tests before. I can't believe that an industry which has received so much funding can't afford the cost of testing an important modification like this one.
More here:
- 
- 


I acknowledge that there's a "difficulty" in this debate. One party supports the current status quo and I understand how frustrating it can be for the other party. Anyway, I don't share the vision of a conflict of interest for blockstream for a very simple reason: anything damaging the value of bitcoin will damage all bitcoin businesses (blockstream included). So, it's in their best interest to take the best decisions for bitcoin. That's also why I don't believe in the crash landing scenario initially described by Mike.
@_date: 2015-07-07 23:39:42


Exactly ! There's this paper published in 2013 ( which defines bitcoin as a set of 3 required consensus (about rules, state and value). 
I really like this paper because it helps to understand that we're all focused on future scalability problems of the bitcoin protocol (consensus about state) but, right now, it's the social process (consensus about rules) which doesn't scale and is damaged. 
It's also worth noting that two of these consensus have decentralized "solutions" (state =&gt; nakamoto consensus, value =&gt; free market). It remains to invent a decentralized solution for the last one and I guess it's a very difficult problem (the most difficult one ?). I guess it may also explain why this debate is so "painful" for everybody.
@_date: 2015-07-08 21:29:57


Actually, I've wondered if I was the most unlucky bitcoiner, having my resources sucked by the rest of the network. :)
The truth is that with the current implementation, if you run a full node 24/7 for a long time, there's a factor of 5-25 between the dowloads and the uploads. It was already observed by many bitcoiners. It's a pity this fact is so often forgotten in the actual debate...
@_date: 2015-07-09 21:20:13


So true. But this part of Bitcoin (the social process) is also truly fascinating. It's noisy, chaotic, exhausting... because, as a decentralized community, we still lack the "magic recipe" to reach  consensus. The solution remains to be invented but I guess it's a very hard problem.
@_date: 2015-07-08 22:17:18
Yep. Around 1/3 of the connected nodes are SPV wallets.
I know the argument about the equilibrium (in total) but I think it's a kind of fallacy because if everybody was rational (and was just running nodes to send and receive personal transactions), the network would certainly collapse...
@_date: 2015-07-06 15:23:22
I said wrong about the statement that small blockistas are only motivated by "political" reasons.
A fun fact: my initial (and natural) position about the blocksize issue was in favor of bigger blocks (2M?).
Then, I've started to dive deeper in the subject and I felt less and less comfortable with the proposal of "big blockistas".
I think there's a fundamental problem with tests done by Gavin: they check the impact on a single node but don't say anything at network level (bitcoin consensus).
I'm still open to bigger blocks if proper tests are done, showing that bigger blocks don't introduce new weaknesses in the protocol.
There was some proposals to run a "big blocks" testnet but they were rejected by Gavin (to expensive for bitcoin companies...).
I also feel very uncomfortable with the path taken by Gavin &amp; Mike for this proposal. 
I guess there's rational reasons explaining their choice. It's likely the subject has been discuted privately by the core devs before Gavin went public and that something went wrong during this discussion.
Since I will never know what happened, I prefer to focus on objective facts. 
At last, I support all efforts done to find a consensus, like BIP100 proposed by J.Garzik.
While I'm not convinced by the details of his proposal, I truely appreciate his effort towards consensus.
WRT mining pools loosing money with this fork: Indeed, I think the recent fork should motivate them to improve their process but the interesting question is: how much money did they get in the past thanks to this policy ?
@_date: 2017-04-22 13:01:06
Right. Since last version, the icon allowing to display the graphalizer is on the left side of screens displaying the details of a transaction.
@_date: 2017-04-22 10:06:54
OXT : 
Example of transactions graph visualization : 
More info here : 
@_date: 2015-07-10 00:49:57
TBH, I don't know if a clean separation is really possible without any damage for bitcoin(s).
I mean, it's likely that the first consequence would be a geographical split (china/western countries) because of the location of miners. IMO, that would be a first regress (wrt bitcoin as a global ledger). 
The second problem is that it may slowdown the growth of these 2 bitcoins (hash power must increase with value). 
Only alternatives would be to burn twice the energy (meh) or that one network dies and its hash power gets back to the "survivor"...
On my side, I'm still optimistic that a consensus will be found. :)
@_date: 2018-11-03 00:01:32
Nope. Nobody from the team shared this post on telegram or pretended anything.
There's no need to add confusion to the problem met by [u/PurpleShizzle]( 
@_date: 2018-11-03 09:37:42
Hi I thought to something. Have you tried to reinstall the setup (Android 9 Beta + password manager + samourai) which was initially used to create the wallet ? If not, it might worth it to give it a try. My rationale is that the password manager might behave differently in Android 9 Beta and in previous versions and it would mean that your wallet wasn't initialized with the correct seed and it generated xpubs different from what we could expect. If it's the case, I suspect that it will be the only way to regenerate the exact same wallet.
@_date: 2018-11-06 23:04:35
Indeed, I missed this point in your post but Sam from the support later told me that you had already tried that.
Just two additional questions (if you haven't already provided these info to the support):
- When did you install the wallet for the first time ? Was it on 25/11/2017 (date of first tx linked in your post) ?
- What is the name of the password manager used ? 
@_date: 2018-11-02 23:52:07
Hi [u/PurpleShizzle](
I'm a member of the samourai team (but working on another part of the project).
I would recommend that you contact the support once again with a link to this post.  I'm sure they'll agree to investigate the issue but it's likely that it will require some additional information helping to troubleshoot the issue.
@_date: 2018-11-03 00:49:52
Yep. I suspect some problems with the android version or a library. 
The most important for the support is to gather as much information as possible about your "environment" and about the issue in order to reproduce the problem. 
Concerning the segwit/non segwit tx "issue", I don't think that it is related to this problem because the type of the change address generated by samourai depends on several factors (type of others addresses appearing in the tx, privacy setting of your wallet, etc). Anyway, it's better that you provide all the clues that you've noticed.
@_date: 2018-11-03 00:20:28
Hi PurpleShizzle,
Well, I'm not an expert in development for smartphones but considering the description of your problem, I suspect an issue which might be specific to your phone model, your OS version or some combination of similar factors. Troubleshooting the problem may take some time but having as much information as possible about your environment will be key.
I'm going to contact the support to tell them to check that with you again.
@_date: 2019-01-05 15:11:45
You're definitely right that there are connections between the 2 concepts. 
@_date: 2019-01-05 13:55:45
Note that the 2500 figure isn't nonsensical but it's more related to the concept of the unlinkability between an input of the 1st round and an output of the last round.
Both concepts are useful even if I tend to think that the anonymity set is more important for a coinjoin mixer. My rationale is that:
- it's possible to have a high unlinkability with a low anonymity set (e.g. 2 entities mixing together again and again their UTXO =&gt; UL=2^n, AS=2)
- I don't think it's possible to have a high anonymity set with a low unlinkability (e.g.: a "dumb" mixer allowing 50 entities to mix together again and again but their UTXOs must have strictly different amounts =&gt; UL=0, AS=1). 
@_date: 2019-01-05 02:43:14
I have to sligthly disagree with you here. If the same 50 entities repeat a coinjoin together (without any new participant) the anonymity set is still 50. Reaching an anonymity set of 2500 would require 2450 more entities participating to the mixes.
@_date: 2019-01-12 17:30:50
As far as I understand, is saying 2 things:
- Samourai should improve its build process for better deterministic builds.
- Samourai team members defining themselves as privacy activists while not providing a perfect app and build process is a huge red flag.
IMHO, first point is a welcome feedback while the second is unecessary and counterproductive in this discussion. By the same logic, Satoshi proposing a deflationary system while the system had a bug causing infinite inflation should have been seen as a huge red flag casting a shadow on his system and his true intentions... But who knows? May be Satoshi was trying to create the longest con ever seen in history. ;)
@_date: 2015-09-20 15:30:21
IMHO, all this debate has gone nowhere during months for a very simple reason. Almost all arguments made by big blockists are related to economics while almost all arguments made by small blockists are technical.
Taking into account both kind of arguments is what we need and we must stop to oppose them.
So, for the record, I'm fine with the theoretical idea of a free market finding its equilibrium because I "believe" in complex systems. But that won't prevent me to point out the limitations and risks related to each solution because it's what honest engineers and scientists do.
On another forum, posters discussed the fact that small blockists are so pessimistic. That made me laugh because it's such a misunderstanding. Engineers and scientists are not pessimists by nature but by duty. Identifying and preventing risks is an important part of the job. At the end of the day, planes don't fly securely because of the magic of the free market but thanks to the "paranoia" of engineers &amp; air traffic controllers. Engineers and scientists seem "pessimists" because they care of users.
Stating in a discussion that free market will sort out the problems is at best a political argument and at worst magical thinking. This isn't what engineers do.
I really appreciate Gavin's efforts to act as a bridge between the two sides of this "cultural gap" but, with all due respect, he's doing it wrong. The solution doesn't lie in disbandment of economic or technical principles. The solution lies in finding a (rare) path allowing to stay true to all these principles, as much as we can.


I could be fine with the theoretical idea of a bitcoin network self regulating "thanks to" orphaned blocks. But that raises several questions: 
- for now, we don't have the technology implemented to do that (where is IBLT ? Are we sure Matt's relay network can support bigger blocks ?)
- what about 0-conf payments if orphaned blocks becomes the rule ?
- ...
I already wrote it several times, but I'll say it once again: I'm not opposed to bigger blocks per se, but I'm certainly opposed to distorted or partial arguments. Sadly, this post written by Gavin is a very good example of that. 
@_date: 2015-09-12 18:26:14


So, basically you're saying "I don't have any actual metrics I'm basing my opinion on, but my gut says everything's ok!"
@_date: 2015-09-20 00:57:32
I strongly disagree with the conclusions of this post.


This is a good example of a self-fulfilling prophecy. Indeed, if we favor a solution increasing the requirements/burden to run a full node, it's likely that we will see less and less people running a full node. This is the whole point of the discussions about the differences between the SPV model and layer 2 solutions (like the Lightning Network).


This is wrong. A network/system consuming resources in O(n²) while providing value in O(n) is doomed to fail because too expensive. The total work IS an important metric.
@_date: 2015-09-15 18:36:03
FWIW, here are a few personal thoughts. I think some people share these ideas but I know some others have very different views on these matters. So, up to you to build your own opinion ;)
One of the great things with the initial design of bitcoin is the financial incentive to secure the network by running a full node (thanks to mining and blocks rewards).
Unfortunately, centralization of mining in pools breaks this genius idea. Today, there isn't enough pools to imagine a network in which full nodes are only run by miners/mining pools (Satoshi's initial vision).
For now, the "patch" used to solve this problem is built upon:
- commercial entities running full nodes because they've an external financial incentive (web wallets, exchanges, monitoring, ...) 
- individual unpaid hobbyists. 
IMHO, reintroducing in the system a financial incentive for full nodes would be a good idea.
LN may help in several ways:
- software client being at the same time a LN node &amp; a bitcoin full node (as proposed in your post)
- services provided by full nodes (like serving spv wallets) monetized via micropayments over LN.
- [put your own idea here...]
For the record, I also think this kind of solution will imply new challenges. 
For example, it's likely that financially incentivized nodes will come with more "proxy nodes" ("fake" nodes run by a same entity, backed by a single db, but serving SPV nodes as intended). That may be a problem if you see the number of full nodes as a proxy metric for measuring decentralization of the network. IMHO, this metric is already too weak to serve this purpose but that's another story... 
As a conclusion, I like the idea of financial incentives to increase decentralization and network effect. Lately, JoinMarket (incentivized privacy with coinjoin) seems to me like a good example of this kind of model.
@_date: 2015-09-10 18:03:36
From experience, a ssd significantly improves the perfs of bitcoin core (compared to a mechanical disk). 
@_date: 2015-09-21 10:58:36
I still fail to see - from an engineering perspective - what is wrong at all about Satoshi's initial plan.
This is a good question. Actually, it may even be the most important question. I fear it's going to be a long post and I apologize for that.
IMHO, the answer lies in posts written by Satoshi which are often quoted (and misunderstood) to support the legitimacy of XT.










It's important to read these posts in their "historical" context. 
Pool mining, GPU rigs or ASICs don't exist yet ("ironically", it's likely that someone is working on the last part of what will become the first GPU miner when Satoshi writes the last posts).
When Satoshi writes about (full) nodes, he means mining nodes. Actually that makes sense considering that bitcoin is all about honest validation incentivized by a financial reward. The idea that people may run a full node if they don't have a financial incentive is not part of his thinking.
According to this vision, even services (online wallet) should run a lightweight client because the security provided by these clients should be enough.
At last, his vision of full nodes hosted in server farms is a direct prediction of the few mining farms existing in the world.
**So, what went wrong with this vision ?**
Reality. ASICs &amp; Pool Mining have disrupted this vision, especially the estimation of the number of full nodes (between 10k and 100k) and from there the degree of decentralization of the network.
If we stay true to Satoshi's thinking, today we should have a handful of full nodes run by mining pools plus a few hundred run by individual mining with P2Pool. Far from the vision of ten of thousands nodes.
We barely reach 6000 nodes because a few thousands hobbyists and a handful of startups keep running full nodes and continue to audit work done by miners (who sometimes forget to do their validation job ;)
Without the unpaid participation of hobbyists and startups, bitcoin (the currency) would be already as much (more ?) centralized as the legacy banking system.
I know the argument (made by small/big blockists) that people have an incentive to run a full node to verify the ledger by themselves. It may be true but it has nothing to do with Satoshi's thinking.
**What is the "solution" ?**
Ultimately, we have to wait for mining hardware becoming a commodity. Until that happens, we should do our best to keep bitcoin as decentralized as possible.
It's likely that there's no "one size fits all" solution but many small things which remain to be done or improved:
- incentivize decentralized mining (solutions like P2Pool)
- keep current mining industry as decentralized as possible (Matt's Fast Relay Network)
- incentivize full nodes 
- relieve pressure on full nodes as much as possible (layer2 solutions)
- ...
**What's the connection with this O(n²) thing ?**
IMHO, this big O stuff isn't so important but it's bad when it's (mis)used to justify a position.
It's great that Gavin tries to explain what O(n2) means with very simple words but this post should be neutral and honest.
FWIW, here's my own version of this story:
- The bitcoin P2P network in its initial phase (every user running a full node) consumes O(n²) resources.
  Satoshi had this intuition and he proposed a solution but it doesn't mean it's the only one.
- One possible solution works on the scalability of the relation between the number of nodes and the number of users.
  It's the solution proposed by Satoshi with SPV nodes.
- Another possible solution works on the scalability of the relation between the number of transactions and the number of users.
  It's basically layer2 solutions like the Lightning Network (among others)
On my hand, I think Layer2 solutions are important because they help to relieve the pressure on full nodes while allowing greater adoption and this is what we urgently need.
The big block side has a lot of engineers and engineering-types, too. For example is a physicist, and a computer scientist by training/education (if I am not mistaken).
I acknolewdge that and actually I'm always pleased to read works done by everybody. As long as it remains intellectualy honest.
Despite the critics made by many small blockists, I like the work done by Peter_R. It's surely not perfect but at least it's something which exist and can be criticized.
My main "problem" with Peter: I think he should make a clear choice between science and politics. Mixing both of them isn't good for his discourse.
Moreover, leaving the political path may help him to understand that critics made to his work aren't political but true technical points that deserved to be studied.
At last, I think his model should now be validated against actual data. The whole point of applied science is that equations don't worth a lot if not inline with experimental data.
Anyway these choices belongs to him.
Same remark could be made to Gavin, Mike but also to many people standing in the small block camp. If we want to find a good solution let's all focus on our skills and let's learn to listen the disturbing arguments which don't fit with our own narrowed vision.
And that's why I believe the small-block side is pretty close to concern-trolling.
I can feel your pain here. The irony is that I stand in the "opposite camp" but I often feel the same while reading some arguments made by big blockists and these observations tell me that the real issue is all about communication. 
Many damages have been made to the social bitcoin consensus during the last months. From there, the only positive outcomes will require some forgiveness from all parties :)
EDIT: Formatting for readability
@_date: 2015-09-12 18:52:03


I like this approach. :) So, how would you define Gavin's statement: "I think it's plenty decentralised, probably over-decentralised for resistance to regulatory pressure." ?
@_date: 2015-09-10 17:43:05
Do you use a ssd ?
@_date: 2015-09-20 15:39:20
You can do it if you consider that number of users (n) or number of transactions (scaling linearly with number of users) is a good proxy for measuring the value provided by the network.
IMHO, a better metric would be the global transacted volume but it's quite difficult to measure in a pseudonymous system.
EDIT: fixed typo
@_date: 2015-09-12 18:17:35
I have to agree with you here. The last "political" part of his presentation doesn't serve his discourse, quite the opposite actually.
@_date: 2015-09-26 16:20:33


IMHO, Satoshi's vision has been disrupted a while ago... and not by "small blockists" ;) 
See  for details
The good news is that it isn't too late to fix it.
@_date: 2015-11-19 15:23:49
You're 100% right. Payment codes are only one part of the equation.
Better privacy in bitcoin requires 2 components:
- something allowing to avoid address reuse and providing a good UX (e.g. stealth addresses, payment codes, ...)
- something obfuscating the link between inputs and outputs of transactions (e.g. coinjoin, ring signatures, ...)
While I'm all in for solutions allowing to decrease address reuse, I agree with you that the statement made in this video is misleading or let's say a bit too enthusiastic ;)
edit: formatting
@_date: 2015-02-13 11:44:08
I've made something similar and it's good to find the same figures :)
If you plot the same chart at a more detailed level ( you will observe some interesting behaviors: during the last months, the set has increased of 100k utxos every sunday but the  is lower.
My theory is that it could be related to some faucets but it's just a theory.
@_date: 2019-05-23 15:38:47
The thing is that in the zerolink framework, a Sybil attack is cheap if the attacker is able to coerce the operator. It only costs the miner fees for a few mixing transactions.
I agree that using bitcoin is certainly not a good idea for anyone trying to escape the surveillance of a powerful adversary. Actually, I would even say that using a digital payment system is certainly a bad idea if you plan to fund very nasty things with your money. :D
@_date: 2019-05-23 12:34:15
The main issue with the zerolink model (wasabi, whirlpool, etc) is when the entity operating the coordinator is forced by a third entity to deanonymize particular UTXOs (as in "Hello Sir, here's a subpoena for you"). IMHO, absent a better solution, a warrant canary is a must-have.