@_author: bissias
@_date: 2017-11-22 15:03:06
I think I oversimplified originally.
Imagine a dial that ranges from "no doublespending" to "doublespend everything". I assume rampant doublespending would lower the fiat value of BTC considerably, but there is also the possibility of moving mining power to some altcoin. It's not clear to me that *any* amount of doublespending plus honest behavior after moving to an altcoin would result in higher fiat profit than simply behaving honestly in BTC.
With that said, if some amount of doublespending is profitable, then PoW requiring ASICs limits the set of altcoins the attacker can mine when he jumps ship, therefore lowering the potential fiat profit. It is in this sense that I believe the investment in specialized hardware is a liability to the doublespending miner in this scenario. 
@_date: 2017-11-22 19:45:41
It all depends on how the market reacts. I suspect that even a small number of successful doublespends would be fairly devastating to the trade price of BTC (as well as any alts minable with the same PoW).
@_date: 2017-11-18 22:58:32
Crazy thought: what if inv messages for transactions included the associated fee? Then miners could pre-filter before they end up in the mempool and even before they are downloaded. It would also limit the propagation of spam or too-low fee transactions. Of course this all depends on miners deciding upfront what they consider to be too low.
@_date: 2017-11-22 19:18:38
I think there are services that constantly monitor the Bitcoin network for doublespends. So I assume that it would be detected as soon as the attacker tried to initiate a reorg of the blockchain.
@_date: 2017-11-22 13:28:32


I see that as being more of a liability than an asset. The attacker has gone to the trouble of investing in purpose-built ASICs and can only profit mining other coins to the extent that their PoW is compatible with those ASICs. If mining those other coins is more profitable than mining BTC, then the attacker should apply his / her power to them in the first place. If not, then there is no utility in destroying BTC.
@_date: 2017-11-22 19:10:34
It's hard to estimate how much someone could make in a 51% attack without knowing how the market would price such an event. What's more, if an attacker was successful in doublespending in BTC, how could the altcoins that share PoW with BTC be sure he wouldn't do the same to their coin next? It seems like their is strong potential for cascading drops in the fiat value of all coins having the same PoW.
@_date: 2018-03-22 17:50:17
Ah, I think I see the issue. The density function you gave above is not actually exponential (t/10 should be 1/10). With that change, the PDF achieves its highest density when t=0, which is what is shown in the histogram. That the inter-block time follows an exponential distribution seems to be generally accepted. See for example, the [Selfish mining paper](
Regarding criticisms from twitter, we hope to have a conference paper with all the details in submission soon, but here are some responses. 1) The latest version of Bobtail proposes equal reward for each proof included in the mining package. The paper proves that this results in expected reward proportional to hash rate. 2) We have included protocol tweaks that prevent miners from profiting from proof withholding. For example, honest miners are directed to assemble a mining package using proofs in FIFO order when there exist more proofs than necessary. Thus withholding miners are less likely to have their proofs included in a mining package. 
Brian Levine [will be presenting]( Bobtail this weekend. Depending on time, he might be able to talk about our responses to some of these criticisms.
@_date: 2018-03-22 15:03:28
I'm not sure I follow your argument. The plot in the video is based on real blockchain data. The fact that it closely resembles an exponential PDF (the one you have provided) suggests that exponential is a good model. So I agree that the PDF you've given is a good model for the inter-arrival time for blocks. And the mean of that distribution is 10 minutes. But using this exponential model, there is no upper limit on the inter-arrival time for any given block. 
@_date: 2018-03-22 14:54:14
The problem with doing this is that the lowest order statistics (OSes) of the distribution of hash values are not independent. So, for example, it is true that a miner with share x of the hash rate expects to come up with the 1-OS fraction x of the time. But he also expects to come up with the 2-OS fraction x of the time. So if you reward him for each proof commensurate with that proof's difficulty, then he ends up getting proportionately more reward in expectation than miners with less hash power.
However, since releasing the arXiv report, we have settled on what we think is a better reward scheme: each miner should receive a fixed reward for each proof contributing to the mining package. Under this reward scheme, it's not difficult to show that the miner's expected reward is proportional to his hash rate.
@_date: 2017-10-30 21:46:18


No each miner will hash an entire block (as we know them today), *her own* block. She does this because there is no way for her to know if her hash will end up being the lowest one. Note that the probability of being the lowest hash does not change under this scheme relative to today. Only the block corresponding to the lowest hash value will be added to the blockchain (along with the other 9 proofs).


In principle yes. But the idea is that you don't know how long it will take to find that 10th hash, so you ought to start at the beginning of the block interval. Also, like I mentioned above, you're still no more likely to be the lucky one who successfully mines the lowest hash.


I claim that the expected reward for a miner with a given hash rate is identical to what it is in today's protocol. Although we have not yet proven this analytically, we do have a simulation result that demonstrates it.


This is a keen observation because there are indeed new issues to worry about. Perhaps the thorniest, related to timing as you suggested, is that it becomes relatively easy for someone "steal" a block if he has the second smallest hash and is able to assemble enough other proofs. A naive implementation would suffer from high orphan rates as a result. We deal with this (quite effectively I think) by incentivizing those who submit proof to honestly vote on who they think has the lowest proof. The end result is that the lowest hash ends up with an overwhelming advantage in winning the block.
@_date: 2017-10-30 18:18:31
I must admit that I'm not familiar with K-example hashcash, but it sounds like perhaps that idea is to average the lowest hashes of individual miners independently? I agree that in that case, low variance would lead to a scenario where only the highest hash rate miners would ever succeed.
However, in our approach, all hashes *network wide* are considered, and the k lowest order statistics are chosen from those. It can be shown that there is no change in the expected reward for any given miner with any given hash power.
@_date: 2017-10-30 20:58:49


This is correct except that, somewhat counterintuitively, the lowest hash still has an expected time of 10 minutes. Higher hash values will have shorter expected times, and for a fixed target, the whole mean of the 10 hashes will have a longer expected time than 10 minutes. So the target must be adjusted accordingly.


Yes, that's what we had in mind.


The lowest hash gets to assemble the other hashes (which we call "proofs"). He'll only keep those hashes necessary to get the mean below the threshold. The others are left out.


No, good question though. When producing a hash value, each miner works on his own block with associated set of transactions. But only the lowest hash (the one that assembles all the other hashes) will add his block to the blockchain.
EDIT: Fixed typo
@_date: 2019-05-29 14:31:29
congrats on the new protocol. I just wanted to defend graphene a little, particularly with regard to transaction retransmission and failure rate, based on some [recent performance results]( In this test, which covered more than 500 blocks, we experienced 2 failures and needed to request missing transactions 4 times. The failure rate is tunable, if slightly lower compression is acceptable, then the failure rate can also be lowered. Currently we have it tuned to fail roughly once a day. As you pointed out, the failure rate was previously much higher prior to the release of some new heuristics for estimating the degree of synchronicity between sender and receiver mempools.
Also, I don't know much about FIBRE so please correct me if I'm wrong, but they seem like orthogonal / compatible technologies. I don't see any reason why a graphene block could not be sent over the FIBRE network.