@_author: thezerg1
@_date: 2016-08-02 14:36:56
Its not the meeting so much as the date.  It was when the prior HK agreement expired so people who believed in more capacity on the "onchain" bitcoin network had hoped that some miners might start voting for larger blocks.
Since that does not seem to be happening, a lot of long term bitcoiners seem to be abandoning ship.  And I'm not drawing this conclusion from the price drop, but the activity on "that other reddit forum" and on the creation of a new reddit forum that deals specifically, operationally, on how to split off from Bitcoin Core and already has "100 users here now".
@_date: 2016-08-26 01:38:59
Yes but POW (modified by doing it on data with a uniquifier) is a subset of the possible algorithms which is why I tried to describe it generically. 
@_date: 2016-08-26 01:13:37
I don't think so.   The majority of people in developing nations can run phone wallets.  If we had 100 million users and 1000 SPV wallets per full node that would still be 100000 full nodes.  Great decentralization compared to to today!
@_date: 2016-08-12 01:05:22
A miner should be able to use whatever fee priority algo it wants.  And a txn that provides a 64 to 1 reduction in utxo space should be very high priority...
@_date: 2016-08-30 13:44:46
This is an awesome story but I'm not sure if your conclusion tells the whole thing.  
First of all, you grabbed the "low-hanging fruit" in terms of efficiency improvements.  Things would get harder much quicker if you tried to weave clothing or smelt iron.   Secondly your eventual optimum productivity was probably not enough to pay for a major surgery later in life or even the x-ray and resetting of a broken bone.  I guess what I'm trying to say is that societies' enhanced productivity is not entirely wasted on 3rd party inefficiencies... it goes into a longer lifespan and more energy intensive lifestyle -- the daily commute, and the weekend ski or beach trip.  
And don't get on an airplane -- a several thousand mile trip is what? maybe decades of subsistence energy use, and that completely ignores the R&amp;D and support infrastructure required to produce the modern aircraft transportation system.
@_date: 2016-08-26 01:21:10
There are techniques that basically calc and store a hard to calc easy to validate fn on a node id + every byte of the blockchain.  Then you req a few random blocks and validate them and that the response occurs quickly.
@_date: 2016-04-14 10:47:16
Seems like u dont disagree with core message tho -- VCs can be poisonous to a viable but "percolating" idea due to the funding model which infuses a lot of cash quickly but then expects rapid growth. 
@_date: 2016-08-02 16:11:38
could be... but the sub was created a few hours ago AFAIK.  233 seems pretty good for a few hours old.  
A wise investor would take inputs from diverse sources to make investment decisions:
 
* price appreciation after Jihan's comments in june  
* price fall after Aug 1, 
* ethereum price activity, 
* tone of that other reddit forum
* creation of that new sub
* the disappearance of online presence of key onchain members
* recent comments by tom zander (classic maintainer)
* comments from the Core team about their recent meeting &amp; forthcoming notes
* SW software analysis
* what block versions are being hashed
@_date: 2016-04-26 21:06:48
from reading the white paper:
it seems to be a huge java toolkit for P2P and micro-payment apps, including features that create a component marketplace for the development of the app itself.
Has a vaporware appcoin with issuance similar to Bitcoin that "started" in April 2014, so millions of coins have essentially been premined and paid (at this point, promised I suppose) to contributors.
@_date: 2016-04-16 13:27:17
Sometimes you have to make something to attract enough interest for a "lay person" to spend the time to analyze.   Also sometimes its completely obvious to person B but not to A so doesn't get specced.  Hopefully the OB team will be responsive to these comments. 
@_date: 2018-01-15 04:00:00
The purpose is to enable the software technology.  Clearly  the physical network and typical hardware is not there yet.   These other technologies will limit the block size through the mechanism of miners setting rational limits creating a fee market where fees are actually reasonable.  This will be as good as any crypto can offer, because we are all stuck with these fundamental physical limits.
@_date: 2018-01-03 13:25:45
Retail investors couldn't get into pre-ipo...
@_date: 2018-01-03 13:31:03
Overtaken would be fine I think.  But it would be very bad for alts if it failed catastrophically either technically, chain death, or price collapse.
@_date: 2018-01-15 15:57:16
Its just terminology.  Even if you send A&amp;B in one message I still see it as an extension block. If you send multiple files in an email is that one file?
Other readers, when a project has to resort to people like this you know the end is coming.
@_date: 2018-01-15 02:37:13
1 GB
@_date: 2018-01-15 14:43:26
I am of course well aware of the signature extension block and how little it is being utilized.
Having the dragon's den use their SEO techniques against me just shows their fear and willingness to engage in disreputable practices.  Meanwhile value is pouring out of BTC into altcoins and back into fiat.  Reasonable people are sick of these antics.
@_date: 2018-01-15 14:35:11
Layer 2 tech is not at all rejected.  We believe that it should be tried in a competitive marketplace and will likely find some use cases.
@_date: 2018-01-15 02:44:03
It'll handle 8mb that like its nothing.  Pay attention and learn how stupid the 1MB limit is.
@_date: 2018-01-08 01:29:30
You are just repeating the propaganda.  I'd recommend that you actually look at the history of the engineers working on it and make up your own mind.
@_date: 2018-01-08 13:39:47
Look at the commits across XT, BU, and ABC and then research the committers technical and educational  background.  I don't have time to do it for you.
@_date: 2018-01-03 13:44:25
Salty much?  All you had to do was compromise in the smallest way and bitcoin would have had the value, the businesses, and the users that's flowed to alts.
@_date: 2018-01-08 01:22:19
And BU almost got there beforehand with 45% actually running large block capable software as opposed to 90% promises.  Adoption was ultimately stopped by certain core affiliated miners, and critcally by f2pool which has significant gpu mining capacity (and were the first to leave the S2X IIRC).  Without simultaneous adoption of 2mb and segwit I did not feel that BU could get behind S2X, and so here we are with 2 coins.  You guys will either fire the authorities that are behind all these poor decisions or the market will do it for you by "firing" the entire coin.
@_date: 2017-12-19 12:46:11
It may reenable bitcoin for small purchases, but no one can use btc for that now.  However, to make N small purchases, a user must open/close channels so 2+ more tx.  So, more on chain demand...
@_date: 2017-12-14 19:36:46
This, by the way, is the crux of the "large blocker" argument.  You think we don't like Lightning, Schnorr, BLS or other ideas to take transactions off-chain or make them smaller?  
Think again.  We simply want to keep onboarding users and use cases while these currently experimental technologies mature.  
And BTW this is many people's philosophy who are supporting the unnameable fork, so if LN (or whatever) is wildly successful (and maintains certain properties like decentralization), it'll be ported over there very quickly.  So the users that BTC has lost will stay lost.
@_date: 2017-12-16 14:14:40
In related news, the National Association of Tree Farmers is suing a famous tech company for stealing their product name.
Luckily the word apple (when used for fruit) is not a trademark.
@_date: 2017-12-16 14:15:28
No we want you to control the block size.
@_date: 2015-12-28 18:00:15
Miners do not have to add transactions to blocks.  Rather than let the free market and miner's block size choice optimize miner profitability (and therefore security) vs transactions in blocks, let's let and his cronies do it.  Because they know best. /s
@_date: 2017-12-14 12:18:00
They went for gold but found California.
@_date: 2015-12-04 17:36:24
As I was trying to say in my OP, you are using the term "worth" with no direct object like it actually means something universal rather then as a relationship.  Better to say: stability between 2 entities (A,B) is defined as there exists a function of time F(t) that relates the price of A in units of B with an error less than lambda*t.  
This defines stability between USD and oil for example.  But this is not that useful to people or companies because they have diverse living/operating requirements.  So now to make definition of worth and stability without the direct object (the "oil"), you must use the summation of C*Ffood(t) + D*Foil(t) + E*Felectricity(t) and so on.  The constants C,D, E... reflect your use of each commodity. This is the "basket of goods".  Unfortunately everyone's basket of goods is different, new products enter the marketplace all the time, and economics people who define the makeup of the "basket of goods" are susceptible to the temptation to adjust the basket quantities to produce a desired outcome.  This can easily be done over a short time span (say years, but not decades) because even though prices of most goods may be trending upwards, they do so with a lot of noise.  And technological improvements cause dramatic reductions in the prices of some goods.
@_date: 2015-12-29 14:58:26
I would be very happy if miners chose XT or BIP101.  BU generates blocks that indicate BIP101.  Its essentially Bitcoin Core + traffic shaping + choose your own block size.  And on a slightly different topic, Miners should WANT non-mining full nodes to run BU -- have a lot of BU nodes reduces the chance a block will be rejected for some pedantic reason.
But to address questions directly:  Single transaction (empty blocks) are a "get out of jail free" card causing a miner to only lose .5% to 1.5% of possible revenue (based on today's typical txn fees).  They reduce network's average block size, and can be utilized to mine in low bandwidth areas.  At the same time, a block sequence of sizes 3N,0,0 is arguably better for users than N,N,N because more transactions are deeper sooner.
You end up with a network throughput (transaction commitment capacity) equation that looks like this:
 Th = Q/ (Q/z) + T 
Where Q is the average block size, T is the 10 minute expected block discovery time and z is the "propagation impedence" (or how many kB/s of transactions on average the network can send and validate)
My analysis of a year+ worth of single transaction blocks indicate that the bitcoin network is limited to 60 kB/s.
See 
Additionally, the Keynesian beauty contest analogy is flawed because:
1. miners can generate blocks conservatively but accept blocks liberally -- there no real metaphor for this, maybe its like saying "I vote for X, but I benefit if any of the candidates win" 
2. miners can put the max size they will accept in the coinbase transaction.  It would be nice to have a standard for this but its not necessary.
3. People claimed that 90% of the hash power was up on stage during the last Blockstream conference.  They can email each other (communicate before voting).  Small miners probably shouldn't be the guys breaking new ground by generating blocks bigger than any seen before.  
@_date: 2015-12-30 18:40:11
Atomically? 
@_date: 2015-12-08 13:15:21
yes, but who would be generating the old transactions?  My understanding is only old clients.  The new clients will generate all their transactions using the segregated witness format, right?  
This is very different from a new transaction TYPE (for example, multisig) that enables specific functionality and constitutes a minority of transactions on the network when initially enabled. 
@_date: 2015-12-07 20:13:57
I have a very hard time seeing this as a soft fork, if I understand it correctly.  I mean, in a soft fork you add some new txn type and basically old clients won't understand it but that's not a disaster because its expected that only a small % of the txns use this cool-but-just-getting-deployed new feature.  If you want to use this new txn type, then upgrade.
But if I understand correctly segregated witness would be applied all transactions.  So basically an old client would not be able to understand ANY of the transactions in the block since they all now use SW.
So its basically its a hard fork with a candy coating.
Don't get me wrong, I think its a cool idea.  But let's just hard fork so it can be done cleanly and fix up a bunch of other stuff at the same time.
@_date: 2015-12-23 02:44:00
Bitcoin Unlimited tracks consensus as defined by the mining majority regardless of block size. Its a vote that tells miners, exchanges, and everybody else to "just do what is best, I will follow along". So it essentially removes the block size from the consensus, allowing miners to decide what the think is best. 
This release of BU is based off of the Core 0.11.2 release.  It has the block size change and offers off-by-default traffic shaping to help residential nodes cope with the increased traffic. 
You may interpret this acceptance of any sized block as a vote for every block size increase compared to what we have today, but it is not -- it is a vote to follow the miners no matter what in order to MAINTAIN a single blockchain.  For this reason it cannot be considered in any way a fork or "alt-coin".  It is less likely to fork than XT or Core.
@_date: 2015-12-23 02:23:44
When the smell hits me I just can't stop myself... Teriyaki mmmmmm... you better start wrapping those packages in mylar! :-)
@_date: 2015-12-26 17:20:15
Ethical people recuse themselves when they have a potential conflict of interest.
@_date: 2015-12-30 18:06:37
What if bob took from Alice and doesn't pay Carol?
@_date: 2015-12-06 01:44:33
Except that your SPV wallet can connect to multiple unrelated nodes.
Except that if you cannot afford a node all by yourself, you and your friends can pool resources and get one.  Everyone can get together during installation to make sure that an unmodified version is actually run. 
Except that the fact that you can do the two above things -- the permissionless nature of joining the network -- is as valuable or even more valuable than the actual doing of it in keeping people honest.  This is similar to how having the code on Github as OSS keeps developers from committing stuff that would ruin their reputations -- because people can and eventually will see that commit even if every line is not being scrutinized on a daily basis.
@_date: 2015-12-28 18:21:26
With Bitcoin Unlimited there is no authoritative "we" that can effect a compromise and no object of compromise.  That is fundamentally how it differs from Core.
There *is* no block size limit.
And each individual sets his/her "excessive" size and communication bandwidth as he/she sees fit. 
@_date: 2015-12-22 19:30:18
Wow... TIL some dude (and I'm SURE its a guy) bought $73 bucks worth of beef jerky! :-)
@_date: 2015-12-02 14:22:03
First define "stable".
Realize that there is no absolute stable -- it is always measured against something else.  Even buildings are only stable relative to the earth, which is in constant motion.
The USD has been very unstable as measured in barrels of oil lately.
So its actually really easy to make a decentralized and stable crypto currency -- just think in terms of BTC and BTC will appear stable while everything ELSE fluctuates.
But ignoring that, I think you will find that there is no universal definition of stability that can be applied across people and time.
So you need to create a system that includes humans to do non-algorithmic stuff like identify emerging technologies.  If the people included in the decision making do not include ALL the people using the currency then those who make the decisions can do so in a way that advantages themselves and disadvantages everyone else.  Voila the banking industry.
OK, maybe with perfect economic knowledge (which is almost possible today but not desirable) you could create a monster "basket of goods" that perfectly reflects the average person's economic activity.  However, a particular person may have very different economic activity and so from his/her perspective the currency is again unstable (or everything else is unstable which as I showed above is just looking at the situation from another perspective).  For example, if you live in Minnesota (cold) and commute an hour to work each day the falling price of oil has put thousands of dollars in your pocket, as compared to someone living in Florida and working from home.
@_date: 2015-12-30 22:34:08
LN is cool... I hope it happens but not at the price of 0-conf and on-chain scalability.  I'm thinking dynamic second-by-second payment of bandwidth.  Stuff that's impossible to do on-chain.  BTC proves "network neutrality" -- pseudo-anonymous payments make it impossible to preferentially treat some customers.  See: 
@_date: 2015-12-27 03:30:57
This is a fascinating idea to which I have given much thought.  In one (poorly conceived) sense you are right.  Let's say that we are debating the deployment of Uber within a city for example.  Should taxi drivers recuse themselves?  The answer is "yes, if you want the BEST human transport for hire service".  
So if your mandate/power is "let's pick the best alt-coin for all users", holding bitcoin is a conflict of interest.
But what if your taxi question is "how can we make the taxi service work the best?"  In that case, should we trust the opinion of people who ride taxis, those who work for Uber and occasionally ride taxis, or people who live in the country?
In other words, you are completely wrong.  Holding Bitcoin is an *alignment of interest* in Bitcoin's success and so the people guiding its development should be owners and users.  But at the same time those very people do not have the overall "good of the people" in mind.  So holding Bitcoin is a conflict of interest if (for example) you are a government official tasked with choosing which cryptocurrency to add as official tender.
@_date: 2015-12-31 01:40:49
Can a client make a guess about how long it will take to validate a block?
@_date: 2014-10-09 01:17:19
I'm trying to convert, but VLC is only doing it at real time so its gonna take at least 2 hrs...
EDIT: Ok somebody beat me to it: 
@_date: 2014-10-08 17:26:27
July 9, 2012: 
If the bitcoin protocol evolved into a fractal blockchain, where the larger, slower, blockchains do not verify every transaction of the smaller, faster ones it will solve both interplanetary (extremely slow) and regional (high frequency) transactions.  Note that on a single planet, "regional" would not have to refer to a physical region -- it could be any social network; in fact "regional" blockchain membership could be created automatically by looking at transaction history of the parent blockchain.
Essentially we'd end up creating blockchains both slower and faster then the current one. 
It would be possible to do this with multiple independent "coin" chains but then you'd have the friction of independent markets that trade these different coins.  Just like we have today with gold -&gt; USD.  This is what "litecoin" is attempting to do.  A "better" solution would be to allow the same bitcoin commodity to transfer into and out of the slower and faster blockchains and have the greater blockchain verify some invariants (such as total quantity) of the lesser blockchain -- but not verify every transaction on it.  An additional advantage is that periodically the leaf (final state) of every account in the lesser blockchain could be "committed" to the greater chain, and then that entire chain (the entire history of transactions within that chain) could be restarted.
If you are confused, imagine a "tree" of blockchains...
This architecture relies on the premise the the network has the "scale" property.  That is the premise of localization of payments (i.e. that the majority of payments happen locally); if the average user paid randomly across the entire network (a "scaleless" network), the largest blockchain would see more traffic then the local ones and there would be little point in a fractal design.  I think that bitCoin is "scaleless" as used today, but will become very scaled if mobile payments take off.
A big problem with shoehorning this into the existing system today is the BTC mining award.  There is no way today's blockchain could recognize coins "mined" on other blockchains... so the lesser and greater chains would have to rely entirely on transaction fees.
@_date: 2014-10-08 18:12:38
Please remember that this proposes the maximum block size.  You'd only be seeing large blocks if bitcoin either took over the world's M2 -- in which case the success would be so great this becomes a small problem -- or someone was burning txn fees by spamming the blockchain.  But that would get expensive and accomplish little.
@_date: 2014-10-08 17:53:35
Trustless micropayment channels are only useful for repeated micro payments to a single entity.  Like pay-as-you-go bandwidth.  So they are not true micropayments like you would need for reddit tipping (for example).
@_date: 2017-02-03 12:49:49
Not really true.  High tx fees limits utility, and who will run a full node if it is not usable by the node operator?  There is no proof behind the idea that increasing block sizes caused fewer full nodes.  More likely it was caused by the rise in phone wallets and poor performance and capabilities of the Satoshi wallet.
If millions more people can use Bitcoin but only .001 of them run full nodes that's still a lot of new nodes.
@_date: 2017-02-07 19:11:38
just don't start believing your own spin.  you might get an ugly surprise someday...
@_date: 2017-02-03 17:24:00
all of this data seems to be pre-xthin and expedited blocks.  My data correlates pretty well with the final data in your link, but if you look at the slides below you'll see that, the expedited block technology can get from China to London in &lt; 200ms.
@_date: 2017-02-06 15:13:36
I don't think that u/cryptoconomy really understands the purpose of the EB/AD settings.  But to understand them, you first need to understand some stuff about consensus works.  
Basically, any higher layer mechanism to determine the block size, feature activation (like SegWit), or whatever is a consensus algorithm.  You are trying to come to agreement about something, and so if you do it without a centralized authority it is by definition distributed consensus.
Now, if we HAD distributed consensus to make these decisions, we would not have needed Bitcoin's consensus mechanism!  But we don't.  All of these higher layer consensus algorithms are flawed.
For example, if 60% of the network turned on segregated witness today then Bitcoin would have segregated witness today, irrespective of the 95% vote consensus algorithm designed in Core.  
Similarly if 60% of the miners "vote" for segregated witness but are lying (and say 45% vote truthfully so activation occurs), then when the time comes to activate segregated witness, it won't activate.  
Interestingly, this dishonest strategy could be extremely lucrative in BTC terms since all the money in the segregated witness transactions could be taken by any miner.  However, we TRUST that the miners and the ecosystem won't do these things.  We choose to trust them because these actions would likely dramatically reduce the value of a bitcoin, causing the miners to lose money even though they may gain bitcoin.
But if we are already forced to trust the miners as a group to responsibly upgrade the network, what is the point of a voting scheme?  It is simply to help them to organize the transition, and to announce non-binding intention.  But they can very easily use alternate mechanisms -- like people talking to people -- to organize an activation date.  And this may even be "better" because it would allow important companies who are not miners to have a say.
So what you are missing is that miners/ecosystem players are expected to organize themselves and agree to a simple upgrade plan (depending on how "algorithmic" the plan is, you could even run a script on top of BU to implement it).  You can search around for postings by miners for some of these plans. 
Posting your EB/AD in blocks and nodes is for informational purposes only -- BU does not fool itself into thinking that cheating cannot occur, and so these statements are not a vote that triggers activation.
  
And the EB/AD algorithm is a "last resort" mechanism to limit losses in case a higher level activation plan fails.  Finally, note that Core is a subset of what it possible with BU WRT the "EB/AD" fields.  Its equivalent to running EB=1.0 and AD=infinite (it sticks to 1MB max block size no matter what).  So in the case of an unexpected change, losses when running Core are theoretically infinite (in practice miners will upgrade their software), whereas miners running BU will have worst-case losses of AD blocks.
@_date: 2017-02-08 01:03:02
Exactly.  You are being served a dish of FUD. The software offers no additional attack beyond what is already possible. 
@_date: 2016-03-02 13:05:43
No I'm saying that as daily individual use declines, replaced by inter institution settlement it will lose value.  Probably not gradually either.  It'll be a cliff.  Suddenly no institution will accept it... normally at this point no big deal -- bank just sells it into demand.  But if bitcoin becomes a settlement network there will be no demand. So it is very important that L1 continues to create individual demand while an L2 denominated by the same token slowly starts filling individual demand.
@_date: 2017-02-03 17:33:35
light clients is just one example.  others are the centralization of mining, and the maturation of cryptocurrencies -- weekend hackers need to be replaced by real users, user's groups, or companies.  In ethereum's case the bubble pop and DAO embarrassment let the wind out of the sails.
My point is that there are simply too many variables to allow one to point at one particular variable being the main cause of node count reduction.
@_date: 2016-03-09 13:29:37
Ahh the smell of napalm in the morning... someday this war's gonna end.
I feel sorry for you guys.  Forced into weak arguments and justifications out of foolish pride and not-invented-here.  But on the other side I say "libsecp256, univalue, LN, segwit, headers first mining?  Great! Thanks"
@_date: 2017-02-03 13:43:28
Lately it takes 100s of milliseconds to propagate blocks to miners so perhaps the situation has improved wrt faster blocks...
@_date: 2017-02-06 16:26:23
this was a statement by a miner, not by Bitcoin Unlimited.  I doubt most members would support killing a small block chain...
@_date: 2016-03-03 16:16:22
Prior artificial and temporary surges in txn rate have no relevance to this situation because we are not arguing that there is a technical problem.  Quite the opposite actually... we know bitcoin can handle the load and all that those experiments proved is that we are correct.
We are arguing that people will stop using the system.  This should be obvious.  Btc is no longer useful for small payments. 
Another indirect data point is the big surge in complaints about unconfirmed tx. 
@_date: 2016-03-06 13:19:54
Most people I think realize this.  Many are unwilling to see bitcoin give small fee (where how small is "small" is unknown but could be thousands of USD) transactions to some other coin.  Especially since such a coin could transfer large txns just as well as Bitcoin so could easily end up taking over since most people start small and move up.
@_date: 2017-02-21 04:08:39
In octal because thumbs
@_date: 2016-01-30 20:38:45
But space companies make the most powerful first stage allowed by physical reality.
@_date: 2016-01-24 21:18:56
What's awesome about this is u can atomically transfer the ticket and pay in a single txn
@_date: 2017-02-07 13:28:53
The bu crowd is fine with 2nd layer solutions.   Just don't force it by artificially limiting 1st layer.  Let the market decide.
@_date: 2016-03-15 15:28:09
Given all of the time and effort required by me and others to correct and refute your endless postings about this the least you can do is actually research your topic.
I should be sending you a bill.
@_date: 2016-03-15 15:05:58
No.  You'll then just know 1 additional tidbit of information but still be wrong about everything else.  Go, study, and learn before you start making authoritative statements.  
You don't see me making statements about the relay network, although I'll bet I'm more of an expert in it (I've run it on a few separate instances and modified the source code a bit to log exactly when a block arrived for my empty block research) than you are in xthin blocks.
@_date: 2016-03-02 01:40:47
A few reasons:  To be a settlement system the token must have value outside of the system.  Slowing growth at this point jeopardizes that.
LN needs bigger blocks to even scale to a small country. 
1mb is detached from reality.  Networks can easily handle it. other coins will take advantage. 
L2 solutions do not exist yet.
Dismissing fee paying transactions as spam is narrow minded.  No analysis has been done on these txns.  Paxful was posting how ppl were buying a buck of btc for online ads for example...
An artificial fee market reduces utility because bitcoin handles zombie txns poorly.
@_date: 2016-03-06 13:58:05
For better or worse there is one governance system defined.  Nakamoto consensus.  Read last para of btc white paper.  Given this diverse worldwide community no other system has the authority or capacity to mediate.  
Attempts to use other systems are the techniques of the loudest voices and biggest guns -- exactly what you are trying to avoid..
@_date: 2016-03-03 13:16:17
You ask an impossible question since Bitcoin is the first cryptocurrency and this is the first time txn demand exceeds supply.  I could easily turn your question around and ask for hard evidence that holding off on and increase will NOT be disastrous.  No one will be able to give any.
But there is a lot of "soft" evidence such as the rise in market cap of alt-coins and how recent job postings and announcements have focused on integrations with alt-coins not Bitcoin.
@_date: 2016-01-29 01:15:02
Let's let the readers decide.  These are the last 2 sentences of the Bitcoin Whitepaper:


(emphasis added)  It absolutely IS "we make the rules up as we go along coin".  If the rules need to change, mining majority is the ultimate consensus mechanism and is only restrained by economic laws and the economic majority's willingness to follow the mining majority.  And don't argue that the rules aren't changing -- Core is making lots of changes and Satoshi himself is the one who changed a rule to ADD the 1MB limit in the first place.
Money itself is "we make the rules up as we go along coin".  It is inherently a social construct (that is, a set of rules "made up" by society).  While some money (gold, copper) has "intrinsic value", that value is often much lower than the value of that quantity of material used as money and it often interferes with the money function, or the money function interferes with its applicability to other uses.  In other words, people don't gold-plate many electronics because doing so is literally painting it with money.  Therefore, the perfect money is simply and fundamentally a distributed memory.  Its how society tracks and chooses resource allocation over time.
@_date: 2016-03-27 19:39:59
Its not so much the fee at 5cents.  Its the txn supply inflexibility.  How can you plan a business and project for expansion when blocks are full?  How can you project txn fees when txn supply space curve is vertical. 
@_date: 2017-02-03 17:17:54
correlation is not causation.  There a lots of reasons why node count may be dropping.
@_date: 2017-03-15 12:51:57
Its not faked.  You can see that the OPs shot also shows fewer clients.
@_date: 2017-03-15 11:57:15
The screenshot  is for convenience its not meant to be authoritative.   Just check one of the sites the provide historical data as I suggest for the one of the incidents.  But note actually check numbers (by hovering) because 250 nodes down out of 5000 is hard to see on some graphs
@_date: 2016-03-23 01:55:06
I think with thousands of readers votes can happen pretty quickly.  But it would be interesting to compare votes from minutes 0-N to votes from minutes N to 2N, because presumably automated bots will refresh and vote quickly... 
@_date: 2017-03-13 17:13:24
The pruning issue is fixed in our dev branch.  The only effect of this issue is that actual pruned data size may be larger than the node's configured target, especially when that target is very low.  This issue affects very few (if any) nodes.
  
The release branch and the minor release cycle is for small impact (like documentation) or critical fixes.  I did not originally consider meeting the node's exact pruning target important enough.  If actual users (rather than people looking for any excuse to criticize) request inclusion, we will certainly look into merging it for release into 1.0.2.
@_date: 2017-03-15 22:11:55
Nothing was cut.  The images were not modified in any way.  If you look at the Firefox container, in one image there is the "file edit..." header, in the other there is no header.  AFAI remember this is because I dragged a firefox tab out of the window to create another window.  So you are looking at 2 different instances of the same web page.  If you look closely the space taken up by the "file edit ..." it is exactly the same size as the blank that disappeared.  Its like when the menu bar was added the working space in the window got squashed and the graph redraw has a bug.
And clearly there are bugs in the text display as supermario was able to reproduce.  Although he was unable to reproduce the exact issue seen in my images, that does not mean the my issue does not also exist.
But the top grey satoshi 13.2 data is clearly there even though the text is not, and the BUIR doesn't make any claim that is relevant to these image differences.  
Honestly, you are nitpicking about irrelevant details...
@_date: 2016-03-15 12:38:05
mmeijeri said:


Oops!  You broke the first rule of trolling: never say anything refutable.
You're wrong.  And you clearly don't know the most basic information about Xtreme Thinblocks yet you persist in posting everywhere about it -- I hope that is an anonymous account because you are making a fool of yourself.
@_date: 2016-03-22 20:36:32
might help if you told us your city/country
@_date: 2016-01-29 02:43:36
Its sad that you can't distinguish between when Satoshi was making recommendations and when he was talking authoritatively -- that is when he is making statements about how the system mathematically works, as compared to what he would prefer (which is everyone to use his client of course).
Anyway, the point of this entire thread was not to appeal to authority but to point out that Gavin's comment ultimately comes from the Bitcoin whitepaper not from Coinbase.
I am a long time bitcoin user who has my own interests at heart and so probably yours as well in the sense that our interests are probably aligned.  
Let me leave you with the thought that with Ethereum (technically more sophisticated and versatile),  Monero (more anonymous), and Litecoin (scales bigger, settles faster) waiting in the wings, the only thing that Bitcoin really has going for it is its huge network advantage -- its greater adoption.  If you stop or slow the adoption rate, if you drive users onto another network, you will lose the only advantage the Bitcoin has.
@_date: 2017-03-08 00:53:40
Please be civil.  There is no need for anger as the core community investigates whether there is internal willingness to explore a compromise. 
@_date: 2017-03-14 19:17:46
Peter, this is not responsible disclosure.  FYI, we have contacted Core developers about a bug whose effects you can see as approximate 5% drop in Core node counts on Feb 23, 2017 and Mar 6, 2017.
Although we disagree about the block size, if you care about Bitcoin I think that you should practice responsible disclosure.
@_date: 2016-01-08 19:40:55
Well, when your visions of uniformity and consensus stall please consider being flexible on block size.  That way your solution will interoperate with every other large block solution.  Its a way for us to be united and opinionated simultaneously :-).
@_date: 2016-01-29 02:23:17
First let me comment that I started this reply thread because the OP came up with some stupid ad-hominem attack on Gavin implying that he's just regurgitating stuff that Coinbase said.  The point is that Coinbase didn't say it... Satoshi did and Gavin and Coinbase are both referring to the original Whitepaper.
And yes appeals to authority are an unconvincing form of argument which is why I added a lot more rational and convincing arguments.  However, if you are going to appeal to authority, the context of the discourse matters a LOT.  Some random email matters not at all compared to the carefully conceived whitepaper.
Additionally, your quote actually reinforces my position -- did you read it carefully?:


Read the bolded section.  Satoshi is CLEARLY referring to using the mining majority to determine the "winner" -- note that he's not saying that HIS version will be the winner.
And the portions that speak towards discouraging multiple clients are that its inconvenient, ugly and may reflect badly on Bitcoin -- not that it should absolutely never be done.  Well guess what?  We know it will be painful which is why we have waited so long for Core to wake up to the needs of the Bitcoin users.  Unfortunately that is clearly not going to happen so it is time to use the big hammer (miner consensus) to make the determination.
@_date: 2017-03-10 16:23:49
My response.  Sorry to not put it inline.  But every day people bring up the same issues, so at this point I'm hoping to build a set of responses that I can just link to.
@_date: 2016-01-28 19:11:14
That point was made by Satoshi in the last line of his white paper.
@_date: 2017-03-18 02:16:09
Wow the Core deception is unbelievable.  This is a very significant difference to us.  I basically suggested exactly what you are now saying you agreed to last week.  So I think that it makes lots of sense.  Exchanges should facilitate the market making the decision by allowing trading between the 2 and making good fees off of that.
I hope that you recognize that this trick is eerily similar to the trick pulled on the miners in the HK agreement. 
@_date: 2017-03-07 18:31:09
yes, transaction size limits remain as they are today even though the block size is increased.
@_date: 2016-03-15 00:06:01
Do you have any data to back this latency claim up or do you just like making stuff up?
@_date: 2016-01-08 13:41:17
The only problem with this proposal is that it seems impossible to get the bitcoin community or even the large block advocates, or even a subset of them like the large block payment processors, to agree on a single proposal. 
So I'd suggest that the code be changed to be flexible.  This can be the block generation limit.  But if a larger block appears in the network and miners are building on it, I'd recommend that this client not reject those blocks simply because they do not follow this rule set.  This way you won't all have to switch clients if a different rule set gains mining majority. 
@_date: 2016-06-06 18:18:18
I am working on eXpedited blocks, which is what we conceived of and named the technology where a node requests immediate forwarding of blocks (and tx) from another node around Feb or Mar I think.  
eXpedited blocks works with extremely low latencies when it works.  But if the nodes network-wide are missing the tx that an expedited block leaves out, then it wastes bandwidth or reduces to the 2-phase speed of Xthin blocks.
We are testing it now across our 7 node worldwide BU cluster.
@_date: 2016-06-09 15:16:52
Don't make me out like some kind of corporate kool-aid drinker -- large blocks advocacy is going against the grain of the established Blockstream Core.
But its all the corroborating evidence that makes me more inclined to take evoorhees at his word than doubt him.  But of course hard data is always valuable.  But you know that he could just make up that "hard data" so at some point you have to trust data sources especially when multiple independent sources are pointing to the same thing.  
At some point you have to compare people saying X who have an obvious conflict of interest with those saying Y who have no obvious conflict of interest and who even have an obvious alignment of interest and choose to believe Y in the absence of other data.
Corroborating evidence:
  1. ETH volume and market cap
  2. New services on ETH not BTC
  3. Exchanges integrating ETH
  4. companies pivoting away from BTC
  5. Huge price spike when Jihan said that SW isn't coming without 2MB HF 
  6. Statements from Coinbase and other major Bitcoin companies.
  7. Roger Ver claiming specific knowledge of a fortune 500 company giving BTC a pass due to scalability
  8. My own thoughts about creating any new service:  For example, Hey,  we should create a service that lets you pay for individual NYT articles -- oops no capacity and tx fee already exceeds the cost per article.
  9. new users complaining
  10. My own understanding of supply and demand economics.
@_date: 2016-06-25 18:25:53
He does not understand that we don't want an entirely virtual ephemeral digital currency.  He right, That IS fiat.  We want a currency that has some concrete immutable properties and some ephemeral properties. 
For example a well known inflation and issuance limit (concrete) and instant p2p worldwide transfer (ephemeral).  His dollar has it exactly backwards.
@_date: 2016-06-24 11:52:48
What country are you in?
@_date: 2016-06-06 18:43:57
BU would simply note the collision and request a thin block (the full SHA-256), resulting in slightly lower compression.  
By default, you should take anything not written by the few guys involved in BU with a grain of salt since it is extremely unlikely that they have read the code or even bothered to run BU.
@_date: 2016-06-09 13:18:21
Ok good for you but is running a company that produces hundreds if not thousands of transactions daily.  
I also haven't hit any major snags because I pay a relatively big txn fee and don't care if it takes hours from my txns to clear (I just fire and forget).
So who to believe?  Yours and my experiences, or those who handle orders of magnitude more volume?  I'll believe the higher volume... because evoorhees has no reason to lie.
And his product allows him to watch value flowing between Bitcoin and alt-coins... so if he says he's seeing a migration, I'd believe him over your limited experience as a day trader.
@_date: 2016-06-07 01:10:39
Resorting to name-calling only reflects poorly on the author.
@_date: 2016-06-06 18:37:02
We do not recognise the BIP process as authoritative -- instead it is a fake standards process entirely captured by Core/Blockstream.
There has always been a tension between english specification verses simply getting the job done using "the code as the specification".  While Core has been off specing, we have been running a 7 node worldwide cluster that is pushing blocks rapidly across the bitcoin network, helping to reduce orphans.
It is an amazing coincidence that after so much time Core suddenly decided to produce a competing implementation.  Could it be that our efforts actually drove certain engineers to work on things that are better for Bitcoin, rather than things that are better for companies with products built on top of Bitcoin?
And "skewered" is a very exaggerated statement of the critiques.  BIP152 looks to be pretty much 90-95% copied from xThin, and the few criticisms will be quickly addressed.  
Thank you for your analysis although I question its intent since for some reason you felt it necessary to redesign xThin rather than adopting it with a few small changes.  Regardless, I don't care.  I am happy to accept and utilize Core's hard work, if it furthers the goal of Bitcoin as a worldwide P2P currency.  Rather than reciprocate, if you want to waste your time and money with an alternate implementation of our work I guess its your money to burn.  Not really in the spirit of FOSS though... what will happen if you drive everyone away and then run out of money?
@_date: 2016-06-04 16:22:58
And the major positive movement directly followed jihan's pro-large block statement....
Its like they are sticking their fingers in their ears saying nanana as the market screams large blocks already!!!
@_date: 2017-01-23 14:45:43
I don't know the source or context of this screen shot.
But, I'm guessing that the argument is that any increase of the 21M limit would create a hard fork that creates an inflationary coin of therefore lesser value, that nobody would use.  So this hard fork would be worthless the moment the first coin is mined and Bitcoin will remain valuable and having a 21M coin limit.  So from an academic standpoint, Nakamoto Consensus and the ability to hard fork is more "central" than 21M supply limit.
Regardless, I think that the 21M coin limit is a central property of Bitcoin and will not check code into Bitcoin Unlimited that changes that property.
This post is the opposition trying to paint Bitcoin Unlimited in their own colors:
 (originator of "Freicoin"  a coin which charges all holders negative interest rate "demurrage".  This technique is equivalent to monetary supply inflation)
@_date: 2017-01-30 01:18:10
To briefly root cause the issue, in BU I attempted to put "just a few more transactions" into the block by removing the 1000 byte empty space, and replacing it with 100 bytes of empty space that would be enough for miner's coinbase strings.  However, this change did not take into account the space required for the rest of the coinbase transaction, which I mistakenly assumed were tallied along with all the other transactions.  
So this mistake can cause blocks that are too large if the block is nearly full.
The workaround, which can be inserted "live" is for miners to set their max block size to 999000.
./bitcoin-cli setminingmaxblock 999000
 
@_date: 2017-03-13 17:57:35
This is pretty typical development practice.  Fixes are not targeted to the prior major release unless specifically called out, either by targeting the PR to both branches (preferred for anything significant) or by a comment.  For example:  see comment 
Of course we support people running with limited resources.  I personally have a few ARM machines on my desk (orange pi+).  My personal experience with this issue was that if you pruned at (say) 500MB, you might end up with 1.5 GB.  This is not critical.  However, this fix could have been targeted to the stable branch since it is quite small, but it was not and so it did not get merged.
@_date: 2017-08-25 21:39:21
Please demonstrate it then
@_date: 2016-06-07 01:08:50
If you read my comment, you'll notice that I never said we have a specification.  In fact, I strongly implied we didn't by saying we focused on coding instead.
I did not know that you wrote about this over a year ago, sorry ... but by "copy" I meant to be focused on the similarity (and so why not save time and use Unlimited's implementation) rather than the claim of precedence to this frankly rather obvious optimization, especially since our work is well known to have emerged from XT's.
But that 9 month "gap" from mid 2014 to march 2015 on the Bitcoin wiki history (and sudden flurry of edits in March) basically proves my point that the Unlimited work forced you to actually make it happen.  Nobody "refines the design" with 9 months of silence, and especially for a relatively simple problem like block optimization.
But maybe since I have your attention, you can explain why you chose not to use Unlimited's implementation...
@_date: 2016-11-18 17:41:37
You need to check your bias.  All the code I referenced was in Bitcoin Core.
The linked video shows the bug expressed in the Bitcoin Core 0.12.1 release as downloaded in binary form from the web site.  You will see me start with about 110 Testnet coins, spend 100 of them, get an error message and end up with ~10 coins in my wallet.
@_date: 2016-11-18 12:00:21
We are looking and pulling which is how we found the dd_mutex problem in Core.  there's a PR in BU to pull that code in.
  
Of course, the segwit question is why we are currently trailing.
@_date: 2016-12-08 14:16:03
Actually the description of the problem and a "flexcap" style solution was proposed (before the catchy name was coined) here:
This is the first post about the concept AFAIK (May 5, 2015).  Maxwell's design (Nov, 2015) comes after a proposal by Meni Rosenfeld ( which happened a month after this original posting, in June.
@_date: 2017-01-30 12:42:09
I was thinking that there's space for as many as 5 more transactions per block in there, and we desperately need to cram every transaction we can :-(.
@_date: 2016-11-18 12:43:40
You are correct.  In review the txiter is dereferenced in line 1 so it should not be affected since the iter is to the map.  So my comments are not correct.  This is why this issue did not make it on the blog post.  Yet I am seeing a reproducible problem here and the patch "solves" (or avoids)  it.  You probably haven't seen it reproduced  because it only happens when I run 6 computers generating 30 tx per second each for several hours.  We still need to figure out why...
@_date: 2016-11-18 17:49:50
I cannot spend the time to defeat all of your FUD.  But I will do so for   You are the one that clearly does not understand the code.  In the attached video taken using a plain vanilla 0.12.1 binary downloaded from the site (I still had it on my laptop which is why I used that version), I reproduce the bug.  As you can see, it gives the error code and still the balance deducts from the wallet.  If you understood the call flow you would see how this is possible.  In fact we are very careful about what we change.  For example, the only change we made in BU in the wallet was to fix this bug.
@_date: 2016-12-16 20:59:31
If blocks are not limited to 1MB, other miners will be able to mine larger blocks and therefore scoop up the extra transaction fees the Jihan is leaving on the table.  
Mining blocks smaller than the maximum is a way that the network can "naturally" regulate its speed to that of its physical capacity -- if a miner can't get all the transactions due to network or validation speed limits, he'll generate smaller blocks.  This allows the miner to reduce the average (and that's what's really important) block size in a hash-power-weighted manner.
@_date: 2016-12-08 22:30:38
I think that the basic "big blocker" anti-SegWit argument goes like this:
People with great control over Bitcoin Core have said no to ever scaling the bitcoin block over 1MB via HF, and some have even signed a document saying that they would deliver a HF to 2MB and then failed to deliver.  
Additionally, I think that almost everyone agrees that SegWit functionality would be a lot cleaner done as a hard fork.  So if we are going to HF anyway, why not do a clean SegWit HF plus the 1 line of code required to bump the blocksize to 4MB?
Therefore it seems unlikely that a HF is actually going to happen after SegWit.
@_date: 2016-11-18 12:36:20
Yes I did this reluctantly due to the repeated rude, negative and unsubstantiated comments posted in public forums by people strongly affiliated with the Core project.
@_date: 2016-12-02 12:51:11
Why doesn't a loaf of bread cost a penny?  Its essentially a in a state of perfect competition at this point.  Because breadmakers price their bread above the total cost to produce.  There is no reason why miners won't do the same.
@_date: 2016-12-08 15:51:00
"Attacking" BU in this manner would require a tremendous amount of hash power and money (due to orphaned blocks), the amount increases exponentially as the "accept depth" setting increases.  Any concerned miners can simply increase their "accept depth" setting today.
In the event of a successful "attack", research has indicated that large blocks will either be orphaned or be followed by a series of empty blocks, reducing the average block size to what the network can handle.  So an "attack" will likely simply prove our research.  If you are actually interested in reading about this rather than in demagogy, please read "An examination of single transaction blocks..."  and "a fee market..." papers on our web site  
The implementation of the change is so small that pseudo-code is included in the BUIP.  Its maybe 10 LOC.
@_date: 2016-11-18 12:16:05
I reproduced  on 12.0 core about a year ago.  As I said in the article I have not retested on subsequent releases.
In  I am reading many of you (several posts above, and thebluematt on wechat) justify the use of code that is formally undefined.  A responsible mature organization would simply say "you are correct, thank you for pointing that out".  Your argument that the code is ok because its in debug mode completely ignores the interaction between development in debug mode and production code.  The larger effects of this problem creates other bugs that causes core dumps in production code.
In  there are clearly calls to this function executing outside of the lock. This happens VERY rarely. If you haven't triggered it you are likely not hammering the APIs hard enough.
@_date: 2016-12-08 16:06:59
The flex cap family of proposals provide a cushion that handle short term transaction space supply "crunches" by allowing the payment of higher fees to actually increase supply.  This models similar short-term supply changes in traditional economics -- for example, factories can add another shift, but they need to pay people more to work late at night, so they must charge more for the product.
However flex cap proposals don't model long term process improvements or volume efficiencies.  For example, the Tesla "giga-factory" is expected to increase supply and reduce battery price for as long as the factory is in operation.
The basic issue is that flex-cap proposals allow flexibility (and its typically an exponential function) around a certain baseline.  But that baseline does not change.  So there is still a low asymptotic limit to the "max block size" in the flex-cap proposals.  For example, maybe the 1MB block can be pushed to 1.5MB if fees approach 100%.  This may be why Greg supported it -- it looks like a block size increase but actually does not allow significant scaling.  It just smooths out the bumps in the road...
An algorithm that averages the flex-cap block size changes into slower moving changes to the "baseline" capacity would be very interesting.  EDIT: However, I think that any increase to the "baseline" capacity is not acceptable to most of the "small blockers", but I would love to be surprised!
@_date: 2016-12-08 16:35:39
I read the OP of the post you linked and couldn't see anything directly related.  I'm sorry that I don't have the time to read further right now.
But I am sure that the idea of a changing blocksize was discussed many times.  I think that the "flexcap" proposals specifically address a short term transaction space supply crunch, with the idea that the supply "relaxes" back to a baseline when the demand subsides.  This "supply crunch" concept is well studied and modelled economically.  I'd be interested in prior links dealing specifically about that.
@_date: 2016-11-18 17:43:15
I spent the time to reproduce and record.  Here it is on 0.12.1:
@_date: 2016-11-18 11:51:32
In the 12.0 release you just load up QT send 100 btc and put 1 btc as the fee.  Thats how i triggered it.  IDK about subsequent releases.
@_date: 2016-11-18 12:32:12
The indentation errors were due to tabs vs spaces which should now be fixed in my editor.
There are some pep8 recommended practices that I was unaware of because I learned python before pep8.  And so I prefer 2 space indentation rather than 4 etc.  However as you'll see in the PR review rather than fight these issues, accuse the reviewer of pointing out trivialities,  or whatever, I simply thank the reviewer and fix it.
Also note that no actual bugs were found.   Just formatting and one instance of redundant code that actually came in as a PR from XT.
@_date: 2016-12-08 17:26:34
I was thinking more along the lines of:
    average flexcap effect = (SUM (actual blocksize - baseline) over the last M blocks)/M 
    newbaseline = baseline + (average flexcap effect / N)
N is an arbitrary constant that controls how fast the "flexcap" demand feeds into the baseline change.
So immediate demand and the premiums paid create more baseline supply, in a manner similar to the way it works with physical goods.
That algorithm would also SHRINK the baseline if blocksize doesn't meet the baseline.
If I wanted to be the "Fed" of Bitcoin, that's the algorithm I'd be proposing.  
But it looks like transaction and block propagation times will naturally limit the average block and maximum block size, so no need for the "Fed of Bitcoin"...
@_date: 2016-11-18 12:22:09
If I was able to reliably trigger a remote core dump of course I would have privately and responsibly disclosed instead of posting here. 
What is not classy is your deliberate misinterpretion of my comment.
@_date: 2016-12-08 21:01:57
I think the "small blockers" are a diverse group which is why I said "many".  You are right though that I shouldn't present that opinion as fact.  What do you think is the fastest blocksize scaling the majority of "small blockers" would accept?
@_date: 2017-11-10 20:50:10
IDK it may not be a binary thing.  Bitcoin with its high fees could climb the payments ladder -- today its still quite applicable for individuals making larger purchases.  Tomorrow, maybe just business 2 business payments.  In the future, maybe only fortune 1000 businesses.  Even longer term, maybe only banks...
The other one (I won't say its name to avoid the censor) is filling in the bottom rungs of the ladder, handling the smaller payments that are no longer economically feasible due to bitcoin's tx fee.
@_date: 2017-11-22 21:08:32
The size of the blockchain is correct as a comparison to the upper left graph, but irrelevant to the processor and network speeds.  
But if you put the blockchain size and disk size on the same graph it would be obvious how irrelevant that is because the blockchain data storage requirements are an order of magnitude lower. 
@_date: 2017-11-13 16:08:59
There are always extremists.  Please don't take what you read on reddit by anonymous individuals seriously, on either side.  The last few days were not an "attack".  To me it looked like people who held onto the dream of onchain Bitcoin scaling until its final death rotating into crypto that is more aligned with that vision.
I wouldn't be too worried short term though if I were you.  The coverage of Bitcoin in the media seems to be ensuring that for every person heading to the exit there is someone waiting for entry (see coin.dance volumes).  But if there is a psychological max tx fee and wait time, Bitcoin may maintain its price, acting instead as the "gateway currency" to the larger crypto world (although, this psychological limit is more realistically a "chilling effect" proportional to the tx fee and inconvenience, not a hard limit).
In effect, the shall-not-be-named fork IS onchain scalability for Bitcoin.  Long time holders, change tip, satoshi dice, and daily use can move to it while Bitcoin finds its place as digital gold.  This situation may persist until a transacton alleviating technology (lightning?) matures and gains user adoption, or until Bitcoin loses its first-mover network-effect premium.
Its unfortunate that Bitcoin could not scale onchain until technologies like lightning alleviate block space demand.  If it had done so, I think that a significant portion of the market cap of the major alt-coins would be instead priced into BTC.
@_date: 2017-11-13 18:51:52
I glad you linked to all this stuff.  New adopters, please read.  The worst stuff this guy could come up with is actually quite boring once you get beyond their fervor and search engine optimization (google it) tricks.  Come to your own conclusions.
Look at the poisonous culture here and realize that there is a highly sophisticated propaganda machine and that these people have only themselves to blame for the fractured community and resultant dispersal of value and economic activity into other crypto-currencies.
@_date: 2017-11-10 20:58:12
One side is "for" the deployment of larger blocks AND additional layers like lightning.  One side is for lightning (et al) only.
you go from terabytes of storage to petabytes of bandwidth, strongly implying that you haven't actually done any math since you don't need a million times more bandwidth than storage.  Do the math and do projections.  Ask yourself when 1GB blocks are needed, how big will disks be?
@_date: 2017-11-04 05:30:57
whatever happened to the $1k party?  and now we are talking about $10k!
@_date: 2017-11-13 17:33:00
Segwit has a few percent adoption (I think 8%-15% last heard) and very little GUI support in clients.  Its really Bitcoin's gateway without-a-hard-fork to the massive scaling Lightning may offer if users adopt Lightning.
No images were photoshopped, there is a multi-thread issue in all Satoshi clients that could be rarely triggered by bursting certain messages at clients.  I still stand by all the bugs I reported in that document.  Its all sitting right in the labelled core .12 code, you should actually take a look.  The point of a trustless network is that nodes shouldn't trust.  Classic trusted, they should have ECed...
But, you clearly didn't look all this past history up in 30 minutes.  It says something about you that you clearly maintain an extremely biased list of negative postings.  And that something is pretty grim.  
"Small minds talk about people, average about events, and large about ideas" (or something like that).
@_date: 2017-11-11 15:09:41
Don't worry.  We'll put layers 2 &amp; 3 on top of a scalable layer 1.  Segwit unnecessary when you can hard fork
@_date: 2017-11-13 15:42:22
We spent years trying to compromise.  We finally gave up and are going our own way and lol you call it classless.
@_date: 2017-11-10 21:27:22
When the people behind a product, especially one that grows via the network effect, celebrate expelling a large percentage (nobody knows how much) of its user base, you know that there is something wrong.
However, I basically agree -- the popular momentum behind bitcoin will likely allow it to recover (in price anyway) from the people diversifying.
@_date: 2017-11-10 21:16:46
good thing then that BCH is building paypal 2.0 on top of gold 2.0...
@_date: 2017-11-13 20:43:31
I agree, read.  Unfortunately I don't have the time to rebut every non-contributor who happens to send nasty-grams on the internet.
@_date: 2017-11-10 21:05:48
segwit is completely unnecessary for lightning.  Segwit is only necessary if you won't hard fork.  BCH is willing to and has scheduled hard forks, and will do so again in the future.
@_date: 2017-11-10 20:51:27
think about diversification?
@_date: 2017-11-10 21:20:02
what changed is the people holding out for BTC's network effect AND scalability have finally given up and are moving to a coin that will actually support peer to peer payments.
@_date: 2016-02-19 03:57:50
Not directly.
tx/sec is dependent on block size which is hard coded to 1MB max.  One reason some people believe in this maximum is the thought that these 1MB blocks create bursts that cause problems for users with low bandwidth connections.  This work removes these bursts, which might help convince the people with this objection (but somehow I doubt it, because 1MB is quite small even for home users).
@_date: 2016-02-26 20:54:55
That's a huge hammer to solve a small problem.  A simple possibility is to allow the GETDATA (or similar) message specify a bitcoin address rather than a transaction hash to your peers.  Don't SPV wallets do this?
Anyway, I think we all know that LN is meant for larger things -- 1000x scaling for example.
@_date: 2016-02-18 22:39:37
note "compression" is in quotes... although really the best compression is not to send it at all.
In the same sense that the best way to "recycle" is to reuse (for example).
@_date: 2016-02-19 12:03:49
Possibly a very small incentive that disappears as txs get older.  Most of the "therefores" you used to get to that conclusion are supposition. 
 A more likely but still unproven statement is that very new transactions or hoarded transactions may be disincentivized.
@_date: 2016-02-07 13:05:44
I think you'd find that NH already leans the libertarian way even if some of the big Issues on the national radar get a big "meh".
@_date: 2016-02-18 17:30:06


Yes, but not germane to this discussion.


Yes and they don't know if they can include a txn because they have not received and validated the entire prior block.  Why not?  Because of network and CPU limitations.  This is what I mean by "natural reduction".
I'd cite a formal paper on the subject which looks at both theory and analysis of the bitcoin network but its hosting location will probably get this post removed.  Google  bitcoin 1txn or try this: 
@_date: 2016-02-08 13:56:17
What you are describing is basically called a "checkpoint" but there is no need to create a new blockchain.  Your node just doesn't bother to look back any farther than the checkpoint.
A "better" way to do it is "UTXO merkle tree commitments".  Basically the "UTXO set" is the "now" of the blockchain -- its the list of what addresses have all the money.  So what you do is add a hash of this list to every block.  Now clients can reverse the way they process the blockchain.  Rather then starting at the oldest and moving forward, they can start at the "now" and trace the history of the coins backwards.  Clients can trace it back as far as they want, 2 weeks, 1 year, all time, depending on how important security is to them.
 
@_date: 2016-02-19 11:57:34
Many bitcoin proponents cite as an advantage the fact that mathematical laws and resulting incentives make bitcoin self regulating.  You clearly haven't internalized this idea, does it make you nervous when no one is in charge?
This is the worst miner.  Letting the network regulate itself in this manner gives underperforming miners a small incentive to upgrade while not knocking them off the playing field.
@_date: 2016-02-19 03:24:39
Whether a node already has the transaction or not.  
Interestingly, if you believe that larger blocks propagate more slowly, that slower propagation creates more orphans, and that miners don't like orphans then this work slightly encourages miners to choose older transactions since those are likely well propagated through the network.
@_date: 2016-02-19 01:50:35
Facepalm.  And then you'll have 2 centralized distribution networks... people often "mirror" popular ftp services but doing so doesn't get you bittorrent. 
@_date: 2016-02-24 09:50:30
Turn on traffic limiting.  You'll be fine.
@_date: 2016-02-18 22:22:55
I would guess that the relay network is able to do this (create a transaction ordering) because it is a centralized broadcast network.  You might consider the danger for a distributed trustless p2p currency to rely too heavily on a centralized broadcast network.
Also, we can't all connect to Matt's network without costing him a lot of $ :-).
We already do not transmit the entire hash because such a large hash is not necessary to differentiate between mempool txns.  
We are considering ways to eliminate the bloom filter.  Worst case, eliminating it only increases latency (after receiving the block, the recipient has to get the txns from somewhere).
@_date: 2016-02-26 12:58:10
Also his node clearly was propagating txns to many other nodes.  It is a large branch.  If it was a little twig or even a leaf the results would be very different. 
@_date: 2016-02-26 14:36:50
I think that blocks-only could be very valuable for some use cases.  For example, with pruning, a small merchant could run a native Bitcoin payment acceptance solution on his $100 a year web virtual host, rather than having to go with a payment processor.  
Its a great feature in preparation for on-chain scaling (larger blocks), so I'm glad Core put it in.  Why?  Well, the average web site request returns 2.2MB of data, and the operator needs to be prepared for influxes of hits when he gets media coverage.  So the capacity to cover that means that even this small operator could handle (say) an incoming block of 8 MB or 16MB every 10 minutes or so, without needing to upgrade his virtual machine.
The awkward part might be not being able to provide a receipt to the buyer rapidly though...
@_date: 2016-02-19 04:09:04
read about 
Basically, choose your basis vectors so that the multiples that -&gt; your data are clean, easy to compress numbers. so you convert a string of random looking numbers into a string of very compressible numbers.
this is the math behind a lot of compression IIRC, so you were pretty close.  Your problem is that Pi isn't going to track your data stream for its duration
@_date: 2016-02-19 12:08:40
I have. Read 
@_date: 2016-02-07 12:57:31
I'm pretty sure libertarians are very familiar with the concept and literature concerning "externalities" but not sure if you are because you never use it.
Also WRT selling yourself into slavery its not interesting its boring -- or at least well considered.  In every concept there is the paradoxical exception.  For example, to practice tolerance as a society you need to be intolerant of intolerance. 
@_date: 2016-02-19 01:41:53
Well since the relay network is a centralized broadcast network then if we have its only in the sense of bittorrent being a reinvention of ftp.
@_date: 2016-02-24 15:04:02
You won't be behind.  But you won't be serving too many other nodes either.  I've traffic limited far below your specs for days to make sure it works.  You will need to use "some other" client to get the feature I am talking about though...
@_date: 2016-02-18 19:22:48
I wasn't advocating BU in this case, that's just where the paper is located.
  
Also, BU is not an SPV node -- its fully verifying -- a thin patchset on top of Core.
@_date: 2016-02-19 13:31:19
woah... a 193x and 865x block overnight:
2016-02-19 10:43:46 Reassembled thin block for 000000000000000004e9fd8453dfe5ef4644a21e4aa7052a2fb1903c80b88d9e (934510 bytes). Message was 4828 bytes, compression ratio 193.560486
2016-02-19 11:24:40 Reassembled thin block for 000000000000000001327c3e66faa14b6d84957e25045cce7d7afa2fccf88cdc (999834 bytes). Message was 1155 bytes, compression ratio 865.657166
That kind of ratio only happens when the block contains large transactions.  In this case it is a 1MB block but only has ~100 tx
@_date: 2016-02-18 22:01:06
That is about weak blocks.  Weak blocks are different.  The idea behind them is that if a miner finds a block that does not meet the current difficulty it publishes the block anyway.  This tells everyone else what it is working on, so if it later finds a full solution it can just publish a short message like "my weak block but with this nonce".
And BTW, Peter R's subchains are weak blocks extended by the idea that miners could publish a message like "the block is weak block A, B, and C with this nonce"
This is much less disruptive, works WITH weak blocks/subchains and does not involve miners at all.  It basically converts a block announcement "Block with transaction A, B, C and nonce" to "Block with transaction HASH A, B, C and nonce".  Because clients most likely have already received transaction A, B, C so no need to send them again.  Any client can do this conversion and forward the more succinct representation to other clients that support this format.  
And also I'm skipping lots of detail, for example a client who requests a thin block can pass a Bloom filter saying "these are the transactions I know about, so give me only hashes if the transaction matches, otherwise give me the full transaction."  This is why we get different amounts of "compression" for different blocks...
@_date: 2016-02-19 01:36:10
We have 6 to 10 nodes running worldwide running on mainnet, passing thin blocks and also engaging in normal communication with normal nodes.  Your node may have received blocks from us!
@_date: 2016-02-17 21:59:55
2016-02-17 20:59:20 Reassembled thin block for 00000000000000000103fbd5825a1ef895303c9791115a25974acebdeb797cb1 (949180 bytes). Message was 16580 bytes, compression ratio 57.248493
@_date: 2016-02-19 01:32:51
The txn(s) is requested from the originating node... so this may increase latency.  But it may not because you are comparing a 1mb transmission with a 10k transmission a short reply and a few k response.  In the data from today there were no misses
@_date: 2016-02-13 22:02:04
If you're used to the maple flavored corn syrup that supermarkets sell you've got to try the real thing!  Pour a bit into a coffee mug and microwave for 15 to 30 sec.  Then pour over your pancakes.   Or on vanilla icecream.
I haven't tried this guy's product but his prices are basically what I get direct from sugar houses... you are probably getting a great deal here given he has to ship it.
@_date: 2016-02-19 17:26:09
Infrastructure worked fine, but WRT the video was expecting a lot more for $.25.  Like using the dissolved gold for electroplating as he suggested at the end and other fun uses.  Oh well, amortized over his other videos it was worth it.  
Too bad we have to spend about $.04 for a transaction... if it wasn't for that he could reduce the price per view to a nickel.
@_date: 2016-02-07 12:42:19
IANAL but there are fascinating laws in the US like the state LE does not have to enforce federal laws. But they always do, colorado Marijuana being the notable recent and only exception .  I imagine that if NH libertarians sway NH policy and the federal govt goes the other way this default federal state cooperation may break down.
@_date: 2016-02-19 03:21:38
No the opposite is the case.  By "compressing" large blocks into much smaller sizes, it allows large blocks to propagate much faster.  Probably almost as fast as empty blocks but I haven't made measurements.
@_date: 2016-02-25 00:30:58
BU has always been about unlimited choice... not unlimited bandwidth
@_date: 2016-02-08 13:35:59
A wallet is actually pretty awful as you'll learn if you ever lose it.  But to get equivalent security to a wallet, just download a bitcoin phone app like Mycelium, use a stupid PIN like 12345, and don't bother to write down your backup words.  
But if you want a LOT BETTER security than a wallet for a tiny amount of effort, choose a pretty good PIN and write down the backup words.  That way if you lose your phone, the PIN will protect the BTC on your phone from being taken from casual attackers and you will be able to use the backup words to move the BTC onto your new phone (without ever finding/recovering your lost phone).
@_date: 2016-02-19 12:07:08
This is orthogonal to segwit... it'll crush segwit blocks too.
Yes we already only send the prefix at times
@_date: 2016-02-19 01:37:32
Actually, I think Mike created a proof of concept before leaving...
@_date: 2016-02-08 13:48:40
There are very strong economic pressures that will force a "winner" and "loser" chain very quickly.  The "loser" chain's block creation rate will drop dramatically (because difficulty will be the same but hash rate will drop to 5% (say), meaning that blocks are found on average once every 3.3 hours), making it very difficult to use and also no merchant or exchange will take the loser chains coins so the price of the coins on this fork will crash to zero.
So technically, yes your coins are doubled, but in actual value no.  
By the way, this happens all the time, whenever 2 miners accidentally create sibling blocks.  I haven't looked at the latest incidence numbers but its basically a "daily" occurrence.  These "forks" get 1 or 2 blocks long and then are abandoned.
@_date: 2016-02-18 16:20:59
It is not an "unwarranted drop in transactional capacity".  In fact its the opposite.  It expresses a limitation in the underlying network throughput or validation capacity.  Empty blocks will therefore naturally limit transaction throughput to what miners can handle.
On the other hand, arbitrarily setting a maximum block size is an "unwarranted drop in transactional capacity".
@_date: 2016-02-07 13:14:28
What do you do?
@_date: 2017-10-17 23:44:09
Do people prefer bolivars still or do they want other stuff?  
@_date: 2017-10-26 17:32:02
You might have a pretty unique perspective.  Try it on a stranger.  Tell them BTC is priced at $6000.  First shock.  Then "oh I can't afford that.  I missed the boat".  Tell someone else its price is .006 dollars per bit. You'll get dismissal -- "its almost worthless, not really something worth spending my time on".  Finally tell someone $6.  "Wow that much?", "yes, but you know it could easily go to $100", "really? I should look into it".
Its well known that stocks have a "sweet spot" pricing...
@_date: 2017-10-26 21:42:28
I mean small business accounting software.  Quicken, etc.
@_date: 2017-10-17 18:25:35
Have you tried to get any local merchants to accept it? If so what happened?
@_date: 2017-10-26 17:24:10
The thought was easy integration into existing financial packages which only allow 2 decimal places.
@_date: 2016-10-24 14:01:25
Here is a rebuttal.  Let's see how long it lasts before "moderation":
1. BU is a vote for onchain and offchain scaling; let the market decide and let different applications use the most appropriate technology.  If L2 technologies are viable, they will act to keep block sizes low.  But if we force L2 technologies on the user, bitcoin will fail if they are not viable.
2. Most/all other semi-successful blockchains have higher capacity than Bitcoin.  The average web site is &gt; 2MB today.
3. The OP's has his OWN blockchain that with a 2MB limit (Siacoin).  Do you see any conflict of interest disclosure in this post?  Siacoin is blockchain based storage.  Kim Dotcom claims Mega with Bitcoin is being released in January...
4. As others have mentionned the OP's article is factually incorrect in regards to the BU consensus algorithm
5. As others have mentionned, BU simply makes Nakamoto consensus and free market determination easier.  The OP's argument (if he has one) is not against BU, but against Nakamoto consensus and the free market.  Instead he argues that a cabal of people should make decisions for all, yet fails to propose any mechanics around the selection of these people and how they will pick block sizes.
6. The OP's title "A Future Led by Bitcoin Unlimited..." shows how he fundamentally misunderstands BU's proposition.  Miners and full nodes will settle on block sizes through Nakamoto consensus, transaction space supply and demand, and physical network limitations.  Bitcoin Unlimited will not be "leading" this future...
7. The OP projects a wildly successful yet strangely dystopian future.  To meet the block sizes the OP fears, Bitcoin's sound money advantages must be available and used by hundreds of millions of people worldwide on a daily/weekly basis.  The value of your coins will have increased by many orders of magnitude.  The only (argued) downside is centralization of full nodes.  Would you prefer a Bitcoin that is either unused (see 2), or whose use is centralized into a settlement network only available only to bank-like institutions, or would you prefer a Bitcoin that everyone can use?  So the OP argues that with BU we will somehow be so successful we will fail. In contrast, by limiting the block size, we will drive users off chain, somehow failing our way to success!  The graphs show a different story, which is of value moving from Bitcoin into alt-coins like Ethereum.  Its much easier to fail your way to failure...
8. The OP fails to account for personal storage and network increases.  Today, the Bitcoin blockchain (100GB) fits in $5 worth of disk space (1TB costs ~$50).  Twice today's blockchain can fit on a single flash chip (  Average home internet connection speed is around 5x faster in US and UK than in 2007.  The slowest countries, like Chad in Africa, have an average speed of 744kbps.  So 1MB can be downloaded in 8 seconds in Chad ( on the average home connection.
9. The network connectivy and bandwidth challenges in traversing the GFC (great firewall of china), and reaching mining stations located near hydroelectric plants in the mountains are far greater than reaching most people's homes.
10. Permissionless participation: If full node requirements do exceed that of a user in second and third world countries, user's groups, church groups, etc can still pool funds to afford a full node.  In contrast, with a 1MB block size (and presuming a successful Bitcoin), individuals cannot afford the transaction fees for all but the largest purchases.  Why would individuals run a full node if they cannot use the network?  Also, Lightning nodes fit the definition of a money transmitter in the USA leaving a very good chance that they will be only available with "permissioned participation".  Regardless, the uncertainty of this determination will hamper Lightning, like it did Bitcoin until approx 2013.  And Lightning dramatically favors large Bitcoin holders who have the capital to open channels with many participants.  Participation is proportional to how rich you are. 
11. In a "bitcoin-success" future where users MUST use Lightning due to exorbitant onchain fees, Lightning centralizes full nodes (see 10) and still requires a larger block size to handle a reasonable number of channel open and closes.  Why hamstring Bitcoin's adoption and use now if we must scale onchain in the future anyway?  So you can basically take the OP's entire article starting at "But that's not the bad part..." and apply it to a network where Lightning is the only inexpensive payment choice.  
@_date: 2016-10-10 18:07:42
of course there is.  We've added major new features to Bitcoin Unlimited, lots of minor ones, and lots of bug fixes with multiple releases.  Our first release was last december.
@_date: 2015-11-26 17:10:38
On the video it says if the device is tampered with the keys are deleted.  This implies that the keys are held on the computer and the NFC key simply unlocks them.  This seems weak compared to bitcoin for example.  Anyone know the details about this?
@_date: 2016-05-20 13:35:24
Bitcoins are divisible to 100 million units, and a hard fork could be used to increase the divisibility, in a manner analogous to a stock split.  
Modifying the protocol to produce more causes existing holders to lose value which is transferred to whoever "produces more" (probably the miners), in a manner similar to the inflation of fiat currencies.  Since many Bitcoin holders believe in sound money, the majority of users are unlikely to update their software to include such a fork, which would mean that the main fork -- the one we'll end up calling "bitcoin" -- will remain at 21M
@_date: 2016-05-18 15:20:06
Dammit you're destroying my idiot filter!
@_date: 2016-05-20 14:43:23
And there are economic consequences to an expanding money supply -- just look at Venezuela (and countless others).  Where are similar examples of failure using a highly divisible shrinking money supply?
Politicians and the economists they employ have a strong bias to prefer inflationary monetary policy.  Yet this strong bias is never acknowledged.  Of course, having a bias does not automatically make them wrong, but some revisionist economists have differing views.
Regardless, Bitcoin is without a doubt an experiment with an ultimately fixed or slightly shrinking money supply.  If you want an inflationary coin there are lots of alt-coins to choose from -- dogecoin I think for one.
@_date: 2016-05-23 12:13:16
I find a phone wallet faster and more convenient than a CC.   Just point the phone at the laptop and enter your pin.  Done in a few seconds compared to typing name address cc number expiration etc
@_date: 2016-05-27 11:35:01
If you look around the web you'll surely see the chinese miner news that is driving this rise... ;-)
@_date: 2016-05-20 13:30:25
A single Bitcoin can be divided into 100 million units.  And a hard fork could be used to increase that further, in a manner similar to a stock split.  So I don't think it matters if even 99% are lost through utilization.
@_date: 2016-05-20 11:27:04
Addtl use is good.  Losing dust doesn't much matter
@_date: 2016-05-24 14:03:13
You have to pay for the seller's time.  So if you tell him you are trying it and will buy X more that may convince him.  Or offer a better (for him) exchange rate.
@_date: 2015-11-30 14:23:59
that is not a great definition.  Centralized systems typically have redundant controller nodes or even the ability for any node to assume the role of "controller".  But still, one node is directing the others.
@_date: 2016-05-20 20:31:16
Sorry, I've been giving the cliffs notes version to you assuming that you were not a specialist in the field.  There is not much focus on the economics of bitcoin because it has been/is still being dismissed by most economists.  
Some of us think that the people who want to increase the block size limit is pretty much correlated with people who are thinking a bit about the economics of Bitcoin, rather than allowing technical considerations be the prime decision criteria.  You can find these people here:  although, unfortunately at this point thread used to discuss many topics so you might find the signal to noise ratio to be pretty low.
@_date: 2016-07-22 19:27:01
I have heard that the people who measure these things mark the dollar as having failed on Aug 15,1971 (when is was no longer redeemable for gold), and was replaced 1 for 1 by a new currency (federal reserve note) that we also call the dollar...
So it has lasted longer than average (45 years) but not as long as you were probably thinking...
@_date: 2016-07-20 14:39:44
Next, install Gyft and use your BTC to make daily purchases.