@_author: ja282
@_date: 2015-08-04 18:24:42
Allowing the maximum block size to increase in line with the average load on the network answers a lot of problems, and allows the maximum size to grow without having to predict it years in advance.
Possibly something along the lines of this.
    // average size of the last 2016 blocks, plus 1 Standard deviation.
    average(sizeof_recent_blocks) + stdev(sizeof_recent_blocks)  
An approach like this also brings problems.  A greedy miner with large hashing power could artificially pump up the maximum block size with spam transactions.  Then when the blocks are so large that small miners with limited bandwidth can not keep up, the greedy miner then has seized a larger proportionof the hash rate.
@_date: 2015-08-02 14:59:23
I sort of see what you are saying.  Having a known, pre-determined limit would be beneficial and predictable.
With a variable limit, for a miner to pump up the block limit.  A miner would need to have the majority of the hashing power and they would need to consistently pump out transactions and include them in blocks (Unless they were willing to continually pay fees to other miners)
@_date: 2015-08-18 19:46:32
I was thinking the same as *Piper67* until someone pointed out how game-able these were, then all the other ways on how to game the system started flooding into my head... 
I did have a few other ideas on how to organically grow the size, but some of them didn't have legs.  Most ideas were on the lines of calculating the maximum block size when the new difficulty was calculated.  The one which was the hardest, and most costly to game was just linking the maximum size to the mining difficulty target and re-calculating both every 2016 blocks.
@_date: 2015-07-09 22:01:45
Makes sense.  I guess anything suggested here now would already have been thought of unless it's decidedly left field...
@_date: 2015-07-09 21:33:07
Or, could full nodes/miners not check the 1st IP that relayed them the transaction.  Does that level of logging exist on full nodes ?
@_date: 2015-07-31 17:14:03
Out of curiosity, why do all these methods impose a static then increasing limit on the block size?
There is functionality within bitcoin that calculates the hashing difficulty every few weeks.  Could we not use the consensus of the network to calculate the new maximum block size.    
At the same time as calculating the difficulty target, the maximum size can be calculated ?
Something along the lines of...
    // average size of the last 2016 blocks, plus a 5% increase
    1.05 * average(sizeof_recent_blocks)  
    
    // average, plus 1 Standard deviation
    average(sizeof_recent_blocks) + stdev(sizeof_recent_blocks)  
*I picked 5% as an arbitrary number*
@_date: 2015-08-03 16:59:45
I wondered this until someone pointed it out to me too.  I had thought the maximum blocksize could be calculated the same time the next difficulty target was calculated.  Basically the new maximum would be the average size of the last 2016 blocks, plus an small increase.
I can see the merits of imposing a well defined limit, or a dynamic one such as this.  