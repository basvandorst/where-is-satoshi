@_author: thereal_jl777
@_date: 2016-03-16 17:23:16
thanks! this is first time I have been tipped
@_date: 2016-03-16 15:50:51
the original answers I got were incorrect and when that was corrected, I changed my analysis. Are you sure it isnt possible to get into "impossible" states by using manually constructed rawtransactions? And if it gets into an impossible state, are you sure it wont create horrible forks or double spends? 
It only requires that old nodes TRUST that the tx was verified, since they cant do it themselves. The question about how an old node can spend to another oldnode without double spend, was not answered.
In the case where the segwit nodes operate as if the older nodes dont exist, ok, sure it "works". But are you 100% sure there are no attacks using the anyone can spend status that is required. And what happens to segwit tx received by old nodes spent to another old node? How can the receiver verify it. They just need to trust it is valid?
so segwit converts the old nodes into requiring trust? Is this what you are saying? Or do all the old nodes just become like SPV nodes?
@_date: 2016-03-10 11:43:46
There will be a deterministic method that ensures that a specific tx is only eligible to be on a specific interleave, and all the interleaves are valid, so any interleave would be able to spend any other interleave's outputs.
otherwise it would create 10 different subcurrencies that are constrained from being used, not to mention really hard to understand
no need for complex math stuff, to solve a problem that doesnt exist. maybe there is an attack vector against interleaves, but I havent heard of one yet
@_date: 2016-03-06 15:56:41
iguana is still being developed, so only a few nodes are running it. It is mostly coded and being debugged now.
docs.supernet.org has the API bindings.
 has some technical details
source on github.com/jl777/SuperNET
It is not meant as a replacement for the reference bitcoind, as its default mode is for a non-relaying node as that allows to purge the sigs, which are half the space needed. I estimate after purging it will be around 20GB for a full block explorer level dataset.
And the bitcoin core part of iguana is just the platform that all the other SuperNET services will build on. So it is being designed from the point of view of the user, not the miner and most users will never mine a BTC block so it isnt really a negative and also removes a lot of potential bugs as there is no need to make it 100% bug for bug compatible with the installed network.
@_date: 2016-03-10 00:56:11
not sure what you mean?
p2sh is a type of output script. what I talk about is interleaving entire blocks
@_date: 2016-03-09 23:30:39
1st.0 N[201] Q.199 h.401938 r.354036 c.0:134000:0 s.354036 d.155 E.67:355887 M.401937 L.401938 est.8 115.3MB 0:29:46 27.945 peers.84/256 Q.(0 79)
I am currently optimizing the parallel sync. Above is from first 30 minutes of the test run (29:46), it issued requests for 355887 blocks, saved 354036 to disk started processing 155 bundles.
I am tracking down why it got so much slower. The system performance is very sensitive to any change in load from any of the different threads.
The timing is from a dedicated VPS server with 1gbps connection, 32GB RAM and normal HDD. It used to saturate the full 1gbps, so it is going at half speed of what it used to.
@_date: 2016-03-04 13:12:35
turns out there are 105 data structures in total, not counting the crypto777 library. but not all are actively used, yet
@_date: 2016-03-16 18:54:25
I want to understand clearly.
currently if it takes N bytes for a normal tx
with segwit it would take N+4 bytes in the witness space and 0 bytes in the normal blockchain?
how is that possible? isnt there bytes needed in the normal space too? what is the total space used?
assume the current size of the tx is N bytes, there is 1vin/1vout.
in witness space it would take N+4 bytes? and X in the normal blockchain.
what is X?
@_date: 2016-03-03 22:48:24
If the files are invariant, why not put them into a readonly filesystem. then as long as you can detect that it hasnt been modified, there is no need to revalidate an already validated dataset. If you dont trust your readonly filesystem wasnt tampered with, I guess you can always generate a hash for the data set. I dont think it would need to be crypto strength if it just needs to verify the data didnt change, so it would be able to be calculated at HDD bandwidth.
the 36 hours was on my incredibly slow laptop I develop on. I like to run things in slow motion as it helps identify slow operations as it can get REALLY slow. it only has 4GB of RAM and that is probably what causes the slowness. When I say 3MB codesize, I forgot to say that by using memory mapped files for all the dataset it doesnt use any fixed system RAM, if the system needs it, it is just swapped out. If you have the RAM, it all stays in RAM, if not it will swap out, but I got through a sync with 3GB of RAM allocated to the buffers.
I think what you say about downloading the blockchain in parallel is at the level of just copying the data. When I say parallel sync, I mean it is downloading things in parallel, building the data set and indexes, combining raw block files into bundle files, verifying the blockchain. All in parallel. Bandwidth is mostly maxxed out other than a few dropouts, but I have a solution in the works for that and CPU usage is about half the cores for the first part and then 90% toward the end.
I am not claiming iguana is any replacement for the core, iguana doesnt mine any blocks. I dont want to have to worry about creating a fork, so it just follows what the network does. It is the bitcoin core for the mass market that can be put into a one click install chrome app. 
@_date: 2016-03-16 19:43:13
ok, so per tx, the extra combined space is 2 bytes per tx and 1 byte per vin? 
if true, it is not near the impact I feared, but still it is more permanent space per tx
Are there estimates of how many tx per 2MB (total space) we would get using segwit vs 2MB hardfork?
also what happens to the segwit tx sent to a non-segwit node, which then spends it to another non-segwit node? it seems even a full node would just have to trust it is valid, but how could a full node mine this secondary spend if it doesnt have the witness data?
@_date: 2016-03-16 16:24:41
all we need to accept is that nodes that dont update to the softfork just have to trust that any segwit tx they get is valid, right?
And the other old nodes that receive such tx, just need to trust it is valid, right?
And there is conclusive peer reviewed proof that there are no attack vectors found from the massively increased complexity, right?
And that compared to a 2MB hardfork, the segwith is less efficient with permanently required data for full nodes.
As far as your "In the future 100% of nodes will likely be pruned, so that the only real expensive storage is UTXO for long term scalability"
Let me get this right. You are saying that since everybody has already agreed that all nodes will be pruned in the future, that utxo reduction is the bottleneck and that makes reducing utxo space the most important and nothing else matters.
Requiring older nodes to have to trust segwit transactions, that doesnt matter.
And how exactly can 100% of nodes prune? I must have missed the quantum magic data reconstructor that allows a new bootstrap node to verify against nonexistent data.
You guys are just way too smart for me. I am just a simple C programmer trying to make sense of nonsense
@_date: 2016-03-03 23:33:20
i dont see it very likely for home users to ever find a block
also any serious miner will have custom mining software
iguana is designed for mass market use case, not for miners
and by not mining it avoids any risk of creating a fork
@_date: 2016-03-16 18:59:12
but it is less tx per MB of permanent HDD space needed. the entire premise to push through segwit via softfork is invalid
I never ever said segwit wasnt cool and does important things. I said segwit consumes more space per tx, which means this additional cost needs to be weighed against the benefits.
it is unclear what the cost is, so how can anybody make any rational decision as to whether it is worth paying this cost for the malleability, etc.
with iguana I encode utxo in 4 bytes each, in any case this is a local implementation matter and the inefficiency of an implementation shouldnt contaminate the protocol and blockchain, should it?
improved security claims have not been peer reviewed, or are we just supposed to trust the experts judgement? who are the experts anyway? 
@_date: 2016-03-16 20:12:06
sure avoiding 80bit lookup tables for a shared secret contract, this is closing a security hole. OK, but what new unknown attack vectors are created with the separated witness data, nodes not being able to validate tx, etc.
is it really wise to put such a major change in a minor update release? To my mind segwit changes bitcoin more than all other releases combined
@_date: 2016-03-16 22:08:50
my question is about the existing full node, who wont have the witness data. i dont see how it could mine a block that has any segwit inputs. so any full node wont be a full node anymore.
segwit seems to not increase the size so much, but it certainly isnt reducing the size, so it does not help from that aspect of scaling. If only it wasnt presented as the panacea for scaling, then its benefits could be evaluated in isolation...
and why put all this new functionality into a small update, it is quite a major change to how bitcoin works. a defacto hardfork that creates an overlay signature protocol
@_date: 2016-03-04 00:23:51
Not sure why you think iguana wont validate.
validation cant be done until all the vins can be fully resolved. I skip validation prior to when it can be done. As soon as all the prior unspents arrive, then that vin can be validated. However, since the time to sync everything else is not that long, it makes sense to validate serially. 
so it does a parallel sync and at the same time the normal serial sync which does the validation. does that make sense?
@_date: 2016-03-03 22:10:50
On identical laptop and connection, 0.12 took 36 hrs vs iguana's 6hrs
If to skip validation, iguana can get the full blockchain in 8 minutes. it takes as long as 30 minutes due to all the processing that is needed.
I havent finished performance optimizations yet in any case, but my goal is 1 hour (lunch break) for systems with 100mbps connections and 6 hrs seems as good as you can get on a 20mbps connection as it stays saturated the whole time, unless i add syncing of the iguana bundle files, which would cut the data needed in half. actually would reduced it to around 15GB or so, especially if all the hashtables are stripped for the sync and let each node rebuild it. The sigs are about the same size though at 15GB+, but i was thinking to do that in the background with a way for the user to see how much has been validated. I havent optimized the sig validation at all and I plan to use the libsecp256 library. Right now I just have a generic tx JSON mechanism where you can add inputs and outputs and sign it, but that is of course way too slow for a mass validation.
The codesize is 3MB and it has very few dependencies so it can run on most systems, including as a chrome app. Now iguana is not mining any blocks, so there is no worries about it creating a fork, it is designed for mass market use to allow a fully decentralized client without any user adoption barriers
@_date: 2016-03-07 00:35:40
Most bitcoin forks are 99% the same as bitcoin, so I abstracted out the differences into a configurable JSON. I am starting with a BTC/BTCD dual iguana, but other than different blockhash algos, most other coins should work with iguana. The first coins will require some changes to support them and coins like VPN which have made tx format changes will require a bit more work, but with the protocol and blockchain and RPC all basically the same, it is just a matter to do the 1% that is different.
@_date: 2016-03-16 15:33:34
Yesterday was the first day I looked into the segwit details. I got responses from knightdk. I didnt like the answers I got, I asked more questions and it is become clear that there is nowhere near enough details for an outside dev to implement the segwit.
Also, wuille is engaged now, but is avoiding the question about how you can store more data per tx and yet have it be less data. I dont care about the SPV or pruning node's HDD usage as they are not full relaying nodes and they are pruning things down to 2GB so who cares if they use even less space.
I am objective in all the tech questions I ask, since I have to actually implement it. 
@_date: 2016-03-16 19:06:43
a full relaying node will need to permanently keep the 1MB standard blocks and the witness blocks. Please do not say we can just trust some company to keep backups for it, bitcoin is not ripple.
So I need to know how much space is used TOTAL. 
rawtx is taking N bytes. per segwit vin/vout how much TOTAL space, as in the space in 1MB part and the witnes part, is needed for a tx that would currentely take N bytes.
I do not understand why you dont understand the question.
I see the need for a lot of bytes in the original chain and the witness chain, both. and both combined compared against what it costs currently. That is the segwit space cost. what is the segwit space cost?
hopefully you can make a formula so simpletons like me can plug in number of vins, number of vouts into a formula and estimate the typical difference of TOTAL space used.
fully validating and relaying nodes will need to keep the TOTAL combined datasets. therefore the TOTAL size is kind of an important thing to know.
it might well be that the difference justifies a much more complicated setup and tradeoffs, but I cant even begin that part of the analysis until I know the space tradeoff
@_date: 2016-03-16 15:45:47
hello. I asked questions yesterday as it was the first time I looked into segwit. So of course early in the thread I didnt know anything about it.
Now that I do, it is clear that the claims that segwit saves HDD space is bogus. Nobody has refuted this. In fact wuille confirms it:
"If you're talking about storage space used by segwit-compatible full nodes, well, obviously it will use more space, because it increases block capacity - that capacity has to go somewhere."
So it is obvious to me that the capacity has to go somewhere. Let us ignore pruning nodes as they dont have any problems with HDD space of 2GB and wont regardless of blocksize, so saying segwit makes them more better is silly.
Now, if segwit forces everyone to update and the extra stuff has to go somewhere, let us compare a 2MB hardfork versus segwit. Does the tx use more or less combined space if it is a segwit tx?
If it uses more combined space, then how does that not reduce space?
You clearly dont know what you are talking about, or are pretending not to.
@_date: 2016-03-16 20:07:37
but segwit is marketed as the scalability solution
and it seems that all the benefits can be kept, without making it the mainstream way of sending bitcoins
@_date: 2016-03-16 15:21:30
where in the BIPS are the protocol messages needed defined?
where in the BIPS does it define the actual format of the witness data?
without these things how is it possible to implement anything that works with it
@_date: 2016-03-10 02:57:12
I like to keep things as simple as possible.
to create complex emergent behavior out of simple components
i still dont understand how interleaves and p2sh makes sense, but I am just a simple C programmer, not any rocket scientist
@_date: 2016-03-16 19:17:13
let us agree to disagree about the impact on security of relying on archive nodes.
the witness data needs to be some place permanent.
1-byte - OP_RETURN (0x6a)
   1-byte - Push the following 36 bytes (0x24)
   4-byte - Commitment header (0xaa21a9ed)
  32-byte - Commitment hash: Double-SHA256(witness root hash|witness nonce)
the commitment hash for example. this appears to be 32 bytes that is needed permanently which wouldnt be needed for a non-segwit tx.
I do not understand how 32 bytes is 3 bytes. Or are you saying we just dont count the size of the commitment hash? because?
@_date: 2016-03-07 00:40:19
iguana will become the bitcoindark daemon over three phases.
1. iguana as a non-relaying node verifying the existing blocks
2. iguana as a relaying node, but not staking new blocks
3. iguana as a full node, including staking
Each step can then make sure all is well before having iguana do more and more of the network. After the third step, each browser will be a potential node, especially as the 10GB limit for chrome app storage wont be an issue and even a low CPU node can stake new blocks. 
There will probably be a stage 4, where iguana does a hardfork to add new features, and where the existing daemon wont be updated with it due to the codebase being so different. Not sure on this point yet as it is several steps away and details are subject to change
@_date: 2016-03-16 18:49:47






show me the exact tx bytes please. I find a 2vin/2vout tx taking almost 700 bytes and that is without the truncated one.
so a P2WPKH saves 3 bytes of combined space? this seems like nonsense to me. How can the combined space used be 3 bytes less than just a normal tx. I await to be learned about this magic. I like magic.
iguana can allow light (non-validating) nodes to not transfer vin data, and the current size of the iguana non-vindata is less than 20GB. So if that is the metric to softfork something, when can I softfork iguana to the entire bitcoin network?
also I plan to use the bittorrent network and not bitcoin nodes to sync the static read only data. That is ZERO load on existing bitcoin nodes. And it doesnt break anything at all, as it is all local optimizations and fully compatible with existing network, protocol and blockchain. use a decentralized storage like bittorrent nodes for archiving the old blockchain data.
Using that method, it allows for fully scalable bitcoin at the local HDD and node bandwidth level. The CPU load for sig validation probably becomes the limiting factor.
@_date: 2016-03-16 18:08:46
what do the expert hackers think about the segwit enabled attack vectors? 
My intuition is that even if there are no implementation bugs, adding a layer that is arguably more complicated than the existing system for mainstream spend tx is very courageous.
segwit is worse for scalability than a 2MB hardfork. I am not afraid of implementing segwit, I just dont see the sense to spend all that effort to create something that is more complicated and is less scalable. Why should anybody spend effort to make something more complicated and less scalable?
@_date: 2016-03-05 01:14:05
I would need to change a  to allow for a larger buffer size 
and the 32bit numbers used for some data fields might need to be increased if more than 4 billion items can happen in 2000 blocks, but that would mean 2 million vouts (which would be the first things to overflow) per block. Since each vout requires an output script and amount, 32 bytes would be about the smallest and if there is a tx with just one vin funding 2 million vouts, that is the worst case, so I guess after a 64MB blocksize the 32bits has a chance of not being big enough.
iguana deals with bundles of 2000 blocks, which are in some ways like having a 2GB "block"
blocksize is just not affecting anything other than possibly overflowing some internal buffers and the size of the indices
@_date: 2016-03-03 23:22:59
I have personally written the vast majority of the code in the repo. the GPL2 seemed a good place to start as it can always be changed to MIT, but going the other way not so easy
@_date: 2016-03-06 17:50:23
@_date: 2016-03-09 23:47:06
It would need a hardfork
From that moment 10 parallel chains are started. Either based on 10 variations of the hardfork blockhash, or maybe going back 10 blocks. Not sure the best way to start the 10 interleaves. And 10 is just a convenient number to use due to 10 minute average block time. The nature of interleaving is that you can interleave it however many you want, within reason. So if a way is made to adjust the number of interleaves automatically every 2016 blocks, this would allow the number of parallel chains to adapt to the recent volumes.
now we have 10 almost independent interleaves. The miners would be able to mine on all 10 interleaves at once. An ordering of the blocks is simply based on the order of the interleaves, regardless of when the block is mined.
This creates the one case of a later interleave getting a block before the current interleave is done. So it is possible that tx that is valid without considering that interleave and gets into the future block is invalidated when the current interleave is solved.
However this is a feature not a bug!
It allows to create different service levels, ie 1 minute average confirmation time by the user submitting 10 tx all using the same vins. It will only get confirmed in one of the interleaves (the next one that completes) so there is no double spend, just 1 minute time, but still there is 10 minutes block time for all interleaves.
So users that want 1 minute confirmations would pay 10x the txfee, which will make miners happy, but if you are in a hurry, then it is well worth it.
Once the interleave genesis hash is determined, then implementing this in the core is pretty straight forward.
@_date: 2016-03-16 16:17:01
what happens if a non-segwit nodes spends a segwit anyone can spend to another non-segwit node?
I said that segwith technically is a softfork. My point is that "softfork" is presented as not being a big deal and people wont have to upgrade, ie optional. There are some changes that can be implemented as softforks, but these changes can be quite dramatic, ie changing max coin supply.
So using the "its a softfork, dont worry" is total disingenuous.
Why not a softfork that just siphons off bitcoins to a few special accounts? there must be foundations that are worthy and needing of funds. And why not just tweak the emission schedule a bit and increase the max supply?
Can those things be done via softfork?
If so, then what about a softfork is safe?
@_date: 2016-03-16 18:29:53
So the official position of the bitcoin devs is that segwit is as efficient in space usage as a 2MB HF?
it is equal in space?
that part is what confuses me, it seems like 40 bytes more per tx, or maybe 200? not sure. that is why I ask
from the BIP:
01000000000102fff7f7881a8099afa6940d42d1e7f6362bec38171ea3edf433541db4e4ad969f0 0000000494830450221008b9d1dc26ba6a9cb62127b02742fa9d754cd3bebf337f7a55d114c8e5c dd30be022040529b194ba3f9281a99f2b1c0a19c0489bc22ede944ccf4ecbab4cc618ef3ed01eef fffffef51e1b804cc89d182d279655c3aa89e815b1b309fe287d9b2b55d57b90ec68a0100000000 ffffffff02202cb206000000001976a9148280b37df378db99f66f85c95a783a76ac7a6d5988ac9 093510d000000001976a9143bde42dbee7e4dbe6a21b2d50ce2f0167faa815988ac000247304402 203609e17b84f6a7d30c80bfa610b5b4542f32a8a0d5447a12fb1366d7f01cc44a0220573a954c4 518331561406f90300e8f3358f51928d43c212a8caed02de67eebee0121025476c2e83188368da1 ff3e292e7acafcdb3566bb0ad253f62fc70f07aeee635711000000
the above is the serialized bytes for a segwit tx in the witness section for a 2 input 2 output spend. In addition the normal tx (truncated is needed). So it seems to me, who is counting every byte needed, that a lot of extra bytes are needed for every tx that is sent via segwit.
If it does use more space and is more complicated then the other benefits of segwit need to be weighed against the cost of using segwit for spends. and that ignores the implementation costs, market confusion, possible attack vectors, making bitcoin require trusted tx, etc
and not to pick at small points, but what use is saving disk space for pruning nodes? they are already pruning things to very small, so i dont see any benefit from that.
@_date: 2016-11-13 20:40:31
nubits used a centralized system, so it isnt comparable.
steem back dollars SBD is somewhat close to pax v1, and SBD's current price of .00138 = 96.7 cents. a bit of a discount and this is after a steep decline in the steem price.
pax v2 will have features nothing else has
@_date: 2016-11-13 20:37:36
ripple uses gateways to give value to the tokens. if a gateway goes under, the tokens lose all value.
that is not exactly decentralized and pax is nothing like that