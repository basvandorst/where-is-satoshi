@_author: laurentmt2
@_date: 2015-06-14 21:26:42


I guess the best ELI5 will be the explanation by the author himself (Matt Corallo): 


My very own interpretation of this paper is that the core component (around 100 nodes) detected by their tool corresponds to mining pools, services, merchants connected via the relay network and/or private peering agreements.
It's not one of their conclusions (it's difficult to prove this point with certainty) but just an intuition on my side.


To be fair with the "armchair experts", bitcoin is a strange beast and it comes with a lot of new challenges related to its pseudonymous nature.
Everything has to be done on these matters but we should see this as a stimulating challenge.
It's also very likely that it will provoke a lot of headaches for regulators (note for myself: invest in pharmaceutical companies)


On this front, I'm quite optimistic. There's an increasing number of academic works related to bitcoin made by talented people like A.Miller.


From my understanding, the relay network relies on a small number of special nodes (the relay nodes) and increasing their number might help to increase the resistance of the relay network.
Anyway, without a P2P network which is able to propagate blocks very quickly, we may lose the redundancy of the 2 networks and imho it's a serious modification of the threat model. Hence, my "concerns" about the roadmap.
@_date: 2015-06-12 14:59:58
I agree with you on the importance of this kind of simulations. Anyway, I fear your tl/dr is a bit "biased". The report also states that propagation of blocks is an issue with current p2p protocol. My main concern is that current Gavin's proposal doesn't address this issue and completely relies on the relay network. Without a proper solution implemented (like iblt) it means that the p2p protocol cannot be used by miners to relay big blocks if a problem occurs on the relay network (b/o long delays required to propagate big blocks). Imho, this "temporary" solution introduces a serious weakness in the network and that should be avoided.
@_date: 2015-06-13 15:17:05
Actually, my point isn't about the "large miner, large block attack vs small miners" but is related to something more fundamental.
- Studies have shown that time required to propagate blocks to 50% of nodes, on the P2P network, increases linearly (at best) with the size of the block. (see 
- The Nakamoto's consensus requires that the delay for validation &amp; propagation of a block (to others miners) is much lower than the average time between two blocks.
- Currently, most miners are encouraged to use another network (the relay network) to propagate blocks more efficiently. Propagation of bigger blocks over the current implementation of the relay network should be fine.
Anyway, it's important to note that there is a redundancy of the 2 networks. 
If a problem is met with the relay network, miners can use the P2P network to propagate blocks.
- With bigger blocks but without a solution implementing faster propagation on the P2P network (like IBLT), there's no more redundancy of the 2 networks. If a problem is met with the relay network, miners might be unable to use the P2P network to propagate blocks in a reasonable amount of time, increasing the number of forks in the blockchain.
So, that was my point: bigger blocks without faster propagation may introduces a weakness at network level.
I've discussed this point with people supporting an increase of the blocksize and they usually agree that they would feel more comfortable with a proposal covering a higher blocksize limit and O(1) propagation.
I know there's some "real scale" simulations running. I hope they'll try to simulate this kind of scenario, in order to validate/refute this risk. It might also be useful to determine the max blocksize maintening the redundancy of the network.
@_date: 2015-06-13 17:15:21


The relay network implements optimizations like avoiding to propagate transactions twice (once with tx propagation and once with block propagation)


Yep. The important point is that miners (pools or solo miners) can propagate blocks quickly, with a low probability of forks (whatever the network used).
Time required to propagate blocks among others nodes seems less critical. It might have consequences for some use cases (like use of zero conf. by merchants) but I don't think it would endanger the protocol.


Andrew Miller and al. have done important works on these subjects. 
They've mapped the topology of the main network which is a first prerequisite for credible simulations (see 
Moreover, they've developed a framework for scalable simulations (see  with thousands of nodes.


It's a difficult question and may be it's even too early to try to answer it, because the answer is related to the ecosystem and the use cases (which change very fast) and to the degree of centralization (full nodes, SPV wallets, wallet services, ...).
Actually, I think we don't even know how to measure the activity on the network, especially when we want to do comparisons with other payment networks.
E.g, this single tx ( is a payment done by a faucet and it embeds 2500 payments !
How should we count it: 1 transaction ? 2500 transactions ?
For the fun, note that a single 1Mb block embedding 12txs like this one would allow 50 payment/s, which is half of paypal activity ! Not so bad bitcoin :)


Correct. The relay network is open to everyone. It just requires to download an additional software. I think it's also better if you have a good connection.
@_date: 2015-06-12 15:34:06
No problem. Imho, having discussions based on concrete analysis like this one allow us to better understand the problem and move towards consensus. Everything else is a waste of time &amp; energy.
@_date: 2015-06-14 21:41:51
For now, this metric has also some drawbacks: a P2PKH and a multisig script (without P2SH) have very different sizes but both are a single payment. Note: that might be different in the future, if every output is a P2SH.
Question: why not just counting the number of outputs ?