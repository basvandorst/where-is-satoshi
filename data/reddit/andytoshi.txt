@_author: andytoshi
@_date: 2016-08-04 20:31:17
Oh, no! Maybe Voldy miscounted or something, but I think you're right, and I think this wrecks the "this would immediately have saved space if Bitcoin had used it" claim of the document.
Before I redo the paper's calculations with real numbers, which will disappoint people who were thinking "over 50% savings!", let me start by saying that this is a _huge asymptotic savings_. As I said in my first comment, if this scheme works, it means that most transactions won't have much of any effect on the size of the data needed for full verification, and some transactions would even decrease it. Long-term Bitcoin will keep growing at a linear rate (actually superlinear until we start hitting any caps) while history shows the utxoset growth rate is much, much lower. So my personal excitement about this isn't diminished in the least.
Ok, so here's the bad news:
With 40m outputs at 2.5Kb each (the author used 3Kb but I think this was an overestimate, even without future CT space savings we think are possible), we get 100Gb, plus another 15Gb or so for the `kG` values and header chain data. Against Bitcoin's 80Gb this is not so impressive (though against "Bitcoin plus CT" which would be around 1Tb I think, it is) (does anyone know the actual "total number of outputs ever" count for bitcoin?).
@_date: 2016-08-13 02:35:14
By analyzing just the Bitcoin chain, no, the "move" on the Bitcoin side would be represented by creating a P2SH output, and these all look like random data. This is true for federated pegs and would be true for any trustless peg mechanism developed in the future.
However, on the MW side you've gotta prove to the MW validators that you did the Bitcoin half of the transaction, which involves revealing the Bitcoin output on the MW chain. (You can do this with some large delay, so that the Bitcoin output is buried too deeply to be reorged if you're worried about censorship, but eventually to get the coins you'll have to do this.)
So somebody trying to see MW users would be able to, they'd just have to track both blockchains.
@_date: 2016-08-29 23:32:07
Thank you for catching this screenshot, Greg.
@_date: 2016-08-13 04:57:06
Well, "segwit" would look very different with MW (I think the rangeproofs are the only witness data since there are no scripts), but yes, I'm pretty sure MW can be instantiated in an entirely malleability-free way, so my point may be moot.
@_date: 2016-08-13 02:27:56
Here is my current scheme (which might be broken, and also I don't like because it involves adding extra permanent data to the blockchain if an OP_C(R)LTV branch is actually taken).
A rangeproof can be thought of as a signature with the blinding factor (this is not its purpose, but it's what it is), so you could use it to sign some data, and the result would be a signature of that data by the owner of the output. So sign the following data: "at blockheight X, or Y blocks after this output is committed, this output may be replaced by this other one". To make the math add up, the replacement output needs to be accompanied by a second "dummy output" -- and this dummy output needs to sign the original output.
Later, after the locktime has expired, anybody can put this dummy output onto the blockchain with a specially formed transaction that says to replace the original output with the new one. (Since the dummy output signed the original output rather than nothing, as typical dummy outputs do, it can't be used in a normal transaction, and validators will know to check the locktime condition.)
If the locktime does not expire, then nothing happens and the original UTXO gets spent, just like normal. But if it *does* expire, then you have this weird "utxo replacement" transaction that goes onto the blockchain and can't ever be pruned because otherwise new nodes can't verify that the locktime was executed faithfully. (I guess you could prune this and then you'd be weakened to SPV security but only for expired locktimes, which is IMHO an acceptable security tradeoff, but it's complicated to explain to people so I'd like to avoid it..)
If anyone has comments on this or suggestions for improvements I'm all ears. I haven't had a lot of time this week to really poke at it yet.
@_date: 2016-08-03 13:36:26
The short answer is: the 30Gb chain is enough.
The long answer is:
First, it's impossible to translate the 80Gb chain into a MW chain, sadly, so this is purely a hypothetical comparison. But having said this, 30Gb would give you (with full node security) the entire current UTXO set, and assurance that no theft (by which I mean the system rules were obeyed, everything signed by the right key, etc.) or inflation had ever occured.
Where this is weaker than Bitcoin is:
* With the pruned chain you cannot learn the history of every coin, only that by some path every coin eventually came from some coinbase or pegin.
* By pruning the history it is possible to lie about the age of UTXOs (noticed this). So whatever the tip of the pruned history is, you have to assume every UTXO is buried no deeper than that. I'm still having trouble understanding what this actually means, security wise. (It does mean that any work on locktime-based payment channels is complicated.)
In any case, reorgs of the pruned chain can't be handled without full copies of the reorged-out blocks, so it's best to always have the most recent couple thousand blocks stored explicitly, and with explicit blocks you cannot lie even about age.
@_date: 2016-08-02 22:10:28
I think Voldemort made a minor mistake here -- it's possible to unlink transactions because the original transactions' commitments sum to these "excess" kG, which get included in the block unchanged. If someone can find subsets of inputs and outputs that sum to the original kG values, they can successfully untangle the transactions. (This is the "subset sum" problem, which is NP hard, so maybe it'll take some grinding, but for small transactions it's probably doable.)
There's an easy fix, add a second explicit k' value, construct transactions so that the excess is (k + k')G, and when merging transactions people can directly sum all the k' values. After this summing the transactions really are inseparable.
Posting this here so hopefully it doesn't get lost. :) Not sure where errata should go for an anonymously posted paper.
**Edit:** Ah, beat me to the punch on this.
@_date: 2016-08-29 21:29:13


Obviously I did not list everything that has been done to improve scaling. What I gave were examples of how to make the system actually more scalable (i.e. do more with same resource usage) rather than just increasing demands on unpaid users, and pointed out that these things are actually being done.
@_date: 2016-08-02 21:29:31
It's unfortunate that there's no code with this drop. Sometimes there is (Cryptonote, Bitcoin), sometimes not (OWAS), but I think there's something cipherpunkish about dead-dropping original cryptography with no name on it.
@_date: 2016-08-08 20:19:50


What do you mean by this?
@_date: 2016-08-28 18:41:42


Sorry, can you clarify the relationship between addresses and bandwidth? Even in base58-encoding, addresses are like 50 bytes each.


Well, you don't know that what you're doing is legal everywhere now and for the entire future, which is what's relevant when you're using a global consensus system. When you correlate coins like this, you make surveillance and censorship much easier.
But ignoring the privacy implications (which you perhaps really don't care about), this harm's bitcoin's fungibility -- the property that all coins have equal value. If people see some coins as being cleaner than others, people doing business are basically going to have to pay the people defining "cleanliness" to vet all of their coins, and the entire system is less usable. Further, different jurisdictions are likely to have different notions of cleanliness such that nobody can satisfy them all at once, and now it's costly for bitcoins to cross borders. There are several companies that exist today whose business model is "chain analysis", being the defining authorities for the relative value of different coins. This is very bad for a currency.
Ultimately, you're free to do whatever you want -- if the system depends on every user being a social activist about it, then it's failed -- but I wanted to make you aware of this.
@_date: 2016-08-29 20:47:14


Out of curiosity, are you aware of the libsecp project which gave a 7+ times improvement in verification time without increasing any storage or bandwidth requirements, nor weakening security (actually _increasing_ it, by removing a system-dependent OpenSSL encoding, which would've been a consensus bug)?
It's easy to double the capacity of a system if you also double all its resource requirements. But if you do that to a system designed to operate globally in adversarial conditions at the cost of those attributes, you can scale _way more_ by just using a centralized system in the first place. Improving scalability without these costs is much harder.
In some cases it requires [original mathematics]( In many cases it doesn't -- for example, designing a script signature scheme without quadratic hashing ultimately just required some engineering skill. But I've never heard of people describing themselves as "big blockers" and throwing around wild accusations of negligence suggest either sort of improvement.
@_date: 2016-08-28 14:55:45


Out of curiosity, is there a reason that you _ever_ reuse addresses? It would be better for your privacy if you didn't, and would entirely avoid the confusion you're avoiding by never reusing quickly.
@_date: 2016-08-07 19:48:36
Good question. The rangeproofs actually are tied to the real value of the output; if you change this, the rangeproof will become invalid, and the recipient would have to create a new proof for the new value.
It's a bit of a subtle thing, since the resulting proof is actually zero-knowledge in the value (at least for the part that's rangeproofed over, the Alpha CT implementation has some knobs you can turn to reveal some information in exchange for space savings).
The rangeproofs actually do need to work this way: if you could add value to an output and the proof would remain valid as long as you were in range, you could actually bisection-search the value: try subtracting 100BTC, see if it's still valid, try subtracting 50BTC, subtracting 75BTC, and so on..).
@_date: 2016-08-03 13:21:25
Cryptonote was launched with a whitepaper which contained some original cryptography describing an innovative way to obfuscate the transaction graph. I think whatever person(s) came up with this and wrote the paper had a cypherpunk agenda.
Having said this, Cryptonote's "reference implementation", Bytecoin, was a comparatively clumsy scam involving a premine, obfuscated code and forged document timestamps, which makes me suspect that some cryptographer pulled a Doc Brown on a traditional crime group. But I'm only speculating here :).
@_date: 2016-08-13 04:37:53
Oh, hey, I just realized that you can make locktimed transactions by having outputs sign the minimum height at which they are allowed to be included in the blockchain (and say, by default this is zero).
If an alternate transaction happens before this height, the "locktimed tx" will be invalidated by the normal means (and can only become valid if the alternate tx is reorged out), otherwise it can go into any block with sufficient height.
Since to the best of my knowledge, MW can be constructed to not have any malleability, this pretty-much does it, with no extra permanent data at all. (The big limiting factor is that this cannot give you relative locktimes.)
**Edit:** To avoid linking outputs within unmerged transactions, observe that it's sufficient that a single output per TX have a locktime on it, and the others have to come along for the ride. All non-locktimed outputs should have a locktime of 0.
@_date: 2016-08-08 15:18:55
Yes, unfortunately MW uses the scanning key to authenticate transactions, so you can't just expose it like you can in CT. But there are other ways to reveal a value, for example explicitly showing it, using the explicit value to subtract off the value-part of the CT commitment, then signing with that.
@_date: 2016-08-17 15:32:38
MW is nice with all this pruning but I would really like the historic blockchain data to be constant-size, you just add blocks into an accumulator or something and then anyone can verify they're all legit with just the accumulator.
Can you dream up a crypto protocol for me that does this? I think I could handle the implementation but for some reason I'm having trouble with the dreaming part :/.
@_date: 2016-08-29 21:19:24


Signature verification is by far the most CPU-intensive part of verifying a bitcoin block (aside from hashing in pathological cases, which I also addressed). If you doubt this, you can look at your system load when Bitcoin is processing a block or during initial sync from a fast peer.
But imagining otherwise, if CPU load is irrelevant to bitcoin's ability to scale, what is relevant? If it's storage space or bandwidth, then increasing specifically those demands is the worst thing you could do, scaling-wise.


Well, Bitcoin is used _far_ more than any other system, so it naturally can't increase proportionally as much as competitors. But ignoring that, can you name a decentralized competitor to bitcoin that doesn't use libsecp? It seems remarkably popular for a "premature optimization" that requires the use of unreleased software with a clear "Use at your own risk" warning on its README.
@_date: 2016-08-18 16:27:58
If your concern is that there isn't a record of the transaction creation that the sender can point to and say "here, I sent the coins", I think that with stock MW the transcript of communication between the two parties will have to do. (Though this makes "sender double spent" look the same as "recipient changed the outputs" as far as an outside observer is concerned, hmm.)
Interestingly payment channels, as they are half-imagined in my mind, solve this problem. The channel is opened by sending funds to a 2-of-2 (which has to be confirmed), then the actual value transfer requires both parties to cooperate (in a provable way).
I'll add this to my list of "problems that need solving for this to be practical", thanks!
@_date: 2016-08-17 15:38:16
You're overthinking it a little bit :). The incomplete transaction does need to be encrypted to you somehow (otherwise anyone could fill in the transaction), but "how do I send data to a device that's often offline" is more-or-less a solved problem.
I'd guess in practice the messages would wind up queued on some server like they are with ordinary messages on e.g. Signal or Telegram.
@_date: 2016-08-08 15:10:53
Yep, you are correct. There are some algebraic reasons I think this is impossible, but more importantly the rangeproof is a specially constructed [borromean ring signature]( The choice of keys is what makes it a rangeproof, but it is still a signature, so we use it to sign the commitment itself. This way any change in the commitment will require forging a new signature.
@_date: 2016-08-07 14:18:25
No, because if the recipient did this then the rangeproof on the change output would become invalid.
@_date: 2016-08-08 15:16:10
Wow, you are fast!
@_date: 2016-08-17 15:15:33
What is calling "fast sync" is actually "full verification without all blockchain data". If you delete scripts it's impossible to tell that they were spent correctly.
To contrast, with some crypto magic you _can_ delete MW signatures and still verify that whatever signatures existed, were legit, and no signatures were reversed.
@_date: 2016-08-17 15:18:39
There's a bit of a caveat here which is that if you _use_ script, and your script you use requires you sign the whole transaction, this will get in the way of OWAS. (The vast majority of transactions in Bitcoin do not fall into this category though, and they'd've been able to use OWAS this way.)
@_date: 2016-08-12 17:08:42
Well, the problem is to make sure you've got the right receiver in the first place. Each kG will be uniformly random.
@_date: 2016-08-02 21:16:40
It would be possible to implement this from a given block depth, but unfortunately it's a no-go for Bitcoin because it does not support arbitrary scripts. So things like atomic swaps, noninteractive multisig, payment channels, zero-knowledge contingent payments, etc., don't work with mimblewimble.
I was doing a bit of work in this direction before this showed up on -wizards and completely superceded me, and I *think* it's possible to get payment channels working, though they'd be a bit more technically complicated.
@_date: 2016-08-13 04:15:51
I'm not quite sure what you mean. Roughly what my scheme does is creates another locktimed transaction, but ties that transaction to an output in the blockchain so that it can't be invalidated by any manipulation of the output before it's confirmed.
You could have multiple such transactions per output, I suppose .. "at block X this becomes a different output, and at block Y it becomes yet a different output!".
@_date: 2016-08-17 15:26:49
answer is correct, but let me try to give a higher-level answer:
Every transaction basically changes all the private keys of its inputs to get private keys for its outputs. (It doesn't do this for individual inputs and outputs, but for the whole sets, but bear with me.) The total change is called an "excess" kG value, and it's required to know the secret `k` to create a transaction. After creating the transaction `k` can be dropped, it's no longer useful.
However _without_ `k`, it's impossible to reverse the transaction without rewriting its block (same as any blockchain) or just spending its outputs (also same as any blockchain). And to spend its outputs you need to know their secret keys.
Now suppose all the outputs get spent. Then MW lets everyone delete the old outputs from their history. They didn't exist. All that's left are these "kG" values. Well, as long as the first one is in the chain you know that whatever transaction created it was not reversed, and its outputs are either still there or were spent honestly, and that's all a verifier needs to know. The person who created that transaction, of course, cares that the _correct_ kG value is there, but the usual immutability properties of blockchains assure this.
@_date: 2016-08-14 16:47:53
As annoying as the coincidence of names is, I don't think this should be downvoted -- it's a pretty common question that I've heard in real life too.
@_date: 2016-08-18 19:25:57
Oh, I think I've made things sound worse than they are by the word "interaction". What's actually needed is half a round of interaction -- the sender sends an unfinished transaction and the receiver completes it (or vice-versa).
This is actually the same as in bitcoin, the receiver sends an address and the sender "completes it" into a full transaction. The difference now is just that the communication has to be private for security reasons.
@_date: 2016-08-08 15:24:03
In addition to what said, the way this scheme improves privacy is actually unrelated to Monero.
MW improves privacy by combining transactions within blocks, so that you can't tell which outputs came from which inputs. Monero improves privacy by blurring _which inputs were even used_. You could even imagine a system that does both (though I have no idea what kind of math magic would be needed).
I think it'd reasonable to say that Monero's privacy is stronger, but it's really hard to compare because they do fundamentally different things.
@_date: 2016-08-17 15:29:56
You don't need an intermediate server for this, your phone is perfectly capable of handling this. People send an incomplete transaction to you (by SMS or through whatever app you're using), your phone fills in the rest. Or maybe it forwards it to your hardware wallet, or stores it for later, or something.
Unfortunately you can't outsource "fills in the rest" in a useful way, safely.
@_date: 2016-08-11 07:04:35
What I was thinking about doing would be having the communicating wallets do an ECDH exchange, encode a hash of the shared secret as 16 words or something, showing 8 of them, and asking the user to type the other 8. On the other end, the opposite 8 would be shown. So the users then need to phone each other or something and say the words, and this ensures they have communicated out of band to authenticate.
This is a half-baked idea (and leads to goofy UX like having the user copy 8 words from the wallet into an exchange website, then copy another 8 words from the exchange into the wallet) but shows that this is a solvable UX problem, in principle at least.
@_date: 2016-08-02 21:19:33
To be clear: this is much more than "better scaling". What it means is that the total blockchain data needed for full verification scales with the utxo set, not with the transaction set, which is an asymptotic improvement. In particular transactions with more inputs than outputs actually _reduce_ the total amount of data full verifiers need.
From the paper:














@_date: 2016-08-17 17:22:13


Yep, multisig works with MW with the following caveat: it's interactive. In Bitcoin the sender can say "send to m of n of these people" and those people don't even need to know about it. But MW is not so simple.
As mentioned in some other post here, the receiver has to create the outputs, so if you're sending coins to a multisigner they all have to interact to create the multisignature output. Also to prevent individual parties from making outputs for themselves, the sender would have to give different data to each party ... the UX for this would be weird in general. But mathematically it's certainly possible. And for the 2-of-2 case needed for payment channels it'd be fine, this can be done alongside the rest of the interaction.
@_date: 2016-08-03 23:03:11
I didn't realize what you meant when you posted this earlier. Let me summarize our discussion on -wizards since.
First, "two parallel histories" means something like having three utxos, `C1`, `C2` and `C1 + C2`, throughout your blockchain, and pruning your blocks such that to some users you reveal `C1` and `C2` but to others you reveal `C1 + C2`. The result will be a consensus disagreement for which neither user is clearly correct, but a very weak sort of consensus disagreement, in that you can only do this if you know the secret keys to all of `C1`, `C2` and `C1 + C2`. In other words you can provide different utxoset views to users but both views will reflect the same ownership. That is, as near as I can tell the attack is entirely limited to breaking consensus and cannot directly enable theft or inflation (except to the extent that breaking consensus will let you permanently fork the chain).
This is strongly mitigated by the fact that anyone tracking this chain as it was created would not have accepted the chain, since when they saw the full blocks they would have demanded to see every output (and if all of `C1`, `C2` and `C1 + C2` were then revealed, the CT algebra would prevent the chain from validating). But this is a Tendermint-style "always be online or trust those who were" assurance and you don't want that for something as critical as consensus.
A solution to this is to commit to the full utxoset in every block. During IBD users cannot verify these commitments, since they do not yet know the full utxoset, but the first full block they encounter will let them verify it. At this point, if the commitment verifies, they will be on a different header chain than those who were fed a different history (and the "real" chain that people validating from the start would see would be different yet), and the usual "highest-difficulty chain wins" is sufficient to distinguish the real UTXO set from impostors.
Now, this is SPV security, but it's not SPV security that the ownership of all coins is what you believe it is (this is guaranteed by the CT math, you have full security assurance of this). It's SPV security that _your specific view of consensus_ is the same as others'.
This is hard to think about because nether Bitcoin nor any other cryptocurrency I know of has scenarios like this, but I think this should actually be considered full security, in the sense that "SPV security that your consensus history is the real one" is the strongest security that's even possible in any PoW blockchain.
**Edit:** Regarding making duplicate outputs so that you can replace one with the other and thus lie about its age, you could have the utxoset commitment also commit to the blockheights that all outputs appeared at, which would cover this.
@_date: 2016-08-30 04:31:09
I'm referring to the serialization used by segwit -- I'm not sure if this is the solution suggested by JL or not. You can see this in the current Core 0.13 codebase, in the function `SignatureHash`. What I see is that the format itself requires hashing only a constant amount of data per input -- including three hashes which cover the entire transaction (modulo sighash flags).
There are TODOs to cache these three hashes, since they don't change per input, so there is still work to be done (and until these hashes are cached, we are in fact doing a quadratic amount of hashing) but this is an implementation detail of Core. The protocol itself does not require quadratic hashing.
@_date: 2016-08-09 07:28:29
Well, the current chainstate is guaranteed to have been produced by a series of valid state transitions. If the blocks in the longest history commit to things that _aren't_ valid state transitions, that doesn't change this fact. For example, from the perspective of somebody validating the chain by the pruned history, the difference between a chain that commits to some invalid UTXO and one that doesn't is simply that the blockheaders will be different. It's no different than saying that the nonces in blockheaders can somehow be "invalid".
Validators who saw the full blocks in real time would not accept them; this is fine. Blocks can be valid after pruning while being invalid before. What this means for people who rejected the block when it first appeared is that if it becomes part of the highest-work chain, they need to reorg onto it (and because reorging pruned blocks is such a mess with MW, this would involve basically IBDing from whoever has the new history, in which case the pruned-block validation rules apply and the block would be accepted).
It is weird that invalid blocks can become valid after pruning, I agree, but given that this doesn't cause bad transactions (meaning unauthorized spends or inflation) to become invalid, and that differing-consensus issues can be dealt with by the highest-work-chain rule, I don't see that it's a problem.
@_date: 2016-08-10 22:59:41
Yes, the sender basically creates a transaction with a "hole" in it that the recipient is expected to fill in with her coins, with a bit of extra data, and anybody who obtains this information can fill in the hole with their own coins.
@_date: 2016-08-03 14:49:16
If anyone is seeing a 200 but nothing else from the wpsoftware link, like in some replies to that tweet, can you please PM me describing your setup?
I've been having some trouble with another service that is supposed to connect to the wpsoftware server, but it also gets this "200 but no data" behaviour and I can't reproduce it.
I can assure you there is no javascript or anything being used to display the .txt file :).
**Edit:** Oh, to be clear, the wpsoftware. server is my own, which I use to host bitcoin papers whose original links have gone down (or I suspect will go down).
@_date: 2018-01-19 16:30:29
Blockstream's patents remain with Blockstream. Regarding the patents with Greg's name on them, as per the inventor's patent agreement (one prong of our defensive patent strategy), Greg retains the ability to intervene in any hypothetical patent aggression regarding those patents. This means that if these patents were ever enforced against somebody, and said somebody wasn't already threatening Blockstream with IP claims related to those patents, Greg could retroactively grant them a license, thwarting the attack.
Regarding "things he develops no longer have the protection of Blockstreams defense only patent strategy", I suppose Greg is welcome to build a defensive portfolio and join the DPL as an individual if he's personally concerned about being patent trolled. But I've never heard of an individual needing to worry about these things.
For more details see 
@_date: 2018-01-18 20:56:52
Hmm, on second thought maybe I'm wrong. Presumably no such change would turn a multisig output into a non-multisig one, so in a scriptless script, both participants would have to agree to use the new spending mechanism.
@_date: 2018-01-18 17:48:25
Whoever heard of a _Schnorr signature_?
@_date: 2018-01-18 20:48:41
Oh, nice. I will reply on the list, but


In a future with Schnorr signatures, scriptless scripts would be exact the sort of "excessively fragile" thing that users might be doing.
@_date: 2018-01-19 17:22:21
It's impossible to overstate what an amazing experience it was to work under Greg. He extended a lot of trust and autonomy to me, but was always available when I needed guidance or advice. I've worked remotely since I first joined Blockstream, which is a scary thing to do because it's so easy to get disconnected from day-to-day stuff. But he spent a lot of time, even proactively reaching out, to help me deal with immigration, contracts, taxes, retirement accounts, rbtc harassment, you name it. And even though I spend a lot of my workday deep in the algebraic weeds, he had a remarkable understanding of pretty-much everything I did -- his ability to complete sentences may be NP-complete. And I'd see him do this not only with my crypto stuff, but with Core/Bitcoin architecture, with Russell's formal language work, with people in the wider Bitcoin industry, with Mozilla people and their codecs, and countless other examples.
It's funny to see people on rbtc suggest that he spends all of his time on Reddit, because of his apparently boundless capacity to engage with people on technical issues, even when they're being slow or adversarial. The truth is that he's like this in every single aspect of his professional life.
Having said all this, there are a lot of CTOs in the world, and a lot of companies in the world, but there is nothing like Bitcoin and there is nobody like Greg. As much as I enjoyed it, I am ecstatic that he can stop spending so much energy on this sort of personnel stuff, and on board meetings and company direction and whatnot, and focus on the areas where his talents can directly and dramatically change the world. And as his friend, I've been happy to see his increasing cheer and optimism over the last few months, as these responsibilities go away.
@_date: 2018-01-18 15:41:48
@_date: 2018-01-18 23:37:20
Can you link to this criticism? It seems fundamentally confused -- signature aggregation decreases the weight of transactions that use it, which makes _all_ transactions cheaper by virtue of decreasing block capacity pressure.
Ordinary users should be thrilled if large actors would aggregate their transactions. It seems like current practice is to make them gratuitiously large, driving up fees for everybody.
@_date: 2018-01-18 15:40:27
Unfortunately this would be a hardfork, by definition, and for such a small benefit it wouldn't be able to get support. It also sets a dangerous precedent that changes may happen which allow coins to be spent in ways their owners did not originally anticipate.
This is both directly coercive -- imagine somebody who was confident ECDSA was secure but not confident in Schnorr (this person would be silly, but you can imagine an analogous situation with some novel post-quantum signature rather than Schnorr, and then they're not silly) -- and also might theoretically break some off-chain system which somehow depended on coins only being spendable via ECDSA.
@_date: 2017-12-21 18:55:36


A market maker needs to be uncensorable and non-frontrunnable and it needs to be able to operate even when other people are using the network. A blockchain fails every one of these points, and it incentivizes miner manipulation. Ditto for whatever a "stablecoin" is, ditto for prediction markets, ditto for most every open-ended smart contract that you might want to create.
All of your examples are limited to being purely toys, not only because Ethereum is so slow that it's impossible to sync from start and takes [over eight hours even to fast sync, if you can do it]( but because a blockchain is fundamentally the wrong tool to be doing any of this on. Blockchain space is expensive and scarce, censorship-prone, permanently publicly visible, and requires global agreement to upgrade the rules.
And you haven't argued that these are impossible in Bitcoin, only that they're much harder. Except that Ethereum doesn't give you any tools to fix these limitations (it'd be much harder to do so on Ethereum than on Bitcoin because of the lack of multisignatures and because the account/nonce model handles 0confs so weirdly), it just makes it possible to deploy things without involving anybody smart enough to see them.
@_date: 2017-12-03 22:30:25
It is absolutely not. If your computer is hacked then your secret keys can be exfiltrated from your software wallet as soon as it is decrypted, and thereafter be used in any capacity at any time to sign any transaction foreverafter, even if the original computer is destroyed.
On the other hand your hardware wallet can only sign things that it is programmed to sign, while it is powered up, and requires a user ACK through its physical input. No secret data ever leaves the device for any reason, so there is nothing the compromised computer can do except request signatures.
@_date: 2017-12-03 15:31:43


Yes, a "worthy target" that is never connected directly to the Internet, does not communicate using arbitrary protocols, and spends 99% of its time physically powered off.
@_date: 2018-09-19 14:44:32


It's unfortunate that Bitcoin Script works this way, because there isn't really any need for it to. What you call "execution" would be better thought of as "verification of an execution", and there are many cases where these are different (and the verification is cheaper and more private than the execution).
As a simple example, verifying an ECDSA signature can be thought of as "verifying the execution" of the signing algorithm.
A more subtle example is the use of `OP_IF` with an explicit 0/1 input passed to indicate which branch to take. In ordinary programming you'd never see an explicit "take this branch" flag alongside an if-statement, because the point is to react differently to different conditions. In Bitcoin, a script pubkey has many conditions which might satisfy it, but at spending time, the precise condition is known, and validators only need to care about the semantics of the specific condition that gets used.
Had Satoshi thought in these terms he might have invented MAST, which AFAIK came from Russell O'Connor in 2010 or so. This is the idea that you replace `OP_IF`/`OP_ELSE` with a Merkle tree committing to all the different 
possibilities, and at spend-time you reveal only the one that gets used.
The idea behind scriptless scripts, mentioned a few posts down, is that you can go even further and _only reveal the script to the counterparties of the transaction_, publishing to the blockchain only a "proof" that the script was satisfied. In practice, there are a lot of constructions where this proof consists of a single Schnorr signature.
Segwit makes a small move in this direction by replacing the "scriptSig" (literally, a "signature" with opcodes that verifiers have to execute) with a "witness stack" (directly pushing the output of the scriptSig). Long before Segwit, non-push opcodes had been considered nonstandard in script signatures.
@_date: 2017-12-04 00:23:52


Ok, sure, but you can't even do this except convincing the hardware wallet to do so, somehow using an extremely limited, simple protocol to trick it into displaying bad data (or no data) on its display. I am not aware of a single instance of this ever happening to a product from one of the two popular Bitcoin hardware wallets.
To contrast, if your computer is owned enough that an attacker even has the opportunity to do this, a software wallet would require approximately zero additional effort for the attacker to empty.
@_date: 2017-12-05 14:43:40
Yes, this is about unnoticed inflation. For example, with Pedersen commitments you can create a 5 BTC output, say, and then later with a quantum computer "reinterpret it" as a 10 BTC output. In effect, in a post-quantum world, every Pedersen commitment would stop committing to anything.
By replacing the Pedersen commitments with El Gamal commitments, we could make the commitments untamperable, but Bulletproofs are only able to work with Pedersen commitments. It seems that adapting them to Bulletproofs would eliminate the logarithmic scaling, making them just as large or larger than the old rangeproofs.
@_date: 2018-09-22 15:29:05
Opentimestamps typically gets your commitment into the next block without causing any additional verification load on Bitcoin nodes, and without increasing fee pressure.
@_date: 2017-12-05 14:38:58
You can see from the perf numbers that verifying a single 64-bit rangeproof requires about 40 signature verifications' worth of work. The sublinear scaling is cool in that if you were verifying an entire blockchain's worth of rangeproofs, this 40x would drop to close to 1 if you had enough memory. (Though, if we had Schnorr signatures or a different variant of ECDSA, we'd be able to batch-verify the signatures too, so the 40x comparison would hold constant as both signatures and bulletproofs became more efficient.)
However the fact remains that when you see an ordinary transaction in real-time, it's only going to have a couple of outputs probably, and it'll take 40x as much work to verify its rangeproofs as it will to verify its script signatures. And in real-time you don't have a ton of other data to batch with, you've just gotta take this 40x hit, and then take it again a second later when the next transaction appears.
In my personal opinion, if we got this 40x number down to 5x, I'd stop considering performance to be a problem for CT-on-bitcoin (and assuming we continued to have batch verification of some form, so that IBD wouldn't require 5x the power). Whether that happened through better asymptotics or just better constants probably wouldn't be too important to me.
Having said that, once performance stops being a problem, we still have the issue of unconditional versus computational soundness. Right now an attacker with a quantum computer is able to make fake Bulletproofs, which I expect would be a hard pill to swallow for a large portion of the community.
@_date: 2017-12-04 15:03:32
I'm saying that breaking a hardware wallet requires you first do everything required to break a software wallet (owning a PC, which is not difficult) _and then_ do an extremely difficult task that has never been done before.
Trusting a software wallet with nontrivial amounts of value is insane. It is bad security practice. You will lose money doing this.
@_date: 2017-12-18 18:43:18
It's the opposite. You divide by 0.09 rather than 0.1, so it's actually roughly a 10% increase. $3220/GBTC corresponds to $35k BTC.
@_date: 2017-12-05 19:12:47
Both :(
@_date: 2018-09-19 15:11:38
It's disappointing that this article fails to cite any of the people who actually invented this technique, or to draw attention to the improved privacy and scalability aspects versus the technique in the OP.
@_date: 2017-12-19 17:59:37
Can you cite anything that's possible on Ethereum but actually isn't on Bitcoin?
It appears that the only reason there are hundreds of projects on Ethereum is that the ecosystem encourages reckless development from companies who lack the (extremely scarce) cryptographic or legal expertise required to write non-toy smart contracts.
@_date: 2015-06-04 16:49:05
For reference (in case the Monero folks are thinking about using our sigs), you can have each ring sign a different message, and so combine the ring signatures for multiple inputs in a single transaction without any requirement of homogeneity.
That'd give them the 2x space improvement from the adam/LWW sigs plus the smaller (~1/n for rings of size n) savings from our improvement.
@_date: 2015-06-02 15:17:57
This is a good observation. I'm not quite sure how to say this, but with a hash chain you *want* it to fall apart if any link is broken, in the sense that validation of the chain should fail.
The point of a hash chain is to prove that some data has not been tampered with. So if anything changes, the hash chain "falls apart" but this is exactly its purpose: to reveal tampering. So apparent fragility is actually exactly the security property that you want.
@_date: 2015-12-09 07:43:00
Can you give an example of an actual Bitcoin script that implements a loop? You can certainly put indexes etc onto the altstack but you have no way of jumping around the code so you can't use them to produce loops.
@_date: 2018-09-21 15:35:46
Thanks! Looks good now.
@_date: 2015-06-09 14:27:30
Unless I've misread the code, that's 54 bytes: one byte version, a 20-byte pubkeyhash, and a 33-byte "blinding key" which is an EC point. Traditional addresses are only 21 bytes; they don't have the blinding key.
@_date: 2015-06-02 15:12:10
Not quite -- for use in Monero you would need to adapt this construction to be linkable so that double-spending could be detected. Our ring signatures are just plain old ring signatures, no linkability.
(This is really easy to do -- the AOS signatures that we adapted have been adapted to be linkable by Lui, Wei and Wong in [this paper]( and since their changes are unrelated to ours, you can easily apply both. But it does have to be done.)
@_date: 2015-06-02 15:02:38
Thank you :) that one was me.
@_date: 2015-06-09 14:05:46
Yes, an attacker with infinite computing power can determine the discrete logarithm of one of the generators with respect to the first, then the Pedersen commitments change from a "normal hash" to a chameleon hash, granting him or her the power to change commitments.
Chameleon hashes are explained in the [Borromean Ring Signature writeup]( (Coincidentally, the mystery application of those ring signatures is that they are a critical component of CT.)
@_date: 2015-06-02 15:27:28
It's closer to "just implementation" than there being any theoretical challenge.
More technically, in the language of our paper, AOS signatures have these graph structures where each vertex v is labeled by a hash of some EC point R_v. The LWW signatures add another point S_v which is computed by the same algebra but starting with a different basepoint; this checks that the key images are valid. ("Key images" in LWW scheme are computed from private keys same way as public keys, but with the different basepoint.)
Our innovation was to enable more complex graph structures, but the computation on the individual vertices (almost) stays the same, except that now some vertices can have multiple R_v values. So to making it linkable is as easy as adding S_v values, except unlike in LWW some vertices will have more than one of them.
Having said that, there is a "theoretical challenge" in explicitly writing that out and making sure that the resulting signatures are actually signable and secure. Please don't go implementing cryptosystems off of my Reddit posts. :)
@_date: 2015-06-10 15:12:25
Can you PM me an email address or other means of contact? I'd like to hammer out section 3.5, calculate what the security parameters mean in terms of bit-security, and write a security proof. (Security proof will be easy: the hard part is showing the discrete-logs-are-equal proof works, but that's folklore so we can skip it :))
On a side note, you might want to put a blurb about yourself on your website -- I had to look up who you were on the Doom Wiki!
@_date: 2015-06-09 14:48:26
"Mixing" by transferring to and from a sidechain will get you an anonymity set which is the size of all outputs of the same size currently locked to that sidechain. If the sidechain is in active use this will always be a fairly large number, no timing gymnastics required.
Having said that, describing CT as a "Bitcoin mixer" is a gross simplification and misrepresents our goals in designing the system. The goal is to prevent analysis of the type mentioned in e.g. [Mike Hearn's old article on merge avoidance]( for transactions on the sidechain itself. This is critical for privacy of ordinary users using the system.
@_date: 2015-06-09 16:13:05
Mining is done by a 5-of-7 signature by the developers; alpha does not use a distributed consensus. It's a test network and naturally does not have economic resistance toward reorg attacks -- and we did not want to deal with such griefers for a prototype release! :)
**Edit**: Full disclosure: for those who don't know, I am Andrew Poelstra. I'm a blocksigner, a functionary for the peg, and I'm one of the 2-of-3 signers for the alert system.
@_date: 2018-08-01 22:07:36
Please implement this in sage and label your variables as to what is secret (and whom it is known to) and what is public. (Or at least write a Reddit post with proper formatting. Use Ã— for multiplication, * is interpreted by Reddit as a formatting state toggle. If you multiply scalars and points, please keep the scalar on the left. If you are indexing things, use _ to indicate subscripting, we cannot infer what characters within words have independent meanings.)
As written this looks like gibberish, and it takes a lot of time and energy to try and interpret it as anything else. Ditto for everything you have posted on the mailing list. I don't mean this to be mean, I literally mean your posts are incomprehensible.
It might help if you answered Pieter's question: what are the specific formulas used in the 2-of-3 case?
@_date: 2015-06-09 22:15:24
Hi Mixlez! Are you the author of this paper?
I'm pretty excited about this -- I think it works, though I'm not certain about the security parameters (like, it's not clear to me that either curve can be as small as 256 bits while retaining the 128-bit security that discrete log gives us for ordinary signatures). I'd like if you could add a sentence explaining what the parameters `t` and `l` are supposed to represent as well as upper and lower bounds on `T`.
Also, maybe this is a dumb/blind question, but can you clarify in Section 3.6 what the variable `b` is supposed to be?
@_date: 2015-06-09 14:11:08
There is a substantial amount of BTC which has not moved in a long time and which are likely to be unmonitored; these could be stolen without detection by somebody in possession of an unpublished ECC break. So the incentives are already pretty high.
Also, "break ECC" in this case means to break the discrete logarithm problem, which is literally the strongest break possible (in the sense that breaking discrete logarithm would immediately break all other cryptographic assumptions, while the reverse is not true). The idea of somebody doing this entirely in secret, while public academia has been working on this for thirtysomething years without even getting to the "possible with all the computing power in the world" level of feasibility, does not seem very plausible to me.
@_date: 2014-10-21 19:22:02
In fact, when I wrote this article there were many people advocating pure proof of stake. This sort of thing has noticeably gone down over the last several months in favour of hybrid arguments, I suspect largely because of the existence of the paper.
@_date: 2015-06-09 14:52:36
If you think we're clever, I encourage you to look at our arguments surrounding blocksize with a more open mind. In particular, there is not a "smart side" and "stupid side"; there is a practical disagreement on the scope and timing of required changes, and also a philosophical one on the value of decentralization versus scalability.
Please reconsider throwing insults around; I'm not sure what benefit you derive from it, and there is a lot of stress in the world without your adding to it. We are all in this together.
@_date: 2015-06-09 16:45:54
Well, PoS would get griefed too by nothing-at-stake attacks .. and no, we don't learn anything from griefing if we understand it well enough to predict exactly how it'd happen!
@_date: 2014-10-21 19:24:25
The paper's thesis is "you cannot form a distributed consensus from proof-of-stake". It argues that logically. It is certainly not a strawman --- you can find countless posts on reddit and BitcoinTalk, as well as early claims by the developers of systems such as Peercoin and Blackcoin, suggesting that one -can- form consensus from PoS.
Plus, it's not exactly a trivial result. The paper is not an easy read exactly because I was forced to say some quite subtle things in order to remain correct while being general enough to prevent "counterexamples" which dodged the argument but not the result.
@_date: 2014-10-21 19:36:44
I see Bruce Schneier arguing that you cannot brute force a 2^256 space. This is obviously true. But what does it have to do with grinding through proof-of-stake signer sets?
@_date: 2014-10-21 19:45:25
Sure, but the "amount of stake" associated to each set is a consensus issue, so you'd need some sort of distributed consensus to do that..
@_date: 2014-12-07 00:04:17
(a) Yup, as Gibbs says tacotime is in contact with many of us. In addition I am personaly in comminication with the Monero research team.
(b) Yes, I have. (Arguably I am one of their experts.) **Edit:** To clarify, I did do my own independent audit of the Bytecoin signature scheme. I concluded that their sucurity argument was sound. I've been in contact with the Monero research team and we have discussed this among other things.
@_date: 2014-10-21 20:23:50
The total amount of stake may be a function of blockheight, or it may be a function of user action. In either case the distribution will be determined by the set of accepted transactions, which requires a distributed consensus to well-define.
@_date: 2014-10-23 03:33:38
It's not quite an abuse -- if you move a bitcoin into a sidechain, what appears in the sidechain is a "bitcoin" in the sense that it is definitely only redeemable on the Bitcoin blockchain. But it's still a distinct asset. For example, if I ask for 1BTC and am given the choice between a bitcoin on a well-known well-secured sidechain, and one on a sketchy sidechain without much mining power and with questionable features, I'll take the one on the good chain every time. And this is more than theoretical: if you move the "good" bitcoin and the "bad" bitcoin to some third chain, they will still not be interchangeable, since the good one will be only redeemable on the good chain and the bad one only on the bad chain.
My feeling is that the "move" language is more abusive than the "peg" language. But both are appropriate in the "common" case where you are only watching bitcoins on sidechains that you trust. Then you can treat them all as bitcoins, even though on a technical level they are distinct things. Thens whether you say "move" or "peg" is simply a choice of what level of abstraction you are thinking at.
@_date: 2014-12-06 22:32:48
Hi, I'm the primary "author" of this paper --- author in quotes since as many of you have noticed, almost none of the paper has actually been written :). This is not supposed to be a public document; it is an outline-stage draft I was hosting to share with other researchers to give an idea of what we were planning to write up. I did not expect Reddit to take interest in random junk on my webserver. I guess I'd better check what else is on there!
A couple quick comments:
1. The main thrust of this paper will be an extended version of the ring signature scheme used in Monero. This provides some minor efficiency improvements and adds the ability to do multisignature transactions which appear identically to single-signer ones. We'll also provide a new security analysis of the signatures, since the analysis in the original was a bit terse (and its author somewhat discredited by the scandal surrounding Bytecoin). This stuff is quite technical and has not been written out anywhere. However, the rest of what will be in the paper is publically available [here]( and [here](
2. The paper will describe some new cryptographic primitives which could be used in a blockchain to avoid explicit linkage between transactions. It certainly does not describe or advocate how to implement such things in Bitcoin. Indeed, this would dramatically change the efficiency and privacy properties of Bitcoin in ways that would be unacceptable to many existing users.
@_date: 2014-02-27 02:19:30
I'm not clear why you think this is less complicated (espicially given that "merkle-sum tree" is hardly complicated in the first place). You've expanded the amount of public data from a single hash to the entirety of the company's address book, and you've published individual account balances to boot. By publicizing this kind of data you are exposing company internals for no benefit, giving advantage to competitors and introducing a privacy risk.
Furthermore, there is now no link between what one user sees and what another sees. If two users receive different data, who is to say which one (if any) is correct, and who is to say that the business is at fault rather than one of the users trying to sow doubt? It is not clear what such a publication is supposed to be 'proof' of.
The validation is now much harder and requires every user to download a massive amount of data and then troll through it all in O(n), versus gmaxwell's original O(log(n)) verification. Later in the thread you propose outsourcing this verification to "trusted" third parties and having users inform these third parties of their balances. This requirement is a huge breach of privacy and is absolutely insane from a business perspective. In suggesting this you seem to have given up the goal of user-auditability.
Further down, you suggest that the business paginates the data to make the massive verification load smaller. Surely you can see that it's now trivial to give different users different data? (Trusting third parties with the "whole" list has the same problem I mentioned above in that it's impossible to tell who is at fault if the data does not match.)
I'm not sure why you accuse nullc of not being open-minded. He has taken the time to analyze your idea and explain to you the problems with it, despite the fact that it's objectively terrible and all of the problems with it were addressed in the original merkle-sum tree proposal.
@_date: 2014-10-21 19:20:51
Why is a brute-force attempt automatically infeasible? Can you write down some numbers and see why they don't apply to grinding through a random individual selection from a small set?
@_date: 2014-10-23 03:35:43
Well, it would be very unlikely for a sidechain to totally "die" in the sense that no mining occurs on it. In particular, if mining is all that's needed to move some bitcoins off of an otherwise-worthless chain, that mining will occur since it's valuable. (Worst case the holders of "stuck" coins would pay the miners, or mine themselves.)
@_date: 2014-10-22 21:37:18
I am also a co-author of the paper. While the language used is often quite strong, there are [serious problems with most differentiating altcoins]( and it's not clear what (if any) purpose non-differentiating altcoins serve. A market for markets' sake is not a productive use of money, and there is a lot of technical misinformation that makes it even more wasteful than it needs to be.
Note that the sidechains paper does discuss innovation from a number of altcoins -- notably Monero and Freicoin. We are not "anti-alt" for business or irrational reasons, and recognize innovative projects when we see them. But the fact is that the innovation happening in the altcoin world could be done in a much more efficient (and effective) environment than the altcoin community.
@_date: 2017-02-04 01:59:42
As u/maaku said, the on-chain scalability is limited to initial block download and catch-up, which MW achieves by compressing blocks together. Somebody tracking the chain in real-time has to deal with entire blocks that are roughly the same size as Bitcoin blocks plus CT.
Also, the scalability does not apply to the CT rangeproofs; so while the "90Gb of chain becomes 1Mb!" looks super impressive, it's not great next to a UTXO set in which every output has a multi-kilobyte rangeproof.
To contrast, off-chain scalability has none of these limitations and it's definitely something we'd like to be possible.
@_date: 2017-02-10 21:41:18
In this scenario, the closer to realtime you want to be, the smaller the p2p network you can interact with is. This is bad because many people actually want to use the network in realtime.
@_date: 2014-12-07 01:19:05
It's not fatal. The "failure mode" here is still better than current Bitcoin, or even current Monero.
@_date: 2017-02-26 13:06:33


If you're referring to deterministic nonce generation, this has nothing to do at all with curve selection, and libsecp256k1 has supported it for years. 
As for the safecurves commentary, this has been addressed several times, such as [this bitcointalk post](
@_date: 2017-02-04 02:39:51
I agree with your concern. I don't know the developers on the grin project (beyond their pseudonyms) and don't know their intentions. At this point though, the project is far from being a deployable altcoin, so I don't think that I'm unwittingly pumping any future ICO or whatever by giving talks like this. In fact part of my motivation in being so public about the tech is to undermine any attempts to monopolize it.
Aside from that, it's worth noting that my talk is purely about the cryptographic aspects of MW, which are very general and potentially applicable to Bitcoin (and will definitely be applicable to Elements). For example, I described a trick using the algebra of Schnorr signatures to do undetectable atomic swaps that would be applicable to Bitcoin if it supported Schnorr signatures.
@_date: 2017-02-04 00:22:36
It is a design for a completely separate blockchain (though in the far future, after the technology has proven itself and the user experience story is sorted out, it could perhaps be grafted onto Bitcoin). Its scaling properties are quite different from those of Bitcoin.
This doesn't require the chain have its own token; it could be implemented as a sidechain or even simultaneously be a sidechain and have a native assets (in a freimarkets-style multi-asset chain).
At the technical level, this is the sort of question that can be decided very late in the game. Right now there is a ton of activity on the research and development level which is completely agnostic to the asset types on a working blockchain.
@_date: 2017-02-10 16:16:23
You'd only be able to validate transactions that happened before your last catch-up.
So as an extreme example, suppose you'd always catch up every morning. If someone sent you coins in exchange for something, you'd have to say "sorry, I won't receive your payment until tomorrow morning". Further, during that day, if the network were so resource-intensive that most people were always on a delay like this, then it would make people trying to process transactions in real-time much more vulnerable to sybil attacks since the apparent p2p network is very small.
Though on the other hand, if it's acceptable for you to only process payments once a day, you can do this on purpose and save resources.
@_date: 2017-03-21 23:06:18
To summarize -- what this construction lets you do (if Bitcoin supported Schnorr signatures) is construct a Lightning path where each channel appears to be a simple single-signer transaction, with no linkage between them, even as far as the participants in the individual channels can see (they each have an "adaptor key" that they can use to link their sending channel to their receiving one, but this happens off-chain).
It'd be a big win for Lightning's privacy and efficiency.
@_date: 2014-05-30 20:48:16
I have a recent document which describes the main problem with Proof of Stake, the problem of "costless simulation" (thanks to nullc for the term). I see your karma fluctuating wildly every time I refresh this page, and the first reply says simply that "the devs don't like it" so I want to clarify that there are serious problems with PoS as a mechanism for distributed consensus.
The animosity that you see toward PoS is because it is often proposed as a solution but these problems are never addressed (or even mentioned), leading to long and repetitive conversations. It has nothing to do with personal bias, an aversion to new ideas, or some mining-themed motive to keep PoS down.
@_date: 2017-03-24 20:00:37
For those who don't want to follow the link: the first line of the article is a correction explaining that the headline is actually entirely false.
@_date: 2018-10-12 13:46:19
Yep, exactly. You would also need to write code to support blocksigner and peg-out signing because the bundled Python scripts in Elements (if we even still ship those) are not production-ready.
@_date: 2019-11-07 23:55:01
Even if threshold cryptography were simpler than MuSig (it is not, by a long shot), it is not a replacement for MuSig. It is not in the same category.


If you are suggesting the use of a Knowledge of Secret Key (KOSK)-type scheme for combining keys, rather than the completely noninteractive MuSig scheme, the result will be insecure when used for Taproot outputs.
@_date: 2018-10-10 23:13:09
Yes. The validation is basically a vendored version of [Elements]( if you want to get your hands dirty seeing how the code works. You can search the `validatepegin` command-line option to see how Bitcoin consensus interacts with Liquid/Elements consensus.
@_date: 2018-10-10 22:58:19
They monitor the Liquid network and validate transactions of course, and also monitor the Bitcoin network to confirm that all peg-ins actually have sufficiently-deeply-buried corresponding transactions on the Bitcoin blockchain.
@_date: 2018-10-02 14:36:19
You are correct, though as mentioned in the BIP this has probability 2^-127 of happening, i.e. it will not ever happen to anybody.
This is not true for all elliptic curves, just the one that Bitcoin uses, which is probably why the specification suggests to do this. But it results in extra nontrivial logic for implementors which is literally impossible to test, so you can also see why they wouldn't do it.
@_date: 2018-10-12 13:52:39


You need to write new software once to support CA on Liquid -- but from there, every asset is essentially the same.


I don't want to ignore this but I don't have time to go into too much depth here -- but essentially, the trust and custodianship model is completely different. Issued assets on Liquid are completely divorced from the Bitcoin blockchain, can be SPV validated (on Liquid), have well-defined issuance semantics (determined at initial issuance and publicly verifiable), etc.


Yes, though you will need to pay network fees in LBTC.


If somebody creates COOLASSET this will be visible and clearly not LBTC. If they want to reveal the amount that they are issuing, they may do so, but the protocol also allows them to blind this. But when *sending* COOLASSET, even to its initial destination, there is no distinction (from the pov of an observer of the blockchain) between LBTC and COOLASSET.
One major exception is when doing a peg-out -- you need to reveal your asset and it needs to be LBTC. But transactions within the chain have completely blinded assets.
@_date: 2018-10-10 22:55:40
It isn't. Transactions within Liquid are created by the individual transactors and collated into blocks, just like in Bitcoin. Unlike "green addresses", these transactions have no identifying information whatsoever, and the network is set up so that they are only sent to the network over Tor. The system is designed specifically to prevent participants from "signing off" on each others' transactions, except to the extent that they "sign off" on the network's continued functioning.
@_date: 2018-10-10 23:01:38
In addition to being a Bitcoin sidechain, Liquid supports other assets (which would be backed by some trusted issuer) which can be transacted in the same way as LBTC and which validators can't distinguish from LBTC.
@_date: 2017-08-30 23:37:53
You probably want to [rethink that 1% number]( which is insanely conservative.
And perhaps also rethink your lifestyle if you're somehow burning through $200k every year. That's quite a difficult amount of money to spend without deliberately buying pointlessly expensive versions of everything.
@_date: 2018-10-10 23:11:09


It would plausibly be a better business, but it wouldn't be good for users who switched from Bitcoin. Liquid requires much more computing power to validate than Bitcoin does, and its blocks are signed by a fixed set of known participants rather than the open mining ecosystem.
Basically, Liquid is a clear improvement over any system that involves trusting individual exchanges, but Bitcoin itself is definitely not such a system.
@_date: 2017-08-30 23:43:13


Can you give an example of one?
@_date: 2018-10-10 23:24:41
As Adam says, Bulletproofs will make the transactions smaller, though the verification cost per byte stays roughly the same so this wouldn't affect the 1Mb limit. But it would increase the maximum number of transactions per block. We have some more crypto tricks up our sleeves which are similar.
Less excitingly, but more practically, as the network operates we will learn more about how transactions and blocks are propagated (and have more time to optimize this aspect of the network). I expect that this will let us reduce the block interval from 1 minute.
@_date: 2017-08-28 17:01:06


On Aug 1st the blocksize limit was replaced by a block weight limit which leads to an effective increase to 2Mb blocks for typical transactions.


Anybody can fork. Aside from the blocktime increasing by half an hour or so for a couple of days (which eventually led to a ~3.5% difficulty drop, which tells you how much total delaying actually occured), can you name any negative effects of people forking off?
Does a 3% drop in transaction throughput warrant recklessly increasing the blocksize 3 months after the last increase and doing an unprecedented hardfork that requires all validators to update immediately? Ignoring the fact that this is actually impossible, "Core" cannot unilaterally change the rules like this and Bitcoin can't be hijacked by random groups of elites.


The last fork didn't result in multiple versions of Bitcoin. If you were paying attention you could've gotten a 5-20% increase in your Bitcoin holdings but if you weren't you wouldn't have noticed a single thing from the last one.


I don't know the LN roadmap, but segwit wallets will certainly be on the scene in the coming months. The changes aren't very elaborate, they're just easier and safer to do after the consensus changes are locked into place.
@_date: 2017-08-06 15:05:10
Here's one scheme, which I believe originally came from Tier Nolan.
Each party puts their coins into an output that can be spent by (a) both parties or (b) the other party, provided she provides the preimage to some hash. Each party's output should have the same hash, and one of them should initially know the preimage.
Then the one who knows the preimage has to reveal it on the blockchain to take her coins, allowing the other party to learn it and then take his coins.
You need to be a bit careful about having locktimed back-out transactions so that nobody can lose money in case the protocol stalls or somebody backs out. But technically that's all there is to it. No need for any fancy extra infrastructure.
Having said that, Lighting basically works by chaining atomic swaps together (and updating them before posting them to the blockchain, so it's a bit more elaborate). So it wouldn't surprise me if widely used software for doing atomic swaps came out alongside Lightning ... but there's nothing stopping someone from doing it today, and indeed it looks from another post like Nicolas Dorier has been doing that.
@_date: 2018-10-10 22:50:36


I can tell you've never written an adversarial multisignature wallet designed to run with no human involvement :)
@_date: 2019-11-08 18:18:40
Can you cite a specific attack that signing of public keys (not signing *with* public keys) would prevent? Who signs? With what key?
@_date: 2017-08-15 23:30:38
Bitcoin data is all self-authenticated. The satellites can't do anything except refuse to relay valid blocks. If you have other sources of data (and note that these only need to have enough bandwidth to transmit headers, e.g. SMS is sufficient), this will have no effect. If you don't, you couldn't even access the chain in the non-adversarial case before the satellite link.
@_date: 2017-08-06 15:11:55
[Scriptless Scripts](
@_date: 2017-08-23 22:04:26
I wasn't sure what your point was so I tried to word my reply to share the same level of snarkiness, whatever that turned out to be :)
@_date: 2018-10-10 22:49:28


Yep. The CT consensus code is in [this function]( and the underlying crypto implemented in [this module]( of libsecp256k1-zkp.


It definitely would! For example, if our [method of generating a second EC generator]( were not public, we could have chosen it maliciously and nobody would've been the wiser.
@_date: 2016-11-18 16:08:53
Here is a [summary of Segwit and some of its benefits](
Segwit is not [unnecessarily complex]( ([link 2]( ([link 3]( It in fact [reduces complexity]( in [several senses](
Segwit is opposed primarily by the denizens of another subreddit, who routinely post [outright lies]( about it and [advocate theft]( ([link 2]( as an alternative. They also [argue that segwit should have been a hardfork]( though they have an [unclear understanding of what a hardfork is](  or [what Segwit would look like as a hardfork](  while the people [who actually implemented the first version of Segwit as a hardfork disagree]( ([link 2](
@_date: 2017-08-10 23:28:04


Are you saying that the likelihood of finding a PoW solution is affected by the contents of the block? That is not true.
@_date: 2016-11-01 23:28:34


Thanks for the support, but the Schnorr code that's in Alpha does not resemble the code that will be eventually proposed for Bitcoin. In particular, its signatures do not sign the public key, which is fine in Bitcoin (where transactions already commit to all relevant pubkeys) but might be a footgun in other situations; also the multisignature code is not secure against adversarial signers.
@_date: 2017-08-23 15:28:25
This is very worrying. It will be extremely damaging to the fungibility of Bitcoin if the IRS decides that certain coins are "stolen" or otherwise tainted.
I hope all Bitcoin users will keep privacy preservation in mind when using the network -- not reusing addresses, using coinjoin whenever possible, etc. I've been working on some things (aggregate signatures in particular) that hopefully will greatly improve the situation: incentivizing coinjoin, enabling new types of off-chain swaps, enabling multisignature addresses that look like single-signer ones. Generally making coins look the same and the transaction graph less meaningful.
@_date: 2016-11-02 23:04:35
I have an old article [arguing that the thermodynamics of mining prevents these economies of scale and even creates a diseconomy of scale](
Certainly my timing was wrong, but I'm not convinced that my overall thesis is wrong.
@_date: 2017-08-18 00:47:18
I understand s2x is designed so that it cannot become the longest valid chain, because it is going to follow an invalid block at some predetermined height.
@_date: 2016-11-20 17:55:05
It would be nice to have new RPC calls, but it wouldn't be wise to do so by simply adding a `getblocksw` that copied all the warts of `getblock`. In particular, we'd want the new RPCs to have some sort of versioning capability to avoid problems like this in the future; we'd want to evaluate every single RPC call to see if it was affected under any combination (for example, `getblock &lt;block&gt; false` which is what the OP is talking about, is a little-known corner of the RPC interface; most uses of raw hex-encoded blocks would be better served by using the p2p protocol which saves you the hex-encoding and is backward compatible); we'd also want to consider fixing other warts such as having decimal-encoded amounts in JSON fields everywhere.
I'm not saying this is a bad idea, just that the scope is pretty big and has a lot of room for bike-shedding. Probably somebody would have to implement a full new RPC interface, write software that exposes it, and get some mindshare and usage experience behind it before it could be considered as something that Core could expose.
Simply adding extra RPC calls without considering future-proofing or maintenance burden is very unlikely to be accepted into Core.
**Edit:** Adding a flag to `getblock` to restore the old behaviour, or making it default, probably would stand a better chance because this is a known issue that's caused people problems in the wild, and it has a much smaller scope than adding a new RPC would.
@_date: 2017-08-23 21:59:34
Yes, you can tell he's a real expert by how he [replaced the authors' names with his own, violating the terms of the MIT license and stealing credit]( for [work that he doesn't even understand](
@_date: 2016-11-20 17:48:37
`getrawtransaction` should not be affected, though it will return extra data that you need to throw away if you don't understand the witness data.
@_date: 2018-06-28 13:48:45
I'm pretty sure if you validate a transaction and cache the pairing that was computed as part of its signatures, you can reuse that cached value when validating a block-level aggregate. So you still do get the benefit of caching, it's just more complicated than caching a "transaction is valid" bit.
@_date: 2017-08-06 15:51:24
I believe Monero does not support hash preimages (this would interact badly with their ring signatures) so atomic swapping with them is quite difficult without something like scriptless scripts.
@_date: 2019-08-20 16:29:59
An example of something you can't do in Miniscript is [put up a bounty for finding hash collisions](
@_date: 2016-11-06 21:51:55


Readers should be aware that in [a thread he posted on rbtc, Happy5488Paint admitted he does not actually know anything about the engineering decisions in segwit]( he is just making false claims for their own sake.
As for "why can't miners choose resource limits", this has been covered ad nauseum, recently in [this blog post by David Vorick]( Miners are compensated for resource usage; full nodes are not; further miners are incentivized to reduce others' ability to validate.
@_date: 2018-06-28 13:46:54
As maaku says, if you noninteractively aggregate, there is still one pairing per original message, which is the bulk of the validation time.
@_date: 2016-11-20 17:31:43


You can run an old bitcoind behind a new one (and they only speak the p2p protocol, which is absolutely backward-compatible), if you are worried about the consequences of updating.
@_date: 2019-08-21 16:14:28
Sure, there is [my Rust library]( which has an `examples/` directory with some example usage.
I want to clean up some things related to checking necessary/sufficient conditions, and fix up the API for fee estimators, then I will release a new version. But for now the [latest release]( is usable and pretty close to the final API.
@_date: 2019-08-20 20:57:33
One benefit of Miniscript is that you can write a human-parseable representation of your policy, and your heirs can type that into commercial-off-the-shelf software to be able to find and access your coins.
With traditional Script it's hopeless; you could write down a hex-encoded script but good luck attaching auxiliary information like BIP32 derivation paths, and even if you can do that, there's a good chance whatever software you used to generate it won't be around in a couple years (and isn't even very usable today).
@_date: 2019-08-21 02:20:49
Well, "only a concept" is [underselling the extensive ongoing development on Simplicity](
But you're right that you can't really *use* Simplicity today, at least not on a blockchain, and certainly not without knowing Haskell :)
@_date: 2019-08-20 20:09:22
No, it means that the compiler, when finding an appropriate script, will make the 99% case cheaper even if it means making the 1% case more expensive.
@_date: 2018-06-03 21:15:49
\&gt; Gotcha bitch.
Your link has nothing at all to do with "green card holders or non\-residents" and everything to do with China. Why would you use US immigration categories when refering to Chinese border control law?
@_date: 2019-08-20 22:03:52
Great question.
One important way Miniscript differs from regular Script is that Miniscript *describes the conditions under which coins can be spent*, while Script *describes the actions that a Script interpreter must take*. This difference is crucial because the former maps well to how a user thinks about their coins, while the latter does not.
In fact, if Script had unbounded loops or jump opcodes, it would be impossible in general to prove that a given script had the semantics that a user desired. Script doesn't have these things, so it's theoretically possible, but practically it's probably not.
However, Miniscript can only do three things: check signatures, check hash preimages, and check timelocks. Simplicity lets you do **arbitrary computation** in the same "describing conditions for satisfaction, not instructions to execute" paradigm. This makes it extremely powerful, much more powerful than Script or Miniscript, but also much more complex to use and implement.
As an aside, Miniscript, by virtue of mapping into Bitcoin script, is also a fair bit uglier than Simplicity, which was designed from scratch to be easy to analyze.
@_date: 2019-08-20 17:34:14
Not *really*... what this script does is custody coins in the 11-of-15 functionaries, but in case the functionaries are unable to spend the coins (say, because all of their HSMs blow up) then the coins transfer to custody of the 2-of-3 emergency keys.
* As long as the federation is alive they have the technical capability, by colluding, to take the coins regardless of users' wishes.
* If the federation is dead and the emergency keys are active, this does not mean the coins return to the users' control (this would be impossible with a federated sidechain design since coins can change hands much faster than Bitcoin can track). Instead the coins are controlled by the emergency keyholders, who are in turn bound by the terms of the Liquid contract. So the failure mode of the thing is to revert to a traditional trust-based model rather than to revert to Bitcoin's security.
@_date: 2019-08-20 16:51:19
Sure, you could write a policy like
    100
    thresh(1,and(older(365),pk(A2)),and(older(365),pk(B2)),and(older(365),pk(C2)))
which does exactly what you describe (if I understand this correctly). This compiles to the following miniscript
Which, if you replaced the six keys `A1`, `B1`, `C1`, `A2`, `B2`, `C2` with actual public keys, would correspond to a Bitcoin Script you can use on the blockchain.
**Edit:** Our current compilers are not very smart about n-of-n "thresholds"; you could write the same policy as 
and it would instead compile to
which might be cheaper (I didn't check). So there is certainly room to improve here :)
@_date: 2019-08-21 02:23:41
One advantage is that Miniscript may let you quickly identify and fix certain common mistakes related to atomic swaps.
For example, when you put up your coins, you want to require your signature on both branches (both the "reveal hash" and "emergency backout" clauses). This seems unnecessary if you're not the one revealing the hash, but not doing so actually leads to a malleability vector in some cases.
With Miniscript you can say "I want to check that I'm (a) always necessary; and (b) after a timeout, also sufficient, to spend these coins" and quickly/programmatically check that. The alternative is to do a bunch of ad-hoc analysis to assure yourself that you're safe, which is hard both because it's ad-hoc and because you're dealing directly in Script.
@_date: 2018-06-03 21:13:18
If a customs agent finds a hardware wallet on you, it won't matter what cryptocurrency you have. They'll be interested in what utxos are controlled by that wallet.
@_date: 2019-08-20 17:17:12
Transactions in Bitcoin are atomic; there is no notion of "initiating" or "completing" a transaction.
It's straightforward to define a policy which allows spending by (1) 3 signers; (2) a timelock and 2 of the above signers; or (3) a bigger timelock and 1 of the above signers.
Signers should use distinct keys for each of these branches. If you reuse keys you can get something similar to what you describe, where a signature on one branch can be reused as a signature on a different branch if more signers show up, but this introduces potential malleability vectors which require extra ad hoc analysis to deal with. So this is strongly discouraged.
@_date: 2016-11-02 22:59:15
They are still going and [even filing bugs on unrelated projects to insist they have a different trust model than they do](
@_date: 2016-11-05 12:11:29


Don't accept invalid blocks, don't relay invalid blocks. The people spending money to produce them don't even factor into it.
@_date: 2016-11-05 13:26:12


If this assumption fails then Bitcoin has failed. There's not really any way around this that I see.


Even if this is true (what I see is more like "people with nontrivial amounts of money pushing memes without code"), these major actors can't change what code you run on your system or what blocks you accept.
@_date: 2019-08-20 17:20:37
Allow the coins to be spent if
* 11 of the 15 `A`, `B`, ..., `P` keys are willing to sign (probability 99%); or
* 2048 blocks have passed since the coins were received, and 2 of the emergency keys `Q` `R` S` are willing to sign (probability 1%)
@_date: 2017-11-07 05:09:20
The oracle can just sign anything and you can build a SNARK around verifying the signature. (Really what I'm doing here is a form of "witness encryption" but zk-snarks are more widely known in this space, hence my use of them.) You're right that this would be basically a 2-of-3 where one party didn't know about their particiatpion.
@_date: 2017-11-20 14:09:59


No, adding more signatures will not solve the problem of signatures being non-unique ;)
@_date: 2017-11-17 14:07:17
He said it was _designed_ to be private. Certainly there is still work to be done.
@_date: 2017-11-14 17:13:26
Yes, the verification is basically computing a giant sum `a1G1 + a2G2 + ...` with 128 give-or-take terms per output, and checking that the final result is 0. It is extremely parallelizable.
Relatedly, you can do batch validation though I don't know how much that'd gain you.
@_date: 2019-02-18 17:59:13


I really like reading comments like this. They are much more encouraging than the inverse "omglol ethereum is going so much faster, bitcoin is dead!" comments that seem to be more common.
My view is not that things are speeding up. Eventually I expect we will wind up at the many-year or even multi-decade development cycle common to standards bodies or large banks. But for now the industry is young and somewhat flexible and we have room to make some fairly dramatic changes. Schnorr has been something we've talked about for many years, and the last year in particular has seen constant and sustained development to ensure that it was something we could deploy in a robust manner. Migration difficulty, and risk (actual and perceived) are things that we take really seriously in the Bitcoin space, even though compared to other high-assurance industries things must look pretty reckless. This reflects the fact that Bitcoin is a very new technology and there are still a lot of high-impact things that we're discovering to be possible.
The code changes required to validate Schnorr signatures, i.e. to run a full node after a Schnorr-enabling softfork, are very small.
The additional wallet changes for ordinary (single-signer) users to use the new signatures are even smaller, assuming they've done the migration work to Segwit addresses.
The real engineering difficulty is for people who want to use the new capabilities that Schnorr enables, such as compact multisignatures or adaptor signatures. I expect this will be slow, but there's a significant incentive to use these because they enable significantly more compact multisignature transactions, and for large multisignatures this could even be the difference between "possible" and "impossible".
@_date: 2017-11-14 20:39:43
No, I think it would most naturally be done as a separate block area with "pegins" and "pegouts" moving from normal Bitcoin outputs into MW outputs, and this can be soft-forked in. Basically a merge-mined sidechain where everyone is required to validate the rules.
@_date: 2017-11-16 16:41:33
Luke has never said "if he could propose a hardfork, he would propose quadrupling the blocksize", and he _can_ propose a hardfork, anybody can.
@_date: 2017-11-14 20:19:40
I think a MW sidechain (or any trustless sidechain) requires efficient general zero-knowledge proofs a la STARKs, for which entire proofs of sidechain validity can be made in a reasonable amount of time. 5-10 years.
The grin folks appear to be making an altcoin and I think they intend to launch next year, with a couple years of frequent hardforks as things settle.
MW directly on Bitcoin is a more interesting proposition, in theory somebody could write a BIP today describing how to do it. I'd guess that the verification time on the rangeproofs is not yet small enough for this to gain consensus, and the soundness story (a discrete log attacker can inflate coins, at least within the MW part of blocks, which in fairness could be firewalled off) would probably also need to be fixed.
@_date: 2017-11-24 15:33:50


You're conflating two things: adaptor signatures (my thing) and Discreet Log Contracts (Tadge's thing). The former lets you modify signatures so that they reveal a sort of hash preimage, and the latter is used with oracles to enforce that they do not reveal multiple distinct answers.
By the way, adaptor signatures are super simple. Here's the core idea. There is this map `f` from numbers to elliptic curve points which is (a) computationally infeasible to reverse and (b) homomorphic, meaning that `f(x + y) = f(x) + f(y)` for any numbers `x` and `y`. So like if a secret key is `x` then the corresponding public key is `f(x)`. Schnorr signatures must satisfy an equation full of secret data, but by putting both sides of the equation into this `f` function you get an equivalent equation with only public data. So suppose you have some secret data `t` and its "hash" `f(t)`. Then you add `t` to an otherwise-legit Schnorr signature, add `f(t)` to the verification equation, and now you have a thing which anyone can verify is "a valid Schnorr signature plus the preimage to `f(t)`". Later, when the actual Schnorr signature is revealed, you subtract that from the thing to get `t`.
@_date: 2017-11-14 14:32:38
Sorta. Using techniques beyond those in the paper I think it is possible for miners to aggregate rangeproofs across a block, leaving only ~160 bytes on each output (so it is not the full logarithmic aggregation that you can get if everybody is interacting, as they would in a coinjoin).
This has two problems that I think make it impractical for MW:
1. Even after aggregation the work done to validate the rangeproofs is still roughly the same as with the old scheme. So that would be a limiting factor.
2. After aggregation the rangeproof would look very different from the original rangeproofs on the transactions on the wire, so validators who had seen the transaction and cached the rangeproof validity would be forced to re-do the validation when they see the block. Ouch.
3. If you aggregate rangeproofs for _N_ outputs, that rangeproof can't be deleted until _all_ of its outputs are deleted, and in the meantime it will be _N_ times as expensive to verify than a rangeproof on a single output.
The first two points apply even for Elements or Monero or whatever non-deleting blockchain you can think of. I think it means that noninteractive aggregation is a no-go.
@_date: 2017-11-10 22:14:04


Because this is such a fundamental misunderstanding that nobody could seriously work on Bitcoin for very long without having it corrected.
@_date: 2017-11-16 18:42:48
There is no "segwit discount" except that there is more block space available for segwit transactions than there is for non-segwit transactions.
@_date: 2019-02-18 17:47:27
Correct, we're trying to pull together a proposal that includes Schnorr but it won't support signature aggregation because there are too many interactions with other parts of the system, and even the non-aggregation stuff has led to an exhausting amount of discussion :)
@_date: 2017-11-20 19:30:18
So these extra signatures aren't consensus-validated? What is the point of them then?
@_date: 2017-11-24 15:24:33
Thank you for the flattering words.


This is true.
@_date: 2017-11-15 16:40:23
Mimblewimble requires CT, so a MW extension block would definitely have CT. It's orthogonal whether the rest of Bitcoin also had CT.
Given CT, Mimblewimble gets you better privacy and scalability at the expense of any scripting features and the ability to do noninteractive transactions.
@_date: 2017-11-20 22:23:29
Ok, so now we're at a radically different consensus model that increases the validation load for all real-time verifiers (in both CPU time and bandwidth/storage), and in return we've eliminated this specific form of malleability. And it still involves segregated witness data.
Let's move on to the next specific form: a 2-of-3 multisig. Do you require all three signers to sign off on everybody's nonces? If yes, this is actually a 3-of-3. If not, now the third and second signer can collude to malleate a transaction out from under the first signer. How do you propose to fix this?
@_date: 2017-11-05 22:27:27
Benedikt is referring to BLS signatures here which require pairing-based cryptography and are much slower to verify than ECDSA or Schnorr.
Typically when we talk about aggregate signatures in Bitcoin, we are referring to aggregate Schnorr, which are indeed 32 bytes each (rather than 32 bytes total).
@_date: 2017-11-17 20:15:13
More importantly, OP should read the sidebar there and learn how to manage and grow money. If he or his wife is coming into $2MM with a "money is for spending" mindset then he is going to wind up with a lot of stress and no money.
There is a grain of truth to what the crazy "wealthy people deserve it" guy said, which is that if you have an eye toward growing and accumulating wealth, barring some horrible random life event you'll wind up being pretty wealthy by the time you're middle aged. Conversely, if you don't have this mindset, you'll find yourself living paycheck-to-paycheck with no savings, and any money you _do_ come into will get eaten up.
I don't mean to make any value judgements here, I'm just saying that maintaining wealth is a learned skill. If you weren't pulling it off before the windfall, that strongly suggests that you should spend some time and energy building that skill before reacting in any other way.
@_date: 2017-11-15 17:36:30
Most BCH blocks are under 100Kb. They could have no limit and it wouldn't matter because nobody uses it.
@_date: 2017-11-15 20:11:22
If there were a serious proposal for CT in Bitcoin (which would require the engineering problems pwuille has mentioned to be solved) I would strongly advocate including a MW extension block alongside it. So as far as Bitcoin is concerned, the timing for CT and MW are roughly the same, unless for some reason it turns out that consensus for one is much easier to achieve than consensus for the other. And neither is on the immediate horizon, unfortunately.
Regarding a MW sidechain or altcoin, those could be done today. The current Elements codebase supports MW transactions, which are really just CT transactions with the scripts forced to the empty string, though there is no wallet support (Thomas Dudzik made some progress toward that this summer but stopped to work on his degree for some reason) nor does the standard node support the agressive pruning this would allow. Meanwhile the grin altcoin continues to move forward, with plans to later allow a Bitcoin peg, using Confidential Assets to let the two currencies work side-by-side on the same chain.
BTW, it appears that MW can only use half of the magic of Bulletproofs: rangeproofs can't be aggregated because outputs aren't part of well-defined transactions so it's not clear what proofs would be aggregated with what. There is also the issue that when an output is deleted we'd want the rangeproof to be deleted with it, which can't be done if it's been aggregated with non-deleted outputs' rangeproofs. So we "merely" get a halving of the size of rangeproofs alongside a squaring of the allowed range :)
@_date: 2017-11-15 20:22:55


It's actually more dramatic than that: Bulletproofs are _logarithmic sized_ in the total number of bits being proven. So every time you double the number of rangeproofs that are aggregated together, the size of the aggregate only increases by 64 bytes. The 10x number is based on some specific transaction type with only so many outputs.
However their verification time is linear in the total number of bits proven, so whenever you double the number of rangeproofs you double the time needed to verify them, which sucks. Naively counting operations suggests that the performance of Bulletproofs will be similar to the performance of the old rangeproofs in terms of CPU time, though there are reasons to be optimistic that we'll actually get a nontrivial speedup:
* The bulletproof verification formula is a single giant multi-exponentiation where every term can be calculated upfront from the proof data. So unlike the old rangeproofs, verification can be trivially parallelized to any number of CPUs and in the end we just add together all the independent work.
* On a single CPU we can do the multi-exponentiation much faster than just doing a bunch of exponentiations in a row. So I think the actual performance will be more like `O(N/log N)` rather than `O(N)`
* There are a lot of little minor things that are easier when verifiying Bulletproofs -- we never need to convert from Jacobian coordinates to affine (which is slow), we can avoid doing more than one scalar inverse, we can use log space when setting up the multiexp, etc.
All this to say that I should probably get back to coding this up instead of speculating on Reddit about how the finished code will behave :)
@_date: 2017-11-24 15:24:01
Thanks for this summary! A few nits:


You're thinking of Bitcoin Core -- we have 1.5 people whose job it is to work on Core. Pieter and Greg (who is also CTO of Blockstream, hence the 0.5). On Lightning we've always had an integer number of people :).


That would be pretty cool, but the truth is that Blockstream is a for-profit company and spends the bulk of its resources working on its revenue-generating products, primarily Liquid. Our contributions to the Bitcoin ecosystem are there partially for noble idealistic reasons (i.e. it would be impossible to hire key people without letting them do this) and partially because we're building tech on top of Bitcoin and need the system to survive -- as more than a Paypal 2.0 -- for that to be feasible. Unfortunately at the time Blockstream was founded, and largely even today, there were no other companies willing to devote development resources to the system that they freely use and base their entire businesses on.


Thaddeus Dryja is at the MIT Digital Currency Initiative.
@_date: 2017-11-20 23:04:19
It is radically different, you're saying that invalid blocks which are sufficiently buried will become valid, which is a selfish mining incentive and will also cause consensus splits at the tip (where "the tip" is now moved down to blocks that previously could be considered long-buried). You're also hardcoding some binary notion of "confirmed" which isn't like anything else that exists in Bitcoin today, and with good reason.
At what point do you say "gee, segwit solves this problem way more generally, much more efficiently, without any of these pile-on issues, maybe it was a smart thing to do after all"?
@_date: 2017-11-08 00:59:36
Then printed and put a photograph on your Myspace.
@_date: 2015-08-11 19:40:47


You misunderstand how testnet is used, both currently and historically. Testnet runs the same code as the bitcoin main network with minimal discrepancies.
None of P2SH or CLTV or any of that was integrated in testnet before it was ready for deployment on the main network. (Modulo the activation thresholds being set differently).  Testnet is a test network, not a test codebase.
Beyond that, some of the features in elements required substantial changes and several of them could not safely or sanely be carried in a common codebase with mainnet.


Those things which are suitable and mature enough to go into Bitcoin are going there, e.g.    but really, "only contribution", take a look at the actual activity on the Bitcoin codebase: someone's work on Bitcoin Core doesn't stop existing because they also did some test work elsewhere.
@_date: 2017-11-20 16:58:12
Because they're not included in the txid?
@_date: 2017-11-15 16:51:14


This is also true of you and I, and yet there are no posts suggesting that it is true of either of us. What's the difference?
@_date: 2017-11-17 12:55:28
Yes, and for a given sat/signature fee, it is more profitable to include multisignature transactions than single-signature transactions. Does this constitute a "multisignature discount" even though nobody uses this costing method and any miners who did would lose money?
@_date: 2017-11-17 12:56:25
@_date: 2017-11-03 16:20:45
AFAICT the only use case for Ethereum contracts vs Bitcoin+Schnorr is that token sales can be done with far less technical ability. That is, it supports ICO scams. The [chain can't be validated quickly enough to catch up before the next hardfork]( it is _significantly_ worse than Bitcoin as far as privacy, scalability and decentralization, it has an ethically questionable origin story, and its design is based on a fundamental misunderstanding of the difference between code execution and verification.
It has at various times sold itself as being Turing complete, by which they mean "not Turing complete but nonetheless impossible to analyze", having "rich statefulness" which means that node operators need a massive amount of resources to validate the chain, and more generally as a platform for unremarkable developers to write legal contracts that are shaped kinda like Javascript. This works about as well as you'd expect.
I wouldn't bother looking into it at all, it's fun as a spectacle but there's a lot of it and none of it will give you particularly useful knowlege.
@_date: 2017-11-06 15:32:46
If you have a predetermined set of participants and a predetermined problem to solve you can do a [zero-knowledge contingent payment]( with this stuff.
Along with the adaptor signature with secret `t` you provide a SNARK or something proving that `t` is the encryption key to a solution to some problem. In theory "some problem" could include stuff like signing valid transactions on other blockchains, validating a signature from some oracle on the outcome of external events, etc. Or you can do a crappy form of relative timelock by proving that `t` is the solution to an RSA timelock puzzle, so the other participant(s) can get your signature by grinding it out.
In practice the cool stuff is usually intractable with SNARKs so I do more direct things, like the atomic exchanges that I talked about. There's a wide space of things you can do here that we're just beginning to explore; basically any elliptic-curve based interactive protocol can exchange data using these adaptor signatures so you can add money transfers to unrelated cryptosystems.
@_date: 2017-11-25 16:13:03
The public key doesn't need to change .. the person creating the adaptor signature passes `f(t)` along with the signature. So a better way to think about it is that `f(t)` is part of the adaptor signature, rather than being something that modifies the public key.
@_date: 2015-07-28 16:43:18
I think I was simply wrong here. If I'm remembering right what I was thinking when I made that comment, I was thinking of privacy purely in terms of what appeared on the main blockchain (a coin is locked; later a SPV proof appears showing that a coin on another chain is locked, which gives no hint to its history). Somehow I overlooked that if the sidechain does not have its own anonymity scheme, anyone can just look at its chain to see the coin's history!
Sorry for the confusion.
@_date: 2017-11-03 16:07:40
There is value in correcting misconceptions (and even saying "nope" to what might otherwise look like unanimous acceptance of something wrong), but this is emotionally draining, unending, and doesn't require a _ton_ of knowledge. It's not a bad way to learn things, as anything simple but incorrect you say will be jumped on, but it's also easy to overdo and get discouraged.
@_date: 2017-11-17 13:44:35


I love this sentence.
@_date: 2017-11-20 16:31:09
Ok, and are these extra signatures segregated, or are they also malleability vectors?
@_date: 2017-11-03 16:13:24
Something I've been working on which manages to several things you listed are [scriptless scripts]( which is a general research program into doing smart contract stuff using nothing but ordinary signatures.
Most of this stuff requires BN signatures; discreet log contracts can be considered as a type of scriptless script; and this was originally developed for MimbleWimble which has nothing _but_ signatures to work with.
OP, I encourage you to check out the slides that I linked and see if that's something that seems interesting. If you have questions or thoughts you're welcome to ping me here or email me at the address listed on those slides, and I'm happy to help out.
@_date: 2017-11-26 15:36:39
Sounds like you've got it. In step 5 the seller chooses `t` randomly and gives `tG` to the buyer. He makes a partial signature `(s, R)` to the transaction from step  but gives the buyer `(s + t, R)`.
In step 8, the total signature will look like `(s', R')`, which s' being a sum of the two parties' partial signatures. The buyer subtracts his part to get the seller's `(s, R)` and then subtracts `s` from `s+t` to get `t`.
@_date: 2017-11-17 13:50:17
BCH is also very easy to rewrite, and the only software out there for it is written by people of questionable skill and questionable ethics. If they had no minrelayfee it might be worth filling their chain with backups or other crap, but as it stands this is actually very costly relative to the risk that your system might be compromised and/or the blockchain rewritten without your data.
@_date: 2019-04-06 14:07:35
The relationship between nodes and lite clients is not so simple. In terms of technical capability, anyone who wanted to pay for the bandwidth and AWS time could run a single node that could serve every single lite client. The reason for wanting more nodes is not to improve the technical capacity of the network (although they may do this).
What nodes actually contribute to the network are redundant, independently verified views of the network. People often say "the only node that matters is your own" for this reason - the point of Bitcoin is that you can, and should, verify all the data for yourself. The truth is less myopic, of course; you can't verify the data if you can't get it, and the more nodes that exist across the world, the more difficult it is to censor this data (or run a "spy node" recording everyone who connects to it, expecting to learn a lot about Bitcoin users' activity). There is value in having a well-connected network of non-malicious peers.
Sometimes people talk about how they "started X many nodes on AWS to help the network" or "they have 5 nodes running in their home!" or something like this, which I think comes from the intuition that nodes somehow add capacity to the network. But this isn't helpful, because none of the nodes after the first that you set up on AWS or your living room are contributing additional redundant views into (or access to) the network.
@_date: 2017-11-02 13:15:16
As a rule, Trump avoids betting his own money on anything, and if he can he reduces his exposure to simply licensing his name to various things (which has limited upside risk but literally zero downside risk) or by being a celebrity (ditto).
Financially, for somebody so involved in the business world, he has always been extremely conservative.
@_date: 2015-08-11 19:40:58
Welcome to Reddit peanutbuttercoin!


It very clearly isn't---- all the coins on it are tnbtc. The only way to use it is with testnet coins. All coins on it can be extracted and used on the testnet network.
@_date: 2017-11-28 18:49:01
I'm not sure whether or not to be sad that this is buried on account of $10k day :)
@_date: 2017-11-20 21:10:52
There is never any "fully committed to the blockchain", it just becomes increasingly expensive for a 51% attacker to replace something (and increasingly extremely unlikely that something will be replaced by a probablistic attacker, or by accident).
What you are suggesting is a consensus rule that is only enforced by real-time validators, and perhaps by validators checking recent blocks (but how recent? is this configurable?). This will cause consensus failures.
@_date: 2017-04-28 01:53:35


Your initial post made a claim that was a total non-sequitor. nullc asked you to fill in some steps so he could find the logical error and you replied with a tirade about Blockstream's funding which totally ignored his question. It's hard to believe that you're honestly looking for the truth.
@_date: 2019-04-27 16:22:36
Yeah, there are no good ways to do timelocks with scriptless scripts.
@_date: 2017-04-25 13:31:40
For HSM authorship, you no longer need to provide entire previous transactions in order to validate transaction values; to compute a transaction signature the rules are much simpler and require less memory (compare [the segwit signature hashing code]( to the [Satoshi CHECKSIG signature hashing code]( for nodes trying to validate the transaction graph but who don't know how to interpret scripts, they don't even need to download the scripts in the first place.
These are three that have affected me directly and repeatedly as an author of Bitcoin tools and libraries. Interestingly, they are also [listed on the Segwit Benefits Page]( which has been linked in this sub probably over a hundred times.
@_date: 2019-04-03 21:22:26
There is a [fork of the Bitcoin BIP repo]( with a proposal for Schnorr signatures, but it has not been assigned a number (and as far as I know, Pieter has not even asked for a number or PR'd the branch). We refer to this document as "BIP-Schnorr" with the expectation that it will get a number and will be part of a larger proposal.
By itself BIP-Schnorr is not very interesting. It describes an elliptic-curve based signature scheme, how to encode these signatures, how to compute and validate them, etc., but says nothing about how they'd be used in a Bitcoin context.
In practice we expect Schnorr signatures to come along as part of Taproot, which will be an entirely new Bitcoin output version, and include several other changes, including
* elimination of the non-`VERIFY` signature-checking opcodes, which prevent batch verification but otherwise have no use
* a replacement of `CHECKMULTISIG` threshold-signature opcode, which is currently inefficient to encode and inefficient to validate
* MAST (replacement of `OP_IF` and friends with the ability to just selectively expose only the parts of a Script you want to satisfy), which improves privacy and scalability for almost no technical cost, so why not
* improved sighashing modes which cover some limitations with the current Segwit sighash scheme (described in BIP143) and which will also introduce some form of `SIGHASH_NOINPUT`, which is a technical requirement of some future directions of the Lightning network
None of the things I've described have BIP numbers and I'm not sure how they'll be partitioned into separate BIPs. There's something of a tradeoff between throwing everything in one big proposal and having nobody read it, and overwhelming the community with a dozen proposals and having nobody read them :).
Right now there's a group of several people working on this collection of proposals, including myself, and we hope to have them ready for community review (including having actual documents written up which we can request BIP numbers for, having working code for the proposals, working code for things that the proposals support, etc) Real Soon Now.
I'm sorry that I can't give any more precise dates than that. It's been a long slog since we've started talking about these ideas with the media and wider community, but along the way we've learned a lot about how these things could be used, potential risks, potential optimizations and how to best address people's concerns about intangibles like software complexity.
@_date: 2017-04-30 23:54:19
Mimblewimble has nothing to do with asicboost (or mining, for that matter).
@_date: 2017-04-19 18:26:07
You might be interested in [this slide deck on "scriptless scripts"]( which describes a variety of things you can do in principle with the same "everyone can see nothing was tampered with, no trust required" model, but without any extra data hitting the blockchain.
@_date: 2017-04-30 22:38:27


Segwit _is_ a compromise, in the sense that it has a capacity increase bundled in with an otherwise unrelated technical change (committing to witnesses, which are needed only for transaction verification, separately from the normative transaction data, which is needed to determine the state of the network).
There isn't much to disagree with on technical grounds: it is [a reduction in complexity in several senses]( including to [the user model of Bitcoin]( has several [auxilliary benefits]( and even includes a direct capacity increase for those who feel such a thing is necessary beyond increasing throughput per resource (which is what, e.g. signature aggregation, something which can be implemented much more easily in a post-segwit world, will achieve). And indeed, despite spending hours of my time in the Other Sub engaging with people, I've never encountered _any_ technical opposition that wasn't based on simple misunderstandings.
So I'm not sure what more you believe Core could propose (as others have said, Core cannot actually unilaterally change Bitcoin, only propose things) that would have "almost no chance of failure".
@_date: 2017-04-19 22:41:33
If Bitcoin had Schnorr signatures there would be no need to publish a hash on the chain.
@_date: 2018-11-11 14:49:15


No. The curve addition formulas for secp256k1 and curve25519-dalek are very different.
@_date: 2018-03-25 19:44:58
This wouldn't be a defensive action and would therefore violate Blockstream's patent pledge. "Defensive" does not mean "sending a C&amp;D to people who use a patented invention", it means pretty-much the opposite.
Anyone can use Blockstream's patents for good or for evil.
@_date: 2018-03-25 16:41:38
How can you "flagrantly infringe" a defensive patent?
Or in this hypothetical, is A using non-defensive patents indirectly through the third party, thus evading the BDPL and actually violating the intent of the license?
@_date: 2017-04-27 18:55:56
One current example is the [longest chain rule]( But in general, it's not useful to complain about specific known instances of consensus failures; it's easy enough to fix individual ones, but very difficult (impossible?) to find every single one of them. And just because nobody is publicly finding any more, and there is apparently no more low-hanging fruit, does not mean that there are none left.
It would be much better if parity-bitcoin were to [use the consensus rules of libconsensus]( which would (in the future when libconsensus is complete) avoid all these problems while still writing all the network/disk/user-facing stuff in Rust.
@_date: 2018-03-29 16:24:06
There are a couple of projects implementing it. If you want you can go use it on Elements Alpha today, just hack your wallet to use empty scripts for everything...but of course you won't get any of the scaling benefits without validating software that can exploit the simple structure of these transactions :).
@_date: 2018-03-15 14:40:55
Yes, for example [aggregate signatures]( [Taproot]( and [Graftroot]( are all privacy technologies which have the nice side-effect of (a) improving scaling, and (b) having no effect on the security model of Bitcoin.
@_date: 2018-03-29 16:22:57
You need some form of timelock, and nobody knows how to do timelocks without script (or at least, explicit blockchain support for timelocks).
@_date: 2018-03-25 17:40:20
Right, but then this permits a DPL member to set up a shell company disguised as a third-party, who then uses exactly this ability to selectively include patents under the DPL, violating the spirit of the license.
@_date: 2018-03-25 17:02:34


Then your claim then is that the entire DPL (and BDPL) is misguided. Which is very different from "BDPL is a poison pill" because it does exactly what it says on the tin.
@_date: 2018-03-14 20:22:25


Definitely not. Shor's algorithm can make quick work of any cryptography based on cyclic groups, which all EC stuff is.
@_date: 2018-02-09 12:54:37


No, they are 100% unrelated at every level of abstraction.


The fact that a discrete-log attacker (e.g. a quantum computer) would be able to exploit Bulletproofs to silently inflate any currency that used them.
@_date: 2018-03-14 20:29:35


I think you misspelled "would be significantly easier" as "would be no point". We have spent a tremendous amount of engineering effort making Liquid work efficiently with the Bitcoin blockchain rather than campaigning to increase the validation load we're allowed to force onto unpaid validators.
Obviously a blocksize increase would do absolutely nothing to increase finality speeds or give a binary notion of "finality" at all, let alone introduce additional script opcodes or confidential transactions.
@_date: 2018-02-22 22:13:12
Anyone can burn coins at any time.
@_date: 2018-02-09 12:53:50
10x over the non-batch performance, which in turn was 3.5x the performance of the old rangeproofs.
@_date: 2018-02-08 22:17:21
Yes :P. Though eventually it bottoms out, currently it looks like at around half an ECDSA sig per range.
**Edit:** We have a couple optimizations in the pipeline that I think will improve this, but it gets increasingly difficult to code as we do more elaborate things, so I don't have an ETA.
@_date: 2018-02-22 18:20:32


It does.
@_date: 2018-02-23 13:30:43
Silent inflation risk is definitely not opt-in :)
As mentioned in another post, you could put CT in an extension block only, and limit the inflation risk to that extension block so that it would be opt-in. Though that's a fair bit of complexity and I worry that any opt-in privacy scheme would cause regulators or businesses to taint coins that had used it in the past.
@_date: 2017-09-26 13:40:30
Regardless of the poll, the basic argument that developers are not going to join some project that has hurled vitriol at them for its entire existence, and whose ethos is in direct opposition to the ethos of Bitcoin, seems pretty solid to me.
@_date: 2017-09-15 15:53:14


This is false and it is trivial to see that it's false. Look at the code of any node implementation, or just try to submit invalid transactions to nodes on the network and see what happens.
Better yet, mine a block with invalid transactions and see what happens to it.


There is value in not being coerced.
@_date: 2017-09-20 14:18:36
Hypocrisy isn't defined in the whitepaper, but in the English dictionary.
@_date: 2017-09-25 00:59:28
If you're accounting in bytes then it will appear that segwit transactions are prioritized. This is because there is no limit on the number of bytes in a block, only a limit on weight, and segwit transactions have more bytes per weight.
@_date: 2018-03-14 20:42:39
Yes, it can (and eventually will) be faster, as we continue engineering. For example, Liquid uses the well-studied but very large original Confidential Transactions rangeproofs, which is extra problematic because all functionaries communicate over Tor. Once we have complete and audited Bulletproof code, that will give a big performance boost.
@_date: 2017-09-12 22:36:32
It can't possibly be free because it needs to span all notions of taint everywhere and somehow weight how much the coins' utility is damaged by each one. It's more likely to be literally impossible than free.
@_date: 2018-03-14 20:36:49


No individual controls these signatures, a quorum of mutually distrusting parties are required to sign everything.


Can you cite a single example of multiple companies distributed across the world (a) merging, (b) immediately colluding to outright rob several other companies, and (c) doing this all within fifteen minutes before people had time to move their coins out of the system?
There are many plausible attack scenarios for Liquid which don't apply to Bitcoin (and a couple in the other direction, which is why we require additional signatures on peg-outs into the wider Bitcoin system), but the one that you describe seems to be physically impossible.
@_date: 2018-11-10 19:06:55
Yes, the bulk of the speedup is from the use of AVX2 instructions, and the rest is due to a faster curve addition formula than that in libsecp256k1.
@_date: 2018-02-08 22:10:00
Ah, what I was referring to there was specifically that you can do part of my "adaptor signature" construction with ECDSA. But not all of it. So you can do an atomic swap between Bitcoin and some Schnorr-coin, but not between Bitcoin and Ethereum, and you can't chain these off each other to get payment channels. and I have been referring to this as "Lightswap", and we had a goofy way of doing it before, but and I realized we could do it with BPs.
(And then later we realized we could have just done the BP thing with a sigma protocol, so my crappy method was never needed :) But sometimes you need the generality of BPs to search for a solution, even if in the end what you get could have been done other ways.).
If somebody could find a way to witness-encrypt data to an ECDSA sig, you could get a full adaptor signature, and that would be very very interesting.
@_date: 2017-09-23 15:24:16
Both Ethereum and BCash support this type of swap. On the Ethereum side you will need to structure it differently because of their account/contract system but it'd be essentially the same.
@_date: 2017-09-15 15:53:14


This is false and it is trivial to see that it's false. Look at the code of any node implementation, or just try to submit invalid transactions to nodes on the network and see what happens.
Better yet, mine a block with invalid transactions and see what happens to it.


There is value in not being coerced.
@_date: 2017-09-19 15:58:32
Other than being a support layer for token sales, what interesting things is Ethereum doing that Bitcoin struggles with?
@_date: 2017-09-05 00:24:36


It is? How many can you list?
@_date: 2017-09-10 14:18:46




You are correct up to this point, and it's a good summary of the situation.
For the rest you are wrong. Any ordinary transaction on one chain is invalid on the other, and vice-versa, because they changed the OP_CHECKSIG semantics to prevent exacly this sort of thing.
@_date: 2018-02-22 14:40:57


If you have a non-2^n number, at some halving steps you'll have an odd number of (a, b) scalars to deal with. So mix all of them except the last and reveal the remaining pair explicitly as part of your proof, and there you go.
Unfortunately this makes the one-big-multiexp verifier much more complicated, which is why we haven't implemented it yet.
The resulting proof would be bigger (asymptotically worst case 2x the size) than if it had just been padded to the next power of 2. But on the other hand, verification and proving time would be reduced. So that's another tradeoff to consider.
@_date: 2017-09-26 23:34:42
This function  does all of the validation of a block. Nowhere does it count hashes or do a "majority rules" of any metric to decide any piece of validity. This has never been how Bitcoin worked.
"One hash one vote" refers specifically to the fact that the most-work path in the blockchain is considered the true history, but to be in the blockchain in the first place blocks need to obey the rules of the system.
@_date: 2018-02-09 15:52:03


*pretends to take nose, hides hands behind back*
I've got your transaction amount! Where'd it go? Where did it go?
@_date: 2017-09-19 19:00:31
Oops, the "one-two punch" comment was supposed to refer to the lack of scripting and the lack of zeroconf chains. Scaling has nothing to do with it.
@_date: 2018-03-03 12:27:24


Not to downplay the work the Monero team has done, or the positive effect on the ecosystem they have by putting privacy tech in the hands of real users today, but libsecp256k1 has an implementation of BPs which is over twice as fast (and continuing to improve) which also has support for arbitrary arithmetic circuits, and works over the curve that Bitcoin uses. This implementation was used to produce the benchmarks in the paper, which themselves are out of date since we've implemented some post-publication optimizations.
Because Monero is using a different curve and different codebase (and actually quite different-looking algorithms even for the high-level BP stuff), their code audits unfortunately won't transfer to libsecp.
@_date: 2017-10-25 11:00:28


What does that gain you if you aren't verifying the transactions themselves?
@_date: 2017-10-29 00:05:44
A surprise Weil pairing would be bad, historically speaking, because it was originally introduced as a way to attack certain elliptic curves which supported it, and there was a long time between this and the realization that there were "safe" curves that supported the pairing (and even recently in 2016 the set of "safe" curves was shrunk when it was realized that pairings could be used for stronger attacks than we previous thought).
To be clear, there is no surprise pairing. There was no speculation in my claim, this is well-established old mathematics (and there is actually even [a safecurves page about it]( whose embedding degree matches the calculation I did on Reddit). CSW is simply 100% lying.
Now, if there were a pairing with embedding degree 10 or 12, there is good evidence that this would be safe, in that it wouldn't enable discrete-log attacks that would undermine ECDSA or EC-Schnorr or whatever. It would make decisional Diffie-Hellman easy while making computational Diffie-Hellman hard, creating a "gap group" which can be used for a variety of neat cryptosystems. For example, Identity-Based Encryption, BLS signatures (which are very short and can be aggregated very well), and SNARKs are all things you can look up that depend on pairings. Actually using these in Bitcoin would likely require consensus changes, but they'd be within the realm of possibility while right now they are not. (In that they would require adding support for a new curve which we'd be less confident about the security of.)
@_date: 2017-10-31 14:07:12
In principle, any parts of the script that get executed still need to be serialized and transmitted. However Simplicity supports jets which allow common operations to be replaced by optimized code (which can be proven to be equivalent to the Simplicity code) that validators would have local copies of. They could then use that rather than needing to download the full code.
Similarly, even stuff that isn't jetted can be shared across contracts and cached rather than re-downloaded.
@_date: 2018-02-22 14:45:46
Source code is here: 
As you can see by poking around a bit, this is far from production-ready. There is stuff like 30000-gate circuits encoded as C arrays and ` into tests and benchmarks :).
ETA is hard to estimate. I expect for Bitcoin users this stuff won't appear except as part of off-chain protocols, because there isn't any clear on-chain application of this stuff. (Except, I suppose, Confidential Transactions, but because they would make inflation-soundness violable by quantum computers, I think that's a no-go.)
There are more nontrivial optimizations in the pipeline, both for prover and verifier. One, for arithmetic circuits, will require we rethink how we generate and encode circuits, so there is still a lot of design work to be done on this front.
@_date: 2017-10-25 21:45:25
I think it would be impossible to do non-contentiously without several years of planning. In your hypothetical "Core splits in two and they both have their chains" I would expect that which ever parti(es) hardforked from the original Bitcoin would also change the PoW as part of the change.
I wish that "big holders" corresponded more closely to "intelligent and mostly technical people", and the constant requests for people like Olivier or Roger to trade Bitcoin for B2X coins is an effort to make that more true, but unfortunately intelligence and understanding biases you _against_ holding a lot of Bitcoin, both because smart people are much more aware of the risks, and because people who are betting their careers on Bitcoin's success would be reckless to also bet their portfolios. But who knows. Anecdotally, I know several extremely smart and capable people who own a lot of Bitcoin.
Regarding your wider comment about marketing winning over tech and "what if rbtc weren't totally incompetent?", I have a few comments.
1. Core isn't bereft of marketing power, it can (and is) getting better at communications.
2. Regardless, reality is inherently biased toward the techies in this space. Cryptography is difficult and involved and requires a lot of expertise and caution. Further, the people who have this expertise are in high demand and therefore difficult to obtain and manipulate, especially cryptocurrency people who are typically doing what they do for idealistic reasons and lack the amorality that most other species of engineer appear to have. We see this playing out over and over as everything non-Bitcoin is either a trainwreck of highly public security failures, or is a contentless scam.
3. Bitcoin really doesn't depend on tons of people understanding it or even knowing what "Bitcoin" is vs what random media corporations say it is. It's designed to be censorship-resistant with no single point of failure, and people who really need to use Bitcoin will continue to be able to use it no matter what happens.
4. Bitcoin also doesn't really depend on miners doing the right thing. If Coinbase and Xapo and others weren't artificially and stupidly trying to abdicate their responsibility to define the assets they supposedly have custodianship over (I can't imagine what a court would think about assigning this duty to a group of anonymous ephemeral timestampers), miners would have basically no power and this whole controversy would be a non-issue.
5. Bitcoin is a very resilient system which has been attacked several times before and ultimately has continued to survive and become more decentralized.
Of course, these points don't mean that you're wrong or that this is not a serious risk. But they do give reason to be optimistic. I really don't think a marketing-only attack on Bitcoin can ultimately be successful. And real competence, with R&amp;D power behind it, is basically impossible to find these days, something that isn't going to change very quickly.
@_date: 2017-10-10 16:43:55
No, you can not, which is why nobody has proposed removing signatures from blocks. (Segwit definitely does not do this.)
@_date: 2017-10-25 19:40:21
I think "Skills not available on the street - consider building in-house" was the funniest line. If there is a cryptographer shortage, why not substitute random bank employees?
@_date: 2017-10-08 20:01:10
With transactions that have been broadcast already the additonal bandwidth is negligible thanks to compact blocks. Even without CB the difference in orphan rates caused by &lt;1Kb's bandwidth delay during a 10-minute poisson process is very small.
@_date: 2017-09-13 15:09:36
It's completely unrelated to miner signalling, which occurs in blocks rather than on the p2p layer.
@_date: 2018-02-06 15:04:21
There seems to be some confusion on this. Sorry about that -- we found the optimization, swagged its speed, added it to the talk at the very last minute, and only implemented it a couple days later. We wouldn't have added something so immature to a public talk except that the benefit was really really big.
1. The time to verify ECDSA on our test system (my laptop, an i7-7820HQ throttled to 2.00Ghz) is about 86us. Not 800 as listed in the talk.
2. The 200us number was a conservative estimate, but based on my initially overestimating the extent of the optimization. I would've said 500us had I not made this mistake. Our tests show 470us. We have a further optimization in the pipeline that I think will get this down to 400us.
3. Benedikt later pointed out that the batch verification speed grows only logarithmically[*] in the size of the aggregate. So an aggregate of 2 rangeproofs takes 570us, or 290us per range. An aggregate of 16 takes 730us, or 180us ... so we _do_ get the 200us number, you just have to be working on aggregate rangeproofs :).
4. In fact if we go up to 32-range aggregates, the batch cost drops to 2.58ms, or 80us per range... beating (non-batch!) ECDSA. So there _is_ a sense in which you can verify a range in a bulletproof faster than you can validate a signature.
On my laptop without throttling all of these numbers are about 60% as large. All of them are single-core measurements; we haven't implemented a parallel version of our efficient multiexp code. The old rangeproofs in the same setup take about 8.5ms per 64-bit proof. So even the 500us number is a 17x speed improvement. 80us, which we can expect during IBD or when verifying a Provisions proof, is over 100x.
The verification memory usage is trivial, under a kilobyte of memory per proof in a batch (plus whatever precomputed tables you have for the standard group generators). The old rangeproofs were also trivial.
[*] Everything eventually grows linearly, since you have to at least _read_ all of the input data. So when I say things like "grows logarithmically" or "grows like N/log(N)" I mean only for reasonably low batch sizes, like &lt;10000, after which the otherwise-trivial linear components start to dominate.
@_date: 2017-09-23 15:22:56
The currently published ways to do atomic swaps require they both need to have script support for the same hash function (any overlap is sufficient, Bitcoin supports SHA256, SHA1 and RIPEMD160), _or_ they both need to support a variant of Schnorr signatures.
To do them safely, both chains need to support timelocked transactions and the ability to chain zero-conf transactions without worrying about malleability.
@_date: 2018-02-22 18:20:03


No, there is no notion of "officially attaching to the bitcoin blockchain".
@_date: 2017-09-30 01:03:25
It shouldn't. does this sound like a known issue?
@_date: 2017-10-10 16:46:52
He is wrong because Segwit does not remove signatures from blocks.
I have an [old post describing Segwit]( which explains what it actually does.
@_date: 2017-10-25 11:02:05
Do you believe that would _not_ be the case for a corporate coup of a country?
@_date: 2018-02-09 13:03:54
Bulletproofs and CT suffer the problem that they are only computationally sound: a quantum computer would be able to create false proofs, silently inflating the currency. The best unconditionally sound scheme we know of is a variant of our original rangeproofs which came up with which IIRC gets soundness with 'only' a 20% increase in size (and a similar increase in verification time).
And in this case, a QC could learn all the amounts.
Boneh told me that we can create a lattice-based variant of BPs which would be quantum-resistant (assuming whatever lattice problem he has in mind is quantum-resistant). This would retain the log-size scaling and even the batch verification, and the perfect hiding, but the constants would be much much bigger, and I suspect in practice that Oleg's scheme would be more efficient.
Schnorr/MAST/Graftroot on the other hand have none of these problems. Before we have a concrete proposal and get a feeling for how much work and discussion needs to happen, it's hard to say how long the entire process will take. But hopefully we'll have a concrete proposal out there by the end of Q1.
@_date: 2017-10-23 23:09:11


It did say this, but "the Bitcoin community" was not a signatory to the NYA, so this was a crazy thing for it to say.
@_date: 2017-10-25 12:07:16


It can't possibly do this. A valid BCH chain is _required_ to have at least one invalid Bitcoin block in it.
@_date: 2017-10-27 18:58:25
There is not even a superficial similarity between users enforcing a new rule and somebody starting a bunch of sybil nodes.
@_date: 2017-09-13 14:40:02
The linked topic refers specifically to _segwit addresses_ using uncompressed keys. I hope that no websites are producing those because they would indeed be unspendable.
Ordinary addresses are completely unaffected by segwit (though as others have said, you ought to use compressed keys there too because it's cheaper for you).
@_date: 2017-10-09 17:29:24
Nobody is talking about _requiring_ `OP_RETURN` outputs on one chain, they're talking about _forbidding_ a _specific_ `OP_RETURN` output on one chain.
@_date: 2018-02-09 15:50:05
They improve multisig, but they also enable signature aggregation (50% size improvement), batch validation (50% verification time improvement, at least during IBD), and enable scriptless scripts (elimination of hash preimages in protocols like Lightning or atomic swaps).
So aggregate signatures alone reduce the majority of uses of Bitcoin script to single signing keys and single (half-sized) signatures. Taproot goes further and extends this to almost _all_ uses of script, including locktimed backouts, at least in the common case when these backouts are unused.
Graftroot goes further and allows an arbitrary number of backout branches to be supported, and even decided after coins have been received, with no more space cost than Taproot.
As for "who uses multisig", off the top of my head these are used extensively by any customer of Bitgo's, by any user of Greenaddress, by Liquid and Elements. It's impressive that there's so much use, given that currently they're a real pain to use, but tech like [PSBT]( will make them much easier, and of course Taproot/aggsig will make them much cheaper.
@_date: 2017-10-04 22:05:49
I think without the ridiculous EDA it would have survived, though. (Unfortunately I'm not optimistic about the proposed replacements either.)
@_date: 2017-10-13 13:25:55


Ignoring that this scenario will never happen, the language of the contract does _not_ set BT2 to 0. If there is one chain then BT1 goes to 0. If there are multiple chains because Core users would not "upgrade" to a 2xCore, it falls to Bitfinex to sort out whatever acid trip the world had turned into.
@_date: 2017-10-25 11:04:50


They 
_did_ do this, after 8+ years. That's what BCH with its "fast inflation" difficulty calculation is. Fortunately Bitcoin was unaffected because validators are not trusting miners to set the rules of the system.
@_date: 2017-10-11 19:32:45
Ok, agreed that it's silly :)
@_date: 2017-10-08 14:42:57
Miner incentives are not in general aligned with user incentives or the broader goals of the system.
@_date: 2017-10-10 16:53:15
Even your URL says "when not verifying".
Yes, you can ignore data if you don't want to validate it, and Segwit makes it possible to do this more selectively. It does not remove anything from blocks nor does it make anything valid without signatures.
@_date: 2017-10-31 13:49:15


It's not just halting, but "when this halts will all the money be in the right place", which is often much harder than you expect. For example I recently saw some Ethereum blockhash-based lottery which maintained a ring buffer of recent blockhashes (to give winners time to claim their prizes in case they are more than 256 blocks late, since you can't directly look up blockhashes more than 256 blocks ago). This was an enormous pile of code with very complex semantics.


Auditing and being well-established are not sufficient in practice, and even if they were, adding formal auditability would still be a huge advantage because it gives significantly more confidence in a contract's behaviour for less work.
@_date: 2017-10-31 13:40:58


"Segregated witness" was chosen, among other things, to use technical words with no confusing common meanings or emotional charge to them. I'm not sure a GUID or whatever else you might suggest would've fared any better against determined FUDers.
@_date: 2017-10-10 17:07:31
Surely it would only be a dilemma if there were any reason to purchase other cryptocurrencies?
@_date: 2017-09-19 20:32:13
Interesting, fair enough. It would be useful if you can find a way to support "refundable transactions" somehow though. Where one of us puts coins into a 2-of-2 multisig but they don't want to publish unless they know they'll get their coins back if the other party disappears.
@_date: 2017-10-03 22:34:56
In [this thread]( several Core developers said that they would "fire themselves" in your words if s2x took over the world.
The problem is not the doubling rule, the problem is that if the market bought into 2x, it would mean that random businesses were able to change the rules of Bitcoin behind closed doors and ram that change through regardless of community support or technical merit. If that were possible then Bitcoin will have failed, so I would expect most Core people to move onto other non-failed things.
Contributing to the hijacking project would not only be pointless but actually _against_ the commonly-held values of user sovereignty, privacy and freedom from censorship. And this is not even mentioning how personally hostile the s2x project has been to these developers.
@_date: 2017-10-25 19:51:41






Sure, but this is costly and takes away hashpower from the larger chain. In a contenious hardfork scenario I'd also expect one or both forks to change hashing algorithms to prevent a BCH-like scenario where hashing hardware switches on and off very quickl.


I don't, but "temporarily" can be pretty long. Basically as long as somebody is willing to throw money-losing hashpower at it.


Thanks. I didn't mean to argue from authority, but "argument from the common man" when I said that I wouldn't follow a contenious hardfork and I doubted that many others would. The reason being that one of Core's most important jobs, as the code running a supermajority of nodes, is to prevent consensus failures, and if they decided to abdicate this responsibility, I'd consider them compromised (or defunct) and ignore them. I can't really say what users would do in practice but my anecdotal evidence from talking to Bitcoin users I know is that they share this attitude.
@_date: 2017-09-12 17:42:09
Check out the sidebar of r/financialindependence. This number comes from the "Trinity Study" which looked at fairly standard retirement portfolios over a 30-year time horizon and found that an inflation-indexed 4% withdrawal rate gave a 95% chance that the money would not run out (and a high probability that it would actually grow).
@_date: 2017-09-12 22:43:39
How on earth would Liquid benefit from restricted Bitcoin capacity? It has huge multisignature transactions which are large and expensive.
@_date: 2018-02-09 16:37:56
Well, Boneh says we could do Bulletproofs with lattices, though the resulting proofs would be much larger and slower to verify. (How much larger and slower? I have no idea.)
The old rangeproofs also have an unconditionally sound variant which would reduce the damage to deanonymization, but not allow an attacker of any computation ability to make fake proofs.
@_date: 2017-09-19 14:38:53
It has bad asymptotic scaling, it has no script support (this one-two punch means it's currently impossible to do trustless atomic swaps with Bitcoin BTW), you cannot chain unconfirmed transactions off of each other. Its transactions take significantly longer to validate and they're significantly larger. It's has much less volume and mindshare which weakens its privacy offering because the anonymity set is so small.
I like Monero, they're building real things that are pushing research forward and they don't present themselves as a get-rich-quick scheme (quite the opposite) and they're mostly very friendly agreeable people. But the blind fanboyism that I see on Reddit is completely misguided.
@_date: 2017-10-26 13:48:09
Has anybody associated with the project said this 75% number?
@_date: 2017-09-13 15:10:52
I've been to meetups all over the US, and there is actually a really big difference between how chill they are.
@_date: 2017-10-28 23:25:09
Unfortunately my initial post had bad numbers, the real value should be 10^78 which is even more ridiculous. I edited the rbtc post but he'd already quoted me on Twitter (and I don't think correcting him would be worth giving him the opportunity to distract from his idiocy).
@_date: 2017-10-03 23:07:07
I think we're saying the same thing -- yes, if s2x takes over then this means "the community wants it". But if the community wants corporate-rule-coin rather than decentralized sound money, I wouldn't continue my efforts to support that community, and the vast majority of Core contributions come from people with a similar conviction.
I agree that this seems very unlikely and isn't consistent with the rhetoric I'm hearing from multiple directions in the Bitcoin space, but that's the hypothetical.
@_date: 2017-10-08 15:08:55
It has nothing to do with trust. Mining profit margins tend toward zero, any miners not following the incentives of the system will eventually go bankrupt. In that sense they have almost no autonomy about setting parameters that are under their control.
The problem is that miners have a fundamentally different incentive structure than all other participants in the system, and everything they have discretion over will naturally be set to optimize for _their_ incentive structure rather than that of the wider ecosystem. For blocksize, this means sizes beyond that which can be reasonably processed by volunteers or amateurs, and eventually sizes beyond which even small miners can't keep up, which directly encourages mining centralization.
@_date: 2017-10-10 16:43:14
This is totally wrong. Signatures are absolutely still in blocks and they are still necessary to determine block validity.
@_date: 2017-09-12 17:47:28
Is your theory that if you can't have those coins, then nobody should be able to have any without expensive analysis?
@_date: 2018-03-25 16:30:32
IANAL, but


actually refers to "if the third party takes action against M using T". There is nothing M itself can do to trigger this clause. This clause prevents A from using non-BDPL patents even indirectly through third parties. Which is exactly the DPL loophole that the BDPL intended to close.
I suppose that if M and the third party colluded to trigger this clause, by having the third party license T to A and then send a C&amp;D to M, you could maliciously knock somebody out of the BDPL. The third party's license to A should contain language preventing this.
@_date: 2017-10-08 15:05:56
There aren't any [from addresses]( in Bitcoin. What essentially happens is that you destroy coins labeled with your address and create new coins labeled with Sally's. When you move coins around what you're really doing is re-assigning spending authority.
Spending authority is then determined by digital signatures. Each address has a corresponding public key (though not all possible public keys in Bitcoin correspond to addresses) which validators can use to verify digital signatures. Coins can't be spent without a signature with the appropriate public key; so the difference between your address and Sally's is that only you're able to produce a signature with your key while only Sally is able to produce a signature with hers.
@_date: 2017-10-11 19:17:42
It is correct based on my read of the contract. They say that the repositories on Oct 5 (which is long past) will determine the winner in a "one chain" outcome.
@_date: 2017-10-06 00:51:31


It may have been [this]( or [this](
@_date: 2017-09-15 17:33:34
How does them being miners affect the trust equation here at all, and how is trusting somebody else comparable to trusting your own software?
@_date: 2017-10-28 23:26:55
Can you cite anything remotely decentralized that he has created?
@_date: 2017-10-25 19:24:08


I doubt either would win in this absurd scenario, if Core split apart and then both tried to change the rules of Bitcoin I wouldn't follow and I don't expect many others would either. The original chain would just keep going unfazed. But even granting this, I see no reason that the most-hashpower chain should win. Somehow there are thousands of blockchains out there running, apparently oblivious to Bitcoin's massive hashpower dominance, so it seems empirically true that hashpower can't decide consensus rules.
@_date: 2017-10-08 14:40:55
Why wouldn't they be? It is basically zero-cost to add an additional transaction.
@_date: 2017-09-19 14:18:09


There is no reason to delay transaction propagation (well, there are privacy reasons, but no fee-related reasons).
@_date: 2018-03-25 12:52:42
TumbleBit works today on Bitcoin, it's blocked only on somebody spearheading a production-ready implementation.
@_date: 2017-09-27 18:29:55
How are "nobody is using Segwit" and "businesses need extra capacity but don't have it" congruent? Surely if businesses were really hurting due to a lack of capacity they would be using the extra capacity they now have? (And surely before then they would be batching transactions, consolidating during off-peak hours, encouraging customers to transact outside of US banking hours, etc, etc)
Not to mention the fact that even in the last 3 months, which have seen higher fees than past years, there have been only a couple week-long periods during which 50sat/byte was insufficient for next-block confirmation.
The facts are simply not behind this "2x is a much-needed capacity increase" meme.
@_date: 2017-10-30 20:59:24
Do you have any ideas about what a scriptless-script programming language would look like? I was thinking about this this morning and it felt like a wide-open question.
@_date: 2017-10-08 14:35:28
There's an [old article]( by Mike Hearn that gives a good overview of the ways that different addresses can be linked together. It's overly pessimistic about CoinJoin but with privacy it's better to err on the side of pessimism.
If you want to preserve privacy a good step to take is to move some coins into [a JoinMarket wallet]( You can also make a bit of money idly by doing this, while helping to improve privacy and fungibility for all participants on the network.
@_date: 2019-09-09 13:53:04
It was windows-only for a while; then it required an unreleased version of wxwidgets for the longest time to compile on Linux. It wasn't simple at all.
@_date: 2017-10-17 14:54:31
There is a defense against this that AFAIK invented which AFAIK is not implemented anywhere. Maybe he has a link, I first heard about it from him on IRC.
It goes like this: the hardware device produces a nonce `R` (using 6979 or whatever) and gives the public nonce to the PC. The PC generates a random scalar `x` and submits this to the device. The device then signs with `R + xG` as nonce. The PC checks this and then forgets about `x`. Since the device precommitted to the nonce, if `x` was random then any bias is erased.
This is closely related to sign-to-contract. In fact just supporting sign-to-contract would cause this functionality as a side-effect.
@_date: 2017-09-19 15:40:42
Right, they definitely should have pushed them all out on Friday with an underestimated fee, knowing that at some point on the weekend fees would hit a low point, even if they couldn't predict exactly when.
@_date: 2017-10-25 19:15:24
It does not. That is not how Bitcoin works, that has never been how Bitcoin works. There is nothing semantic about this.
@_date: 2017-10-30 21:16:50


I guess what's throwing me are
1. What does a language where the target assembler has interaction look like?
2. In scriptless scripts you often have steps of the form "authorize X and as a side-effect reveal some data Y", and Y might itself have a bunch of structure (like it could be a signature itself) which really doesn't map to any model of programming I know well. And you want to prove that these conditions are actually enforced.
@_date: 2019-09-08 11:16:48
This is a blog post Pieter and I wrote about Miniscript, which we announced on the bitcoin-dev mailing list a couple weeks ago.
Mailing list: 
Online analyzer/compiler: 
Miniscript is a tree-based representation of Bitcoin script where every spending condition appears as a branch of a tree. So answering high-level questions like "what signers are necessary" / "what signers are sufficient", or lower level questions like "what is the maximum weight of a spend of these coins / how do I construct a witness given signatures etc", etc., suddenly become easy to answer in a very general way.
Since Miniscript can encompass all sorts of policies, from Lightning HTLCs (though unfortunately it does not support the actual script used by the deployed LN) to atomic swaps to split-custody wallets to multisignatures, it also enables the creation of general-purpose tooling that can work with all these applications, which (along with PSBT) hopefully should dramatically reduce fragmentation in the Bitcoin wallet space.
@_date: 2015-04-09 19:46:53
You can confirm that you are not using the bitlib/ECTools to do signing? I apologize if so, and will edit my comment.
@_date: 2017-10-04 00:19:16
These aren't "community leaders" though, they are a collection of CEOs, several of which have the attitude that the current Core developers are not using their power enough (because in reality the power they want to see used does not exist), trying to take that power into their own hands (and whether they realize it or not, they first need to make said power exist, say, by setting a precedent that they can unilaterally make rule changes in hotel rooms in closed-door meetings). What users would be deciding by flocking to s2x is that _this is the way the rules of Bitcoin are determined_. An economic majority making this decision doesn't change the nature of the decision.
IOW if it turns out that the community, or a silent majority or what have you, desperately wants corporate rule, it will still be corporate rule.
@_date: 2017-10-08 14:37:55
No, each address has a distinct corresponding private key, but this is an implementation detail of wallets -- it doesn't have anything to do with privacy or linkability of addresses.
The GP said "an address isn't _just_ a random string of numbers and letters" (emphasis mine), but it _is_ a random string of numbers and letters which under normal circumstances will be uniformly random (except the checksum) and independent of all other addresses. It's the structure of transactions on the blockchain that cause addresses to be linked, not anything inherent to the addresses themselves.
@_date: 2017-09-25 20:26:45
Also, obviously software is "obligated" to be usable without creating unrelated valid Bitcoin transactions and publishing them, but there is nothing Bitcoin can do to prevent this. It's hard to describe what s2x is doing as anything but theft.
@_date: 2017-10-03 19:15:46
(a) there was just a capacity increase in August, (b) there is lots of research into ways to increase capacity without increasing resource load.
But yes, the opposition to 2x is mostly that it is a recklessly implemented attempted corporate takeover of the system, not that increasing the blocksize by a small constant factor is destructive in itself. (At least, not now that we have headers-first validation, compact blocks, libsecp, a series of database optimizations, better caching, etc etc)
@_date: 2017-10-08 15:40:41
Sorry, by "processing" I was referring to validating transactions, not mining.
@_date: 2015-04-09 19:51:21
Sure, but it is in the source code linked to from your website.
If you aren't using bitlib in released products, I will edit my comment to no longer accuse you. But you're including this broken library, so it seemed like a reasonable assumption that you were actually using it.
@_date: 2017-10-06 00:52:05
@_date: 2018-02-22 15:39:53
I'm not sure this will be very helpful, but here is a stab at it:
Take a value v, convert it into its bit vector. Each bit b will satisfy b*(b - 1) = 0 iff it's actually a bit, and v will satisfy `v = b0 + 2*b1 + 4*b2 + ...` iff these are actually the bits of v.
So rewrite this last equation as `(v - b0 + 2*b1 + 4*b2 + ...) = 0`, and now we have 65 equations in secret data which should all equal zero. To prove that a bunch of linear equations equal zero, it is sufficient to prove that a randomized sum of all of them equals zero.
So commit to all the bits and v, hash these commitments to get 65 randomizers, and then check that 
`r0*b0*(b0-1) + r1*b1*(b1-2) + ... + r64*b64*(b64-1) + r65*(v - b0 - 2*b1 - ... - 2^64*b64) = 0`
Notice that this is a giant inner product of secret data. We can make it into a giant inner product of public (well, publishable) data by replacing each secret object s with a secret polynomial s + r*X, where r is a randomizer. Now our above inner product equation, rather than equalling 0, will equal a bunch of things times X plus a bunch of things times X^2.
Ok, so the prover commits to the X coefficient and the X^2 coefficient, and hashes _these_ to get a random challenge `x`. Call these commitments T1 and T2. Then the prover reveals[1] the two vectors that are supposed to be inner-producted with all components evaluated at `x`. Since each component is randomized the original bits and `v` are hidden. The prover then computes the inner product, multiplies it by G [2], subtracts `x*T1 + x^2*T2` to cancel out the coefficiets of `X` which are really just noise, and checks that it's left with 0.
[1] Ok, actually rather than revealing the vectors, the prover computes an efficient inner-product argument.
[2] Actually, there is a `r65*v` somewhere in there which _isn't_ committed as part of either vector. The verifier also subtracts `r65*V` from its sum, where `V` is the Pedersen commitment that supposedly commits to `v`.
@_date: 2017-10-17 14:46:31
FWIW with Bitcoin transactions and the Ledger protocol there is no freedom for grinding.
@_date: 2017-10-28 23:29:03
CSW is claiming that secp256k1, the elliptic curve group that Bitcoin uses, supports something called the Weil pairing, which is used for non-Bitcoin-related cryptography. It does not, which is something that can be trivially checked by somebody familiar with the algebraic elliptic curves.
Well, strictly speaking it _can_, but so inefficiently that it would require the entire universe just to encode the numbers that you need to do it. That was the point of my post.
@_date: 2017-10-30 00:10:42


This is an unfortunate impression because his knowledge of cryptography seems to me to be well below average, since most people have far fewer specific wrong beliefs.
@_date: 2017-09-11 23:55:19
Something like Bitcoin kinda has to be MIT. This level of permissivity is needed to let people work with Bitcoin protocol even in situations where they're professionally or legally unable to use GPL software, such as traditional companies interacting with the Bitcoin network who do not what their code to be public.
Regardless of how you or I might feel about this, if the code were more restrictively licensed they would likely be forced to write their own consensus code, creating the risk of forking themselves off the network and more generally increasing the risk of widespread consensus faults.
It's also worth noting that the MIT license _already_ prevents the kind of crap that the bitcoin-abc people have been pulling, but here they are doing it anyway. Ultimately there's only so much a license can do.
Finally, it sounds like in this scenario Garzik has every right to be using code like this, and should have every right to do so. He isn't claiming credit for anything he didn't write, he's just doing his own thing without reinventing the wheel.
@_date: 2019-09-08 11:34:21
Yep - one simple reason is that Miniscript uses `OP_CSV OP_VERIFY` instead of `OP_CSV OP_DROP`; this saves us a fragment in the definition of Miniscript. Obviously if this were the only thing blocking LN-compatibility we'd just change Miniscript.
But more seriously, the existing LN HTLC switches on a pubkeyhash in a way that has no analogue in Miniscript. I don't see any way to add this to Miniscript without significantly increasing its complexity; and given that a lot of the tooling that Miniscript enables has already been implemented for the special case of Lightning, there's not a super strong benefit. (Though as somebody on lightning-dev pointed out, the ability to ask Core to sign LN transactions would be potentially very useful..)
The final result is logically equivalent to a Miniscript but the individual components are not.
Our Miniscript compiler was able to find smaller scripts equivalent to both the Lightning HTLCs without using this pubkeyhash-switch construction. I suspect, though I haven't tried, that something even more efficient could be found by combining the two approaches.
@_date: 2015-04-09 19:39:21
Regarding the "second long glance at the software" comment, I can also vouch for this. In fact, a few days later I was able to find the same bug in [Mycelium, which I was linked to while skimming LedgerHQ's codebase]( in under a minute. This is what let to the above observations by nullc:
Since I was aware of empirical evidence of Trezor's timing sidechannel vulnerability, I checked to see if they were using the same broken algorithm as Ledger. They were not, and in fact their code seemed unlikely to have the millisecond-order variances that were being reported, so I checked git blame and found their [fix]( three weeks ago, which is what users are now being implored to update to. The commit message mentions that the fixed code is still not fully timing-resistant; nullc looked at it more closely and noticed that they were doing silly things like manipulating secret values which were encoded in wNAF.
To be clear, I was doing not any sort of code audit here. I was answering a question on IRC, and doing an appropriate amount of effort (i.e. two minutes searching github) for this task, when I noticed these things.
@_date: 2017-10-08 15:58:35
In theory, the blockchain cannot distinguish an address of yours from anybody else's address. But in practice there are a lot of ways to distinguish them and the "leap of faith" you refer to is unfortunately solidly backed by data mining science.
Remember that it's not just Coinbase who knows your ID, but _any_ merchant or other party you transact with (at the very least, stores likely have surveillance video that shows you paying). In the case of light wallets such as Mycellium, the server providing blockchain data to your wallet is probably also able to infer data about how you're using Bitcoin by virtue of what specific data you request.
Combined with all these "leaks" of blockchain&lt;-&gt;real-life linkages there is a lot of blockchain data that can extend the analysis beyond specific addresses whose real-life purpose has been leaked:
* outputs spent in the same transaction likely have the same owner
* outputs that have the same address almost certainly have the same owner (don't reuse addresses!)
* outputs that have weird scripts, e.g. multisignatures or timelocks or hash-preimage challenges, are very distinct
* change outputs tend to have less round amounts than "real outputs" and some wallets even put them in the same position for all transactions
* transactions have a ton of "wallet fingerprints", small quirks of transaction construction which are unique per wallet software and can be used to link transactions to one transactor
* analysis of amounts and timing
* analysis of network propagation, where transactions appear to originate from, etc
This data can be combined in various ways. For example I know Poloniex withdrawals are extremely identifiable, so given a transaction that I can link to some person which I _also_ know is a withdrawal from Polo gives me a ton of information about what they're up to. 
These are solvable problems but the current state of the system is that they are very far from being solved. Plus a lot of this analysis can "see through" things like coinjoin if they are poorly executed or if they are executed with sybils (fake joiners who are recording data about the unjoined transactions), so some proposed solutions (e.g. bc.i's "sharedcoin") have been unhelpful or even harmful.
On the more optimistic side, I've been working with a few others on something called [aggregate signatures]( which we hope to eventually propose for Bitcoin in some form. A direct consequence of this would be to make CoinJoin cheaper to use than to not use, which would be a boon for privacy. Another side-effect of this is that it will make [many forms of smart contracts]( including multisignatures, atomic swaps and payment channels, possible to do such that they are indistinguishable from ordinary transactions. Other people are working on wallet fingerprinting with the goal of eliminating known forms of fingerprints, and the [JoinMarket people]( are pushing coinjoin forward.
@_date: 2015-04-09 19:52:33
I'm sorry. Thanks for your quick response. I've edited my comment.
@_date: 2017-10-12 14:06:02
A "compromise" that results in no fork would be the same as 2x capitulating -- 2x consists of a single change which is a fork.
You're correct that "the NYA people regrouping and trying again later" is not reflected in the BT2 contract, but it's not reasonable that it would be. That would be a different event, and if it involved a fork, that fork's tokens would be represented by a different future.
@_date: 2017-10-23 23:39:44
Whew, that's pretty bleak.
**Edit:** So everyone is jumping on me about how self-interested individuals can still come to good outcomes, and Bitcoin needs to work in a world without altruism (and it does). All true. Saying "There is no community" is still bleak to the point of being incorrect. The world is not so lonely.
@_date: 2018-04-14 14:28:58
It's a bit unfair to credit only Rust and Ristretto with the speedup -- in fact, the bulk of it comes from the use of AVX2 and some comes from the fact that the Ristretto benchmarks start from proofs represented by full points rather than compressed ones, both of which result in unfair comparisons. Correcting for these, the speedup for verifying a 64-bit rangeproof is about 17% [*], which is still a phenomenal result. I think the authors weaken their claim by conflating the speedup from hardware accelleration with the speedup from their own optimizations.
This 17%, as best I can tell, comes from a combination of using flat vectors to store the exponents in the multiexponentiation, which unfortunately has a nontrivial memory cost, and the use of Rust, which eliminates a lot of callback logic that compilers have trouble optimizing through. This is especially impressive because the underlying crypto primitive library, `curve25519-dalek`, actually has worse performance than libsecp for both scalar operations and batch scalar-point multiplying.
The code is much smaller than clearer than that in libsecp (thanks again Rust!) and the [documentation is incredible](
[*] On my 2.00 Ghz test machine, the libsecp code takes 3756 microseconds to verify a 64-bit rangeproof while ristretto-bulletproofs takes 3224.
**Edit:** Actually, when I run the curve25519-dalek benchmarks on this same system, it appears that a 128-point multiscalar multiplication takes 5190 microseconds, longer than a 64-bit rangeproof verification which should involve a 147-point multiscalar multiplication. So now I'm unsure what to make of these numbers.
**Edit2:** After chatting with Oleg about this, I realize that I wasn't setting a flag in curve25519-dalek to use some new optimized code; so the underlying crypto is actually faster and the numbers are all consistent.
@_date: 2018-04-16 13:37:21
The bulletproof code uses function pointers, yes.
@_date: 2018-04-21 17:28:46
Sure, what I mean by inflation is that with any reasonably-efficient implementation of CT, somebody who can forge signatures can use the same technique to create transactions that don't balance. For example, they might make a transaction that consumes inputs worth 10 BTC and creates outputs worth 100 BTC. And because CT hides the amounts, there is no way for anyone to detect this.


Yes, but it should have scalability and unconditional soundness like Monero doesn't ;)


Yes, in general the Monero philosophy about new privacy tech is far too bleeding-edge for Bitcoin. But if CT were unconditionally sound, or if ring signatures did not require an ever-growing list of key images, I'd be trying to push these things forward and get them the kind of scrutiny and quality assurance that Bitcoin requires.
@_date: 2016-10-07 21:57:57
Here is fine, thank you very much. I've fixed these in my working copy.
@_date: 2019-09-18 12:54:23
As soon as the elements you're operating on are larger than 4 bytes, basically none of the opcodes work anymore. Just hashing and equality checks are left.
I think if there were an unencumbered â‰¤ operator that'd be enough, in principle, to verify any computation. But (a) that'd be horribly inefficient, and (b) there's not :).
And of course, as implemented in Bitcoin, there are also many resource limits (script length, # of opcodes, tx size, etc) that you'd quickly run into.
@_date: 2016-10-09 08:18:33
Yep -- in Bitcoin every transaction has a set of outputs, and inputs that are outputs of previous transactions (or coinbases). So you can draw a graph where vertices are transaction outputs, labelled with scripts and values, and an edge exists between two outputs A and B if a transaction spends A and creates B. (And the edges are labelled by transactions.) This gives a "transaction graph" which anyone can create and which contains a lot of information about the relationship between outputs, in particular since an edge between A and B likely means "the party who owned A sent money to the party who owns B".
This graph is also highly structured: every output is created by exactly one transaction, and spent by at most one transaction.
Confidential transactions removes the 'value' label from vertices; MW does this and also allows merging of transactions (creating many edges that don't have semantic meaning, hiding the ones that do) and cut-through (removing some verticies entirely).
With Monero, transaction inputs are ambiguous: each one might refer to any of some set of outputs, but only one is actually spent. (I think they always use sets of size 3.) So you can draw "A was maybe spent in a transaction that created B" edges, but A may be apparently-spent indefinitely many times in the future, and ditto for B.
@_date: 2016-10-09 08:40:45
It will increase the value and utility of any chain which it is sidechained to.
I'd strongly discourage anyone from trying to make a MW altcoin: ignoring the standard "you aren't allowed to issue securities" "you'll have to make a market" "your trustless token will require trusted on/off ramps" altcoin reasons, MW has a very bad security story in the presence of quantum computers and it would seem immoral to create a chain where people could not trustlessly move their tokens to a more resiliant system.
@_date: 2018-04-21 14:07:59




Bitcoin itself has very little surface to "take over" and no evidence that the usual suspects have tried (*cue rbtc theme music*), except perhaps in very clumsy ways. What you're concerned about is actually a small collection of current user-facing Bitcoin companies, a collection which historically has had high turnover, poor understanding of the system, and little ability to affect its functionality beyond creating large volumes of low-efficiency transactions.
One example of their poor understanding of the system is how willing they are to play along with chainanalysis schemes, which is counter to their own interests because they're essentially claiming that they're able to detect certain types of behaviour on the blockchain when this is actually impossible (in general, that is -- ordinary users who don't take extra steps are affected). Not to mention the direct cost of storing this data, shipping it off to analysis companies, storing the results, etc.
I agree that this behaviour is extremely concerning and dangerous to Bitcoin, but to the extent that it's a problem caused by Bitcoin itself, it's one that's been around for a long time and is very hard to fix. It _does_ need to be fixed, but this is not a race against the current crop of exchange companies who'll be gone in a few years anyway. It is a race against huge swaths of the regulatory regimes in the world getting the idea in their heads that chainanalysis is useful or desireable, requiring some form of it to participate legally in the space, and thereby kill the fungibility and value of the system. And fortunately, regulatory bodies are very slow and cautious (for much the same reasons that developers of consensus systems are). Also fortunately, the _presence_ of privacy-focused altcoins in the space undermines that, no need for panicked Bitcoiners to risk their funds by using these things day-to-day.




Bitcoin has been useful as a surveillance tool since its inception. This is not a new problem, but it is definitely not one that people have been ignoring. As improvements come out which don't have serious and consensus-preventing tradeoffs (Taproot will be one such thing, I think) you will see them deployed. Things will, and are, improving.
The amount of research going into this is tremendous, to the point that it's very hard to keep up with even for somebody actively working in this space. I keep half an eye on what's communicated to the public and I appreciate how slow and frustrating this looks, but behind the scenes it's anything but. When I got into this space I had a lot of ideas about new crypto that needed to be invented and how it would transform the privacy landscape of Bitcoin. Five years later we have (almost) practical technologies that are better than anything I _imagined_ back then.
Consider how potent [Taproot]( plus [Scriptless scripts]( are -- payment channels that are indistinguishable from ordinary transactions, are unlinkable even by different parties along the same path, offer [multipath payments]( with transferrable proofs of payment, and are smaller and cheaper to verify than ordinary transactions today.
But translating this rapid rate of progress into consensus changes is hard. You don't get to iterate, so what gets deployed needs to be the best it can be. You need to get consensus, meaning you need to demonstrate no regressions in scalability, privacy, usability or security, all of which are multi-dimensional and any of which will stall a proposal. The proposal needs to be sound, because if it breaks the system then it's effectively game over, and beyond _being_ sound you have to _convince_ everyone that it's sound.
In particular, by the way, anything CT-based will introduce a scalability hit, though BPs largely eliminate this, and reduce the soundness of the system to the discrete-log problem.
@_date: 2016-10-09 08:35:57
Sidechains do need p2p code (if they want to have a dynamic user set).
@_date: 2018-04-08 15:15:13
I have a couple quibbles, two minor and one not:
* Rather than saying "everything is really just G blahed with itself a number of times", you could say "if S satisfies the other rules, so will the subset of S consisting of G blahed with itself some number of times, for any G. So pick a G and assume S consists only of those elements that are G blahed with itself."
* When you compute `s = (m + rx)/k` "modulo the size of S", you need that `k` is coprime to this size. You may want to insist that S has a prime size and make a note that division is a sensible operation modulo a prime.
Then the more serious one:
* In `s = (m + rx/k)`, `r` is not "G multiblah k", since `r` needs to be a number mod |S| and "G multiblah k" will be an element of S. In ECDSA this is the x-coordinate map, in DSA it is a bizarre map where you reinterpret an element mod p as an element mod q. You could use a hash function here if you want. But whatever the map is, I think you need to explicitly show that it's there (I have seen Ï† used as a symbol for this purpose.)
Unfortunately the properties required for Ï† seem to be subtle, and probably confusing to go into. Essentially, for the zero-knowlege property there are no requirements, it could be a constant function and it wouldn't matter. For soundness you require a very weak form of collision resistance where "not too many" values of k lead to the same value of r.
@_date: 2016-10-08 12:49:56
It's possible to merge transactions before putting them on the wire, but doing this automatically on a p2p layer without DoS attacks or potential for stuck transactions seems to be a hard problem.
So.. yes, but there's no fundamental reason for this to be so.
@_date: 2016-10-09 08:20:03
Hi, sorry, I forgot to push the source somewhere public.
Here it is: 
@_date: 2016-10-07 11:09:00
Yes, I believe support for peg-ins and peg-outs can be added to Mimblewimble, although these would not be mergeable or prunable like the other data is -- however, there would in general be far fewer of these than on-chain transactions.
Unfortunately I see no way to support cross-chain atomic swaps on MW, as this would require something analogous to Bitcoin's "spend by revealing a hash preimage" script support.
@_date: 2016-10-09 08:34:35
Mimblewimble does support multisig. Sorry, my paper really does not emphasize this enough, it comes for free from the Schnorr-style rangeproofs.
@_date: 2018-04-16 23:41:31


I think 1% is about right for a 64-bit proof. For small proofs, and for batch validation, and for rewinding (which is a thing we do in Elements to encrypt small amounts of data into Bulletproofs) it gets to be a nontrivial cost. But good point regarding the fact that you compress them when feeding them to the hash function -- this means that you are actually paying the cost, I'm just whining about nothing :).


No, you're right, I was failing to turn on this flag when I was comparing curve25519-dalek to ristretto-bulletproofs.
@_date: 2018-04-21 14:09:41
I'm not working on it because it would allow a discrete-logarithm attacker to silently inflate the system, which I don't think is acceptable for something like Bitcoin that ought to last centuries.
@_date: 2016-10-09 10:04:08
I think a MW chain should have a separate merkle root in its header for free-form headers, and miners can put whatever they want in there (perhaps for a fee) and give proofs to the people who care about them. Normal nodes would just download the merkle root and not verify them. It could be used instead of a nonce so there's less wasted space (not that a single hash per block is a ton of space..).
Also, I think timestamp-verifying nodes would need the entire header chain, the compact chain stuff that I've been working on is completely unable to prove timestamps because of its statistical properties.
@_date: 2018-04-23 12:53:07


You can replace all this stuff with lattices, they're just much bigger. So there is a plan B.
@_date: 2016-10-07 10:22:59
Thanks so much for this comment :)
@_date: 2016-10-13 16:57:28


MW does nothing to eliminate the need for PoW. Where did this rumor come from?
@_date: 2017-05-08 16:28:14


It can be both, using Confidential Assets.
There are many reasons to avoid altcoins, but one benefit of them (vs a sidechain) is that they are conceptually simple and easy to get off the ground. So I've [proposed a design to Grin, which seems to be accepted]( where they can support multiple assets in a way that they can later soft-fork in a peg to Bitcoin, so the extra design complexity (and solving open problems around trustless pegs) will only delay the use of pegged Bitcoin, not the rest of the tech.
@_date: 2015-11-16 20:12:39
Until BIP66, using OpenSSL was absolutely a consensus rule, and btcd was not running the same rules as Core. (This is almost certainly still the case, but BIP66 closes at least this one discrepancy.)
@_date: 2018-04-08 15:24:07
* The integers, with addition as "blah" and 0 as O, are a group.
* The nonnegative integers, with addition as "blah", are not a group (no element that you can add to 2 to get 0, for example)
* The integers with multiplication as "blah" and 1 as O, are not (no element you can multiply by 2 to get 1, for example)
* The integers with _subtraction_ as "blah" and 0 as O, are not a group. (A - B) - C is not equal to A - (B - C).
* The positive rationals with multiplication as "blah" and 1 as O, _are_ a group.
* But the nonnegative rationals.
* The integers mod 10, with addition as "blah", are a group.
* The same integers with multiplication as "blah" are not (0 has no inverse). Even if you exclude 0 (then 2 blah 5 is no longer in the group).
* But {1, 3, 7, 9} with "blah" being multiplication mod 10 _is_ a group.
* The set of invertible functions from any set onto itself, with function composition as "blah", forms a group. For finite sets this is called a "permutation group" because all these functions are just ways of rearranging the elemnents.
* The set of strings over some alphabet with concatenation as "blah" and the empty string as 0, is _not_ a group (nothing you can concatenate with "a" to get the empty group).
* Invertible rational-valued 2-by-2 matrices, with matrix multiplication as "blah" and the identity as 0, _is_ a group.
* The [Monster group]( is a group. It is simple, but probably not in the sense that you meant, so I put it last ;).
You can make digital signatures using any of these, with varying degrees of security.
@_date: 2017-05-15 17:24:47


This is a bizarre example. docx files are incompatible with doc parsers and vice-versa, something that caused a tremendous amount of headache when Office 2007 was introduced. It was several years before Linux users had easy access to software that could understand docx files. You may recall [this letter from RMS about it]( where he makes arguments very similar to Greg's about FT ... about .docx!


I think [this is the flextrans specification]( I'm trying to understand how sighashes work, but all it says is "If the hash-type is 0 or 1 we hash the tx-id of the transaction. For other hash types we selectively ignore parts of the transaction exactly like it has always worked."
The best read I can get of this is that 0 or 1 (which traditionally were SIGHASH_ALL) will be interpreted as signing "the tx-id" (defined later under the name "TxId"), but that any non-1 and non-0 values will fall back to the old `CTransactionSignatureSerializer` method? But [the code in Bitcoin Classic]( behaves very differently, using CMF and whatnot for every sighash type.
Can you point to documentation on how the signature hashes in FT work? Does it include an explanation for why the various weird behaviours of `SIGHASH_SINGLE` are preserved even though the hash is otherwise incompatible with existing software?


Can you distinguish "a different way of serializing" (which apparently is the entirety of FT) from "encoding" (wrt FT does not introduce anything new)? Otherwise your comment appears inconsistent.


Another strange example. Browsers have never had consensus on how to parse HTML documents, and are actually so far away from even _appearing_ to render websites similarly that a cottage industry of user-agent-detecting hacks has grown around it to serve different HTML/CSS/javascript to different browsers, with varying degrees of success. Anybody who has tried to do web development in the last twenty years would conclude that "strict in what you encode/generous in what you accept" is a dismally failed philosophy for designing failed protocols.
@_date: 2015-11-16 20:11:14
If switching to libsecp is a consensus change, then this is a serious bug. :)
Having said that, until BIP 66, it would've been, because OpenSSL weirdness was leaking into consensus rules, and libsecp is unable to replicate these because of a combinatorial complexity explosion.
@_date: 2016-10-09 08:32:36
The story is the same as for any sidechain (complicated; Brian gave some comments; note that a sidechain can still pay Bitcoin-denominated fees, which may help). But this is highly unlikely, given that BTC supports all sorts of use cases that MW does not, since it does not support any scripting. I list some of these in my paper: HLTCs, zero-knowledge contingent payments, coinswap, tumblebit, atomic cross-chain swaps, etc., etc.
Its security story is also much worse in the presence of quantum computers (there are a few ways Bitcoin can soft-fork to retain its current security, even if this has a bandwidth/CPU hit; whereas Mimblewimble will allow undetectable inflation in this scenario).
The UX around Mimblewimble is also inherently harder than that for Bitcoin. There shouldn't be any "addresses" (ignoring size/encoding issues, if somebody creates the same output twice they will be exposed to replay attacks); block explorers will not be able to see individual transactions; there won't be any notion of "txid"; with my sinking signature construction transactions will have a maximum block height they can appear in (so you have to re-sign if you miss your chance); the payment channels I've devised are similarly unergonomic.
These are solvable problems, but given the lower mindshare of MW and higher difficulty, I don't see how it could possibly get a jump on Bitcoin.
@_date: 2015-03-20 18:19:32
I think not, because the trust model behind DPOS is very different from Bitcoin's. In particular requiring the existence of any indentifiable party means that their consensus is not "distributed" in the sense of my paper.
I haven't looked very closely at DPOS (nor do I have time to, sorry), but I have heard from people I trust that Bitshares has churned through a /lot/ of underspecified and/or broken cryptosystems. Please don't take this post as an endorsement of any sort to any extent. Only that I don't believe my paper is applicable to their system.
@_date: 2016-10-23 19:27:59
r/btc forgets very quickly. You can spend hours there correcting falsehoods (which takes forever because people fight you over absolutely stupid things, and it's unfair because you're required to look things up, reason them out, etc., while the people responding appear to simply invert random sentences of yours without even parsing them). They will also move the goalposts, throwing snide falsehoods about unrelated things that you have to decide whether or not to bother responding to. And then two days later, the exact same stupidity is being posted again, as fact, in multiple threads.
If you maintain a database of old posts of yours you can link to them in the new thread, but this is (a) still a lot of work, and (b) you'll get the same trolls responding to your links with random contradictions _and_ responding on the original thread with random contradictions.
Add to that that it's mixed in with all sorts of weird conspiracy theories and threads so stupid that they make you look stupid if you even post in them (again, something you have to think about and the trolls don't).
And this is the situation for technical subjects -- on non-technical ones where the claims are unverifiable or conspiratorial or simply ill-defined personal attacks, it's pretty much hopeless.
@_date: 2015-11-16 20:11:45
All consensus rules are "validating signatures" for a sufficiently wide definition of signature :).
ECDSA validation is definitely consensus code.
@_date: 2016-10-09 09:59:49
Anything that requires hash preimages: bidirectional payment channels (it can do one-directional ones and you can use two in parallel to get two directions, at cost of twice the transactions and twice the tied-up liquidity), coinswap, cross-chain atomic swaps, zero-knowledge contingent payments..
@_date: 2016-10-05 16:13:18
This is great to see -- congratulations Rusty and Christian!
@_date: 2016-10-08 12:55:11
Monero's privacy improvements are orthogonal to MW's: MW obscures the shape of the transaction graph while Monero makes the vertices themselves ambiguous.
Conceptually you could even have a system that does both, but crypto-wise I don't have the first clue how to do something like that.
@_date: 2015-03-21 00:50:56
It's true that I don't give a good definition of distributed consensus in the document. I cheat by deferring to Andrew Miller's analysis. I take umbridge with this affecting the "quality" of the document. The original purpose was to quell the influx to  of people promoting stake-based systems (which were always broken by some variant of costless simulation, and every person would spend a -long- time trying to patching our suggested attacks, rinse, repeat). After a few months of this, which started around the advent of Peercoin, Greg Maxwell and Andrew Miller (among others) extracted the general pattern behind these attacks, and I wrote it down to satisfy my own curiosity about how fundamental it was. So while I believe my argument is solid, it depends on a lot of -wizards folklore, and I don't have the time or energy to write all this out (though this is a long-term goal of mine).
Later, several people, notably Gavin, started publishing my document away from IRC as a "generalized anti-proof-of-stake argument", which it sorta was, but it was definitely not as readable as it should've been to serve that purpose. So I rewrote it (a) in light of how much -wizards lore had been written down in the meantime, and (b) to make it more accessible.
I still don't have time to write it to an academic level of rigour the way you would prefer. I do believe that this is possible.
Thanks for the link to your paper. It is long but I will read it; it is a cool abstraction and describes desynchronization attacks which I don't recall having seen addressed before. I handwave them away by defining a "synchronous network" to be one where this doesn't happen. So I have this weird pseudo-synchronous requirement where I claim time-ordering doesn't exist but the network can still have "heartbeats" every ten minutes on average. I agree this sounds suspect, but I claim it's just an artifact of my writing a popular-level argument.
Unfortunately I'm rarely in the Bay Area. My home these days is Austin, TX. I hope our physical paths will cross in the future.
@_date: 2017-05-20 15:49:36
Which tag and which fork of which repo?
@_date: 2015-03-24 02:45:44
No, not directly. In future there may be a sidechain which uses this or some similar signature. What it allows is transaction signatures which don't specify exactly which output is being spent; it might be one of several, and everyone else has no better than uniform chance of guessing which one. However, there is one piece of the signature, a "key image", which is entirely determined by the "real" output. So if you try to double-spend, the network will see the same key image being used twice, and reject it.
This greatly improves privacy since blockchain analysts cannot see well-defined chains of transactions moving coins through history; at each step there are several possibilities.
However, things are not as great as we imagine they could be. When I say "one of several", I mean one of several. When creating a transaction you choose 5 or 10 "fake outputs" to mask the one that you are spending, and the outputs you choose define your anonymity set. If you make your anonymity set twice as big, the signature will be twice as big, so this limits. What we'd like is to get rid of this size dependency, so you can pull /every/ output into your anonymity set. Is this possible? Yes, by introducing slow new questionable cryptographic assumptions .... otherwise we don't know. Answering that is why I posted this link; I was hoping somebody had some obscure useful reference in the back of his or her mind.
This will not be forked, hardly or softly, into Bitcoin, because (a) these signatures are larger and slower than normal Bitcoin signatures, (b) it is a significant amount of complexity, and (c) it improves the privacy of the system, which some feel will invite unsavory behaviour and corresponding attention from legislators or media. The latter point means that even if proposed, we would never get consensus.
Besides, this kind of "cool new feature" is exactly the sort of thing sidechains are made for :).
@_date: 2017-05-28 13:30:38


I am fascinated to hear what happens if you try to use this "Bitcoin is not a currency, it's freedom" argument in a legal context. ;)
@_date: 2016-10-13 16:05:43


Implementing MW is very technically difficult. It's not something you can sham together in a couple weeks or months.
@_date: 2017-05-19 14:36:31


I'm a developer affected by segwit, and [as I've argued many times before]( Segwit is a _marked_ decrease in complexity for everything related to signature hashing, and also greatly reduces the amount of data that needs to be shunted around to deal with transactions.
Have you ever tried to sign a raw transaction with a cold wallet? You have to shunt the entire transaction for every input, along with their signatures, just to learn how much money you're spending. And on the other side, you need to reconstruct the entire transaction to sign it (you can edit it inline if you've got enough memory, which many hardware wallets do not), and do this _again_ for every input. It's insanity.
@_date: 2017-05-26 18:34:03


In this case, each timestamp took significantly less than one bit.
@_date: 2017-05-02 22:59:35
We cannot "simply delete the pledge", it is legally binding on us.
The DPL covers anybody who also offers their patents under the DPL. This is not "asking for protection in advance" and beyond that they do not need to take any action to obtain a DPL license from us.
@_date: 2016-10-08 13:00:37
In principle nodes can aggregate their own transactions onto other transactions that are passing through the network. No interaction with the original senders is needed.
Actually doing this in a p2p way is hard, but even if MW's p2p layer doesn't support it, peers are welcome to do this themselves outside of the protocol.
@_date: 2015-03-23 19:57:11
This is the result of some work Adam Back (and a tiny bit me) has been doing recently trying to get short ring signatures without controversial cryptographic assumptions. He found a way to get ring signatures which are around 1/2 the size of those used in Monero, then discovered a 2004 paper which did basically the exact same thing.
A fun problem to think about (which I'd really like to see solved): can we somehow adapt these signature schemes to get ones that are smaller than O(n) in the number of keys? Can we get ones that are constant-size?
@_date: 2015-03-24 18:33:21
Yeah, option 2. I'm not sure how to interpret "an improvement on Zerocoin"; it's complicated. (Zerocoin is much slower and has a trusted setup; on the other hand its anonymity is much better; by virtue of using simple algebra, these ring signatures also easily handle some forms of multisig, which I don't think Zerocoin can; the same algebra means we need to be careful designing things like BIP32-style address derivation to not expose key-&gt;key image mappings.)
Also note that Zerocoin has been superceded by Zerocash, which has these problems, but to a lesser extent :).
@_date: 2017-05-02 23:30:32
Need to check with my lawyer. My understanding is that the license request is retroactive, i.e. you can do it after somebody initiates litigation against you and it will then stop this litigation in its tracks. (The exception of course is if Blockstream withdraws from the pledge, at which point all other pool members would have 180 days in which to contact us regarding our existing patents.)
In any case, it is not a lot of work to send a form letter to all the members of the DPL requesting licenses for every patent (it is actually less work than requesting specific patents because you don't need to list them), and to send such a letter to every new member. 
I am aware that the DPL is not a public license. It is available only to other members of the patent pool, which anybody can be, provided they agree to abide by the terms of the DPL, which are not onerous requirements.
@_date: 2017-05-26 18:17:08
Well, if using one 750 millionth of a transaction to timestamp something is too much of a burden on the network, they could instead use sign-to-contract and get this down to 0. But this is not a huge difference, so work is a bit slower than you might hope.
Having said that, your anger probably misplaced... the multiple other services linked in the OP which use an entire transaction (or even two!) to timestamp individual files. Convincing them to use a more efficient method would save more space than opentimestamps uses _in total_, many times over.
@_date: 2018-04-16 18:42:51


Where do you get that number from? A 2-in-2-out transaction, which is normally 250 bytes in Bitcoin, would be about 1Kb with CT+Bulletproofs, so in total 4x the size. Larger transactions would see a much lower increase; with 16 outputs the BP would be less than 60 bytes per output.
@_date: 2017-05-02 22:49:28


The only terms that we can enforce are the ones forbidding others from patent litigation related to our patents. Under the DPL and patent pledge these are the only terms that exist.
Are you worried that we will let some people sue us, but not others?
@_date: 2015-03-29 18:40:28
Nope. No information about your private key (well, except that it maps to a specific public key :)) is contained in a transaction, or indeed anything that is ever transmitted over a network. Private keys really are private.
@_date: 2015-03-20 17:19:12
I think this is fixable by more aggressive inflation, or limiting the amount of currency that can be bonded as stake (e.g. say you can only bond 1% of any UTXO; then only 1% of the currency will ever be used for consensus).
There are a lot of "obvious" economic problems with PoS along the lines of wealth begetting wealth, tendency to oligarchy, etc., but I'd be surprised if any of these weren't fixable by appropriate monetary policy. (Generally monetary policy is hard in cryptocurrency because it's part of consensus code and therefore really rigid. But structural problems like this seem pretty static so I don't think it's a problem.)
I could be totally off-base here, of course. I don't have any economic training and I haven't spent a lot of time thinking about it.
@_date: 2015-11-29 00:06:33
As I posted in another thread:
While this attack is effective against signatures produced by OpenSSL on the secp256k1 curve (which is simply a mathematical curve), this has nothing to do with libsecp256k1 (which is a cryptographic library). Bitcoin Core, as well as several other popular wallets, uses this library (and not OpenSSL) for producing signatures, so this vulnerability is not a concern.
In fact, Bitcoin Core has produced signatures this way since 0.10 -- the remaining use of OpenSSL was only for signature verification, which naturally does not involve secret data at all and cannot be subject to compromise.
A few people have mentioned that `libsecp256k1` appears in the paper, in a survey of different scalar multiplication functions. The paper itself points out that `libsecp256k1` does not use the vulnerable scheme and therefore the result does not apply to it. They do comment "all of the above approaches have potential side-channel issues", which sounds ominous. However, our scalar multiplication scheme avoids any potential issues entirely: its memory access pattern is deterministic and unrelated to any secret data, which prevents all cache timing attacks.
@_date: 2015-03-20 20:14:02
Well, if your genesis block is wrong you're simply not using a Bitcoin blockchain. If you're sending or receiving coins it is easy to ask the other party what his or her genesis block is to confirm that you are using the same chain; for each genesis block (with sufficient hashpower, I suppose) there will be consensus on what the "true chain" beginning with that block is, so it's sufficient that transacting parties agree on this blockhash (which is static, so not dependent on any universal time ordering) for them to know that they both have the same view of the network as any other participant.
I agree that in practice users trust that the genesis block hardcoded into their client is the correct one, and that in principle if there were multiple "Bitcoin" genesis blocks floating around that there would be no way in principle to distinguish them without finding some trusted source to ask.
But what Bitcoin seeks to obtain is a distributed consensus on the history starting with block `000000000019d6689c085ae165831e934ff763ae46a2a6c172b3f1b60a8ce26f`, since this blockhash is part of Bitcoin itself and changing it means you are on some other system. It does not seek to obtain global agreement on the definition of Bitcoin.
As for checkpoints in Bitcoin, these have nothing to do with consensus, which has been argued many many times.
@_date: 2017-05-27 03:05:40
Yeah. You could submit digests to miners and they could form a tree that they add to blocks ...but you can do this today. In fact, miners don't even need to be involved. This is what OpenTimestamps does :)
@_date: 2017-05-27 12:46:17
I'm saying that you can essentially do this today, OT-style. Adding a special area of the block for this stuff wouldn't gain you anything but _would_ cause coordination problems.
@_date: 2015-03-20 21:49:10
Tendermint uses a different trust model than Bitcoin. Last I checked the whitepaper, this is not explicitly addressed; also, there is a lot of confusion between blocktime and real time. Its author, Jae Kwon, has discussed these things at length on IRC, though little progress was made because at the time we were not thinking in terms of "trust models" and there was a lot of talking past each other. I messaged him privately and (I thought) made a lot of progress on this front, but this has not been reflected in the whitepaper.
Section 4.3 "short vs long range attacks" of my new PoS describes the Tendermint trust model as I understood it last we had these discussions.
I met Jae Kwon in San Francisco about a month ago. He was very friendly. We did not discuss Tendermint :)
@_date: 2017-05-16 22:12:44


If I were to replace blocksize with weight as defined in segwit, I could reverse your argument, claiming that ordinary 1Mb bitcoin blocks are giving extra room to "spam" transactions whose resource load isn't reflected by their block capacity, that Satoshi is giving an unfair discount to such transactions, perhaps I would suggest that his company's business plan depends on such transactions in an unstated but implied-nefarious way, etc etc.
Your real argument is that blockweight as defined in segwit does not reflect the resource costs of transactions, but this is not borne out by the evidence, nor is it borne out by looking at the network requirements of witness vs canonical data. But since that claim wouldn't hold up under scrutiny, you disingenuously take it as a background assumption and then complain about its consequences.
This argument has been made, and debunked, hundreds of times here and on rbtc. Please stop repeating it.
@_date: 2015-03-20 16:12:14
I rewrote the document since the original was missing a lot of context and was pretty hard to understand (even by experts who already knew the argument!) I've often seen it mentioned here that the original paper has been debunked, is old, doesn't mention specific system X, etc. Usually "specific system X" was vulnerable to costless simulation, sometimes it would have a distinctly non-Bitcoin trust model, sometimes it'd just be broken, but it would never have a clear argument showing that the authors understood what costless simulation was, had found some flaw in my argument, and had exploited it to solve the problem of cheap distributed consensus.
So I hope I can at least clarify what the problem is, and my argument that it's inevitable, so that people who think I'm wrong can focus their efforts more productively.
Oh, and this also serves as a quick introduction to the "dynamic membership multiparty signature" thing mentioned in Blockstream's sidechains whitepaper. I don't expect it to be "the DMMS paper" but I needed a written-down definition so I put one in.
@_date: 2015-03-23 19:53:01
I like this idea, but it's hard to distinguish between users who are initially bootstrapping and users who are coming online after temporarily being disconnected from the network. I don't want to separate the latter from my notion of distributed consensus --- an "always online, never partitioned" requirement is IMHO too strong for a worldwide system in an adversarial setting.
@_date: 2017-05-08 18:55:13
The distinction is that Bitmain has clear opinions about Bitcoin's direction and a clear willingness to directly influence the system toward these goals. Further, their actions show that they want to hold Bitcoin back in at least one specific way (preventing the activation of segregated witness). Receiving funding from them _is_ a reason for higher scrutiny.
To contrast, AXA to the best of my knowledge has not shown any specific technical interest in Bitcoin or its direction, beyond one of its investment companies putting a bet on the technology through Blockstream. To read this as anything alarming or even interesting requires one to believe false conspiracy theories.
@_date: 2017-05-16 22:33:12


Ok, and in segwit the witness data no longer requires as much bandwidth because it doesn't need to be transmitted just to verify what transactions are on the chain, unlike the actual transaction data which (by definition) does.
@_date: 2017-05-20 17:45:59
Thanks for the replies (from several people). I was hoping for a signed git tag that I could validate with `git tag -v` and build with my usual workflow, but a gitian sig is just as good.
@_date: 2015-03-22 15:27:15
A digital signature: tied to a specific message, only one person can produce one, and anyone can verify that it's correct. Mathematically the signer's identity is represented by a "verification key". (To produce signatures the signer needs a secret "signing key" which is mathematically related to the verification key.)
Bitcoin is a distributed ledger in which all coins are tied to verification keys, or "addresses". The owner of a coin is simply whoever can create signatures which match the verification key of that coin. To send coins to someone else, you use this appropriate key to sign the message "this coin, which previously could only be spent by my verification key, now may only be spent by [pick student's name]'s verification key".
This ledger, which includes all such signed messages in Bitcoin's history, can be verified by anyone on the network, and is copied across everyone's PCs.
@_date: 2017-05-26 18:12:55
Anybody using OP_RETURN for commitments can just use the existing opentimestamps servers, and they would effectively be doing this.
Having direct blockchain support is harder because typically each OP_RETURN is signed by every input in its respective transaction and you have to preserve that, or else there is room for malleability...and then meaningful aggregation would seem to require interaction between the transactors and the miner who did the aggregation.
@_date: 2015-03-24 02:55:38
The three paragraphs starting with "Having said that" in [this crypto.SE answer]( link to the relevant definitions for ring signatures, etc., and how they apply to Bitcoin.
As for learning the algebra, [this article by Dr Stefan Brands]( covers tons of magical things you can do assuming only that the discrete logarithm problem is hard for elliptic curve groups, and that hash functions are indistinguishable from random ones.
@_date: 2017-05-28 13:23:37


Correct, it does not. Every transaction is still self-contained and unmodified by anybody who isn't participating in it. In fact it requires _less_ coordination because now parties who re-sign it can't change the txid for everyone else.
@_date: 2019-05-23 12:57:17
No, no details about the HSM are public, sorry. But you are correct (trust me :}) that it will not sign forks more than 1 block deep.
To be clear - the federation members can't directly create more L-BTC out of thin air than there are backing BTC. This is prevented by consensus. But of course they could collude to spend the backing BTC out from under the L-BTC, which amounts to the same thing.
Regarding joining the federation - the existing federation is limited in scale largely by our use of CHECKMULTISIG rather than Schnorr signatures. In time this will improve. You're welcome to start your own federation if you want (the [elements daemon]( has what you want) and even create a peg into/out of Liquid in the same way Liquid does to Bitcoin.
@_date: 2017-05-02 23:39:34
Yes, we can hack the system that way, provided other members are incapable of sending us a form letter in less than six months time, and that the pledge fails, and that the inventors of the relevant patents fail to intervene.
@_date: 2019-05-08 19:26:53
Please note that while you can pegin yourself you wonâ€™t be able to pegout without some exchange assistance/account.
(For example bitfinex, the rock, sideshiftai, etc)
@_date: 2019-05-24 16:04:52
There is no such "button". If a majority of the federation actively collude to spend the Bitcoin backing L-BTC in violation of the rules of the network, first disabling the many alarms this would trigger in their own software, and rewriting their functionary software to allow it to operate in an insolvent state, they could in principle do so.
But this would amount to a conspiratorial exit scam the likes of which the world has never seen. Users would not continue to use the network, and the network would not continue to operate, in an insolvent state. It is simply not designed to do so.
@_date: 2017-06-22 13:13:18
Please _please_ do not ever use `dumpprivkey`, there is almost never any need for users to be handling raw cryptographic material, especially secret keys in a system that uses tons of ephemeral keys whose loss or exposure directly results in loss of funds.
I wish this RPC was locked behind some "expert mode" flag or something.
Just wait for the sync.
@_date: 2017-06-17 08:39:34


Miners provide timestamping. Users do _everything else_, in particular ensuring that transactions are well-formed, non-inflationary and correctly authorized.


Research and development are certainly not cheap.
@_date: 2017-06-17 11:33:33


Two limits is obviously more complicated than one, it would require 2-dimensional analysis for miners to reason about fees, which is significantly more complicated than the optimal 1D "order by fee/weight, take transactions til you can't" strategy.
Also, being a softfork or not has absolutely nothing to do with the weight limit. It changes the location of some Merkle root (trivial) and adds another case to transaction parsing (which would also be the case with a separate transaction type or however you imagine a HF would work). Neither of these are particularly complicated, in fact they both pale in comparison to the complexity segwit _removes_ from transaction production.
@_date: 2017-06-16 17:51:36


Even on the face of it, segwit doesn't increase the amount of work for miners.


This sounds reasonable, though so early in Bitcoin's history it's hard to argue that hindering the system is in any individual investor's best interest, especially long-term.
I agree that BIP9 seems to have been designed in a "miner naive" model where people weren't expecting miners to act adversarially to the wider network, even if they were given greater technical ability to do so. My guess is that it won't see much use in the future.
@_date: 2016-07-19 21:57:35


This is an interesting perspective -- from your context it's clear you mean this so that Blockstream can deter others from weaponizing IP, but the flip side of this is that any weakening of the conditions allowing Blockstream to sue increases the risk that Blockstream (or some future holder of our patents) can itself weaponize patents.
We actually tightened the conditions under which Blockstream can sue, versus the Twitter IPA that we based this on. Had we loosened them, I (and probably some other engineers) would have expressed strong concern about it.
@_date: 2017-06-16 08:29:56


If you mean that any capacity increase will undercut the fee market, I don't think that a fixed capacity bump like this is much of a long-term concern, especially given the increased value of the network from the technical improvements in segwit.
If you mean more specific things, a lot of work went into designing segwit to avoid hurting anybody's existing business practices, including both open calls for review and specific contact with major ecosystem players. Unfortunately the covert AB stuff was not disclosed at this time, and wasn't discovered by the rest of the community until much later in the game, after most of the politicking was in full force.
@_date: 2017-09-26 15:08:42


I've seen comments to this effect here and on the other sub, as well as more directly "developers will follow the money". I can tell you from my experience working on Bitcoin with Bitcoin developers that this is very far from the truth. If they were really so single-mindedly interested in money, and they were willing to deal with the constant barrage of shit thrown at them in this space, they'd go do some scam ICO and not bother with the heavy lifting of responsible software development. Or if they decided that constant bullshit wasn't really their thing, they'd take one of the many open job offers that they typically have, by virtue of being skilled developers on a high-profile open source project. 
But by and large, they don't do these things, and from talking to many of them it's clear that they're excited by what Bitcoin can be and they want to push it forward because that will change the world in a way they want to see it changed. Many of us were working on this project before finding anyone who would pay them to do it, in whatever spare time they had. For a long time nobody was paying for Core development at all, and it is still the case that almost nobody in this space pays for Core development despite their businesses depending critically on this work getting done. The situation has gotten much better in recent years but the fact remains that being a Core developer was never a way to get rich, nor was anybody doing it because they needed the money.
Having said that, in practice this industry is incredibly skewed in favor of employees, because there aren't many Bitcoin experts and there's a lot of hype around it. I personally started working on Bitcoin for free instead of working on my PhD. Now I work on Bitcoin professionally, and my Bitcoin work hasn't changed one bit (except that I'm able to fly aronud and physically meet other developers more often). My contract has an explicit clause that ensures nothing related to Core or libsecp (or several other things or "any non-competing open source projects") is **not** IP-assigned to Blockstream by default. If I didn't have these conditions I wouldn't have joined the company, and if they were to ever pressure me to do act against Bitcoin (say, write supporting code to recklessly change constants with no spec or scope and an impossibly tight timeline because some room of businesspeople had decided I should do that) I would laugh and walk away.
And even if the employment situation weren't so rosy, there are several prominent developers who are independently wealthy and don't even need employment (some are even employing other Bitcoin developers).
Finally, if somehow every user of Bitcoin decided that they wanted to go some other direction and that they didn't care at all about the goals that drive me to work on the project, I would move on to something else (say, biopunk) where my efforts would be useful. Like everything else I've mentioned, this is not just my personal feeling but something that is shared by many people working on the Bitcoin project.
My point is that if developers aren't selling out *now*, and there are a ton of reasons they're not, it's an implausible claim that they'd sell out to join some project which has spit on them for years. It's much more likely they'd just move to some other industry, but this has been an option all along and it doesn't seem like a very popular one. Nobody is forced to be here and nobody will be forced to be anywhere else.


And yet, they are extremely behind at even this. (Also, without the prodding of some Core developers they wouldn't have even changed the right integer.)
**Edit:** added important "not" :)