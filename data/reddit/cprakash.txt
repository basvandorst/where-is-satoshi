@_author: cprakash
@_date: 2014-02-24 00:40:47
(Disclaimer : MaidSafe employee)
Multiple copies of data are stored on network.
There are decentralised group of nodes around a node monitoring its connection. As soon as a node is detected to be offline these managers inform another group of nodes whose responsibility is to replicate data. These managers of data also ensures integrity of data all the time. So if the harddisk corrupts, replication will be triggered anyway. At any time network keeps 4 or more copies of the data chunk. The network is built with assumption of nodes being switched on and off. 
@_date: 2014-02-24 02:15:05
Similar concern is addressed in below comments
 
@_date: 2014-02-24 02:19:23
It's an acronym; "Massive Array of Internet Disks, Secure Access For Everyone"
@_date: 2014-02-24 10:32:33
note papers in sidebar)  More detail 
@_date: 2014-02-24 01:13:38
(Disclaimer MaidSafe employee) I must say one thing in common is "Your password never leaves your computer".
Data on Waula can still be deleted on the server. But in case of maidsafe, its very hard to delete someone else' data. Even if someone manages a successful attempt to issue a valid delete request (very difficult I think), this will result in the data being read only. This means data will belong to nobody and stay on network forever.
Hope it helps.
@_date: 2014-02-24 02:25:59
Have a look at this blog 
@_date: 2014-02-24 03:40:02
(note papers in sidebar)  More detail 
@_date: 2014-02-24 02:59:30
[disclaimer: MaidSafe employee]


All data chunks are almost treated uniformly on network. Giving special treatment to certain data will leak information that it is important. Yes the replication is dynamic. 
Notes from faq:
"How does the network deal with data redundancy, to ensure whatever data is shared remains accessible?
Each file is encrypted and split into chunks during our encryption process (Self Encryption). The network keeps and maintains 4 copies of each encrypted chunk and moves these fragments around the network as nodes become unavailable, either through failure or power down. In order to cope with this churn, the network is able to reconfigure globally extremely quickly (possibly as fast as 20 milliseconds). These steps ensure data is always available, provided the user retrieving the data has an Internet connection and the correct credentials.
" 


To store that you would have given your share of resources, so even if you don't access your data for long it will stay.
The case is different when you pull out resources as well. We do have provision for deleting very old data, but some of us think if we use the system fairly we don't need to do that. Will be good to know what others think.


Very valid question. Initial days this will be an issue for sure. But once network grows, it will be difficult to do this. As network distribute data randomly, even on availability of huge pool, it will take time to fill up those pool. We hope this time will be enough for network to grow to a safer state. Initially to avoid this we will run good number of independent nodes and pull them out once network balances. Contribution from different communities will play a great role in bootstrapping the network.
Hope my answer helps.
@_date: 2014-02-24 03:25:02
The group of nodes near (XOR distance) a given user node's (512 bit) address or data chunk's address (512 bit) in the network is referred as Managers. These nodes are responsible for certain actions based on the events happening around. In short all nodes have multiple personas and they act differently on different events.
Say if they receive a request to store data, we refer them as Data Manager persona. Task of this persona is to ensure sufficient copies of valid data chunk available all the times. These nodes pick up random nodes and store the data chunk on them.
Pmid Managers are referred as the group of nodes around the node which actually stored the data. These node have live connection with the data storing node. These nodes monitors the connection with the it. On any connection failure, the report data managers about the loss to have them sort out the replication if required.
Hope it gives you some hint about what we call Managers.