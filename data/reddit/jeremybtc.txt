@_author: jeremybtc
@_date: 2019-11-13 00:18:58
Do you have a better name suggestion?
@_date: 2019-03-24 21:37:16
Appreciate the sentiment.
I was happy to give details btw on the specific things that I found frustrating while attempting to contribute, just not the names of particular contributors who I had felt 'dropped out' due to being frustrated (a set larger than just myself) that have privately shared their sentiment with me.
PR14387 was definitely frustrating to me, but not particularly the reason I stopped frequently contributing to Core. I felt to me that you were out of line telling other developers to not spend time on it -- I thought it was an interesting space of improvements because it allows us to clean up the block validation code quite a lot which I felt at the time could be a valuable contribution. I was very appreciative that you did take the time to clarify a framework for falsifiable demonstrating some benefit. I have a branch floating around somewhere with WIP on the full refactoring I was envisioning, but I haven't touched it recently because I'm a bit worn down on major core contributions in general.
A lot of the discussion on John's style issue I think rings true. I hated that every time I would open a PR, I would get a litany of random failures and nits -- either from Travis bugs (failures not even in my code), appveyor enforced things, random new linters that got added, no auto-formatting tools, new naming conventions (do I name it m_count something or nCount or count etc), frequent rebasing (with nagbots), etc. I think abandoning style guidelines is sad -- I think we'd be better off to do John's Big Cleanup and then auto-enforce with tools that can reformat for you -- but I'd rather stop caring about style things than have style be an ever-changing goalpost.
But structurally, you're right to point that the broader issue is a lack of guidance on the project on what is a welcome v.s. unwelcome contribution. I don't want to fight with people about what should be done or not. I want to work on solving interesting problems that help the project. But it seems by default everything is an uphill battle to improve, and there is a lot of 'behind the scenes' ownership where people feel that they will be the one to solve some problem, and don't welcome outside solutions because they've worked it out already and just need to code it up but have other things that are higher priority right now or are demoralized on working on that issue currently. (I'm guilty of doing this at least once, and have suffered from it multiple times at the hands of others). Better delegating of such things would improve our contributor parallelism.
Another issue as a result of the structurelessness of the project is that we're always toeing a fine line on large PRs and small ones. Large PRs are crappy because you end up having to rebase them again and again (rebase hell), and they may be large because a good portion of it is non-automatable refactoring making the rebasing more tedious. They are also difficult to review. Small PRs are crappy because they only 'checkpoint' the work you're trying to do, and may not fully demonstrate the need for the change you're putting in, but are easier to review for correctness. Finding the balance on this is something that feels like it's a rapidly moving target and is relatively hard to predict. More thorough project management would help contributors advocate for the smaller, easier to review chunks so that progress gets made over time, with knowledge that 'someone' has a longer term view on where this can take the project. As a downside, if the work gets abandoned, the code ends up being a bit more fragmented (e.g., libconsensus).
Anyways, I recognize the above is a bit rambly, happy to provide more detail or example where useful.
@_date: 2017-08-25 16:52:07
Can accessing in non-segwit mode access segwit utxos?
@_date: 2017-08-26 19:09:13
post another one, no good experiment is done with one data point :p
@_date: 2017-08-26 18:58:23
Very steam punk.
I'd wear one if it's waterproof.
You know, because of the steam.
@_date: 2017-08-25 21:09:01
Bitcoin transactions per second can increase via on chain and trustless off chain methods.
@_date: 2017-08-26 19:01:44
Why would anyone want to stop it?
@_date: 2017-08-26 19:12:37
This is hilarious. Combine with  for tracking!
@_date: 2017-08-25 16:50:11
Nice work from [Justin Camarena]( who created it!
@_date: 2017-08-26 19:06:53
The chain with the most valid work, of course. Valid means that the block is correct.
@_date: 2017-08-26 19:08:17
Missing RBF txns with low fee to start and bumpfee if you get tired of waiting.
@_date: 2017-08-26 19:05:09
Cool that you got the coin back! nice guide!
@_date: 2017-08-25 17:54:04
How many UTXOs do you have?
@_date: 2017-08-25 19:41:56
Pieter Wuille never skips legs day.
@_date: 2017-08-26 19:02:31
other than like maybe the "big banks"
@_date: 2019-10-05 19:25:10
This is closer to being accurate.
In a tree version the overall penalty is only log(n) though, so you aren't paying for the other half you're paying for log(n), and that's worst case -- the average case is O(1) more work. Log n grows pretty slowly so we're not talking about a huge number. Log4(100000) is 8. This means 1700 extra bytes maximum in extra transactions, of which, all can be pruned at the storage layer with no penalty to validatability (as they are deterministically computable from the leaf nodes).  That's 0.04% the size of 100000 outputs.
@_date: 2019-10-04 21:43:30
Correct. You need to maintain it to get paid (or, one of your siblings in the tx-tree does).
By default though, a Bitcoin Core wallet should keep these transactions in the wallet (regardless of mempool behavior) because you can see that it's paying you.
Depending on the agreement you have with the payer, they may also have an obligation to retain that record.
@_date: 2019-10-04 21:07:47
Yes and No.
It has nothing to do with Segwit and can't take advantage of the witness space really.
However, the "interior nodes" of the transaction tree can be computed deterministically from the leaf nodes, and can be pruned from the node without losing any re-validation ability.
@_date: 2019-10-07 23:40:54
1. It permits the shifting of this burden. Exchanges can still offer the same amount of fee as they were willing to before. Fees don't have to be purely CPFP driven. Further, OP\_SECURETHEBAG serves to lock in the transaction earlier than it could be otherwise; so it should be strictly a bit better. Last, you already make the point that exchanges are already overcharging here -- OP\_SECURETHEBAG should decrease exchanges need to overcharge because they don't need to overcompensate for fee variance.
2. No I don't think what you're saying can happen.
3. That would be a potentially good thing were it to happen, because it would mean this is meeting exchanges needs in a simpler way. I would bet if anything though, this would increase lightning adoption because OP\_SECURETHEBAG makes it simpler to open channels and makes it easier for channels to support more HTLCs than present.
@_date: 2019-10-05 15:46:15
This is completely inaccurate, recommend you re read the article
@_date: 2019-06-13 23:41:16
I'll toss in my own project here: I've been working on  on and off for the last couple years without support.
I've built the initial prototypes, simulations, and tests but I'd love to have funding to help me to do the following:
1) Put myself 'on the clock' to work on it more dedicatedly rather than here or there
2) Design &amp; prototype wallet code for users handling it in Bitcoin Core (or other popular wallet)
3) Design &amp; prototype a decentralized mining pool based on it
4) Design &amp; prototype software for exchanges to batch their payments using it
5) Spend more time advocating for its inclusion in the next soft fork 
6) Produce more rigorous simulations to show the affect on experienced feerates
I think that it fits within the initial bounty purpose.
Addendum: As a former Scaling Bitcoin PC member, I think that it would be a good idea to let a neutral body decide, but I also think it's fine for you to just pick. Ideally we'd see more Bitcoiners giving out grants like this :)
@_date: 2019-10-05 19:32:31
You can think of it this way maybe: yes, someone can freeload and wait for someone else to pay. But the amount of cost to any user is small and everyone benefits from the freeloaders/waiters because they are postponing their transactions anyways, decongesting the network
@_date: 2017-04-10 02:44:00
It's a good point you make, but they aren't mutually exclusive, and this is a Pro/Con. Getting rid of ASICBOOST does certainly advantage and disadvantage (from status quo) certain actors.
@_date: 2017-04-08 15:14:17


This is not my understanding of how ASICBOOST works. When you load up your miner with N Merkle Root collisions (e.g., enabling ASICBOOST), you are able to compute N times more hashes per second. I don't see how this would be just an efficiency gain rather than a performance gain, unless you concurrently decreased your hash rate.
You can see my notes on the subject here: 
@_date: 2017-05-24 15:27:57
I didn't get the memo, I guess I'll have to work on it!
-- Jeremy Rubin