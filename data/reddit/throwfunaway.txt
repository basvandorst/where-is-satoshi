@_author: throwfunaway
@_date: 2016-11-05 06:38:40


    
Why redundant? Unless all of the logic paths in CheckBlock return in the above lines 100% of the time, I'd say that code is very much still executed and still very much affects behaviour!


Aha! And THAT is precisely why I'm saying it's ~~disingenuous~~ inaccurate for you to say that the block size limit is increasing. It's not. It's comparing the block size to a limit that has not changed since put in place by Satoshi and continues to be in place!
@_date: 2016-11-05 06:09:46




Check this line out in the method CheckBlock:






So it looks like if the number of transactions in the block's vector array is 0 or greater than 1000000, or **if the serialized size of the block (in bytes), is greater than 1000000, then the block is rejected for failing the size limit test**, and that's the code. No MAX_BLOCK_SERIALIZED_SIZE to be found here. How odd!


OK, first of all, it's code (C++), also while coders could name these variables arbitrary names like COOKIE_MONSTER_LOVES_COOKIES, they typically use variable names that give a clear explanation of what they're doing, and you can verify this by looking at the code. All of these static const variables (MAX_BLOCK_BASE_SIZE, MAX_BLOCK_SERIALIZED_SIZE, MAX_BLOCK_WEIGHT) are in consensus/consensus.h, so claiming it's non-consensus-critical code is just flat out ridiculous so I'm really not sure what you're saying here.
@_date: 2016-11-05 02:12:53
I'm sorry but that's like saying reducing the weight/size of the clowns in the clown car is a clown car size increase. Reality is while you can fit more clowns in the car if they're at reduced weight, you have NOT increased the car size. Also you can only technically reduce the weight of a clown so much. Please stop spreading misconceptions.
@_date: 2016-11-05 08:22:03


Sure, but CheckBlock is the trivial rejection before ContextualCheckBlock is even executed! They're both "real" checks. Seriously, have you written code?


Okay, seriously? You're being needlessly stubborn and not constructive with your arguments and I'm not sure what to say that I haven't already said, so I'm not continuing this. I encourage you to continue to learn how to code if that's your thing, it's a great skill and train of thought. But please leave Bitcoin to the adults :)
@_date: 2016-11-17 08:41:34
@_date: 2016-11-05 03:36:01
Sorry but I'm not convinced. Kindly explain how Segwit allows blocks on the Bitcoin Blockchain to be greater than the current limit. Keep in mind I'm referring to the block size limit and NOT it's "effective" limit due to clever ways of optimizing transaction data. Are you actually saying that miners can add blocks &gt; 1MB on the Blockchain as a result of Segwit? Because I didn't see that anywhere in the code.
@_date: 2016-11-05 07:51:20


Are we looking at the same code?   
Yes if the test fails on the Size limit, the function will return false without executing the code below that, which makes sense, because it's a trivial rejection (ie if size test fails, no need to test other cases). I beg to disagree with your claim that it's "impossible for the rest of CheckBlock to succeed even if this test is removed. You see, line 3451 returns true and it can and likely will execute right to the end. Just curious, do you write code? I'm hoping my comments are helping to explain, but then again you're the "Bitcoin Expert" as I see in your flair.


Yes, and if you read what I actually said, blocks with size &gt; **MAX_BLOCK_BASE_SIZE** (1MB), are rejected because of the check in CheckBlock. Whatever clever tricks being doing here (by stripping out half the data or whatever) with segwit is beside the point. The fact is, size of the block is being compared to the same damn limit of 1000000 bytes, and that hasn't changed.
@_date: 2016-11-05 04:31:32




Unless my eyes are deceiving me that static integer is set to 1000000 (1MB), not 2000000 (2MB) or 4000000 (4MB). I'd say that's as real as it gets, wouldn't you agree?