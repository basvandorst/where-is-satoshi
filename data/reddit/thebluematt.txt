@_author: thebluematt
@_date: 2016-08-09 04:41:30
There is some limited funding to help cover travel and cost of living for those who need it.
@_date: 2016-08-08 23:30:35
While this may be true for businesses that are now forced to KYC, luckily developers are just fine under the bitlicense regs.
@_date: 2016-04-18 06:42:36
Yea, sorry for the delay...been travelling so won't be able to upgrade for another ~24 hours.
@_date: 2019-07-27 00:30:12
They're both very similar in feature set, in fact, stratum v2 is the result of some good feedback from the  folks on Betterhash plus both of us working together towards a common goal. Its more of a Betterhash v1.1 than something different, plus with a better (read: not already taken) name :).
@_date: 2019-07-27 00:30:57
Its still a ways off. While the folks have been working on an implementation, the spec writing is still rather in its infancy.
@_date: 2019-07-27 00:30:23
They're  both very similar in feature set, in fact, stratum v2 is the result of  some good feedback from the  folks on Betterhash plus both of  us working together towards a common goal. Its more of a Betterhash v1.1  than something different, plus with a better (read: not already taken)  name :).
@_date: 2019-07-27 01:58:24
Dunno about stuff, but the spec is still much too unfinished to be looking seriously (beyond playing around) at implementing it.
@_date: 2015-12-13 19:07:53
Note, also, that some of the propogation delays are related to compression failures on the relay network (ie weak blocks/IBLT/etc would also fail to compress those blocks).
@_date: 2015-12-16 00:13:30
Responded there.
@_date: 2015-12-15 23:00:48
I would love to be able to set it so that you can't mine without being required to set some policy flags, but this is in direct competetion with our much stronger desire to make it as easy as possible to solo mine (or mine on p2pool). If you have a clever way to resolve the tension between those, I'm all ears :).
@_date: 2015-12-18 07:09:08
@_date: 2015-12-13 18:55:18
Its useful to note which are already implemented. Pruning was implemented in Bitcoin Core in a released version months ago. The validation time increase you're referring to for transaction pre-forwarding (you called this "Thin Blocks", though, note there are much more effecient encodings than Mike's thin blocks patch) has been in Bitcoin Core for years (though it will be somewhat more effecient in 0.12, largely bringing its effeciency back to what it was before the spam that has been hanging around in mempools for months).
I'd also be careful talking about serialized block broadcast on the P2P network. For most of the other improvements, we have a reasonable idea of how to implement it (or did so some time ago) and have a reasonable understanding of the improvements it provides. For serialized block broadcast, there are some ideas of how to do it (and certainly some knowledge of which neighbors have what bandwidth profile is required for it to not be a net-negative), but playing games with networking like that is really, really tricky. It would be incredibly easy to implement some kind of serialized block broadcasting which ends up a net-negative for block propagation.
@_date: 2015-12-14 07:49:00
Of course there will be a much more full-fleshed solution before the option is fully removed, but, due to major mempool rewrites to make both mempool and block creation much faster and make memory usage much, much more effecient, keeping everything working exactly as they had been working wasn't an option. Thus, the priority space was marked deprecated, both because many miners already turn it off and because it is easy to replace if you want it.
@_date: 2015-12-14 10:03:02
I'm assuming the validation time increase you claim from thin blocks is actually the filling of signature and UTXO caches. Note that I didnt claim filling those caches isnt important, but its somethinve we've had in Bitcoin Core for years. Decreasing relayed data will also speed up broadcast time, I was only talking about validation time here.
@_date: 2015-12-13 23:31:34
I wouldnt say "we have all these ways of speeding things up"...Go look again, there is only one thing in your list which improves validation time which is not in any released version - libsecp256k1, and for VPSes with oversold I/O capacity, it may not help much at all (Bitcoin Core 0.12 has some big improvements for I/O during validation, which should help as well, however).
As for "why aren't the pools doing anything about it", you should go ask them.
@_date: 2015-12-14 22:01:32
Though I agree many miners today do simply use the default policies set by Bitcoin Core, I dont agree they are neccessarily the most qualified to do so. I'm not sure major miners neccessarily are, either, but certainly developers who work on one specific project are not the most qualified across the entire ecosystem.
@_date: 2015-12-14 10:19:50
Ugh, the idea that Bitcoin Core should be setting default policy is incredibly annoying. Yes, there should be a little python utility that makes it easier to customize policy to your liking (miners can pick whatever they think is in their best interest), and we can absolutely make that happen, but the additional code complexity in security-critical sections caused of the current API is completely untennable.
@_date: 2015-12-14 05:32:41
The change was implemented as a way to notify users that the old method of calculating priority is deprecated and will be removed in a subsequent version. This was done because, over the years, sorting the mempool twice for two different methods of sorting has added a ton of code and complication to block generation methods and mempool management.
That said, there has been a lot of discussion about how to replace it, and there are a few options. One is to incorporate the old concept of priority (as bitcoin-days-destroyed) into one overall sorting order (ie instead of sorting transactions by feerate alone, sort by feerate and priority combined in some multiple). Another option is to use the API that has been around for quite some time that allows people to write external daemons which scan the mempool and manually add an adjustment to the effective feerate (for mining and mempool sorting) to any transactions which they want to prioritize.
@_date: 2015-06-28 18:17:29
@_date: 2015-12-16 00:13:28
Indeed, having something that asks "How much $LOCAL_CURRENCY are you willing to forgo in order to give long-time Bitcoin users free transactions?" would be nice (patches welcome...in this case we should probably build a big fancy intro-to-mining GUI, but, wow that is a lot of work). It does all end up being a development resources question, in the end, sadly. 
@_date: 2015-12-14 06:35:17
There is, of course, no intent to make anyone do anything manually, thats insane! Writing a daemon to implement any policy you want (whether its bitcoin-days-destroyed, or any more custom policy), however, takes only a few lines of python (literally under 10 if you just want to use bitcoin-days-destroyed).
@_date: 2015-12-14 09:39:23
Because when the block has been uploaded to the peer is impossible to determine! Remember you have no way to know which packets you send on the wire have or have not been dropped. What's worse, APIs to see how much data is still in the TCP send queue are really hard to deal with, and are incredibly platform-specific.
Assuming you use a userspace TCP implementation (I dont think any good ones exist, but lets assume we're using UDP with forward error correction, and implement our own rate limiting ala TCP) so that you do have good visibility into the state of the TCP socket, you still have some major issues to solve. One would probably assume that once you finish sending the last batch of packets the other side will have received most of them (actually a rather shaky assumption, but I'll avoid diving toooo deep into congestion control), allowing you to just send a few stragglers while uploading the block to the next peer. Note how complicated we already are, and this is assuming your peers are all honest!
Of course what if a few of your peers are maliciously trying to keep your block from being relayed? The remote peer can act absolutely normal, until you start sending a block, and then stop ACKing your packets, making you delay sending the block to further peers, potentially ruining any advantage you had gained in the first place :(.
A much simpler proposal that has been made in the past is to try to guess your upstream bandwidth to individual peers (I assume by doing a test at some point while your other sockets are not under heavy use). You would then simply assume that, if you fill the outbound sockets will blocks at that rate, you will sufficiently use your upstream bandwidth without flooding it. This is a much simpler solution, but also opens you up to similar attacks which could end up making your blocks propogate slower than the current state :(.
I'm not trying to claim it is absolutely impossible to solve this problem, but instead that it appears incredibly, incredibly difficult to solve without opening you up to any number of attacks and risk ending up with something that broadcasts blocks slower than the original, naiive solution :(. Note that there are completely different proposals which may actually accomplish similar goals without this insane blowup in complexity.
@_date: 2015-12-16 00:27:07
Great! Always love seeing more contributors to various Bitcoin Projects.
@_date: 2015-12-19 20:24:08
500Kbps effective bandwidth across the network, not on a single peer. But, yes, its right (and measured in several different ways)
@_date: 2015-12-07 04:57:38
Feel free to ask questions to be asked live at  on channel 
@_date: 2015-12-16 00:22:01
In all honesty, mining code in Bitcoin Core generally doesn't get that much love. As an open source project, what gets implemented is driven largely by what is either horrendously broken, or what is "fun" to implement for the individual committers :(. This sucks, but without a driving project managment team, its the way things are. There have long been discussions about improving the levers miners are given to play with so that they are more "leyman-compatible", but no one every got around to implementing those. Even if they were implemented, putting them in Bitcoin Core in a security-critical codepath means someone has to commit to maintaining them (hence why I prefer an external-daemon approach to the problem).
All that said, I wouln't neccessarily disagree with a push to disable mining until such an external daemon were registered and providing policy feedback to Bitcoin Core. Though some things in the mining ecosystem need to change first. I think mining is clearly already far too complicated - we see nearly all the miners with a few PH or a few hundred TH simply pointing all their hashrate at a pool instead of at their own local bitcoind. I would think there is a nice market for a really nice GUI solution that spins up a bitcoind, sets policy and runs a lightweight pool server (and I think there is a company or two trying, but evidently not making great progress).
@_date: 2015-12-21 23:45:35
I'm referring to measures of the time it takes to propogate a block from one point to many other locations around the globe. Indeed, bandwidth is a strange term for this, but I don't know what else to call it.
It has been measured by looking at the times pools announce new blocks on stratum to measure the time from the first pool which found the block to the time most other reasonable pools have started working on it. I also found similar numbers independantly when I looked at the lower bound on the time it takes a block to propogate from the first Bitcoin Relay Network node to the last one to receive it (which is largely a measure of TCP throughput through a network of well-placed nodes).
@_date: 2015-12-13 23:27:44
There are various proposals to do UTXO commitments in Bitcoin. Sadly, most all of them get rather expensive rather fast (what is acceptable? double the cost of validation of a block? 10x in the worst case?). If we assume we have UTXO commitments in place, requiring that transactions which spend UTXOs older than X (note that this is not related to reorgs of depth X) provide a proof they are spendable is a possibility (see a recent ML thread), but, again, adds overhead. Since the current size of the UTXO set on disk is somewhere around 1.2G, I think we've got a while before thats really a problem.
@_date: 2015-12-03 03:07:30
Seems to be fixed now - apt was braindead in resolving deps and the addition of two versions of boost in the same distro confused apt.
@_date: 2015-06-23 04:46:26
I know it can be done a bit quicker (it should be pre-hashing transactions so only the merkle tree need be built at relay-time - though that isnt gonna win you everything, the merkle tree is gonna take some time to build on 1/2 core VPSes, not to mention if you want to support sending blocks out over bitcoin p2p you have to run a full double-hash over the whole thing, which is going to take multiple 10s of milliseconds no matter what you do).
Still, this is far from my only conern. I've raised my concerns with increasing the block size in the immidiate future before on the mailing list, and others have been far more vocal than me. My point in responding here is that relay-compression protocols are not free, they can take 10s of milliseconds to compress a reasonably small block, and you start butting up against your speed-of-light latency real quick.
@_date: 2015-06-23 00:13:33
Hmm? I've been actively avoiding reading content from this debate because its a waste of my time. Was there somewhere I should have responded? In any case, the relay network as it stands today will fail miserably if you increase the block size much. Time-to-compress-block right now is on the order of 40ms for a host with a fast intel processor, and its nearly all dominated by just building the block's merkle tree to check nodes arent ticking the transaction-duplication bug in Bitcoin. There isnt really a good way to optimize this significantly (though it could be built while the block is being read from the network, which would help, but when blocks are just a few packets anyway, it wont help much). Once you're taking 300ms to check the block is sane before you can relay it, the relay network's fancy compression stuff starts to become entirely worthless. :(
Check out  for stats.
@_date: 2015-12-19 02:58:34
I havent actually looked at your model, but as for numbers to plug in for real-world usage today: measured total effective bandwidth across the network (between only miners) is around 500Kbps. According to some large miners, if their orphan rates are over 0.1-0.5%, they will do things which are potentially incredibly centralizing in order to get them back down.
@_date: 2015-12-13 19:06:46


The crappy propogation times we see aren't because the relay network is ineffecient, and also not because miners are falling back on the p2p network for propogation much at all, but because various pool servers are very ineffecient and because validation time on VPS or otherwise slow hardware can be very high. The statements as to the relay network's reliability guarantees are largely related to orphans and reorgs. In practice, most miners (ie the only people for whom propogation time is critically important) get their blocks first from the relay network or other private, compressed, peering agreements.
@_date: 2015-12-03 03:07:26
Seems to be fixed now - apt was braindead in resolving deps and the addition of two versions of boost in the same distro confused apt.
@_date: 2015-12-13 23:19:42
You claim


without providing any such "basic math". Note that there is really no half-decent way to serialize sending data over the network. Since delays are due largely to packet loss at a router you have no control over (or visibilty into), you cannot know how fast you should be transmitting in order to "serialize".
@_date: 2015-12-16 00:23:51
See-also my comment at  , specifically:


@_date: 2015-12-14 05:19:28
Though turing completeness probably isnt very useful in a decentralized blockchain, compiler-targetable potentially is (ie write code in any language you want, as long as its compileable, and run it in a bitcoin script!). Of course you're extremely limited in both storage and computation (think size of around 10k, if you're willing to pay enough in fees, and processing time similar to running only a few signature checks). If none of this matters to your use-case, then putting your use-case on a decentralized blockchain could be accomplished by putting any number of simple instruction sets in the script engine. In Bitcoin, an ideal way to do this is to introduce a new script type (eg P2SH or a segwit with version byte 3 instead of 1 or 2) as it is a trivial softfork.
@_date: 2015-12-05 03:50:18
You should try to upgrade normally first, but, indeed, if apt complains about it being braindead and not knowing how to upgrade, try the above apt line and it should work.
@_date: 2015-12-14 06:29:19
There absolutely is. The prioritizetransaction API call has been in place for a long time.
@_date: 2015-12-14 09:15:37


I wouldn't claim this will cause problems, mostly because there are some pools which will absolutely continue to set this value to something much higher (eg Eligius), and there are already several pools which disable this option (more to come, I'm sure, after the next halving).
I'm not sure why you call out 0-conf transactions here, as many 0-conf acceptors already require a fee sufficient to get into the blockchain since relying on priority metric to provide enough security for 0-conf transactions is much less secure than requiring a reasonable fee. In fact, just as Bitcoin Core code for transaction sorting will become much simpler in the future when the option is removed fully, 0-conf acceptors who do risk scoring based on their estimated mempool order to miners will also be simplified :).
@_date: 2015-12-15 22:57:36
The complexity arises largely not from the existince of bitcoin-days-destroyed as a second metric, but from the way the old priority interface (the now-deprecated one) works. Instead of combining priority and feerate into some magic transaction order, the old API picks the top N bytes of transactions based on priority and then fills the remainder of the block with transactions based on feerate.
When I talk about implementing this behavior by using an external script, I'm talking about creating a simple script which just calls getrawmempool, sorts them based on priority and then calls the existing effective-feerate-adjustment API to set the top N bytes of transactions to have an effective feerate of 50BTC/byte or something very high. Of course we absoluely could do this in Bitcoin Core itself, and this is what I was referring to in  when I mention incorporating bitcoin-days-destroyed into the overall sorting order. The problem with that is that there is no easy answer to what that combination function should look like. Because there is no clear answer to what we should set the default to, it is much better to let miners pick themselves by providing an extensible API and some sample code to let them come up with their own policies ranging from "Bitcoin days destroyed with a limit of X until I've given up Y BTC in fees, then sort by feerate" to "Effectively bump feerate by Z if you solve a captcha on my website and put in your txid".
@_date: 2014-12-16 04:36:23
I would have gone with "Umm.....WTF IS WRONG WITH THESE GUYS?", not "this is bad"...
@_date: 2014-01-03 09:38:51
Now that I actually launched it, some of you noticed the prices dropped a bit, I'm still debating dropping more, but here's the post-launch URL: coingen.io
@_date: 2014-01-02 20:48:46
Heh, I linked this to people to get early tests since its still very early beta (the pricing is far from fixed) and the features aren't complete yet...
@_date: 2014-01-03 09:37:43
This was previously at 
but that was a temporary URL before I launched it (damn leakers...), but its now in beta and here's the real URL.
@_date: 2014-10-25 07:10:47
The comments in this paper are far from wrong, though I disagree with the perceived significance presented here. An alternative way of thinking about the two-way-peg is in the same model under which you might analyze bitcoin itself...namely, while it is decentralized and you can chose to "be your own bank" (or in this case do a two-way-peg yourself), centralized systems will always be at least marginally more efficient (given similar levels of resources invested), and so will be atomic swaps with a market maker.
To be a bit more concrete, any sidechain with reasonable adoption will have sufficient liquidity for a market maker to be able to make some money on arbitrage and, because of the perfect competition in such a space (its all entirely trustless), there is no reason for a sidechain's coins' value to drift much further than only marginally more/less than the fees required to move coins back to Bitcoin.
@_date: 2014-04-13 01:12:38
I'd just like to echo part of the introduction...the way I see it:
Yes, sidechains have a different security model and there is no question that a sidechain with any less than 50% (+margin of safety) of /Bitcoin's/ hashing power is not secure. However, Bitcoin is also not "secure" with a miner/coop having &gt;50%. Thus, assuming you require at least 50% of the Bitcoin hashrate on a sidechain, the difference comes down to what you can do with 50% of mining power. I see two major differences:
1) You dont need to have coins first to exploit (ie double-spend using) your mining power, you just get to steal the coins. This one is pretty important, however if you have 50% of Bitcoin's hashrate you have a pretty large investment in ASICs, or you can use your mining power to mine some coins and use those as attack capital.
2) You don't have to find a target to attack. In the attacking-Bitcoin-with-51% model, you have to find some merchant/exchange/etc which you can double-spend against to make money, you dont have to do this against a sidechain. Note, however, that it is not in any way hard to find such a merchant (especially when you're comparing to a few-day wait to withdraw from a sidechain) today and I don't envision it getting harder as more merchants come online.
@_date: 2014-11-01 00:36:18
It appears the proper checkout link to donate to the BitHub is 
@_date: 2014-12-16 04:42:18
Also, IP addresses are NOT the issue here...Knowing relatively accurately when a transaction was announced gives you good yes/no indication that a node you are passively monitoring is the originator of a transaction.
@_date: 2014-10-22 18:01:48
See-also  which implements at least the listed algorithm
@_date: 2014-10-23 02:17:11
BlueMatt is TheBlueMatt on reddit :)
@_date: 2014-12-16 04:40:33
Its also about open listening connection slots, which is critical for user privacy.
@_date: 2014-04-13 07:11:34
You're pointing out essentially the same issues as are discussed at  ...sidechains are not and will not be secure without at least 51% of BITCOIN's mining power on that chain as well.
@_date: 2014-10-25 07:13:32
The two-way-peg mechanism is a way to move bitcoin (or some other asset) from Bitcoin (the blockchain) to another blockchain and back at a fixed (you can assume 1:1) rate.
ie you can create a blockchain with most any feature which you would previously have needed to create an altcoin to use, and use bitcoin directly.
@_date: 2014-12-16 04:52:06
See-also 
@_date: 2014-10-22 18:18:13
See appendix A in the paper and  .....how would you know  if anyone were testing early sidechains already? :p
@_date: 2015-05-29 20:02:54
Three patches:
1. Was rejected by Bitcoin Core developers as it encourages people to trust random peers they find on the network (which Lighthouse used to? does still?).
2. Also similarly encourages people to accept 0-conf transactions, though I'm not sure people have as strong of objections here.
3. It removes a DNSSeed which deliberately doesnt use live-network-scanning to increase diversity of seed mechanisms, and adds one which was removed after the person running it was found to be actively attacking the network.
@_date: 2014-10-23 07:02:44
I would highly recommend you read 
@_date: 2014-10-22 18:06:57
Alt-chain without altcoin.
@_date: 2015-05-30 21:19:03
I'd like to start by pointing out that, by far, the main patch to Bitcoin Core in -XT is the first, which you did not argue with my point on?
As for the third point, I agree that Jeff's DNSSeed should be improved by updating it more often. That said, I do still think it is valuable to have seeds which are not running live-network-crawlers...diversity is good here (though whether it is too broken to be kept today, I dont care too much either way, still, I'd rather see a push to convince Jeff to update it more often than just remove it).
As for Addy's crawler, it was added to Bitcoin Core and, as a part of commiting the document which you quoted, was removed for violating parts of it.
In any case, Bitcoin Core has no problem finding peers with a broken seed, and, in fact, is designed to not use the DNS seeds as much as possible (these days it really will only use the seeds if it has been ofline for a long period of time or is a brand new node).
@_date: 2014-04-11 19:35:43
I'd like to note that version 0.9.1 is identical to version 0.9.0 if you're using a dynamically linked openssl (ie using the ppa).
@_date: 2016-01-26 21:46:20
Indeed, this resulted in nodes that cannot reorg properly (we've had a bunch of similar bugs over the years) but there is no known block which was ever created which old nodes will not accept (even without the BDB locks limit reset). This doesn't mean that such a block cannot be created, but its difficult to call it a hard fork in the sense of "we've done a hard fork before" when there was no fork that would break clients syncing from scratch.
@_date: 2016-01-15 21:35:30
@_date: 2016-01-16 17:03:31
I plan on writing something like this, but I've got about three posts half-written to publish first. I think turning off the relay network would not "kill Bitcoin", it would increase some miners' orphan rates some, but it certainly wouldnt have a huge impact on users. Still, I'm not turning it off immediately, so don't worry yet :).
@_date: 2016-01-09 20:09:21
I agree with have self-inflicted pain...and we should push those individuals out so that it becomes external pain, because we can deal with that.
@_date: 2015-05-30 21:11:27
A number of reasons...mostly, to get the full security model proposed you need to embed rather large proofs in the blockchain (several KB), and larger blocks makes that much easier.
@_date: 2016-03-26 05:42:51
This was fixed.
@_date: 2016-01-08 02:44:02
Bitcoin uses SHA256-d in a ton of places, its definitely not "changing the crypto"
@_date: 2016-01-09 17:15:02
Define "outside actors". I think we should /make/ anyone who is spending most of their time fanning the flames of discontent an outside actor. 
@_date: 2016-01-29 07:12:42
WOOOO, most consistent rainbow FTW!
@_date: 2016-01-26 21:42:43
Just gonna leave this here... 
@_date: 2016-01-27 01:09:22


Huh? What does this even mean? The only denouncing "Core" can do would be some kind of big sign-on letter, and this is still coming from individuals. This is like arguing "Bitcoin" should denounce something. If you really want people to denounce something, then please organize a sign-on letter complaining...there is no reason "Core" should run its own private sign-on letter to denounce censorship in Bitcoin, but something like that should come from the community at-large, not some small group.


You should go read the proposed additions to the bitcoincore.org site. They make the distinction here pretty clear.
@_date: 2016-01-16 02:30:14
This post was not in any way related to the recent 2MB drama, and was written long before it.
@_date: 2016-01-24 04:09:29
There is (obviously) no good way to measure contribution to a project. Commits can have taken hours to make, or seconds. They can also have huge diffs that took seconds to make (like code moves, copying in external code, etc), huge diffs that took hours and hours to build (like adding new features, rewriting how some part works, etc), tiny diffs that took seconds to make (fix comments, change variable names, etc) or tiny diffs that took hours and hours to make (eg fix some major bug 5 layers away that is caused by a three-character bug in a completely separate part of the codebase).
Indeed, pick any one metric and you can find examples of people for whom the metric is very broken. Take, for example, Mike Hearn in your data...If you look at the number of lines committed that are still in the codebase, he "wrote" a ton as LevelDB was copied into Bitcoin Core's github repo for consensus-consistency reasons. In reality, of course, Mike wrote about 7 lines of non-code changes to .gitignore in that commit. On the other side, if you look at Greg with an eye to either commits or LoC, you miss a ton of work marshalling contributors, interacting with Bitcoin researchers to push forward the science involved, and distributing such knowledge between different groups within the Bitcoin development community (and also with the entire ecosystem).
Not to claim data about contributors isn't useful, but truly the only way to get any reasonable understanding of someone's involvement/contributions to a project is to ask others who are involved.
@_date: 2019-11-21 15:20:07
While I don’t know the private plans or any existing pools, I can say that I plan on providing a sample, robust pool server implementation to lower the overhead in creating a new pool, similar to the code I released implementing betterhash. While it won’t implement any of the “pool” side of things, fully implementing the “bitcoin” side of things in a standard, common way makes it much easier to upgrade things and add some of the more advanced features like future block prediction across more pools.
@_date: 2019-11-29 01:18:54
Right, could do, though over the past several years every three months the certbot emails were highly reliable... this time they were not. In any case it is automated now.
@_date: 2016-06-27 17:39:48
100ms breaks the spped of light (in fiber). Technically you could achieve it if you built out a global network of microwave links, I think, but you'd spend hundreds of millions doing so, sooooo.
@_date: 2019-11-21 15:14:55
Adding to this, the cryptographic authentication can’t be understated. If you’re mining today, how do you know your ISP isn’t silently stealing 1% of your hashpower (hint: you don’t!).
Further, the protocol being simple and extensible makes it easy for pools to provide monitoring for smaller farms, and for larger farms to implement their own infrastructure.
@_date: 2019-11-21 15:49:29
The "benefits" question is largely answered in a different way wrt "incentives for users to upgrade" (after all, those are the benefits for users) at [ . Further, the benefit to the Bitcoin ecosystem of hashers being able to chose their own work means, at a high level, instead of being focused on the centralization of pools in Bitcoin, we get to focus on the decentralization of \*miners\* in Bitcoin. While good numbers are hard to come by, this appears to be orders of magnitude better already today, and hopefully will get better as time goes on.  
As for when it will start being adopted, I'll leave that question for the SlushPool/Braiinz OS folks :)
@_date: 2019-11-21 15:12:07
Not sure about the above paper, but, at a high level it’s largely true that Stratum v2 only does so much about miner privacy. Ultimately, most stratum endpoints are well-known (or can be probed), so it’s pretty obvious that you’re making a stratum (mining) connection. But that’s not the only point of (authenticated) encryption. Today’s mining ecosystem is incredibly vulnerable to a large set of attacks due to how easy it is to MiTM it. I even did a demonstration of a BGP hijack on a Stratum connection a few months ago to show how miners just seamlessly switch over! Authenticating with cryptography fixes this.
Still, the overhead of modern encryption is incredibly, incredibly low, so we don’t anticipate it being a blocker in the slightest.
@_date: 2019-11-28 18:58:23
Right. This was my fault. Apparently letsencrypt changed their email notifications so this refresh was missed. Of course we’re not gonna run a large python program like certbot on the server serving binaries, which makes this somewhat more complicated.
Luckily we recently started hosting DNS ourselves so we can run certbot in DNS mode on a separate machine. That’s done now (which is why it took a while to renew this morning), so hopefully this will not be an issue in the future.
@_date: 2019-11-21 15:44:57
As to your first point, indeed, the Bitcoin model largely assumes that, given a lack of block subsidy, transaction fees will be substantial enough to support a diverse, decentralized set of miners. I don't think anyone really has a crystal ball as to how the fee market will develop, but its critical that it does. At least personally, a key element that I think needs to devlop (in addition to more on-chain demand), the elasticity of demand for block space. In other words, we need more volume of transactions that are willing to sit at the bottom of the mempool for a week before getting a confirmation instead of everyone rushing to get into the next block. I'm optimistic given the ongoing work in second-layer solutions (which often are more than willing to wait for some classes of transactions, for example, channel opening transactions in lightning are often not particularly time-sensitive) can make a dent here, but its also on wallets to give users the options and users to use it.
I'm certainly not concerned about a 51% attack in the immediate future, but not for good reasons (mostly the "goodness" of actors in the space). In the long term, without work like Stratum v2 and miners selecting their own work, I'm incredibly concerned about.
@_date: 2019-11-28 19:03:08
That seems like a doubly bad idea, though mostly from a privacy perspective, I presume.
@_date: 2019-11-29 19:02:25
Cloudflare absolutely can read the data being exchanged. They make super misleading claims about “not having the private key”, but for all intents and purposes they do, and do for some time even if you switch away from them.
There are other DDoS providers who simply pipe traffic through without terminating the SSL connection, in fact nearly every DDoS provider *except* Cloudflare works this way.
@_date: 2019-11-21 15:29:47
The SlushPool folks had been thinking about what features they wanted in a new mining protocol since long before BetterHash, so they had some ideas they really wanted to see. So, while its technically a group-up new protocol, it does share some key similarities - notably (optional) custom work selection, a similar message framing, etc.
That said, it has a number of big departures from BetterHash - the concept of "channels" is new, and represents something that the SlushPool folks have wanted in their operation of a mining pool for years. On the work-selection front, the use of a separate protocol/endpoint to communicate custom work selection is very much required for the proper separation of pool server backends, and was not present in BetterHash. Finally, in a somewhat recent change, Stratum v2 is now AEAD instead of the BetterHash per-message signing, which is definitely much better, but required some extensive back-and-forth to analyze the exact latency this introduces on a pool server with 10s of thousands of connections.
@_date: 2018-10-25 16:19:48
 =D
@_date: 2019-11-21 16:06:31
None of the folks answering questions are large miners, so...the market price of going to buy 12.5 BTC? :P
If you're curious, you may want to look into HUT8's public disclosures (they're a publicly-traded Bitcoin mining company in Canada, so they have to publish lots of statistics on their costs, including cost-to-mine-a-block).
@_date: 2016-06-27 02:53:37
Wait a second...thats faster than the speed of light...somehow I dont think the falcon folks claimed that and Kristov is jumping the gun a little bit.
@_date: 2017-01-10 20:53:54
Heh, no problem...While luke may not spit on you, I might suggest you just download the binaries from bitcoin.org and validate them instead :).
@_date: 2017-08-19 19:30:25
Like any other decentralized group, we can get together and craft a statement that everyone signs off on. A number of active contributors were involved in writing it, and it was discussed at the weekly IRC meeting, so, yea, its the view of the vast majority of active contributors by volume.
@_date: 2017-08-25 23:52:46
As you might have read things escalated rather quickly. This was mentioned off-hand in a very unrelated thread. The emails were not from or to Barry, hence why I'm concerned with the privacy of the individuals.
Edit: I was reminded that while most of the emails were not to or from Barry, some were,
@_date: 2019-11-21 15:53:18
This is \*huge\* for mining centralization. Instead of being focused on the centralization of pools (which is the world we're in today), we can focus on the centralization of actual miners/farm owners! I did a presentation talking about consensus group centralization a few weeks ago, slides linked below, and have a (relatively poor/fairly optimistic) approximation for what the decentralization looks like in that world on slide 4.
Edit: as for your second point about performance - its ...complicated. On the one hand, with a properly-optimized client (with a reasonably good internet connection), it can be \*faster\* than receiving work from the pool (as you can work with the pool to propagate the block as fast as possible to other miners). But pools today have put a lot of work into properly-optimizing their setups, so there's a barrier to entry there. Luckily, Bitcoin Core is already pretty good, and with just a little bit of simple config tuning you can do 95% as good. This is certainly something that we'll need to look into documenting better/more as things move towards production.
@_date: 2019-11-28 18:50:29
Right, this. Not only is Cloudflare known for rather spurious decisions by management, but, given it’s position as man-in-the-middle for some huge portion of the internet, we have to assume they are compromised by various state intelligence agencies. Further, instead of a single server to secure serving binaries people run on systems that control millions of dollars, Cloudflare serves from thousands of servers in hundreds of locations, all of which have SSL private keys sufficient to serve coin-stealing malware.
Cloudflare is absolutely not an option for cryptocurrency binary-serving sites.
@_date: 2019-11-21 15:24:30
This was already answered, largely, at [
@_date: 2017-08-25 21:56:04
Now to be fair I think Barry may have been under a lot of stress that day and lashed out at some folks who took it as a direct threat to their business. I hope Barry goes back and assures them that the decision to support 2x or not is up to them, and isn't being made for them. I commented a bit more at 
@_date: 2017-08-25 23:56:16
They describe and are based on the behavior I called out in 
@_date: 2017-08-25 23:48:42
I absolutely stand by my statement. As I mentioned in the followup tweets, I've asked for permission to share more on the second case, but have not gotten it yet. I believe btcdrak may publish redacted versions when he's near his computer again.
@_date: 2017-08-18 01:14:20
As Jeff himself advocated for several times, this is just the removal of people who haven't been active in the project for years. No need to read too much into such things...
@_date: 2016-11-02 17:16:02
Assuming they were to use S9s, that would be ~ 140000 / 1.323 * 13.5 TH/s = 1428.5 PH/s. The network is just shy of 2 PH. Of course its unlikely they will fill the thing with their current generation, but the upper bound is right around 75% of current network hashrate.
@_date: 2016-12-16 10:57:39
As in life, everything is more complicated than it first appears.
As we all know, empty blocks come from "spy-mining" ie mining based on another pool's stratum headers without having seen anything about the block. Because of the way most pools are architected, this means that you not only get to skip the block-relay-latency, but also the time it takes both your bitcoind, and also the other pool's bitcoind to validate the block.
Sure, if all pools were using FIBRE (even just connecting to the public FIBRE network), the block-relay-latency would be almost 0 in excess of the speed of light (which you obviously have to eat even with spy-mining), but that isn't the whole story.
Some pools are notorious for having super slow-to-validate bitcoinds causing additional latency for not only themselves, but also pools which start spy-mining on top of them (yes, the fact that most pools re-validate their blocks before relay is far from ideal...). Others are notorious for having super fast-to-validate bitcoinds (which isn't all that hard if you have a reasonably-primed mempool/sigcache, but it depends on your setup).
@_date: 2016-12-14 17:30:25


Actually, a major component of what I was proposing was less hub-and-spoke and more multiple-hub-and-spokes. ie in the case of pseudo-centralized wallets like blockchain.info (where you're ultimately trusting blockchain.info to send your transaction into the p2p network), trusting them to make a connection with another wallet's hub and to relay your payment (in a way where they cant rip you off, only refuse to relay), doesnt change your trust model ~at all. For some other wallets it does, but given the user count on wallets where this holds true, it'd still be a major win (not to mention secure, instant (&lt;10 minute) payments...).
@_date: 2016-11-02 17:19:09
Indeed, the 75% estimate assumes no other hashrate goes offline as they become less profitable.
@_date: 2018-06-05 18:17:11


See, eg,  which I didn't find documented anywhere.
@_date: 2018-06-05 13:40:22


Well it's still not ideal if all miners can get screwed out of 24 hours of income if the pool goes under or if there's no other pool a small user can go to if they get kicked off, forcing them to solo mine. That said, both of these are relatively minor concerns and, were everyone using a system like this, I'd actually be pretty ok with there literally only existing one pool!


This is about how miners switch to new work and improve the latency when a block comes in. Specifically, integrating better inside of Bitcoin Core than getblocktemplate allows for should give us some headroom to be a bit more clever about how we push new work.
@_date: 2018-06-05 13:31:50
Exactly! But the users still have to pay out to the pool (and show the pool that when they submit shares).
@_date: 2016-12-02 22:51:52
As a vegan, I appreciate this fact.
@_date: 2016-12-13 21:29:44
There appears to be some confusion, so let me clear things up. First of all, I think I misspoke on the panel, the date on the email was 1/30/2014, so more like 3 years ago. Anyway, Roger's comments are right, he paid for me to fly down to a conference in an attempt to recruit me, and I chatted with a bunch of folks about various topics. Afterwards, as a follow-up, I spent a ton of time designing a payment channel hub-and-spoke network (ie the exact protocol flow to make things safe) including a diagram of messages flowing around the network and sent that over the blockchain.info's developers at the time, it was ignored.
Ultimately, I get it, Bitcoin businesses have always been fighting to keep afloat of technology in this space, so telling them "hey, here's this awesome technology that I'd love to work with y'all to finish implementing, because I think it'll make Bitcoin scale" when they aren't facing scaling issues at that time just isn't a compelling argument (to put it another way, Roger likes to talk about Bitcoin being on fire, well...welcome to Bitcoin...if you've ever worked seriously on a Bitcoin company or especially on protocol development you know that Bitcoin is /always/ on fire, there's always some critical issue(s) to fix ASAP, and doing forward-looking work is a luxury only a few have become able to afford).
The point I was trying to make on the panel is that Roger claiming there is a lack of urgency on scaling issues from folks working on Bitcoin Core is completely bogus...I've been trying to convince companies to use payment channels to move their transactions off-chain ever since I finished up the payment channels code in bitcoinj. While I understand why I've been largely ignored on that front, complaining about how we've done nothing for scaling for years is just false.
@_date: 2016-11-02 17:17:33
Depends on what they fill it with, but if they were to fill it with S9s (unlikely, sure, but its an upper bound), it'd put them over 50% of network hashrate. BitFury is currently at around 10%, and would be closer to 5 if this were to come online with S9s.
@_date: 2018-06-05 18:18:34
Pools today largely work by reducing payout variance for their users by taking all the income and splitting in proportional to users' hashrate. This proposal doesn't change that, it only changes who is doing the actual work of selecting transactions to be added to a block. And, fwiw, due to payout uniqueness, users across pools don't ever work on the same work, nor would they in this proposal.
@_date: 2016-11-14 01:07:06
There is an additional difference here - Sergio deliberately ignored the defense in depth that Blockstream spent a ton of time constructing - even if you disagree with the DPL there are several other options for using Blockstream's patents without needing an explicit license.
@_date: 2016-11-02 17:12:06
Rather frightening that they're announcing a farm that would put them, alone at over 50% of network hashrate, let alone over 33%.
@_date: 2018-06-05 13:50:28
It's similar to p2pool in the way it works for users, but it's much simpler in design (trading off reward-splitting-centralization for simplicity), and hopefully provides an easier "upgrade path" from today's mining farms.
@_date: 2016-11-14 01:03:24
I'll let respond to your specific question, since IANAL and have no idea, but you'll note that we have a third layer of protection here - the IPA, giving the listed inventors the power to protect folks implementing the patent even if the other agreements do not hold (see 
@_date: 2016-12-14 04:03:04
I'm not aware of anyone who implemented the nSequence based ideas. Indeed, the original Bitcoinj implementation had issues with malleability (not theft, but malicious locking). Since then someone went back and changed the locktime tx to using OP_CLTV, which does not have these issues, though I'm not sure how much review that got.
@_date: 2018-06-05 12:40:38
I think you misunderstood how it works. Indeed, as points out, the whole point is you allow the pool to still handle payouts and remove the variance for users, even as users aircraft the transactions being mined!
@_date: 2016-11-02 17:24:49
mmm, hadn't realized the scope of that project yet. Good to hear there is at least one other miner. :/
@_date: 2016-12-14 17:25:16
Sure, but some folks I spoke to had use-cases where this made a lot of sense....and didn't care anyway.
@_date: 2015-08-15 15:10:35
"The other thing to keep in mind is end2end achievable throughput is more important than last mile bandwidth in isolation" &lt;-- this is by far the most important point. I have yet to see any real discussion of real-world bandwidth, only last-mile bandwidth. Last-mile bandwidth is only really relevant if you're talking about a connection to a server nearby or with very low packet loss and low latency.
@_date: 2015-08-11 23:35:47
No, its backwards compatible...Only miners who dont upgrade will be forked off. Keep in mind miners can always opt to take a lower subsidy than the max, thereby destroying coins.
@_date: 2015-07-27 23:46:57
Mostly ETH-Z and UofM.
@_date: 2015-07-27 23:42:21
That page seems to work because there are only a handful of nodes which all publicly state they are mining...Since bitcoin is much more mature you cant trivially derive those stats by connecting to everyone, instead check out   and the many sites that track transactions live.
@_date: 2015-07-30 21:55:42
x-post from github:
I'm rather concerned by this proposal for several reasons.
Firstly, the selection of a growth rate based on bandwidth ignores reality. The speed of block propogation has absolutely nothing to do with bandwidth growth at the edges. Ignoring whether we want to ensure we can support miners running over Tor, most miners run propogation from servers with 100Mbps or 1Gbps connections to backbone networks in well-placed datacenters. Small-size transfers (ie probably any block size that people have been suggesting) between them are limited nearly entirely by latency and packet loss, not the speed of their connection. Sadly, latency is bound by physical laws and packet loss is fundamental to how packet routing works on the Internet. Without fundamentally changing how packet routing works, or advancing speed of transfers to eek out another 50% improvement in latency (ie, run a perfectly-straight vacuum with lasers between your servers). I'm not against picking constants based on engineering reality, but trying to pick a growth rate based on bandwidth increases is, to me, just picking a random number.
Additionally, I'm hugely concerned about any suggestion that includes exponential increases. As mentioned above, many of the limiting factors here do not scale well with improvements in technology. Though I dont want to wade into the debate around when we reach 2MB and when we reach 20MB, I dont want to sign up to a hardfork that grows exponentially, eventually far outpacing engineering limits. This implies that, at some point, we are going to have to agree to softfork a smaller limit in, which I think is the exact opposite of the position we want to be in. The default becomes "allow the system to grow into something centralized" instead of "push some use-cases off-chain into systems which still potentially have decent trustless properties".
@_date: 2015-07-15 03:56:42
@_date: 2017-11-06 22:13:06
It was, though it may have been since taken down. The writeup above is about the full paper, which talks about both Graphene as well as the p2p network restructuring, which they refer to as "Canary". The full paper is available at 
[edit: noted that there are two versions of the paper - one that includes their p2p network restructuring, and one which does not]
@_date: 2019-12-01 17:54:33
I’m not aware of any which are free. But there are resellers that are incredibly cheap, eg buyvm resells psychz for $3/mo, ramnode used to resell voxility (iirc, though now resells the new cloudflare product which does DDoS GRE tunnels without terminating SSL, which I think they only added a few months ago and almost no one uses, and also costs a bunch, though there’s no public pricing for their transit product), or OVH does their own in-house and will sink a lot of traffic.
If you don’t want to terminate SSL on those providers, you can hook them up as GRE tunnels and just pipe traffic through then yourself.
@_date: 2015-08-02 06:27:59
You missed my point. I'm not talking about home connections (though I'd be interested in seeing your results), I can say that in my experience, 1Gbps vs 100Mbps doesnt make much any difference at all. Hell, I dont even see any way it could in theory, unless your VM host is dropping packets for other VMs because of its smaller port.
@_date: 2015-08-16 21:31:23
This is relevant for "the cost of running a full node", though you also have to consider "the cost of running a mining node", which essentially requires that you have non-local connections as well.
@_date: 2015-07-31 07:55:25
Sure, latency doesnt dominate in the same way it does during slow-start, it dominates more in that, since we're talking about 1, 2, or 100MB blocks, even if you didnt lose any packets and everything magically ran at line-speed, latency is, minimum, 1/10th of the total time anyway (for 12MB blocks, its equal). Really what I was referring to, though, is the effect latency has on packet loss. SACK helps a ton, but there's not much getting around at least one RTT to recover lost packets, which, when we're talking about something thats equal to the total wire-line-speed-time, that means a lot (suggestions on making things better welcome at  - code even more so :))
I'm quite the opposite - even when hard forks are much harder than soft forks, forks around contensious issues (which the block size will be, even as a soft fork) are always hard. If we pick a blocksize which is much too large and the system becomes absolutely, entirely centralized, will there be enough momentum to decrease it when so many services are now relying on large, cheap block space? I'm rather skeptical that this would be the case, and I'd much rather have to have another hardfork blocksize increase than be stuck with a centralized Bitcoin that we cant find the political motivation to change.
@_date: 2019-12-02 22:30:10
Cloudflare’s “keyless” SSL thing is a marketing stunt. See the above list for better options.
@_date: 2015-08-20 03:33:12
Let me clarify a bit - the bloom filter code in Bitcoin Core was definitely a suboptimal design, and really should have waited for some other proposals aside from the really-obvious-simple-thing that is done. The code was written in starts and spurts over a month or three and definitely needed more review, since it was merged with a crash bug... I'm not sure if it qualifies as "rushed", but once it was proposed it had nearly no review before merge IIRC, which I think is what is being referred to here. 
@_date: 2016-02-08 22:02:11
I haven't seen /anyone/ disagree with that recently anywhere? Some want a hard fork faster, some don't see segwit as a fast block size increase, but those are technical questions, not vision ones. So let's have a technical discussion to figure it out :)
@_date: 2016-02-12 19:29:56
Interestingly, it hasnt in any real way. You point to the May, 2013 issues, but fail to note that old clients (ie pre-0.8 clients) will STILL, today, without the bdb locks hack sync just fine. The bug comes into play when they try to reorg. While it may have been a hardfork in the sense of "it is possible to construct a block which would fork off old clients" it is not in the sense that old clients can still follow today's chain just fine!
@_date: 2016-02-09 01:47:15
I was referring to broad unification of what to do next, not more philosophical visions.
@_date: 2016-02-09 06:05:27
Did you read the post? Continuing to attempt to have technical discussions in non-technical forums is a big part of the issue..Lets move this discussion to IRC or the bitcoin-dev ML :).
@_date: 2016-02-10 04:13:41
Hmm, I hadnt seen that particular impementation, good to see more people working on relay improvements (though it'd be nice if it weren't all focused on the p2p network :/). I am concerned that there is so much focus on using the existing bloom filter mechanics to do this stuff, though...that code is pretty braindead (I should know, I wrote it :p) and is not a great way to accomplish block compression. I cant say I spent a ton of time looking at that, but I'm dissapointed to see it seems to include lots of fallbacks and extra RTT-s, which is the primary concern with block-relay-times.
@_date: 2017-04-04 09:38:13
The concerns are more about extension blocks generally, and the precedent they set, not so much the activation method wrt consensus. If we really have the level of consensus required for that kind of change, we might as well just hard fork and fix other stuff while we're at it.
@_date: 2016-02-13 20:04:37
You can find the talk at  he finishes speaking (without interruption, at around 16:45), I dont say anything until around 29:00 (it was mostly Bram before then). We went back and forth for about 10 minutes, but I'm not sure I would qualify much as "screaming and shouting" and I dont actually hear any "fucks" (which is somewhat surprising, for me, theres usually plenty of those).
@_date: 2017-04-04 10:29:18
I think thats somewhat unclear, sadly. I spoke a bit about why consensus is critical for changes to Bitcoin at  and I think its not worth risking that for almost anything. If the broader Bitcoin cannot come to consensus about some change, then it can, should, and does stay exactly where it is. While I tend to agree that this really sucks, its not the end of the world. People can and do use Bitcoin today perfectly well, and many of the designs people have for second-layer systems work great today (LN being an exception, though it does still work). I have worked on and contributed to several other proposals, but, indeed, it seems nothing is all that close to getting consensus these days :(
@_date: 2017-04-02 22:49:31
Strange, you we're on my email list but I seem to have failed to get in contact. DM me an email!
@_date: 2017-04-01 19:33:06
Aww, man, now we're gonna have to reset testnet again because people are assigning value to testnet coins.
@_date: 2016-02-08 20:29:55
We're still talking about 2MBish blocks here. But, yes, before we start talking about more we should probably get someone other than literally just me working on non-validation-time relay improvements.
@_date: 2016-02-08 20:05:51
Aaron van Wirdum wrote a whole article explaining the benifits and drawbacks of a centralized relay network at  . Its a pretty good read!
@_date: 2017-04-04 10:06:16
I've never seen anyone who spends significant time contributing to Bitcoin Core say 1MB is key to Bitcoin (and I'm pretty confident none of them think that, given the increase proposed in SegWit). Keep in mind that lots of people are "Core developers" after getting one change merged, doesn't mean they're someone to be listened to neccessarily.
@_date: 2017-04-04 10:00:40
They are somewhat analogous, indeed, though I'm not sure why you didn't quote the parts of my post which explicitly called out the differences and described why I believe they are actually different. I've included the paragraph below. Further background material which I believe is critical to understanding the distinction comes from Pieter's post from Dec, 2015 at  (tl;dr: segwit's utxo compatibility allows any reasonable transaction risk analysis which is unaware of segwit to see them as "untrusted, miner-enforced", which they should already be doing today for anyone-can-spend transactions).






















Edit: as for your comment about the 21M cap, indeed, its a bogus argument for segwit due to the utxo compatibility, its a much less bogus argument for extention blocks. See the original post for more, but note how users and miners are very strongly incentivized to use the extension block, allowing any chages in the extention block to be adapted more generally by nature of everyone using it.
@_date: 2017-04-04 10:15:49
Tend to agree, sadly its not exactly something which could be succinctly described right now, due to various complications. Generally, however, the key considerations, to me personally, are mostly around "will this make a commitment to block sizes which may result in the subsidy going away and fees not existing to replace it to maintain Bitcoin's security". Note that this has a ton of angles, including the potential migration of transactions to off-chain systems that effectively "compress" multiple transactions into one on-chain one, as well as the rather unpredictable growth of Bitcoin on-chain transaction volume, which has a history of growing rather linearly, etc, etc.
I think one of the key issues here is that things are very, very much in flux right now - if segwit were to be activated and we got to see some LN deployment, we might have a ton more clarity on what happens as people are able to get many transactions with one transaction fee. The reality is there are a ton of questions about how the system will be used and what it looks like in the coming year or two.
@_date: 2016-02-08 20:21:24
TL;DR: While there is, of course, a centralizing effect to a centralized relay network, the decentralizing effect of having a non-P2P relay network that is publicly available (unlike the private ones that big miners/pools can afford to build/run) is much stronger.
@_date: 2017-04-04 09:36:54
Further blocksize increases beyond segwit are absolutely something we should do, when required. Doing them as extension blocks, however, doesn't sit well with me, as described. There are many proposals to bump the on-chain transaction throughput further. They come in both the form of technical improvements with things like schnorr sigs and other technical improvements, as well as hard forks where everyone opts into switching to a new system (of course the consensus requirements here make it difficult, but I'm confident we can overcome that with proposals with careful technical design).
@_date: 2016-02-08 20:40:10
You seem to have missed the whole section on this:


@_date: 2016-02-09 01:44:29
There are two parts: a) efforts to build IBLT/weak blocks/etc into the P2P network haven't started - no one has, as far as I know, build implementations aside from test code to test what kind of performance you might expect. b) you can never get relay times down to where miners want with a p2p network (just speed-of-light latency across a few hops is too much), and no one has put in any serious coding effort to build a relay network aside from me (there is one other guy who has a bunch of servers + peers and seems to do a reasonable job there, but as far as I know he hasn't spent any serious time writing more than a few LoC).
@_date: 2018-11-19 01:44:33
Don't install that crap. [
@_date: 2018-03-04 17:09:05
Tl;Dr: rust-lightning isn't meant to be a rust library, it's meant to be a library, that happens to be written in rust, so there's some stuff that is a bit of a rust anti-pattern, just because you don't know what the caller is.
@_date: 2018-03-05 03:12:37
No, sure, though it is incompatible with the goal of reimplementing from scratch to find spec bugs/corner-cases that others missed by not talking to people during implementation :p.
@_date: 2018-03-04 00:39:20
I dont think we can reasonably expect all LN clients ever to have their own full-node? Making some security relaxations for nodes with less money should be more than acceptable - you're already not getting a full "you've waited for 1 month of confirmations, so now double-spend is stupid expensive" security model you can get on-chain. Still, rust-lightning being a generic library should let people do what they want, even using a full-node!
@_date: 2018-03-04 00:37:29


Happy to have more specific feedback than that. Honestly I'm still playing with various API approaches, mostly trying to do safety-by-design by bending over backwards to prevent library clients from having reentrancy issues (including library clients who may, in the future, call via FFI), but that does result in some funky APIs. If you feel like conributing, ideas are more than welcome!
@_date: 2018-03-04 00:40:33
Yup, what said. Will try to get to this tomorrow, if not Monday.
@_date: 2018-03-04 01:44:18
Heh, thanks for the support, but I have to admit rust-lightning is partially a "I wanted to learn rust" project, as well as a "find spec bugs by building just from spec" project. I'm kinda shopping it around to see if there's interest in more contributors/users before I try to drive it to production, so we'll see what happens with it.
@_date: 2019-01-19 21:44:13
It is my understanding that the range proof forces you to use the native asset unless they deliberately support CA (which has a significant scalability loss, which is already a problem for MW). I may be wrong, however.
@_date: 2019-01-19 16:07:02
I don't believe any of the deployed MW coins support CA (though Liquid does and has roughly similar privacy properties as MW, even if it's trust model is highly different).
@_date: 2018-03-05 03:23:23
Exactly, and that's certainly not a problem, but there's a lot of value in doing an implementation and *not* talking to others until the skeleton is there enough to compare notes.
@_date: 2017-09-25 22:08:14
Man we even had a whole blog post to point this out. I guess people don't quite get the whole "its a significant effort to get all major Core contributors to sign off on something so we can take a group stand" thing.
But, yea, lets be clear, I dont know a singla significant contributor to Core who will ever work on btc1/Segwit2XCoin. If all the miners switch over, most likely some folks will buy hashrate and there will be a Bitcoin chain again to work on. If, somehow inexplicably, the entire community gives up on Bitcoin and uses 2xCoin, then most likely the vast majority of Core contributors will just move on to something other than Bitcoin, though given how 2x has been going, I find that highly, highly unlikey.
@_date: 2019-01-15 21:04:40
I would be strongly hesitant to call original AMP "informational theoretically atomic". The problem is there is, as you point out, no guarantee on the claim side. Indeed, it is the case in original AMP that the payment recipient does not receive the preimage to claim until they can claim all of the parts, but they can also chose at that point to only claim one or a few parts, resulting in the payment claim ambiguity that the proof of payment was supposed to resolve. Ultimately, assuming users treat the "proof of payment" field as proof, I'd call base AMP much more atomic. Original AMP is much more useful for spontaneous payments and also provides ever so slightly more privacy during payment, but the trust required in your counterparty on the case of manual resolution is much greater.
@_date: 2018-03-04 15:06:46
1. Yea, agreed those traits aren't great. They were written before I had thought much about the net layer, and may want to change them as that matures a bit. Still, the net layer is supposed to be very agnostic to the underlying network/language the socket processing is done in. This means no assumptions about threading may be made at the message-parsing/socket event handling level, so you end up needing those requirements. I'm open to other suggestions, obviously, however.
2. The concern over dependencies is also a question of recursive dependencies. This is high-security software, after all, so anything that gets pulled in as a dependency really needs to be reasonably audited. Ideally we'd not use std either, but that's not realistic... Because the library should avoid making any assumptions about the calling threading model, so we at least need locks and such. That's obviously also another reason not to rely too heavily on tokio/etc as we don't "own" any threads for stuff like timer execution. I plan on using tokio in an example rust-lightning user (though it's still super early stuff - I've been using it for another project).
3. Yea, the listener registration stuff is a mess. I should go back and replace it with something more sensible once I have everything else more sanely structured.
4. Good thing invoices/addresses aren't yet implemented :p.
@_date: 2018-03-04 13:51:58
Well it depends on whether you want to keep building a multi-daemon system or if you want to build a library :p
@_date: 2017-09-30 01:36:02
I do have to admit, filing that was somewhat painful. Bitcoin is long overdue for better consumer-level investment products, and as many of the other comments on the BIT ETF noted, the availability of an ETF would go a long ways towards helping and protecting average consumers who wish to invest in Bitcoin.
Sadly, the way the S-1 for the BIT was written (and the BIT's and DCG's history with forks, as noted in the SEC comment), consumers investing in Bitcoin via such an ETF would not be anywhere nearly sufficiently protected, especially at the level that they might expect investing via an ETF (when compared to investing directly using Coinbase or some other 3rd-party).
In any case, it seems highly unlikely that my comment had much at all to do with DCG withdrawing its application a few days ago, just trying to do my best to do what I think is right and informing regulators of potential areas of concern that they likely weren't aware of previously (hopefully while still encouraging them by noting that only simple changes should have been required).
@_date: 2017-09-10 22:21:35
JJ submitted it as the talk he wanted to give, but was told by the organizers that it seems irresponsible to disclose the bug before users have a chance to upgrade, and he agreed not to disclose the bug. He then went and disclosed the bug without having notified anyone that he would do so.
@_date: 2017-10-16 15:41:29
The program includes a lot of collaboration and lessons from all the Chaincode folks, including a few of us who have been contributing to Core for 6+ years :).
John is leading the effort for the second session ("Becoming a Bitcoin Core Contributor") and I'm leading the first ("Security Concerns And Adversarial Thinking"), people are welcome to apply to join for one, or both. John has has more than enough experience to give people a solid understanding of code layout, structure, and introduction on to contribute (and is better equipped to help people navigate contributing to Core than those of us who have been doing it for a long time), but we're all around to make sure all the nitty-gritty details are right.
@_date: 2017-10-16 15:59:01
Indeed, I think someone who has been more recently getting into contributing can share a much, much more useful perspective on helping new people get into the project than anyone who has been contributing for a long time. That blog post is a great example.
@_date: 2017-10-17 18:20:08
 See CVE-2014-3570 ("This issue was reported to OpenSSL on 2nd November 2014 by Pieter Wuille").
@_date: 2017-10-27 03:05:10
Ah, that's the issue. yakkety is no longer supported by canonical. I cannot (I believe) upload any new versions for out-of-support versions, plus it would be an uneccessary risk to run with an OS that no longer receives regular security updates. You should update to (at least) zesty, see 
@_date: 2017-10-26 22:18:43
Mostily putting together a list of changes to make sure the everything is robust during the 2X fork fiasco. There's not really any huge bugs here, but as long as the btc1 folks keep adding options like skipping their version signaling (which is key to ensuring nodes are connected to others on the same network as them), there are small risks that someone turns on a few tens of thousands of malicious btc1 nodes and a small minority of nodes (on either side) are isolated.
@_date: 2017-10-26 15:10:30
Which version of lubuntu?
@_date: 2017-10-17 18:16:31
Please do upstream them. The targets in Bitcoin Core are used, though there are a number of directions currently pending in the current fuzzing stuff, plus review backlog makes it slow work, but eventually having them upstream would help anyone who wants to run more fuzzing!
@_date: 2015-09-04 02:36:06
I'll drop it in the bitcoinrelaynetwork.org address, so it gets used for that :).
@_date: 2015-09-03 17:43:18
I didnt build this...it was just sent to me and I figured I'd share
@_date: 2015-09-03 03:01:12
Sorry for the delay on this. I generally agreed with the majority of the letter, but took specific exception to a few phrasings. No big deal, but didnt want to sign as-is and didnt have the time to go get changes pushed through.
@_date: 2011-04-16 14:26:57
Well written article, though I suppose it is a bit one-sided.
@_date: 2011-10-17 20:16:03
In bitcoin 0.5, bitcoin-qt will support bitcoin: URI links. In other words, just update your bitcoin addresses to bitcoin:Address and bitcoin will automatically open it starting with 0.5. see 
@_date: 2016-10-05 16:50:45
The reference was that any time someone does something which assigns value to testnet coins, we are forced to reset and switch to a new testnet. What you did here seemingly did so, and we must now consider starting over again :p.
(but, no, really, we've had problems with people trying to establish TBTC &lt;-&gt; BTC markets in the past and hence the reason for testnet3, though at this point I think the precedent is clear, so no need...TBTC &lt;-&gt; ASCII cat and TBTC &lt;-&gt; Soduku Puzzle Solutions markets aside)
@_date: 2017-10-25 20:22:25
No? The PPA has 0.15 (with the code patch from 0.15.0.1 backported to it). See 
@_date: 2016-10-16 21:55:48
You missed that its just based on a conflation of several unrelated concepts, see 
@_date: 2016-10-26 21:36:04
How far did the other node get? If they both ended upw ith similar heights before getting a checksum mismatch, can you post an issue to github.com/bitcoin/bitcoin? Though my money is still on hardware failures of some kind (are they similar hardware?) its possible it has something to do with some other configuration between your machines.
@_date: 2016-10-05 17:30:34
Well, we're on testnet3 now, so derive your own conclusions :p
@_date: 2016-10-28 17:00:32
I updated the description to suggest people should prefer the official binaries where possible, and that precise isnt supported. As for reasons to not use 0.12.X, well its no longer supported. Frankly I have no idea whether there are any major bugs in it, I'm sure there are some, but I'd really prefer to not provide packages of unsupported software to people.
@_date: 2016-10-28 18:55:57
I have no control over that, see 
@_date: 2016-10-28 16:04:45
There have been several rounds of bugfixes since 0.12.X, I'd highly recommend against running it. If you're gonna use the PPA, you're stuck with my decisions - and I'd prefer you have to jump through hoops to stay on something which I wouldn't vouch for, especially given that its very easy to run the official binaries, and then you dont have to trust me, which is good :).
@_date: 2016-10-16 20:29:57
Ahh! There's the confusion. No, indeed, GA does not allow you to access your wallet directly from their website. Their desktop client is in the form of a chrome app, which does have somewhat annoying auto-update semantics, but they're working on moving off of that, and you can (as I do) run it manually by just pointing chrome at a copy of the source from github, which will not auto-update, and certainly if you only access via one of the mobile clients you're fine.
@_date: 2016-10-01 17:26:38
Absolutely not. Not only does selfish mining net a miner a profit at only 1/3 of network hashpower (even if their bock relay sucks), this article correctly points out that, thanks to "spy mining" (a term which the author seems to be coining to refer to the specific type of validationless mining that we see today), many pools hold (temporary, for some, possibly longer term for others) control of much more than 51% of hashrate regularly.
Though using 51% of hashrate to perform a selfish-mining-like attack is more crude, it definitely can net a profit (for relatively obvious reasons), and any pools which, today, simply don't relay blocks when others switch to "spy-mining" on top of them could stand to make a significant profit.
All that said, the fact that many pools, today, pay out using Pay-Per-Share (which, itself, has invective incompatibility issues) discourages them from performing this attack, as it will increase their orphan rate (just less than their competitors), equating to a loss on profits. Of course if a pool which pays out using a more secure payout method is being used as a source for "spy-mining", they could perform this attack, making a healthy profit (for their users... or not, depending on the payout scheme) while cutting into the profits of PPS pools at the same time.
Indeed, the incentives here change as fees become more important to miner profits, but note that some payout schemes insulate the pool from this cost - a key issue from which much of these incentive incompatibilities stem.
@_date: 2016-10-01 23:42:12
I think GHash is a good example of quite the opposite, actually. While it is true that some people sold, there wasnt a major hit to the price. Even worse, GHash was (quite probably accidentally) performing some kind of "selfish 51% attack" (to use the terminology from the article). Further, I'm not aware of any active monitoring to detect such attacks today - would people on reddit be up in arms if orphan rates went up by 1 or 2%?
@_date: 2015-11-29 05:56:14
Whatever you think about 0-confs, its interesting to note that the current implementation in Bitcoin Core allows the transaction sender to choose - they can opt for the old behavior where unmodified nodes on the network will refuse to accept any conflicting transactions or they can choose to use the RBF semantics. Its easy for those who wish to do 0-conf risk scoring to see this decision and assign appropriate score based on it. Take, for example, BlockCypher, who recently posted on Twitter that they are fully aware of this patch and are perfectly able to handle such transactions - 
@_date: 2015-11-17 11:09:48
Ubuntu 11.04 has been deprecated since 2013. You have not been receiving critical security updates for your entire system since then. You need to upgrade to a newer version of Ubuntu!
@_date: 2016-10-26 02:58:52
"Corruption: block checksum mismatch" means something got corrupted between CPU and disk and back to CPU. Specifically, it means at some point a block was verified and written to disk, and later when we went to re-read it it came back corrupted. This is almost certainly some kind of hardware failure, though if you have an antivirus running and havent re-created the chainstate since the obfuscation feature was added, that may cause problems like this (that is, after all, why the obfuscation feature was added).
@_date: 2016-10-14 22:26:36
In both the nLockTime and 2/3 cases, you do not lose your coins if GA's servers all catch fire and never come back - in the nLockTime case you take the most recent nLockTime tx bundle they emailed you and publish those to the chain using their open-source tool, in the 2/3 case you already have enough keys to claim all your money!
@_date: 2015-11-30 19:01:38
Yea, launchpad is broken...see-also 
I'll go see if I can dig more into it any maybe fix it today after I land.
@_date: 2016-10-28 16:09:26
That is very strange, though highly unlikely to be PPA-specific. What were you doing at the time (just starting the node? running normally? submitting blocks?). What hardware are you on? Have you seen hardware failures previously?
@_date: 2015-11-17 20:47:27
Yea, sadly, launchpad is broken and is unable to install boost-chono (a dependancy of Bitcoin). I haven't spent much time trying to fix it because I haven't received many complaints, but you're welcome to look at the build log at  and complain to launchpad.
@_date: 2016-10-26 15:00:31
Thats possible, though only if you started your sync on a relatively old node. The chainstate obfuscation feature was added some time ago to combat these issues, but only applies to new datadirs.
@_date: 2016-10-16 14:59:16
Oh, I think you're misunderstanding, in the 2/3 mode on GA, you hold 2/3 of the keys - one you print off and put in a bank vault somewhere, the other is on your phone/desktop app/etc, not on any kind of website or thing that auto-updates. GA only has easy access to the 3rd key.
@_date: 2016-10-28 22:39:21
There is an option in GA under advanced called "Sweep all the funds from the wallet" which will automatically subtract the fee you must pay from your total wallet balance and send the rest.
@_date: 2016-10-26 23:32:36
Could be any, really, though I'd bet bad sectors on the drive that happen to keep getting used.
@_date: 2016-10-14 22:02:38
As the op noted, if you add an email to your account, you can use the nLockTime tx feature to claim your funds (last I heard they were implementing a tweak to this to use CLTV to avoid needing to email updates all the time, though I'm not sure what the status of this is).
GA also has a "3 key" option, where you hold 2/3 of a 2/3 multisig, so that you can offline one key and get 2fa(+other) protection(s) for daily use while still keeping complete spendability all the time.
@_date: 2016-10-14 21:19:30
GreenAddress is not a webwallet.
@_date: 2015-11-22 02:20:49
Though I agree - I dont particularly care about a miner losing revenue, orphans do not help secure the network (an attacker does not have to compete since they have their own private network, probably even in one physical location). So it isnt just about one miner being more or less competitive, orphans anywhere result in less robustness against attack (ie less effective total hashpower).
@_date: 2016-10-26 22:36:43
That sounds highly like transient hardware failures :/
@_date: 2015-10-08 22:34:33
Indeed, it is not yet merged, but its pretty close to it at this point, and its been running on a bunch of my nodes.
Last I looked, the drop-random-txn patch XT is using does not address DoS issues of relaying transactions around a ton, especially transactions which will clearly never confirm. If such a patch were to be deployed broadly, it would be trivial to relay transactions at the same speed as you can create signatures, easily using all of the bandwidth of your peers, not to mention bandwidth across the entire network.
@_date: 2017-05-23 22:38:51
The conversation continued for quite some time after the screenshot. At least to me it seemed clear there was relatively limited desire to have me there unless I was there to sign an agreement to switch to some hard forked chain.
@_date: 2017-05-23 21:59:42
That last comment pretty clearly describes why I didn't go. I wasn't asked to attend to fix technical issues and help with technical details (I offered to do so explicitly), I was asked to attend to sign an agreement in a room full of people who spend their full day trolling. There was still lots of tech feedback prior to publishing, but it was ignored, sadly.
@_date: 2016-05-03 08:21:08
The proposed protocol does not require more memory, it uses the mempool you already have (and doesn't need a very large one to be effective). Indeed, however, this is mostly a bandwidth-savings and bandwidth-spike-removal effort across the network (as discussed in the motivation section). However, it has three major properties to consider wrt latency:
* It cuts the average-case minimal RTTs to 0.5 from the 1.5 any current P2P protocol change uses.
* It uses less bandwidth than other protocols, which is often more important for total transmit time when considering TCP with some real-world packetloss.
* It forms the basis for future work which is intended to hugely improve relay latency (and will replace the relay network's backbone while also being in Bitcoin Core to enable individuals to easily set up their own relay network or even just high-speed relay between two nodes). Check out  for an early preview of how that code will work.
@_date: 2015-10-12 22:05:08
I was under the impression it dropped txn randomly over its memory usage threshold? Is that still the case? If so, all you do is create a set of transactions that is a few x multiple of the default memory usage threshold size. Then just keep repeating them to all your nodes in a block. The nodes will keep relaying them freely forever? Last I saw there was no limit on such relay (request maps, etc are also size-limited). Sadly, if this is the case, then you will do the same to your peers (announce each new transaction you re-added, which your peers will not have, and you'll end up relaying some percent of the spam your evil peer is sending you to all your other XT peers).
@_date: 2015-10-12 22:06:47
There's either a bug in launchpad's armhf usage, or a bug in Bitcoin Core's test-cases on ARM, either way it doesnt build properly (see 
@_date: 2015-10-08 20:53:55
There is code to limit mempool without exposing the network/your node to attacks at  Thanks goes mostly to Alex/Suhas for their mempool tracking code, on which that relies.
@_date: 2017-07-10 19:55:57
No, they couldn't use that hard fork bit because it was pointed out to them multiple times by other developers, and once that happens, they cant use it anymore, or something, I dunno, there's really no explaining their behavior.
@_date: 2017-05-23 20:42:06
We'll see...its been a rather fast moving thing, so maybe they'll turn around and significantly tweak their proposal and make a new one, maybe not, I hope they do :).
@_date: 2015-11-29 00:43:45
LOL. This is a joke, right?
@_date: 2015-10-08 22:36:02
Note that it is actually not easy to make mempool limiting work given the broad set of attacks you want to protect against. It's really only easy to do thanks to the mempool tracking stuff that was merged in Bitcoin Core not too long ago.
@_date: 2016-10-14 23:17:46


In this case you have the same security as any almost all other wallets on the market today - if the wallet provider pushes a malicious update to your phone/device, they can construct a transaction which steals your money. Always verify updates to your Bitcoin wallets before applying!
@_date: 2017-07-22 01:12:25
Yes, USD.
@_date: 2016-05-03 18:08:04
That plus the design includes a 0.5*RTT transmit possibility which, testing indicates, will be hit 90% of the time for blocks which come from peers which also recently provided you blocks first.
@_date: 2017-07-21 16:33:28
Somewhere between 300 and 500 per month, I dont recall the exact cost and it changes here and there.
Edit: Note that the previous version was more expensive (a thousand and change a month), but I cut it back when I switched to paying myself.
@_date: 2017-07-16 17:23:45
I think you meant outgoing connections, but, yes.
@_date: 2017-05-23 21:14:23
Heh, sorry, I dont follow usernames when responding much...text is a hard communication mechanism to not misread :).
@_date: 2017-05-23 21:09:54
Please read the text, I'm not sure I agree with the tl;dr in the title, but that's reddit. My objection stems from technical issues, and as I've mentioned elsewhere I'm hoping they tweak their proposal to fix some rather glaring technical issues in it.
@_date: 2016-07-07 21:20:05
This is about decentralizing fast-block-relay. While a few large miners have their own private networks, and the existing bitcoinrelaynetwork.org works reasonably well, neither work at near-speed-of-light for small miners or miners without the technical expertise to do single-packet-block-relay for their own blocks. This is an easy way for anyone to set up a network which relays blocks around the globe with only a few ms extra time more than the speed of light :).
@_date: 2016-07-07 21:37:37
Oops...no wonder my spell checker didnt see it. Don't think I have the source for that, either :/ Oh well.
@_date: 2016-07-07 17:23:57
Its a bit of both, but, yea, speed of light changes based on the medium its travelling through.
@_date: 2017-05-23 20:36:07
Sadly, we did reach out, repeatedly and many people repeated the feedback which was posted publicly in the past day or two on the bitcoin-dev ML. I'm happy to see more effort to compromise, but sadly this attempt accidentally stumbled into a technical briar patch, and I continue to hope they'll tweak their proposal and seek community consensus on something which fixes the issues here.
@_date: 2016-07-07 16:52:31
Tl;dr: falcon is still really early days...they're still in testing on something that is, as far as I understand, still more often than not much slower than the original Relay Network. I'm looking forward to see where they go, but its gonna take them time.
@_date: 2017-07-23 03:12:12
Not really directly, but FIBRE is designed to be pretty easy to run your own network, and other people running ones is good! Right now several pools also use FIBRE between their nodes globally, though, which helps a lot.
@_date: 2017-07-08 01:08:13
No one can seriously look at this and suggest it was a serious attempt at documenting a proposed protocol change. Instead of describing a set of changes to hard-fork to a larger weight limit, the document sergio put out is a soft fork which does not increase the block size at all, and generally reads like a disjoint wishlist of random ideas.
You can read more of the responses at   and 
@_date: 2016-07-07 20:09:04
It has two modes - "trusted" networks are a bit faster, in that they dont fully reconstruct the block before relaying it onwards, but "normal" mode is also incredibly fast and doesn't require any trust between nodes.
@_date: 2016-07-07 21:18:29
Where do you see crhome (or chrome?) on the website?
@_date: 2016-07-07 16:51:35
You can see a few more references to Falcon in comparison on my blog post on the subject, at 
@_date: 2016-07-22 22:32:44


Few responses here:
a) We have been, no question, various speedups in Bitcoin Core (not to mention lots of nodes going offline - a network-diameter decrease) have hugely decreased P2P latency. Further, /I/ have been - see Compact Blocks.
b) Ultimately no P2P network (or, really, any network) will ever come close to beating a miner doing single-packet relay of their blocks (several, if not most now, large pools do this). Remember that selfish mining attacks become much more profitable/doable at lower hashrates with a relay improvement. Thus, it is critical that publicly-available networks get as close as possible to speed-of-light-through-fiber, as this is key to both preventing such attacks, as well as maintaining Bitcoin's decentralization. While I'd welcome suggestions for an attack-resistant self-organizing P2P network which finds the absolute lowest-latency paths on the internet, I'm highly skeptical such a thing could even be built.
c) It does, in fact, /decrease/ the current centralization. Certainly one public network is not decentralized, but at what point do we have enough centrally-managed public networks that the effect is the same? Pushing towards this is, in fact, one of the key goals of FIBRE.


I'm more than happy to see things like this used on the P2P network - I even stated explicltly that I'd relicense if someone wanted to push it upstream. However, keep in mind the P2P network has competing goals. The vast majority of full-node operators do not care about block propagation latency (within a few seconds, sometimes more, is just fine for most applications) and care about bandwidth utilization (even today a residential internet connection can virtually hang when a full-node starts relaying blocks). Miners, on the other hand, have very different goals. We cannot build a network that is perfect for everyone, and when we're talking about even small propagation improvements enabling selfish mining at much lower hashrates, good is the enemy of good-enough.
@_date: 2016-07-07 20:49:14
I havent done any significant tests through the GFW, but the design of UDP with pass-through of packets to all nodes is pretty ideal for crossing the GFW quickly.
@_date: 2016-07-07 16:50:54
Multiply by ~1.8, because speed-of-light through fiber is not the speed of light. You might also want to check out the top graph on  In my test network, 85% of the time, blocks were received in 35ms or less longer than the speed-of-light (50% of the time in 16ms or less!)
@_date: 2016-07-07 16:46:37
As noted at the bottom of  its more of a temporary precaution, but I'd be more than happy to relicense it if someone wants to ship it as a part of another distribution.
@_date: 2016-07-07 17:26:53
Not sure what "dynamic routing" is in this context, but the FIBRE codebase is pretty much optimal in terms of relay speed (see the top graph on  which comes from a test network of rather-slow VPSes). Further FIBRE is already designed for everyone to be able to set up their own networks, with detailed instructions on picking hosts and finding cheap ways to set up networks, instead of requiring anything centralized.
@_date: 2016-07-22 22:05:16
I'd like to know how you think FIBRE increases centralization in Bitcoin. Its easy to be naiive claim that this means less P2P-use for mining, but this ignores that many miners already weren't using the P2P network for mining prior to the Relay Network, let alone now, where the vast majority of hashrate is using several private networks, in addition to standard P2P peering.
More important still is that networks like this have strongly decentralizing effects on hashrate itself.
Maybe you could suggest a new tenant to why decentralization matters/where it doesnt as discussed at  or explain how FIBRE hurts Bitcoin based on that discussion?
@_date: 2016-07-07 16:54:41
More accurately: Compact Blocks was built as a part of this. The first revision of the Compact Block code was pulled out of an early version of FIBRE. :)
@_date: 2016-07-07 21:23:00
Not at all! The work around compact blocks in bitcoin 0.13 improves relay between nodes in the P2P network significantly, not to mention the massive general validation/relay-time improvements in 0.12 (and some in 0.13, too). However, if we really want to have decentralized high-speed-relay, to quote the blog post:


@_date: 2016-07-07 16:48:50
Not yet, but with the FIBRE codebase much faster/more reliable, I'm unlikely to fix the relay network to support Segwit.  Workig on making sure there is more than one FIBRE network to replace it, first :).
@_date: 2017-09-10 22:15:30
Can't speak to libbitcoin, but btcd and bcoin both had the issue as far as I've been told, and BitcoinJ, while it doesn't have the specific multi-output-per-input-load issue does have other significant scalability issues with its DB backend API (plus its Java, it was already gonna take a lot of memory for full validation mode anyway, whats a few more 10s of GB, well and not to mention that its full-validation stuff is effectively unmaintained).