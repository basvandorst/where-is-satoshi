@_author: pwuille
@_date: 2016-08-03 01:33:17
@_date: 2016-08-02 23:40:35
Can you report this as an issue on  ?
There is something going on here that I don't understand, but Reddit is probably not the best place to investigate.
Please attach debug.log extracts and software version if you can.
@_date: 2016-08-03 00:29:28
I think you are wrong.
Yes, it is well understood that Bitcoin's security weakens when the amounts transferred are many times larger than the block rewards.
However, the attacker is not interested in a secure transaction. He would be happy with a small percentage of the money, so it is likely that he would start outbidding the victim against a reorg by paying miners. Furthermore, he does not require a reorg, so the resulting exchange value for miners is likely much higher by following the attacker's demands.
A likely result is an increasing amount offered to miners until the point where they get nearly everything, and neither the victim and attacker get anything significant.
RE: Your EDIT2: I'm glad to see I misunderstood your message. But I disagree decentralization is something that would fix this: both the attacker and the victim can put up money through huge fees and/or timelocked anyonecanspend outputs that can be grabbed by current and future miners even if all miners were small and anonymous groups.
@_date: 2018-01-18 15:22:46
The paper is just about a new cryptographic primitive that is useful for implementing signature and key aggregation. It doesn't propose any particular change for Bitcoin itself (though we're working on that too).
In general, the issue you're bringing up can't be solved with a soft fork. By definition, you can only spend outputs in a way that was already valid before the new feature is introduced.
@_date: 2018-01-19 02:51:53
@_date: 2018-01-31 06:23:35
No, because Bech32 *is* a (native) SegWit address.
The SegWit addresses you've been seeing before (the ones starting with 3...) are *P2SH* addresses. They're a format introduced in 2012 that is more generic than standard 1.. addresses, but has mostly been used for multisig constructions. If you weren't using a multisig wallet (like GreenAddress or copay), you likely never saw one. SegWit *can* use P2SH addresses, but it's slightly less efficient when doing so (the fees will be lower than with non-SegWit, but not as low as with a native SegWit address. The advantage is that P2SH is compatible with every wallet software created the past years.
So Bech32 is a new format that gives you all the advantages of SegWit, but unfortunately it will likely take a while before all wallets adopt it. As we had to invent a new format anyway, we took the opportunity to build something that fixes some of the shortcomings of Base58 at the same time. This includes a better checksum and no mixed case characters anymore.
@_date: 2018-01-31 06:13:43
Oops, right. We don't currently have a way to upgrade from non-HD to HD, I forgot.
@_date: 2018-01-31 17:08:25
There are no restrictions on what can send to what. 
 You could receive coins using any type of address your wallet  supports, and send to any type of address it supports.
The only reason for the existence of P2SH-embedded SegWit is that not every piece of software will immediately support sending *to* Bech32. So the expected workflow is that for now, you give out P2SH addresses, unless you know you recipient supports Bech32. Over time, as Bech32 gets more adoption, I expect that we'll be able to change the default for newly created addresses.
@_date: 2018-01-25 01:56:22
That's a decent ELI for signature aggregation, something we're indeed working on. But it's not the main topic of the blog post, which is about MuSig, a scheme that's only tangentially related.
MuSig is just a cool scheme that lets groups of signers act like they're a single address (once Schnorr signatures exist on the network, but the network wouldn't use MuSig itself), in a way that's very robust (there are few pitfalls in making it secure).
@_date: 2018-01-31 16:58:54
No, it's pronounced exactly as it's written (in Dutch at least).
@_date: 2018-01-19 03:39:30
Who knows.
This is just a paper on new cool technology (IMHO, at least!); it's not a concrete proposal for Bitcoin to adopt (though we're working on that too).
@_date: 2018-01-25 08:29:22


Specifically that, no, you can't have outputs be created in a softfork that didn't exist before the fork. To old nodes, the spending of that token would look like spending something non-existent.
There are however plenty of other ways a signalling like that could occur, if that were desirable.


I don't see the point of all that complexity. In order to guarantee that nobody can signal in your name, you would need to provide at least an ECDSA signature using the key corresponding to your address. If you're willing to issue a transaction on the network, complete with signature using your key, why not use that transaction to just move your money to a new address? That's:
* More private (it's indistinguishable from a payment) due to less address reuse.
* Far less complicated than introducing tokens.
* Better scalable as full nodes aren't required to maintain a (presumably) ever-growing database of which addresses have signalled support for which features (especially as addresses shouldn't be reused in the first place for privacy reasons).
@_date: 2018-01-24 22:22:39
Probably much sooner.
@_date: 2018-01-25 01:46:07
That's often repeated, but not true.
What existed since 0.13.0 was the `addwitnessaddress` RPC call to convert a P2PKH address into a P2SH-P2WPKH address. However, this was pretty much just enough for testing, and not production use: it didn't support labelling, and needed backups after every newly generated address. Having change go to SegWit outputs was possible using a very elaborate sequence of RPC calls, and it would still show the change outputs as incoming payments.
0.16 fixes all of these things. New addresses and change will automatically be P2SH-P2WPKH by default, but configurable to use P2WPKH or P2PKH instead. Labels will work fine, no backups will be needed after every address, and Bech32 addresses will be fully supported (both in the GUI and RPC).
@_date: 2018-01-30 19:43:29
@_date: 2018-01-18 14:43:12
It's easiest to explain with an example of an insecure scheme.
The mechanism which all these systems are based on is the fact that multiple parties can create a (partial) signature on the same nonce and message, and then later 'add' those partial signatures together to form a full signature, which is valid for the sum of their public keys.
This seems like it can give us an absolutely trivial way for replacing 2-of-2 multisig: just send to the sum of the two public keys, and by the above logic, you expect that only the two owners of those keys would be able to spend it.
It's true, this works. But it has a caveat. Say you want to pay a 2-of-2 multisig of Alice and Bob. You ask what Alice's key is, and she says `A`. You ask what Bob's key is, and instead of replying `B`, he gives you `B-A` (the EC point difference between B and A). You now happily construct the sum of the two responses `A` and `B-A`, and unknowingly send your funds to just key `B` - which Bob can obviously spend without cooperation from Alice, breaking the 2-of-2 assumption.
In most scenarios, this is fixable. You ask Alice and Bob to each prove they have access to the public keys they claim to have (you require the keys to be "certified"). However, this makes the protocol more complicated and easier to screw up. Furthermore, it seems it's hard to prove schemes that rely on this certification to be secure.
The first paper that gave a Schnorr-like signature scheme that was provably secure against the above attack (without requiring certification of the keys) was Bellare-Neven in 2006. This is what "Signers are only required to have a public key" means. Unfortunately, Bellare-Neven does not permit you to compute a "combined key" ahead of time (meaning the verifier needs to see all the public keys, a bandwidth and privacy reduction).
The scheme we propose in the paper is novel in that it combines (A) the lack of need for certification of keys and (B) the ability to precompute a combined public key for a set of signers. The paper also gives a security proof.
@_date: 2018-01-30 17:54:08
Internally Bitcoin Core's wallet has used HD derivation since v0.13.0. If your wallet was created with 0.13 or later, you are effectively using an HD wallet. For now, the only observable effect of that is that you don't need to create a new backup every 100 transactions, as the master key (which is in the wallet file) generates all derived keys rather than randomly.
Bitcoin Core 0.16 doesn't change anything about this. If your wallet was HD, it remains HD, but will now generate and accept payments to SegWit addresses for the same keys. If your wallet was not HD, it won't become HD, but instead still generate random keys every time the keypool runs out, while orthogonally still generating and accepting payments to SegWit addresses for those keys.
@_date: 2018-01-25 01:53:42
What's encoded in (current) Bech32 SegWit addresses is just "version 0 witness program, with data X", where X is either a key hash or a script hash.
However SegWit addresses can encode any witness version number and data. So when a new script version is available through a softfork that enables signature aggregation, and you have a wallet capable of creating such scripts, it will be able to encode those in the existing Bech32 SegWit address format - it would just be a v1 witness rather than v0 witness. Every wallet that supports sending to BIP173 addresses will already support that.
So: yes, you certainly won't be able to use signature aggregation when spending your *existing* outputs sent to Bech32 addresses, but the new addresses your aggregation-capable wallet generates will still be Bech32.
@_date: 2018-01-30 17:50:25
You don't need to create a new wallet.
New addresses with be P2SH-P2WPKH (3... addresses) by default, but configurable to be either P2PKH (1... addresses) or P2WPKH (bc1.... addresses, Bech32).
@_date: 2018-01-31 06:14:58
Every config file option works in the form of a cmdline option as well.
@_date: 2018-01-30 19:23:23
In the case of Bitcoin Core, there is little reason to know the seed, and it's also not exposed. It's just there to guarantee that a recovered backup of a file will generate the same addresses again.
In theory, you could extract the seed from the wallet file, and use that to generate the same keys and addresses as the wallet file. However, there is no guarantee that it will generate *all* the wallet's addresses (the wallet may have keys from before it was converted to HD, for example, or contain imported keys, or ...).
@_date: 2018-01-31 08:14:10
That will work fine.
In a bit more detail, 0.16 will just treat every key as if it had had an implicit `addwitnessaddress` applied to it. So it doesn't matter whether you explicitly called it or not.
@_date: 2018-01-18 16:16:14
Privacy and performance increase. The 1-of-10 would only need a single signature check by the public.
Furthermore, but this is only tangentially related, all of this can be combined with cross-input aggregation (meaning the entire transaction will only have 1 signature, rather than just 1 per input).
@_date: 2018-01-25 02:44:12
STARK = succinct, transparent arguments of knowledge (I had to google too).
@_date: 2018-01-30 20:40:59
Double rainbow all the way 'cross the sky.
@_date: 2018-01-18 15:27:26
What the scheme in the paper would let you do (once there is support for Schnorr signatures in Bitcoin) is go over all 10 combinations of 3-out-of-5 keys, and create 10 aggregated public keys for them (one for each combination). Now you can write the problem as a 1-of-10 rather than 3-of-5 multisignature (which becomes very efficient in combination with MAST).
The advantage this scheme has over existing alternatives is that the different 5 participants don't need to prove anything ahead of time to trusted authority or to each other.
@_date: 2018-01-18 16:12:01
Both, though depending on feedback, it may be easier to just aim for single-input aggregation first.
@_date: 2019-07-21 17:07:36
@_date: 2018-01-25 07:26:46
Bech32 addresses for hypothetical witness v1 scripts (which may support signature aggregation) would still look like "bc1".
The point is that Bech32 can already encode any witness version output. No new *format* will be needed to support that, but you will need *new addresses*.
@_date: 2019-07-13 07:25:15
I've written about this question before: 
TL;DR: binance can't roll back the chain. They can offer a bounty to help them roll back the chain, but the attacker can do the same to prevent the rollback. All things considered it is enormously unlikely it would actually happen.
@_date: 2019-07-08 01:41:37
You cannot compute what xpub an address was derived from.
Given multiple addresses all known to be derived from the same xpub, you still cannot compute that xpub.
Given multiple addresses, you cannot distinguish between whether they're all derived from the same xpub or just random address (unless you have access to the xpub they were derived from).
HOWEVER, if you know an xpub, an address that was derived from it, and a private key corresponding to that address, you can compute all private keys for all addresses derived from that xpub.
@_date: 2019-07-17 04:52:58
With 500000 mid-range PCs (assuming 20 Mhash/s per PC), at today's difficulty, using CPU mining you'd make 0.00028 BTC per day. At today's prices that's about 2.6 USD per day, from all PCs together. At the same time, the *additional* electricity used from mining would likely be over 10000 of USD worth per day. That's even ignoring added wear and tear to the hardware.
CPU mining became superseded in 2010. By 2013 it became nonsensical. Now it isn't even worth bringing up.
@_date: 2019-07-17 06:28:25
Nit: BIP32 defines key derivation, but not seed phrases. You may be thinking of BIP39.
@_date: 2018-01-30 19:18:36
You can upgrade to an HD wallet by starting with `-upgradewallet`. All your old addresses will remain valid, but new keys will be generated through the HD mechanism. You'll need one more backup after upgrading, but not afterwards anymore.
@_date: 2019-07-08 02:09:34
The $5 wrench attack works fine against hardware wallets. 
@_date: 2019-07-10 14:42:01
This is incorrect. Bitcoin Core does not have or support any kind of seed phrase. There is only a master key that is stored in the wallet (encrypted if the wallet is encrypted), but it doesn't correspond to any human readable form. You need backups.
It does use deterministic derivation of addresses, so all copies of the wallet will generate the same sequence of addresses.
@_date: 2019-07-20 22:53:07
Timelocks are specified as 32-bit unsigned integers, representing a timestamp as the number of seconds since Jan 01 1970 00:00:00 UTC.
The maximum time representable is Feb 07 2106 06:28:15 UTC.
There are multiple ways to extend this, but until it is clear which one will be adopted by the network, it isn't possible to create a transaction that will satisfy its requirements.
@_date: 2019-07-23 16:25:47
G'root for Bitcoin: 
@_date: 2019-07-08 01:55:36
I'm happy to see people are interested in thinking through problems, and I encourage you to keep looking into things and contributing.
The idea of talking old non-moving coins out of circulation in various ways is however one that's been suggested at least a dozen times over the past few years.
In my personal opinion, it is so fundamentally in contradiction with the rules Bitcoin was created with that I do not believe it will ever be acceptable to its community: under no circumstances can anyone deprive you of coins you control. I don't think it's a necessarily bad idea - just one that's so fundamental it would need to be part of the rules that people signed up for when they learned about Bitcoin.
Changing this is akin to changing the constitution of the system, and due to Bitcoin's voluntary self-validating nature, this doesn't just need a majority vote, but overwhelming support from the entire ecosystem in order to move forward. Things like that I expect to only happen when the alternative is a complete collapse of the system. The reason for that is that I think one of Bitcoin's greatest strengths is its consistency: it is very hard to change, creating an expectation that fundamental properties can be relied upon for very long periods of time. In this case, even the motivation seems very vague at best, as nobody is hurt by coins that are lost (apart from their owners).
There are technical/incentive issues as well. For example if there is some form of cutoff at which point UTXOs become unusable after which they turn into reward in some way, this may create an incentive for miners to delay accepting transfers of soon-to-expire coins, in order to increase their own rewards.
@_date: 2019-07-21 06:56:23
The input to the hash consists of the block header, which includes the transaction data, a version number, a timestamp, the hash of the previous block, the nonce, and... the difficulty.
So you cannot reuse "lucky" hashes from previous attempts as (1) their difficulty would be wrong (it has to exactly match the difficulty of the block being solved), (2) the previous block hash would be wrong.
Because the previous block serves as input to the next block, no proof-of-work can be "reused" to construct any other block later on. Changing the previous hash in the block would invalidate the PoW.
@_date: 2018-01-19 03:26:20
Yes, the scheme's security does not rely on certification of the public keys that are involved.
This means that computing the combined public key for any given set of individual public keys can be done by anyone, without any communication. It's a simple function of the public keys involved.
Constructing a signature for that combined public key still requires interaction between the signers.
The point is that there is no interaction with the verifier.
@_date: 2019-07-08 02:37:43
Actually, no.
BIP32 derivation includes a hash (SHA512), which if not broken through other means, should provide sufficient security even in the presence of hypothetical arbitrarily strong quantum computers.
Of course, such hypothetical machines can steal coins, but there is no per se reason why they'd be able to link BIP32-derived addresses.
@_date: 2019-07-20 23:00:08
It has plenty of advantages besides signature aggregation. In fact, the first proposal for inclusion of Schnorr (bip-taproot) in Bitcoin doesn't even include permit signature aggregation.
In short: it permits constructing outputs such that each can be spent with just a single signature (rather than one signature per participant, like multisig needs nkw). Together with Taproot it can in fact make every output and input that is spent by signing with a single key (even if there were other more complex spending conditions allowed, like timelocks or hashlocks) indistinguishable from each other.
With signature aggregation this is essentially extended to be possible across all inputs rather than just within one input, but there are significant engineering challenges to make that work, so first things first.
@_date: 2019-07-06 08:39:54
Yep, absolutely. In fact, that's what happens in the most common transactions.
You see, internally, addresses in Bitcoin aren't like "accounts" that hold a balance (despite what block explorer websites may make it look like). Instead, the basic concept is *coins* (also known as UTXOs).
A coin has an identifier, an amount (anywhere from 0 satoshis up to millions of BTC), and an address (or more correctly, a script). Transactions are effectively "reforging" coins, melting down old ones to construct new ones. When a coin is used as an input to a transaction, it disappears completely, and its value must go elsewhere.
As an example, say someone once paid you 5 mBTC, and someone else sent you 4 mBTC. It doesn't matter whether this was twice to the same address or to distinct ones - regardless, it means you have two distinct coins. Say you want to pay someone 6 mBTC. Neither coin individually is enough, despite your wallet telling you you have 9 mBTC available. When you instruct your wallet to send 6 mBTC, it will construct a transaction that melts down both the 4 mBTC and the 5 mBTC coin, and creating *two* new coins from it: one a 6 mBTC one to your payee, and one a 3 mBTC one back to (a different) address of yourself. The latter is called the change output.
There are several reasons for this, but one of them is privacy: the Bitcoin protocol does not distinguish between payment outputs and change outputs. It does not know or care which is which. Instead, it just checks that coins being spent exist, that their values add up, and that you're authorized to spend them.
So to answer your question: most transactions in fact have 2 outputs, even the ones that only perform a single "logical" payment. It is perfectly possible to go beyond that, and add more outputs. In fact, this is a recommended practice to reduce transaction fees for businesses that perform many on-chain payments (often referred to as "batching").
@_date: 2019-07-12 06:57:21
The chance that the next block happens within the next 10 seconds is always around 1.65% (assuming block generation is a perfect Poisson process with rate 1 per 600 seconds).
Improbably, but not exceptional by any means.
@_date: 2019-07-15 03:56:57
You miss the point. The dollar was backed by something; you may think that what it was backed by was overvalued, but every dollar was convertible to a fixed amount of silver. *That* is backing.
The silver itself was never backed by anything. People wanted silver itself, which gave it value. A dollar was only worth as much as the silver backing it, but the silver was valuable on its own. That's the point: something that isn't backed by something is valuable directly, rather by the backing. The same is true for bitcoin: it isn't backed by something, and instead it is valuable directly.
As for the whole discussion about intrinsic value, things are more subjective here, as it's very hard to assess "what the value of something would be if it wasn't being used a means of exchange", but in my opinion: gold and silver have intrinsic value (though far less than its current trading value); the dollar and bitcoin don't have any intrinsic value.
@_date: 2019-07-26 07:59:53
It is called native segwit, as opposed to p2sh-embedded segwit.
You're right that segwit works fine without it, though at the cost of extra scriptSig bytes.
@_date: 2019-07-09 20:42:17
There is no reason to assume why the electricity consumption would increase with growth in transaction rate. They're unrelated.
@_date: 2019-07-15 03:36:04
Sorry, but this is nonsense.
Being backed by something is not the same as having value, or being useful.
Bitcoin is absolutely not backed by anything. "Backed by" in this context means that there is a guaranteed exchange rate with something else.
Of course, neither is the dollar or even gold for that matter. The dollar used to be backed by silver.
@_date: 2019-07-08 07:20:27
Correct, though you need to know the derivation path the key is for as well.
@_date: 2019-07-15 03:30:13
It's... not a fork, it's a mining protocol.
@_date: 2019-07-15 03:54:26
No, it is not a change to Bitcoin, the hashing algorithm, or its consensus rules in any way.
EDIT: I only now look at the title. "New hashing protocols" doesn't mean new PoW hash *function*. It's just a better protocol for communication between hashers, pools, and nodes to select the contents of blocks.
@_date: 2019-07-15 04:26:57
Discussions about intrinsic value are hard, because definitions vary, and are often still based on subjective assessments.
Still, I have difficulty comprehending how you can say BTC has intrinsic value. To me, it's almost the prototypical example of something *without* it. There is literally no use for it, except as a means of exchange... which it is amazingly good at. But you can't eat a BTC, or build a tool out of it, or solve any engineering problems with. Sure, the technology underpinning is useful, but that doesn't provide use for the BTC token itself.
If you define utility so wide that "getting rid of it in return for something else" counts as use, then I think your definition of intrinsic value pretty much becomes identical to market value.
@_date: 2019-07-09 21:55:04
There is no math. These things aren't related in any way.
@_date: 2019-07-05 17:38:31
Bech32 author here.
The question does not really make sense, as it is not addresses that send coins, but wallets. Addresses are just a way of addressing where coins go. What the receiver does with those coins is up to them, and their wallet software.
If your question is whether transactions can have native segwit outputs (which bech32 addresses refer to) but legacy inputs, or the other way around, the answer is yes twice (this is true in general, nothing in Bitcoin's design allows restricting where coins can go based on what type of outputs they were assigned to before). They can even be mixed (so you can have a single transaction that uses both coins received on bech32 and legacy addresses, and pays both bech32 and legacy addresses).
@_date: 2019-07-10 14:15:47
There are two issues that get worsened by this:
* The shorter the inter-block interval is, the more advantage you're giving to an attacker. You see, blocks take time to propagate through the network to all miners, at the very least limited by the speed of light. Due to PoW resulting in a Poisson process for block generation time, there is a nontrivial chance for two blocks being found within a very short time of one another. If that period of time is so short that the first block has not had time to reach the miner producing the second, the second block will be a waste of power (both in terms of income and in security to the network). This wouldn't be a problem if it affected every miner equally, but it doesn't. More well-connected (geographically centralized) miners suffer less, as they're relatively closer to everyone. But most importantly, a 51% attacker does not suffer from this at all, as he only builds on top of his own blocks, and thus never needs to wait for propagation. For some numbers, at block interval 600s there is about a 0.83% chance that two blocks are produced within 5s of each other. With block interval 60s that goes up to around 8.0%. If block propagation takes 5s, that means giving a direct 8% cost reduction to a mining collusion attack.
* Overall resource consumption for full nodes. If you have 10x more blocks of the same size, full nodes will need 10x more resources to verify history and maintain in sync (bandwidth, CPU, I/O, storage, ...).
@_date: 2019-07-11 06:12:12
No, it's perfectly possible that mining just becomes unprofitable for many miners, who turn their equipment off. The result is that the more efficient ones (cheap electricity, paid off hardware, ...) become profitable again due to others dropping out.
More likely, the mining ecosystem starts to price the halving in to their business decisions preceding the halving itself, and nothing drastic happens at all.
@_date: 2019-07-13 07:19:20
Do you put a billboard outside your house with your bank account balance?
Don't do the same with Bitcoin by reusing addresses.
@_date: 2017-12-26 17:14:26
The biggest hurdle is Monero's ever-growing spent output set, which is needed for double spending prevention.
I don't think its resource usage implications are acceptable for a production-ready decentralized currency.
@_date: 2017-12-26 10:52:28
The "3..." addresses are actually P2SH, and were introduced in 2012 in BIP16/BIP13. So far they've only been used for multisig constructions (see wallets like copay and greenaddress). You may never have encountered them for this reason.
With the introduction of SegWit, the choice was made to support them either inside P2SH (for backward compatibility with existing wallet software), or natively (which would require a new address type that old software would not understand). Bech32 (BIP173) is that new address type.
@_date: 2017-12-06 19:26:12
If a miner chooses not to perform the computational work for validating a transaction, they'll just miss out on its fees, and still need to do the work when another miner does include it.
@_date: 2017-12-07 11:46:52
It means more payments per transaction; not more transactions per block.
See it this way: instead of telling the entire world about every single penny in your wallet that moves between people, you just tell the world about the result of many payments at once (or if someone tries to steal). As a result, you'll be willing to pay significantly more for one transaction.
@_date: 2017-12-04 18:04:44
There is really no reason to use the word "backed" here. BTC cannot be exchanged against a guaranteed quantity of something else.
It certainly is true that much of what makes BTC interesting is due to its decentralized nature, which a nationalized cryptocurrency would lack. However, just because something is interesting does not mean it is valuable.
@_date: 2018-09-16 07:59:28
I really disagree with this.
A wallet is *not* just a collection of keys.
It also contains the unspent outputs you own, and how they came to be.
@_date: 2019-07-21 17:09:58
Seems so.
@_date: 2017-12-28 07:53:46
~~That's not correct. There are hunderds more blocks that claimed less than they could have.~~
Oops, you're specifically not talking about claiming less fees. I believe that's indeed the case then.
@_date: 2017-12-06 20:06:31
The actual cost (in terms of electricity and hardware wear and tear) from validating a single transaction is next to nothing - far less than any fee it could be paying. Of course, it's a cost that every node in the network has to pay, and all together it may be substantial, but to the miner who chooses to include it, it is negligable.
So no, miners have all incentive to always include the highest feerate transactions, regardless of whether those are computationally cheap or expensive transactions.
@_date: 2017-12-26 17:08:55
It's not in any released version yet.
@_date: 2015-12-07 21:18:17
Thank you!
@_date: 2015-06-09 03:55:11
We didn't write the article.
@_date: 2015-06-09 21:38:13
No, 2.5 kilobytes per output, if you want to have a 32-bit range of possible values. It scales linearly with the number of bits.
@_date: 2018-09-16 17:16:52
This is mostly a philosophical question and a discussion about semantics - more a "how you should think about it" rather than "right and wrong".
I think it's a fallacy to treat the blockchain as the source of information for what your spendable outputs are. While it is true that the data is there (as long as transactions are sufficiently confirmed), it is not easily or cheaply or privately accessible in every kind of architecture. In webwallets or clients which rely on indexing services like Electrum, you ask someone else to search the blockchain for you, in exchange for privacy. In full node wallets like Bitcoin Core when running in pruning mode, finding unspent outputs in the chain requires downloading the entire chain again.
So for that reason, I like thinking of a wallet is primarily something that manages a set of spendable coins. It is fed information from the chain, but it doesn't query the chain for every operation - it manages the information itself. In fact, I'd go as far as saying that the set of keys is not necessarily part of the wallet itself (think hardware devices, cold storage, ...) where the actual keys are stored not in the wallet itself.
@_date: 2018-09-16 14:37:02
Not if you have unconfirmed transactions, for starters.
@_date: 2015-06-09 21:41:11
The encoding is something like base58(CONFIDENTIAL_ADDRESS_VERSION + inner_address + checksum), where inner_address is again address_version + 20_data_bytes. This way it supports p2pkh and p2sh, plus potential future addreas types in it.
@_date: 2015-06-18 07:53:10
There were two changes being made simultaneously at the time.
One was switching the raw database backend from BDB to LevelDB. Mike wrote the first implementation here.
The other was the 'ultraprune' rewrite, which changes what was stored in databases (UTXO set copy instead of full transaction index with pointers into the blockchain + per output spentness information), and changed the validation code to use it. I wrote this.
I rebased Mike's patch to switch the database engine on top of the rest of ultraprune's changes, and rewrote it significantly in the process.
Mike is credited in the commit message: 
@_date: 2015-06-09 18:15:40
@_date: 2015-06-09 11:31:34
You are already trusting similar cryptography to verify that coins were actually owned by their sender, in a way that conpletely removes your ability as a human to verify it.
Why do you expect anything else for the amounts being transferred?
@_date: 2015-06-25 23:16:56
For posteriority, the larger cryptosystem that was talking about, is  and released inside our Alpha sidechain (
@_date: 2015-06-09 18:11:21
Yes, coincidence.
@_date: 2018-08-15 20:28:57
To be clear: branching just means that Bitcoin Core maintainers can start merging features for 0.18, while letting 0.17 stabilize for a few weeks.
The final release for 0.17.0 is planned for September 8th.
@_date: 2015-06-09 17:18:24
Just fixed the bug with non-confidential addresses in the GUI.
@_date: 2015-06-10 15:03:42
Using a node can mean many things, including other software that connects to it to get verified block chain or transaction data.
I agree that it would be nice to have an authenticated protocol for SPV clients. There has been some discussion about having node keys for authentication and in addr messages, but no specific proposal or implementation.
@_date: 2015-06-11 17:47:08
Can we please not discuss semantics? To me, a full node is a node that fully validates, whether it's reachable or not. You're welcome to use another definition.
@_date: 2015-06-09 04:03:21
I didn't write the article.
I agree with it insofar that sidechains allow us to experiment more easily with new technology that could eventually offer scaling benefits.
But if you think that we're promoting to push for sidechains as an *alternative* for block size increases, you're wrong. I personally believe that a 1 MB Bitcoin block chain + 19 MB sidechain would be far worse than just increasing the Bitcoin block size to 20 MB. If the sidechain has any intention to be actually used, people will need to validate it just the same as Bitcoin itself, with the same centralization pressure. The only advantage is that it is far less risky to deploy, but it comes at a massive cost in security.
@_date: 2015-06-09 21:34:44
Your chain does anything you like, if you can implement it. Elements is there to show what potential technology exists beyond Bitcoin, without needing another currency.
@_date: 2015-06-09 18:05:53
Alpha does use a distributed consensus system. In fact, it uses something that traditionally has been called distributed consensus, while Bitcoin only had distributed probabilistic consensus. It requires trust, though :)
@_date: 2015-06-10 13:21:07
There are two different issues.
One is the availability and relay of block data on the network. We want enough full nodes to serve this data to other clients, but other nodes will still verify this data. The only reason for having many nodes is to avoid a central point of failure and having fast access. This requires fast nodes with port 8333 available, but I don't care who runs the nodes. This is what is measured by the available full node count, but it's not the most relevant decentralization metric.
The interesting part about Bitcoin is your ability to verify that nobody is cheating. That nobody can spend coins they don't own. That nobody can inflate the monetary supply. That everybody obeys the same rules. You can't outsource that ability. The way Bitcoin provides this function is by letting you run software that validates the system, and letting you base your economic activity on it. If you just run a full node in a data center, and don't do anything with the validation it provides, it's a dumb brick that serves some data to other clients, and not something that matters in upholding the rules of the system.
How many full nodes are enough? One, your own, as long as you use it.
Of course, that does not mean the observed number of full nodes does not matter. It's a good indication of how hard it is in practice to run a node. But it is not a number we should try to optimize at all costs. It is just a side effect of having decentralized validation.
To answer your question: no, a node that does not allow inbound connections hardly contributes to the network, but it may contribute a lot to the decentralization of trust.
@_date: 2015-12-07 21:18:16
Thank you!
@_date: 2015-06-10 13:02:09
I could not agree more with this. The full node count is only a means of measuring how hard it is to run a node. But what we should encourage is *using* one.
@_date: 2015-06-09 18:00:54
I actually did make changes to the UI, but did not test them :)
@_date: 2015-06-11 15:46:29
You need a full node for that. But it does not need to accept incoming connections, can be throttled, and can be pruned (upcoming experimental feature in 0.11).
@_date: 2018-08-12 07:18:37


I believe that needs to be 7'600'000'000/(1163 * 6 * 24), as you're dividing the number of channels by channels per day. The result is 45381 days, or 124 years.
Also, you're off by a factor 4 because blocks are up to 4 million weight since SegWit.


You're closer to 100000 times off.
But really - given that Bitcoin hasn't been around for even 10 years, I doubt we can make large predictions about its state or adoption in 25 years. More technologies (including "layer 2" ones) will develop, and scaling to large numbers of users isn't something we should expect - or aim - to accomplish in a short period of time. Ecosystems need time to develop.
Complaining about perceived fallacies with existing technologies isn't going to improve upon that, though.
@_date: 2015-06-11 15:20:12
A node that does not listen for incoming connections is nearly useless for the network. I believed I said so literally.
However, for a decentralized economy we need more than network connectivity. We need people independently verifying the system.
What good is it to have a whole network of nodes that verify everything (a unique thing in a currency!), but hardly anyone is actually paying attention to that verification, and just trusting some website?
@_date: 2015-06-13 22:57:29
It's even more interesting when you combine CoinJoin + CT. It's possible to do in a way where no participant learns the amounts involved with any other participant, and the resulting joined transaction does not leak linkage, even when using non-equal amounts.
The trick (which I learned from is to let individual participants let their inputs and outputs's blinding factors *not* add up to zero, but only let the their sum add up to zero.
@_date: 2015-06-10 14:29:06
You can make alphad connect to a bitcoind testnet node, to validate the transfers fully. Otherwise, only the SPV proofs for the transfers (which are included in the Alpha blockchain) are verified.
@_date: 2015-06-09 03:56:44
We just published the source code for our first test chain: 
@_date: 2015-06-09 21:27:03
Correct. Furthermore, there is an extension to CoinJoin possible so thay the participants do not even learn eachother's amounts.
@_date: 2018-08-15 21:28:53
If miners create blocks that violate the rule that full nodes want, their chain won't be accepted for payments. Since the miner's payout is in their own chain, they lose their income if their chain isn't accepted.
Again, whether to include segwit or non-segwit transactions (or any transactions) is a miner's personal choice. It has nothing to do with network rules. You cannot make a block invalid because a miner chose not to include your transaction (because you can't prove they intentionally didn't include it). There are incentives for including both (generally: the highest fee paying thing will always be included), but they are independent from network enforcement.
@_date: 2015-06-10 16:33:43
You can't outsource the ability to verify that "nobody is cheating", because the party you are outsourcing to is not nobody.
I haven't said that you shouldn't outsource block chain validation. You may have good reasons to trust others, or not care enough. Not everyone will run a full node, and that is fine.
I'm merely arguing that we should not encourage people to just run nodes and don't use them. That only provides some bandwidth and available of block data to the network, but today, there is plenty of nodes for what the network needs.
A business that runs 1000 full nodes in a datacenter, but uses some centralized API to process their own incoming transactions is doing the system as a whole a much smaller service than if they'd just use one full node of their own to validate incoming transactions.
@_date: 2018-08-16 16:39:41
Major versions introduce new features. Minor versions are just bug fixes.
We're not going to only add new features once a year just because it makes for a nice numbering scheme.
@_date: 2018-08-05 18:27:48
That's fair.
Let me offer another perspective. If there wouldn't have been a way to turn the situation at the time into a joke, it wouldn't have been a BIP, and it would just have been treated as a trivial bugfix. The code wasn't just implementing an unintentional subsidy policy - it was actually invalid C++ and compilers or architectures in the future might have treated it vastly differently.
So I think it's completely acceptable that people treat the entire BIP as a joke; the bug would still have been fixed, and it was.
It's a bit surprising to me that now people discover this piece of history and assume it was the origin of Bitcoin's monetary policy. In 2014 it was obvious that it wasn't, as the 21 M limit was featured prominently in every description of the system, including the whitepaper (without the actual number) and the wikipedia page at the time 
@_date: 2018-08-15 22:09:27
I have no idea anymore what scenario you're concerned about.
Is this about segwit's activation last year? Or something in the future?
@_date: 2018-08-02 23:59:16


No, it refers to using verifiable secret sharing but using MuSig to combine instead of adding the shares from multiple participants together.
@_date: 2018-08-10 18:31:32
Can you make a screenshot or give a timestamp?
@_date: 2018-08-15 21:01:44


From a protocol perspective, I would say it's a solved problem.
All that remains is making it user friendly. That's likely a much harder task, and needs people with very different skill sets and ambitions to accomplish.
But user-friendliness is probably the worst possible reason to choose inferior technology.
@_date: 2018-08-15 20:11:37
Final release for 0.17.0 is planned for september 8th.
@_date: 2018-08-15 21:52:46
Segwit activated almost a year ago. What are you talking about?
@_date: 2018-08-15 20:38:12
@_date: 2015-06-09 03:53:57
This is completely orthogonal to the blocksize debate.
@_date: 2015-06-09 15:10:01
Fair enough. In Bitcoin, it is trivial to verify that the coin supply limit isn't, and has never ever been violated, without trusting any cryptography. With Confidential Transactions, this is no longer the case.
However, if the cryptography underlying guaranteeing authorization of spending is broken, doesn't that make the system pretty much just as useless as one where you can't guarantee the coin supply?
@_date: 2018-08-15 21:21:39
This is a new version of the Bitcoin reference software (called Bitcoin Core). It is not a new version of Bitcoin. It adds new feature, performance improvements, better security, ...
Sometimes changes to Bitcoin's base protocol happen too, but it's a slow and rare process, that requires very widespread consensus.
@_date: 2018-08-05 17:50:18
I didn't even find the bug, [ditto-b did](
@_date: 2018-08-15 21:37:48
I don't see any reason why anyone would ever reject non-segwit transactions.
@_date: 2018-05-22 21:04:47
Transaction delay has not been going down, because reducing transaction delay was not a goal (and almost intentionally avoided).
At least in Bitcoin Core, a number of changes were implemented in the last few years that improve privacy and bandwidth, at the cost of propagation delay.
This is completely different from the changes implemented for block relay, which focused on speed.
@_date: 2018-08-21 17:34:19
This is about the software project Bitcoin Core, which is an implementation of the Bitcoin protocol.
Its website is 
@_date: 2018-08-15 21:08:14
The network is the set of users, who operate full nodes (which includes miners). They jointly decide what blocks are valid. This is done through an "economic majority". This is a bad term, because there is no majority of anything involved; it just means that a sufficiently relevant group of participants will enforce a rule that miners have sufficient incentive to not produce blocks that violate the rule.
See it this way: the network's users decide what rules they demand the network has. They choose full node software that implements those rules. Miners are forced to follow those rules, or their blocks won't be accepted by the network - resulting in them not being paid. When the network's user introduce an extra rule, that's a softfork. A miner activated softfork is one where the activation point (the timing of exactly at what point the new rule takes effect) is determined by miners. A user activated softfork uses a time determined directly by the network. But in both, the enforcement of the rules is done by the network.
Miners are capable of enforcing additional rules that the network does not require, however.
@_date: 2018-08-02 07:02:16


It already is. You can run secret sharing on top of a MuSig combined key. That's even referred to in the Schnorr BIP already ("Furthermore, it is possible to replace the combination of participant keys in this scheme with MuSig").
If you think your scheme adds anything on top of that, you'll need to actually write it down in an unambiguous way somewhere (the actual protocol steps, the actual formulas, listing the different participants, and their communication steps; not "see MuSig" or "= via interpolation"). Then you'll need to convince people to analyse it and try to break it, and you'll probably want to convince someone to write a security proof for that.
So far you've been an amazing timesink for me and others, and despite repeated questions about the scheme and requests to describe it, you've only produced vague and indecipherable but increasingly complicated descriptions.
Until you spend the time to actually explain the scheme in a clear way, I'm no longer responding to you. I know you mean well, but this isn't a productive use of anyone's time.
@_date: 2018-08-27 07:18:15
Just because the noise is bigger, does not mean there is no signal left.
I don't think anyone is disputing that BIP157 has far better privacy properties than the BIP37 approach. I'm personally very happy to see it happen, but it is not a silver bullet that categorically removes privacy concerns and we shouldn't sell it as such. It is just a very significant improvement.
See it this way: inherently, *any* method that only downloads a selection of transactions of blocks, where that selection can be partially observed, will teach the observer something. This is even the case without address reuse - an attacker could see see a few blocks being fetched of transactions that are related through the spending graph, and at least conclude there is now an elevated probability they're related to the node fetching them. To increase privacy you can increase the blocks you fetch randomly, or spread things out over more peers. But these approaches come at a cost, and still only weaken the signal.
Perhaps in the common case, there will not be enough possible observations to teach an attacker something meaningful. But we don't actually know under what assumptions that is the case.
So no, there absolutely still is an information leak. It is just dramatically weaker, and we should be happy about that. But if you actually want no leak at all, you'll either need to download every block, or use cryptographic techniques to make queries to a server without it learning what it was being asked about (private information retrieval, PIR).
@_date: 2018-08-01 19:54:46
The definition of interactive here is whether private information needs to be exchanged.
In MuSig the only information transmitted is the participants' public keys.
I don't understand your scheme, but in any threshold signature scheme I know of the participants need to communicate key shares with eachother through secure channels. If the shares are leaked to the public, the scheme becomes insecure (as the combination of multiple shares can be used to compute the original private key). This makes it interactive and annoying in some Bitcoin-like use cases, because the share information must be securely backed up before the address can be used.
Can you answer the question what the exact combined public key is, for a 2-of-3 with public keys P1, P2, and P3? If that combined key includes P3, how are P1 and P2 able to sign without getting information from 3 on how to undo the effects of his private key in the combined public key?
@_date: 2018-08-05 17:49:16
I'm the author of BIP42.
BIP42 is not where the 21 million supply comes from. If you read any document about Bitcoin's early history the 21 million limit is almost universally mentioned. I guess there are many people these days that didn't know Bitcoin before 2014, but it wasn't that long ago. It seems completely unrealistic to think the Bitcoin ecosystem would ever have accepted such a dramatic change in its economic policy. Having the policy being fixed in stone and beyond influence of a central bank - or anyone - is one of Bitcoin's primary selling points.
Furthermore, BIP42 doesn't even claim that it is the origin of the 21 million limit. It claims (jokingly) that Bitcoin was designed to *look like* it had a 21 million limit, but through underhanded programming didn't actually do that.
The reality is that ever since the January 2009 release of the software, the 21 million limit was there, coded in by Satoshi. In March 2014, "ditto-b" on Github [noticed a bug in the implementation]( in Bitcoin Core (and not in other software), that would result to start another cycle of 21 million bitcoins to be issued 250 years in the future. We took advantage of that situation to write an April's Fools BIP that proposed his fix as an actual consensus rule change (which was correct), but also incorrectly misrepresented reality as that bug being intentional.
@_date: 2018-08-15 20:56:32
Activated in this context means that the network enforces segwit rules. In practice that means that they are secured by the network and safe to use.
It has nothing to do with the (individual miner) choice whether or not to put particular transactions in their blocks or not; that is always their perogative.
Activation refers to what blocks are valid; not what miners like.
@_date: 2018-08-15 16:37:06
For the average user, probably nothing.
For a moderately technical user it will allow using hardware wallets, though the process will still be convoluted. This will likely improve in later versions once there is support for watching BIP32 chains, and GUI integration.
@_date: 2014-02-01 12:19:10
Testnet exists to let infrastructure be tested without risking actual money. It's independent from release and code changes, as those are about modifications to the client itself.
Perhaps you are talking about the unit and integration tests for verifying the correctness of the system. Yes, those pass. We don't accept patches which are known to break any tests.
Don't upgrade production systems until there is a final release. This is just the first version that may be good enough to release, but we need many eyes before being sure about that.
@_date: 2018-08-15 18:47:58
That's not entirely correct.
BIP174 is a standard for interchange between wallet software, hardware wallets, multisig constructions, coinjoin systems, ... It is not a Bitcoin Core specific thing; over time, it's expected that various pieces of software and hardware will implement it.
While it is true that specific integrations have existed for a while (e.g. Electrum has plugins for certain hardware wallets, some devices have their own wallets, ...), they're all specific to the combination. For example, you can't use the Electrum plugin for Trezor in GreenAddress - each piece of software has to separately build support for every piece of hardware.
The hope is that with BIP174, everything just needs a single BIP174 driver to interact. For example, there would be a BIP174 driver for Ledger, and then every wallet that supports BIP174 will be able to get a Ledger to sign things.
Furthermore, it is composable. Say, you want to perform a CoinJoin operation, but one of the keys is in a hardware wallet. To this right now, the CoinJoin system you're using must support hardware wallets. With BIP174, the system doesn't even need to know or care a hardware wallet is involved. It only needs to be able to export a PSBT file, which can be fed to the hardware wallet.
@_date: 2018-05-12 19:49:55
They're right. Segwit is not (on itself) a scaling solution. It was a combination of a protocol flaw fix (malleability) that was required for further technology to build on, with a side effect that also increased the block size. The latter is a simple capacity increase though, not an improvement to scalability.
@_date: 2018-08-21 18:48:50
With a bit of work, you can get version 0.2.10 (released in June 2010) of the software to synchronize with the current network, and use it to receive and send payments. Of course many vulnerabilities have been found over the years which are present in such early versions, and the software would be painfully slow. But you could use such an early version - which is a sign how little strict requirement there is to trust changes that have been made since.
Of course, you can also just run another implementation of the Bitcoin protocol.
@_date: 2014-10-17 21:51:02
Oh, I don't disagree with this. Only that you don't need checkpoints (which lock in a particular chain, but are just used to detect whether we're potentially synced).
@_date: 2014-07-13 19:58:37
There would be around 39% chance for that (birthday paradox). When we reach 2^81 (in around 2-3 months time, depending on hash rate growth), the chance for that would be around 86%. At 2^82, that would become around 99.97%.
@_date: 2018-08-15 23:02:21
WTF? You don't need an old client to support non-segwit transactions.
New nodes accept all types of transactions.
@_date: 2018-08-15 18:37:41
BIP174 is not a network upgrade. It's just a standard for communication between wallets, hardware devices, and other systems that participate in transaction creation.
There is no requirement that the entire world agrees on this. If you like it, use it. If you don't like it, don't use it.
This is different from Bitcoin protocol upgrades that require widespread consensus to safely deploy.
@_date: 2018-08-15 23:59:04
Yes, it is a softfork.
What changed was that transactions with an invalid witness are no longer valid. Before the segwit software, witnesses didn't matter.
Nothing related to non-segwit transactions changed. That would be insane - it would have split the network or crippled the majority of the ecosystem.
@_date: 2018-08-21 21:25:31
`scantxoutset` is just an RPC to query your own UTXO set. You need to be synchronized with the network (otherwise you don't have the UTXO set), and it isn't even particularly efficient (it generally takes a few minutes, which is much faster than looking through the entire chain, but still).
Its goal is to enable certain (low-level, expert) operations to be without letting the wallet handle everything for you.
There is no protocol to get UTXOs from peers, because there is no way to verify that such information is correct. Furthermore, it would be a massive privacy leak.
@_date: 2018-08-15 20:07:33
There is a long discussion on the mailing list about various encodings, not going to repeat it here.
I know several other projects are looking into implementing PSBT, including one hardware device (CoinKite) which implements it natively (its communication protocol is literally PSBT). For other hardware devices there is work on writing drivers to convert between PSBT and their native protocol.
@_date: 2018-08-15 20:32:53


I'm not sure that all blocks include segwit transactions, but certainly most do.


Miners will include whatever is more profitable. If a non-segwit transaction pays more per weight, and there is competition for block space, it would be silly for a miner not to include it.


Yes, most do.
@_date: 2014-02-24 22:57:20
Just to clarify, as it seems there are some misconceptions in this thread:
* This is about the minimum fee for transactions to be relayed across the network, by nodes that do not override their own "fee per kilobyte" setting.
* This is not about mining - miners still choose the highest fees first when building a block. As a result, the fee that is just enough to be relayed may not be enough to get it in a block anytime soon.
* This also doesn't change the fee imposed by the reference client's wallet implementation, as we'll at least need to wait until a significant amount of network nodes have upgraded with this change. Even when that is the case, that minimum relay fee may be too low to get it mined, and you may want to set a higher fee manually yourself.
@_date: 2018-08-15 23:01:14
What? No!
Segwit full nodes enforce segwit's rule. The presence of segwit full nodes makes segwit _secure_. At this point almost every node in the network is a segwit node.
It has nothing at all to do with what miners will include. What miners can include is their own _choice_ and _not_ a network rule. Miners will include whatever is profitable. If that is a non-segwit transaction, they would be stupid not to include it.
@_date: 2018-08-15 20:44:19
The blockchain is not a communication medium.
The blockchain is not a communication medium.
The blockchain is not a communication medium.
The blockchain is not a communication medium.
It's the most inefficient, slow, unreliable, expensive, and unpredictable mechanism for communication known to man. It inherently broadcasts all your data to *everyone*, forcing everyone to verify it. Broadcast is based on hard-to-justify incentives, and hope that it is cheap enough that people won't turn it off.
The blockchain's purpose is permitting everyone to validate the integrity of the ledger, so they can remove trust in third party auditing.
Anything that isn't a fully signed transaction has no place there. If you're jointly constructing a transaction with someone else, you *are already* talking to them, over a far more efficient communication medium (be it a website, email, an NFC protocol, ...). It's ridiculous to force that data onto the entire world because of a perceived difficulty of reusing that communication channel.
Furthermore, broadcasting transactions that aren't fully signed is a major privacy leak. PSBTs contain a lot more information that the public has no business in knowing, for example what key chain the involved addresses are derived from, what is change, what outputs are being spent, etc. Publishing this information would radically destroy any hope for a fungible currency.


That's not how Bitcoin works. If you don't have a valid transaction, your money isn't being spent. Paying upfront for something that may or may not happen is pretty antithetical to that.
@_date: 2018-08-01 07:18:44
I'm not sure I follow.
Say I want to create an output which can be spent by 2 out of public keys P1, P2, and P3.
What is the exact formula for the combined key in your scheme?
If it is something P1 and P2 can sign for without P3's involvement, the combined key must be something that doesn't include P3. The same holds for the 2 other combinations of signers. From that I conclude that any non-interactive-setup scheme that lets two of of three spend must in fact not include any of them, and thus be trivially insecure. What am I missing?
@_date: 2018-08-22 02:23:53
For the first 2, there are no transactions in the chain that exploit the difference in behaviour, so nothing needs to be done.
For the third, you need a BDB configuration file that increases the number of locks.
@_date: 2014-07-15 14:03:02
Around 3 months.
@_date: 2014-10-23 19:00:36
Our compensation package includes bitcoin.
@_date: 2014-10-23 19:39:46
In any case, we'll pay attention to that next time.
@_date: 2018-08-21 20:23:45


Support for watch-only addresses was added in 0.10 (Feb 2015). The change in 0.17 is that you can have a *pure* watch-only wallet without any private keys (which mostly protects against screwups; it doesn't add any new functionality).


I don't know what you're talking about here; there is no such thing.


It's not yet gone, but there is a replacement API available, and disabled by default. In 0.18 it will actually be removed from the codebase.


No, this is only the database controlled by the optional "txindex" option. It enables the `getrawtransaction` RPC; nothing else. This database existed before, but what changed is that it's built asynchronously rather than during validation. It's now also possible to enable or disable the txindex without reindexing the entire chain.


Support for PSBT was added, but this is *not* a network change. You still have to send PSBT files from one device to another using some out of band mechanism. What PSBT adds is that *all* information necessary is in one file, so you don't need to communicate long RPC commands, or information about the UTXOs being spent, or what output is change. Furthermore, it's a standard that will hopefully be implemented by multiple wallets and devices, so you'd instantly be able to have them join signing. I wrote more about the lack of network functionality here: 
@_date: 2014-10-22 17:41:43
See  :)
@_date: 2014-10-12 20:30:02
That's what they do, but not what they should do. If there is ever a time where a checkpoint makes a difference in deciding which chain to believe, Bitcoin's consensus model is already utterly broken. This is one of the reasons to get rid of them: they confuse people's understanding of the security of the system.
They were introduced because they were a necessity - we wanted to disable signature checking for well-enough buried blocks, but as the code did not have any way to look into the future (it processed blocks as it received them), it couldn't know that something had enough blocks on top yet.
With headers-first, we do have that ability, and checkpoints could be removed entirely (not in the current PR though). There is however one (much weaker now) attack still possible, where an attacker just feeds you tons and tons of low-difficulty headers, which would cause memory blowup.
@_date: 2014-04-01 22:13:18
Read the BIP :)
@_date: 2014-10-23 19:24:13
Please see Greg's and my answer to similar concerns in the AMA:
* 
* 
@_date: 2014-07-15 14:31:56
If miners go broke, difficulty will decrease until mining is profitable again. It's a natural equilibrium in the long term.
@_date: 2014-04-01 22:14:11
It has all the answers.
@_date: 2014-03-13 02:15:26
The payment protocol consists of several concepts, one of which is payment requests. Go read BIP 70-72 for more information.
@_date: 2014-08-03 09:32:39
I believe we've already been there.
@_date: 2018-05-06 00:53:58
Oh really? So you're guaranteed to be able to exchange BTC (how many?) for "the Entire weight of the Blockchain"?
Stop this nonsense. Bitcoin is not backed by anything. Neither is fiat.
It has reasons for being useful - but those don't necessarily translate to a guaranteed value.
@_date: 2014-02-24 23:18:28
This is the fee required for letting nodes propagate your transaction. It's DoS protection against flooding the network.
It has nothing to do with the fee a miner might enforce to mine your blocks. As such, this is unlikely to actually affect how much you'll pay for the majority of transactions.
@_date: 2014-07-13 20:21:12
I suggest 2^128.
@_date: 2014-03-19 01:24:10
The built-in wallet deals with malleability (and other forms of conflicting transactions) in 0.9 (since rc2).
Transaction malleability itself is a feature in Bitcoin and won't be "fixed". What can potentially be fixed, is making some types of transactions optionally resistant to malleability (see BIP62). Some of the requirements for that are being implemented (since 0.8 already), but it will require a soft fork to deploy once it is all implemented.
@_date: 2018-05-29 19:54:05
Note that this is just the first release candidate of a bugfix release.
Don't expect any major changes. This is just a maintenance release.
@_date: 2014-08-02 06:34:12
No, and _started_ _with_ 80 heads in a row. Even harder.
@_date: 2014-11-19 22:27:21
I don't believe we can use social pressure to make people use Bitcoin in a way that is against their own will. But I do believe that we can educate people that some short-term viable strategy isn't the best solution for their own interest.
We all benefit from a Bitcoin that scales better - at every point in time, including now, and that means that I'm applying the current design to judge technical solutions. That doesn't mean that the future can't be different.
Specifically, the TXO proposal looks very interesting, and I'm eager to find time to experiment with some implementation. But as long as I haven't seen a practically working system using it, I will assume that that's not Bitcoin's future (for technical or political or whatever reasons), and I will consider UTXO bloat something to discourage. If a time comes where that is no longer the case, so much for the better. 
@_date: 2014-04-08 22:37:58
That would require a longer test cycle before release, and we wanted to get this out as quickly as possible.
@_date: 2014-10-12 20:45:46
It relies on protocol version 31800 (which is where the getheaders command was added), released in december 2010.
@_date: 2014-10-12 20:56:55
Not at all.
Maybe we'll change a few things, but that can be done independently of the core logic change.
@_date: 2014-08-02 00:27:40
I don't know the numbers, but it's hard to compare. One unit of work for scrypt-based PoW is one scrypt() invocation, which is orders of magnitude harder than a double sha256 hash.
@_date: 2014-04-01 22:34:32
Thanks :D
@_date: 2014-07-13 22:52:51
It is 'Yup', but not for the reason you say. It has nothing to do with SHA1 being broken, and it would not require a theoretical attack. SHA1 as it exists today, without any known flaws, only requires about 2^80 hash operations to find a collision.
@_date: 2014-12-15 18:27:53
I very much welcome all positive improvements to the codebase - and I value Peter's insights and contributions even if they come from other coins. I think it's silly that most altcoins don't contribute back, and this is a waste of effort. Viacoin is different, it seems, that is good. But collaborating is harder than forking - let's keep working together rather than turning this into a mud fight.
Also, I rebased CHECKLOCKTIMEVERIFY: 
@_date: 2014-04-01 23:39:04
Hmm, TypeScript is probably the better choice anyway. We should rewrite.
@_date: 2014-10-23 18:36:02
It just ended...
@_date: 2014-10-23 18:31:02
See  please!
@_date: 2014-02-01 12:15:43
This is not correct. See petertodd's answer above.
@_date: 2014-04-02 03:21:30
Disclaimer: I did not find the bug, nor did I implement the patch to fix it. Gregory and I just came up with the idea of turning it into a funny BIP.
@_date: 2014-07-13 18:22:02
To put these numbers in perspective:
* That's one hash for every 1.4 cubic centimeter (1.4 ml) in Earth's oceans.
* To redo the same amount of work on a Core2 Duo E6550 thread would take as long as the age of the universe ("since the beginning of time!").
@_date: 2014-10-12 20:37:22
You should file an issue!
@_date: 2014-04-01 22:39:09
As the BIP says: they would be mining fool's gold.
@_date: 2014-04-07 21:05:56
There's quite a difference between the core protocol rules and the reference implementation.
The latter can get changes in efficiency, reliability, new features, wallet changes, relay policy changes, ... that don't affect the choice of which transactions and blocks are valid. That part is much harder to change (and may affect other implementations too).
@_date: 2014-04-02 00:08:54
A bitshift is just repeated division by two...
@_date: 2014-08-02 12:53:49
Interesting; I didn't know that the Avogadro constant (atoms in one mole) was so close to 2^79.
We passed that many hashes on june 5th, by the way.
@_date: 2014-10-23 15:49:57
I am!
@_date: 2014-07-15 14:22:37
When one yodahash more you have done, look so good you will not.
@_date: 2014-10-13 19:16:15
"could make you believe you are synced on the longest chain, while reality you are stuck on a low-work chain" -&gt; assuming you have at least one honest peer that gives you the right headers, this is not possible.
@_date: 2014-03-13 02:16:55
It's just the _default_ value for the fee/kilobyte value needed to _relay_ transactions. It does not change the fee the wallet enforces (yet).
@_date: 2014-07-13 22:37:38
An N-bit hash function has (at most) N/2 bits of security for collisions, and N bits of security for preimages. This means that finding a collision needs approximately 2^80 hashes for SHA1, but finding a preimage needs approximately 2^160 hashes.
Collisions is looking for pairs of inputs that hash to the same (but indeterminate) result. Preimages is looking for an input that hashes to a given output (or to the same output as another - given - input).
As to why you only have N/2 bits of security, look up the birthday paradox.
@_date: 2014-08-27 18:08:59
Just to be clear: jgarzik (and myself) were against the idea of the patch and opposed it; claiming that only "his own little enterprise" mattered is a bit silly...
Still, when the consensus is that the patch is wanted, and we're just talking about implementation, this was working as intended. Sometimes problems are missed in the first stage, but get caught before any release is made.
@_date: 2014-03-13 16:45:22
But fees also need to be enough for your transactions to be mined. Just paying the fee the network requires to relay it may become systematically too low for that. In the end, it will be miners who set the price, not the software run by the network.
@_date: 2014-10-12 20:21:23
You're welcome!
@_date: 2014-04-01 22:40:31
I'm actually on reddit (and sipa on IRC), but thanks!
@_date: 2014-10-12 20:25:00
It does depend on what you call a full node. If your definition requires it to be able to serve blocks to other nodes, then yes. But it's certainly possible to create a fully-verifying node that downloads and processes every transaction, but only keeps the last X blocks around on disk.
@_date: 2014-04-02 13:58:32
Despite that, the actual current implementation does repeat the subsidy schedule every 256 years.
@_date: 2015-05-07 04:34:02
Removing the limit enforced by full nodes is equivalent to making miners choose the block size themselves.
It has been shown that there is an unfairness possible in that case. More geographically-centralized, better-connected, and mostly higher-hashpower miners have an incentive to create larger blocks than their smaller more distant colleagues, increasing mining centralization pressure even more.
Enforcing a limit in blocks is a way full nodes have to prevent large miners from getting an advantage over smaller ones. And even further: it is way for them to not make themselves irrelevant because they can't keep up.
@_date: 2014-10-23 18:29:23
The AMA was supposed to be here we were told - perhaps there was some misunderstanding with Reddit.
I only noticed too late that it wasn't, but good that someone linked it here.
@_date: 2014-07-13 23:46:47
Bitcoin mining, internally, consists of trying to find a number that - after scrambling the digits in a particular way - results in a very low number. This scrambling operation is called hashing, and the entire network together currently tries around 130 000 000 000 000 000 numbers per second (that's 130 million billion).
In total, on tuesday, we'll cross 1 000 000 000 000 000 000 000 000 hashes performed since the creation of the system (1 million billion billion). It's just a really nice big round number to reach.
However, in a few weeks, we'll cross 2^80 (look at the series 1,2,4,8,16,32,... doubling at each step, and take the 80th number) = 1 208 925 819 614 629 174 706 176. This number is particularly interesting, because earlier cryptographic security systems have assumed that it was impossible for an attacker to perform that many operations. The Bitcoin network collectively, however, will have performed that number (though not for an attack).
@_date: 2015-05-07 04:12:57
I'm a Blockstream co-founder, Bitcoin core developer, have commit access, and am very scared about the block size increase.
I think that the reason why many blockstream people are more against this proposal is because we created a company of like-minded people, in order to pull of a change in the ecosystem. It shouldn't surprise that we have similar ideas about some fundamental things.
If you're referring to sidechains: I believe sidechains are primarily a means for experimentation with new technology without needing to bootstrap a new currency. I don't see them as a direct way to improve scalability. To be specific: I think a 1 MB block bitcoin + 19 MB block sidechain would be way worse in terms of security and centralization pressure than a 20 MB block bitcoin in the first place.
@_date: 2014-10-22 00:55:14
They've tried.
@_date: 2014-10-12 20:50:41
When presented with different competing chains, the client will always pick the one with the most *work* in it, not just the longest one.
As long as you are connected to at least one honest and synced peer, you'll always discover the actual chain.
@_date: 2015-05-07 04:08:15
He created BitcoinJ, which Multibit is based on, but did not contribute to Multibit itself, AFAIK.
@_date: 2014-08-03 09:32:28
No, the total number of hashes done doubled.
@_date: 2015-05-07 05:08:53
You should read petertodd's analysis about the centralization incentives of having miners choose the block size: 
@_date: 2015-05-07 04:23:54
I've also replied on the mailing thread now.
@_date: 2015-05-07 08:55:24
It does not require any malicious intent, or acting against the interest of users - at least not in the short term.
Simply optimizing for profit, miners who are larger and better connected will notice they can create larger blocks. Maybe this does not matter now, but declining subsidy can change things. This is not something users can stop... except by demanding block limits in the protocol rules.
On the other hand, users can at any point start using a fork of the software.
Again: I am not saying miners are malicious. I am just saying that a block limit helps keeping incentives for mining aligned with the rest of the system.
@_date: 2017-02-20 06:56:28
In particular, search for Compact Blocks in the 0.13.0 release notes: 
@_date: 2015-05-07 04:17:48
Or how about we first develop the technology that would be needed to relay blocks quickly, build a test network with larger blocks on real hardware on the real internet, and when it's clear what the trade-offs are, either choose to adopt them in Bitcoin or not?
The core protocol is not complete, and many people are working on improvements. Small performances changes to overhaul of the block relay protocol.
Let's just not rush things because of "OMG blocks are getting full! Bitcoin will break!". If it breaks with 1MB blocks, it will also break with 20MB blocks.
@_date: 2015-05-07 05:13:38
My methodology is: you accepted the rules by starting to use the system that was built on them. You have no reason to assume these rules will change from underneath the system.
If you're asking me for how to gauge what changes do have such pervasive support that they can be changed? I have no good answer.
@_date: 2015-05-07 08:33:03
I stopped mining in august 2011. I am not against competition. I am against mechanisms that result in larger miners getting a larger-than-proportional share, and I believe Bitcoin users have reasons to prevent that from happening.
@_date: 2015-01-23 18:54:47
Right, both are pretty much only ever called 'orphan(ed) blocks' these days, but they are very distinct concept. Also, since 0.10 of Bitcoin Core, orphan blocks (the ones with unknown parent) don't exist anymore, as the client only ever download blocks for which the parent headers have already been downloaded.
@_date: 2015-01-09 00:56:27
As far as I know, those don't implement secp256k1.
@_date: 2014-12-29 00:54:19
That shouldn't matter much, since 0.8. You can always increase the in-memory cache of important data for validation by passing the `-dbcache=N` parameter (or put dbcache=N in bitcoin.conf), with N a number in megabytes.
@_date: 2015-01-10 13:33:20
1.0.1e is fine
@_date: 2015-01-09 14:10:11
See 
It's just part of the repository.
@_date: 2014-10-12 20:42:34
The last checkpoint is only in april 2014.
There can be several reasons, but one might be that the UTXO set cache size was exceeded while you were syncing at that point, so it needs to wait more for disk. Setting -dbcache=N to a larger value (N is in megabytes, default is 100) will improve performance.
@_date: 2015-05-07 04:45:55
Good point to bring that up, though it cuts on both sides. Larger blocks have an impact on the privacy of being able to participate in Bitcoin-the-blockchain as well.
@_date: 2015-01-09 14:09:01
That's the eventual goal, but at this point we're not confident enough it works exactly as OpenSSL does, and this is required if we want to avoid forks.
That's the reason why for now it's only enabled for signature creation. The created signatures are still verified by the old code, so there is very little risk.
And no, it will definitely require recompiling.
@_date: 2015-01-09 15:04:25
Even if that's easy it won't have nearly the same performance. Libsecp256k1 gets its advantages from being very specifically optimized for this curve, and not using any generic code. I believe NaCl does the exact same thing, so at this point getting secp256k1 in there would mean a pretty much full implementation of all the parts (field arithmetic, group operations, scalar arithmetic, exponentiation algorithms) anyway, which libsecp256k1 already has.
@_date: 2015-01-09 15:01:00
Up to 8 times faster.
@_date: 2015-01-09 01:20:34
Why darn?
@_date: 2015-01-09 16:42:11
In particular, we're using random numbers that on average have long sequences of 0 and 1 bits, which are more likely to trigger edge cases and bugs in practice that uniformly random numbers.
So it's not that we're using a different set of random numbers (every number is still possible), it's just that more structured numbers are much more likely.
@_date: 2015-05-07 04:22:44
I think that having a limit below what people would use is very fundamental - whether it was intentional or not. I don't think the specific arbitrary number 1 MB is fundamental.
Quoting from my mail:


@_date: 2015-01-25 05:33:17
Bitcoin Core up to 0.9.x requested blocks whenever it learned about them. However, it didn't know in advance whether it would have the parent for it. If it turned out that a downloaded block's parent wasn't known yet, it would keep the block around in memory for a limited time, and ask for the parents. This was orphan block handing, and the original meaning of the term.
As of 0.10, headers-first synchronization is used, which means that we never ask for a block before we have validated its header, and the headers of all its ancestors. That way, when a block with unknown parent arrives, we can just discard it (it must have been sent in error anyway, as we didn't ask for it).
@_date: 2015-01-12 14:20:14
Good to know. 0.9.4 and 0.10.0rc2 will be available soon which are compatible with new/patched OpenSSL.
@_date: 2015-01-23 18:57:52
Yes, I think it's a lost cause: everyone uses 'orphaned blocks' for at least extinct blocks, and sometimes for both. Also, since the original meaning for 'orphaned blocks' (without parent) is disappearing with the Bitcoin Core 0.10 release, I doubt that will change :)
@_date: 2015-05-07 04:31:30
You understand that the moment miners choose your branch, you'll create a fork in the chain, where Bitcoin-as-it-is will ignore your blocks, allowing pre-existing coins to be spent once on each side?
We're all in this together, unfortunately. As a consensus system, Bitcoin only works if everyone agrees. 
@_date: 2014-11-18 12:27:44
Yes, and I've been against increasing the OP_RETURN size since long before Blockstream.
My largest problem with OP_RETURN is not that people will use it for data storage (which is inevitable); my problem with it is that it's too convenient to use for applications which do not need data storage at all. I would very much like to see a healthy and usable system for out-of-band data transport appear before making it even easier to just use the blockchain as a transport mechanism.
@_date: 2015-05-07 04:55:13
I'm sure there are always preferences that can't be met. I think many people would like to see their BTC balance increase. Sorry, can't do that, it would change the system fundamentally. You would like to see more transactions. Sorry, can't do that, it would change the system fundamentally.
IMHO, the only reason why the consensus rules can ever change, is because the support for it is so pervasive that there is effectively no disagreement, and you should never assume the rules of the system will change because you want them to. The default is to keep them - for better or for worse.
@_date: 2015-05-07 07:46:16
This is me leaving the discussion because it was stressing me out, personally. I was not asking to stop the discussion, but sometimes I am simply not capable of participating abymore.
@_date: 2017-03-21 00:29:24


It's more established, but to me, it refers to the currency, not to the Bitcoin blockchain. No common abbreviation for the latter exists, but if we're to pick one, the shorter, less ambiguous one seems better.


That's just comparing the efficiency of QR's alphanumeric mode with the binary mode. A typical base58 P2PKH address right now uses 272 bits of space in a QR code. A P2WPKH address will take 231.
QR codes are not the only place where addresses appear. UI space for showing addresses matters as well, especially on mobile.


Please do! I'll gladly include reference implementations in other languages (assuming they're readable and pass the tests).
@_date: 2017-03-31 18:44:00
Let me elaborate a bit.
Currently we have (let's say) 59 characters of data in a P2WSH address, of which the last 6 are checksum data. Let's say we have 60 instead.
So our addresses are [54 data bytes][6 checksum bytes] and detect up to 4 errors anywhere in those combined 60 characters.
You're proposing we'd have [18 data][2 checksum][18 data][2 checksum][18 data][2 checksum]. Within each group of 20, no more than 2 errors can be detected (as an N-character checksum cannot possibly detect more than N errors).
Now you have a constructed a code which can detect 6 errors overall (great!), but only assuming there are no more than 2 in each group of 20 characters (not so great). A single code over the whole thing is fine with 4 characters anywhere, including 4 characters within the first 20.
@_date: 2017-03-21 08:55:37
It will take time to get adopted - there is no doubt about that. P2SH (BIP16) took years to get widespread integration in wallets, and while I hope this goes faster, there is no rush.
We need a solution to the fact that our current addresses only have 160-bit security. At some point in the future, that is likely to not be enough anymore. The best time to start working on having that fixed by then is now.
@_date: 2017-03-21 01:42:30
From the BIP:


@_date: 2017-02-21 04:49:21
yow yow
@_date: 2015-05-07 04:19:16
Lightning gives a different security/scalability tradeoff than Bitcoin does, and this is massively needed. Bitcoin only has a one-size-fits-all security, with everyone paying for the security of everyone's transactions. Increasing the block size thus also changes the balance for everyone.
@_date: 2017-03-21 08:56:53
I'll keep doing me.
@_date: 2017-03-21 19:52:17
It depends on what kind of script is used, and what kind of attack model.
Generically, an N-bit hash has N bits security against preimage attacks, and N/2 bits security against collision attacks. We only know of preimage attacks against single-key addresses, so 160 bits hashes give 160 bits security there. For multi-key addresses (multisig, and more complicated things), a collision attack applies sometimes, and a 160 bit hash in that case only gives 80 bits of security, which is considered weak - though still outside the realm of practically attackable.
P2SH and P2PKH outputs are 160 bits in size. This is sufficient for P2PKH as that is single key. For P2SH it is more dubious, as a 80-bit collision attack is a concern there.
Native witness programs are between 16 and 320 bits in size. The specific BIP143 outputs currently supported in the SegWit SF are 160 bits in size for P2WPKH and 256 bits for P2WSH.
So to answer your question:
* Single-key attacks against P2WPKH: 160 bits of security.
* Single-key attacks against P2WSH: 256 bits of security.
* Multi-key attacks against P2WSH: 128 bits of security.
* Single-key attacks against a potential future 320-bit witness program version: 320 bits of security.
* Multi-key attacks against a potential future 320-bit witness program version: 160 bits of security.
The proposed address format supports all of them, which means sending to future witness program versions will be supported by wallets that implement the proposal now.
@_date: 2017-03-21 01:38:57
@_date: 2017-03-21 19:58:30


Maybe, but the checksum scheme does guarantee that two valid addresses differ in at least 5 characters - or at least 6 characters if all the changes are "1-bit changes" (which includes most visually similar character changes).
@_date: 2019-03-18 23:47:38
For starters, Bitcoin doesn't use any form of encryption whatsoever. It uses digital signatures (which fall under cryptography, but not encryption).
@_date: 2017-03-20 23:44:45
I pronounce it as /b/, but I don't care strongly.
That's a normal b, followed by the 'e' from tech, and the 'sh' from sheep.
@_date: 2017-03-21 02:35:29
Wrong again. But perhaps it has something to do with  ?
@_date: 2017-03-21 01:10:18
I think you misunderstand.
I am not saying that there is a second step of error detection applied after doing error correction. That's nonsensical, as you say. After error correction, you always have a valid codeword, and there is nothing to detect anymore.
It's about preventing the (single) error correction step from accidentally correcting to the wrong thing. There is no error detection after that. There is only a restriction on how many errors you seek to correct, and _by doing so_ being able to detect more, as the correction step will fail to find a correction in more situations.
@_date: 2017-03-21 20:24:57
Yes, 256-bit outputs are obviously going to be larger than 160-bit outputs, but they also come with an increase in security. Furthermore, SegWit still has a 160-bit output type (P2WPKH).
@_date: 2017-03-21 01:05:53
Read the BIP.
Testnet addresses will use the "tb" prefix, and the prefix is included in the checksum calculation.
@_date: 2017-03-21 01:29:57
P2WSH outputs contain a 256-bit hash of the redeem script, as opposed to P2SH which uses a 160-bit hash of the redeemscript.
Without an address format that lets you create such P2WSH natively, you're forced to use P2WSH-inside-P2SH, where the output still only contains a 160-bit script hash.
This proposal lets wallets construct addresses for native 256-bit witness script hashes.
@_date: 2017-03-21 03:37:24
Old addresses remain forever valid.
@_date: 2017-03-31 04:07:09
If you mean that each of the 1-2 checksum characters depend solely on the characters in the chunk before it, absolutely not. The current code detects 4 errors _regardless_ of where they happen, including the case where all of them occur in whatever chunks you'd split the address into. 2 checksum characters for each chunk could not accomplish that.
If you mean just having the current checksum characters be distributed throughout the address rather than all be at the end, the result would have very similar properties.
@_date: 2017-03-21 08:58:46
Future address types can use a 3-character prefix too, as it's variable length.
@_date: 2017-03-21 00:14:24
A code with minimum distance d (the minimum number of characters in which two different valid codewords differ) is able to detect D errors and correct E errors as long as D + 2*E + 1 &lt;= d.
The code used in Bech32 has a minimum distance 5. This means it could be used to:
* detect 4 errors
* correct 2 errors (meaning up to 2 errors is safe)
* correct 1 error and detect 2 errors (meaning up to 3 errors is safe)
We avoided using technical language in BIP text, but the important part is that you should not implement error correction. If you do, you're reducing the total error-detecting capabilities, so I think that saying that it erodes error detection is fair. This problem does not exist as long as you only point out where the errors may be, as you're still forcing the user to go check again.
@_date: 2017-03-20 23:55:10
The 'bc' just stands for "BitCoin". We felt that "btc" was too long, and the most obvious 2-character abbreviation is "bc".
@_date: 2017-03-21 01:50:14


Arguably no, pay-to-IP addresses did not. &lt;/ducks&gt;
@_date: 2017-03-31 01:30:54
@_date: 2017-03-21 19:59:29
Haha, great. I believe you should write a BIP about that, and publish it 11 days from now.
@_date: 2014-07-13 20:02:57
We passed Avogadro's constant as total number of hashes with block 304295, on june 5th 2014 (with  6.0225 * 10^23 hashes in total).
@_date: 2017-03-21 19:26:58
@_date: 2017-03-21 00:36:39


I see what you did there.
@_date: 2017-03-22 21:17:03
Send to P2SH/P2WSH with redeemscript OP_RETURN. Those are guaranteed to be unspendable.
@_date: 2017-03-21 02:29:34
Thanks, it was a lot of fun to work on though! (and we may have overdone it **little** bit here or there in analyzing things.. but the result is still simple).
@_date: 2017-03-31 01:22:34


It does. Segwit works fine with P2SH. But at some point we'll need something more.


It's completely orthogonal. Nodes and miners don't need to care about Bech32 at all (it's not a consensus or P2P change). Only wallets care, and several developers were blocked by not having native addresses available - some of which were already coming up with their own incompatible address formats instead.
@_date: 2017-03-21 05:47:49
Pay-to-IP was literally just "connect to an IP address, ask them what scriptPubKey to use, and then construct a transaction using that".
There were no separate pay-to-IP **scripts**; it was just whatever script the other side told you to use. You couldn't distinguish them from others, because on the chain, they look identical. If someone were to be using pay-to-IP today, their transactions would work fine.
However, support for pay-to-IP **addresses** was deprecated and removed a very long time ago. It was disabled by default in Bitcoin (Core) in september 2010, and removed in september 2011.
@_date: 2019-03-04 16:23:12
You can enable pruning at any time, but once pruned you can't go back to unpruned (without starting over).
You can change how much to prune, in either direction, at any time.
@_date: 2019-03-04 19:52:15
The slight offset (600.6s instead of 600s on average under constant hashrate and with honest miners) is relative hard to fix, but also almost negligible in impact (the normal random variation of block lengths due to changes in hashrate is an order of magnitude larger than the 0.1% offset as a result of this off-by-two).
The timewarp issue can be fixed with a minor softfork.
@_date: 2017-03-31 01:29:29
Bech32 is designed to be (somewhat) generally usable, so please do.
@_date: 2019-11-30 20:33:50
You're really in a mindset of a solution looking for a problem. You're trying to rationalize how a blockchain can be useful to solve this problem. It can, in some ways, in the same way that you can use a helicopter to go to the shop around the corner. There are better technologies that solve more of the problems.
If you start with the actual requirements for voting, and then design a solution that meets those requirements you will end up with a completely different solution. That's what the author of the blog post I liked to above did, by the way.
@_date: 2017-03-21 08:51:00
We're using an alphabet of size 32, since there are 32 different characters that make up the data section.
We could use a Reed-Solomon code with an alphabet of size 32, but it would be limited to strings of length 31. As for addresses we need significantly more than that (39 for P2WPKH, specifically, and up to 71 for potential future types), Reed-Solomon codes are not usable here.
@_date: 2019-03-04 16:51:45
Every 2016 blocks, the difficulty is adjusted in the hope of keeping the average time between blocks at 10 minutes.
To do so, blocks are treated as being in groups of 2016, all of which share the same difficulty.
When we're between the last block of one group, and the first group of the next group, the difficulty is multiplied by (2 weeks)/(timestamp difference between the last and the first block in the previous group).
There are a number of things wrong with this.
* The last and the first block of the previous period are only 2015 blocks apart, not 2016. This makes it seem ljke the algorithm is really rargetting 2015 blocks to take 2 weeks rather than 2016. (2 weeks)/2015 is 10 minutes and 0.3 seconds.
* Due to a statistical oddity, there is an additional bias in this correction mechanism, causing it to really target (2 weeks)/2014 or 10 minutes and 0.6 seconds. You could say there is really an off-by-two error here, but this one is much more subtle. I wrote more about this here: 
* Because the timestamps of the different adjustment windows do not overlap there is a potential for a known vulnerability, the timewarp attack. This related issue results in the ability for a majority of miners to accelerate the block reward almost arbitrarily (making us bring the 21M BTC in circulation faster). So far, this has not been exploited on Bitcoin, but it was described as early as 2011: 
@_date: 2017-03-31 04:02:08
Thank you!
@_date: 2017-03-21 00:02:45
The checksum effectively implements a BCH error-correcting code. While implementing the correction is nontrivial, error detection is very simple (which is the only necessary thing).
@_date: 2017-03-21 19:26:31
No, there isn't. If there is demand I can maybe create a small demo utility, but you shouldn't need it for production purposes.
The contents of witness programs is specified in BIP 143, and a segwit complient wallet will need to implement these anyway.
For a P2WPKH output, the witness version 0, and the witness program is the 20-byte RIPEMD160(SHA256(serialized compressed public)).
@_date: 2017-03-31 08:19:13
It's unfair to say it's state of the art - none of us are experts in coding theory. We did a bit of research on how to efficiently analyse many of these codes, but apart from that, it's mostly engineering and a bunch of computing power. It was a lot of fun to work on, though!
@_date: 2019-11-25 20:45:17
Right, that makes sense. But that also means that flushing to disk may take several minutes or more. If you exit the software while it's flushing you'll end up with an inconsistent state on disk that needs recovery. At such large sizes, unfortunately the recovery is usually slower than just starting over.
@_date: 2014-10-12 20:36:25
1) The download is from my website.
2) The binaries are GPG signed by me.
3) You shouldn't assume the binaries are safe, or that I'm trustworthy.
@_date: 2017-03-31 01:12:57
The problem that needed to be solved was that SegWit (eventually) needs a new address format. It isn't urgent, and adoption will take a long time anyway. But if we're going to design something new anyway, why not take some time to make it as awesome as possible?
I will gladly admit that much of this is overkill, but I don't think it is wasted effort. I also don't think the amount of time spent on it negatively impacts the result, which is surprisingly simple to use.
In an ideal world, none of these things matter, and no human would ever see cryptographic material. But the world is not perfect, and addresses are being used. And as long as they are used, they are a cause of potential loss of funds.
Also, Bech32 addresses are smaller inside QR codes than existing addresses, so it at least is an improvement for that use case.
@_date: 2019-11-24 22:31:43
Bitcoin Core, as a matter of policy, does not rely on any trusted third parties. As there is no way to obtain exchange rate information without those, it's unfortunately not possible to give a USD balance.
@_date: 2019-11-14 09:14:25
More like 0.001 USD per day, and that's with some pretty have GPUs already.
@_date: 2019-11-25 19:56:04
Do you have to have a large dbcache setting in bitcoin.conf (or the database cache setting in the GUI)?
@_date: 2019-11-29 00:38:50
Schnorr does not provide any privacy on itself. It enables a bunch of things like more compact (and in a way also more private) multisig policies on-chain. It's also the basis for things like simple atomic swaps through adaptor signatures. But regardless, these technologies only have an effect for those who use it. We can't assume that that will be the case, and in any case will be very application-specific.
Taproot is a privacy improvement, but it's hard to compare with Monero or Zcash. Those designs are intended to hide transaction linkage (seeing which outputs spend which inputs). Taproot does not hide this information at all, and bringing anything like that to Bitcoin would be very hard. What Taproot does improve is policy privacy: right now it's obvious on chain which transactions are multisig wallets, which are Lightning transactions, ... just because the Scripts used reveal all that information. Taproot helps by not revealing scripts at all (or even the presence of scripts) in many transactions. This makes normal payments and more complicated things indistinguishable in many cases.
@_date: 2019-11-08 00:36:22
I think you're missing that I'm just making a joke :)
@_date: 2019-11-10 23:07:51
It's primarily a review of the BIP drafts, not code. There is reference code, but that's primarily for demonstration purposes at this point. There will be plenty of time for review of the actual code once it's clear what the exact semantics for Taproot are that we want (and if we want it at all).
@_date: 2019-11-11 19:10:04
Does FIPS-140 even permit/include `secp256k1`-based cryptography?
@_date: 2019-11-04 16:24:29
This is wrong and confusing.
@_date: 2019-11-02 19:12:28
This is what Bitcoin Core does currently since a few years:
The problem is split up in two; one is the maintenance of the mempool during normal operation. The other is constructing a block from the mempool.
Conflicts are dealt with in the mempool: the mempool is kept consistent with itself and with the tip of the chain at all times (meaning that it is always possible at all times to construct a sequence of blocks that confirms everything in the current mempool). This implies that when a conflicting transaction is received (with another unconfirmed transaction), it will only evict the existing one if (a) that transaction has the BIP125 flag set, and (b) the feerate of the new transaction is higher than the average feerate of all conflicting transactions combined and (c) for DOS protection, the fee of the new transaction is strictly larger than the fee that was evicted (to prevent turning the P2P network into a free relay channel for everyone). If these conditions are not met, the new transaction is simply ignored.
Then, block construction. First, all transactions are sorted by average "ancestor feerate". This means that for every transaction, its "package" is computed as the set of all mempool transactions directly or indirectly depended on by the transaction being considered. The ancestor feerate of a transaction is the defined as the fee of the whole package divided by the weight of the whole package. Then, the mempool is sorted by descended ancestor feerates. If a child has higher feerate than a parent, it will sort higher than the parent generally. If a child has lower feerate than a parent, the child will generally sort lower. Then the process iteratively picks the highest ancestor feerate transactions from the mempool, removing them (together with all their ancestors) from consideration and adding them to the candidate block. While doing so, the ancestor feerate of all other transactions that share a parent with the included transaction are updated (e.g. if you have A with children B and C, and B is included together with its parent A, C is updated to no longer have A in its package). This continues until the block is close to full; if adding a package would exceed the limit, it is skipped (we don't go back and reconsider earlier ones).
@_date: 2019-11-08 00:27:49
Buying mining equipment?
@_date: 2019-11-16 00:52:12
Literally every altcoin (hundreds, if not thousands) did this. Forkcoins are a relatively new phenomenon. The fact that almost none of them are even talked about anymore should tell you something.
Ever heard of Litecoin, for example?
@_date: 2019-11-08 00:33:45
Lol, I may have a dusty BFL Jalapeno somewhere, but I probably got rid of it.
The point I'm trying to make is that the most obvious form of gambling with BTC is investing in mining.
@_date: 2017-03-31 01:23:56
Will be fixed soon.
EDIT: fixed now
@_date: 2019-11-24 22:52:26
Because it is never a good idea to complicate implementations just for the sake of dealing with something that will never happen.
@_date: 2019-11-06 00:41:55
This sounds perfectly fine.
@_date: 2019-11-01 18:49:26
It will automatically do that.
Just don't specify -reindex a second time, or it will start over from scratch again.
@_date: 2017-03-17 19:42:35
It's the question that drives us.
@_date: 2017-03-21 00:50:37


But not necessarily the right one.


Yes, that too, but that's not what I'm talking about.
The formula explains how many errors can safely be made (in addition to the number guaranteed to be corrected) before there is a risk of the correction algorithm correcting to the wrong (but valid) codeword.
Assume we're going to correct up to 2 errors. The D + 2*E &lt;+ 1 = d formula tells us that in addition to this correction, no more errors can be detected. Imagine someone makes 3 errors. Given d=5, there may exist a 2-error correction that brings it to a valid codeword, in which case we're made an invalid correction.
Now assume we'd only correct 1 errors. According to the formula, we can detect an additional 2 errors. And indeed, If you made 3 errors, the distance 5 guarantees that you cannot find an accidental 1-error correction that results in the wrong but valid code.
@_date: 2017-03-31 01:23:06
Well this is SegWit + EC (error correction) :p
@_date: 2019-03-04 16:35:46
A few points (all assuming recent Bitcoin Core versions):
* When you're just talking about the block data, a pruned node and unpruned one write almost exactly the same amount of data to disk (as pointed out in this thread already).
* The blocks on disk are often actually not the majority of what is written; the UTXO chainstate database is. As unspent outputs get created and deleted all the time, this causes many writes to disk. You can reduce the amount written by setting the `dbcache` setting higher (up to a few gigabytes is useful; the setting's unit is in megabytes).
* Due to a long standing issue (I need to check whether it is still the case), when pruning is enabled, the database cache is flushed (much) more frequently, increasing the amount of database writes.
* Modern SSDs can tolerate so many rewrites this should not be a problem.
@_date: 2017-03-21 01:24:17


Perhaps I expressed that bad as well. What I mean is that you choose to not correct when you can't find a valid 1-error correction, and fail instead.
@_date: 2019-11-12 01:44:53
If you wonder if it's a scam, it's a scam.
@_date: 2019-11-05 03:00:44
You can run Tor and connect to Bitcoin hidden services. Tor traffic goes over port 443, the same as HTTPS.
@_date: 2019-03-04 16:30:18
This is correct.
@_date: 2019-11-17 17:42:39
Yes, it was found just after 0.19.0 was tagged. There should be a fixed 0.19.0.1 soon.
@_date: 2019-11-03 05:09:18
What I meant is that if the child has higher feerate than the parent, the package containing both parent and child will sort higher than the package of just the parent (remember: packages are defines by a transaction and all its direct and indirect ancestors) and they'll be included together (with the parent of course still placed earlier in the actual block), or not at all. However, if the parent has a higher feerate, it individually will sort higher than the package, and thus will be included independently. When that happens, the child's package will be adjusted to not include the parent anymore (as it was selected already), and reconsidered later individually.
@_date: 2019-11-01 16:11:31
Disable your swap. Whenever during syncing swap gets used, your performance will be completely gone.
Rather, reduce your dbcache to the point where you don't need the swap.
@_date: 2019-11-06 00:48:10
It really depends on how well other nodes know you, and like you. It's hard to reason about.
Note that there is basically no benefit to you to having multiple incoming connections. It's a service you offer the network only.
@_date: 2016-06-02 22:23:29
In Alpha, they are in the witness (but witnesses aren't discounted).
@_date: 2019-11-29 02:22:03
Taproot provides this (under some, but fairly wide, conditions). Schnorr does not.
@_date: 2019-11-02 23:38:33
"Unlikely" is irrelevant. You don't care about probabilities; you care about the cost for an attacker to exploit it. And it's trivial to produce long chains of dependent transactions with complex dependencies.
@_date: 2019-11-23 19:34:18


I see what you did there.
@_date: 2019-11-03 15:51:09
Ah, I assumed you were arguing that nothing needed to be done to prevent too much complexity to construct the optimal block.
On a high level you're correct in how (approximations of) optimal block proposals are constructed. However, if you would simply ignore the overhead of complicated dependency graphs between unconfirmed transactions, it would be trivial for an attacker to construct a sequence of transactions that would DoS the mempool maintenance code.
If instead you put some limits on the number of descendants/ancestors every transaction can have, it is easy to avoid these worst case situations, at the cost of slightly suboptimal blocks being constructed.
@_date: 2019-11-30 16:07:32
This is a very common idea for some reason, but the technology really does not address any of the actual open issues with online voting: 
Of course, that's assuming online voting is desirable at all.
@_date: 2019-11-23 21:33:12
Perform multiple payments with one transaction.
@_date: 2019-11-12 00:01:06
Decent desktop CPUs can do 10s of millions of hashes per second (per 100 W).
High-end GPUs can do 100s of millions of hashes per second (per 100 W).
ASICs can to trillions of hashes per second (per 100 W).
It's fair to say that ASICs are in the order of 100000 times faster than CPUs for Bitcoin mining.
@_date: 2018-10-28 19:22:33
A big nasty flashing red warning "Using this site, or any block explorer, to verify whether transactions are real places an unreasonable of trust in the maintainers of those sites and your internet connection. Use only for debugging purposes."
@_date: 2019-11-16 03:08:57
Taproot makes all outputs, and cooperatively spent inputs look the same.
We don't really have any technology to make all transactions look the same.
@_date: 2019-11-29 05:12:13
Lightning in some ways improves privacy because it moves payment information off chain.
That does not mean that privacy is suddenly a solved problem, not by a long shot. Some information is public on the lightning network instead, some information is still revealed on the chain, and disputes which adversaries can force you to resolve reveal even more.
There is a very large difference between "technique X may improve privacy, depending on how it ends up being used, and depending on what information you consider private" and "X is private".
@_date: 2019-11-23 19:25:30
Short answer: for all intents and purposes this is not an issue.
Longer answer: it's complicated. Deriving a key from a master key in BIP32 is not guaranteed to work at every specific index (but the probability for failure is cosmically small; around 0.000000000000000000000000000000000000373%). BIP32 specifies that in case derivation fails at a specific index, implementations should just proceed with the next index. So perhaps it is possible that someone constructs a seed such that `m/44'/0'/0'/0/313` does not construct a valid key, but they'd proceed by using `m/44'/0'/0'/0/314` instead. In practice, this will never happen (the probability is so small we can ignore it), and likely implementations don't even bother checking the condition.
Source: am BIP32 author
@_date: 2019-11-22 16:59:01
A bug was found in 0.19.0 just after it was tagged,  ut before it was released.
The bug was then fixed, and a new 0.19.0.1 was tagged. We're now going through the release process for 0.19.0.1, which will be out soon.
@_date: 2017-03-20 23:50:44
Another design consideration was implementation simplicity. While the work that went into constructing the checksum algorithm was substantial, the result is surprisingly simple (the actual computation part is 9 short lines of Python code).
@_date: 2019-11-24 21:38:50
Where do you read that?
@_date: 2019-11-30 20:28:16
I think you're missing the point.
There are lots of interesting cryptographic techniques that may to larger or lesser extent be useful for voting privacy. I'm sure you can construct something out of MW, or related techniques.
The point is that you can use these without a blockchain. To go into your example, take MW, but instead of a blockchain or even blocks, just construct a single transaction on a server run by the entity running the elections. You need such an entity anyway (for voter registration or otherwise guaranteeing people's identities don't vote twice), it is auditable (they can publish the final transaction in the end) and the alternative is effectively giving censorship rights based on money (pay miners enough for the duration of the vote) and far less efficient in general.
That doesn't mean you even need as much as MW provides.
Voting is simply not something that needs a consensus algorithm. The only feature provided by a consensus algorithm you're using is publishing, which can often be done in much simpler ways. While there are similarities between those hiding techniques and some things done in certain blockchains, that does not mean the whole package is what you need.
@_date: 2018-10-20 06:11:18
With low dbcache, the UTXO set is continuously rewritten on disk, at a much faster rate in general than blocks.
The larger you make the dbcache, the fewer disk writes are needed.
@_date: 2019-11-29 01:01:56
That will very much depend on the actual prevalence of such indistinguishable atomic swaps.
There is a very big difference between not being entirely sure that linkage between coins exists, and not being able to even reliably guess. If atomic swaps are a tiny fraction of transaction volume, an assumption of linkage will still be almost always correct.
So yes, I think that's an overstatement, or at least very premature to claim.
@_date: 2017-08-24 02:20:20
That's my transaction, but it is not the first.
@_date: 2018-10-28 17:52:39
Originally, because the software (intentionally or unintentionally) did not add the genesis block to its internal database, and as a result that software would not treat a spend of them as valid.
Later that rule was made explicit, to make sure it wouldn't be unintentionally changed. Changing this would be a hardforking change.
@_date: 2017-08-30 23:02:41
They still don't serve historical blocks, though there is a proposal for adding that. They will relay new blocks.
I don't think it matters that much though; there are plenty of nodes that serve the chain to others.
@_date: 2019-11-01 04:29:11
Check out Green.
Disclaimer: I work for Blockstream which created the Green wallet and operates the service it uses.
@_date: 2019-11-28 04:47:37
Version 0.1.0 allowed anyone to steal any coin, or print money out of nothing. Those things took over a year to be discovered and fixed.
Of course, those were logic errors, not typos. But typos are trivially fixed by addressing compilation errors.
Satoshi invented something amazing, and thanks the work he did to follow through with it to actually make it work in the real world we all still get to enjoy it. But it was not, in any way, a flawless masterpiece.
@_date: 2019-11-21 21:32:20
Pretty much.
Disabled opcodes are not a standardness thing though; they effectively don't exist in consensus.
@_date: 2019-03-04 16:04:19
These already exist as keychains: 
(disclaimer: I'm not associated with the seller, but received one for free once)
@_date: 2019-11-24 18:35:10
You can even argue it's not a bug, but a deficiency in the specification (if I'd write BIP32 today it wouldn't be there).
Choosing to implement it would effectively be a layer violation for many implementations. The chance that doing it "right" introduces a bug is many orders of magnitude larger than what following the spec gains you.
This is not uncommon in cryptographic protocols, I believe. Often they specify how to deal with certain conditions out of an abundance of caution, while it can easily be shown that the condition is effectively unreachable. And unfortunately, without very deep and invasive stubbing, cryptographically unreachable also means untestable.
@_date: 2018-10-28 17:50:50
You can't observe what private keys are destroyed, of course. There is no network communication involved when someone loses a key.
However, Bitcoin Core does have `gettxoutsetinfo` RPC call that gives you statistics about the UTXO set. One number is the total amount of BTC in spendable (or at least not provably unspendable) addresses.
@_date: 2017-08-31 06:55:46
One approach is the one you mentioned, where we generate a random 3072-bit number from each UTXO, and then multiply those (modulo a fixed 3072-bit prime) to produce a hash. UTXOs can be removed from the hash again by dividing them out.
The other approach is to associate an elliptic curve point with each UTXO, and then adding them together, and subtract them to remove them again.
The trade-offs between the two are complicated:
* Generating elliptic curve points from UTXOs is much slower (5us for an EC point, vs 1us for a 3072-bit integer)
* Adding EC points together is much faster (0.2us for adding EC points, vs 1.5us for multiplying 3072-bit integers)
* Computing the inverse of an EC point (for subtracting) is trivial (1 bit flip), while computing the inverse of a big integers is many times slower than a multiplication.
All of this boils down to the fact that using integers is much better for raw throughput (we can compute the effect of more UTXO operations per second), but using EC operations is more easily cacheable. Since block propagation is highly dependent on pre-validated transaction already anyway (compact blocks, FIBRE, signature validation cache, ...), maybe raw throughput shouldn't be the limiting factor.
@_date: 2017-08-30 23:05:05
It's not released yet. You can find the URL to release candidate 3 (which will become final if no more issues are found) in this thread.
@_date: 2019-11-06 00:44:35
Any number between 0 and 125 is normal (including 0).
The number usually goes up slowly as you're available for longer on the network, as your IP address will be present in a growing number of nodes' IP address databases.
@_date: 2017-08-25 16:23:01
I used an approach that avoided the risk, using two transactions. The first was a non-SegWit transaction locktime'd to the first SegWit block, which created a SegWit output. Only the second transaction spent that output. Both transactions got mined into the first SegWit block (as intended).
@_date: 2017-08-31 19:04:57
Bitcoin Core has a release cycle of several months for major versions, and at least a few weeks for minor versions.
The plan is to switch to SegWit addresses by default and fully support it in the UI once SegWit has activated. Unfortunately, activation came too late to make that happen for 0.15.0 whose feature set was already frozen before, and is expected to be released in 2 weeks. The release after 0.15.0 will support SegWit fully.
There is an experts/test interface available since 0.13 (`addwitnessaddress` RPC) which works, and is used for testing, but it's not nearly what I'd call full wallet support.
@_date: 2017-08-31 18:55:28
We have two databases.
* $DATADIR/blocks/index stores the disk location of every block, some metadata about blocks, and (if enabled) the transaction index.
* $DATADIR/chainstate stores the UTXO set and the block it corresponds to
Arguable, the chainstate database is far more important than the block files. A full node cannot operate without it, while (in pruning mode) the actual blocks are optional.
@_date: 2017-08-24 02:26:57
Exactly. I was trying to be first :)
@_date: 2019-11-30 15:44:11
Indeed, it means all signatures will be checked.
Are you synchronizing from random peers on the internet? If so, that can be the reason for variance in your measurements.
@_date: 2019-11-06 01:05:52
Right, of course. But I mean there is no problem if for whatever reason other nodes do not choose to connect to you.
You may care that you're reachable, but for that, it's sufficient that you occasionally see one incoming connection.
@_date: 2019-11-21 08:33:29
There are certainly policy rules that apply to inputs as well, but they're either for extension mechanisms (OP_NOP, future witness versions, ...), malleability protection (e.g. ECDSA low-s rule), or resource limits (max input size for example). There are no template-based whitelists for inputs though.
@_date: 2017-08-31 02:42:11
If you upgrade, and want to downgrade again to 0.14.x afterwards, there are two possibilities:
* If you're running in pruning mode, you'll need to start over and download the chain agan.
* If not, you can use `-reindex-chainstate` to recreate the old format chainstate database without downloading again. This may still take several hours or more - it's basically redoing all the work done during synchronization, but using the blocks you already have on disk.
@_date: 2018-10-21 01:37:02
Bulletproofs reduce the size of range proofs.
Bitcoin does not have range proofs.
So, no.
@_date: 2017-08-24 04:46:08
Where do you see 2 witnesses?
@_date: 2017-08-24 07:17:17
Double fail. The first SegWit block was 481824. Sorry :p
@_date: 2017-08-24 08:07:21
I'm more than happy that SegWit got activated on the network, and that the first block with the rules enabled actually included several SegWit transactions.
@_date: 2017-08-24 08:06:19
Bech32 has error _locating_. It can tell you where the errors are, not just that something was invalid.
@_date: 2017-08-31 18:58:05
I wouldn't call `addwitnessaddress` experimental, but it certainly is an experts-only feature.
@_date: 2017-08-30 23:02:50
Mid september.
@_date: 2017-08-31 17:22:05
If we use the most efficient representation for an EC point that's currently used in libsecp256k1, it's 1024 bits for a point (320 bit for each coordinate, and storing the point in Jacobian coordinates X, Y, Z, plus an infinity flag).
And indeed, the inverse in case of modular multiplication would be amortized. You'd store the actual result as a fraction D/N, where added UTXOs get multiplied in D, and removed ones get multiplied into N. Only when you need the actual value of the hash, you'd compute N^-1 * D. The effect of this is that your cached hashes (for example for prevalidated transactions) need to be 6144 bits each, which is significant.
@_date: 2017-08-28 05:26:17
The software I'm using does about 2M tries per second of CPU time. You must be off a bit somewhere :)
@_date: 2017-08-24 08:07:54
Just took two years of CPU time...
@_date: 2018-10-20 06:12:34
It's kept in memory while the node is running, but written to disk at shutdown and read back at startup.
@_date: 2017-08-26 02:54:31
Please don't confuse MimbleWimble (which is a technology) with grin (which is an altcoin that uses MW).
@_date: 2017-08-24 17:40:41


Yes, though that only took 3 days on a 6-machine compute cluster (152 cores, 304 threads).
@_date: 2017-08-24 18:45:27
The only way to get the SegWit benefits is by spending coins sent to a SegWit address (P2SH or native).
`addwitnessaddress` does not let you spend coins sent to the old address using SegWit. Also, getting the equivalent SegWit address for an address you've already used would be trivially recognizable as reuse, leaking some privacy.
@_date: 2019-08-13 08:43:23
Something that proposes a change to be adopted by the entire ecosystem (or a substantial portion of it), is exactly what BIPs are for. Only a small number of BIPs propose consensus rule changes. For example BIP176 proposes standardization of the term "bits" for 0.000001 BTC.
Of course, this is just silly (also, you don't get to pick a BIP number yourself).
@_date: 2018-06-28 09:07:33
One step at a time.
BLS is slower, and relies on stronger cryptographic assumptions than Schnorr and ECDSA. Simply put, that means there is less reason to believe it will remain secure over time. It's not "less secure" - there are just more ways it could theoretically be broken. Sticking with the existing assumptions is the more conservative option. Furthermore, it is presumably easier to find consensus about when the ecosystem doesn't need to take additional risks into account. 
Of course, a softfork could be proposed that introduces both Schnorr signatures and BLS signatures (at once, or separately), leaving the option to the users (which would be decided at the time you construct an address). This has two problems. One is: what advice do you give? It's generally a bad idea to offer a choice (especially a security critical one) when there is no clear way to distinguish them. The second problem is that for privacy reasons we really should aim to have every transaction output look indistinguishable. Unclear choices unnecessarily hurt this, as your choice of outputs reveals information about you - like what software you're using.
On the flipside, BLS (and related schemes) offers some possibilities that are far beyond what Schnorr like schemes can do. This includes far stronger aggregation, and recent work has reduced the cost of BLS multisignatures to one pairing per distinct message rather than one per signer. These are very exciting developments, but from my perspective, we should first focus on thoroughly understood schemes which have the highest likelihood of being acceptable.
And Schnorr vs. ECDSA is really a nobrainer - Schnorr comes with the exact same assumptions as what we already have, but can be leveraged to build compact threshold signatures, adaptor signatures, and certain forms of aggregation.
So: one step at a time.
@_date: 2019-08-26 06:40:24
This is the correct answer.
You don't need to be just worried about the private key being highly visibly put in the transaction directly. There are far more sneaky ways through which a malicious signing device could exfiltrate its key material.
@_date: 2017-08-24 07:17:59
No, the first block with SegWit active is block 481824.
@_date: 2019-08-03 20:53:02
An antminer S9 does 13.5 TH/s. 7 of them do 94.5 TH/s. At that combined hashrate, you'll mine 0.002636 BTC per day in a pool. If you mine solo, you'll find one block per 13 years.
@_date: 2019-08-25 22:18:31
Yes, absolutely.
@_date: 2019-08-24 07:46:29
It's not actually required, but pubkey hashes for Schnorr wouldn't have any benefits, and waste space (which you'd need to pay for).
The reason is that because of security requirements, the hash would need to be 256 bits in size (not 160 bit as the current pubkey hashes are). That makes them the same size as public keys (which in bip-schnorr are also 32 bytes). This means that we have the choice between:
* 32 bytes pub key in the output (128 WU), 64 byte signature in the input (64 WU) = 192 WU total.
* 32 bytes pubkey hash in the output (128 WU), 64 byte signature + 32 byte pubkey in the input (96 WU) = 224 WU total.
Obviously, using a hash doesn't gain you any space anymore (this is different from P2WPKH because 160-bit hashes are used there).
@_date: 2019-08-21 23:21:42
No, it's not.
Miniscript is a way of writing Bitcoin scripts in a structured way, enabling various kinds of static analysis.
It's a tool that can help developers build stuff that uses more advanced features of Bitcoin script. It's not a language that any human would use directly.
@_date: 2017-08-31 22:31:12


I don't understand this question.
@_date: 2019-08-24 18:36:51


For P2WPKH, yes. For P2WSH (e.g. native segwit multisig), the output already contains a 32-byte hash (for the same reason).
@_date: 2019-08-20 19:20:53
Not without (significantly) redesigning Miniscript itself. This is not a limitation of the compiler, but of the language it compiles to.
The challenge is that it's really hard to keep the language composable if we want to introduce common subexpression elimination to it.
@_date: 2019-08-21 01:30:34
Yes, it can, and probably.
@_date: 2019-08-22 02:23:21
Not sure to what extent you mean this as a joke, but I don't think end users should be exposed to scripts or spendability conditions at all. This is a tool to help developers write correct and secure software that uses scripts in nontrivial way.
@_date: 2019-08-27 05:30:48
Will have a look soon.
@_date: 2018-06-28 16:08:35
BLS doesn't need a hard fork at all. I don't see any benefit that could be had from it.
There are the 4 combinations (at a very high level):
* Schnorr signatures for new outputs. This is a SF, and what I'm working on.
* Schnorr signatures for old ECDSA outputs. This would need a HF.
* BLS signatures for new outputs. This is a SF, but carries additional risks for those outputs.
* BLS signatures for old ECDSA or Schnorr outputs. This would need a HF, but is effectively impossible except through DLEQ proofs that would remove any benefit BLS had, by a large margin.
@_date: 2019-08-16 19:37:18
Yes, it's still subject to some standardness checks to guarantee safety in case of upgrades (e.g. using undefined NOP opcodes inside P2SH is still nonstandard). But it's not subject to any "must match one of a few specific templates" rules. You can pretty much use any script inside P2SH.
@_date: 2019-08-11 16:17:24
It's from this forum post: 
@_date: 2019-08-21 03:54:27
@_date: 2019-10-18 03:40:20
If you adjust the BDB lock limits manually; see BIP50.
@_date: 2019-08-24 07:37:10


What you're talking about is cross-input signature aggregation, which needs a multisignature scheme in the consensus rules. While such schemes exist that are generalizations of Schnorr signatures, just Schnorr signatures don't do this.
@_date: 2018-06-27 20:34:09
Sorry, but this is a common misconception.
Schnorr signatures does not equate signature aggregation (the ability to combine multiple signatures into one). Signature aggregation is *possible* with Schnorr signatures, but integrating them across the stack with all of Bitcoin is a major obstackle. It is doable, but we need to make one step at a time.
I hope I'm getting anyone's hopes up. There are many awesome ideas out there to improve Bitcoin's scripting system, but from an engineering perspective it's very hard to include them all at once.
@_date: 2019-10-05 03:01:52
So, by the network's consensus rules, uncompressed keys are *not illegal* in segwit v0 outputs. They are however nonstandard, and have been forever.
You may be able to convince a miner to include your transaction.
@_date: 2018-06-28 00:34:15
6:44 PM in my timezone.
@_date: 2018-06-28 21:18:02
Yes, BLS requires a pairing curve. I don't understand why you think a new curve requires a hard fork. It would just be a new opcode that performs signature validation using another scheme. That can be done as a softfork; it doesn't matter whether the curve is different or not.
Regarding assumptions, in short, BLS requires a pairing friendly curve. The fact that this efficient pairing is available means the curve has significantly more "structure" (or in other words, is less randomly picked) that could be exploited for a cryptographic break.
@_date: 2019-08-26 09:55:14
A backdoored hardware device can leak its private key in a way that is completely undetectable. There are mechanisms to render these powerless, but current hardware wallets do not implement them (nor do the software wallets that communicate with them support or require them).
@_date: 2019-08-20 00:14:52
While it's true that Ethereum has had higher-level languages that compile down to on-chain scripts, I don't know of anything similar to Miniscript existing.
Constructing scripts is only a small part of the puzzle. *Using* them is much harder, especially if we want to do it in a cross-application-composable way.
What Miniscript enables is writing software that works with *generic* scripts. It lets you participate in signing *any* script you have keys for, while also letting you analyse what someone's script does (if they want to include you in their setup).
@_date: 2019-08-20 19:11:13
There are probably many things that can be expressed in Bitcoin, but not at all in Miniscript. The only moderately useful one I can think of is hash collision puzzles.
However, one thing that isn't actually missing, but Miniscript can only express with reduced efficiency, is duplicated conditions. If you can only write the condition as a tree with and/or/threshold inner nodes and key/timelock/hashlock leaves, but you need to repeat some of the leaves in order to express it that way, that duplication will inevitably exist in Miniscript as well, and thus in the resulting script as well.
@_date: 2019-10-16 15:23:00
Yes, that was it. I was there about a year ago.
@_date: 2019-10-09 20:53:47
Seems I'm connected to you now. You don't need to forward anything.
@_date: 2019-08-20 22:02:34
Haha, I also tend to overlook pinned posts. Somehow I seem to have a blind spot for those.
@_date: 2019-08-20 01:03:47
So in `and(pk(A),or(pk(B),or(9 the `pk(C)` branch has 45% chance (50% from the outer or, 90% from the inner or). In the second version it's 33.3% (50% from outer, 66.6% from inner).
It's possible that the optimal compilation is the same for both probability distributions. Try moving the `9 part to the other side of the or; the script will change.
The WU changes because it's computing the expected spending cost. If you change the probability distribution, that expectation will change, even if the script stays the same.
@_date: 2019-10-09 18:38:20
Getting more than 8 connections probably takes a few days, even if everything is working flawlessly. The network needs time to learn about your IP/onion address being rumoured around.
@_date: 2017-08-24 05:13:31
Oh, there is one input which has a witness. However, that witness contains contains 2 items (in this case, the public key and the signature).
@_date: 2019-08-16 14:49:47
Those constants each refer to a particular type of output script (also known as scriptPubKey).
However, not all of them are automatically standard. In particular, TX_NULL_DATA (OP_RETURN outputs) are restricted to 83 bytes of script, and at most one per transaction. TX_NONSTANDARD is for all output scripts that aren't categorized as something else, and nonstandard.
@_date: 2019-10-16 01:59:20
It had preliminary code for those features. It was never functional in any way, and no clear way to build something production quality out of it.
It was indeed removed after years of being unused.
@_date: 2019-10-09 17:00:13
Yes, that too. Because DNS serves IP addresses, not ports.
@_date: 2019-08-17 01:52:48
Sure they can, but why would they want to?
Generally the uplink bandwidth will be the limiting factor.
@_date: 2019-08-19 23:58:08
(Copying from [
Just announced our Miniscript project website on the bitcoin-dev mailinglist: 
In short, it's a way to write (some) Bitcoin scripts in a structured, composable way that allows various kinds of static analysis, generic signing, and compilation of policies.
Imagine a company wants to protect its cold storage funds using a 2-of-3 multisig policy with 3 executives. One of the executives however has a nice 2FA/multisig/timelock based  setup on his own. Why can't that entire setup be one of the multisig "participants"? A lot of work is focused on extensions to the functionality of the blockchain itself to support more complex application, but I feel we're forgetting that using these features in an accessible, composable, analyzable way is basically impossible today. My hope is that Miniscript, together with things like PSBT can reduce some of the barriers between pieces of software. Ideally, the executive's 2FA setup could interact flawlessly with the cold storage setup, computing the necessary composed script, and still be able to sign. The project includes a policy compiler, which you can tell under what conditions an output should be spendable and with what relative probabilities, and it will find the most economical Miniscript-compatible script for it (restricted to a number of transformations). You can play around with the policy compiler on the site, but constructing scripts is just one part of the puzzle. The Miniscript satisfier algorithm can construct witnesses for any Miniscript-compatible script, without even needing to know what policy the script was created from.
This has been a long time coming, and I've been talking about this for a while (including at SBC'19 earlier this year). I wasn't comfortable with publishing things until we had done significant testing against the actual Bitcoin consensus and standardness rules. We're there now. Of course, since then we've also pretty much rewritten the whole design three times over, as continued discussions with u/apoelstra and u/sanket1729 kept up bringing extra possibilities, issues, and analysis techniques.
@_date: 2019-08-16 19:03:34
I think that would be an oversimplification. Standardness is determined by a lot more than just what types of outputs are used.
@_date: 2019-08-20 23:15:19
Even cheaper would be moving the `older(365)`s out of the thresholds. So you should instead use
    100
    and(older(365),thresh(1,pk(A2),pk(B2),pk(C2))
@_date: 2019-08-20 00:22:11
@_date: 2019-10-05 03:13:38
No nodes on the network will relay such a transaction, as it is nonstandard.
Miners have an incentive to include them, but need a way to learn about them.
@_date: 2019-10-09 19:19:39
Your VPN is irrelevant if you're using Tor.
@_date: 2019-10-05 02:59:23
I don't think this is the case. Millions of BTC are stored in outputs with known public keys. It does not matter that your personally owned BTC are not at risk if a thief with a (very hypothetical) QC can take millions and dump them on the market.
Furthermore, pretty much all modern use of the Bitcoin protocol (multisig, Lightning, even some lightweight wallets when talking to their servers, ...) reveal public keys to other parties. Under QC, all of those become theoretically able to take the coins.
@_date: 2019-10-18 00:02:58
Using a wallet?
@_date: 2019-10-24 16:46:06
The obvious path to solving this is:
* Research PQC (post-quantum cryptography; cryptographic techniques that are assumed to remain secure against quantum computers) signature schemes that offer the performance/efficiency/features needed for integrating into Bitcoin. Right now, they're all pretty much infeasible (too slow and/or too big, no public derivation like BIP32, ...).
* When that's done, maybe years from now, propose an improvement to Bitcoin that enables PQC outputs.
* People start moving this coins over time to these PQC outputs.
* Decades later, when the threat for a sufficiently powerful QC becomes real, disable spending from old EC based outputs.
@_date: 2019-10-24 07:01:04
This is the data directory for Bitcoin Core. Make a backup of the wallet.dat file *now*, and install the most recent version (see  It should be able to open the file, but may need some time to synchronize.
Do not accept any offers from anyone who tells you to send them your wallet.dat file.
@_date: 2019-10-05 03:49:48
They're not deprecated. They have never been standard at all. The reason for these specifically is from a hope that one day they can be made illegal. That may never happen, however.
Historically various reasons have existed for making features nonstandard. Early ones were concerns about resource limitations (e.g. scripts that would be super slow to execute). Later ones were for untested features. In recent times they're mostly used for upgradability.
@_date: 2019-08-16 20:23:00
There is no reason why you'd want to run two nodes on the same network where both are publicly reachable.
Run one "router" node, which is reachable from the internet, and if you have need for more nodes, run them internally, and `-addnode` the router node.
@_date: 2019-10-15 23:59:50
What does "SF model" mean? San Francisco? Softfork?
@_date: 2019-10-09 18:30:40
You don't need to forward any ports to accept Tor hidden service connections.
@_date: 2019-10-09 15:38:01
Bitcoin Core nodes strongly prefer connecting to peers that run on the default port.
If you change the port, you will likely not get any incoming connections (or at least not from full nodes).
This is done to prevent turning the network into a DDoS farm. You could rumour the IP:PORT of a service you didn't like, and the concern is that in a future with many nodes around, that service would be hammered by incoming connections from the network.
@_date: 2019-10-23 19:20:14
No, it is an implementation of RFC 6238.
@_date: 2019-08-21 00:26:54
I have no idea about any of those things, as I don't work on those projects.
@_date: 2019-10-05 02:57:28
That's blatantly false. Over 5.5 million BTC is in outputs with known public keys. And there are likely millions more that are known to some parties, but not publicly (xpubs, hosted wallets, any kind of multisig has public keys known to all participants).
The correct answer is that QC is nowhere near capable of breaking EC cryptography, and unclear if it ever will. When that time comes, Bitcoin will softfork to add post-quantum signature schemes, and eventually remove EC based outputs when we become too worried about their breaking.
@_date: 2019-10-19 03:58:23
See my answer here for details (and more on the topic): 
@_date: 2019-10-09 17:01:10
No, though you could set up a hidden service on Tor that's reachable.
@_date: 2019-10-25 07:07:08
I'd say, on a scale from 1 to 10, you'd need to know 9 much in order to be close to being 9 knowledgeable.
@_date: 2019-08-20 21:58:58
Simplicity is a low-level language for spendability conditions. Simplicity would be a language that actual on-chain conditions use. Simplicity is just a proposal however, and not in use in Bitcoin. Adopting it would need a softfork, and isn't something I expect to happen any time soon.
Miniscript is a reformulation of Bitcoin Script, the language Bitcoin uses \_today\_ for spendability conditions. It is unrelated to what is going on on-chain - that just remains the same Script language as we've always had. Instead, it lets wallets and other applications construct and analyze these scripts more effectively. Miniscript is usable today.
@_date: 2019-08-21 01:33:14
Currently Miniscript is only fully specified for v0 P2WSH (including P2SH), as it relies on a number of P2WSH-specific things.
If Taproot gets further along in being accepted/specified/activated on Bitcoin, we'll certainly extend the language to include bip-tapscript constructions (no CMS anymore, add CSA, ...). Once that is done, we can think about extending the compiler to not target a single script but a Merkle tree of them.
@_date: 2019-10-16 06:10:35
Bartender who looks like (is?) a Buddhist monk?
@_date: 2019-08-20 23:13:16
For example what Blockstream Green implements: one key on your device, and one key controlled by the wallet service, but after some time delay you can take your funds unilaterally (so the service cannot permanently prevent access to your coins, even if it would go out of business, or hacked).
The issue is that Green is just a single-party wallet. You can use it to protect your own funds, but it can't be combined with other services/parties to construct e.g. a 2-of-3 where the whole Green policy is just one of the participants.
@_date: 2019-10-30 20:33:28
You know that 0.021 BC will always be at least one billionth of all BTC in circulation - regardless of what units people use.
@_date: 2019-10-16 06:08:09
Golden Gai?
@_date: 2019-08-16 19:23:59
That hasn't been the case since 0.10.0 (Feb 2015), when effectively all P2SH usage became standard.
Sure, in bare scriptPubKeys you can't do much, but it's not usable anyway due to lack of addresses for constructing unusual scripts there.
@_date: 2019-10-19 00:45:05
As of block 600002 the total number of BTC in circulation* is actually only 17999854.82192702. We'll pass 18M in 2 hours or so.
(*) The number of coins in circulation is hard to define of course when you take into account lost coins, but at least when taking into account the unspendable genesis block, subsidies that we overwritten pre-BIP30, coinbases that paid out less than what was allowed, and OP_RETURN burned coins, that's the number you get.
@_date: 2019-08-25 15:19:27
If you're talking about Bitcoin Core, yes, you can copy a datadir from one system to another and it should just work.
@_date: 2019-10-21 16:23:35
Except for debugging nobody should care about that.
@_date: 2019-10-30 20:26:45
No, *everyone* verifies *all* transactions (at least all network full nodes). Objectively invalid transactions are simply ignored by the network. Bitcoin is trust minimized in the sense that full nodes do not trust the validity of transactions claimed by other nodes, ever.
However, there is one thing that cannot be objectively verified by everyone: in case there are two conflicting (but otherwise equally valid) transactions that spend the same coins, which one do you pick? The obvious answer would be that everyone would just pick the first one. Unfortunately, that does not work: it is impossible in general to tell which transaction was created first in a decentralized manner. Imagine I craft two legal but conflicting transactions, and I broadcast them simultaneously: one to a node in Sydney, Australia, one to a node in New York, USA. Nodes close to New York will see one of them first; nodes closer to Sydney will see the other one first. There is no objective criterion to say one is right and one is wrong. This must be true simply due to the speed of light: it takes at least 50 ms to communicate between these two locations, so if the time of announcement of these two transactions is less than 50 ms apart, it is simply impossible to tell which one was first.
Bitcoin solves this through proof-of-work. Instead of just broadcasting transactions, they get grouped together by miners in blocks. Each block is effectively its own chapter in a version of history about which transactions actually happened. The problem of the 50 ms delay is not solved: it is still possible that two blocks get produced \~simultaneously on the network in distinct locations. However, in this case, Bitcoin simply treats this as two distinct but possibly valid versions of history, and the next block gets to decide which of the two will be accepted. This works, because creating a block is expensive, creating an incentive for miners to work together on a single version that the network will accept, as prolonging a "fork" between two versions results in decreased average income for miners, as only one version will get the rewards.
The bottom line is: miners do not validate transactions any more than other nodes in the network do. What they do is provide a tie-breaking mechanism to end up with a single version of history.
@_date: 2019-10-09 17:45:30
@_date: 2019-10-18 03:59:09
It's just a heuristic. Disassembly also tries to guess what is a signature and report sighash type for it, for example.
The script language has only one data type: byte arrays. Any push just pushes bytes, that's it. Depending on what opcode you feed that byte array to, those bytes may be interpreted as a hash, a public key, a signature, ... or a number. Given that number opcodes in Bitcoin script only accept number up to 4 bytes in size, the disassembly code assumes exactly those in fact are.
@_date: 2019-08-05 05:25:34
Wow people use that site still?
@_date: 2019-06-16 18:01:47
Graphene (and Compact Blocks, FIBRE,...) are techniques to reduce the latency of block propagation.
Erlay is a technique to reduce the bandwidth of transaction relay.
They're entirely unrelated.
@_date: 2019-08-21 07:14:55
* bitcoin.org
* bitcointalk.org
* bitcoincore.org
* twitter.com/bitcoin
* reddit.com* github.com/bitcoin
All of these are run by sometimes partially overlapping, but generally distinct groups of people.
@_date: 2019-06-16 16:57:12
Minor releases usually only contain bugfixes. As this is a major change to the P2P protocol, it will likely happen in a major release.
When that would be is hard to say though, it's just a concept for now.
@_date: 2019-10-31 08:20:07
It will get much much slower still.
You can speed it up a bit by increasing the "dbcache" value in bitcoin.conf, based on how much RAM you have. More is always faster, but if you set it too high it may go out of memory (and need an even costlier rebuild to correct). Stay at least a few 100 MB under your amount of RAM (in MB).
@_date: 2019-08-20 18:53:35
An example is the difference between `or_b` (\[X\] \[Y\] OP\_BOOLOR) and `or_d` (\[X\] OP\_IFDUP OP\_NOTIF \[Z\] OP\_ENDIF).
For the `or_b` one, if the X side of things is used, \[Y\] is still executed, and you'll need to provide inputs for it (if it's a `c:pk` for example, it'll need a dummy signature just to get it executed and fail correctly, even though we don't care about its result). In `or_d`, when the X side succeeds, \[Y\] isn't executed at all.
Which one is best depends on how expensive it is to construct a dissatisfaction for Y (for pk it's cheap, but more complex things may have a nontrivial cost), what the relative probabilities of X and Y are (that's where the N@ part comes in), and what the changes of execution of the or itself are.
@_date: 2019-08-24 07:40:52
There is a single limit of 4000000 weight. That's all.
Witness bytes count as 1 weight each, and non-witness bytes count as 4 weight each.
This implies that there indeed can't be more than 1000000 non-witness bytes in a block, making this rule compatible with the pre-segwit 1 MB limit.
However it's important to realize that this is just a single combined limit. A 3.7 MB block with 3.6 MB witness data and a 1.3 MB block with 0.4 MB witness data are both equally "full" (as the weight of both will be 400000).
@_date: 2019-06-16 17:00:21
I'm a co-author of the Erlay paper, and wrote a large part of the set reconciliation code we plan to use for this (see 
@_date: 2019-10-18 03:37:25
4161 here means the bytes with hex values [0x41, 0x61] or decimal values [65, 97]. That's a little endian encoding of 65 + 97*256 = 24897.
@_date: 2019-06-03 06:23:36
That's likely just due to other nodes downloading historical blocks from you, not transaction relay. If you configure an upload limit, that will go away. It's certainly not necessary that every node in the network offers this functionality.
The work around Erlay is to reduce the bandwidth needed for gossiping transactions, which can be in the order of 100s of MB/day, and increases with more connections. Using Erlay, the gossip traffic actually remains almost constant with increases connection count.
@_date: 2019-10-09 15:38:25
This is correct, but likely not the only reason. See my answer higher up.
@_date: 2019-06-05 09:23:12
I have a writeup about the total number of BTC here: 
@_date: 2019-10-05 03:05:08
Can you exchange BTC for a predefined amount of energy? No. It's not backed by energy.
Can you exchange it for predefined amount of algorithms (what would that even mean)? No. It's not backed by algorithms.
Yes, Bitcoin is protected by both economic forces (through proof of work) and cryptographic algorithms. They're awesome. That does not make BTC backed by anything.
@_date: 2019-10-09 20:36:48
That depends on your configuration; hard to say what is needed.
What is your .onion address? I can see if your node is up. You can find it by typing "getnetworkinfo" in the debug console, under localaddresses.
@_date: 2019-06-03 06:26:49
That's unrelated. If you see that much bandwidth usage it's due to your node serving old blocks to nodes that are synchronizing. Erlay is about reducing transactiom gossip bandwidth, not blocks (which is optional).
@_date: 2019-06-06 09:12:56
No, read OP's link.
@_date: 2019-10-09 17:38:02
@_date: 2019-06-03 06:31:56
If you start bitcoind for the first time it will automatically create a wallet for you.
If you want more wallets, you can use the `createwallet` RPC command.
Accounts were a complicated and confusing feature that was deprecated before and has been removed in the latest release.
@_date: 2019-02-10 16:52:20
It is indeed transaction based.
Bitcoin Core has built-in constants that are used to estimate the total number of transactions currently:
* The timestamp T of a recent block B at the time of the software release.
* The accumulated number N of transactions at that block.
* The approximate rate R of transactions per second at that time (averaged over a long period like a month or so).
Using this, the total number of current transactions is estimated as:
* Before reaching B: N + R*(current_time - T).
* After reaching B: transactions_so_far + R*(current_time - latest_synced_block_timestamp).
@_date: 2017-11-15 23:43:17
The capacity gains depend on the type and size of transaction inputs. In general the gains are larger for larger transactions.
I think it's reasonable there will be a concrete proposal and implementation in 2018.
@_date: 2019-02-23 04:57:39
Nearly every script is accepted by default when used inside P2SH or P2WSH; the former has been the case for years.
The IsStandard rules for "raw" scripts are there to make sure future upgrades are smooth, and prevent bloating the UTXO set.
@_date: 2019-06-16 17:10:01
It's probably unfair to call it a phase, as the propagation chain of a given transaction can switch from reconciliation to flooding and back. Which type is used where depends both on the direction of the connection and how the transaction was learned about.
* Whenever a node learns of a transaction through flooding, it relays it further using flooding to its outbound peers, but also puts it in the reconciliation sets for its incoming peers. This will generally only happen when the node has inbound peers.
* Whenever a node learns of a transaction through reconciliation from an inbound connection, it is also relayed further using flooding to outbound peers.
* Whenever a node learns of a transaction through reconciliation from an outbound connection, it is not flooded, and instead only relayed to all (inbound and outbound) peers using reconciliation.
One possible scenario is that a node A which created a new transaction itself only relays it using reconciliation, even for its outbound peers (this has better privacy). Those outbound peers have themselves more outbound peers, and we generally treat them as part of the public network (nodes which accept incoming connections, and are well-connected to each other). Those public nodes will then quickly relay among thenselves using flooding. As soon as parts of the public network have heard about it, they make the transaction available for reconciliation to every node (as every node has at least one outgoing connection to the public network). However, it is certainly possible that they manage to relay it using reconciliation to other public nodes, who can start a flooding relay again.
@_date: 2019-10-19 04:01:32
That depends on how you count created!
* Does the genesis block count? It's a coinbase output, but whether you treat the fact that it can't be spent as a bug or an intentional choice is a matter of perspective.
* Do blocks that claim less subsidy than allowed count? These are BTC that never even pass through a transaction output.
* Do overwritten coinbases count? They're arguably not introducing any new coins, as they just "refresh" the same transactions a second time.
@_date: 2017-11-15 22:21:23
MAST (or any kind of Merkleized branching technology) only matters when there are multiple different conditions under which a transaction output can be spent.
Signature aggregation (which could be provided by Schnorr signatures or related technology) is useful whenever a transaction requires more than 1 signature.
Pure 2-of-2 multisig can obviously use signature aggregation, but MAST is of no use (it's always the same 2 keys that need to sign). However, I believe Lightning uses HTLCs rather than just 2-of-2 (I'm not very up to date, perhaps can comment?), which probably can.
@_date: 2019-06-04 07:29:21
Are you talking about the 43000 BTC stolen from Bitcoinica by a Linode employee?
@_date: 2019-06-02 00:37:39
As others have pointed out, Schnorr itself is just another signature scheme. It's a (mostly) drop in replacement for ECDSA. Just because you change the signature scheme doesn't mean you can suddenly have transactions that fundamentally change the transaction structure.
The idea of changing Bitcoin's transaction structure so that only a single signature per transaction is possible is called cross-input aggregation, and it relies on properties of Schnorr-like constructions, but it inherently needs changes to the script execution beyond just the signature scheme. Also note that even with cross-input aggregation, the transaction will still have multiple inputs, and each of those inputs will have a scriptSig (or script witness with segwit). It's just that only one of those script witnesses will contain an actual signature; the rest would at least need some marker "no signature here! it's aggregated into the signature in input number X!".
What you however \*can\* do with Schnorr signatures, without any explicit support from the consensus rules, is key aggregation. With Schnorr it's possible to combine multiple public keys into one (we wrote a paper on one way to do this this, called MuSig), such that all of the individual participants must cooperate to produce a signature for the combined key. This permits things like multisig, and even much more complex policies over multiple keys, to generally be combined into a single key. That's a privacy improvement, as you leak less information about your actual policy to the chain, and also a performance/cost improvement for complex scripts, as they generally can be turned into versions that just have a single public key.
As far as deploying these goes, we've recently published a number of BIP drafts that propose the inclusion of Schnorr signatures with a number of other changes (in particular, Taproot and Merkle branches), but no cross-input aggregation, into Bitcoin's consensus rules. The reason why no cross-input aggregation is included is because it interacts in complex ways with various other improvements to the scripting language, and the proposal is sufficiently bulky already.
@_date: 2017-11-11 20:54:47
No, that's just a side effect of a functioning fee market. Your transactions are not affected by others that pay less.
@_date: 2015-08-17 11:25:45
Thanks :)
@_date: 2017-11-14 22:14:31
Also called an extension block.
@_date: 2019-02-10 17:06:29
Such a script is perfectly standard as long as you wrap it in P2SH or P2WSH (in other words, send coins to the P2SH script that contains the hash of your script; when spending, reveal the full script and the inputs necessary to satisfy it).
@_date: 2017-11-16 17:50:26
They're independent.
The new SegWit script versioning system means tgat any optional script feature can be introduced as a softfork.
Miner activation for softforks was never needed (and in fact the first softforks weren't, see BIP16 and BIP30). They're just safer - by waiting until enough miners are ready, the chance of a minority chain existing go do rapidly. However, as we've seen, they also permit miners to stall deployment.
@_date: 2017-11-15 23:26:01
Sure, here is a comment:
    /* Schnorr signatures. */
More seriously, if you have specific questions, I can answer those too :).
@_date: 2017-11-24 00:05:42
Stop thinking about it as going through all the nonces for a given composition of transactions. That's really just an implementation detail.
See it this way: every miner tries billions of different block candidates every second. **One** of the ways they vary the block candidate is by increasing the nonce. But there are many other ways, including changing the timestamp, or changing the composition of transactions.
Each of those billions of attempts is statistically independent from every other. The fact that you've tried many nonces already does not increase the chance for the next one. The fact that you've tried many transaction compositions already does not increase the chance for the next one. Effectively every attempt - as long as it is in some tiny way distinct from every earlier attempt - has the same fixed probability of being a winner.
Now, in practice, mining ASICs do the incrementing of nonces automatically, while changes in block composition need to be communicated. However, if you have a 4 Ghash/s chip, it will go through the entire nonce space (which is 32 bits) in less than a second. This means that already, these chips need to be fed a constant stream of new compositions already.
If a new transaction is received on the network, it will probably be communicated to the mining ASICs within seconds. The effect on mining efficiency is zero - it's just another change.
@_date: 2019-06-24 00:15:42
If transactions operating on 1 satoshi are something you consider feasible, why stop there?
It is perfectly legal in the Bitcoin protocol to create outputs of value 0.00000000 BTC, and to spend those. There are policies on the network that discourage the creation of these, but those same policies prohibit outputs of value 0.00000001 BTC.
With 0-value outputs, your question should be what would happen when sending all 21M BTC in infinity transactions. Obviously it won't work.
@_date: 2017-11-15 18:56:33
Just to make sure there are no unrealistic expectations here:
* CT does **not on itself provide anything you could call "full privacy"**. It hides the amounts involved in inputs and outputs to third parties. Together with CoinJoin it gives a much bigger advantage, however, and these new aggregateable rangeproofs would also give a strong financial incentive to do so (it's great when the more private solution is also the cheaper one!).
* While Bulletproofs massively reduce the size of the rangeproofs in CT transactions, the **CPU overhead is effectively unchanged**. This results in CT transactions still being 1-2 orders of magnitude slower to validate than transactions in Bitcoin right now. We'll provide better numbers once there is an optimized implementation.
* Bulletproofs and the Pedersen commitments they operate on are perfectly hiding, but **not perfectly binding**. This roughly means that if they're adopted inside Bitcoin, and elliptic curve crypto is (completely) broken, new money can be printed. On the flip side, it does mean that the privacy of anyone who used CT in the past is unaffected. Alternative formulations of CT exist for which this is the other way around (perfectly binding but not perfectly hiding), where money can never be printed (even if the cryptography is broken), but privacy can be retroactively lost. There is currently a discussion on the mailinglist which of these is the better tradeoff (it is mathematically impossible to have both perfect hiding and binding).
* This technology is far too **premature** to propose for inclusion into Bitcoin.
Regardless, Bulletproofs are an **amazing discovery** that fundamentally changes what is possible. The credit belongs to *Benedikt Bnz* and *Jonathan Bootle* here; our contribution was mostly making the problem and its constraints clear, and promising to implement an optimized implementation and analyze the results.
EDIT: thank you kind stranger for the gold!
@_date: 2017-11-07 23:42:26
A Reddit post with a link to a Twitter post in which there is an embedded image with a screenshot of a Slack channel containing a message mirrored from IRC?
We must go deeper.
@_date: 2017-11-15 19:13:51
Yes, that is possible - by having a conversion factor that translates the expected CPU cost to a term in the virtual size. This would naturally lead to fees proportional to this cost.
However, this is not necessarily desirable. Ideally you want a system where privacy does not come at a price. Getting this right is tricky, but it probably means every transaction should be costed as if it were a confidential transaction, whether it is or not.
Yes, you can tell on a per-output basis whether it is using CT or not.
@_date: 2019-06-06 09:12:28
No, read OP's link.
@_date: 2019-02-13 17:24:11
The amount to which a full node helps the network is tiny; especially when it's running on low-powered (and often I/O starved) VPS hardware.
Run a node when you plan to *use* one. That is how they contribute to the ecosystem: by validating incoming transactions that are economically relevant to you. You help the network a lot more by doing that than by running 1000 nodes that nobody would notice if they'd go out of sync.
How many full nodes is enough for the network? One, your own.
@_date: 2017-11-20 08:42:33


The vast majority of proposed patches to Bitcoin Core are not about changing Bitcoin at all. They're relatively boring software maintainance, improving stability, performance, resource usage, or features the node software or the wallet has. To a large extent, the software's maintainers explicitly do not want to be in charge of what Bitcoin is.
Of course, sometimes changes to Bitcoin are proposed as BIPs, and at some point implementations are needed for those. Bitcoin Core has a policy of only accepting consensus changes (changes which impact Bitcoin's definition of what a valid block is) only when a high bar of acceptance in the community exists, proportional to the risk. In recent history, such changes have always been softforks - changes that do not break backward compatibility. Such changes are safe as long as majority of the network hashrate enforces them, as opposed to what is called hardforks, which effectively require the entire ecosystem to adopt new software, or risk a split into two currencies.
Despite softforks being relatively safe in this regard, they are rare (we've had 7 in the past 7 years, IIRC), and take months or years of preparation. For the last one, SegWit (disclaimer: I authored a part of it), it was only merged into Bitcoin Core after lengthy discussion, review, and contacting many businesses whether deployment would cause issues for them).
Ultimately, the answer to the question of how it is decided how Bitcoin changes lies with its users. Bitcoin Core certainly has an important position in the developer ecosystem, and many of its contributors are individually involved in discussions about Bitcoin's future. Inevitably, these developers do have influence. **But the choice to run certain software lies always with those who run full nodes** and base their economic activity on them. My hope is that the ecosystem does not place blind trust in whatever group of developers, but demand safe practices. We're in this for the long run, and many perceived short-term risks are no reason to deviate from them.
Because it's often forgotten, I'd like to add that Bitcoin Core also explicitly takes certain steps to avoid having maintainers with too much powers. It has no auto-update feature, and never will. The risk of having certain individuals with the keys to push new software - with possibly new rules - to the world would be a massive central point of failure. Furthermore, for years there has been a deterministic build process, which means that anyone can reproduce the byte-for-byte exact binaries from the source code, and verify that they match our published binaries on bitcoincore.org. In fact, the final step before a release is announced is verifying that a sufficient number of people have produced the exact same build, indicating that it is indeed reproducible.
@_date: 2017-11-15 23:45:09
To the best of my knowledge, Ethereum doesn't use zk-SNARKS. ZCash does, but they did use a trusted setup to create the key.
@_date: 2019-10-23 20:30:11
Their demonstration was for a specially-designed program that is particularly easy for quantum computers, and particularly hard for traditional computers. Quantum computers are not simply a blanket speed faster at the same tasks; it highly depends on the kind of problem.
Their quantum computer cannot do anything useful. It's a theoretical result that lets them claim "quantum supremacy" as it indeed is a computation that couls not be done otherwise, but it has no real world implications. Nobody needs this type of result.
A quantum computer sufficiently strong to break elliptic curve cryptography would needs thousands of qbits (which very likely means 100s of thousands of pbysical ones, due to error correction requirements for the computation), and billions of gates. We're nowhere near building such a device.
@_date: 2017-11-15 23:01:17
I believe it is possible to softfork it in a way where all CT outputs and inputs from the perspective of old nodes see amount 0, and miners are forced by the protocol to maintain the pool of hidden coins.
That said, an extension block approach may be easier.
@_date: 2019-06-13 17:52:10
No, but I might switch to one that is cheaper by virtue of not having a rewards program to maintain.
@_date: 2017-11-15 19:52:19
probably knows better how it applies to MW.
@_date: 2017-11-16 17:14:23
Making it mandatory on Bitcoin seems totally infeasible indeed.
I don't think that's needed though, all is needed is to make sure it is not more expensive to use.
@_date: 2017-11-02 14:01:31
In 0.15.2, actually.
EDIT: 0.15.1 will improve the P2P robustness. SegWit wallet support will be 0.15.2.
@_date: 2019-06-03 06:24:48
Absolutely. Greg came up with the concept of using set reconciliation, but the actual protocol design was Gleb's work.
@_date: 2015-08-12 06:42:05
It is backward compatible because there is no requirement that miners consume 100% of the allowed subsidy.
@_date: 2017-11-11 16:59:49
No, better protection against rogue peers.
@_date: 2019-12-24 09:31:17
What does it mean to have it as an emoji? What would it look like, or how would it be different from the existing symbol? Why do we need it?
@_date: 2015-08-25 22:20:36
The first line in the table represents the resulting stack after executing the scriptSig. The scriptSig does not contain the root, only the public key used, the signature with it, and the branch connecting it to the root.
All the following lines represent pieces of the scriptPubKey being executed, and the scriptPubKey is what contains the root being verified. This is similar to how a normal P2PKH scriptPubKey contains the pubkeyhash, or a P2SH script contains the script hash.
@_date: 2017-11-16 02:02:05
Then don't use it?
@_date: 2015-08-11 23:04:32
It was also just a soft fork!
@_date: 2015-08-26 17:00:47
During the talk (video will be available soon) I actually explain how to make this behaviour available in Bitcoin through a soft fork.
@_date: 2015-08-26 00:27:05
I have no clue who that is.
@_date: 2017-11-02 13:11:53
You can't verify that they are generated using RFC6979 unless you have the private key.
I believe people have tested by putting a known key on hardware devices and checking them though.
@_date: 2017-11-11 18:36:37
* Bitcoin Core will now be better at detecting "bad" peers and disconnect them.
* It's no longer possible to instruct the built-in mining code to limit blocks in terms of bytes (only by weight).
* Some bug fixes.
* Some minor improvements to RPC output.
@_date: 2019-06-24 00:09:51
ABCore runs fine on high-end smartphones. Even without enormous storage, you can enable pruning and keep the storage burden as low as a few GiB.
@_date: 2017-11-16 02:24:39
Thanks, and likewise.
@_date: 2017-11-15 22:00:46
How fees are affected depends on how costing of the CT rangeproofs is done. Since SegWit, fees are not proportional to transaction size anymore, but to weight. A hypothetical future CT proposal for Bitcoin could make the rangeproofs not affect the weight.
@_date: 2017-11-02 13:13:47
Generally no actual randomness is used when producing a signature. Instead, the nonce is computed as a hash of the message and the private key (seen RFC6979). However, without knowing the private key, you can't verify that this method is being used.
@_date: 2017-11-14 18:23:13
Thankfully it isn't actually a homomorphic encryption (which would imply you can decrypt it), but just a homomorphic commitment (more like a hash function that actually hides the value).
@_date: 2017-11-14 06:30:20
Exactly right.
@_date: 2017-11-12 01:18:39
You'll be able to configure whether you want legacy addresses (1...), segwit P2SH addresses (3...), or segwit bech32 addresses (bc1...). The default will be segwit P2SH addresses.
@_date: 2017-11-15 23:41:24
With CT, Bob will have to prove to Charlie that the output he's receiving indeed has value 1 BTC. This is possible without revealing all of the coin's history.
Of course, if the transaction that Bob sends to Charlie has just one input and one output, it is obvious that the amount in that input is identical to the amount Charlie is receiving. However for normal transactions with multiple inputs and outputs this does not generally work.
@_date: 2017-11-16 02:41:07
It doesn't actually "compare". This is a generic improvement to zero-knowledge proofs that can be used inside Confidential Transactions. Monero is already using a form of CT, and thus could use it.
@_date: 2017-11-12 02:55:27
See 
@_date: 2019-02-10 21:16:16
There are no other peers involved.
They're constants, hardcoded in the Bitcoin Core software. They're not received over the network.
@_date: 2017-11-16 06:56:04
Bitcoin certainly has some privacy, if used correctly.
But it is far from perfect.
@_date: 2017-11-11 23:36:53
There is a configurable setting for how much work the best chain needs to have at least. You can override it with the `-minimumchainwork` command line option. The default value currently is 0x723d3581fe1bd55373540a, which corresponds to the amount of work as of block 477890.
@_date: 2019-02-10 17:36:27
P2SH outputs can be segwit or not segwit; you can't tell.
Native segwit is P2WPKH (for single sig) and P2WSH (for multisig and others).
@_date: 2017-11-15 19:32:56
Indeed. Thankfully, the validation is easily parallellizable across multiple cores, and can be cached per transaction (which Bitcoin Core already heavily uses for signature verification).
@_date: 2017-11-10 02:12:21
I believe I have answered your question.
It depends on the manner in which the number of decimals are changed.
If it is done through an extension block soft fork, the subsidy can't change. So it would remain 0.
If it is done through a hard fork, anything can happen - anything at all. The answer in that case is "whatever we want".
@_date: 2019-06-16 18:18:39
Compact blocks (BIP 152) have been implemented in Bitcoin Core since 2016, and are extremely succesful in reducing latency of block propagation on the public network : the vast majority of blocks are relayed without roundtrips, using a just few kilobytes per block (assuming ~all transactions in that block are already known to the receiver ahead of time).
Other techniques, including Graphene, aim to reduce these few kilobytes further, but this is just the amount of data *at block propagation time*, not the total bandwidth needed to run a node (which still includes at least 1-2 MB of transaction data per block, ignoring all transaction relay overheads - which Erlay will help reduce). In some settings, where these few kilobytes for the block itself are critical for relay (like very slow connections), such techniques may be worth it, but in terms of total bandwidth reduction it is a marginal improvement at best. I don't know of any plans to incorporate them in Bitcoin.
FIBRE is a more advanced version of compact blocks, with some optional features that are only acceptable to use between mutually trusting nodes (for example, multiple nodes run by the same person, in different parts of the world). It explicitly does not try to minimize bandwidth, only latency. There is one publicly deployed network of FIBRE nodes operation, which relays blocks at very close to the limit of communication speeds across the globe: 
@_date: 2019-12-03 03:21:43
Well C++ is a compiled language with control over low-level datastructures. It isn't hard in that setting to get things to be as fast as the CPU can be.
The common operation is comparing two numbers, and for large integers that really just means a comparison per limb (you can think of big integers as normal human-written integers, where each digit is a 32-bit or 64-bit number, and called a limb). In Bitcoin Core hashes are represented as 8 32-bit limbs, so you need 8 comparison operations. On a modern desktop CPU that shouldn't take more than a few nanoseconds.
Of course, actually computing adjustments to the target is significantly slower, as it involved a big integer division. That may take in the order of microseconds, but it only needs to be done once every 2016 blocks, and compared to even the verification time for a single block, that is a very short time.
@_date: 2019-12-11 04:33:56
In general, you don't.
@_date: 2017-11-03 14:13:31




If you consider your wallet software or the system on which it runs utterly insecure, then indeed this approach has no benefit. However, if you at least believe there is a reasonable chance your system is not compromised, there is. If an attack requires you to use a backdoored HW wallet PLUS a system compromised by someone colluding with the HW wallet manufacturer, it's certainly harder to pull off than just the HW manufacturer alone.




* The HW device generates k1 (probably using RFC6979, from private key and message), and sends R1=k1\*G (the public key corresponding to private key k1) to the wallet.
* The wallet software generates k2 (perhaps using OS or system RNG), and sends k2 to the HW device.
* The HW device now uses k1+H(k2,R1) as nonce, resulting in (k1+H(k2,R1))\*G = k1\*G+H(k2,R1)\*G = R1+H(k2,R1)\*G in the signature's R value.
* The wallet can compute R1+H(k2,R1)\*G for itself, which must match the R value in the signature.
If either k1 or k2 was chosen randomly, the resulting R1+H(k2,k1\*G)\*G point will be indistinguishable from random as well, guaranteeing that it does not leak any information.
@_date: 2017-11-26 20:10:28
In theoretical cryptography, the multiplicative notation is usually used. This makes sense, as often the type of DLP-hard group doesn't matter - it could be a Z/Zp group (multiplication mod p) rather than an elliptical curve. It also has the advantage that in formulas you can clearly distinguish scalars from points (scalars go in the exponents, points go in the base).
In more applied cryptography, the additive notation is more common. These people tend to care about the exact group used, as it impacts performance and security tradeoffs. Additive notation has the advantage that it's straightforward to translate a scalar formula to a group formula. For example, the Schnorr signing equation is `s = k + H(R,m)*x` while the verification equation is `sG = R + H(R,m)*X` (with `R=kG`).
I personally prefer the additive notation, but in the end it's just a group with a single operation, and it doesn't matter how you denote it.
@_date: 2017-11-10 22:20:28
There are two currencies involved here, BTC (bitcoin) and BCH (bitcoin cash). Bitcoin Core is a BTC client. Bitcoin-ABC is a BCH client. Bitcoin Classic started off as an earlier attempt to fork off BTC, but has since morphed into another BCH client.
The people behind the btc1/Segwit2X movement wanted to fork BTC to create the B2X currency (with the intent of it replacing and/or taking the name bitcoin). These people have announced a few days ago to no longer pursue these plans.
Independently, the Bitcoin-ABC people have announced that they'll be changing their consensus rules (its difficulty formula), resulting in a modified BCH chain (and possibly a fork, if some maintain the old version). It seems Bitcoin Classic will not implement this change, and thus may risk staying on the original BCH chain. Furthermore, its developer has now announced to discontinue working on it entirely. That's the story this thread is about, but it doesn't even have anything to do with 2X - it's just the developer of Bitcoin Classic who is quitting.
Bitcoin Core, and most of the development ecosystem never had anything to do with B2X or BCH, and nothing changed for the BTC currency.
@_date: 2017-11-11 23:46:11
The changes for that are still in review, as we temporarily had to prioritize some P2P improvements against for the now-cancelled 2X attack.
@_date: 2017-11-03 03:24:08
You still can't verify that it *always* uses RFC6979 - it may "fail" to do so for 1 in a 1000 transactions, and leak private keys in just those cases through malicious choice of nonces. I expect that this would go undetected for a long time.
I think the best solution is a joint randomness approach, where part of the entropy comes from the wallet software. There are methods through which the HW can generate a random number while proving that it used the entropy you provided, without leaking what it is.
@_date: 2019-12-10 17:56:31
You'll need to ask for a non-bech32 address to pay to, or switch to a wallet that supports sending to bech32.
@_date: 2019-12-08 06:19:18
Paging the speaker, @_date: 2019-02-10 16:58:40
 has that information, I think.
Note that Bech32 is only the name of the address format (the way of encoding the output's information in a human-readable string). The produced outputs are called *native segwit* outputs, or something similar.
@_date: 2019-12-17 19:26:51
Miniscript is really just a toolbox for helping developers work with Bitcoin Script. It's a framework to reason, construct, analyze, and sign for scripts in a for more generic way than is possible with Script directly.
However, it is not an end-user tool. I don't expect that there will be end-user applications that let you develop your own spending policies, and if some expert ones do, those don't exist today.
@_date: 2017-11-21 11:01:44
During the last weekly meeting we agreed to call the next release 0.16 (but with an accelerated release schedule).
@_date: 2019-12-26 07:15:25
the "s" in p2wsh means "script". The size will depend on the size of the script and its inputs.
@_date: 2017-11-22 09:23:56
Not only is it possible; it's actually happening right now.
@_date: 2017-11-15 22:33:25
Unfortunately, no. Bulletproof aggregation is interactive, which means that the proof creator(s) have to cooperate to produce the proof.
has another comment in this thread that goes more into the impact on MW.
@_date: 2017-11-10 15:20:18
They're not lost. They were never there in the first place.
The total maximal number of BTC is not, and has never been 21M. That's just an approximation (but a pretty accurate one).
And if in a hardfork we choose to raise it to exactly 21M, that's possible. But that would be no harder (technically) than making it 30M or any other number. Hard forks can change anything.
@_date: 2017-11-11 21:01:03
SegWit became a consensus rule in Bitcoin on august 24th. Since then, every block must satisfy the SegWit rules, or be invalid. There is also no "base size" rule anymore - the only rule is that block weight is at most 4M.
Thinking about legacy vs SegWit block is probably misleading. Every transaction can have witnesses now, but they don't have to. A block with no transactions with witnesses is perfectly valid. It's just that if witnesses are present, they now must be correct.
The only thing that changes in 0.15.1 is some simplification in the mining logic. It does not affect you if you don't run a mining pool.
@_date: 2017-11-15 20:46:45
I just sold all my bitcoin gold.
@_date: 2019-12-29 05:59:25
BIP32 permits 255 levels in the tree.
@_date: 2019-12-13 22:12:40
New wallets generated by Bitcoin Core versions since 0.13.0 are deterministic.
So this doesn't depend on the version you're running now, but on the first version run on that machine/datadir.
@_date: 2017-11-11 20:56:55
Stop thinking in terms of bytes, they're irrelevant now.
The limit is 4M weight. Every byte in the witnesses count as 1 weight, every other byte counts as 4 weight.
@_date: 2019-12-01 22:10:28
I see. You were arguing that blockchain-based may be an improvement over actually deployed (and generally shitty) used election systems. Depending on your perspective, that may be the case. But it certainly isn't the case if you compare with state-of-the-art online voting systems.
FWIW, I believe (pure) online voting is generally a terrible idea for anything of real importance. Hardware and software will be broken, it is very hard to actually guarantee enforced secrecy if someone can come over to your house to "assist" with the technology, and if things go wrong, it is extremely hard to audit where things fails.
Pretty much the only reliable way in my view is digital (but on site) voting with voting computers that leave a paper record. Ballots are computed automatically, but a statistically relevant random subset of locations are recounted using the paper records.
@_date: 2019-12-14 18:47:04
You talk a lot about "original protocol" and "2nd layer" in your question and comments, which make it sound like you're defining the 2nd layer as whatever workarounds are added elsewhere to overcome deficiencies in the original design.
That is not the separation between the two. The separation is on-chain and off-chain. That's the point: the 2nd layer can achieve things the 1st can't because payments in the second layer do not need to be broadcast and agreed upon by the entire world.
It is not a question of original design or not. Payment channels (in a more rudimentary than LN, and eventually shown insecure way) *were* part of the original design. But even if LN had been known at the time Bitcoin was designed, it would still have had two layers in its implementation: the part of payments that end up on chain, and the part that doesn't. Also, LN as it exists today relies on "1st layer" features that actually weren't in the original design (things like relative locktime, checksequenceverify, and segregated witness). The on-chain layer has improved significantly over the years, and so has our understanding of how to build things on top.
And taking this separation into account, your questions become easier to answer too: no, we don't know how to design a single on-chain way of doing payments that scales well while maintaining decentralization. A blockchain based system inherently scales very badly: all participants see, verify, and agree on everyone's transactions. Thankfully, there is no need for every payment in the world to become such an on-chain transaction. LN takes advantage of that.
@_date: 2017-11-11 20:55:11
It certainly became much better at it.
@_date: 2017-11-26 20:16:44
Yes, SHA256 is just a hashing algorithm. In takes as input an arbitrary sequence of bits, and produces 256 bits of output that are as unpredictable as possible.
There are various algorithms for mapping arbitrary data onto points on the curve. The approach depends on whether you want to create a point which (a) known discrete logarithm (for example, creating a public key - where you know the discrete log, known as the private key) or (b) create a point with unknown discrete logarithm (unknown to everyone).
In case of (a), you simply invoke a normal hashing algorithm (like SHA256) on your input, and interpret the result as a number (in binary), and multiply it with the generator.
In case of (b), you invoke a normal hashing algorithm and interpret its output as the X coordinate of a point, and then find the corresponding Y coordinate. It's a bit more complicated because only around half of the valid X coordinates have a corresponding point.
@_date: 2017-11-01 23:51:12
1 in 10^9, not 1 in 109 :)
@_date: 2017-11-14 06:24:19
@_date: 2017-11-11 21:30:38
Yes, the problem was that you could copy your wallet.dat file and open it twice _inside the same Bitcoin Core instance_. Opening HD wallets simultaneously on multiple machines should work, as long as you don't try to create transactions simultaneously from multiple instances.
@_date: 2017-11-11 18:02:25
It's unfair to say just GUI support is missing. While the `addwitnessaddress` RPC works, it's not full integration even at the RPC level.
The problem is that when you use `addwitnessaddress`, the wallet explicitly imports that address. This means you either need to create a wallet backup after every new address, or risk not finding transactions after a restore.
In the next version we plan to have a configurable address type (legacy, p2sh, bech32), which works correctly with backups. At that point, it will also be safe to enable from the GUI.
@_date: 2017-11-08 01:54:50
**Bag door**
1. Vulnerability in cryptocurrency software or systems that results in hodlers of said currency to become bag holders.
@_date: 2019-12-11 01:30:28


This is a common misconception, but it is not true. Digital signatures and encryption are both domains of cryptography, but they're not the same, nor is one based on the other.
There are some algorithms that can function as both, most notable RSA. I believe that's where your misunderstanding comes from. An RSA digital signature is superficially indeed "RSA encryption with public and private keys swapped".
This is not universally the case, however. Bitcoin uses ECDSA for authenticating transactions, an elliptic curve variant of the DSA digital signature algorithm. DSA is purely a digital signature algorithm and is not usable in any way as an encryption algorithm (in particular, there is no process where you can get the message back out of a signature, given the public key).
@_date: 2015-08-26 16:49:14
@_date: 2019-12-01 00:16:47
Have you ever looked at actual cryptographic voting techniques? There is an entire subdomain of cryptographic research on this topic. State of the art solutions provide enforced secrecy, plus proofs to the individual voters that their votes were correctly counted, as well as being auditable in general. They're not perfect, but in general provide much better features already than anything people think of a blockchain based voting.
As the blog post author notes, there are some things you could use a blockchain for in voting, such as tracking metadata about who has voted (but not their actual votes). It's a possibility, but I suspect it's not what you're thinking about, and there isn't much to discuss either. The interesting part (submitted and tallying votes and keeping them secret) would be done elsewhere.
@_date: 2015-07-28 17:16:10
Note that as long as miners enforce 0.8 or later standardness and don't disable script verification, there should be no problem. It was only because of evidence in Bitcoin of miners not doing validation that this went from "possible but expensive" to "possible with potentially zero cost".
@_date: 2019-12-02 01:03:18
Well, that depends on your own thresholds for who to trust. The point is that Bitcoin is only what it is because you don't *have* to trust anyone. If that is not important to you - great - there are plenty of easier tradeoffs you can make in using the system.
@_date: 2019-12-01 00:39:48
If your question is whether signing a message is secure, when done through well-reviewed software, and while taking care your keys don't get leaked in other ways, the answer is yes. Digital signatures were designed to remain secure regardless of how many signatures are created with the same key.
That said, I'd strongly discourage you from participating in an obvious scam. There are risks from just using software you're not used to, or additional expose to your private keys. If you do participate, don't be the last person out.
@_date: 2019-12-10 23:57:04


I suspect you mean cryptography, or digital signatures. Bitcoin does not use any encryption.
@_date: 2019-12-05 03:43:36
My point is that getting MW into Bitcoin is likely literally a 100x larger effort than Schnorr signatures *and still include all the work for Schnorr*. So I find it strange that you'd want MW because it has Schnorr; if that's the case, just work on Schnorr.
As far as LN and MW, I don't know. I wouldn't be surprised if LN partially undoes the advantages of CT (because it needs balances that public to the channel participants). I also don't know if Lightning is possible on top of MW (it may be, I just don't know).
@_date: 2019-12-05 02:53:43
Maybe I worded a bit too strongly. I think it's very good that people point out where (expected) privacy is broken, whatever the reason. And clearly, some people had the impression that MW itself adds privacy of this type. But I also think it's good to put things in perspective; just because one aspect isn't what people expect does not mean MW as a whole is a bad idea.
@_date: 2019-06-16 18:42:49
"Entities **chose** to set up" and "trustless" don't really go well together.
Certainly having a number of parties who run FIBRE networks helps the public network, by creating something like "wormholes" that connect nodes that are otherwise very far apart. The rest of the network benefits as well, as they see the faster blocks indirectly.
However, this does rely on the parties running FIBRE. They can choose to shut down at any point. If you don't want that, you can run your own FIBRE network.
@_date: 2017-11-06 19:44:54
A good place would be 
@_date: 2017-11-15 22:31:43
Signature aggregation always helps when doing CoinJoin, as otherwise you have at least one signature for each of the inputs. 
@_date: 2015-07-29 10:30:35
Indeed, and it was Bitcoin's fault for using it.
By the way, I learned today that OpenSSL was actually aware of this, and fixed it in June *2014* in their master branch. It was never in a release, however.
@_date: 2017-11-11 18:03:14
Like always, make a backup of your wallet.
@_date: 2017-11-10 00:56:20
Adding more decimals than 8 is either:
* A softfork through an extension block, which cannot change the currency supply. So the number remains the same in that case.
* A hardfork. Anything can happen in case of a HF, so the new total supply will be whatever the community accepts at that point.
@_date: 2019-12-14 18:23:28
Where did they say that?
@_date: 2017-11-08 00:24:45
Pasted into a Word .doc please.
@_date: 2017-11-16 00:27:43
That's hard to say, and depends in what form things get incorporated into Bitcoin. This is all just hypothetical now.
I'm very happy to see Monero experiment with technology long before it'd ready for Bitcoin, but it has problems of its own. The RingCT approach has far worse scalability issues (unprunable txouts), and the privacy it grants is not perfect either (better linking analysis can sometimes rule out some of the chaff).
@_date: 2019-12-17 19:44:06


Yes, you can construct the scripts, but they're not really useful without applications that know how to sign for them. Miniscript (as a concept) solves this too, as it enables generic signing for any Miniscript-compatible script, but using them will require applications/wallets/libraries/signers to integrate Miniscript support.
@_date: 2017-11-28 02:51:25
North Corea, LOL!
@_date: 2019-12-11 02:02:25
If you abstract things far enough, you'll see plenty of similarities everywhere.
But no, encryption, digital signatures, message authentication codes, ... are all distinct things.
Encryption schemes are solutions the confidentiality problem: they make sure that information can only be read by the correct parties. This implies there is a corresponding decryption scheme that can convert ciphertext back into plaintext. Encryption can be symmetric or asymmetric.
Digital signatures are solutions to a distinct problem (authentication and integrity). They aim to make sure a message was created by the right party without tampering, but don't prevent the wrong parties from seeing what the message is.
Password based authentication is a solution to a similar problem, but in a symmetric setting, and without messages. Typical solutions are based on hashes, but you see that that is a very distinct problem from encryption. There is no plaintext or ciphertext at all.
I think a common mistake people make is to confuse cryptographic schemes (encryption, decryption, signing, ...) with some of the lower-level tools these constructions are built from (things like hashes, block ciphers, exponentiations modulo a large number, elliptic curve operations). If you dig deep enough, you'll indeed encounter many building blocks that are common to different types of cryptographic schemes.
@_date: 2017-11-11 23:38:42
Paging @_date: 2019-12-05 04:25:18
People are free to work on what they want, and use what they want.
I simply don't think solutions that inherently permit stealing coins by miners are interesting. That's my personal opinion, and guides what I choose to work on personally.
@_date: 2019-12-05 02:51:24
I don't think (public) sidechains are a good idea for anything beyond experimentation. The fact that a hashrate majority can steal coins isn't something Bitcoin was designed for, and I don't think should be acceptable to anyone actually using it. This is especially true in the presence of significant mining centralization.
@_date: 2015-07-28 22:36:53
It's more complicated than that.
This BER issue could cause forks before BIP66 took place, as soon as a specially-crafted transaction made it into the blockchain. Such special transactions would be nonstandard in Bitcoin Core 0.8.0 and later, so it required either:
* (a) a miner cooperating in the attack (a small hashrate suffices, however)
* (b) a miner running pre-0.8 code
* (c) a miner that intentionally disabled some of the validation
Since the 95% threshold of BIP66 took place, no more forks are possible, as such a forking transactions is now illegal according to the consensus rules of the network. That means that rather than having some node with parsing issues, all of them will outright and in unison reject it. Even if miners now don't validate fully, no forks will appear because those miners would just be cutting in their income by creating an invalid chain.
This was especially worrisome for Bitcoin only after the OpenSSL issue in January 2015 made it clear that some miners were accepting non-DER transactions, presumably because of the (c) option above. Note that absent other problems, the SPV mining we've seen more recently should not cause this.
@_date: 2017-11-15 21:43:50
Yes and no.
I don't expect CT itself to be a concern here, as it only hides amounts. The receiver of a payment always learns the amount anyway (just the rest of the world doesn't), and it's always possible for you to prove to anyone what the actual amount is (without revealing your entire wallet, or giving the ability to spend it).
Things like CoinJoin - which is made much more applicable in a CT world - have the great property that they're indistinguishable from normal transactions.
In general, this issue is inevitable unfortunately, and it's the reason why you want privacy technology to be the default, or at least not come at an extra price. Explaining why you used a more private and expensive approach is much harder to explain than just relying on everyone using it.
@_date: 2015-08-12 06:50:36
I think a jump to 8 MB is a bad idea for technical reasons. There are various pieces of infrastructure that have not actually been tested (block creation, propagation time on the actual internet, memory usage under DoS scenarios) that make a sudden large jump inadvisable. We need more research before that first.
One solution to that would just be to have a faster growth initially to "catch up" on lost growth in the past. I did not do so in my proposal because I can't say that the current situation is undeniably satisfiable (remember: I'm seeking to find a non-controversial solution, not just a feasible one), given miner's need to already not actually validate blocks.
That leaves the option of doing a small increase like BIP102 first. I have no technical concerns about that, but what problem does it solve? It is kicking the can down the road so people can continue their belief that they don't need to cope with the block size being finite, and I don't believe a small constant factor makes much difference in usefulness anyway.
@_date: 2019-12-02 00:50:08
I think u/TheGreatMuffin's point is not that it is necessary for you to run a full node if you don't feel it benefits you, but that you should be aware that it is important for people in general to be able to do so. If you never have, you may not be aware of why people think that capacity in increases are concerning (because they are mainly felt by those who do run a full node).
@_date: 2019-12-05 22:56:09
MW is more scalable than CT, but comes with trade-offs on functionality.
CT is significantly less scalable than Bitcoin is today.
@_date: 2019-12-02 22:43:01
The difficulty is linear. Doubling the difficulty means twice as much work per block. Specifically, difficulty N means around 4 billion times N hashes per block.
Intuitively difficulty is often explained as "number zeroes in the hash" (and it was in the original proof-of-work construction that predated Bitcoin), but that is not how it works at all. In fact, difficulty doesn't even exist at the protocol level. Instead, there is a *target*. Every hash is interpreted as a 256-bit number, and it is acceptable as proof of work if that number is below the target.
Difficulty is a human-readable way of representing the target. Specifically, it equals 2^224 divided by the target. Doubling the difficulty means halving the target.
@_date: 2017-11-15 23:29:36
There could be a special kind of output you have to create that indicates you want to move funds back, together with a proof.
Consensus rules could then be in place that require miners to pay you out the relevant amount from the pool of CT funds in a separate transactions.
That's just a bit of brainstorming. I'm sure better constructions are possible if we start thinking about them, but there is no urgency here.
@_date: 2019-12-19 22:33:49
I am genuinely sorry if my comments made you feel bad. That was not at all my intention.
I'm merely trying to prevent you from wasting your time based on a misunderstanding. If you feel that despite the numbers cited, you want to continue exploring mining on your home system, please don't let my or other's comments stand in your way.
@_date: 2017-11-16 17:12:19
There'd no urgency because the technology is not nearly ready. See my post at the top of this thread.
@_date: 2019-12-19 16:45:31
Bitcoin has no main site. Anyone can buy or run a site with bitcoin in the name. In particular, I would not trust bitcoin.com for anything.
@_date: 2019-12-14 21:09:27
IANAL, but I'm skeptical of the conclusions in that article. As far as I know, stocks are also treated as property, and there is no requirement that you treat every individual quantity of stock as a separate entity for tax purposes. You need to have a consistent strategy to determine a mapping between buys and sales (such as FIFO, ...), but that's it.
@_date: 2019-12-09 10:14:02
By grinding I just mean computationally intensive operation that tries lots of keys until one with the right properties is found.
A bit more technical:
* You give me a key A
* I search for a key B and C such that Hash160(A and B) = Hash160(C). This can be done in 2^80 work using a collision search.
* I tell you my key is B (and it is, it's a legitimate pubkey I have the private key for).
* You use the 2-of-2 address e.g. as an escrow and deposit coins in it.
* I take the coins, by revealing the script as C rather than A-and-B.
The actual mechanics are described in bits and pieces in this mailing list thread: 
Let me reiterate that it does require 2^80, which while by today's standards is not sufficient for long term security, is also not practically feasible to actually mount such an attack.
@_date: 2017-11-11 19:26:02
We used to treat every byte in a block the same, and had a rule that the total number of bytes was 1000000 at most.
With SegWit, a new way of counting bytes is introduced, where outputs (which burden the system more in the long run) 'weigh' 4x more than signatures. The new rule is that the total weight of a block is at most 4000000.
Before 0.15.1, the mining code could be configured to limit the total number of bytes, or to limit the total weight, or both. This has been removed, and you can now only configure the maximum weight.
@_date: 2017-11-16 08:17:01
Sometimes Litecoin also serves as a guinea pig that deploy some features a bit earlier :)
@_date: 2019-12-05 03:08:28
Working on integrating MW because it includes Schnorr and you like Schnorr is like buying a 747 because you need a seat.
@_date: 2019-12-19 21:10:11
You're not supporting the network by adding 0.000000001% extra hashrate (that's approximately the actual max you could add with one GPU).
If you don't worry about mining at a loss because you're burning electricity for heat anyway, get an ASIC miner instead. They're easily 10000x more efficient at mining than your GPU.
@_date: 2019-02-02 17:31:28
You're right, these are equivalent policies, and the compiler should find the same script for both.
The reason for not doing so is because there is no logic to rewrite the input before feeding it to the optimizer, and the output has separate script instantiations for thres policies and multi policies.
In particular, there are 3 special cases of thres that can be considered:
* thres(k,pk(A),pk(B),...) can be multi(k,A,B,...).
* thres(n,E1,E2,...,En) can be and(and(...(and(E1),E2),...),En).
* thres(1,E1,E2,...,En) can be or(or(...(or(E1,E2)...,En).
@_date: 2015-07-29 09:33:46
My parents were clairvoyant, and knew that one day in my life I would end up working in a space where other people named Peter would appear.
Hence, they decided to place a tiny and subtle disambiguating line with a dot above it in between the 'P' and the 'e' in my name.
@_date: 2019-12-19 20:11:42
Then what else would you do it for?
@_date: 2019-12-31 20:26:02
I think that's just because the pricy rally in 2017 and the media attention following it brought in a lot of additional people. Those people, motivated by seeing the price pump, are obviously primarily interested in seeing the price pump.
But those interested in technology are not gone; we're still here. Reddit is perhaps just not the best place to get a good sample.
@_date: 2019-12-24 09:26:25
What does this mean? A Unicode code point already exists for BTC ()
@_date: 2015-07-29 12:12:51
You're welcome!
@_date: 2017-11-02 06:34:38
That's true for the private keys or seeds, but not for the randomness that goes into the signatures your hardware device produces. If that randomness is bad, it could unnoticeably easily leak information to for example the creator of the device.
There are ways to construct that randomness jointly between the device and the wallet software (so that a leak is only possible if both are compromised), but AFAIK no hardware implements this (I'm not very up to date here, though).
@_date: 2019-12-05 22:54:00
It's a very interesting scheme, but I'd need to read up on the details and how it's implemented to comment.
@_date: 2019-12-05 00:28:01
That article is seriously confused. Mimblewimble is not a privacy technique at all.
Mimblewimble is a (more invasive) improvement over Confidential Transactions (a technology to hide transaction amounts). It offers the following:
* Non-interactive CoinJoin, making it much easier to aggregate multiple transactions (reducing costs, and obfuscating linkage).
* Transaction cut-through, allowing validation of history without seeing the details of all transactions, speeding up initial synchronization (at the cost of no longer having a Script language for complex on-chain policies).
Neither of the above advantages above directly contribute to privacy. The first can help, but CoinJoin is always possible - it's just much easier to implement and use in a MW-style system.
What Grin (a specific Mimblewimble-based blockchain, but please don't confuse it with Mimblewimble as an abstract technology itself) did was implement non-interactive CoinJoin directly on the P2P layer. So when people would construct normal non-obfuscated transactions, they'd get automatically CoinJoined with others on the network. This is a great way to reduce bandwidth, but it obviously cannot give a privacy benefit if attackers can actually observe a significant part of the network. It is this assumption that automatic-CoinJoin-on-a-public-network is effective as a privacy technique that is broken. CoinJoin itself is fine, and still benefits from MW - it just has to be used in a private way. 
I believe MimbleWimble - as an abstract technique - is very interesting. It offers a much more scalable design than a Bitcoin-style blockchain with CT (and far more scalable than Monero or ZCash), while offering non-interactive CoinJoin. It does have its downsides of course. No scripts would be hard to sell to the Bitcoin ecosystem, and CT itself is highly nontrivial in validation and bandwidth costs.
@_date: 2019-12-09 17:57:24
When sending coins from the exchange to your own wallet, no private keys of yours are involved. You're instructing the exchange to sign transactions with their private keys, which you don't have access to.
After that, the coins are in your control. If at that point you want to send them anyway - to pay for something - your wallet software/hardware will use its private keys to sign off on the transfer.
@_date: 2019-12-05 22:54:30
Not orders of magnitude. More like 10-20%. The only thing gained by cross-input aggregation is the signatures; not the UTXO references and other metadata.
@_date: 2017-11-15 22:58:32
Right, and signatures can be aggregated much better.
We know how to aggregate an arbitrary number of signatures into a constant 64 bytes.
Aggregated range proofs using Bulletproofs still require a logarithmic size growth (you can have twice the number of proofs with 64 bytes extra).
@_date: 2017-11-16 18:03:06
You're absolutely right that many contributors to Bitcoin Core (and the wider Bitcoin development community) aren't the best communicators. This is a problem, and needs improving. Blog posts (something I dread writing...) are one way.
But Core is not a formal organization. It's a loose group of people who collaborate on a software project. The correct response, on my opinion, isn't to appoint a spokesperson. Nobody really can speak on behalf of everyone involved, and I don't think that's needed either. There are plenty of other ways to communicate.
@_date: 2019-12-19 17:38:45
There is absolutely no point.
With a high-end graphics card you can maybe mine 0.05 USD worth of BTC per year at current prices and difficulty. Meanwhile you'd likely spend several $100 on electricity, and that's ignoring the cost of added wear and tear of your hardware.
You're about 7 years too late.
@_date: 2019-12-01 03:51:48
All the same sounds fine, though definitely a 0.13.1+ one (otherwise it can't serve witness data). There isn't much variation in different version's performance in serving blocks, at least not when internet-like latencies are involved.
@_date: 2019-12-02 22:06:49
This is a known issue that will be fixed in 0.19.1. It is just a reporting problem, the underlying logic is unaffected.
@_date: 2019-12-01 00:44:50
Yes, you can do an independent benchmark with just reindexing that just tests how fast validation is and not any of the block fetching logic.
That won't be representative of real world performance, though. Especially before 0.10, Bitcoin Core would often spend large amounts of time just finding the right blocks.
An alternative is to have a number of fixed servers to download from (maybe even in geographically diverse locations), and use `connect=IP` to connect to all of them.
@_date: 2015-07-29 08:32:02
Short addition: such transactions haven't been relayed/mined by Bitcoin Core software for over two years.
The fix is that such transactions are now unconditionally invalid to put in the blockchain, so it is not only avoided by standard software, but also invalid to do for anyone who would do so due to modified software (accidental or intentional).
@_date: 2019-12-15 04:02:50
Sure, but I don't think that's a meaningful one (also see comment elsewhere).
Yes, in a way, LN results in cooperative cases in transactions that settle the aggregate of a number of payments on chain. So far, that indeed vaguely looks like a squash of a number of commits in Git.
Where it breaks down is that this is not the interesting part of LN. If you're using an online service to manage your coins, the same happens: transfers within the service don't go on chain, and when people withdraw their coins to their own wallets, that's effectively posting an aggregate of internal transfers on chain. Both LN and the widespread usage of such a service improve scalability, but clearly everyone using a single trusted provider is the exact opposite of what we're trying to achieve.
The interesting part of LN (or any payment channel construction) is that it constructs these aggregates without a trusted party even temporarily holding the coins. They're simply negotiated between the participants, and incentives are added that permit taking counterparty's coins when they try to settle an old state on chain.
A better analogy is that LN is "settling" payments out of court by cooperating, while retaining the ability to go to court (the blockchain) when the counterparty cheats. Knowing that the court will always rule justly (consensus rules) means that both parties are incentivized to act honestly, even if they never actually go to court.
@_date: 2019-12-24 10:25:58
Ok, so you just want the existing symbol made visible in common mobile keyboards? That doesn't need an emoji, or anything the Unicode consortium has to do with I think.
@_date: 2015-07-29 10:39:44
Not fully implementing BER is a bug, or at least a feature that would be reasonably considered missing.
@_date: 2019-12-05 22:56:47
I honestly don't know. The MW is very appealing, but it comes with very significant reduction in flexibility.
@_date: 2019-12-10 06:15:32
It certainly isn't backed by hashing power. There is no way to convert BTC to hashing power at a guaranteed exchange rate.
If you're going to offer to trade BTC for gold at a fixed rate, and actually have enough gold to credibly guarantee that regardless of market demand, then I guess you would be able to say BTC is backed by gold. But today that certainly isn't the case.
@_date: 2019-12-09 09:49:17
There exists a (in practice still infeasible) 80-bit collision attack on P2SH. When a number of participants (more than one) are jointly constructing a multisig address, one can grind the key he gives to the others to be such that he can take the funds without the other's consent.
It does not apply for single-user addresses (e.g. P2SH-P2WPKH is inherently unaffected).
@_date: 2019-12-05 04:27:29
I was only responding to "one of the nice things about MW is that it includes Schnorr".
@_date: 2017-11-04 20:44:59
I strongly support the use of SegWit transactions.
@_date: 2019-12-05 03:20:21
Nothing is "sufficient" for privacy. It's a goal to work towards, but it is so multi-faceted that no single piece of technology can "solve" privacy.
@_date: 2015-07-28 16:26:39
Better testing is good of course, but it cannot be guaranteed in this case.
The problem is that we're not testing against a known specification. To prevent forks, we must produce software that results in consistent network behaviour - in other words, mimicks the rules the current network already enforces. We could write down a document to describe it, but we can't prescribe it. If a bug is found in the current software, the document would be wrong, as we can't just change the software everyone already uses.
Comparing different implementations is the sort of testing that could reveal it, but without knowing in what ways to implementations might differ, it is exceptionally hard to randomly produce data which triggers the differences.
At a lower level it is easier to do. For example, exactly that type of testing in libsecp256k1 (the ECDSA validation library Bitcoin Core will likely switch to in the near future) has revealed a bug in OpenSSL's BN_sqr function before.
@_date: 2019-12-11 00:04:45
You don't have any connections at all, it seems. Is there something wrong with your internet connectivity?
@_date: 2015-07-28 15:14:43
It is a separate issue.
@_date: 2017-11-14 20:38:53
Before, with earlier designs of Confidential Transactions, larger transactions would have larger range proofs as well.
With this new design, the size of the range proof (almost) stays the same, regardless of how large the transaction is.
@_date: 2019-12-20 21:36:16
I think Green is a good wallet, but its strength is additional security through an integrated multisig-based 2FA service, not privacy.
Disclaimed: I work for Blockstream.
@_date: 2019-12-05 01:04:08
It's not that simple.
I would personally very much like to see Confidential Transactions in Bitcoin. Hiding transaction amounts by default - while not a silver bullet for privacy on its own - would make CoinJoin a lot more powerful (right now you need to use matching amounts because you'd leak linkage otherwise anyway). I think it's fair to say that it may help achieve a level of privacy that is very hard to reach with existing on-chain techniques.
Confidential Transactions however very fundamentally change how transactions work, as cleartext amounts are currently expected in transactions. Without (extremely invasive) hard fork, this cannot be changed. Even if they're suddenly permitted, nobody can force existing wallets to suddenly adopt them. Doing so would break compatibility, and go against very basic expectations of not invalidating existing non-broadcast transactions. Such a change being successful probably implies Bitcoin lost some of its most valuable properties to begin with. Thus: CT (or any form of amount hiding) has to be opt-in.
But opt-in doesn't need to imply opt-in on a per-transaction basis. An extension block effectively does that: by having two clearly delineated sides and a need for explicit, possibly slow/expensive, operations to transfer between them, you create a world where CT *is* the default, and possibly even cheaper than the other side. Sure, people still have the option to use the legacy side, and for a long time they probably will due to compatibility reasons, but in the long term it probably means much better privacy than any solution with per-transaction choice for CT or not. It turns out that CT-in-an-EB is also far simpler and more efficient than trying to hack it into the existing transaction structure.
So, I believe that Extension Blocks are the only (somewhat) practical way of introducing CT to Bitcoin.
That said, there are many caveats:
* CT transactions are far more computationally expensive and larger than current transactions, and it would be very unfortunate if the more private choice ends up being more expensive to use.
* CT introduces a much stronger assumption on cryptography than we currently have. You can't just run through the UTXO set, sum up the values, and see that it doesn't exceed the expected subsidy.
* In fact, CT inherently either must make privacy condition on cryptographic assumptions, or soundness (=printing of money). Bitcoin currently relies on the ECDLP assumption for theft, but this can (in the long term) be upgraded to another assumption if necessary (e.g. because we believe ECDLP is on shaky grounds, quantum computing, ...). With CT this is not so easy anymore, as this assumption will now cover amounts as well.
* It's a pretty damn big change that would need a huge demand from the ecosystem to be successful.
* All the same issues apply to MW, and more. MW is a more advanced form of CT that has an even more invasive impact on basic data structures, and probably simply cannot be done without an EB at all, as it is so fundamentally different from Bitcoin's current blockchain (the MW blockchain can shrink over time!). It also removes Script or even the ability to have something Script-like.
Let me come back to point (3) above. There is a very fundamental result in zero-knowledge proof techniques that you cannot have both unconditional privacy and unconditional soundness. We however do know of ways to have either unconditional privacy or unconditional soundness, so there is a design choice between them.
* CT with unconditional privacy but computational soundness is the most common choice. This means that if somehow ECDLP breaks (math breakthrough, unexpected structure in secp256k1, quantum computer), someone could undetectably print coins, but the privacy of past (and future) transactions would be unaffected. To the best of my knowledge, Monero, ZCash, Grin, all use this model.
* CT with unconditional soundness but computational privacy is also possible. This means that an ECDLP break would not let anyone print coins, but the privacy of future (and past) transactions would be at risk. Unfortunately, this choice is much less efficient, and pretty much no systems use it.
Given Bitcoin's design focus on controlled inflation, I expect that many people would prefer the second model over the first if a choice needs to be made. There is however also a point to be made that if ECDLP is broken, the future of the system is inherently at risk, but we may not want to give up the privacy of the past when that happens - a point in favor of the first model.
The nice (or scary...) thing about an Extension Block based CT design is that we could have either, or both, and without actually directly affecting the value of the legacy chain. You'd need to move coins explicitly to the CT side, and if unexpected inflation would happen there, simply not all of it would be able to move back to the legacy side. Unexpectedly, this may actually mean a different exchange rate for coins on both sides, if the public's trust in the security for one is seriously affected.
@_date: 2019-06-24 15:00:04
Yup, absolutely (I do).
@_date: 2015-07-28 15:27:12
Thanks :)
@_date: 2017-04-05 09:43:38
All lowercase addresses (for P2PKH) would be around 100 million times harder to generate than random ones. I don't think that's really acceptable for general recommendations.
@_date: 2019-04-17 02:31:49
@_date: 2019-12-31 12:27:11
You've been told already that the Unicode Consortium explicitly lists petitions as the wrong way to get something included.
A BTC symbol already exists too. How would an emoji be different or add anything?
@_date: 2019-12-05 20:28:45
Those provide a completely unrelated type of privacy. Personally I'm certainly focussing on those, but it's not like it's one or the other.
@_date: 2019-12-14 23:22:52
I agree, that's the best analogy I've seen.
@_date: 2015-07-13 02:37:19
We're working on mempool limitation, but there are no such changes in 0.11.0 yet.
@_date: 2015-07-29 12:13:04
Working on it.
@_date: 2017-11-11 23:54:33
@_date: 2017-11-15 21:38:23
Well, CT does not exist in Bitcoin.
Bulletproofs improve size and perhaps performance for validating CT transactions, but compared to Bitcoin transactions as they exist now they're still a size increase and performance reduction.
The extra privacy comes at a cost, which is why it may not be easy to get this accepted for Bitcoin.
@_date: 2015-07-29 12:16:18
It's easy to bash OpenSSL - it's an old piece of software, that was not very well maintained for a long time, implementing a lot of very complicated algorithms and protocols.
The bottom line, I think, is that it is just not designed for consensus-critical applications. Bitcoin should never have relied on the parsing implementation of a third party library to be consistent across versions and platforms.
@_date: 2012-12-10 12:18:24
Source code can (and is) verifiable by many eyes. Release binaries are built in a deterministic environment (virtual machine) so that it can be  repeated by several developers (and anyone who wants) to get byte-for-byte the same ouput, and signatures on the binary are obtained before release. The process is hard, but at least it gives assurance to the extent possible that the binaries match the source code (which is signed cryptographically by git).
A pre-indexed block chain does not have this property. it's just data, and you can try to rebuild it, but nobody would see that someone makes a mistake (even accidental, if it's caused by a bug), which causes some future spend to be considered invalid, and cause your client to switch to fork.
If you're going to trust a centralized source to sign the block history for you, you're better off using a lightweight client which doesn't require block history data in the first place. At least the data it relies on is verified automatically by the entire network.
@_date: 2012-12-15 00:18:11
When you wrote the OP, I really wondered why you thought that as the code "obviously" randomized things. I should at least have checked :)
@_date: 2012-11-27 11:42:53
This type of payment is one where there is already communication between the sender and the receiver - typically in the form of some web shop, or a promise for a certain service. No anonymity is lost by sending an invoice/payment/receipt back and forth. In fact, this improves privacy, as it simplifies using a separate address for every transaction.
@_date: 2012-12-02 22:56:19
This is incorrect. The reference client lets you spend unconfirmed change. For incoming transactions from others, it requires at least one confirmation, but when the sender is itself, this is not required. I assume the OP isn't using the reference client.
@_date: 2012-12-03 22:13:44
The data is coming from other nodes on the network (just like when the reference client synchronizes its block chain, lightweight clients synchronize their block headers). In contrast, Electrum is not a node participating on the network - it simply gets its data from a central server (though recent versions have some protection against the server lying to you).
You are giving up trust to the network. In particular, the security model of lightweight clients assumes that the longest block chain only has valid transactions. This only makes sense because a miner trying to add invalid transactions to blocks would make him be ignored by every full node on the network. So in practice, lightweight clients get their security from the prevalence of non-lightweight nodes on the network.
@_date: 2012-12-12 10:32:28
It is true that revealing a public key makes it less secure, but only in a very theoretical setting. In practice, a single key is certainly secure for any amount of transactions.
The reason you want to use a different key for every transaction is privacy (or you and of everyone in the network). Bitcoin works pseudonymous and not anonymous (as the whole world can observe all transactions). The only means for privacy is making it hard to link addresses/pubkeys to identities. Reusing the same address makes it significantly easier to do such linking analysis.
@_date: 2012-11-28 08:13:02
"Ultraprune" is the name I've been using for the new database layout + validation engine for 0.8. It's not a setting, and it also doesn't mean you get a pruned block chain (though it will support that somewhere in the future).
@_date: 2012-11-28 08:16:39
It shouldn't be I/O bound unless you're on very slow storage (like a USB drive, or VPS with virtualized storage). The most apparent problem with initial block chain sync now becomes the crappy logic for fetching blocks for peers. There may be an improvement for this before 0.8 is released, but a definite solution is probably for later. After the last checkpoint (block 193000 currently), signature checking is enabled, and this is very likely the bottleneck after that point.
@_date: 2012-12-14 00:22:34
I disagree that wallet is a bad abstraction. In addition to private keys, it also contains the actual transactions which credit you, and you need those to be able to spend coins. They can be synchronized from the network if already confirmed, but there is no requirement for that.
And yes, you can copy the wallet file, and that will effectively duplicate the money from the perspective of the program. The network deals perfectly with copied money though :)
@_date: 2012-12-06 19:08:35
I wish this was possible, but there is no secure way to do so right now. The reference client implements a full node, which is supposed to verify everything. The only way to be sure about everything, is by processing the entire history. This means downloading and working through all the block data to date. Version 0.8 will be much faster in doing this processing than existing versions though, because the much smaller database it needs to maintain while doing this. Currently, it is not downloading that takes time by the way - it's maintaining the transaction database. To you give an idea, I just benchmarked (under very laboratory conditions, with some experimental patches that may not make it into release, with the block data already on disk, with a lot of cache, and a very fast machine) a rebuild of the entire 0.8 database in under 14 minutes. Synchronizing from a (fast) node over the internet from scratch on the same system took 37 minutes. Note that in normal conditions, synchronization may be a lot slower than that because the current way for fetching blocks from peers is quite stupid.
There are two possible future improvements you may have heard about, and are confusing with what 0.8 will be done. The first is initial headers-only mode, the second is committing UTXO data to the coinbase.
So, initial headers-only mode means the client will start as lightweight client (comparable to Multibit or Bitcoin Wallet for Android right now), where it synchronizes by downloading headers, and fetching only transactions that may influence the wallet (and discard the rest). When this is done, the client upgrades itself to a full node in the background when processing/storage power is available to do so. Note that while in lightweight mode, a client doesn't provide service to the network, and is unable to mine. This is not implemented yet.
The other idea is to make full nodes maintain a single hash (a merkle root) of the UTXO database, and store and verify this hash inside the coinbase of every block. That would indeed allow what you propose: nodes just requesting the UTXO database, and verifying that the block chain agrees with it. This means giving up the ability to fully verify history though, but it does mean fast startup for a node that can do full validation of everything in the future. This is probably even further away, as it requires a protocol upgrade.
One last thing to mention is that the 0.8 database layout does allow pruning - it will not be included in the 0.8 release itself because of possible repercussions on the network, but technically it will be very easy to add. This would still mean downloading and processing all blocks like before, but not storing them all on disk. The disadvantages would be that such a node cannot serve blocks to other nodes (duh), and can't rescan for missing wallet transactions.
@_date: 2012-11-29 19:50:56
Very small nit: SPV clients technically do not validate the block chain at all - they only validate block headers.
@_date: 2012-12-11 23:09:30
The model is still full validation, and there are some limits to that (such as needing to download full history, build the unspent-outputs database and verify signatures). If you're on a Rasberry Pi, that will take a while in any case. But on reasonable hardware, if it takes 12 hours there's simply something wrong. I think aiming for 2-3 hours is doable.
@_date: 2012-12-03 20:53:01
Multibit does not rely on "blockchain servers" - it implements the lightweight model that Bitcoin was intended to provide from the start (read the paper), and only downloads block headers to verify history. This means you give up the "no need to trust anyone" privilege the reference client provides, but require much less resources.
@_date: 2018-03-22 03:41:41
No, it does not.
@_date: 2012-12-12 01:32:56
12 hours in Windows 7, using -connect to sync from a fast node?
The debug output is written to debug.log in your data directory (~/.bitcoin by default on Linux).
Thanks for testing.
@_date: 2018-11-12 17:48:14
Almost. Uncompressed public keys are not allowed in SegWit scripts (by a standardness rule).
@_date: 2012-10-27 12:55:08
As a matter of fact, the format of the files that contain the actual block chain don't change at. Only instead of an index that says where to find which transaction in the block chain, it uses a database with a pruned copy.
@_date: 2012-12-16 01:29:00
3447, actually.
@_date: 2012-12-14 00:25:18
The database format changed completely for 0.8. Luke's next-test has a simple auto-upgrade mechanism so you don't have to download everything from scratch, but apart from that, it leaves the 0.7 database untouched.
@_date: 2012-12-06 02:52:34
Version 0.8 will require downloading and storing the entire block chain, but it will not index it. Instead it will indeed keep a database of unspent transaction outputs, which is significantly smaller and much faster to use.
Maybe I misunderstood your post. It seemed to me you believed it would work without downloading the block chain.
@_date: 2012-12-10 00:14:35
The torrent does not contain a pre-indexed block chain - only a file (bootstrap.dat) to be placed in the data directory, at which point blocks will automatically be fully verified and imported from it.
@_date: 2012-12-10 11:30:50
If it takes you 12 hours, the bottleneck was probably a slow peer. There are some known problems with how the client selects peers to select from, and the download process is somewhat of a hack and easily disturbed (causing it to stop for several minutes frequently). This is certainly intended to be changed before 0.8 is released (older clients had this problem as well, but there CPU/disk were the bottleneck anyway in most cases). In any case, a recent benchmark of mine (on rather beefy hardware, and from a fast peer over the internet, with some experimental patches) took 37 minutes to synchronize from scratch.
@_date: 2012-11-24 18:12:44
The Bitcoin-Qt software assumes it has exclusive control over the keys in its wallets. This means for example that if you make a transaction from the blockchain.info site, and shortly afterwards another from Bitcoin-Qt, it may try to use the same coins again.This will not be accepted by the network, but the client will keep assuming its own transaction is valid. This risks corrupting your wallet, making a large part of it unspendable.
@_date: 2012-12-11 01:19:11
If anyone wants to help with translating the reference client (Bitcoin-Qt), see, direct them to 
@_date: 2018-03-14 20:11:56




That's a false dichotomy. It's not because a technology cannot be incorporated in Bitcoin's blockchain that it inherently can only benefit other currencies. 
The comment you're responding to was spot on, I think: Mimblewimble is just a technology. Grin is an implementation of it as an altcoin. For the time being, I believe that is likely the only way it will be deployed, for the reasons you mention. But it is not inherent, and I think it's very reasonable to distinguish the technology from its most apparently deployment today.


It cannot use Bulletproof aggregation (which makes them grow logarithmically in the number of things being proven), because that requires interactivity.
They can use Bulletproofs fine, and even without aggregation Bulletproofs are better in every way than the older Borromean signature based range proofs we proposed for CT.
@_date: 2012-12-11 23:03:59
Booting as an SPV node, and gradually upgrading in the background sounds very good indeed. That will require a lot of code changes though, and won't be for any time soon.
@_date: 2018-03-12 16:49:02
Bitcoin Core is software that implements a Bitcoin full node.
@_date: 2012-12-10 11:38:23
The long-term idea is certainly to move more end-users to SPV nodes. A recent protocol change (transaction bloom filtering) was proposed that would make these about as fast as clients which run on a centralized server (like Electrum). The Bitcoin system was designed to support light clients from the start, but I don't think it would be good for the network if everyone switched at once. We need full nodes.
@_date: 2018-03-18 20:30:05
I know of no client that does anything like that.
@_date: 2018-03-06 23:41:34
The website hasn't included any of my suggestions :(
@_date: 2018-02-14 03:31:08


No coin is ever vulnerable to a chain fork to prior protocol version, as a large portion of fully validating nodes enforce segwit's consensus rules. If miners would stop running segwit software, they would at best just stop including segwit transactions, and at worst produce blocks that the network ignores.
I don't know who keeps spreading that nonsense that segwit is somehow vulnerable to be reverted. It's a softfork like many others have been done before, and its security relies on nodes verifying what miners do - just like all the rest of the network's rules.


I'm making a guess about what you're asking here. Please correct me if I misinterpreted it.
To spend a legacy output, you must present a transaction with a signature in the scriptSig, and no witness.
To spend a segwit output, you must present a transaction with an empty scriptSig, and a signature in the witness.
You can't spend a legacy output using a signature in the witness field, or a segwit output using a signature in the scriptSig. If that were possible, it wouldn't solve the malleability problem.
@_date: 2019-01-06 11:53:33
You keep the whole UTXO set when pruning. Without that, you wouldn't be able to independently validate later blocks.
The UTXO set is around 2.8 GiB currently.
The UTXO set is not enough to re-validate history, this means it can't be used to bootstrap other nodes, unless they blindly trust your UTXO set.
@_date: 2012-12-11 23:18:13
No, I tested snappy and it doesn't help one bit. So, it's disabled by default so we don't need to support it on every platform. If necessary it can always be enabled later.
@_date: 2012-12-09 01:13:04
I do consider SatoshiDice harmful, but I feel it is wrong to call it an "attack". By no means do they intend harm to the network, and I suppose they consider the growing pains they cause not to be their fault. Well...
I do agree that if Bitcoin wants to scale, it needs to deal with the sort of load produced by SD, but the statement "it should be able to deal with this" is not the same as "it should be able to deal with this right now". Bitcoin will scale - in fact, SD is what motivated me to rewrite Bitcoin's block validation engine (aka "ultraprune") - but scaling takes time. With gradual change, and a growing network and the economy around it, time and incentives to improve software (and write new software) will be available.
But forcing things to scale can be harmful, and SD caused a load on the network few expected. It certainly chased certain users away from running a full node (not that every user is required to run a full node, Bitcoin is designed to support lightweight nodes, but full nodes are still required). In addition, it took away developer time to deal with the sudden developments (for example, Armory's original method - loading the entire blockchain in RAM - had to be adjusted), which could have been spent on more useful changes. Yes, adapting to scale was necessary anyway, but needing to rush things is not good.
Also, I have little problem with true economic growth - and I suppose SD does indeed represent some true growth. Still, I consider their implementation to cause unnecessary load.
@_date: 2019-01-06 20:43:39
If you have a few hundred megabytes of free RAM, it very much pays off to increase the database cache size. Up to a few gigabytes makes sense (if you have that much to spare).
@_date: 2018-03-18 18:58:53
Bitcoin has absolutely no DHT.
@_date: 2019-01-12 20:34:25
I believe it still is. Where do you find support for this architecture was removed?
@_date: 2018-02-01 19:11:59
Seems I forgot the year changed. The slides were made the two days before the talk.
@_date: 2018-02-28 13:26:48
The graph shows the percentage of transactions that have segwit *inputs*.
@_date: 2018-02-27 18:27:55
Check the 'bech32' checkbox when creating a new address, or if you want it by default, start `bitcoin-qt` with the `-addresstype=bech32` option.
@_date: 2018-02-10 03:51:57
The next full moon is on march 1st.
@_date: 2018-02-14 04:40:36
See 
Almost all nodes on the Bitcoin network are Bitcoin Core 0.13.1+. All of those would reject a block which tries to spend a segwit output without valid witness.
@_date: 2012-12-05 21:48:56
Sorry to disappoint you, but 0.8 will quite certainly still require processing the entire chain before being up to date. However, it should be very significantly faster in doing this than previous versions.
There are some ideas about having the client start in lightweight mode and upgrade itself in the background, but those aren't implemented yet.
@_date: 2017-09-14 21:29:46
No, that functionality is planned for 0.15.1.
@_date: 2018-02-27 14:09:53
The PPA wasn't updated for 0.16 yet.
@_date: 2017-09-12 03:49:25
I don't think anyone disagrees with you on that. It is not released yet, and OP posted this too soon.
That doesn't change the fact that building from source on Linux is much easier than on other platforms.
@_date: 2017-09-11 22:31:10
When the binaries are released (we don't release until multiple contributors have independently produced the exact same binary). It may take a number of days due to travel.
@_date: 2017-09-04 19:17:41
No. segwit-to-segwit and segwit-to-legacy are discounted.
The size of signatures depends on the type of the input, not the type of the output.
@_date: 2018-02-03 23:41:53
0.16 does not change any blockchain database structure on disk. It will just continue where 0.15 left it.
@_date: 2018-03-14 20:25:54
SNARK validation is O(1) in the complexity of the statement being proven.
Bulletproof validation is O(n/log n) in the complexity of the statement being proven. For statements with a circuit size up to 50-100 gates or so, this is faster than SNARKs.
@_date: 2018-02-03 21:10:10
Bitcoin Core 0.16 will support sending to Bech32 addresses.
It will also support creating such addresses to receive on, but by default it will stick to P2SH-SegWit addresses.
@_date: 2017-09-14 17:58:08
As the link in the post does not include release notes, [they are here]( and a blog post is [here](
@_date: 2017-09-14 20:15:20
You can make $DATADIR/blocks a symlink/mountpoint/junction to another disk. There's no easy way to configure it, though.
@_date: 2017-09-14 19:20:28
@_date: 2017-09-01 05:55:13
SegWit transactions are only smaller to non-SegWit nodes. Compatible nodes will see the witness, and when that is included, they're about the same size as equivalent non-SegWit transactions.
However, the witness gets a 4x discount when determining their size for purposes of the block size limit and fee estimation.
@_date: 2017-09-30 04:15:00
Hard to say.
@_date: 2017-09-15 21:37:48
I would be very interested in how high you can set your dbcache without OOMing your bitcoind.
@_date: 2017-09-12 02:04:41
It's not released, just tagged. OP was a bit too soon, see [the first comment](
Tagging means that the source code is finalized, but there are still a few steps remaining before the release (such as building binaries, verifying that all developers agree on the result, finalizing release notes, publishing in various locations, ...). This will happen in the next few days. Only after that will distributions have the ability to pick it up.
was pointing out that building from source is much easier on Linux based systems. He wasn't talking about it being prepackaged in distribution repositories already.
@_date: 2018-02-01 00:14:23
Wow, I learned something today!
@_date: 2017-09-14 20:39:56
They do need your permission to change what transactions you would accept.
@_date: 2017-09-14 17:58:08
As the link in the post does not include release notes, [they are here]( and a blog post is [here](
@_date: 2017-09-12 20:11:41
Within Bitcoin *Core*?! BIPs are proposals for improvements to **Bitcoin**, especially for things that affect multiple pieces of software. Things that are specific to Core are usually not published as a BIP.
In fact, many BIPs aren't authored by Bitcoin Core contributors, and only a small subset of them are implemented in Bitcoin Core overall (compare the [total list here]( with the list of [those implemented in Bitcoin Core](
@_date: 2017-09-01 06:03:08
When determining what to pay for SegWit transactions, you need to know their *virtual size*, which is defined as their weight divided by 4.
The virtual size of a non-SegWit transaction is equal to its normal size. The virtual size of a SegWit transaction is always lower than its conventional size.
Feerates (when expressed in satoshi per byte) apply directly to the virtual size. For example, at 40 sat/byte you would pay 10000 sat for a 250 byte transaction. But a 250 byte SegWit transaction may have only 600 weight (= 150 vbytes), and so would only pay 6000 sat at 40 sat/byte.
In short, you need to know the weight or virtual size of SegWit transactions. Their conventional byte size is mostly irrelevant.
@_date: 2017-09-16 18:22:00
You can configure a maximum upload target in Bitcoin Core since version 0.12, using `-maxuploadtarget=N` or `maxuploadtarget=N` in bitcoin.conf.
N is the maximum number of megabytes per day you're willing to spend on giving blocks to other peers.
@_date: 2017-09-11 21:56:21
Yes, known issue. We try to avoid patching up LevelDB too much, but perhaps we should bite the bullet here.
@_date: 2017-09-12 00:18:05
No, this is about the UTXO/chainstate database, not the blockchain.
The new database is 2.7G instead of 2.2G (and it does grow, but much slower than the blockchain).
If you run in pruning mode, this is effectively most of your storage.
@_date: 2017-09-14 20:39:56
They do need your permission to change what transactions you would accept.
@_date: 2017-09-11 21:10:29
Would you mind also adding these extra tests:  Thanks!
@_date: 2017-09-12 21:04:22
Final really doesn't mean anything (I've argued for removing it). If you're talking about which BIPs are actually relevant to a substantial portion of the ecosystem, there are plenty of BIPs that are not implemented in Core (including 38, 39, 43, 44, and even 32 which I wrote myself was only added after several years).
About the hosting of the bips repository, I agree. It's a historic thing that should probably be moved at some point.
@_date: 2018-02-22 14:26:35


Yes, Bulletproofs can be used to prove arbitrary statements in an arithmetic circuit, just like SNARKs. You translate the problem into a circuit with N multiplication gates, with 3N secret variables (left multiplicand, right multiplicand, output product for each gate), plus a number of linear constraints that hold over these variables. One advantage is that it's possible to treat the values committed to by Pedersen commitments directly as variables in the circuit, so there is no need (typically) to reimplement EC multiplication inside the circuit.
@_date: 2017-09-01 00:27:03
Sure, as long as the old blocks are still there (not pruned), you can disconnect the blocks along the new branch of the reorg, and connect the blocks along the old branch.
The `invalidateblock` RPC does that for you. It allows you to forcefully mark a particular block as invalid. The node will reorganize to the next best chain it knows about, which may be the old branch.
@_date: 2017-09-14 20:32:51
@_date: 2017-09-07 17:21:23
That's the plan for 0.15.1; having a per-wallet setting to use P2SH-P2WPKH addresses rather than P2PKH.
@_date: 2017-09-15 06:40:34
@_date: 2017-09-14 20:32:51
@_date: 2017-10-07 17:01:52
You're missing the point. It's not helping the network, it is helping _you_.
If you're actually using Bitcoin directly, please, by all means, run a full node in order to do so, no matter how weak the hardware is.
@_date: 2017-09-19 17:18:57
I don't believe there are any problems with that.
ECDSA is believed to be reusable; if it's fine to reuse a same private key for multiple signatures, related keys should be at least as safe.
The worst known issue with ECDSA is when bad randomness is used when signing, but most projects in the Bitcoin space now use deterministic signing (RFC6979), avoiding that problem.  
@_date: 2018-02-26 12:26:12
There's no need to do that, generally.
If your wallet was originally created with Bitcoin Core 0.12.x or earlier, you may want to start with a new wallet, as those wallets were not HD wallets (which require a backup every 100 transactions). Still, everything will work fine with 0.16 even with an old wallet file.
@_date: 2017-09-20 00:29:37
To avoid all confusion: Blockstream launched a _satellite service_, namely broadcasting of Bitcoin blocks over some continents (to be extended to others later), using existing satellites, available for free.
We're not launching our own satellites.
@_date: 2018-02-06 02:23:49
P2SH-P2WPKH. These addresses look like 3.... (on mainnet), and almost every wallet software and service can send to it.
Optionally you can also request Bech32 addresses ("bc1..."), which are more efficient later on, but only modern wallets can send to those.
@_date: 2018-03-18 21:00:31
I know how DHTs work. But to the best of my knowledge, they are not used by Bitcoin software.
There are good reasons for this. Generally every node needs to have access to all data, rather than just a subset and outsource storage of other parts to others. Doing so introduces trust on other nodes, which is contrary to Bitcoin's security model.
All of that is besides the point, though. You started out arguing that Bitcoin can't be implemented in a small number of lines due to the necessity of implementing a DHT. Bitcoin Core has no DHT implementation, so regardless of whether you think it needs one, your point makes no sense.
@_date: 2017-09-14 20:15:20
You can make $DATADIR/blocks a symlink/mountpoint/junction to another disk. There's no easy way to configure it, though.
@_date: 2017-09-15 02:46:47
BIP148 has no effect anymore, as SegWit is already active on the network.
@_date: 2017-09-15 02:46:47
BIP148 has no effect anymore, as SegWit is already active on the network.
@_date: 2017-09-02 21:53:35
Perhaps you can donate some money to the meetup group to do this? It's all run by volunteers.
@_date: 2017-09-29 22:45:06
No, the plan for 0.15.1 is to produce P2SH-wrapped SegWit addresses by default. The reason is that we can't expect other wallet software to support sending to Bech32 in the near future.
However, producing Bech32 addresses will likely by an option in 0.15.1
@_date: 2017-09-14 17:53:38
That's not voting, at least not in the traditional meaning. There is no measurable majority that can decide what the outcome for everyone is.
Running a full node however is certainly deciding where your money. Moreover, it's deciding what money *is* for you in the first place.
By running a full node, especially when you base econonic acitivity on its validation (e.g. by not accepting payments that your full node doesn't accept), you're forcing anyone who wants to bypass the rules you chose (whether it is for fraud, theft, or changing the rules of the system), to have your permission first. That's what gives you the ability to not need trust in anyone - because you validate everything for yourself.
I assume that's what you mean - but using the term 'voting' is confusing, as usually refers to system where a majority can decide for others. Here, it's really just everyone deciding for themselves.
@_date: 2017-09-20 19:29:57
This means the client was shutdown uncleanly, and is now recovering from that situation.
This may take as long as the amount of validation work done in between the last clean flush/shutdown, and the time the crash occurred.
@_date: 2018-02-27 14:01:23
Bitcoin Core very deliberately has no auto-update functionality.
Nobody (not even its maintainers) should have the power to cause widespread changing of deployed software that people use to verify the rules of the system.
@_date: 2017-09-11 23:09:52
0.15 at the time of release will sync about as fast as 0.14 *at the time 0.14 was released*.
0.15 right now is significantly faster than 0.14 for the current chain.
@_date: 2017-09-04 17:57:25
Everything is compatible. You can send SegWit coins to legacy addresses and the other way around.
A SegWit transaction is a transaction that has at least one spend of a SegWit coin.
The discounting of witnesses sizes is what makes SegWit transactions cheaper. The discount is proportional to how many SegWit coins are being spent. What the transaction sends to does not matter.
In all, I agree it sounds complicated, but I don't think it needs to be. When all transaction sizes are shown in vsize or weight units (which take the discount into account), I hope users won't need to care about SegWit or not.
@_date: 2017-09-03 16:35:14
Fees are not consensus, and they can't be in a free market. Nodes on the network cannot observe demand.
However, if miners follow a rational policy, they maximize fee income per block. The best strategy for doing so is by sorting the mempool by fee per byte, and picking as many blocks as fit in a block.
After SegWit, this is fee per byte, where the bytes are the bytes according to the discounting formula. If you have 250 byte transactions without witness, you can put 4000 in a block. If 167 of those 250 bytes of transaction are now witnesses instead, and their modified size becomes (250-167)+(167/4) = 124.74, you can put 8000 of them in a block. As a result, a miner who discounts the size of witnesses for their purpose of transaction selection will win, by having more space left for other transactions.
@_date: 2017-09-30 04:14:52
If you need to produce a 32 character subset from alphanumerical characters, there are only 4 you can exclude (32 out of 36 must remain). So having some degree of similarly-looking characters is inevitable.
This is the similarity data used, by the way: 
@_date: 2017-09-03 16:12:53
If you include SHA256 in computing how complicated Base58 is, it's massively simpler though.
I'm not sure if that's a fair comparison, but if you for example look at the [Python refererence code]( I think's very reasonable.
@_date: 2017-09-01 00:14:35
In case you're talking about the chainstate (the UTXO set):
The chainstate database always corresponds to the UTXO set at the time of our 'active' block. When a new block is received, its effects are applied to the UTXO set, and we move forward. When a reorg happens, the blocks along the old branch are "disconnected" (meaning their effects on the UTXO set are reverted) in reverse order, and then the blocks along the new branch are connected.
If you're talking about the blocks themselves:
Bitcoin Core stores all blocks that satisfy proof of work and have valid headers, regardless of their validity. We certainly don't delete blocks because they're reorganized out, as another reorganization may activate them again. When pruning is enabled, old blocks will be deleted, but again, that happens regardless of validity or active status.
@_date: 2017-09-13 00:37:11


I assume it's because their authors haven't requested it. Why does it matter? As I said, I don't think the 'Final' state means much. I haven't bothered to request an update of status to BIP141, BIP143, and BIP144, which I'm an author of and are now active consensus rules on the network.


That's nonsense. I don't understand why you think implementing it in Bitcoin Core is at all relevant here. Several BIPs don't even apply to Bitcoin Core (for example BIP15, parts of BIP23, BIP64, BIP80, BIP81, BIP120, BIP121, BIP171 which are about functionality that wasn't ever considered for Core). 
In any case, I think you're missing my point. BIPs are about things that impact the ecosystem as a whole. They're the opposite of "cooperation within Bitcoin Core ", even for BIPs that were originally proposed by Core contributors. They're about things that require cooperation across multiple software projects, so by definition _outside_ of Bitcoin Core. For internal workflow we have GitHub issues and pull requests.
@_date: 2017-09-14 19:20:28
@_date: 2018-02-22 14:43:12
Inevitably, any proof technique (all of them, including quantum secure ones) will have at least one of the following:
* Perfect blinding but only computational soundness: someone with a sufficiently powerful computer, or a cryptographic break, can create fake proofs)
* Unconditional soundness but only computational blinding: someone with a sufficient powerful computer, or a cryptographic break, can deanonimyze the secret data.
In the context of CT this means that you have a choice between proof systems that in the case of a break either allow someone to invisibly inflate the currency going forward, or allow someone to break the privacy of all historical transactions.
The old rangeproof constructions we had in the original CT formulation could do either (there is a version with Pedersen commitments whose blinding is perfect, and a version with ElGamal commitments whose soundness is perfect). Bulletproofs can only be perfectly blinding.
Which of the two is preferable is unclear to me, and there are good reasons for both.
If you expect that someone in secret may have an EC break or computer capable of solving ECDLP, you probably want perfect binding. This means that this one party can break people's privacy, but at least for the time being the currency itself remains useful.
If you expect that such a break will be something that people see coming from a long time ahead (for example through incremental improvements to QC), perhaps perfect blinding is preferable. Yes, the system will be broken once that happens, but this is inevitable. It still will not result in breaking the privacy of everyone's past.
@_date: 2018-02-08 22:43:36
Relativistically speaking, distance and time are the same thing (with the speed of light being the conversion factor).
@_date: 2018-03-20 03:05:01
If you only count P2PKH addresses ("1..."), there indeed exist 2^160 = 1461501637330902918203684832716283019655932542976 = **1.46e+48** addresses.
If you also count P2SH addresses ("3..."), there exists another set of 2^160, bringing the total to 2923003274661805836407369665432566039311865085952 = **2.92e+48**.
If you also count version 0 native segwit addresses ("bc1q..."), there exist another 2^160 (P2WPKH) + 2^256 (P2WSH). This brings the total to 115792089237316195423570985013072412765262693420251618537606433066880927268864 = **1.16e+77**.
If you also count all future segwit versions (with all valid sizes, "bc1..."), another 16 (witness versions) * (256^2 + 256^3 + ... + 256^40) = 2^20 * (256^39 - 1) / 255. This brings the total to 34309815290713912539292297612650984671551718920592875337091076127301867292253307189514146449195008 = **3.43e+97**.
If you count all possible scriptPubKeys (literally any byte sequence up to 10000 bytes) that addresses could be devised for, the total instead is 256^ + 256^1 + 256^2 + ... + 256^10000 = (256^10001 - 1) / 255 = **2.52e+24082** (unfortunately Reddit doesn't let me paste the whole thing in decimal...).
@_date: 2018-02-04 09:24:15
If you need accountability for the 3-of-5 case (where accountability means that the signers can learn which of them participating in a spend), you indeed need something like key trees.
However, if you don't care about accountability, and have the ability to run a key agreement protocol ahead of time to determine what the address and keys for the 3-of-5 outputs will be, you can be much more efficient. In fact, any reasonably sized set of permitted combinations of signers can be made to look like a 1-of-1 on chain.
@_date: 2017-09-14 17:53:38
That's not voting, at least not in the traditional meaning. There is no measurable majority that can decide what the outcome for everyone is.
Running a full node however is certainly deciding where your money. Moreover, it's deciding what money *is* for you in the first place.
By running a full node, especially when you base econonic acitivity on its validation (e.g. by not accepting payments that your full node doesn't accept), you're forcing anyone who wants to bypass the rules you chose (whether it is for fraud, theft, or changing the rules of the system), to have your permission first. That's what gives you the ability to not need trust in anyone - because you validate everything for yourself.
I assume that's what you mean - but using the term 'voting' is confusing, as usually refers to system where a majority can decide for others. Here, it's really just everyone deciding for themselves.
@_date: 2017-09-04 19:18:20


That's also nonsensical, as old clients can't give out a SW address to receive on in the first place.
@_date: 2017-09-15 06:40:34
@_date: 2017-09-04 18:02:24
A RPi Zero only has 512 MiB of RAM. That may be insufficient to run Bitcoin Core.
In any case you'll need to tweak the configuration to reduce memory usage. [Here]( are some suggestions, but they may be outdated.
The upcoming Bitcoin Core 0.15 release will have more efficient memory usage, so things may improve soon.
@_date: 2018-02-26 12:10:30
No. 
@_date: 2017-09-03 16:07:47
All correct.
@_date: 2017-09-07 17:20:04
Change addresses are marked differently in order to avoid showing receives on them as incoming transactions.
It doesn't matter for the balance calculation, but it does matter for anything that tries to list incoming transactions.
@_date: 2017-09-14 18:30:17
Fair point. I should have said "chosen" rather than "agreed to".
@_date: 2018-02-22 14:34:00
"Intended" depends on who you ask. As is, Bulletproofs are just a generic zero-knowledge proof technique that may be used for a number of applications, Confidential Transactions (CT) only being one of them.
If you're asking how CT may make its way into Bitcoin, that's independent from Bulletproofs (except that Bulletproofs may make it an easier bullet to bite in terms of chain size and validation cost). 
The only realistic way I see (but this is very early still) is through a form of an extension block: an independent part of the block that contains CT transactions, and you explicitly need to move coins from and to the original block area (this is different from a sidechain in that all upgraded full nodes validate both sides, rather than have mainchain nodes blindly trust the sidechain's miners).
@_date: 2017-10-11 16:58:57
0.15.1 is expected to be a somewhat larger change than point releases usually are. In particular, the plan is to have full SegWit wallet support (SegWit P2SH receive addresses by default, and optionally Bech32, SegWit change).
I say plan, because if a major bugfix is necessary, that may become 0.15.1 as well.
SegWit wallet support is being added in a minor release because the timing (of SegWit activation) came just a bit too late for it to make it in 0.15. Waiting for 0.16 would unnecessarily delay access to this feature.
EDIT: SegWit wallet support was pushed back to 0.15.2, and we're releasing 0.15.1 first with some P2P robustness improvements.
@_date: 2017-09-30 01:56:18
Yes, that's part of the functionality that was merged today.
@_date: 2017-10-29 10:48:18
For starters, because it isn't a separate chain.
Payment channel transactions are just normal Bitcoin transactions, except they're not immediately broadcasted to the network - allowing them to be updated many times to reflect more and more payments before the whole world sees them. The difficulty (solved by LN) comes from preventing counterparties from broadcasting old versions.
Sidechains are just a separate blockchain, with a separate "currency" - though with systems in place to guarantee a 1:1 exchange rate between the two.
In my personal opinion, payment channels are a far more exciting evolution. They don't just move the transactions to another chain (where people still have to broadcast and mine and validate it) - they keep them mostly private to just the involved parties.
@_date: 2018-02-04 00:50:03
The error is in the database, not in a block.
The database is the result of processing all blocks.
If the database is corrupt, you need to rebuild the database. You can't download the database from somewhere without trusting who gives it to you. As a result, you need to process all blocks again to build a new database.
@_date: 2018-02-26 12:11:22
That's a bit of a misrepresentation: 
@_date: 2018-02-05 22:15:53
Unfortunately there isn't an established standard for "here is a private key whose corresponding address is supposed to be P2WPKH", so existing software may act in a variety of ways.
Bitcoin Core will import all 3 addresses when you import a private key (P2PKH, P2WPKH, P2SH-P2WPKH).
@_date: 2017-09-11 22:50:04
From the [preliminary release notes](


However, have you looked at the `dbcache` setting (or database cache size setting in Bitcoin-Qt)? You basically want to set that as high as you can (if you have a 4 gigabytes of available RAM, dbcache=4000 will significantly speed up validation).
However, binaries for 0.15 are not released yet. You can compile them yourself, or wait a few days.
@_date: 2018-02-04 09:26:33
About 13.5 picoseconds (when measured in vacuum, anyway).
@_date: 2017-09-30 07:24:35
It depends on what kind of errors you're talking about.
When you expect at most 4 errors per address, Bech32 has a 100% chance of detecting it, while Base58Check always has a very small (0.000000023%) chance of accepting it.
@_date: 2018-02-27 14:08:05
Because bech32 is new, and not supported by old wallet software.
If you expect people with older software to send you money, you must give them an address their software understands.
@_date: 2017-09-14 18:30:17
Fair point. I should have said "chosen" rather than "agreed to".
@_date: 2017-09-11 23:38:57
The best you can do is participate in economic activity, and make it clear it is dependent on a full node whose rules are chosen by you. If you refuse to accept transactions that violate your node's rules, people will need to convince you to change software before changing the rules.
@_date: 2017-09-12 03:44:11
"I even tested it! It validates the entire historical chain without problems!"
@_date: 2017-09-01 00:14:35
In case you're talking about the chainstate (the UTXO set):
The chainstate database always corresponds to the UTXO set at the time of our 'active' block. When a new block is received, its effects are applied to the UTXO set, and we move forward. When a reorg happens, the blocks along the old branch are "disconnected" (meaning their effects on the UTXO set are reverted) in reverse order, and then the blocks along the new branch are connected.
If you're talking about the blocks themselves:
Bitcoin Core stores all blocks that satisfy proof of work and have valid headers, regardless of their validity. We certainly don't delete blocks because they're reorganized out, as another reorganization may activate them again. When pruning is enabled, old blocks will be deleted, but again, that happens regardless of validity or active status.
@_date: 2017-09-12 19:22:08
BIPs are just a means of publishing ideas and obtaining feedback, and plenty of BIPs are terrible ideas (in my opinion).
Bitcoin Core as a project gets to choose which BIPs it implements, and other projects can choose theirs - or choose to ignore them entirely.
Are you concerned about the role of the BIP editor (whose job is just making sure discussion has happened about the idea, not value its merits)?
@_date: 2017-10-29 10:28:33
Lightning is very exciting technology, but it has nothing to do with sidechains.
@_date: 2017-10-26 08:52:08
Even with a fixed peg it's not a scaling solution, see my comment.
@_date: 2017-09-03 21:04:01
This is the function that computes the bech32 checksum, given the 5-bit character values:
    def bech32_polymod(values):
        generator = [0x3b6a57b2, 0x26508e6d, 0x1ea119fa, 0x3d4233dd, 0x2a1462b3]
        chk = 1
        for value in values:
            top = chk &gt;&gt; 25
            chk = (chk &amp; 0x1ffffff) &lt;&lt; 5 ^ value
            for i in range(5):
                chk ^= generator[i] if ((top &gt;&gt; i) &amp; 1) else 0
    return chk
Sure, implementing full error correction is significantly harder (see the javascript implementation [here]( but just computing and checking the checksum is trivial.
@_date: 2017-10-31 09:41:30
It's mostly being consistently up for a long time.
IP addresses propagate in the network, and nodes regularly retry connections. If you're up for a long time, more nodes will learn about you as a candidate peer.
@_date: 2017-09-14 18:14:58
The 6MiB file is just the source code, it's platform independent, and doesn't include any binaries.
The ARM Linux files are also smaller, as they only include `bitcoind`, not `bitcoin-qt`.
@_date: 2017-10-29 10:26:57
SegWit changes how the "size" of transactions is defined.
Specifically, the digital signatures and keys present in transactions now only account for 1/4th of their actual size. Overall, this means that the size of transactions is now around half (depending on the type).
SegWit transactions are only marginally smaller, and only when native SegWit addresses are used (see BIP173, addresses starting with "bc"). As long backward-compatible addresses are used (those starting with "3"), they're actually a bit larger on the wire and the savings aren't as large).
@_date: 2017-10-03 22:38:35
I'm afraid you're indeed screwed.
The consensus rules do not permit spending from a P2SH output that requires a &gt;520 byte push.
@_date: 2017-09-11 22:30:25
I don't understand your question. Is it faster to download the entire chain compared to what? And how would waiting affect this?
@_date: 2017-09-14 18:14:58
The 6MiB file is just the source code, it's platform independent, and doesn't include any binaries.
The ARM Linux files are also smaller, as they only include `bitcoind`, not `bitcoin-qt`.
@_date: 2017-09-22 02:18:57
That's not a SegWit address...
@_date: 2018-03-10 15:22:25
Bitcoin Core 0.16 was released on February 26th.
@_date: 2018-03-27 18:17:42
The page actually says it isn't a BIP, but a sidechain. Still wrong...
@_date: 2017-10-29 15:49:17
Only the sender's side matters.
So when you give me a SegWit address, and I send to it, then you get the discount when you *spend* those coins you received on that address.
@_date: 2019-09-21 03:23:03
@_date: 2017-10-02 19:56:15
Okay, if you call that ASIC resistance, I would call that a bad property.
You want a system whose security does not depend on the economy growing too large to make ASIC design profitable. Because if that happens, it will require an even larger investment than a system with "easy" ASIC production, resulting in a likely even worse centralization of ASIC producers.
Plan for success.
@_date: 2017-10-02 15:47:24
There is no such thing as ASIC resistance.
Certainly some PoW algorithms are harder to construct specialized hardware for, but if the cryptocurreny's economy grows large enough, at some point enough money will be available to invest in it.
Specialized hardware will always be faster (or more efficient, or lower latency, ...) than general purpose hardware.
My view is that aiming for ASIC resistance is a battle that is lost before it has begun. A good PoW function is as simple as possible, so that there are no non-trivial optimizations (which could be kept secret to gain an advantage in efficiency over those who don't know about it).
@_date: 2017-09-13 04:42:55
Use a wallet?
@_date: 2017-10-13 22:23:00
That's an unfair representation. Nobody at the time was aware of the effect of locking limits in BDB on Bitcoin's consensus rules.
@_date: 2017-10-07 17:09:08
You're missing the distinction.
I think it is vitally important that people who actually transact have a way to cheaply participate directly. A large part of my own time goes into reducing the resources needed to do so.
I don't think it's all that important that random people just run a node and then don't look at it.
@_date: 2017-10-26 08:48:19
You're confusing scaling with capacity increases.
If all we want to do is get twice the capacity at twice the cost (and then again, and again...) there would be little reason to oppose a hardfork that just doubles the limits, apart from safe deployment practices. It's the obvious and best solution for that problem.
The same concerns apply to just offloading some of the traffic to another chain. Now you have extra capacity, but have two systems that need bandwidth, validation, database growth, and associated centralization pressure. Plus they come with their own issues like volatility.
Merged-mined/voted sidechains are not a scaling solution either. I really wish people would stop talking about them as such. They can provide extra capacity, just like using a separate currency entirely, but not only do they incur the proportional validation costs like bigger blocks - they also incur a massive security reduction by removing the ability to validate transfers (miners can steal coins from the sidechain, and full nodes can't prevent them). The idea of sidechains is useful because of _flexibility_. They show that new technology does not require a wholly new currency before it can even be _experimented_ with, which was a major cause of distraction a few years ago with dozens of altcoins appearing. I don't expect public sidechains to be a very useful, until perhaps research on SNARKs makes fully-validatable sidechains feasible.
The bottom line is: the idea of using blockchains for capacity does not scale, period. Not using a bigger chain, not using a separate chain, not using a sidechain.
What does scale is approaches that reduce how much validation the public needs to do for security. The most radical idea there is payment channels, as they replace the normal counterparty risk that exists with a third party with a simple temporary holdup risk - while at the same time allowing many payments to be combined into a single (or a few) transactions the public at large needs to see.
Other improvements, which are more incremental individually are things like MAST (which remove the need for complex scripts to be published, in most cases), signature aggregation (just one signature per transaction regardless of complexity). A related domain of improvements are better privacy technology, as they may reduce the need for using multiple on-chain transactions that are currently needed to mix to a certain privacy level to just a single transaction.
In short, scaling is a hard problem, and there are many promising avenues of research. But moving the problem elsewhere is not a solution.
@_date: 2017-09-28 18:53:53
Weight is defined as 3 * size_as_seen_by_old_nodes + size_including_witness.
That equals 4 * size_as_seen_by_old_nodes + (size_including_witness + size_as_seen_by_old_nodes). The expression between brachets is never negative.
So now we have 4 * size_as_seen_by_old_nodes + non_negative_value &lt;= 4000000 as rule for SegWit nodes.
This is the same as size_as_seen_by_old_nodes &lt;= 1000000 - (non_negative_value / 4).
Or just size_as_seen_by_old_nodes &lt;= 1000000.
Thus, the weight limit rule being valid for a particular block to SegWit nodes _implies_ the original size limit will be met to old nodes as well.
@_date: 2017-09-07 17:18:05
Bitcoin Core 0.15 isn't released yet. It's expected in 1-2 weeks.
The preliminary release notes are here: 
@_date: 2017-09-19 22:58:32
Full wallet support for SegWit is planned for 0.15.1 (while 0.15.0.1 is just a single bugfix on top of 0.15.0).
@_date: 2018-02-03 23:40:30
Usually it's not the signature that is corrupted, but the key it is being validated against... which is in the UTXO database (in an entry created by a previous block's output).
As the UTXO database is the result of processing all older blocks, if there is a corruption in it, really the only way to rebuild the database.
@_date: 2017-09-01 00:27:03
Sure, as long as the old blocks are still there (not pruned), you can disconnect the blocks along the new branch of the reorg, and connect the blocks along the old branch.
The `invalidateblock` RPC does that for you. It allows you to forcefully mark a particular block as invalid. The node will reorganize to the next best chain it knows about, which may be the old branch.
@_date: 2018-07-22 19:31:51
BIP are just proposals. They are a mechanism for publishing ideas, giving them a canonical name to refer to unambiguously, and streamline discussion. There is a minimum of structure imposed by the BIP editor, who makes sure ideas have had sufficient public discussion. The Status a BIP has is proposed by its author, again subject to the editor verifying whether it corresponds to reality. This status \*describes\* reality; it does not \*prescribe\* it.
BIPs are not necessarily good ideas; they are simply ideas that went through the process of getting published. Whether particular pieces of software adopt BIPs is up to them. There is no vote, and most BIPs don't require consensus. In particular, BIP174 can be adopted by particular wallet software, or not. Those that do will be able to use the standard to interact, and others won't. There is no requirement at all that the enitre world agrees on whether BIP174 is a good idea.
It is slightly more complicated for BIPs that propose actual changes to the validity rules of Bitcoin blocks. Due to Bitcoin's nature as a consensus system, a large degree of agreement in the community must exist before the network can safely adopt it. Such BIPs tend to include sections about how to safely trigger the activation in lockstep. Again, however, BIPs are just a means of publishing ideas, and whether such a rule change gets implemented by particular software is up to them.
BIPs don't tell you what Bitcoin is; they tell you what it could be. What Bitcoin is is determined by what gets implemented in software and whether people run that software.
@_date: 2017-10-20 04:34:33
A Bitcoin node is a piece of software that interacts with other Bitcoin nodes to form the Bitcoin P2P network. In other words, it is what the network is made of.
Nodes relay to each other new blocks and transactions, but also IP addresses of other nodes.
Some nodes fully validate every block and transaction before accepting and/or relaying to others. These are called full nodes. Full nodes maintain a database of all unspent coins (effectively the "final state" of the ledger, or which address controls how many BTC) to verify new transactions against. Having your wallet be connected to a full node you control is the only way to not be dependent on any third party for processing your transactions.
Some full nodes maintain the entire historical blockchain on disk (and allow downloading those blocks by new nodes that are trying to verify history). Other full nodes are pruned, which means they still download and verify all blocks, but don't remember them after validation. They only remember their combined ledger state (the unspent coins database mentioned above).
Some nodes only make outgoing connections, and some will also accept incoming connections from others. This is related to the advice of opening up port 8333, as that will permit other nodes to make connections to you. As for every outgoing connection somewhere there must be an incoming open connection slot elsewhere, it is necessary for a healthy network to have a sufficient number of nodes that accept incoming connections. These days it seems there is no shortage of those, though.
@_date: 2017-09-03 16:24:26
SegWit transactions count as (size without witness) + (size of witness)/4 for the purpose of computing their contribution towards the block size.
Because of rational mining strategies (assuming miners want to maximize profit), that also translates in fees that are proportional to that modified size function.
This does not necessarily mean a 1.75x fee decrease. It depends on what your transactions look like. If your transactions have a relatively large witness (which is true for multisig transactions, for example), the discount can be larger.
@_date: 2019-09-01 18:11:56
You can run \`ots info file.ots\` to see the verification steps.
@_date: 2017-10-12 06:24:05
You start it with `-prune=1000` on the command line, or put `prune=1000` in the `bitcoin.conf` file. There is probably also a way to enable it in the GUI, but I'm not familiar with that.
The 1000 above is an example to make Bitcoin Core keep 1000 MB worth of blocks around. It can be any number above 550.
@_date: 2017-10-26 10:58:39
That assumes it's even possible to usefully partition an economy into subdomains that don't interact with eachother.
And even if that is possible, you're still handing miners to power to steal those coins. Perhaps the impact is reduced if it's just a small subset of users (which hos we envisioned it for experimenting), but i still think it's an unacceptable property for a production system, with increasing systemic risk if it ends up being adopted.
@_date: 2017-10-10 18:28:02
This is wrong. The signatures are always needed when a node first processes a block, whether that is now or later.
Some nodes may choose to not perform validation, which means they don't need witnesses (lightweight nodes).
This means that there is now a theoretical third pruning mode in between full and pruned, where witnesses are pruned but not the rest. Such a node would be able to serve lightweight peers, but not other full nodes. To the best of my knowledge, no full node software has implemented this partial pruning. Pruning is independent of validation though, and doesn't change the security model.
Signatures are part of SegWit blocks. They're just in a part that's discounted, and can be ignored by non-SegWit nodes or lightweight nodes.      
@_date: 2019-09-23 17:59:54


This sounds like you think BIP70 is being removed from the Bitcoin Core Qt GUI because people dislike BitPay? That couldn't be further from the truth; we've been discussing deprecating and removing BIP70 support for years. When BitPay announced their push for BIP70, those efforts were delayed.
BIP70 is being removed from our wallet implementation because it just doesn't meet the cost/benefit bar anymore. It's hardly used, and even in places where it does, there are serious deviations from the spec (especially BitPay, who use a modified version). It never accomplished its goal of providing a payment infrastructure for Bitcoin, for a multitude of reasons. It was broken from the start by not guaranteeing that messages and acknowledgements were exchanges before the transaction reached the network. It tried to simultaneously solve an identity problem by integrating a PKI, a notoriously hard problem, making implementations even harder. At this point we feel it's more a maintenance burden for an undertested feature than a benefit.
I was the one who originally started advocating for a payment protocol for Bitcoin. I gave a talk about the need for one at a Bitcoin conference in 2012, and worked on a [draft for one]( which Gavin later used as a base for writing BIP70. But at this point, I have to admit those efforts have failed. Textual addresses and BIP21 URIs are how the world thinks about requesting Bitcoin payments, and I don't believe that will still change.


That would be increasing maintenance and QA burden, not lessen it.


You're welcome to maintain a version, or find someone who does, which retains this feature. That includes the ability to provide binaries for others. For example Bitcoin Knots is a maintained fork of the software, with a different set of features.


No offence, but we don't, and go to extraordinary lengths to make sure this is not the case. Bitcoin Core intentionally has no auto-update feature, to prevent having a small group of developers able to push changes to the network. The binaries are compiled using a reproducible deterministic build process, allowing anyone to verify that the published binaries match the claimed source code they are built from.
You're free to use any software you like, including older versions of Bitcoin-Qt, or other wallet software. I understand that the removal of any feature may make people reconsider what software they use going forward, but in this case I sincerely doubt it will impact many users.


I couldn't agree more.
@_date: 2017-09-10 04:45:15
This is correct.
@_date: 2017-10-31 09:26:56
If you have somewhat recent hardware, you in fact need neither 150GB of disk space or 28 hours of time.
To reduce disk usage, run in pruning mode. This means your node can't serve historical blocks that are trying to synchronize, but you'll still relay new blocks and have a fully-validating wallet (=the real reason why you'd want a node in the first place).
To reduce synchronization time, configure the size of your database cache. By default it's only 450 MB (to accomodate running on low-powered hardware), but most systems have far more memory available. Raising it to a few GB significantly improves throughput of validation.
@_date: 2017-10-31 09:45:15
"Simplicity" is the name of the language.
@_date: 2017-10-01 17:55:34
Using (not just running) any full node that implements Bitcoin's consensus rules, is a voice against a change of consensus rules. That is what Bitcoin is: your ability to verify independently.
All 0.15 does is prepare for a topology change that would be drastic and sudden at the time S2X, and spreads it out over time while people adopt 0.15.
@_date: 2017-10-14 22:18:50
Bitcoin Core is always a fully validating node, regardless of pruning or whether it's reachable.
@_date: 2018-07-30 05:58:59
The PPA is still on 0.16.0. If you want 0.16.2 you'll need to install it manually.
@_date: 2017-10-27 09:19:49
Correct, but you're not really interested in how many need to be generated until a collision occurs. It only matters if you generate an address that has any coins associated with it. Given that there are only a few million of those, the number is much larger yet.
@_date: 2017-10-03 23:05:17


I think that is very unlikely.


It's known that the limit is small, and this was a design flaw in BIP16 that was not noticed until several years later.


Yes, I agree. It seems the service you used here should implement a sanity check (most wallet software that supports multisig includes such sanity checks).
@_date: 2017-10-07 08:32:46
Only if you actually use it for something.
Your node doesn't "help" the network, apart from relaying blocks to other nodes (but generally, weak hardware nodes on home connections hardly benefit anyone). Every node individually redoes all the work that every other node is doing already. Why does it do that? So that you don't have to trust another node.
@_date: 2019-09-04 03:59:59
The networks split naturally, because they consider each other's blocks invalid. When a peer sends you invalid data, you disconnect them.
At the time of the fork itself there is a risk that this does not happen fast enough, which may in theory lead to nodes that are partitioned off as an "island" of forkA nodes surrounded by all forkB nodes, which don't relay them their blocks and transactions.
@_date: 2017-10-14 00:50:59
Every node validates everything.
One is enough, as long as it's your own.
@_date: 2017-09-23 17:47:53


Your number looks like 2^256 / 1.28 billion. That's wrong in three ways:
* At least for normal pay-to-pubkey hash style outputs (1xxx... addresses), you don't need the exact private key that the owner has. You only need a private key whose corresponding RIPEMD160(SHA256(pubkey)) matches the funds' address. Since RIPE160 only has a 160-bit output, you only need to try a bit over 2^160 public keys to be able to take them all.
* There are far fewer than 1.3 billion addresses with remaining coins. There are only 51 million unspent outputs (currently), so I don't expect more than a few 10s of millions of addresses.
* For addresses where a public key is known (because coins assigned to them have already been spent), a more efficient algorithm than brute force is possible (it's still O(2^128), though).


3 per second? Vanity address generators can compute millions of private/address keypairs per second per CPU. The same logic could be used to bruteforce things.
None of that doesn't change the final outcome though. An analogy I like is saying that if all Bitcoin mining were converted to brute forcing addresses at the same speed (which would require very different ASICs), it would take 30000 times the age of the universe to have a chance of bruteforcing any Bitcoin key.
 
@_date: 2018-07-25 18:42:09
You're mostly correct.
The name of that state that results from validating the chain is called the UTXO set. It isn't actually a account-balance sheet, as Bitcoin doesn't use an account model. Instead, we have UTXOs (unspent transaction outputs), which you could see as "coins". Every coin is owned by an address (or script, but ignore that), and has a certain amount. Transactions basically "reforge" coins. Every transactions melts down one or more coins completely (by signing to prove they are authorized to do so, destroying the coins in the process), and creates one or more new coins (assigned to new addresses, with a value equal to the sum of the values of the coins destroyed). When you have a 1 mBTC coin at address A and want to send 0.4 mBTC to address B, you will create a new address A2 yourself, and then create a transaction that destroys the 1 mBTC coin, and creates 2 new coins - one of 0.4 mBTC assigned to B and one of 0.6 mBTC assigned to A2. The network can't tell that A and A2 are owned by the same person, hiding the actual amount transferred.
The issue you point out is real as well. The entire UTXO set on disk is currently around 2.7 GB. Bitcoin Core heavily caches parts of it in RAM to speed up validation, and generally needs at least a few hundred MB for reasonable validation speeds.
However, basically no incentives exist to limit the growth of this set. That is potentially an issue in the long term, as it could grow beyond what commonly accessible hardware can keep in fast memory. Repercussions of this could be a slowdown in propagation of blocks across the network, for example, or worse, raise the costs to audit the system.
There is some ongoing research to instead of having every full node maintain this set itself, only have full nodes maintain a subset of the information (potentially just a single hash). In order to validate things in such a model, transactions need to include proofs that the inputs they're spending actually exist. Search keywords for this include Peter Todd's TXO MMR model or Bram Cohen's bitset model.
Also, [ is probably a better place for questions like this.
@_date: 2017-10-07 08:31:10
No, it is validating everything - meticulously downloading every block and asserting that no theft or unauthorized inflation happened.
But if nobody looks at what the node is doing, who cares? It could go offline and nobody would notice, apart from a small number in a statistics site changing - a number that is easily manipulatable in any case.
@_date: 2019-09-20 16:19:41
Create a bitcoin.conf in *the same folder* as the one where you see mempool.dat. Not *inside* mempool.dat.
@_date: 2018-07-29 03:31:20
yuck. lasagne.
@_date: 2019-09-08 07:37:38
I'm pretty sure the Internet is a more powerful computer network.
@_date: 2019-09-03 19:11:20
A friend sent me this picture.
Google translate claims the top line means "Brine physiological saline".
I'm confused.
@_date: 2019-09-16 15:18:49
* The bech32 prefix for testnet is "tb1"; I don't know what "tc1" addresses are.
* I don't know what NP2WKH is. Do you mean P2WPKH or P2WSH?
Note that I don't know anything about LND.
@_date: 2018-07-28 05:07:02
No, that is for the upcoming major 0.17 release, which is planned for september (see the [release schedule](
This is just a small bugfix release in the 0.16 series.
@_date: 2018-07-30 03:54:06
Yes, there is?
@_date: 2019-09-05 21:40:36
No reason why you'd be restricted to specifying hashes per second.
6 ZH per minute.
@_date: 2019-09-08 01:58:14
They reinforced the notion of thinking of Bitcoin addresses as some sort of permanent identifier of an account.
That thinking probably seriously reduced privacy by encouraging people to design services that had just one address per user.
It probably also caused serious losses early on, by people thinking a private key backup was enough to secure their funds, only to discover that their wallet software moved change to different addresses.
I think the situation got better after new generations of wallets came with BIP32 and other derivation schemes by default, but I still believe the introduction of block explorers set us back several years in that regard.
@_date: 2018-07-22 19:21:11
It taps into the Merkle root.
@_date: 2017-09-06 15:11:30
`addwitnessaddress` adds a new wallet address with label "", using the key or script specified by the passed address. It's the minimal functionality needed to test the SegWit receive/sign logic in the wallet, but it indeed doesn't support 'change' status or labels/accounts. It also doesn't support HD recovery.
Full SegWit wallet support is planned for 0.15.1 (SegWit activated too late to make it for 0.15.0 whose feature freeze had already passed).
@_date: 2019-09-05 07:47:24
You can change the port, but in practice you will not get many legitimate incoming connections on it.
Bitcoin Core strongly prefers connecting to other nodes that run on port 8333.
@_date: 2017-10-11 23:47:50
Pruning does not change your bandwidth requirements, only storage.
@_date: 2018-07-28 02:02:33
You're free to write a Bitcoin client yourself in Rust.
I don't mean this as a criticism on Rust; from what I hear it's a very good language, and I plan to learn it myself.
But Bitcoin Core is a C++ project, written and maintained by an ecosystem of developers who are familiar with the language, and are sufficiently competent in it to write production-ready code and review new contributions with a high standard.
It's a bit absurd to suggest that they all move a to new language, one which many will not be equally proficient with.
@_date: 2019-09-08 16:48:56
By "shape of witness", you mean something like an or that branches based on the output of OP_DEPTH (the number of stack elements)? That is indeed something that's possible (at the top level), but I doubt it's useful in anything but exceptional cases, actually. It needs a lot of opcodes to do so, and opcodes always increase the spending cost (by growing the script), while increases to input stack only apply on the condition the respective branch is taken.
I believe that what BOLT3 received/offered HTLCs do includes two tricks we don't support in Miniscript:
* Take as input a stack element, compare its hash with a constant pkh, and if it succeeds go into a branch that uses it as a pubkey. If it fails, treat the same stack element as a hash preimage.
* Have a branch that assumes a pubkey is already on the stack, and one side uses it for CHECKSIG directly, the other side uses it as part of a CHECKMULTISIG.
I think we found a way to add the first construction to Miniscript (generically; but I don't think there is any simplicity to gain from restricting it to the top level), but it significantly increased the complexity of the language.
The second is even harder to model in the sort of structure we have. That's a limitation, but given that IIRC at the time it was at best a 1 WU gain for the BOLT scripts, and we've since improved the language in other ways, I wouldn't be surprised if it wasn't a gain at all anymore.
Adding those BOLT3 scripts as special templates in the descriptor language and signing code in Bitcoin Core wouldn't be too hard.
@_date: 2017-10-05 06:14:51
There are two independent problems here:
* It is impossible to use redeemscripts larger than 520 bytes. This was fixed using SegWit, by introducing P2WSH style outputs that can have redeemscripts up to 10000 bytes.
* The fact that if you accidentally sent money to a P2SH output that is the hash of a redeemscript over 520 bytes. Solving this requires a hardfork that only benefits people who used broken software.
@_date: 2018-07-23 16:09:43
The major reason why you'd want to have a transaction that spends multiple owner's coins, is CoinJoin. It's a privacy enhanckng technique specifically designed to break the "all inputs of a transaction on chain are owned by the sams person" assumption in many analyses. Look for JoinMarket for a practical implementation.
The expected workflow with PSBT and hardware/offline wallets uses an online watch-only wallet which derives addresses, and stays in sync with the chain, learning about incoming payments to those addresses. To spend, a PSBT is created on the online wallet, with inputs from the payments received. This wallet also adds the key and script information (how the addresses were derived and can be spent, but excluding the private keys) to the PSBT file.
That PSBT file is then given to the offline signer. All information necessary to analyse the transaction is present in the file. The device could show the fee, the amounts, the payments made, and even learn what parts are change. The device shows this information to the user, requests confirmation, and then signs. It puts the signature back in the PSBT file.
Any PSBT software connected to the network can now verify the PSBT is complete, extract the fully signed transaction from it, and broadcast it to the network.
@_date: 2018-07-06 18:59:54
I think they're awesome.
However, they're also slower to verify, and introduce additional security assumptions. That roughly means there are additional ways in which they could be broken, which don't apply to Schnorr or ECDSA. As a result, I expect it to be much easier to find widespread agreement to switch to Schnorr (which has no additional assumptions, and is slightly faster than ECDSA).
First things first.
@_date: 2018-07-06 21:59:34
So in what is called a multisignature scheme (not what is called "multisig" in Bitcoin), a number of parties together produce a \*single\* signature that proves they all authorize the message. There are no 5 separate signatures - just one. You can't swap anything out.
@_date: 2017-10-14 03:49:46
Not just the patch.
It was *every Bitcoin version before* that was never exposed to a kind of load that triggered the BDB limits.
The LevelDB upgrade _removed_ the limit, and thereby inadvertently introduced a consensus failure.
@_date: 2017-09-01 00:27:03
Sure, as long as the old blocks are still there (not pruned), you can disconnect the blocks along the new branch of the reorg, and connect the blocks along the old branch.
The `invalidateblock` RPC does that for you. It allows you to forcefully mark a particular block as invalid. The node will reorganize to the next best chain it knows about, which may be the old branch.
@_date: 2017-09-02 19:18:52
My theory is that they count the number of transactions with a witness, and subtract 1 (because the coinbase technically has a witness in SegWit blocks, but it's probably not what you want to count).
483162 is not a SegWit block (which is valid, if it includes no SegWit transactions), so even the coinbase has no witness. If the count of witnesses is 0, and they subtract 1, you get -1.
@_date: 2017-09-01 00:14:35
In case you're talking about the chainstate (the UTXO set):
The chainstate database always corresponds to the UTXO set at the time of our 'active' block. When a new block is received, its effects are applied to the UTXO set, and we move forward. When a reorg happens, the blocks along the old branch are "disconnected" (meaning their effects on the UTXO set are reverted) in reverse order, and then the blocks along the new branch are connected.
If you're talking about the blocks themselves:
Bitcoin Core stores all blocks that satisfy proof of work and have valid headers, regardless of their validity. We certainly don't delete blocks because they're reorganized out, as another reorganization may activate them again. When pruning is enabled, old blocks will be deleted, but again, that happens regardless of validity or active status.
@_date: 2017-10-12 01:31:34
If you run in pruned mode, you're still downloading and verifying all of history. Old blocks are just deleted as you move along.
@_date: 2018-07-31 08:03:48
You can choose to run it or not. You can also choose to run different software.
@_date: 2017-10-31 09:41:59
Try increasing your database cache size.
@_date: 2018-07-30 17:02:20
You can put it anywhere you want.
There is no installation procedure or expected path.
@_date: 2018-07-04 01:04:47
I agree with you here, and I've asked people internally not to represent the security of issued assets on Liquid as related to Bitcoin.
@_date: 2019-09-06 18:21:30
You don't need to know or care about the change address. Your wallet manages your address pool. It will present you with the aggregate value of all coins all your addresses (including internal change ones) have access to. When trying to spend, it will pick from the set of all of those as inputs, and if necessary, create more change.
You can look it up of course (how will depend on the wallet software), but there is no point apart from debugging.
@_date: 2019-09-01 18:09:25
Creating an address from the root and sending to it is _one way_ of committing the data to the blockchain. I wasn't specific on how to construct such a commitment.
In practice I believe that's not what is used, because it's wasteful; you need a whole transaction, plus burning some coins, plus adding bloat to the UTXO set (which is not a cost to the committer, but to all full nodes).
Another approach is using an OP_RETURN output in an existing transaction, which can just contain a hash directly. This avoids the UTXO bloat, as OP_RETURN outputs are provably (and recognizably) unspendable, so full nodes just forget about them once created.
An even more efficient way (not sure if it is in use) is using sign-to-contract. This is a cryptographic trick that allows one to create an ECDSA signature (or later, Schnorr) that commits to some external data without affecting the validity of the signature. So it's a totally normal signature that anyone could use at any point in a normal Bitcoin transaction they would send anyway, but together with some external data, it can be shown that it was actually "tweaked" (in a way that doesn't affect validity) with the data committed to.
@_date: 2019-09-05 23:10:23
\*The only kind of correct.
@_date: 2019-09-08 08:23:28
I'm not going to disagree that Bitcoin Core's UI/UX can be much better (not my area of expertise, though).
However, I don't think requiring access to a file is a bad thing. I would never in my life trust my savings to just a memorized phrase. Human memory is not nearly as good as we'd like to think it is.
@_date: 2017-10-14 00:40:49
That's not what backing means. You can't exchange BTC for a guaranteed amount of nodes or blockchains(?).
I'm sure you mean to say that its value and operation are secured by those, and that is correct. But it in no way changes the also correct claim that BTC is indeed not backed by anything.
Of course, dollars aren't backed by anything either.
@_date: 2019-09-09 22:06:20


I knew I had seen that phrasing somewhere before: [
@_date: 2019-09-02 23:33:31
Depending on how much RAM you have, increasing the database cache size may significantly improve the sync speed.
@_date: 2018-07-06 22:05:51
No need to jump ahead of things, the Schnorr BIP isn't even a soft fork - it's purely a description of a signature scheme. Talking about deployment really seems very premature.
@_date: 2018-02-26 16:54:28
The protocol version number is the same because no protocol changes are included.
Do you not upgrade to a new version of your web browser just because it doesn't make any changes to HTTP?
@_date: 2017-10-07 09:41:21
Right, that's true, but the benefit added by low-powered hardware on home internet connections does not add much today. There is currently no shortage in available nodes to download from or relay blocks (though that may always change of course - it was the case a number of years ago).
@_date: 2017-10-31 06:50:29
I have no frickin' clue.
@_date: 2019-09-08 20:47:47
That question depends on how you define account.
In traditional wallets, an account is simply a collection of addresses. They're not really generated "from" an account; you just generate an address, and remember to credit payments to that address to a particular account. The answer in this case is indeed "all of them".
If by account you mean something like one level of a BIP32 xpub derivation, the answer is 2^31 (about 2.1 billion).
@_date: 2019-09-20 19:30:54
What? I'm talking about Bitcoin Core, which you are clearly running.
Electrum does not use much disk space at all.
@_date: 2019-09-20 19:16:29
A text document.
@_date: 2018-07-23 18:18:51
Nodes are not required to follow a BIP.
BIPs are proposals. It is up to maintainers to include it in their software, and up to users to adopt such software or not.
@_date: 2018-07-04 00:58:18
No, they're not tied to the Bitcoin blockchain at all.
Liquid is a private blockchain, managed by a federation of its users. One of its assets is BTC, auditably pegged to BTC on the Bitcoin blockchain, which makes it a sidechain. The announcement here is about Liquid gaining the ability to also represent other, custom issued, assets. These new asset types are completely unrelated to BTC, apart from the fees being paid in BTC.
@_date: 2019-09-13 23:48:41
There is no guaranteed amount of transaction data corresponding to a given amount of BTC.
Bitcoin has no intrinsic value. That's ok. Its value is entirely set by the market choosing it as a currency.
@_date: 2019-09-06 18:04:19
I don't understand your question. Generally you don't care about change as a user. Your wallet software will make sure change addresses are generated and outputs to them are added to whatever transactions you create.
@_date: 2017-09-01 03:14:15
No, you won't need to.
@_date: 2017-09-14 21:29:46
No, that functionality is planned for 0.15.1.
@_date: 2018-02-27 16:34:24
@_date: 2019-09-01 06:06:11
Not even quite that much: 
@_date: 2019-09-01 16:40:48
Gah, that's embarrassing.
@_date: 2017-10-23 19:13:50
Thankfully, all transactions are still on the main Merkle tree.
In fact, even their witnesses are still on the main Merkle tree - only indirectly so through the coinbase.


Thankfully, that is no longer needed.
@_date: 2017-10-31 06:30:42
Technically, the coinbase scriptSig had to be at least 2 bytes before BIP34.
@_date: 2019-09-23 19:41:11


To the best of my knowledge, BitPay is the only payment processor that requires people to use BIP70 (but even then, it's a modified version that was never supported in Bitcoin-Qt in the first place). That's why I'm bringing them up specifically.


Developers choose which features they support in their own software. They also choose which features they stop supporting. BIP70 is supported by some wallets, and not others. For example, Wasabi never supported it. Also bitcoind (the daemon/command line version of Bitcoin Core) never supported BIP70.


Personally I strongly suggest you use BIP21 URIs for this. They also convey the amount, and are widely supported by nearly all software. As not all wallet software supports BIP70, you likely need to do this as a backup anyway.


That sounds like throwing out the baby with the bathwater. Why does a decision of one specific piece of wallet software cause you to drop bitcoin support entirely?
@_date: 2019-09-20 19:36:38
Perhaps, but you're clearly also running Bitcoin Core.
All the files in the directory you listed above are files Bitcoin Core creates, not Electrum. All the diskspace you're complaining about is due to Bitcoin Core.
Maybe you should just uninstall Bitcoin Core if you're not using it? Be sure not to delete wallet.dat though, if you have any funds in it.
@_date: 2019-09-09 16:37:02
Transactions don't have messages in Bitcoin.
@_date: 2015-09-09 15:18:09
Nit: and are always *at most* half the size.
@_date: 2019-09-16 22:59:02
The primary reason against address reuse is privacy, as it gratuitously reveals linkage between transaction outputs.
While it's true that in theory it is harder to find a private key for a public key's hash than for the full public keys, the difference is not relevant in practice. Typical usage reveals public keys everywhere (including e.g. by any kind of xpub/bip32 based lightweight wallets, spending fork coins, ...), and several million worth of BTC is already in outputs with known public keys.
@_date: 2017-10-31 09:36:23
The original reddit post is here: 
@_date: 2018-07-06 21:54:08
The BIP here is \*\*just\*\* a specification for a signature scheme. It says nothing about even a proposal for integrating it into Bitcoin, much less a deployment strategy.
@_date: 2018-07-06 22:26:28
Smaller transactions = more transactions in a block. They're the same thing.
@_date: 2018-02-14 04:26:51


In theory, yes. But:
* That would be an enormous waste of money for the miners who creates such a block, as almost all nodes on the network would reject the block - creating a hard fork between old and new nodes in the process.
* The owner of those segwit coins would reject such a block, as he by definition is using segwit-compatible software.


All of P2SH, CSV, and CLTV changed the validity of certain anyone-can-spend transactions.
@_date: 2018-07-01 00:54:26
The usage of uncompressed public keys is not legal in segwit spends.
Software shouldn't let you create segwit addresses with uncompressed keys. Generally it's best to first try things like this on testnet or with small amounts when trying new software.
@_date: 2019-09-06 17:19:49
It's important to note that the protocol does not care about the distinction between payment outputs and change outputs. It simply has outputs, and the sum of all input's values has to match the sum of all output values (plus fees).
It's up to the wallet to add outputs back to the originator if change is desired, but these cannot be distinguishes from other outputs at the protocol level (for privacy).
@_date: 2018-07-06 20:47:27
It's a complicated issue.
With the emergence of so many ideas for improvements to Bitcoin's script execution (MAST, Taproot, Graftroot, new sighash modes, multisignature schemes, ...) there is simply too much to do everything at once. Since aggregation really interacts with all other things, it seems like the better choice to pursue later.
@_date: 2015-09-10 02:33:41
The blog post was in fact mostly written when only the initial work around this was done, though we decided to delay publishing until a full implementation was available. I guess it shows in the summary.
The main point is that this offers a new means of doing multisignature destinations in cryptocurrencies like Bitcoin. They are useful for example for funds that are controlled by a group of people, or for funds secured by keys in multiple devices (desktop and phone, and maybe a third party), or for escrow systems. In Bitcoin, some of these are possible already (and used!), but the system used in Bitcoin has downsides (it's not flexible, and it is expensive to the network).
What Key Tree Signatures offer is mostly a generalization of what K-of-N multisig can do (it supports arbitrarily nested structures - what if you want to have 2-of-3 for shared funds, where one of the participants uses a multi device 2-of-2 wallet?), and it has very low cost to the network (in terms of size and validation time), so there is no cost to using security-enhancing features.
Furthermore, because the difference is cost between one policy and a slightly more complex policy is minimal, it is cheap to add "dummies" into the mix, to hide my real procedures. For example, if VapourSoft tomorrow launches a fancy new wallet that by default uses 6-of-9 multisig, they are likely the only ones who do so. Seeing a 6-of-9 transaction on the blockchain would be a strong indication that it's a transaction by VapourWallet.
Finally, how close is this to being usable in Bitcoin: it would require opcodes for Schnorr signature validation and Merkle tree branch validation. Thankfully, both already exist and are easily added using a softfork.
@_date: 2019-09-05 21:47:43
According to [ a Core 2 Duo E7300 can be overclocked to perform 7.76 MH/s. That's 670 GH/day. One difficulty is 4.3 GH. So such a CPU could do 156 difficulties per day. At the current difficulty of 10771996663680.4/block, this CPU can mine 0.000000000014 blocks per day. Around $ 0.0000018 worth per day.
@_date: 2018-07-07 01:57:42
They're the best kind of questions, as they make it clear what can be explained better.
@_date: 2018-07-30 17:08:02
Yes, but MuSig has non-interactive setup, allowing it to be used without needing new backups to store key shares in your wallet etc.
If you accept an interactive setup, many more schemes become possible, including thresholds based on secret sharing like you point out, but there are also techniques to implement arbitrary monotone functions over keys (say (A and (B or C)) or (C and (E or F)).
And MuSig is already provably as secure as Schnorr itself. No need to change it.
@_date: 2015-04-12 09:02:33
I'm sort of glad he was wrong :)
@_date: 2017-10-31 09:47:45
Haskell does not have easily computable bounds on CPU time and memory. Simplicity is far more restrictive, but does have these bounds.
You don't want a Haskell interpreter inside your consensus rules. Simplicity is incredibly simple to specify.
@_date: 2017-10-03 07:46:41
You get the ability to transact without needing to trust anything but your own hardware and software.
@_date: 2018-04-21 17:26:10
In a system that uses CT, if someone finds a break in the fundamental security of elliptic curve cryptography (ECC), he can print money. Worse, because the amounts are all private, he can do so *undetectably*. That's an existential threat for a currency which has controlled inflation as prime goal.
Such a break would be dangerous for Bitcoin (even without CT) regardless, as it relies on ECC for protecting against theft. However, Bitcoin can migrate to use a different signature scheme if necessary. It would be chaotic, but not necessarily pose an existential risk.
There are people who agree with this perspective. It's a mathematical fact that any privacy system that hides amounts and is built on top of ECC will at least have one of the properties below:
* An ECC break will permit printing money undetectaby.
* An ECC break will permit deanonymizing amounts in the historical chain.
So we are faced with a choice between those two regardless if we want better privacy. Some people believe the second is worse than the first, and choosing the first actually allows for more efficient technology. This is a choice that Zcash, Monero, Grin,  all make. CT with Bulletproofs would do the same.
However, regardless of your own opinion in this matter, Bitcoin's evolution is a large collaborative process, where such fundamental changes need buy-in from large parts of the ecosystem. My fear is that the question of giving up the ability to exactly audit the monetary supply is going to make it hard to make CT acceptable.
Furthermore, there are other challenges. CT makes transactions larger, slower to verify, and poses a very significant engineering challenge to incorporate into Bitcoin without breaking backward compatibility.
There is lower hanging fruit to improve, thankfully.
@_date: 2019-09-20 19:45:38
The executable file for Bitcoin Core is called bitcoind.exe or bitcoin-qt.exe (on Windows).
You don't need to touch Electrum at all. If you're not using Bitcoin Core, delete it along with its data directory (the folder in which you found mempool.dat and others).
@_date: 2018-04-22 19:49:42
Let's start over, because there are several conversations here in parallel.
You can't remove the signing "feature" without crippling Bitcoin - spending coins is signing.
Key reuse is a concern, but it's an existing one that is based on social conventions. I expect it to be fixed, but it will take time.
An ECC break right now would hurt Bitcoin. It's worsened by key reuse, but that's not all. It for example also affects applications that rely on multisig (as they reveal public keys to eachother). The point I want to make is that we shouldn't feel safe just because our public keys are temporarily hashed.
However, it is likely that if an ECC break happens, we will see it coming years ahead. There will likely be incremental improvements to attack algorithms or technology that reduce the security bound. As long as we only rely on ECC for authorizing spends, we can migrate away from it over time.
With ECC based CT, we inherently must accept at least one extra risk:
* An ECC break means the currency can undetectably be inflated (going forward).
* An ECC break means the ledger's history can retroactively be deanonymized (goin backward).
Both of these are unfortunate, but which is worse depends on how we expect ECC to be broken, in my view. If we expect an ECC break to happen with ample warning, it's perhaps better to accept the inflation risk, because we can again migrate away from it, and at least the historical privacy can be protected. If we expect an ECC break to happen unexpectedly, silent inflation is very scary as it's an existential threat to the currency.
Personally, I think both of these viewpoints are relevant. Unfortunately we can't protect against both (except with slower, larger, more novel non-ECC based methods). Perhaps the solution is to provide both options, but it does become harder to get ecosystem buy-in for, I'm afraid.
@_date: 2018-04-21 03:54:26
No,with confidential transactions the amounts are "encrypted" in a way that makes them private, bit still permits the public to verify that the amounts in the inputs add up the same value as the amounts in the outputs.
@_date: 2019-09-03 19:59:03
@_date: 2018-04-21 04:20:28
No, encrypting the amounts is what CT does, not Bulletproofs.
CT however requires proofs that there is no overflow in the amounts. One way of doing so is Bulletproofs.
And no, that's not going to happen in Bitcoin any time soon. It's very different from how things work now.
@_date: 2018-04-21 17:09:07
's answer is great, and I agree completely with it.
What I want to stress is that privacy is not a yes-or-no question which depends on the existence of a single technology. There are multiple facets to it, some depend on wallet practices (reuse addresses?), security practices of participants (transact over Tor?), some on network routing (Random delays on propagation hide where transactions come from), and some on the actual data that is stored on chain (where things like CT help).
All of these things and more are being researched, and will improve over time. 
@_date: 2018-04-21 17:43:12


I don't care much about (public) sidechains as more than a research project. They make trade-offs that I think are very hard to accept for production use (the miners can steal sidechain funds).


Yes, and Monero too. It's great to see projects experiment with technology to let it mature, but they also take risks that I don't think would be acceptable to Bitcoin (see elsewhere in this thread, the risk of silent inflation).


The pruning is a local choice an individual client makes, not something that happens globally on the chain. It is immaterial here.
You're essentially suggesting creating a new currency to introduce CT? I think that's throwing out the baby with the bathwater.
@_date: 2019-09-13 23:59:36
Nice post!
Don't worry about not being an actual cryptographer, I also just play one.
The usual solution to picking a random r is using a hash of the private key and the message. Clearly something private needs to go in there, or attackers could just guess its value.
When using sign-to-contract you correctly point out there is a risk of (internal) r reuse. If you create two signatures with the same message and same key, but different commitments, then anyone with access to the signatures, and the proofs can compute the private key used, because there is known relation between the (tweaked) r values in the signatures.
I believe the solution is simply computing the (internal) r as a hash of message, private key, and data being committed to. This guarantees that no relation is present unless the entire signature is identical.
@_date: 2015-11-16 20:21:47
The general message you're giving is right, but the details aren't.
The 'k' in secp256k1 indeed stands for Koblitz, but ironically, secp256k1 is not a Koblitz curve (which are a type of characteristic 2 curves, secp256k1 is characteristic p), only a Koblitz-like curve.
Due to that 'k', secp256k1 indeed allows a specific optimization (efficiently computable endomorphism, GLV), and in fact libsecp256k1 started out as an experiment to see what speedup could be gained from that optimization. Despite that, we're not enabling that optimization in Bitcoin Core, for various reasons. If we would enable it, there would probably be another ~30% speedup.
The current speedup comes from other optimizations, and due to being much less generic than the OpenSSL code.
@_date: 2018-04-19 20:46:06
All correct.
@_date: 2015-11-20 18:04:34
Pretty sure the 'w' is an approximant in Flemish, not a fricative. (EDIT: Seems the diacritic below it indicates that indeed. Interesting!)
'ui' is indeed supposed to be pronounced as /y/, but in my dialect it's not a diphthong.
@_date: 2018-04-08 14:04:12
The correct term for the function that goes from private key to public key is **trapdoor function**. This is a function that's easy to compute in one direction, and hard in the other.
It is decidedly not a oneway function. That would imply there are multiple private keys for each public key.
@_date: 2018-04-04 04:59:39


But you need to create a mechanism through which they end up agreeing.
What is the rule you're proposing? Assume both transaction pay the same fee, and that they're broadcast within 1ms of each other.
@_date: 2015-11-20 15:32:05
I'm certainly no expert in IPA.
@_date: 2018-04-19 18:50:28
Weird guy.
@_date: 2019-09-01 16:27:05
OTS already does this, but goes a step further. It will aggregate multiple documents that are submitted for timestamping into a Merkle tree, and only commit the root of the tree to the blockchain.
@_date: 2018-04-08 14:52:24
You can at least pick a simpler group, like multiplication of integers. See my explanation above (if that helps at all...).
@_date: 2018-04-22 18:46:38
Due to the widespread practice of key reuse, it doesn't really matter.
@_date: 2018-04-21 17:45:29
I'm not working on it (right now) because there are other facets of Bitcoin to improve, which are easier and more likely to be adopted.
@_date: 2018-04-08 20:56:40
Yes, but they would also be infeasibly large (as in: more digits than atoms in the universe).
@_date: 2018-04-21 03:53:03
Bulletproofs is a general technology to implement zero-knowledge proofs.
Bitcoin does not use zero-knowledge proofs. There is no way you can just "plug in" Bulletproofs anywhere in Bitcoin.
Perhaps you are talking about confidential transactions (CT). This is a technique to permit a public ledger that makes the amounts hidden while still permitting everyone to verify the balances adds up. If that sounds like magic, that's because it is.
CT rely on zero-knowledge proofs. They could use Bulletproofs. They could also use other types of zero-knowledge proofs (there are several, zk-SNARKS is the well known, each with their own trade-offs). Certainly the invention of Bulletproofs made CT more accessible.
However... CT is still very different from how Bitcoin works right now. There is no obvious way to integrate them. My best hope would be using something like an extension block, where you can have coins on the "legacy" side or coins on the confidential side - with explicit operations to move them between the two.
However, that is far from simple, and AFAIK nobody is really working on it. Much as I love this technique and would be very excited to see it in Bitcoin, it's unrealistic to think that it will be usable anytime soon there.
@_date: 2018-04-23 16:57:54
The existance of a theoretical execution model in which ECC can be broken is not the same as actually breaking it. I'm very happy to see research in post-quantum schemes, but actual quantum computers right now don't come close to solving ECC.
The reason for not proposing it yet for Bitcoin is that all post-quantum digital signature schemes have enormous keys or signatures.
@_date: 2018-04-21 10:37:07
Practical zero-knowledge proofs have existed for a few years now, and ZKCPs are indeed an application of them.
Bulletproofs are *just* a new type of zero-knowledge proof. Compared to zk-SNARKS, they're much more conservative in their security assumptions, and don't need a complicated setup procedure before proofs can be created. On the other hand, they're also larger and slower for complicated problsms. And they don't do anything that couldn't be done before - they're just more practical depending on your requirements.
@_date: 2015-11-21 17:05:03
@_date: 2015-11-20 15:12:57
I believe it should be close to /pit wl/.
@_date: 2015-11-19 23:38:50
Proposing is a big word. But yes, I'm working on a softfork segregated witness idea.
@_date: 2015-11-22 02:02:10
I beg to differ. They sound banal to you, because you haven't been trained to distinguish them. For example, in Dutch, "ui" and "eu" are distinct vowel sounds, and occasionally the only distinction between two words (for example between "ruis" (noise) and "reus" (giant). They likely both sound like "eh" to English speakers.
I don't mean this in a bad way. For example, Dutch does not have anything like a "th" sound in English, and I often fail to distinguish between a "d" and a "th" in spoken English, because to me, "th" is just a banal and familiar (but slightly weird") "d" (or "t"). This is even more apparent when you realize that speaker of some other languages that don't have that specific "th" sound interpret it as an "s", "z", "f" or "v".
@_date: 2018-07-06 21:54:33
Ah, but new signature schemes can be introduced by redefining a class of scripts which would formerly be trivial true (spendable by anyone).
Also, the BIP draft I published today isn't a fork at all. It's just a description of a signature scheme. It doesn't say anything about integration into Bitcoin.
@_date: 2015-11-16 20:34:30
It's unfair to compare OpenSSL with libsecp256k1 in general. They have wildly different scope and complexity. Libsecp256k1 isn't trying to implement SSL (which is on its own a tremendously complex task to do right, even ignoring testing and review).
Despite that, OpenSSL is not known for being particularly well tested or easily reviewed, while these are explicit goals for libsecp256k1.
@_date: 2018-04-25 03:53:30
The fact that it's possible to deanonymize things does not mean it's impossible to forge things!
Every system must lack *at least* one of unconditional soundness and unconditional blindness. It's perfectly possible to build a system that lacks both.
I'm pretty sure that Monero is not unconditionally sound. Even if it is, after switching to Bulletproofs they won't be (Bulletproofs cannot be made unconditionally sound).
@_date: 2015-11-20 22:23:16
f v s z sh v th 
@_date: 2015-11-23 10:15:40
I confuse the voiceless form with a t, and the voiced one with a d.
@_date: 2015-09-08 16:18:42
Read the blog post :)
@_date: 2015-11-20 23:48:27
Yes, absolutely. Otherwise you have no way to know whether a block is invalid (a relayer could just have changed the witness data).
@_date: 2015-11-21 00:01:05
@_date: 2015-11-16 20:45:38
Thanks :)
@_date: 2015-11-16 20:56:01
What changes in particular?
Libsecp256k1 has is being maintained in a separate repository ( Occasionally, the subtree in Bitcoin Core is being updated to synchronize with the upstream version.
Furthermore, there have been commits to switch:
* signing (0.10): 
* verification (0.12): 
@_date: 2018-04-08 14:48:33
See it this way: *All* of elliptic-curve cryptography's protocols (digital signatures, key negotiation, ..) do not actually need elliptic curves. They just need **things**. And perhaps it is a good idea to explain what you can do, without first going into the details of what those things are. Below is a very awkward attempt to to do without using any group theory terms.
You need the following:
* A finite set of things S.
* You can blah things together to get new things.
* A blah (B blah C) is the same as (A blah B) blah C.
* There is a special thing O that doesn't change things when blahed together with them.
* Every A has a unique anti-A such that A blah anti-A is O.
* Every thing is really just G blahed with itself a number of times. We'll call that multiblah. So G multiblah 4 = G blah G blah G blah G, for example.
* If I give you A and B, you cannot easily find what multiblah of A equals B.
A private key x is now a number smaller than the size of S. A public key P is G multiblah x.
You can infer properties of such a thing system, relying on nothing more than what I listed above. For example: (P multiblah a) blah (P multiblah b) is equal to P multiblah (a + b).
A digital signature using a private key x, using a random nonce k on a message m can be written as (r = G multiblah k, s = (m + r*x)/k), using all numbers modulo the size of S.
If you have a signature (r,s) on a public key P and message m, you can verify it as (G multiblah m/s) blah (P multiblah r/s) has to equal r.
The above is actually ECDSA, without any EC. I believe all the properties necessary to show ECDSA is correct are listed above. So there is really nothing inherent to EC in ECDSA - it's just an older algorithm (DSA) applied to an elliptic curve group. But the entire construction of how EC operations work is immaterial, and distracts people from how these algorithms work. It is not the precise curve equation or encoding of coordinate, or finite field math, or the weird geometrical construction using lines that intersect to add poonts together that matters. What matters is that elliptic curve points are things you can blah together, while remaining hard to figure out how many times you blahed them.
In case you think EC groups are the only ones in which these things work, and you'd say "sure, but all you've done is introduce weird names for elliptic curve addition and multiplication... you still need to explain what they are to make people understand why it's hard to compute the discrete logarithm", that's not really true. You could legitimately use integer multiplication (excluding 0) modulo a prime instead of EC groups, and still get a cryptographically secure system (it can't just be any large prime, though). O would be 1 in that case, and G could be 2. Blah is multiplying two numbers. Multiblah is exponentiation.
Furthermore, the only reason why we believe EC cryptography is secure is because people have tried to break it (in particular, ECDLP) and failed. There is nothing illuminating about the EC addition construction or formula that explains why ECDLP is hard. It's just that we don't know how to do it quickly. The same is true about certain other groups that are easier to explain.
@_date: 2015-11-16 20:16:15
The dbcache is limited on 32-bit version, as it requires 1/3 or 1/4 of the address space usable by a 32-bit process, and other parts of Bitcoin Core use signicant amounts too - even if they don't need all that much RAM.
@_date: 2018-04-09 21:01:30
There used to be, instawallet.org. It got hacked in 2013 and shut down.
@_date: 2015-11-21 11:45:20
I believe it's a bilabial approximant. I wasn't aware that the 'w' in English was not the same (but it sounds plausible).
@_date: 2015-11-19 23:42:51
In the design I'm working with, the signatures (witnesses) just remain part of transactions. They're just not hashed into the txid, and stripped before relaying to old clients.
@_date: 2015-11-20 15:08:26
In Flemish Dutch, the "w" is pronounced exactly like in English.
@_date: 2015-11-19 22:56:06
This was actually my opinion too for a long time... the thought of adding new data structures looked scary to do in a softfork.
Turns out, it isn't all that hard. We just add some extra data to transactions (the witness) which is not always serialized, and isn't included for hash calculation, and stripped before talking to old clients. Furthermore, we need some way to make the block commit to these witnesses.
It's still a substantial change to rollout, but at least as a softfork, it means that all participants in the network can upgrade at their own leisure, rather then be forced to do so before a flag date or be kicked off the network.
@_date: 2018-04-04 04:42:33


Here is your problem. They cannot be guaranteed to always agree. In fact, if they did, we wouldn't need a blockchain at all.
Because the speed of light is finite, it is impossible to guarantee that every node sees all transactions in the same order. Imagine you broadcast two conflicting transactions (double spends of each other, they spend the same money twice), simultaneously, one from a server in Australia, one from a server in Canada. Nodes near Canada will see one transaction first, while those near Australia will see the other one first. There is **no** objective criterion that is independently verifiable which will let you pick one over the other as more legitimate. The fact that they are connected through a randomly wired network, where each and every one of them may intentionally or accidentally delay propagation, makes this even worse.
In practice, every solution to this problem is somehow choosing a leader which picks the order in which transaction are processed. In Bitcoin, that choice is random, and proportional to hashing power. Competing to participate in this process is called mining, and one of the innovative things Bitcoin did was tie successful participation with distribution of the currency and fees - incentivizing cooperation financially.
@_date: 2015-11-19 23:22:16
There actually exist types of signatures that are even non-malleable with access to the private key. This requires that they don't have nonce. One example is BLS, but it's about an order of magnitude slower than ECDSA or Schnorr for the same security level.
However, even that wouldn't help us. Someone with access to the private key can just double spend.
@_date: 2015-10-03 02:20:33
I think it's completely off-topic here...
@_date: 2015-03-23 13:41:02
Satoshi's paper does not specify a protocol, only the ideas underlying it.
(Intentional) changes to the protocol are _described_ in BIPs.
But BIPs or whitepapers cannot _prescribe_ the rules of the network. If we find out that the actually deployed implementations on the network do not follow those documents, it will be the documents that are wrong and not the software. The other way around would mean convincing the entire world to change their implementations, or risking a permanent fork.
@_date: 2015-11-20 11:00:08
Yes, I'll be talking about segregated witness in Hong Kong.
@_date: 2015-11-16 20:44:03
Upvoted, I think you raise a good point.
What was probably not obvious from the pull request is that it was _not_ about reviewing libsecp256k1 itself - only about switching validation to it. That happened in a separate repository, and has been going on for over a year. Most of the work the past months was specifically introducing tests, formal validation, cleaning up the API, ... and fixing bugs. We (well, did find one bug in the cryptography ( that could in theory have led to incorrect results, though not in a way that was reachable or exploitable from Bitcoin. Other bugs have only been things like not clearing memory after use, not dealing correctly with missing arguments to functions, etc.
@_date: 2015-11-21 01:40:48
I'm perfectly fine with "Peter".
@_date: 2015-10-03 00:07:13
That's not actually true. The interlacing in FLIF can be disabled, and this usually results in slightly smaller files.
@_date: 2015-11-16 21:49:40
Bitcoin Core 0.12, to be released early next year, will synchronize and relay blocks faster than earlier versions. This is because instead of using the generic cryptographic library OpenSSL, we are switching to libsecp256k1, a library written from scratch specifically for the exact type of cryptography Bitcoin uses.
@_date: 2016-05-04 10:35:15
I have not had any correspondence with Mr. Wright.
@_date: 2015-10-03 00:04:55
Lossy. Lossless.
Image. Audio.
'nuff said.
Also, most of my work on what is now called FLIF was before I ever heard of Bitcoin or :)
@_date: 2018-04-25 06:51:25


Yes, with the old rangeproof construction there was an alternative that was unconditionally sound, but not unconditionally private.
This is not possible with Bulletproofs (and theoretically impossible with anything with similar compactness; unconditionally sound proofs cannot be small).


No, that's mathematically impossible.
@_date: 2018-04-21 04:34:51
A lot!
Bulletproofs are a general zero-knowledge proof technique. That means that they can be used to prove any statement over secret data without revealing that data.
You could prove you know 2 numbers that add up to 7.
You could prove that you know a string whose SHA256 hash is 16c28109a2719ebd4a123db11ff966f02d814354e3dc932449484f1c5a804af4, without revealing anything else about that string.
They could be used instead of zk-SNARKs in Zero-Knowledge Contingent Payments (swapping money for solutions to a problem). These can be done without any changes to Bitcoin.
You could (in theory, read on) use them to build a blockchain similar to ZCash (which heavily relies on zk-SNARKs for proving that their ledger makes sense), but without the trusted setup procedure or novel cryptography (Bulletproofs use very conservative security assumptions). Unfortunately, Bulletproof validation is much slower than zk-SNARK validation for complex problems, so it's not really comparable.
I expect that the most interesting applications are things we haven't considered yet. They suddenly bring zero-knowledge proofs into scope for a lot of problems and protocols where it wasn't really reasonable to go through the effort of using one of the existing proof systems.
@_date: 2015-02-19 17:16:58
It indeed starts from the beginning, because we can only fully validate blocks if we have all their ancestors.
@_date: 2015-02-06 04:09:39
It will speed up CPU load caused by signature validation, yes.
A conservative estimate is around 4.5 times faster on 64-bit systems, and around 1.5 times faster on 32-bit (the 32-bit code has had much less optimization effort).
@_date: 2015-02-05 19:37:50
It has two functions.
One is to coordinate when the new block validity rule becomes enforced. If we don't want to risk a fork in the network, at least a supermajority of miners need to start enforcing it at once. Ideally, every other node in the network does the same thing. By letting exactly the miners that pledge to enforce it put a marker in blocks (the version=3 field), we know that 75% will enforce it when the threshold is reached.
The other reason is indeed warning nodes that a new validity rule is active on the network, which their software does not know about.
@_date: 2015-02-05 19:31:17
The problem with BIP62 was that we were discovering new and mostly mild issues with it very late in 0.10's release cycle. It's not so much that any discovered issue was actually a problem, but more "hmm, nobody thought about this before", which doesn't inspire confidence in that we have carefully thought everything through.
Basically: just trying not to rush things that don't seem complete.
@_date: 2015-02-05 19:39:02
I would be very surprised if it happened that fast.
@_date: 2015-02-14 00:54:05
No, it's already merged for 0.11.
@_date: 2015-02-05 20:41:26
The best benchmarks are around 8 times faster "only"; and that's including experimental optimizations and with a dependency on libgmp.
@_date: 2019-05-26 02:02:36
No, you'd only download the headers between genesis and the assumevalidutxo point. That's trivial.
@_date: 2019-05-25 17:45:40
There is no encryption anywhere in the Bitcoin protocol. There is other cryptography of course, including digital signatures, but this is distinct from encryption. Encryption is relevant for privacy in some context, but what we're after here (security of your coins from theft), you need authentication. More specifically, you need a type of authentication that everyone in the world can check (to determine whether transactions are valid). Encryption in generally conflicts with this, as you can't verify encrypted data without the key. Digital signatures can be.
Somewhat simplified, your address is the public key. Every address has a corresponding private key, only known to the address' owner.
In order to spend coins that were previously sent to address A, you produce a transaction T, along with a digital signature on T using the private key corresponding to A, and broadcast. Everyone can verify that this signature is indeed valid for A (which is public), and thus the network accepts it. However, nobody knows the private key but you, so nobody can construct another transaction that spends the same coin without your consent.
Unless they have your private key...
@_date: 2015-10-03 01:57:33
I haven't actually worked on this for years. It was a hobby project of Jon and mine before I discovered Bitcoin, which he seems to have picked up again recently.
@_date: 2015-02-17 00:37:05
People running non-pruning nodes.
@_date: 2015-02-19 16:51:05
Just did a benchmark on libsecp256k1's current master, without GMP, without hand-written assembly, and it's around 3.6x faster than OpenSSL on my machine (64-bit code, i7 cpu). When the assembly is compiled in (which does not require any extra dependencies anymore), it's 4.9 times faster.
@_date: 2015-02-05 20:06:07
I'm pretty sure that's indeed the case. Nobody had come up with the block version based upgrade threshold mechanism when BIP16 had happened.
@_date: 2015-02-17 21:01:32
Mining our own business, since 2009.
@_date: 2015-02-06 18:14:41
I very much hope that the consensus rules of the system are not set by a trust of developers. I would hope that for example if we decided to to make Satoshi's coins spendable by my private key, a large part of the ecosystem would oppose it strongly and refuse to upgrade. Yes, developers have power over what their software releases do, but the community still has to agree with running it.
Ideally, no changes to the consensus rules are necessary at all. They encode the rules that people agreed about the system should enforce, and changing them means changing something that some may have assumed would never change. This is certainly true for things like not being able to spend coins without signature or printing money. It's almost certainly true for economic parameters like block rewards.
Reality is of course less than ideal, and sometimes changes are needed. Bug fixes, scalability improvements, ... Providing an automatic means for changing the rules (either through a blockchain of some kind, or by a signed update block published through other means) sounds incredibly dangerous to me. Anyone who gets access to the keys/miners effectively controls the entire system. I promise you I'll give my key to anyone who points at gun at me, and you'd be a fool to believe otherwise.
In my opinion, the only means through which consensus rules can be changed is by effectively convincing near-everyone that they should be changed. In a large ecosystem, that probably means that changing the rules is hard. That's fine. In my opinion, it should be hard to change the assumptions of what everyone signed up for.
Note that the above is not true for softforks, which can just be enforced by a majority of miners (and in fact, miners inevitably always have the ability to enforce stronger rules than the rest of the network), and don't require the entire network to change their implementations.
That said, several people (including me) like the idea of having the consensus rules written in a very simple script language, which for example has some primitives for cryptographic operations and database store/lookups (so these can be optimized). The benefit here is mostly making it more easy to analyse for consistency, and making it possible to reimplement the system on another platform by only porting a simple interpreter, rather than to reimplement the entirety of the consensus rules (which so far has seemed to be impossible to do exactly). I don't think this is viable for Bitcoin itself as it would be huge change, but perhaps in a sidechain or in altcoin? :)
@_date: 2019-05-29 01:47:44
Source: [
The original text is much longer than the quote here.
@_date: 2019-05-07 00:26:44
@_date: 2015-02-05 19:34:18
Thanks :)
@_date: 2019-05-10 06:51:09
Yes, the program was just called Bitcoin (it was renamed to Bitcoin Core to be distinguishable from the currency/network in 2014). The UI was built using the wx toolkit originally, but changed to Qt in 2011.
Also, it is true that P2PK was probably supported before P2PKH addresses were added, but this was before the software was released. The name "address" comes from the fact that they were originally IP addresses (which nodes would connect to to request a script to pay to); the P2PKH addresses were an alternative when the recipient was offline.
@_date: 2015-11-17 13:48:15
Bitcoin will be faster. Wheeeeeee.
@_date: 2015-02-05 19:43:17
Its purpose changed over time.
I initially started writing it as a means to experiment with some of the theoretical performance improvements that were possible with the elliptic curve that Bitcoin's signatures are based on (called secp256k1).
Since then, a ton of effort has gone into testing and reviewing, with a lot of contributions from Peter Dettman, and others, making it increasingly attractive for use in Bitcoin Core itself, especially now with several problems being discovered in OpenSSL.
@_date: 2019-05-07 06:56:47
Not any more than the fact that you can already create an OP_TRUE script.Yes, they're restricted to certain ranges of opcodes.
@_date: 2019-05-19 22:41:09
It is in fact designed not to; the data passed to the ECDSA algorithm for message signing is the hash of the message prefixed by "Bitcoin signed message:".
@_date: 2019-05-26 00:42:18
*If* checkpoints don't affect the chain, they have no effect. If they ever do, they enormously break Bitcoin's security model, as it means the assumption that miners will always work on the most-work valid chain is incorrect.
Whether or not they do depends on how they're incorporated and adopted. A once-a-year update to the Bitcoin software that includes a checkpoint in the software, set at a block months back, going through the same review process... very likely is not ever going to affect consensus. It also doesn't accomplish anything that can't be achieved using other means (like assumevalid).
Checkpoints that are incorporated frequently, through auto-update features, or even by having broadcasted signed network updates, are a different matter entirely. Those certainly may affect consensus, and if those are needed, it probably means PoW is broken.
What you're talking about however are not checkpoints; they're UTXO snapshots. They're distinct in that UTXO snapshots, like assumevalid, don't need to force a particular chain to be valid; instead they can be formulated in the form "I know the UTXO set corresponding to block X has hash Y"; only when X happens to be in the best chain, you skip building it from scratch. Concerns about the ability to validate such hardcoded snapshots are relevant though, and allowing them to be configured is even more scary (e.g. some website saying "speed up your sync, start with this command line flag!").
@_date: 2015-02-06 08:58:00
The numbers are for x86 and amd64 platforms. It may be very different for ARM.
@_date: 2019-05-28 15:59:12
Growing the number of peers = increasing the number of outgoing connections :)
Every connection is outgoing by someone.
@_date: 2019-05-30 00:38:15
This isn't implemented yet, and your questions seem to be about implementation details.
All we've established is that relaying to 8 outgoing nodes is a good idea. Whether the implementation allows just that, or whether there are reasons to make this configurable, and to what extent, are things to be determined in the next couple of months probably.
@_date: 2019-05-28 16:01:02
No, they're orthogonal. The complexities of Dandelion and DoS protection still remain.
Erlay on itself also weakens the ability to trace the origin of transaction somewhat, though not nearly as effectively as Dandelion.
@_date: 2019-05-26 02:09:32
Yes and no.
If you have 1% of the hashrate, you'll on average get one block in a 100, so once a day or so you'd be able to prioritize your own transactions arbitrarily.
On the other hand, prioritizing your own transactions in your own blocks means not being paid the fees that other people are offering for transactions you could include instead, which is lost income. Interestingly, that lost income is exactly equivalent to what you'd pay yourself in fees to have your transaction mined by others. So, no, this doesn't on itself improve your relation between cost and speed of confirmation.
If miners were intentionally censoring your transaction however, and it were to be delayed for that reason, being your own miner is an enormously effective way to improve the situation. In fact, that's why mining is (intended) to be decentralized in the first place.
@_date: 2019-05-27 15:56:01
I wrote this a number of years ago: 
@_date: 2015-02-05 20:05:13
So do I.
@_date: 2015-02-06 08:41:04
If you're going to broadcast changes to the consensus logic through the blockchain, you're at the very least allowing miners to censor your upgrades. That's very much in contradiction with Bitcoin's philosophy at least, which only lets miners decide on the order of otherwise valid transactions, and not about the actual per-chain validity rules.
I do like the idea of having the consensus rules written in some very simple script language, with a very deterministic interpreter, but making them updatable through the blockchain is not very useful. 
@_date: 2015-02-16 23:46:08
Note that pruning only removes the need to store the entire blockchain. You still need to download and process everything (as otherwise you'd need to trust someone about the effects of the historical chain) - it's just that older parts of the chain get deleted as verification progresses.
@_date: 2019-05-28 18:07:09
Yes, exactly. It's about (mostly) removing the bandwidth *increase* from additional connections, which is one step towards making more outgoing connections per peer feasible.
@_date: 2019-05-07 05:29:24
Among the features included in the proposal are a number of extension mechanisms. Specifically:
* Leaf versions: every script in the MAST tree has a version number of its own, which can be used to redefine script semantics (independent from the segwit script version, which goes in the output).
* OP_SUCCESSx: a number of always-failing opcodes are turned into automatic successes. This means they can be redefined to have any kind of semantics (as opposed to OP_NOPx which can only be redefined to operations that don't affect the stack). They can even be redefined to mean "if you see this opcode, all arithmetic is up to 64-bit instead of 32-bit", or things like that.
* Upgradable public keys: when a public key with an unknown initial byte is used, it is treated as a success. This enables introducing new signature schemes or sighash types without needing new sets of OP_CHECKSIG* opcodes every time.
Important to note is that none of the above are observable when the Taproot key spending path is used. So it's not so much that features need to be *in* segwit v1; if this sort of proposal gets adopted, lots of features could later be enabled *under* taproot v1 that aren't visible in outputs or most inputs.
Of course one softfork can enable multiple upgrades simultaneously, having mechanisms that let you separate the into distinct ones with no reduction in effectiveness means not everything needs to be done at once.
@_date: 2019-05-07 21:57:12
I'd like to know as well.
@_date: 2019-05-30 03:01:41
@_date: 2019-05-28 16:07:18
The parameters suggested in the paper are to use flooding on 8 outgoing connections (even if the number or outgoing connections would ever be higher), for transactions that were received through flooding. For all other connections/transactions, reconciliation is used.
This results in a certain % of bandwidth going to flooding, and a certain % to the various steps of reconciliation (numbers are in the paper).
Gleb also performed experiments to determine how this parametrization compares to others, and concludes that using flooding on just the 8 outgoing connections is indeed optimal.
@_date: 2018-12-25 10:44:41
@_date: 2013-02-20 22:46:43
What filesystem is B? Do you have write permission there? Are you using the full pathname, or literally '%appdata%' in the name? Perhaps you need to quote arguments (put "" around them, maybe duplicate the \'es, ... sorry my Windows-fu hasn't beed updated since WinME). How do you get the error? Is there any debug.log being written?
@_date: 2018-12-25 11:58:02
This is a minor release 0.17.1 with only bugfixes compared to 0.17.0.
@_date: 2019-05-26 01:32:24
Ok, semantics.
By the term "checkpoints" in Bitcoin I mean what has historically been called checkpoints. These force the chain to include a particular block, and don't affect the UTXO set. These were introduced in order to skip validation safely, before the concept of assumevalid was possible.
If you're suggesting (a) introducing more checkpoints and (b) tying checkpoints to UTXO snapshots... that seems pointless. It's unnecessarily more invasive than just UTXO snapshots, and I don't see any benefit for doing so.
To be clear: I'm in favor of (continuing to) research things like assumevalid for UTXO sets. I don't think there is any point in adding checkpoints.
@_date: 2015-02-19 17:28:47
Indeed, in Flemish the 'ui' is a single long vowel sound, not a diphthong (the English word for tweeklank).
@_date: 2019-05-02 22:00:29
I believe there was a misunderstanding, and the issue \`lnd\` had was with the master branch of Bitcoin Core (i.e., what will become version 0.19).
@_date: 2013-09-28 13:31:22
If you define "intrinsic value" as "the value something would have if it were not used as a currency", Bitcoin has none. Neither does the dollar or most fiat currencies, afaik. Gold does have intrinsic value, but way lower than the price it is currently traded at.
@_date: 2019-05-17 15:48:44
The deltas you see are for persisting the effect of `prioritizetransaction` RPCs.
@_date: 2019-05-28 16:07:43
@_date: 2013-03-18 23:48:54
Just to make sure this is not misunderstood: if you are running 0.7.x or below, you do have to upgrade or manually modify the database limits.
@_date: 2019-05-07 20:42:11
Wow, also news to me!
@_date: 2013-02-12 17:21:04
In 0.8.0rc2, a script contrib/tidy-datadir.sh will be present to clean up old files, for those who don't need downgrading anymore.
@_date: 2013-05-30 04:29:31
Transactions are always relayed as a whole or not at all.
@_date: 2019-05-29 08:27:20
That's not how it works. The number of addresses involved is not relevant.
One way to see it is that transactions are operations that melt and re-forge coins.
Every coin has a value, and its owner (address) printed on it. You'll roughly get one coin whenever someone pays you; even if it's multiple payments to the same address, you'll still have multiple coins.
If you now want to use your coins, you'll need to reforge them. Say you have a 5 mBTC coin and a 3 mBTC coin and you want to send someone 6 mBTC. To do so, you create a transaction that melts that 5 and 3 mBTC coins (so 2 inputs) and creates two new coins: a 6 mBTC one with the recipient's address on it, and another 2 mBTC one that goes back to yourself (though typically to another address of yourself for privacy reasons, called a change address).
User-facing wallets usually don't show you the exact inputs and outputs, as they're not actually relevant (you also don't keep track of exactly which dollar bill you received where or paid with). However, it does matter when determining fees. If your wallet consist of lots of small coins, you may need lots of inputs in your transactions to gather enough value to send on.
@_date: 2013-09-17 23:42:32
@_date: 2013-08-31 15:17:00
No, the torrent is just the raw block data, and it is processed the same way as data received from the network is. The main reason this torrent exists is because the block download mechanism in bitcoind is quite stupid and often degrades into downloading from just a single peer, or even stalling for several minutes. This is being worked on.
@_date: 2013-02-05 09:47:05
Bitcoin uses cryptography (digital signature algorithms in particular) - it doesn't use encryption (except to protect local wallet files, but the protocol or infrastructure doesn't depend on that).
@_date: 2013-02-14 21:52:07
I haven't expressed myself clearly, I think.
The current progress bar has several problems. First, it's inaccurate (by counting blocks instead of a better representative of verification time). Secondly, it's easily confused. Every solution which depends on relying on peers to tell the current state will not improve the second problem, and at least require a protocol change.
There are other solutions available though. Very recently there's been work (which may still end up in 0.8) to use time or total transaction count corrected by time for calculating progress. This does not rely on peer data, so cannot be cheated. It does require a hardcoded number to estimate the transaction rate in the future, but the worst a bad number can cause is some speedup/slowdown at the end.
@_date: 2013-05-30 23:33:37
A hard network rule is one that cannot be changed without updating all validating nodes on the network, as they would reject/ignore a chain that used it.
The maximum block size is such a rule. If it requires updating all nodes, it cannot be called "backward compatible". That doesn't mean it's impossible, but it's not easy, and requires widespread consensus.
@_date: 2013-02-20 23:06:16
The bootstrap.dat torrent has existed since the 0.7.1 release. It was 2 GB back then, but versions before 0.8.0 couldn't deal with &gt;2GB bootstrap files.
@_date: 2013-03-04 02:39:58
This is incorrect: ECDSA is vulnerable to a modified version of Shor's algorithm.
Edit: ECDSA is indeed not a hashing algorithm, and if only send-to-pubkey-hash style transactions are used, the window of attack is limited to the time between broadcasting a spending transactions and it being mined (essentially requiring a double-spend attack simultaneously with cracking the key). QC does also reduce the keyspace of hashing functions in half, but perhaps even a remaining 80-bit security level is too hard for QC computers in that hypothetical time.
@_date: 2019-05-30 18:06:10
I'm running a full node on a ROCK64 with 4 GB RAM.
@_date: 2013-02-17 00:09:31
A pruned node is not the same thing as a lightweight node. It is perfectly possible to create a pruned node which only retains unspent transaction outputs (~150 MB now) and does full verification. It still requires downloading the chain (from a non-pruned node) to verify history, but doesn't need remember the blocks it processed.
@_date: 2013-02-20 00:03:51
In general: you can delete blkindex.dat, blk0001.dat and blk0002.dat if you don't want to downgrade to 0.7.x or earlier anymore.
There is a file in the docs/ directory, called files.txt, which lists all files used by the client and in which versions
@_date: 2013-08-21 21:01:29
Multibit reuses keys until you create a new receive address.
@_date: 2013-02-20 22:21:18
Can you be more specific?
@_date: 2013-09-17 23:45:49
On first connect, the basic source of peer IPs is indeed the DNS seeds, but those serve fresh and random IPs. Once a few connections are made, peers exchange other peer's IP addresses among eachother.
The hardcoded list of bootstrap addresses is only used as a last resort, when DNS seeding fails to get any connection at all for some time.
@_date: 2013-05-29 19:56:34
First of all: the anti-dust-relay rule is a local client policy, and not a forking change in any way (and absolutely not a hard fork).
Second: the rule is not "outputs below 5430 satoshi are dust"; the rule is "outputs that at least marginally increase the fee for a transaction spending it by 1/3 of its value (according to the current fee policy), are dust". Currently, with the default fee set to 0.0001 BTC/kb (but it is configurable), that means outputs cannot be more than 5430 satoshi.
@_date: 2013-02-21 15:47:47
Yes, we're using LevelDB now instead of BDB.
@_date: 2013-02-24 23:03:12
Version 0.8 of the reference client (Bitcoin-Qt and bitcoind) was released last week. This is about a different client, Bitcoin Wallet for Android.
@_date: 2013-02-20 00:14:55
"Filtered mode" as you call it requires not being a full node but a so-called SPV node (simplified payment verification), which is what Multibit and Bitcoin Wallet for Android implement. It boils down to not verifying transactions, but verifying the block chain headers, and trusting whatever is in the best current best chain.
Long-term, that is indeed planned for the reference client, but there are a few steps (probably) before that happens:
* Pruning: downloading and verifying all history and all transactions, but not keeping the old ones on disk. This roughly corresponds to deleting the old files in the blocks/ directory now.
* Headers-first sync: instead of immediately fetching blocks and trying to connect them, first just download and validate headers, and when a good long chain is known, start downloading blocks.
* Headers-only mode: Once we have that, the step to just not doing the full block fetching at all, unless the user wants to 'upgrade'. This effectively turns the program into SPV mode.
Note: no guarantees about when these - if ever - get implemented.
@_date: 2013-05-09 20:31:53
No. Upgrading to 0.8.x reuses the chain you already have (though it needs to reindex/revalidate it).
@_date: 2013-02-13 09:25:29
But there is no reliable &amp; authenticated way of knowing the size of the blockchain before processing it. Except for hardcoding the size of the chain at different historic points in time, there is no true solution to this problem, and even that would result in a guessed progress speed after the last known size. The alternative, relying on peers to tell you the size of the chain before downloading it, requires a protocol extension and is prone to abuse (peers can just lie, without a way to verify it).
@_date: 2013-05-29 23:35:08
Do you mean reindexing or rescanning (because reindexing is always from scratch)?
@_date: 2013-02-21 10:31:35
Thanks for the report. It would seem that the Windows LevelDB environment doesn't handle network drives properly.
@_date: 2013-04-08 21:50:47
Every bitcoin release to date has been "beta", the system is still experimental.
Still, it is talking about 0.8.1 yes, and not any of its release candidates.
@_date: 2013-05-29 23:36:05
Every release ever has intentionally had the 'beta' marker.
@_date: 2013-04-01 03:14:14
I believe the fundamental problem is the abstraction provided by the wallet view. The client represents the user with a ledger-like interface and a balance of available coins. This is completely different from how the underlying block chain works, and in my opinion, the user shouldn't need to know how the underlying mechanism works.
However, when a user visits sites like blockchain.info, or reads about the mechanism of exporting/importing private keys, and sees addresses per balance, it's very natural to assume that this is how the wallet works as well. The client (at least the GUI) doesn't expose of these features, and (at least in a non-"expert" mode) shouldn't. The fact that both the internal representation and the wallet interface show a concept of "addresses" (one as a unit to compute balances over, the other as just an destination point for transactions) probably adds a lot of confusion.
In my opinion, a full solution for this is:
* Support deterministic wallets (see the BIP32 proposal), so that paper backups of just the secret seed is indeed a full wallet backup and not just a single key backup. At that point, I'd feel much more safe with exposing such an interface in the GUI as well.
* Add an "expert view" to the interface, allowing an interested user to see through the wallet abstraction and learn the internals. This feature is often called coin control, and will certainly be useful for people who want to micro-manage their wallets, but presenting it as such is not ideal in my opinion: if there is a need to have better control over linking between coins, there are more high-level tools that don't require breaking the abstraction (like multiple wallet support - also coming soon, and privacy-improving coin selection algorithms).
When choosing between the two evils (losing coins through misunderstanding of how change works, and the incidental loss of privacy by re-using addresses, we currently have the worse of the two. However, once the above problems are dealt with, I think there is no need for address reuse, and fundamentally I believe it is not how Bitcoin is supposed to work, as unlinkability of addresses is the basis for its privacy model.
@_date: 2013-06-01 08:54:40
What's a hard fork: two different clients on the network who (irreconcilably) disagree about which chain is the valid one. What happened earlier this month was a hard forking change: a change which risks causing such a fork. The actual fork didn't happen yet, but it's why unpatched/configured &lt;0.8 clients are no longer supported on the network.
Who else needs to upgrade: everyone who runs a full node (which in practice means Bitcoin-Qt and bitcoind), as such nodes do full validation of every block, and will not accept blocks over 1 Mbyte in size - they'll just ignore it.
I just want to stress that hard forks are very different from the several "soft forks" that happened in the past (BIP16, BIP30, BIP34, the march 11 switchback to 0.7, and workarounds for several of the very early bugs). These soft forks (what I'd call backward-compatible updates) just require a majority of miners to enforce them, and then have a switchover point defined after that point. Hard forks require _everyone_ (not just miners, and not just a majority) to upgrade, as anyone running a (full) node that doesn't completely match the rules used by the rest of the network, will effectively separate themself from it.
@_date: 2013-05-30 04:47:28
Bitcoin is inevitably a compromise between decentralization of "who can create transactions" and decentralization of "who can verify that no fraud occurs". Neither extreme is interesting (if only a few banks can create transactions, we have something that isn't more useful than the current international payment network; if only a handful of companies can verify the system, we have something that isn't less controlled than the Dollar or the Euro).
So whatever happens, there are limits to scalability. It all depends on where we want Bitcoin to go (fewer high-value but very transparent transactions, or more low-value but less transparent transactions). 
And to start that evolution, we need at least a free fee market: make miners choose their own fee rules, and have people bid for space in blocks. Then the network can adapt to relay whatever seems to get confirmed. And a first step towards that is what 0.8.2 does: it computes what the marginal fee increase is for consuming a transaction output you create. If that fee increase is more than 1/3 of the value of the output itself, it is considered dust.
To compute that fee, it uses whatever fee policy you have configured. The default is 0.0001 BTC/kB, but you can change it. The next step is instead of a hardcoded default, try to determine what the network itself confirms within a reasonable time. This is the plan for 0.9, and it will enable an interaction between those creating transactions and those mining them.
So, in case this isn't clear: there is no hardcoded value of 5430 satoshi anywhere. The only hardcoded value is the _default_ fee policy; all the rest is based on this... and it is planned to be removed in favor of an auto-detection system in a future version.
@_date: 2013-06-21 23:00:38
Thanks :p
@_date: 2013-02-18 22:59:53
The site linked to is mine.
The algorithm behind the graph does some mild form of extrapolation, which is more correct than simple averaging in the case of continuous growth, but it is also sensitive to sudden changes and may exaggerate them. So be careful when you see spikes.
@_date: 2013-01-12 22:47:47
This is correct.
@_date: 2013-04-04 23:27:38
Cryptography is wider: it can also be used for digital signatures (which is what Bitcoin uses). Encryption is about making stuff unreadable, not about proving authenticity.
@_date: 2013-05-29 23:32:31
You can still spend tiny outputs, according to the fee policy (the default for which was reduced in this release); it's transactions that _create_ them will not be relayed/mined by new nodes anymore though.
Spending such small outputs may still cost more in fees than it's worth, but that didn't change at all.
@_date: 2013-03-17 15:50:10
No, that's safe as long as the majority of miners enforces the new rule, as the resulting chain will in that case be accepted by everyone. This is a so-called soft fork, and we've done several of those already (including BIP16 and BIP30).
A hard fork in comparison requires everyone to upgrade, as the chain created by new miners risks simply be rejected by everyone.
So: soft fork is safe as long as 50% of miners upgrades. A hard for is safe as long as 100% of everyone upgrades.
@_date: 2013-04-08 08:59:00
If there is an issue with the 0.8 version, please report it on  instead of assuming it will be fixed at some point.
@_date: 2013-02-19 23:56:12
The old files are actually hardlinked to the new files. Windows is known to count the size wrong in this case: they are actually only stored once. Still, deleting the old ones won't hurt.
@_date: 2013-05-15 23:26:41
Formerly, only outputs of value 0.00000000 were considered dust. As of 0.8.2, any output that costs more than 1/3 of its value to spend it will be considered dust. To determine this cost, the standard fee policy is used. 0.8.2 however does reduce the standard fee policy from 0.0005 to 0.0001 BTC/kB, and also makes it configurable. At 0.0001 BTC/kB, any output below 0.0000543 BTC (54.3 uBTC) will make the transaction dust.
So, technically this release increases the minimum output size (a lot!), but the average fee (for transactions creation and relaying) goes down and becomes more flexible.
@_date: 2013-02-12 17:17:14
Note that "download only the blocks you need" is something only light clients can use. So the bloom filtering functionality in 0.8 refers to supporting such light clients. Since the reference client implements a full node, it needs all transactions to validate history.
@_date: 2013-02-20 22:22:35
Matt (who maintains the PPA) said he'll try to update them this weekend or earlier.
@_date: 2013-03-02 16:36:20
fixed, there was a problem with my site.
@_date: 2013-02-12 17:17:29
You are right.
@_date: 2013-04-08 21:53:05
Your wallet will be fine. The problem is that your client, at some point after may 15th, will likely no longer accept the real block chain. At that point you will not see new transactions confirming anymore.
Anyway, if that happens, and you upgrade afterwards, you'll be fine.
@_date: 2013-09-01 18:17:36
I just rescaled the graphs a bit, and changed to Thash/s instead of Ghash/s.
@_date: 2013-04-01 03:25:23
See my separate answer :)
@_date: 2013-03-17 15:51:33
Not yet released.
@_date: 2013-03-17 13:07:22
0.8.1 only imposes extra limits compared to both 0.7.0 and 0.8.0. A hardfork by definition can only occur if something becomes allowed that was formerly not allowed. Following this definition, 0.8.1 will cause a hardfork after may 15, if a block too large for 0.7.0 is created.
@_date: 2013-02-19 13:08:25
Bitcoin is a consensus system. Obeying to the rules is necessary so different implementations agree. One may try to formalize the rules (and that would certainly be a good evolution), but eventually, the correct rules are those implemented by the network. If a formal spec and nodes on the network differ, the spec is wrong.
Saying that the rules are defined by the reference client is true (because that's what the set of fully verifying nodes mostly consists of), but misleading. A new version of the reference client entering the network is not more authorative than any other alternate implementation (but perhaps more trusted). Rules can be added in "soft forks" (see BIP16, BIP30, BIP34), but apart from that - and until a hardfork - all rules were essentially set by the first version of the program.
@_date: 2013-01-12 14:04:49
It is not the same attack.
@_date: 2013-04-01 04:13:56
Yes, this is planned. We're still working out the details (see BIP32).
@_date: 2013-02-19 23:57:06
Yes, this is expected. The database format on disk changed, so it needs to rebuild it. This only happens once, and reuses the blocks you already downloaded. It will take half an hour to a few hours.
@_date: 2013-11-06 21:22:01
What matters is not that the Bitcoin network doesn't do any floating point calculations. It is that most of the hardware running it _cannot_ do such operations at all.
I've asked bitcoinwatch to remove that number from their site, as it is utterly meaningless now.
@_date: 2013-05-29 20:05:38
In my opinion, the current way of doing transactions (transfer a static public key hash known as an address to someone, and have him broadcast a transaction that pays to it, and hope the P2P network relays it quickly) is the largest flaw in the Bitcoin infrastructure today. It grew because it was the only convenient way (and for a long time, the only way) of initiating transactions, and tons of infrastructure grew around it. They have no short-term interest in changing that, but we as a community do.
A payment protocol can change that, by moving the responsibility for getting a transaction confirmed from the sender to the receiver (as the receiver gets it immediately, and the sender can just forget afterwards). It also means we rely less on the P2P system as a communication protocol (which works, but it is slow, unreliable, expensive, and terrible for privacy), and get to transfer metadata along with payments. In essence, it separates "payments" (monetary transactions between identities) from "bitcoin transactions" (which are just ways to describe how individual coins move, and let the world validate that nobody cheats.
Of course, it also has the benefit of using URI's (which can be signed) over random-looking strings as addresses. Humans shouldn't need to see such garbage to check they're paying the right person.
@_date: 2013-05-31 03:40:22
You do realize that for a hard fork, not only miners need to upgrade?
@_date: 2013-05-29 19:35:48
Even if hard forks themself become common (I don't think they will), this would be more invasive than most validation rule changes. It would require not only full nodes, but also SPV nodes, ASIC manufacturers, and pretty much the entire mining subeconomy to adapt. The only reason I can come up with why a community would accept such a massive change, is if there is an imminent threat to the system otherwise.
@_date: 2013-02-11 21:06:54
You can delete the old files (blkindex.dat, blk0001.dat, blk0002.dat), after reindexing, assuming you don't want to downgrade to 0.7.x or lower again.
@_date: 2013-01-03 01:47:13
That is correct. Bitcoin private keys are however ECDSA and not assymetric crypto (which that page is talking about). Our keys only provide a 128-bit security level. See  
@_date: 2013-05-31 01:55:00
Well, "everyone needs to update" is not something I call "backward compatible". But sure, if that's what it means to you, you are right.
@_date: 2013-08-31 15:18:38
That's very uncommon. In most cases, synchronization speed is now limited because of the block download mechanism, and not because of bandwidth or CPU. So yes, until the block downloading mechanism is improved, the torrent makes things significantly faster.
@_date: 2013-05-29 20:07:21
That's not correct. A client can request two payment descriptors from different merchants and construct a single bitcoin transaction to fulfill both, and then reply that to both merchants. Sure, it's not exactly trivial, but it's not impossible either.
@_date: 2013-05-29 19:58:30


The 1 MB block size limit is a hard networking rule. Changing that requires every fully validating node on the network to be updated.
@_date: 2013-06-16 23:07:37
You only need to construct a private key for which the hash of the public key is equal to the one being attacked. So that's a 160-bit brute force, not a 256-bit one. In fact, once the public key is revealed, it's only a 128-bit attack anymore, because EC crypto with N bits keys only has N/2-bit security.
@_date: 2013-05-15 23:30:42
Most of the changes aren't directly GUI related. There is a large rewrite of the network code, and the memory usage has been reduced significantly. Furthermore, many bugs have been fixed.
@_date: 2013-02-25 22:19:14
You'd need around 200-300 MB still with maximal pruning. Note that pruning however doesn't change the amount of data to download or CPU power needed to verify it: it's just about not remembering all history that has already been verified.
@_date: 2013-02-12 20:26:53
This is wrong. See my comment above.
0.8 does not download any less data than 0.7.
@_date: 2013-10-15 19:27:05
I believe this is about Fedora, not RHEL.
@_date: 2013-02-20 00:01:46
You can delete blkindex.dat, blk0001.dat and blk0002.dat - though the latter two files are hardlinked into the blocks/ directory now (the new place, don't delete anything there), so deleting those will not save you any space.
@_date: 2013-10-29 00:34:40
It's final, and it's being implemented :)
@_date: 2013-04-08 21:53:31
"last" ?
@_date: 2013-03-11 00:29:21
Pruning would not be a problem for mining (at least, when considering pruning in its simplest form: still downloading and verifying everything, but not remembering historic blocks).
@_date: 2013-01-12 22:49:53
But do realize that if the site gets hacked (or the owner is malicious), they can instantly change the code that runs in your browser to steal the keys. This is not to say they shouldn't be trusted, or that you can't store moderate amounts of coins there for convenience, but just know that using a web-based client has inherent risks.