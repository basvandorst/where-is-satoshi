@_author: jtoomim
@_date: 2016-08-16 17:39:34


Correction: SegWit's "effective" max block size is 1.0 MB to 4.0 MB, depending on the distribution of transaction types. 
The 1.8 MB figure comes from 100% of users switching to SegWit transactions, but with no change in the proportion of P2SH+multisig vs P2PKH transactions. It is unlikely that 100% of users will switch wallets on day 0, but it is likely that the mix of P2SH will not change much on day 0, so the effective max block size will likely be less than 1.8 MB for quite a while. I personally will be surprised if we exceed 1.2 MB in the first month.
The 4.0 MB figure comes from unpractical transactions that are designed to be nearly 100% signature/witness data and nearly 0% transaction data. The only known use for such transactions at the moment is DoS attacks.
If you're going to use the 4.0 MB figure as the upper end of SegWit's max blocksize range, then you should use 1.0 MB as the lower end. If you're going to include the capacity for theoretical attack transactions as "effective block size", then you should also include transactions from non-upgraded wallets (which could also be spam attack transactions).
@_date: 2016-04-10 11:34:47
It's probably due to people upgrading to 0.12. Version 0.12 of Core, Classic, and Unlimited include a fix to the batching of INV messages, which makes it far more bandwidth-efficient. An INV message for 1 tx takes 193 bytes, and an INV for 10 txs takes 517 bytes, or 73.3% less bandwidth per tx.
@_date: 2016-04-04 18:17:46
If you don't like it, downvote it.
Edit: When I said "it", I was referring to the claimed astroturfing and shilling. If you don't like that behavior, I think the proper response to it is to downvote it. I apologize that this wasn't clear in context.
@_date: 2016-04-05 01:28:59


How is it "vote abuse" to downvote comments that you think are "astroturfing and shilling"? It seems to me that downvoting comments that you think do not contribute effectively to the discussion is exactly the designed purpose of the Reddit vote system, and I think astroturfing and shilling both qualify as non-constructive.


a) They are not "my crowd". I do not own them or control them. I do not belong to them. I am not their mother or their police.
b) I've seen allegations of vote cheating, but I haven't seen evidence supporting it. Everything that I have observed so far is adequately explained by people downvoting unpopular opinions. (That might not be how the Reddit creators intended the system to be used, but I do not think it qualifies as "automated vote cheating" so much as "using Reddit similarly to a Facebook Like/Dislike system".) Can you point me at some data that shows that what has been happening is more than people on getting downvoted when they state unpopular opinions?
One data point: you probably can't see it because of the [score hidden] stuff, but my "If you don't like it, downvote it" comment is now at -4 points. Is that because my comment was unpopular, or was it because of automated vote cheating coming from "my crowd"?
@_date: 2016-04-05 03:13:09


I see how you can have interpreted my comment this way. I have edited the original comment to clarify my intended meaning.
However, I disagree with the way you label the buttons to be "contributive/noncontributive". That is not what they are. They are up/down buttons. Voting based on contributiveness is like giving money to charity. It is nice when people use the buttons to promote the visibility of comments that contribute to the discussion, regardless of whether they agree; however, that is a matter of good reddit ettiquette, and is not the only valid way to use reddit. I think most people use up/down as a way of indicating "this is a good/bad point," where "good/bad" is determined after filtering the point through that person's own opinions. This might not be the optimal way to use reddit, but it's the most intuitive way to use reddit, and I think it's better than not using reddit at all. I would rather spend more my finite time reading things which most people consider to be a good point than things which most consider to be a bad point.
Complaining about how people use reddit buttons to indicate like/dislike instead of contributing/not is like complaining about how people push instead of pull when they encounter a **[Norman door]( If you see a handle on a door, you pull; if you see a bar, you push. Reading the sign on the door saying "push" takes longer than identifying a handle, so when the two conflict, the design of the door usually wins, even though the sign is less ambiguous. The way people use Reddit voting is mostly the result of how the buttons were designed, not whatever is written in some obscure [reddiquette]( document. Don't blame the users for the fault of the design.


The serious problem is the rift between the renowned cryptographers' economic vision for bitcoin's future and the vision held by a majority of users. Downvotes are the modern equivalent of rotten produce thrown at an unpopular autocrat. Banning tomatoes is not the correct response.
@_date: 2016-04-05 03:43:38


Thanks for the **[almost-link]( and especially the **[bitco.in post]( 
Voting bots are lame. Votes should be based on the content of a message, not the name that wrote it. 
(Btw, the fast response latency distribution can also be partially explained as the result of the use of RSS feeds. I know of one user who **[uses RSS feeds]( to follow certain individuals (e.g. me) and reply to their comments.)
Moderating based on politics is also lame. 
@_date: 2016-04-29 22:25:38
That log file only contains debugging information. You can safely delete it with no ill effects. (I just deleted mine to test.)
@_date: 2016-04-05 03:47:57
Moderation based on politics is lame. 
It sounds to me that the OP is promoting a moderation policy that would effectively be political. I do not think that claims of astroturfing and shilling are objectively verifiable (except in rare cases of extraordinary evidence), so any moderation attempts would have to be strongly colored by the subjective judgments of the moderator. Those judgments will inevitably be either motivated politically or at least accused of political motivations. Therefore, such moderation shouldn't be done.
Reddit has a mechanism for expressing subjective judgments like this. It's called voting. If you think a post is astroturfing or shilling, downvote it and move on.
@_date: 2019-07-07 03:02:29


I don't see Roger Ver's name among the [list of investors of purse.io]( Charlie Lee (Litecoin founder) is on there, as is Bobby Lee (BTCC), and Barry Silbert's Digital Currencies Group was one of the lead investors during one of their seed rounds. Roger Ver may be a minor investor who isn't on that list, but saying that he *owns* the company is clearly false.
@_date: 2019-07-01 18:14:31
Yes, each p2pool miner creates their own blocks. P2pool has solved the problem that Betterhash was trying to solved since 2011, with the only exception that p2pool doesn't have enough miners using it.
@_date: 2019-07-07 03:30:38
I'm not "promoting" purse.io. The OP asked about it. I use it (saved $110 so far this month), so I gave a review.
@_date: 2019-07-08 02:24:47


No, this is false. The difficulty is adjusted every 2 weeks so that one block (and 12.5 BTC) gets mined every 10 minutes, regardless of the hashrate.


In other words, the exchange rate is high. The hashrate is just a delayed indicator of the exchange rate.
@_date: 2019-07-02 21:56:25
Betterhash is no easier to use.
P2pool is trustless and decentralized. Betterhash requires that miners trust their pool operators, and pool operators still have a significant amount of power and centralization. Betterhash pool operators can simply choose not to pay their miners.
P2pool has 0% fees charged by the pool, and all transaction fees go directly to the p2pool users. Betterhash allows the pool operator to charge whatever fees they want.
P2pool is a decentralized pooling system. Betterhash is a protocol that lets you get all of the inconvenience of running your own local full node and decentralized block template creation system while simultaneously getting all of the fees from using a centralized pool server.
I don't know if anyone is actually using Betterhash. From the perspective of a miner, it seems to be a strictly inferior option compared to traditional pooled mining, so I'd be surprised if many people are actually using it.
@_date: 2019-07-01 10:32:24
P2pool has been around since 2011. Betterhash has been around since 2018.
P2pool is the thing that Betterhash is trying to replace. Matt Corallo thought that the performance issues that p2pool had were intractable, so he tried a different design. This p2pool release is expected to solve most or all of performance issues.
P2pool is more powerful than Betterhash is, as it's a complete decentralized pool instead of simply a mining protocol. P2pool is also fully compatible with stratum-based miners, so it's a drop-in replacement for a traditional centralized pool for any miner who wants to use it. Also quite a bit more complex, though.
@_date: 2019-07-01 10:24:05


The p2pool share chain. P2pool is basically a merge-mined sharechain that keeps track of which pool participants are owed what amount. Shares that are lucky enough are also valid Bitcoin blocks. Metadata that is only useful to p2pool and not to Bitcoin is hashed and then committed via an OP_RETURN in the coinbase transaction. Shares that do not meet the block difficulty threshold still get committed to the share chain, and serve as proof that the p2pool miner in question was making a good-faith effort to generate a block that would pay previous p2pool participants via the coinbase.
The hard fork changes the rules of what can and cannot be a valid share, but does not change the rules of what can and cannot be a valid block. Thus, this change affects p2pool miners (who need to upgrade), but does not affect Bitcoin users or miners at large.
@_date: 2019-07-07 02:57:37
I use purse.io on a regular basis. It works pretty well. I'm usually able to get a 13% discount or greater. 
The most common issue with Purse is that it won't always get the shipping and tax amount correct. It will frequently not include tax or shipping in its cost estimates even when tax and shipping end up being charged. When this happens, you have to manually fix the amount, which is annoying. Before you place the order, you should check to make sure it has a tax and (unless the item has free shipping) shipping cost estimate included. If it doesn't, add extra funds to cover that.
@_date: 2017-12-20 03:10:16
Millions of tx per second? Seriously? That's called "moving the goalpost."
Visa: 2000 tps average
Alipay: [2025 tps average](
I'd say 10k tps would be about enough for the whole planet. 10k tps would require about 2.8 GB blocks.
I can build a computer that can handle 3 GB blocks *today* for about $2,000. If transaction demand doubles every year, we'll need that level of capacity in 10 years. A computer that can handle 3 GB blocks will probably cost about $200 then.
@_date: 2017-12-25 22:02:40
The only way to get close to 4 MB is to fill the blocks with 63-of-63 multisig spam. The limit for organic transactions with Segwit is around 1.8 MB, assuming 100% of transactions are Segwit.
@_date: 2017-12-27 21:15:35
The issue is that there was a bug in how gas was calculated for certain opcodes, and around October of 2016 someone found this bug and exploited it with spam, causing about 1 month's worth of blocks that take a very long amount of time (about 2 weeks?) to process. Ethereum was hard forked to fix those opcodes' gas prices, so blocks before and after that spam window take a reasonable amount of time to process.
If you want to sync a node from scratch, without using the state root trie hash commitments, it's possible to do so. However, you will need to use a good SSD and increase the amount of RAM that parity or geth is configured to use as cache in order to make it through the attack period, or else it will take much longer than two weeks to get through those blocks. Parity seemed to work a bit better than geth, so I suggest that.
@_date: 2019-07-01 18:19:45
It's very difficult to do that with p2pool, since p2pool depends on the coinbase being used as the payout method in order to prove that current miners are playing by the rules and paying out previous miners for the work they've done. Centralized pools could easily do payouts via LN, but for a trustless decentralized system, it's hard to make the jump to a layer-2 system without losing the trustlessness that p2pool has.
It's also not necessary given typical miner sizes. The vast majority of the hashrate is held by miners with over 1 MW of hardware. This would earn them around $5k per day, even though they only have only 0.02% of BTC's total hashrate. Furthermore, since p2pool pays miners directly out of the coinbase, each payment only requires about 30 bytes, and no fees get paid on the payouts (though it displaces a small amount of fee-paying transactions from the blocks).
@_date: 2017-12-27 21:19:14
The block sizes are small, usually less than 20 kB per block. Most of the sync load is in updating the state trie for each transaction, which is very IO heavy and somewhat CPU heavy. The SSD and RAM are the most important attributes, with CPU speed (mostly single-thread performance) as a secondary attribute.
@_date: 2017-12-20 03:45:29
The pump happened because over the last couple days two things happened:
1. Bitpay announced Bitcoin Cash support
2. Someone noticed that Coinbase's API already showed support for Bitcoin Cash.
Both of these pieces of information were readily available on the internet. However, they were not available on this subreddit.
If you're blaming Coinbase for missing out on this trading opportunity, that's not really fair. You should be blaming yourself for using a restricted source of information.
@_date: 2017-12-21 05:22:55
Pruning is a thing. So are 10 TB hard drives. The $2k budget includes a large SSD for the UTXO set, an HDD for recent blocks, and pruning.
@_date: 2017-12-29 06:15:33
No, that is incorrect. Archive nodes store extraneous, unnecessary, and redundant information. 
Let's say you're writing a text file, and you record every keystroke (including backspaces) you make and the time you made it. You also store the finished text file, with deleted characters removed. Every keystroke would be stored either 1 or 2 times. This would be a full node. 
An archive node, on the other hand, would store every keystroke, *plus* the whole text file as of each keystroke. If your text file took 30 keystrokes to write, and the first keystroke was never backspaced over, then the first keytroke would be stored a total of 31 times. 
@_date: 2018-09-13 22:59:52
No, they can be spent multiple times per block. Only the first spend per block can qualify as a 0-conf payment, though.
Can you think of any specific use cases in which a vendor might need to accept a 0-conf transaction from a customer, then immediately use that money to perform a 0-conf transaction with another vendor? I can't. A grocery store might accept a 0-conf transaction at the checkout lane, and then they might want to buy more inventory to cover what they sold, but they can either (a) use mature funds from their wallet, (b) make the purchase later, or (c) place the order as a 1-conf payment, since a small delay is entirely acceptable for a business-to-business non-interactive order.
Perhaps you're thinking that the rule forbids 0-conf when a transaction has unconfirmed children? That is not the rule. The recipient or sender can chain as many transactions as they want on top of the 0-conf transaction, but only the first transaction in any chain can be used for 0-conf.
@_date: 2019-07-08 02:29:09
Currently, revenue with an Antminer S9 (the 3-year-old machine that currently makes up a majority of the network hashrate) is about $0.18/kWh. Miners spend about $0.05/kWh on electricity on average, plus a few cents on other expenses (like amortized capital costs), so the hashrate generally will increase as long as revenue is above $0.08/kWh. As the hashrate increases, the difficulty also increases, which decreases revenue per kWh. Eventually, this reaches an equilibrium and investment in new miners stops.
The network hashrate has no effect on ordinary users unless it falls dramatically and suddenly (e.g. by 50%). In that scenario (which has never happened yet), the blockchain becomes more vulnerable to attack, as it may become cheap to acquire or rent used but inactive hardware in that scenario. The more the hashrate increases now (before the block halving), the more likely it is that the hashrate will fall in the future.
So all told, I think that the hashrate increasing is mostly irrelevant, but slightly negative.
@_date: 2018-09-25 23:27:04
FWIW, I fixed antbleed by adding this one line to my DHCP/DNS server's /etc/hosts file:
    127.0.0.1       auth.minerlink.com
That eliminated the problem for all miners on my LAN in one swell foop.
@_date: 2017-12-02 17:48:11
Hi, industrial miner here. Currently, an Antminer S9 earns about $0.80/kWh when mining BCH. If a block reward halving happened immediately, that would drop to about $0.40/kWh. I pay about $0.03/kWh for electricity. With the current difficulty and exchange rate, we could still be slightly profitable after four halvings even with zero transaction fees. 
Four halvings should take 15 years. Historically, Bitcoin transaction volume doubled every 12-18 months until the 1 MB limit was reached. If that limit were lifted, then in 15 years we might expect a transaction volume about 1024 times higher than we have today. If fees are about 1Â¢/tx, then that would result in about $30k of revenue per block from transaction fees, which is greater than the current BCH block reward.
@_date: 2018-09-13 23:03:06
Yes, it's very funny, [but it doesn't work](
@_date: 2017-12-03 03:05:59
"Stupid" doesn't sound like a word I use in that context. Crazy, risky, and trusting, sure, but not stupid. Our customers aren't unaware of the risk of sending their gear to someone; they just are willing to accept it.
Whatever; I don't see how this is relevant.
@_date: 2017-12-27 21:23:11
Archive nodes are more than full nodes, especially in Ethereum. Archive nodes store the state trie at every point in the past in addition to all blocks. Full nodes just store all the blocks and prune the excess data from the state trie. Archive nodes are more like storing a copy of the UTXO set at every Bitcoin block, which is not something that Bitcoin nodes do. For more information on this distinction, you can read this article:
@_date: 2018-09-26 18:05:40


The concern is that SPV nodes do not fully verify the blockchain, and rely on economic incentives to keep miners honest, which means that if all users use SPV, they might not notice errors.
Personally, I think that objection is concern trolling.
@_date: 2018-09-12 19:12:28
This block was 2.26 MB, but it only contained 230 transactions. This is not a coincidence. The way you get large blocks with SegWit is by making transactions with bloated witnesses. This means having a lot of inputs per transaction (32 per tx on average for this block), not very many outputs (2.1/tx average for this block), and heavy multisig usage with a large average size per input (about 320 bytes per input for this block).
This block specifically contained a large number of 64.3 kB transactions like [this one]( This suggests that some institutional user is consolidating their multisig UTXOs right now. 
This kind of thing seems to happen once every week or two. It is not an indicator of long-term SegWit adoption; it's just somebody's cron job.
@_date: 2017-12-02 22:38:35
I think it's just frightening to the people who already have BTC and want other people to buy BTC. 
I don't think anybody is actually making the mistake. Any place that sells BCH also sells BTC more prominently.
The same concerns were raised during the Ethereum Classic fork, except that time, the Ethereum Foundation actually held a trademark on Ethereum and chose not to enforce it. 
@_date: 2017-12-27 23:20:48
If you don't have (or can't afford) enough bandwidth to run a full node, you can run an SPV wallet.
If you can't afford transaction fees, you simply can't use Bitcoin.
Bitcoin does not need to have full nodes in every country in order to function. While that is a desirable stretch goal, it should not be mistaken as a requirement for the security and stability of the system. What we need is to (a) ensure that Bitcoin has full nodes (and mining nodes) in *many* countries, so that no handful of colluding governments can substantially disrupt it, and (b) to maximize the availability of the system to users when taking all factors into account, including transaction fees and computational costs.
The point remains, though, that 3 GB blocks are feasible for a high-end machine *today*, and by the time we actually need them, will almost certainly be feasible for a cheap machine. I would be shocked if gigabit internet isn't widespread even in developing countries by 2027.
@_date: 2018-09-11 19:13:06
You mean 8 GB blocks *peak*, not average.
Keep in mind that Singles Day is already a worst-case scenario. It's a national holiday focused on consumerism.
@_date: 2017-12-29 08:38:48


Oops, should have been "state trie root hash"
Each block includes a hash at the root of the trie that stores all account states. This means that if someone says that as of block 3452354, the state of account 0x12345 was that it had 1 ETH in it, you can verify that by checking the hash of that value combined with its neighbors in the Patricia-Merkle state trie to see if that derived hash matches the one recorded in the block as the state trie root hash. This allows much better SPV security in Ethereum than is possible in Bitcoin without UTXO commitments.
Alternately, if someone sends you a copy of the account state as of block 3452354, you can simply hash it and check that it matches the state trie root hash encoded in the block, and you can verify the proof of work for that block, plus the proof of work for the next 1000 blocks or so. (This takes about an hour to fully sync.) If it matches, then you know that either it is the correct value, or someone was willing to mine at least (and probably a lot more than) 1000 blocks (5000 ETH, $4m) in an attempt to fool you. Furthermore, they would have needed to do that well in advance of your node requesting the state as of that block, without knowing which block you were going to request the state for, and they would need to continue mining that fake chain for as long as they wanted to trick you.
Basically, the state trie root hash gives Ethereum the ability for a node to trustlessly sync the blockchain without replaying all blocks. Bitcoin could have the same capability by adding UTXO commitments, but so far has not done so.


On Windows? Probably the same as on Linux or OS X, as long as you're running it from the command line. I suppose you could also make a shortcut and add the command-line options that way. See also:
@_date: 2017-12-02 22:58:19
If you think the business model revolves around name trickery, then you have no idea what Bitcoin Cash is about.
It shares part of it's name with BTC because everyone who held Bitcoin before August 1st has some Bitcoin Cash. Of the 506747 blocks in the Bitcoin Cash blockchain, 478588 are shared with the Bitcoin Core chain.
Its purpose is to more closely follow the Bitcoin whitepaper than the Bitcoin Core project does -- to create [a peer-to-peer electronic cash system](
You may disagree with the changes that they made in the Bitcoin Cash source code. You may think that an 8 MB blocksize limit somehow compromises the safety of the system. However, arguing against the right of people who disagree with the direction of an open-source project to fork that project and make something that better aligns with their goals and intended use-case is a negation of the very spirit of permissionless innovation that makes Bitcoin interesting in the first place.
@_date: 2018-09-11 19:10:57
No, you don't get to wave your hands and say that the data which I cited for real-world usage of an electronic payment system is somehow magically an underestimate by 10,000x.
The 1 billion tx/day figure is a worst-case scenario (a shopping-oriented holiday) and already includes all business-to-business use of Alipay. The factors you cited as changing the figure are already included in the figure.
@_date: 2018-09-06 23:09:39
Yeah. I was a frog swimming in a pot, and eventually noticed that my skin was starting to peel off. Turns out the water is boiling. Who knew?
@_date: 2017-12-27 21:02:54
3 GB per 10 min is 5 MB/s. I already have 100/100 fiber, which is capable of 12.5 MB/s in each direction. That's borderline sufficient, though it doesn't leave much headroom for high peer counts and INV overhead. That line can be upgraded fairly easily, though. So overall, bandwidth is not a blocker on gigablocks.
@_date: 2018-09-10 23:19:18
The wires are probably fine. Aluminum wires are usually able to withstand a multi-second arc fault at 1000 to 20,000 amps without significant damage. Really, what you have to be worried about isn't the aluminum getting damaged; it's the wooden poles being set on fire that's the main risk.
@_date: 2018-09-11 03:30:20
Math check: Alipay handles about 40% of China's total payments. On Singles Day, they saw a peak of [1 billion transactions]( in a day. That translates to 11574 tx/sec average, or about 2.7 GB per block average for 24 hours. That's for 40% of a 1.4 billion person country, or roughly 560 million people. If you needed terabyte blocks for a single city, that city would need to have 1000 GB/2.7 GB*0.560 billion = 202 billion people in it. 
I don't know of any single cities with populations of 202 billion. The comic is off by a factor of 10,000.
@_date: 2018-09-06 22:42:14
I think trademarks are okay. If I download something claiming to be the Electrum Wallet, I want to be sure that it has not been tampered with. If someone decides to clone Electrum and change it so that it always sends funds to the new author's wallet, and [calls the new wallet Electrum Pro]( I want spesmilo to be able to shut them down with a court order if needed. Brand names are useful in indicating the origin and provenance of a product, and I think that applies to open source organizations just as much as physical products.
Obviously, the *correct* way to protect against this is with developer signatures, but very few users verify the developer's signature in the first place, and in the second place most new users are not going to know the difference between spesmilo's meaningful signature and Lucas Lofgren's meaningless signature.
I think patents are probably a net detriment to society, and copyright is a wash.
@_date: 2018-09-03 00:56:25
@_date: 2018-09-27 00:59:09
Eliminating internet connectivity does not seem to be a priority among miners. If someone hacks in, the most they can generally steal is a few hours' worth of hashing. Securing a wallet is way more critical than securing the hardware.
@_date: 2018-09-11 00:59:51
Not usually. Substations typically have capacities around 30 MW. Power plants typically have capacities around 1,000 MW. The transmission grid links multiple plants and substations together, and typically has a capacity around 10,000 MW in a particular region. A loss of a substation will be a significant blip on a power plant's or a utility's radar, but will not usually cause a cascading loss of power.
@_date: 2018-09-13 23:02:27
Except [it doesn't work](
@_date: 2018-09-26 16:36:54
I think you're referring to a stratum proxy. Yes, that is one possible way to do things, but it's not particularly common. Most people have each miner connected directly to the pool server on the internet. Doing it this way does not take much bandwidth -- about 2 kB/minute -- and makes it easy to configure them with secondary and tertiary pool accounts in case the primary one goes down. Even in cases where machines are mining to a local poolserver or proxy (as is the case with my mine), it's usually desirable to leave them with a connection to the internet for off-site backup poolservers and firmware upgrades.
@_date: 2017-12-01 00:11:46
These days, I think it will be more like using Internet Explorer to download Netscape Navigator.
@_date: 2015-12-31 02:36:18


 
You mean 17 txs? I'll assume you do.
With Gavin's preferred method (which is actually what Pieter Wuille was working on initially until I think luke-jr noticed that SegWit could be done as a soft fork), if I understand it correctly, you would take a 17 tx transaction subtree tree and treat it as the left side of the full merkle tree. This would mean your 17 tx subtree would effectively get padded out to 32 tx during the merkle node hashing process by duplicating singleton nodes. Then you'd put your 1 witness program in the right side and duplicate it to effectively fill up 32 entries on the witness side. You'd get a 32 byte hash for each of those subtrees, and then you'd concatenate them and hash the result to get the merkle root in exactly the same way as you would do if the stuff on the right subtree were another 32 transactions instead of witness programs.
@_date: 2018-09-12 19:05:44
No, batching has nothing to do with this. Batching is the use of a single transaction with a large number of outputs. Outputs do not get any of the SegWit discount, and are limited to 1 MB.
The reason why this block was 2.26 MB and not 1 MB is that it contained a lot of 64.3 kB SegWit multisig transactions.
@_date: 2015-12-09 00:42:27


Correct, sorry.
@_date: 2018-09-13 23:03:59
They are an in-person retailer. You pay with a 0-conf transaction and you immediately walk away with the goods.
@_date: 2018-09-13 22:48:40
You are correct, 3 would not be propagated, nor would it be accepted to mempool. Transactions will only be accepted to mempool if their parents are all either confirmed or themselves in mempool.
@_date: 2018-09-06 22:58:54
I thought it was a quickly written hack-job, but thanks.
@_date: 2017-12-20 02:59:48
Because it got leaked yesterday. Someone noticed that they had added API support yesterday and posted that on the other forum. This announcement from Coinbase came before Coinbase was ready, and happened as a response to that leak.
@_date: 2015-12-28 21:09:33


It was an attempt to see if there are any blocksize increases that all miners can agree to -- i.e. a census to check for consensus. So far, the answer seems to be yes: BIP102, 2-4, and 2-4-8 seem to be acceptable to all miners that were queried, with 2-4-8 appearing to be the preferred choice.
@_date: 2015-12-07 17:32:41
If this is referring to my proposal, it is a fundamental misunderstanding of my proposal. I am suggesting using the stratum protocol as a block compression method, not as a validation-free mining method. In fact, the use of stratum that I am suggesting makes full validation easier and less expensive. The purpose of my proposal is to allow the coinbase transaction to be determined by nodes in China (where all revenue-critical systems should be done for security reasons), while block assembly and validation can be done in regions with faster internet (so as to not disadvantage non-Chinese miners and perform accidental selfish mining attacks).
The only risk that I can think of is that someone will take over the VPSs or colocated servers in foreign datacenters and use these nodes to intentionally mine invalid blocks for SPV double-spend attacks, or will include valid but undesirable transactions like double-spends. These attacks will be detectable after a few seconds (we can send the full transaction data to China on a channel that is not used in the latency-critical path in order to allow block publishing to happen simultaneously inside and outside China under most circumstances), and they're basically the same risk as running any pool server in a datacenter instead of in your own home or office.
I think the objections that have been raised about this issue come from people misunderstanding what I'm proposing. They hear the word "stratum" and think it means validation-free, but the truth is that 99.9% of all mining (including fully validating mining) uses stratum. I'm just suggesting using stratum a little differently than it has been used so far. The use I am suggesting is a fully-validating one.
@_date: 2015-12-08 12:48:40
Nope, you didn't miss anything. SegWit has the same bandwidth and CPU requirements as a blocksize increase for a given number of transactions for a fully validating node. It's mostly just an accounting trick to allow an increase in transaction throughput with a soft fork.
It's also an ugly soft fork in my opinion, and if it is deployed, it should be deployed as a hard fork anyway. Otherwise, it basically converts all fully validating software into non-validating software unless the software is rewritten to support the new block and transaction data formats.
@_date: 2018-09-10 23:23:18
The explosion happened at a substation, not a power plant. Substations convert power from the high voltages used in long distance transmission (e.g. 230 kV typical for distances longer than 20 km) down to the distribution voltage (e.g. 13.2 kV or 34.5 kV for distances up to 20 km). This is done with transformers. 
"Power plants" are generating stations, and burn fossil fuels or use nuclear, hydro, wind, or solar power to generate electricity. Most/all power plants are adjacent to substations for converting power to transmission voltages, but most substations are not adjacent to power plants.
@_date: 2017-12-02 18:24:34
The vitriol comes from the assumption that it's a zero-sum game, and that Bitcoin Cash's success somehow compromises Bitcoin Core's value proposition. In this particular case, people are worried that at some point in the future, when a person goes to buy "Bitcoin", they will end up buying Bitcoin Cash (BCH) instead of Bitcoin Core (BTC). This is a frightening prospect to people who only hold Bitcoin Core coins, since they would be missing out on the price bump.
@_date: 2017-12-13 21:30:34
Suggestion: Buy carbon offsets or invest in renewable energy to help offset the environmental impact of Bitcoin mining.
@_date: 2015-12-07 17:42:16
No. With SW, you still need to download all of the signature hashes if you want to validate. It's the same amount of bandwidth. All SW does is (a) allow you to increase the blocksize without increasing the "blocksize" (by moving it onto a merge-mined sidechain type thing), and (b) makes it easier to prune signatures off for reduced long-term storage requirements.
This is just allowing you to do some important functions on creating the new block inside China while still validating fully and without running into the performance problems of trying to publish blocks from within China. It's just using stratum to shift where tasks are being done, not using stratum to avoid doing certain tasks.
@_date: 2017-12-02 17:29:19


The value transmitted in the average Bitcoin transaction right now is [about $60,000]( Lightning will not be suitable for transactions that large. Consequently, LN will not be able to offload very many transactions from the blockchain, and transaction fees will remain high.
@_date: 2018-09-13 22:52:08
You are confusing a "0-conf transaction" with "a transaction that depends on another 0-conf transaction." Payment processors will accept transactions that are not yet confirmed as long as (a) their fees are sufficient, (b) they are valid, (c) they are standard transactions, and (d) all transactions upon which they depend are confirmed. Chained transactions are generally not accepted as 0-conf payment, so if you make a chained transaction like you proposed in the gist, you will usually need to wait for it to be confirmed before you get your goods.
@_date: 2015-12-30 11:00:41


Actually, no, increasing price worsens the situation. Fees tend to stay constant in real (USD) terms per kB. However, when the exchange rate increases, the block subsidy increases in real terms. This makes the mining network grow, which makes the cost of mining a block increase, and makes it harder to sustain the same network on fees alone in the future.
@_date: 2015-12-07 17:42:02
This suggestion of mine is not SPV mining or validation-free mining. I suggest you read my other comments.
@_date: 2018-09-11 22:48:59
Your multiplying factor would be to account for the daytime peak vs. the daily average, as I understand it, since my numbers were already for an abnormally busy day. 
If you zoom out to 30 days on [statoshi's transactions page]( you'll see that the typical diurnal variation is from about 1.9 tx/sec at "night" (i.e. daytime over the Pacific Ocean) and 2.5 tx/sec average versus around 3.5 tx/sec peak during the "day" (i.e. daytime over Europe/North America, evening over China). This suggests that a factor of 1.4x is the minimum needed, and 3x should be plenty.
@_date: 2015-12-26 21:16:02


I calculate $120,000/day, using Chinese electricity prices. (100 MW at $0.05/kWh.)


This is accurate. My electricity is about half the price of China's, and my internet is faster and cheaper too. There isn't as much mining in my location as there is in China because the capital costs in China are much lower. Over time, it is likely that operating costs will be more important than capital costs, and so mining will migrate into regions like mine.
@_date: 2015-12-24 10:50:37
Segregated Witness is like a blocksize increase only if all wallets upgrade to create transactions in the new format. This means SPV wallets like Multibit, Electrum, Mycelium, etc. all have to upgrade. If they don't upgrade, they will continue to generate transactions that take up the 1 MB limit. They will also have compromised security until they are upgraded to understand the new transaction and block formats. 
If 100% of transactions generated are in the new SegWit format, then we would get about 1.75 MB worth of effective space. However, a specially crafted block could use up to 4 MB of bandwidth, storage, and processing time.
@_date: 2015-12-30 15:00:39
@_date: 2015-12-31 00:05:14


I have had some brief conversations with Mow about SegWit. Mow was certainly aware of SegWit at the time that I conducted my consensus census. He was in the room in Hong Kong when several of the devs (including Peter Todd, Adam Back, and Pieter Wuille) argued in favor of the SegWit roadmap. I am quite certain that he was not ignorant of the Core plans. So were all of the other miners that I talked to, with the possible exception of BW.com (who I can only say was probably aware of SegWit).
Neither he nor I brought up SegWit during our conversation.
This does not mean that they do not support SegWit. It just means that they did not tell me that my census should have included SegWit.
@_date: 2015-12-07 16:51:33


My proposal has nothing to do with validation free mining. It would actually be fully validating, and would make the cost of full validation easier. The point of using stratum in this way is to move block creation and block validation out of China while still controlling the coinbase transaction (with its 25 BTC reward) in China (for Chinese pools). This gets rid of basically all of the performance problems of the Great Firewall and the consequent accidental selfish mining risk while having very low additional security risk. Basically, it just means that you have block creation and validation being done in a commercial datacenter on a VPS or dedicated server in a hosting environment instead of in your own office. I don't see how that can be used in an economically meaningful attack, but if you do, please let me know how.
It seems I'll need to have a conversation with Peter Todd and about this, as it seems a few Core developers misunderstood my proposal. I'll try to sit down with him tomorrow about this.
@_date: 2015-12-28 19:01:18
Most miners prefer 2-4-8 (except Bitfury, but they're okay with it). I prefer 2-4-8. I think we should do 2-4-8 instead.
@_date: 2015-12-26 22:47:18
I live in our datacenter in central Washington.
@_date: 2015-12-28 18:41:46


Quibble: It is *currently* an unacceptable solution (to a majority of miners and developers). That may change once we have IBLTs, blocktorrent, libsecp256k1, better parallelization, UTXO checkpoints, etc.
@_date: 2015-12-31 14:48:28
The problem is that the algorithm used for SIGHASH_ALL is O( n^2 ), and requires that you hash 1.2 GB of data for a 1 MB transaction. See  slide 12 and later.
@_date: 2015-12-20 02:34:01


Only if we're using the current block propagation algorithm sans relay network. If we replace the block prop algo with something that doesn't thoroughly suck, then much larger blocks should be feasible.


No, the RN is fully validating as typically used. With the RN, the intermediate servers do not validate the block, but the only one that matters -- your own bitcoind -- does.
@_date: 2015-12-30 20:47:04


Hearsay testimony it is not. The non-CEO criticism is accurate.
Isn't everybody personally invested?
@_date: 2015-12-07 17:45:22




Incorrect. The aliens are actually really nice.
@_date: 2015-12-19 14:57:08


Only in the big cities on the coast, e.g. Beijing, Shanghai, Shenzhen. The mining farms in e.g. Yunnan or Inner Mongolia have terrible bandwidth and heavy packet loss. Thus, most miners are already running getblocktemplate in datacenters on VPSs.


This will not happen. The main Chinese pools either already have block publishing servers abroad or are adding them now. Most will also be adding block creation servers abroad, and some will be eliminating their block creation servers inside China.
Note that you can publish blocks inside China without doing GBT in China. If Chinese pools take it upon themselves to get their blocks out of China quickly, and if Western pools take it upon themselves to get their blocks into China quickly, then there would be no problems, and no special censorship concerns beyond simply having 60% of the hashrate inside one country. Currently, the Chinese pools are planning on doing more than that, adding ways to get blocks both into and out of China quickly.
@_date: 2015-12-28 17:56:01


Not quite. Thanks for tagging me so I can add nuance.
I had some conversations on the long-term vision for bitcoin with a few of the miners and pools. I think BTCC and Bitmain believe that we need to be working hard to drastically increase the blocksize so as to be able to pay a substantial share of mining expenses with transaction fees by 2020's block reward halving. They just do not think that we're technically ready to commit to blocks that large yet.
So many of them do want "XT level growth", they just don't want it right now. Right now they want 2-4 MB. They also want performance improvements as soon as possible. After the performance improvements, they want much larger block sizes.
Antpool/Bitmain wants the larger blocks to come from Core, but BTCC does not care so much. Both Antpool and BTCC are unhappy with the way Core is making decisions. Antpool's approach is different from BTCC's in that Antpool wants to change how Core makes decisions (e.g. based on a vote of stakeholders, including miners), whereas BTCC wants to change how the Bitcoin community makes decisions (e.g. should not be reliant on one implementation or one set of developers if those developers do not follow the will of the users).
@_date: 2015-12-28 01:56:27
I traveled around in China for a couple weeks after Hong Kong to visit with miners and confer on the blocksize increase and block propagation issues. I performed an informal survey of a few of the blocksize increase proposals that I thought would be likely to have widespread support. The results of the version 1.0 census are below. 
My brother is working on a website for a version 2.0 census. You can view the beta version of it and participate in it at  If you have any requests for changes to the format, please PM Note that he has been banned from this subreddit due to an administrative error, so you should not expect him to reply if you simply tag him here.
Or a snapshot for those behind the GFW without a VPN:
This post is a cross-post. The original post is on in  I may respond to comments in this thread, but I would prefer to have conversations in a forum without censorship.
**Edit: The doc and PDF have been updated with more accuracy and nuance on Bitfury's position. The imgur link this post connects to has not been updated. Chinese translation is in progress on the Google doc.**
@_date: 2015-12-28 11:42:40


It is not simply an amplification of BIP101's controversial features, but it does amplify many of BIP101's controversial features.
Miners choosing their own acceptance blocksize means that you're playing the [Keynesian beauty contest]( Now miners no longer have a hard limit on the size of the block they need to engineer their networks to be able to handle in a timely fashion. Instead, they have to guess what blocksize 51% of the network might be willing to create, and they have to engineer their networks accordingly, and they also have to choose a blocksize limit that they think will put them on the majority side lest they lose out on orphan races. This new game theory may result in even stronger hashrate centralizing forces than exist with big blocks alone. It also removes the weak protections that BIP101 has on total blockchain size and initial block download (IBD) times, and may make it very expensive to run a full node.
Miners are conservative and fearful, and I think that BU reduces the amount of security guarantees that miners and non-mining full node users are given. As such, I do not see it gaining enough support to be feasible. 
Whether I think BU is a good idea or not is irrelevant. I am trying to find miner consensus right now, and I do not think that BU has enough of a chance of being that consensus to be worth the time and social capital it would cost me to ask the miners about it.
I think BU has some potential as a long-term solution to scaling Bitcoin. However, it is even less viable as a short-term fix than BIP101 was. I think that we need to be thinking in the short-term right now. After we've done one hard fork successfully, we can work on a more permanent solution.
@_date: 2015-12-24 13:05:11
The data I collected showed that large blocks were unsafe for China if China were just using vanilla Bitcoin Core or Bitcoin XT with no relay network, no SPV mining, no UDP GFW crossing, etc. If your security model states that the network needs to be secure without those things, then the limit may be around 2-4 MB.
@_date: 2015-12-31 14:14:17
No, devs are strongly against increasing the blocksize right now because it's a hard fork. 
Segwit would have the centralization effect equivalent to a blocksize increase to something in between 1.7 MB (the typical capacity with SegWit) and 4 MB (the adversarial conditions capacity limit). If mining centralization were the limiting factor, then SegWit would not be an option to them. Devs support Segwit regardless because Segwit is a soft fork.
@_date: 2015-12-28 14:32:13


 
I think they have been saying that it *would* take many months to actually roll out such changes, not that it *should* take that long. The reason why I think they think that it would take so long is that it takes time to get everybody on board with the fork. That process was started quite some time ago, and people have been discussing the merits of different proposals for quite some time. The question is how far are we and much work do we have left? 
Keep in mind that BIP101's grace period was only two weeks. Gavin seemed to think that two weeks was long enough for people to upgrade their full nodes. Sipa seems to think that 6 to 12 months is the necessary duration. My opinion is 1 to 2 months. 


I do not currently have data on that. My impression from the discussion at the conference was that they thought that a hard fork sooner was preferable, but that's just an impression. I have not thoroughly discussed timelines or SegWit with miners yet. To-do list item.


Yes, that seems to be an attitude that is prevalent among Core (esp. Blockstream) developers and is not shared by the miners or most of the users.  Core Devs would get most of the blame in the event of a disaster, but they do not get a proportionate amount of reward in the event of a success, so I think they are suffering from a conservative bias compared to what would be optimal for Bitcoin as a whole. I think that Blockstream is serving as an echo chamber for anxiety and fear, and I think that this is a problem for all Bitcoin users.
@_date: 2015-12-30 02:11:38
At 360 MW and $0.05/kWh, mining costs $3000 per block. As of Dec 29 2015, fees are $0.16/kB: 19 MB would be enough. 
It doesn't even take any fancy fee markets. Default fee settings in Bitcoin Core for miners and for wallet software are enough. Collective bargaining amongst miners on a fee floor would also be plenty powerful enough to keep us at $0.16/kB. There is no need to increase prices if we can increase volume enough.
@_date: 2015-12-31 02:27:38


No, and they do not have to be. If you have a merkle tree with 17 entries, then you would get 16 entries in the left side and 1 entry in the right side. This is normal and acceptable. In order to fill the 2^n spots (32 spots) needed for the merkle hashing algorithm, you just duplicate any hash that doesn't have a natural partner. So in our 17 entry case, the last entry would get duplicated then hashed to form its parent node, and then that node would get duplicated and hashed to form the grandparent, etc. through to the great-great-grandparent, which gets combined with the left side of the tree and hashed to form the merkle root.
@_date: 2015-12-30 20:51:42
Correct, I decided not to ask about SegWit, because I did not think that the pools and miners had had enough time to digest it. I knew that at the time I was conducting the survey, I had not had enough time to digest it either.
I also did not present Bitcoin Unlimited.
@_date: 2015-12-28 08:53:14


Not really. I did not discuss anything that I thought was unlikely to get supermajority or near 100% support. This was an attempt to look for proposals that might have consensus, not a canvas of how much opposition there was to more ambitious proposals. I knew before I started that large long-term increase proposals would be flatly rejected by several large entities (e.g. F2Pool and Bitfury), so I did not waste any time discussing them.
@_date: 2015-12-20 02:28:47
The counterpoint is that they get better performance by putting their servers abroad. Also, they want the Bitcoin exchange rate to be stable and high. I think it is in their financial best interest to have their fast-path servers abroad. Do you disagree?
@_date: 2015-12-28 18:45:51
Less than double. More like 75% once 100% of transactions are using it. If only 50% of transactions use it, then it will be more like a 37% increase in capacity.
@_date: 2015-12-28 09:16:10


It forces them to accept and process those blocks in a timely fashion should another miner create them.


I don't follow your logic. I think the issue may be that miners want to be protected from other miners who may not be so conservative in their created block sizes.
@_date: 2015-12-30 15:13:06
Note that this is logic/code that would need to be written for all bitcoin implementations, including block explorers and thin wallets and not just bitcoind, should a "generalized softfork" be implemented as proposed by the OP.
@_date: 2015-12-30 18:56:51


Comparing 600 second average with one payment method to 1 second peak on another payment method is neither fair nor useful.




Well, you brought it up.
@_date: 2015-12-31 14:15:01
O hai! I didn't realize you were on reddit!
2 weeks might actually be borderline possible... 
@_date: 2015-12-30 18:45:45
The numbers for bitcoin would be for the average over 10 minutes, whereas the numbers for Visa would be for the peak over a few seconds. I don't think those are comparable. I think the average number of 1971 per second is more directly comparable. Otherwise, you have to account for the fact that Bitcoin can also exceed the average/sustained rate for a short period if you count 0-conf transactions being broadcast in bursts, or if you count several blocks being published in rapid succession due to good luck.


China Union Pay is **[13,000 tps peak]( I was not able to find average figures in a brief English-language search.


I think it is even close.
I'm not saying that using Bitcoin as a settlement layer is a bad thing. I'm just saying you should do the math before making assertions.
@_date: 2015-12-28 14:45:51




Because I think a hard fork blocksize increase is less disruptive to the Bitcoin ecosystem than a soft-fork SegWit would be, and will require less work (e.g. in block explorer code and wallet implementations) to make it viable.
Because I think that a one-time 75% increase in typical capacity is not the scaling solution we've been asking for for years.
Because I think it would be better to deploy SegWit as a hard fork only, and not require all Bitcoin software to be able to recognize SegWit data in two different places during IBD for the rest of Bitcoin's life.
Also, because I am worried that SegWit is actually a stalling tactic, the purpose of which is to avoid ever having to do a hard fork and increase the blocksize limit. 
@_date: 2015-12-26 20:58:52


This is incorrect. I have personally talked with representatives of organizations accounting for about 70% of the network hashrate, and all of them support a blocksize increase in the near future. Their support is guarded -- most do not support 8 MB right now, but all support at least 2 MB.
@_date: 2015-12-31 23:59:41
Thanks for the reply.
The two of us seem to have incompatible definitions of the terms "on-chain" and "off-chain". Perhaps we should use a new term like "chain-settled" or "chain-dependent" to refer to transactions that depend on the blockchain, but are not contained directly in it, like sidechains and Lightning? And maybe "in-chain" to refer specifically to transactions that are contained byte-for-byte in the blockchain? I like "on-chain" best to refer to transactions that are byte-for-byte included in a block, but, you know, I'm willing to compromise.
I think of a Lightning transaction in which Bob buys a beer at the pub as off-chain, because the marginal size of that transaction on the Bitcoin blockchain is zero, and the marginal fees for the Bitcoin blockchain is also zero. The only part of the Lightning transactions that hit the Bitcoin blockchain are the buy-ins and the settlements. As those are coupled mostly to duration and not to the number of transactions processed, and only weakly coupled to the amount of bitcoin processed (insofar that movement is asymmetrical between parties), it turns the fees Bitcoin sees from Lightning into a subscription model instead of a pay-per-use model. Performing a single in-chain p2pkh transaction could cost as much as a month worth of Lightning subscription in that case. Either the subscription cost is high and strongly discourages in-chain p2pkh/p2sh transactions, or the subscription cost is low and chain-settled Lightning transactions are not contributing significantly to the total fee pool. In either case, chain-settled transactions do not pay a fee that is proportionate to how much they benefit from the blockchain, because the current model is to pay for the block space (which chain-settled transactions use nearly zero of) instead of the hashpower (which chain-settled transactions still make use of).
We both want to pay for mining with fees. If we are relying on the Lightning subscription fees being high enough to be significant, that crowds out simple in-chain transactions, which I don't like. An alternative is that we let capacity grow, and let Lightning be basically free, and make in-chain transactions be affordable and very numerous. I like that much better. I think a Bitcoin in which in-chain transactions are plentiful and cheap is inherently better than a Bitcoin in which in-chain transactions are expensive and scarce.
But how do we walk the fine line between "cheap" and "free"?


Yes, I've noticed that too. The highest fees ever in real terms occurred when blocks were around 150 kB on average. Fees/kB then were about 6 times what they are now. They went down again shortly after bitcoin-qt 0.8.2 was released, bringing the default fee from 0.0005 to 0.0001. However, I reach a different strategic conclusion from that. I think this means that people just don't care about the fees at current levels. This is a good thing. If we encourage people to stick to reasonable default minimums, at least for the next few years, as a sort of good-citizenship thing, I think people would stick to default fees. I think this could be a more consistent and (actually) reliable method than a fee market driven by a hard blocksize limit.


Yes, this is one of the dangers of the fee market with capped blocksize. Since supply is inflexible once the limit has been reached, you can get rapid fluctuations in price as free capacity gets exceeded Monday through Friday during business hours in Europe and the USA, etc. Nighttime transactions might be basically free, daytime transactions prohibitively expensive, even though both bloat the blockchain equally. Fluctuations in fees might cause a fee-panic similar to a bank run, where high fees make people think that the system is becoming unusable, which makes them want to sell, which shifts the exchange rate, which makes them want to sell.... Eventually, most people have fled to an altcoin, and blocks are no longer full, so fees drop to zero and miners go bankrupt... Also, if the fee market became lucrative to miners, it may be hard economically/politically to ever increase the blocksize again. 
I think that if we're going to be doing economic market shaping, using the blocksize as the tool is rather crude and inefficient. I think that really what we want to be doing is setting a price floor. Interestingly, that is pretty close to the effect of the default fee settings in miners and wallets. I think that if we institutionalize that, it can go a long way (maybe 4 to 8 years). My calculations show that it is entirely feasible to pay for mining with large-but-reasonable block sizes and low-but-nonzero fees. I think that we can get enough users and transactions that find $0.05 to $0.10/tx to be close enough to zero that it's not worth changing for most users. 
If default fees end up not being sufficient, then perhaps we can look into system for setting up a collective bargaining system for miners to choose a minimum acceptable fee-per-kB, enforced by consensus. Yes, I know that such systems can never be perfectly enforced because of the potential for back-channel deals between users and miners, but we don't have to make it perfectly enforced. As long as fee-per-tx is small (on the order of a few cents each), the incentive for people to try to dodge fees should be small enough that we only need to make fee-dodging a little bit awkward and inconvenient, and possibly embarrassing. (E.g. you're paying your girlfriend back for the plate you broke while at her apartment, and in doing so you use a coinjoin transaction with F2Pool to avoid the mandatory minimum fee. She gets fed up with what a cheapskate you are and dumps you.)
The main thing I don't like about the collective bargaining approach is that it gives miners a way to choose a price that maximizes their revenue. While that sounds like a totally reasonable thing for a (decentralized) business to do, and would probably result in fees-per-kB that are acceptable to both miners and users (i.e. hits a reasonable point on the demand curve where volume is high but elastic vs price), I think it might give us miners too much revenue and will cost users too much. I'd prefer for us miners to make enough money to pay for a reasonable amount of hashpower, rather than as much money as possible, because... well, I think we're already hashing with more electricity than the use-case requires.
By the way, sorry for not supporting 2-4-8 a few months ago. Do you still support it, or do you think SegWit changes that? I think it's funny that we might have effectively switched positions.
@_date: 2015-12-25 07:38:32




You're right. My mistake.
@_date: 2015-12-28 08:00:27
I think he means doubling semiannually with linear interpolation.
@_date: 2015-12-24 12:51:30


If there is more money flowing out of BTC than flowing in, the price will drop. Day traders can't support the price, they can only improve liquidity and/or cause temporary manipulations of the price. In order to support the price, someone has to be buying the ~$3000 per block worth of bitcoin that miners need to spend in order to pay for electricity. 


Or they disagree with your interpretation that it means that Bitcoin is broken. My interpretation is that that piechart means that Bitcoin is imperfect, but usable. 
Regardless, you don't know what the effect of that pie chart is because you don't have any data from when that pie chart was more decentralized with similar economic conditions. If mining were more decentralized, it's possible that Bitcoin adoption would be higher. Or lower. Who knows? Not you or me.


I don't care what they say in public. I care what they do. They don't have to tell anyone they're panic-selling.


Not quite. Using large blocks allows you to effectively partition the network by connectivity. It allows a divide-and-conquer approach to selfish mining, and allows you to recruit collaborators in your accidental selfish mining attack by simply taking advantage of differences in network connectivity.


Peter Todd will probably have a talk with them and tell them that because it doesn't violate the protocol, it should be performed.
@_date: 2015-12-30 18:02:46


1 GB blocks at 400 bytes/tx would allow **4167 transactions per second**. Visa and MasterCard combined do an average of **[1971 transactions per second](
Such a system with current technology would end up being less centralized than Visa and MasterCard, but more centralized than most (including me) would consider desirable. In 10 years, though, that may be different.
@_date: 2015-12-26 20:53:42
Most miners run stock Core with default settings.
@_date: 2015-12-30 15:54:26


And the argument that you have to do the same thing with a softfork, except that you no longer have the option of the old chain, is also still there.




It's a safety feature that I hope will never be used except on the meta-level as a game-theoretic aspect motivating behavior of actors to keep them honest. 
As for determining which branch of the fork you want to take, you can achieve this by asking the individuals you wish to transact with which branch they are taking and use that version of the code. If there is actually controversy about which branch your friends are using, then you can run both branches at the same time. It's not an elegant solution, but it's also not a likely situation.


A softfork can be used to eliminate a transaction type that you care about, such as, I dunno, any transaction that comes from one of your addresses. A softfork is basically a controlled and institutionalized 51% attack.
@_date: 2015-12-28 14:52:41


3 MB as a witness+tx limit would be equivalent to subsidizing complicated and large transactions. 2 MB is too, but to a much lesser extent. Perhaps 1.75 MB is the right number for the summed witness+tx limit. If you have 1 MB allowed for tx data and 2 MB allowed for witness/scriptsig data, you're effectively taking the position that the cost of verifying witnesses is lower (or less important) than the cost of verifying UTXO availability. In actuality, verifying scripts takes much longer (10x to 100x, I think) than verifying UTXO availability, especially in adversarial conditions.


I think I mentioned it. I think it was mostly ignored.
Edit: more information on the limit here, 
@_date: 2015-12-07 17:38:53
In that slide, the "guy in Tokyo" is a VPS being run by the pool that also runs the node on the China side. It's the same company. They just don't trust their hosting provider in Tokyo to not tamper with or control the coinbase tx, so they put that in in their office in China.
@_date: 2015-12-28 15:37:53


If you do the softfork first, then you have the consensus data in two places in the blockchain for all eternity. 


I disagree. I think we could do a hard fork on the blocksize faster than SegWit. The only thing that has been keeping a hard fork blocksize increase from happening is foot dragging. Even with some foot dragging from Core devs, I think a block size increase could be active around March. With more foot dragging, it might take until June. 
On the other hand, I think something with changes as large as SegWit should not be rolled out in less than about 6 months, and even that is pushing it. I would prefer much longer code and concept review, and more time after the proposal has been finalized for wallet and other developers to update their software to support the new formats before it is rolled out. I tentatively think about 9 months might be a decent timeline.
@_date: 2015-12-28 11:20:35
Why not a SegWit soft fork instead of a blocksize increase hard fork? Here are my opinions. 
1. SegWit is a lot more complicated than a simple blocksize increase, and has been under discussion and investigation for a much shorter period of time. I am not comfortable with it being deployed on a time scale that I think a capacity increase should be deployed on. 
2. SegWit would require *all* bitcoin software (including SPV wallets) to be partially rewritten in order to have the same level of security they currently have, whereas a blocksize increase only requires full nodes to be updated (and with pretty minor changes).
3. SegWit increases the capacity for typical transaction loads (mostly P2PKH) to a maximum effective size of 1.75 MB. SegWit increases the capacity for blocks full of specially crafted transactions or multisig transactions to 4 MB. This means that it eats up a lot of our safety headroom (e.g. for adversarial conditions) while only providing a very modest increase in typical capacity. This seems like it might actually be intentional, as it makes the types of transactions that will be needed for Blockstream products like sidechains and Lightning relatively cheaper, and will not benefit on-chain transactions much. The reduction in headroom will also make any subsequent blocksize increases harder to gain political support for, which also may be intentional. This concern can be addressed by limiting the total size of SegWit block+witness to around 2 MB.
4. SegWit makes more technical sense as a hard fork. As a soft fork, it would be deployed by putting the merkle root of the witness data tree into the coinbase message or into an OP_RETURN transaction somewhere. The coinbase message field is already quite cramped with block height, extranonce1 and extranonce2, merged mining information, blocksize vote information, and other miner/pool-specific data, and I believe that the remaining space should be reserved for miner use, not for developer use. An OP_RETURN transaction sounds technically preferable, but still pretty crufty and nasty. Both options would have the result that all witness proofs would require including the merkle path to the transaction in question plus the transaction in question, and would increase the size of such proofs by about 50%. Putting the witness data in a sibling tree to the transaction data makes way more technical sense, makes the whole system easier to code for in all wallet and Bitcoin software that will ever be written, and reduces the size and complexity of verifying the witness proofs. However, doing so would require a hard fork, which is what the Core developers are trying so desperately to avoid.
5. SegWit makes more security sense as a hard fork. With SegWit, all Bitcoin wallets would no longer be able to accurately verify new transactions, and all Bitcoin software would need to be updated to be able to verify the new transaction+witness data format. With a soft fork, old wallets will see all new transactions as being valid regardless of whether they are actually valid or not. Pieter Wuille (sipa) makes the case that this will not result in any actual dangers to most users because miners will be selfishly honest and will only create blocks that are valid under the new rules. With a hard fork, old nodes would still fully validate transactions according to the rules that they were written for, and would not see new transactions or would mark them as invalid. They would remain on a separate and fully validating blockchain that lacked economic and had nearly zero mining. I think it is preferable to indicate the consensus rules have changed by moving mining and economic activity off of the old rules rather than to write the new rules in such a way that old nodes are relying on their trust in miners instead of verifying the rules themselves. I think that with a soft fork, nodes and software will not get upgraded as fast because the old code will still be dangerously mostly compatible, whereas with a hard fork, people will notice that they're not getting new blocks and upgrade because that's the only way they can maintain functionality.
@_date: 2015-12-30 11:48:47


We can and will fix block propagation. The current algorithm is extremely inefficient. A half dozen methods have been proposed to improve on it. This limitation will not last.
For adversarial conditions, blocktorrent holds a lot of promise as a robust solution.  I expect a good implementation to have ~20x speedup in adversarial conditions with large blocks.
For non-adversarial conditions, blocktorrent will be pretty fast (&gt;= 50x speedup with large blocks), but IBLTs will likely be better.
For the near future, we might be able to get thin blocks to work pretty well with minimal effort. 
We can also use a variant of the algorithm used by the Relay Network.
Or we could use something like the UDP algorithm implemented by Kevin Pan of Bitmain/Antpool to use bandwidth more efficiently. Their UDP algo improves GFW-crossing speed by about 10x without reducing the amount of data transmitted at all, simply by handling packet loss and congestion detection better than TCP does. (Note: The blocktorrent proposal will likely be implemented primarily using UDP and should get all of these performance benefits.)
My proposal is to make a can kick on the block size to some level that is relatively safe given current algorithms and technology, then take the time that buys us to fix the major performance issues in bitcoind (like block propagation, but not just that), and then pass a second blocksize increase plan that is more ambitious and long-lasting.
@_date: 2015-12-24 07:09:52
Yes, that is correct, as long as when you say "most miners" you mean "100% of the miners that were at the conference and made their opinions known."
@_date: 2015-12-28 15:50:30


6 months for SW sounds a bit scary to me, too. 
My rough schedule for the block size increase is this: 1 month of coding and code review, 1 month of testnet and/or regtest testing, and 1-4 months of deployment and activation grace period. 
It would be nice if we had gotten this done a year or two ago, eh?
@_date: 2015-12-31 02:12:27


Not accurate. Stratum mining clients don't care what the data represents. They just need the merkle path to the coinbase transaction, and the coinbase transaction itself. Since the transactions and witness data would be the left and right sides of the main merkle tree, respectively, stratum clients would be unaffected and would continue to operate, as they would just treat all of the SegWit data as a single 32 byte hash, the only purpose of which is to compute the merkle root.
On the other hand, if you stick the merkle tree into the coinbase message field, you might run into some problems with mining, as miners already use that coinbase message field for blockHeight, extranonce1 and extranonce2 (which usually total 8 bytes, but are larger or smaller in some implementations), merged mining information (usually 32 bytes), random voting, and a few other miner-specific things. In particular, 21 Inc's hardware has some of the coinbase transaction (including the mining address) *hard-coded in silicon*. While there probably won't be any conflicts with the exact place they want to put the SegWit merkle root in a soft fork deployment, I can't be certain, as I don't know every miner's setup well enough to say for sure. It's certainly going to be hairy, and miners do not like it.
@_date: 2015-12-08 22:58:27
Don't forget merged mining, which uses another 44 bytes:
That makes ~~76~~ 48 bytes already non-negotiably in use. Add to that miner votes and miner bookkeeping notes~~, and there's really not enough space to comfortably fit SegWit into the coinbase~~.
Really, sticking SegWit data into the coinbase is a kludge anyway. Putting SegWit data into a subtree off the main block's merkle root is technically a better place for the data anyway, and I think should result in significantly smaller fraud proofs. If you put segwit on the coinbase, your fraud proofs need the merkle path to the coinbase (about 10 hashes, or 320 bytes), plus the coinbase transaction (another 500 bytes or so), plus the path in the witness merkle tree to the signature data (another 10 hashes), and finally the signature data (about 200 bytes). If you put segwit where it belongs, in a branch of the main tree, you just need the path to the signature data and the signature data itself (320 + 200 bytes). please correct me if I'm wrong on any of this, as I was a little distracted during sipa's presentation.
In addition, there's the fact that doing segwit as a soft fork turns fully-validating nodes that don't upgrade in time into non-validating nodes. This opens them up to double-spend attacks and violates existing services' security models. I think it is preferable to do it as a hard fork, where nodes that do not upgrade will simply be cut off from the rest of the network and will not receive new data. I think it is better to receive no data than it is to receive potentially erroneous data.
Doing SegWit as a soft fork is a kludge. It really works better as a hard fork. It's being proposed as a soft fork right now because it's a way to increase the effective block size as a soft fork.
Also, SegWit in both its hard and soft fork versions is far more complicated than a simple block size increase. SegWit would require nearly all bitcoin software to be partially rewritten to accommodate the new transaction signature location and format. Pushing this through as a last-minute scaling solution does not sound wise when the alternative is (at its heart) just changing a constant on a single line of code.
@_date: 2015-12-28 12:02:23


I think you mean "Remote miners connecting to the major Chinese **pools**."
F2Pool (22%), BTCC (11%), Slush (4%), and Eligius (1%) are mostly remote miners. These are the true public pools.
AntPool (22%) is primarily self-mining, but accepts remote miners as well. I estimate that about 75% of their hashrate is their own.
Bitfury (20%), BW.com (7%), KnC (6%), 21 (2%) and Telco 314 (1%) are self-mining or solo-mining operations.
That totals to about 44% remote miners in public pools, and 53% in self/solo mining operations.
@_date: 2015-12-31 01:57:27


That is incorrect. It increases the capacity to between 1.6 MB and 1.8 MB for typical transaction loads, while increasing the maximum blocksize for artificially constructed 15-of-15 multisig transaction loads (i.e. adversarial conditions) to nearly 4.0 MB. Furthermore, to reach 1.7 MB of transactions, 100% of transactions would need to be using the new SegWit format, which means replacing all currently used wallet software.
On the other hand, a blocksize increase does not require updates to most wallet programs at all.
@_date: 2015-12-08 23:38:24


I don't think it's fair to count unit test lines as part of a patch. If you do that, then deleting all of the unit tests would make a patch simpler and therefore better. Also, I was not referring to BIP101 as a single-line patch. I meant something like BIP102, which could be a single line. Obviously, SegWit and BIP101 are not equivalent, because SegWit is a one-time increase to 1.75 MB capacity whereas BIP101 changes continuously.
Many of the changes to the bigblocks tree are not related to BIP101. The actual blocksize patch in BIP101 is this: 
You'll notice that about half of those 786 added lines of code are unit tests. That makes about 350 lines of code. A lot of the other lines are things like adding validation cost metrics and limits to the block size. 786 lines of code is better for a block size change than 1 line of code, if the extra 785 lines make it safer. (This is one of the reasons why I prefer something like BIP101 (possibly with different constants) over BIP102.) That would be true for SegWit's 2589 lines, if those 2589 lines made things safer.
@_date: 2015-12-28 08:17:58


So do I. However, "perfect" is the enemy of the good. Here is the data that I have now. If I get better data later, I'll release that.


I think your argument shows that miners are more important than pools, not that users are more important than miners.
I tried talking to several miners who use pools (both in Shenzhen at a meetup organized by the Lightning ASIC CEO, and via email with a few others), and I could not get an opinion out of them. A lot of miners, especially in China, just don't care about anything that doesn't directly and clearly affect their profit margins right now. Most of them trust their pools to make decisions like these for them.
On the other hand, the pools that I talked to all had clear and nuanced opinions on the blocksize debate. As a result, I decided to poll the pools and ignore the miners behind them.
@_date: 2015-12-28 11:54:58


That statement is factually false. Many people care what their opinion is on the blocksize debate, including most Core developers. There is a reason for this. Many Core developers do not think it is safe to have a hard fork in which a substantial amount of hashpower continues to operate on the old/small fork, due to the possibility of a malicious actor using the old fork blocks as a tool to defraud users who haven't noticed the fork and have not yet upgraded their full nodes. These developers believe that no hard fork should ever occur without nearly unanimous support for the hard fork among miners. I disagree with this risk assessment, but I see where the reasoning comes from, and will not dismiss it out of hand.
@_date: 2015-12-28 19:00:25


on Bitcoin mainnet.
@_date: 2015-12-30 13:24:12
I think this proposal is intellectually interesting, but crufty and hackish and should never actually be deployed. Writing code for Bitcoin in a future in which we have deployed a few generalized softforks this way sounds absolutely terrifying.
Instead of this:
    CTransaction GetTransaction(CBlock block, unsigned int index) { 
        return block-&gt;vtx[index];
    }
    
We might have this:
    CTransaction GetTransaction(CBlock* block, unsigned int index) { 
        if (!IsBIP102sBlock(block)) {
            return block-&gt;vtx[index];
        } else { 
            if (!IsOtherGeneralizedSoftforkBlock(block)) { 
                // hooray! only one generalized softfork level to deal with!
                return LookupBlock(GetGSHashFromCoinbase(block-&gt;vtx[0].vin[0].scriptSig))-&gt;vtx[index]);
           } else { 
               throw NotImplementedError; // I'm too lazy to write pseudocode this complicated for reddit.
        }
    }
    
    bool IsBIP102sBlock(CBlock* block) {
    // ...
    }
    bool IsOtherGeneralizedSoftforkBlock(CBlock* block) {
    // ...
    }
    CBlock* LookupBlock(uint256 hash) {
    // ...
    }
    
    uint256 GetGSHashFromCoinbase(CBlock* block) {
    // ...
    }
It might be possible to make that a bit simpler with recursion, or by doing subsequent generalized softforks in a way that doesn't have multi-levels-deep block-within-a-block-within-a-block stuff. Still: ugh.
        
@_date: 2015-12-07 17:23:47
So far I have not found any Chinese pools that are doing what I've suggested. Several thought they were doing what I suggested, but when I talked with them in more detail it turns out that they just didn't understand my proposal. I suspect there may have been an error in the live translation. I will be working with at least two of the pools to implement my proposal before I leave China.
Just to be clear: what I'm suggesting is to have only one transaction (the Coinbase) pass through the Great Firewall in the latency-critical path, and to have all other transaction validation and publication done abroad (e.g. in Tokyo). What the pools I've talked to are doing is using compression methods to only send the transactions that are not in the other node's mempool to communicate blocks across the Great Firewall. My proposal gives about 1 kB transfers across the GFW per block in the latency critical path, while continuing to fully validate, regardless of the block size, and it means that the rest of the world gets blocks at essentially the same time as China does (which means no accidental selfish mining attacks). Their current activities reduce the transmission requirements by something like 90% (uninformed guess), and still result in China getting the block long before the rest of the world does, and so consequently leave selfish mining effects significant.
@_date: 2015-12-07 17:41:14
Yeah, you've got my suggestion right, as far as I can tell.
@_date: 2015-12-29 14:44:45
They will soon, I hope.
@_date: 2015-12-31 19:19:40
I will admit that this thought has crossed my mind.
@_date: 2015-12-31 02:07:36
Not just multisig. 15-of-15 multisig. 100% as plain 2-of-2 multisig only gives you 2 MB.
@_date: 2015-12-24 12:34:24


I will not support any blocksize increase that does not somehow limit the number of bytes hashed either per block or per transaction to reasonable levels. This is easy to do. Gavin's BIP101 implementation includes code that does this, and should be easy to merge into whatever hard fork branch we decide upon.




No, I didn't find a limit of between 2-4 MB. I found data that made me think that 3-4 MB would be safe. I did not find data that made me think that above 4 MB would be unsafe.
I also had a test setup that did not accurately reflect mainnet's performance, and was designed to fail easily.
@_date: 2015-12-19 14:45:39


No, that is not what my conclusion was. I concluded that 3-4 MB was safe, but I did not say that larger than 4 MB is unsafe. Also, my testing was for a near-worst-case scenario in which there was no relay network. My early testing (before we fixed a network receive buffer size bug) showed about 10 seconds for a 1 MB block, and the actual network is running at about 4 seconds for a 1 MB block. Adding the relay network and nothing should bring the times for an 8 MB block down to about 2 seconds transit plus 3 seconds verification, which I think would be acceptable.
The main conclusion of my testing was that my nodes in China were far slower than the nodes elsewhere, and collapsed above around 3 MB. However, talking with Chinese miners and pool operators since then has taught me that my Chinese nodes were not configured in a way that reflects actual mining in China. The Chinese pools and miners have already implemented a lot of proprietary techniques for crossing the GFW. F2pool uses block prepublishing, "SPV mining" (VFM, verification-free mining), and a few other techniques. BTCC uses limited VFM (one block only) and a few other techniques. Antpool uses UDP and a few other techniques. If my tests used these techniques, we probably would have seen tolerable performance up to 8 MB. But we didn't.
I'm not sure I like a scenario in which everybody has to use Matt Corallo's relay network just to get tolerable performance on block relay. The decision to not use RN was a conscious one. I was testing safety in the absence of the RN. I was not testing economic viability, which is better done with the RN. Without RN, outside of China, 9 MB blocks were safe, but slow (up to 20 second propagation times).
@_date: 2015-12-24 13:01:15


Yes, but a tiny one. ~~Keep in mind that for a SegWit transaction to be used, all recipients and senders of the transaction must support SegWit.~~ My guess is that if BitPay, Coinbase, and Core supported SegWit and nobody else did, we would see about 10% of transactions use SegWit, and we'd get an effective block size limit of about 1.08 MB.
If 50% of wallets were upgraded to support SegWit~~, that would mean that about 25% of transactions would have both the sender and recipient supporting SegWit, and~~ we'd get an effective block size limit of around ~~1.19~~ 1.375 MB.


BitPay and Coinbase use a business model that depends on high transaction volume, not low fees. Neither one pays a significant amount of transaction fees. The reason why they support large blocks is that they can't get very many customers while block sizes remain so low.
@_date: 2015-12-08 23:59:00


BIP102 by itself doubles the severity of currently possible DoS attacks, which are also already way too severe. Fortunately the worst we've seen so far was only a 10 minute accident. The validation cost limits in BIP101 are a partial fix to an existing problem.
@_date: 2015-12-31 14:17:41
It won't stay on CPU mining. The choice of scrypt for Litecoin was supposed to make mining ASIC-proof. It merely delayed the release of ASICs for scrypt by about 1 year. 
With Bitcoin's current mining profitability, it would not last 1 year. I'd say 3 months to FPGAs, 6 months until the first ASICs, unless the PoW function was absolutely mindblowingly brilliant. In that case, 12 months.
@_date: 2015-12-28 08:23:08
I did not use his name to indicate that he supported it. I used his name to indicate that he proposed it. It doesn't have a BIP number, so using his name is the best way I could think of to identify it.
@_date: 2015-12-07 17:40:23
It's the same pool, just different servers. Chinese miners already have servers all around the world that they use for block propagation. I'm just suggesting they do their getblocktemplate on those servers instead of in China.
@_date: 2015-12-30 15:32:28




I don't think that is the hardfork mechanism that should be used. Miners can easily prove that they support a hard fork by voting on it, either by using coinbase messages (like the BIP100 voting) or block version voting. If you want to run your node in such a way that you can support the new block format if it gains a supermajority, but if support for the new block format is less than that you remain with the old block format, that is trivial to code. BIP102 does not include this forking mechanism, but BIP101 does. As such, I think this criticism applies only to naÃ¯ve forking methods, and not to hard forks per se.




I'm now unclear as to whether you think the legacy chain is the low-security chain (because it has less PoW) or the new chain is the low-security chain (because it has less code review). In any case, I think that miners are unlikely to move to a branch that does something undesirable like giving Greg 10 BTC/block because it makes it unlikely that users will follow that branch in the long run, which means that assigning hashpower to that branch is likely a waste. I think the ability to stay on the legacy chain if you disagree with the effects is a nice safety feature of hard forks.
On the other hand (since you're using extreme examples), a soft fork can be used to perform attacks against a minority of users. Let's say a soft fork is implemented that says Greg Can't Spend His Coins. Any block in which Greg tries to spend his coins is now invalid according to the new rules, and will get orphaned. Greg has no recourse except to talk to the miners and get them to revert their changes or to buy up enough miners to have 51% of the hashrate supporting him. He can't simply stay on the old chain, because there isn't one.
@_date: 2015-12-28 08:06:02
I had some conversations with a few of the miners, and at least BTCC and Bitmain/AntPool are sympathetic to much larger increases in block size in order to increase transaction fees and make mining pay for itself. However, they want to see performance fixes first. Both BTCC and Bitmain are interested in hiring some developers to help with performance fixes.
@_date: 2015-12-28 08:21:59
Are you referring to "consensus"? Consensus is a word which is very important for the Bitcoin Core development community. Its a word I do not like, but since it means a lot to a group of individuals I wish to influence, I use it to refer to the concept that they value. I would prefer a simple vote (majority or supermajority) for most issues like this, including whether or not to have a hardfork to increase the blocksize. However, if a majority of people think that we should only have a hardfork if there is consensus (no significant opposing hashpower), so be it.
@_date: 2015-12-31 03:41:27
It's a capacity increase, yes. It's not a blocksize increase. One reason why I'm being particular about this is that trying to call it a blocksize increase will just result in confusion, as there is no particularly good way to describe a non-SegWit blocksize increase except to say non-SegWit blocksize increase if your definition of blocksize increase does not include changing the MAX_BLOCK_SIZE constant in bitcoind. Some people might ask, "do you support a blocksize increase?" meaning a hard-fork increase in MAX_BLOCK_SIZE, and you reply "Yes!" meaning a soft-fork SegWit, and everybody just gets confused later on. On the other hand, there already exists a good succinct term to describe the set that includes both SegWit and a MAX_BLOCK_SIZE modification: a "capacity increase." Using "capacity increase" and "blocksize increase" as synonymous is thus a redundant use of terms as well as contrary to the practice of most other people (including gmaxwell and sipa, who wrote the arguments for SegWit).
With SegWit, you still have at most 1 MB blocks. It's just that those 1 MB blocks include a 32 byte hash that references a merkle root for the witness data, and that witness data is not inside the 1 MB block, and can contain about 0.75 MB of additional data for a 100% SegWit transaction mix with a typical proportion of multisig and p2sh transactions. The block is the same size as before. The difference is that you now have more data needed than just the block.
@_date: 2015-12-31 10:57:38


Yep, exactly.
@_date: 2015-12-31 01:54:11
No, Jeff Garzik's BIP102 is not currently the preferred option among miners. 2-4-8 is. The 2-4-8 option also has more support on  at the moment, though that could easily change as more people (like you, dear reader!) sign up and add their opinions.
@_date: 2015-12-30 14:41:54
not Your reply addresses the potentially recursive aspect of multiple generalized softforks fairly well, but does not address the complexity added by even a single generalized softfork.
@_date: 2018-09-13 23:02:50
No wallet will do this for you, because [it doesn't work](
@_date: 2015-12-07 17:16:51
Zaromet is correct. This is not VFM. This is making full validation easier by avoiding the bandwidth issues for domestic (within-China) nodes and making the security issues for international (VPS or colocated) nodes lighter. My proposal has been misunderstood by several smart people at the conference, and I'm trying to correct these misunderstandings. If you think this is dangerous, please talk to me about why. I will probably ask you to describe in detail what I'm proposing to verify that we're talking about the same thing, as that has usually been the problem so far.
@_date: 2015-12-28 08:03:38
I did not discuss this exact proposal with any of the miners, but I can guess what they would respond. I think F2Pool would oppose it because it scales too long over the long term. I think Bitfury would oppose it because it scales far past what Bitfury thinks is currently safe, and they don't like the idea of jumping out of a plane without a working parachute. Both F2Pool and Bitfury want to see performance fixes before any hard forks that get into the double-digit allowed blocksizes.
@_date: 2015-12-31 17:53:49


That's interesting, thanks. 
The thing that worries me about Blockstream's interests are that they seem to be in conflict with the interests of miners (of which I am one), not that they are in conflict with the interests of users. Miners need on-chain transactions in order to get fees. Blockstream's focus seems to be on ways of getting transactions off-chain, which would eliminate those fees. That worries me.
I'm not generally worried about Blockstream killing Bitcoin in other ways.
@_date: 2015-12-08 03:55:20
Twitter is not a reasonable forum for technical discussion. I don't touch the thing. If you want to encourage him to come here (anyone know his username?), I will be happy to talk with him.
@_date: 2015-12-30 20:16:59


We don't have the software. We might have the software in 6 months, though. This sounds like a fun project.
@_date: 2015-12-07 17:39:27
Yes, this is the point of my suggestion.
@_date: 2015-12-19 15:51:58


I think you mean one Aliyuné¿éäº ECS zone. Aliyun is the Chinese equivalent of Amazon, and uses terminology that is largely copied from Amazon, which might be the source of the confusion.
You can design the system in such a way that revenue-critical infrastructure is separated from infrastructure used for maximizing performance. With the GBT-abroad scenario, for example, there are a few attacks you can perform if you gain control of a foreign server:
1. You can change the coinbase transaction to steal the block reward.
2. You can insert an invalid transaction, making the whole block invalid, and denying the block reward.
3. You can use the hashrate to perform a Finney attack.
4. You can report an incorrect block height (invent a nonexistent prevHash) and potentially steal all  the hashrate for the whole pool network. 
The thing is, you can defend against all of these attacks pretty effectively. For 1, you can defend by simply not using the coinbase tx generated by the GBT server. For 2, you can send the full block after the stratum job on which the block is based (useful for doing multi-site block publication anyway) and check the block validity once the download is complete. If you detect an invalid block from one of your nodes, you ban it from the network and email an admin, and you only lose maybe 10 seconds of hashing. For 3, you do pretty much the same thing, but with a ban only if you see repeated occurrences. For 4. you check to see if you at least are receiving inv messages from other peers with the same hash as your GBT server is using for the prevhash, and if it isn't, you ban that node and use one of your other servers' GBTs instead.
I'm not including 5, where a government attempts to censor transactions, because that only works if all of the foreign governments ask to censor the same transactions, or if the government in which the mining is actually performed decides to censor the transactions. I expect that these pools will be setting up networks with nodes in 3 to 10 countries, and I think that the likelihood that all 3 to 10 countries decide to censor the same transactions is negligible. If that does end up happening, then the pools can just, you know, move their nodes.
Any attacks I'm missing?
I've explained all of these attacks to all of the pools i've talked to. I don't know if they will be implementing all of the checks that I'm recommending, but they know about the attacks and the defenses, so if they run into issues, then at least they will be motivated to add the checks.
@_date: 2015-12-24 10:49:13




Yes, I was talking about the actual block sizes, not the block size limit. The block size limit is almost irrelevant if nobody makes large blocks. (It still would guide hardware purchasing decisions and faith in the security and scalability of Bitcoin even if the capacity is not used.)


It depends on how you define the word "money." If you're referring to actual value or purchasing power, I think that the answer is no. Any proper response of the Bitcoin community to a selfish mining attack (accidental or not) includes a large reduction in the exchange rate. In order to profit from such an attack, the attacker would need to either perform it undetected, or perform leveraged shorts on the bitcoin price. If they did the latter, then they would need to be quite certain that they would not fail, or else they could go bankrupt after one attempt.
If you're referring to raw Bitcoins earned, then the answer is yes, after the difficulty adjusts to account for the higher orphan rate.
@_date: 2015-12-29 10:23:02


BTCC mentioned this as something they are open to. AntPool mentioned this as something they are not currently open to.
@_date: 2015-12-24 12:36:30
Huh? Are you saying everybody would be using BitPay or Coinbase? 
As I understand it, if 50% of transactions use SegWit, then you have an effective max blocksize of around 1.375 MB. You only get to 1.75 MB when 100% of transactions in the block are using SegWit.
@_date: 2015-12-30 15:02:50
In retrospect, I think I may have put the snarky comment and the dispassionate technical comment each in the wrong forum. Oops.
@_date: 2015-12-28 08:24:06
Yes, it is missing. I was doing most of this census too soon after it had been proposed and hadn't had time to digest it myself. I didn't think the miners had had time to digest it either.
@_date: 2018-09-06 14:01:00
He's a threat to all cryptocurrencies, not just the one he promotes. His pending patents don't care which side of the block size debate you're on. In fact, he may be more likely to use a patent against a cryptocurrency that he views as a competitor than the one he's personally invested in.
@_date: 2018-09-03 01:02:16


The sum total amount on-chain fees needed to pay for mining is the same regardless of the number of transactions that happen on-chain. Either you can charge a small fee on each on-chain payment made, or you can charge a large fee on each channel creation. Can you explain why the second option is better than the first?
It seems to me that the first option slightly discourages each transaction, whereas the second option discourages users from getting started at all but encourages each active user to perform a very large number of transactions. Is there a reason to think that one of those is better than the other?
@_date: 2015-12-30 14:09:58


You may notice that my opinions on these two topics **[are consistent](
@_date: 2015-12-28 21:12:11
371337 very lol.
@_date: 2015-12-30 14:38:41


Centralizing effects of hard forks? I don't follow. Can you elaborate?


I think it would be better described as an obsolete chain, not a low-security chain. 
Note that bitcoin-qt and other clients send alerts to the UI if no new blocks have been seen in some period of time to warn users to check their internet connections. In order for someone to be unknowingly left behind, they would need to ignore those UI messages as well as all the news articles and Alert System messages sent over the last month or more by devs urging everyone to upgrade. In order to be vulnerable, users would need to ignore those warnings and continue to transact with significant amounts of volume at the same time. I find it difficult to feel sympathy for an active Bitcoin business or user that pays so little attention to urgent warnings coming at them from multiple sources.
@_date: 2015-12-28 13:55:57
It is a reasonable proposal. Thank you. 
It is not my preference for how to scale bitcoin, mostly because I think SegWit should be a hard fork, but also in part because I think the timeline is backwards. I think the hard fork should be activated first, and SegWit should be second. 
I also think that the witness+tx space should be limited to some number (e.g. 2 MB pre-hardfork) rather than having a separate hard limit on the witness data to allow for more flexibility in transaction type mixes while still using the same amount of computational resources.
@_date: 2015-12-28 10:48:38
I think that miners do not and will not support Bitcoin Unlimited. Consensus does not exist for BU, so I didn't bother asking about it. The purpose of my census was to uncover existing consensus or near-consensus.
Bitcoin Unlimited takes the things that were controversial about BitcoinXT and amplifies them. It is the opposite of a compromise. I think that choosing not to compromise right now is a recipe for political deadlock and making the 1 MB limit remain. I do not support the 1 MB limit. 
I also do not support ignoring the explicit requests of a supermajority of miners. Bitcoin Unlimited does that.
Whether I personally think that 8 MB blocks are too large right now (I don't) or whether the blocksize limit is actually necessary (I'm skeptical) is irrelevant. This is not about my opinions. This is about the miners' opinions.
@_date: 2015-12-31 01:43:14
Nanoakron is correct. Miners do not favor continuing the 1 MB status quo.
@_date: 2015-12-28 18:59:29




Can you list a few altcoin hardforks? I've mostly been off of the altcoin scene, but I have been involved in two p2pool hardforks, plus I hardforked testnet about a dozen times.
@_date: 2018-08-10 06:55:10
Bitmain has about 170 MW of datacenters, the biggest of which is [135 MW](
The total Bitcoin mining infrastructure uses about 5,000 MW. That means that the portion of the hashrate that is not controlled by Bitmain is about 4,830 MW.
In order to perform a 51% attack right now, Bitmain would need to acquire an additional 4660 MW of mining capacity.
Bitmain probably produces about 60% to 80% of the world's Bitcoin miners. However, they don't have anywhere near the datacenter capacity to run all of the hardware that runs off their assembly lines.
@_date: 2015-12-30 16:50:42


I think absurd examples are great. I think the issue here is just making sure that your example can be applied only to the condition you are trying to oppose.
As for "Disagree with" forks, yeah, that's kinda how I feel about SegWit. With Bitcoin transactions, most of the validation CPU cost is spent in verifying the signatures, and a relatively small minority is spent in checking the UTXO set. With SegWit, one byte of signature data is arbitrarily given the cost of 1/4 of a byte of non-signature data, implying that signature data is easier to verify, which is backwards. The result of this is to subsidize transactions with complex scripts, such as those used for things like Blockstream's main products, sidechains and Lightning. I disagree with that. I don't know if I disagree with it enough to not use SegWit, but if it's deployed as a soft fork, I don't even get the choice (unless I control enough hashpower to block its IsSuperMajority() detection). I can't even just ignore the fact that other people are using SegWit transactions and be unaffected, because I'm still competing with their subsidized transactions for block space.
Why should I adopt SegWit? Because Greg's blog (or bitcoin-dev post) says so? This sounds oddly similar to the scenario you were complaining about with hard forks... except that it's actually happening.
@_date: 2015-12-31 01:48:56
Segregated Witness is a capacity increase to 1.75 MB. The 4 MB is only the blocksize that can be created as an attack block or a specially crafted block. It will never happen organically.
I do not support the implementation of Segregated Witness as a soft fork at all. If it were a hard fork, and if several other things were changed, then I'm okay with it. You can read my opinions **[here](
@_date: 2018-08-10 06:41:24
Bitmain historically does not announce new products until they are ready to ship within 2 weeks. Innosilicon, Canaan Creative (Avalon), Pangolin (Whatsminer), and GMO have all announced products which won't be shipping until around [mid-September](
All of these announcements have been made because the contract chip fabs only just started making 7 nm wafers recently. These companies have all been sitting on their 7 nm designs for months or years, just waiting for the manufacturers to add the capability to actually fabricate the chips they are asking for. As volume production of 7 nm wafers [began in April]( the first batches of wafers should be getting packaged into chips and delivered to the miner makers right about now. Given that it takes a few weeks to put the finishing touches on a design (e.g. voltage and frequency control, which depends on silicon quality), and a week or two for production itself, these manufacturers should all be no less than 3 weeks away from shipping. Bitmain usually waits until later before announcing a product.
@_date: 2015-12-28 13:48:37


If that is the attitude that people have, then that is how long people will take to do a hard fork. 
Most of the miners I have talked to do not like that timeline very much, and prefer something much faster. Most of the Core and Blockstream devs I've talked to do not like a fast timeline very much, and prefer something more like 1 year.
We will have to get the relevant parties together and see if we can come to an agreement on the timeline. I think we should do that after we have agreement on what the other parameters of the increase should be.
@_date: 2015-12-31 01:52:45
The intentional hardfork was actually activated May 15 2013: 
The unintentional hardfork was March 2013.
@_date: 2015-12-26 22:49:08
I am not referring to that discussion at the conference. I am referring to conversations I had over the next few weeks with each entity individually.
@_date: 2015-06-21 01:57:46
If you own bitcoin when the fork occurs, that means you continue to own bitcoin in both branches. Regardless of which branch becomes dominant, you'll be part of it. The only thing you have to worry about is making sure that you perform all important transactions on both branches for as long as it's uncertain which branch is going to be dominant.
If, on the other hand, you divest for an altcoin, you'll be part of neither branch. Instead, you're betting on being able to identify which branch is going to dominate before most other people do, and thus be able to buy into that branch while the price for that branch is still lower than its eventual market value. This is risky.
@_date: 2018-08-10 08:03:54
The 170 figure has two sources. The first source is from personal communication with Bitmain employees and other miners in China which indicated that they had around 30 MW of datacenters in the Inner Mongolia and Yunnan provinces before they built their megafarm. Costs per MW to build datacenters generally decrease with datacenter capacity, so it's unlikely that Bitmain would choose to build more small datacenters after they started to build one or more big ones. 
(I've experienced this first-hand. After having built a 2 MW facility and started work on an 8 MW facility, I turned down the option to cheaply retrofit a building as a 1 MW facility, because even though it was a good opportunity (motivated seller), the 8 MW facility was much better and cheaper overall.)
The next datacenter after the 135 MW one that Bitmain is building is the [Texas]( facility. A capacity has not been named yet, but assuming that 1/3 of the allotted $500m pays for electricity at $0.03/kWh suggests a capacity of around 475 MW, which would seem to make sense for Bitmain from a business perspective. However, that won't be active until some time in 2019.
The other source for the 170 MW figure is from Bitmain's own [hashrate disclosure]( which states that they have 1692.05 PH/s	 of SHA256 (Bitcoin) mining. Those miners are probably Antminer S9s with an efficiency of 100 J/TH, indicating a power load of 169.2 MW for their Bitcoin mining activity. While Bitmain could easily be lying about this hashrate number, the fact that it is broadly similar to what we can estimate based on our knowledge of their datacenters suggests that it is an honest number.
The 5000 MW figure comes from the network hashrate (43 EH/s) times the average energy efficiency for miners on the network (about 115 J/TH, in my estimation).
@_date: 2015-12-31 14:46:40
This type of transaction is described in 
This issue was addressed with BIP101. It will be easy to incorporate code from BIP101 to include a limitation on bytes hashed in any other blocksize hardfork proposal. The BIP101 fix limits the number of bytes hashed to the same level that is currently allowed, regardless of the new blocksize limit. A better fix is desirable, but that would require a softfork, which I think is better done separately, and should be done regardless of whether a blocksize increase is done.
@_date: 2015-12-28 08:32:03
Among other things, yes. These proposed fixes include block propagation improvements like IBLTs, blocktorrent, and built-in versions of Matt Corallo's RN algorithm. They also include fixes to the mutex/lock problems that Core currently suffers from which makes most of Bitcoin Core effectively single-threaded. Block pruning and checkpoints are also a medium-high priority. libsecp256k1 is another one which is almost done, as is the getblocktemplate stuff. Bitfury has also asked for performance improvements to the RPC interface, although I don't fully understand the performance problems he was referring to.
Block verification time limits would also be helpful for this goal, although it's not strictly a performance improvement.
@_date: 2015-12-30 12:58:11
Why not a SegWit soft fork instead of a blocksize increase hard fork? Here are my opinions. ([Cross post]( from 1. SegWit is a lot more complicated than a simple blocksize increase, and has been under discussion and investigation for a much shorter period of time. I am not comfortable with it being deployed on a time scale that I think a capacity increase should be deployed on. 
2. SegWit would require *all* bitcoin software (including SPV wallets) to be partially rewritten in order to have the same level of security they currently have, whereas a blocksize increase only requires full nodes to be updated (and with pretty minor changes).
3. SegWit only increases the capacity for typical transaction loads (mostly P2PKH, some multisig) to a maximum effective size of 1.75 MB. Achieving this increase requires 100% of new transactions and wallets to use SegWit. With 50% adoption, the capacity increase may only be 37.5%. (Previous rollouts of new transaction formats have taken about 1 year for widespread adoption.) On the other hand, SegWit increases the capacity for blocks full of specially crafted multisig transactions to nearly 4 MB. This means that it eats up a lot of our safety headroom (e.g. for adversarial conditions) while only providing a very modest increase in typical capacity. This seems like it might actually be intentional, as it makes the types of transactions that will be needed for Lightning and Blockstream products like sidechains relatively cheaper, and will not benefit on-chain transactions much. The reduction in headroom will also make any subsequent blocksize increases harder to gain political support for, which also may be intentional. This concern can be addressed by limiting the total size of SegWit block+witness to around 1.75 MB.
4. SegWit makes more technical sense as a hard fork. As a soft fork, it would be deployed by putting the merkle root of the witness data tree into the coinbase message or into an OP_RETURN transaction somewhere. The coinbase message field is already quite cramped with block height, extranonce1 and extranonce2, merged mining information, blocksize vote information, and other miner/pool-specific data, and I believe that the remaining space should be reserved for miner use, not for developer use. An OP_RETURN transaction sounds technically preferable, but still pretty crufty and nasty. Both options would have the result that all witness proofs would require including the merkle path to the transaction in question plus the transaction in question, and would increase the size of such proofs by about 50%. Putting the witness data in a sibling tree to the transaction data makes way more technical sense, makes the whole system easier to code for in all wallet and Bitcoin software that will ever be written, and reduces the size and complexity of verifying the witness proofs. However, doing so would require a hard fork, which is what the Core developers are trying so desperately to avoid. Doing SegWit initially as a soft fork then moving the SegWit merkle root later with a hard fork is an option, but that would permanently commit SegWit data to both places in different blocks in the blockchain, and consequently would require all Bitcoin software ever to be written to be able to read SegWit data in both locations in order to be able to complete initial block download.
5. SegWit makes more security sense as a hard fork. With SegWit, all Bitcoin wallets would no longer be able to accurately verify new transactions, and all Bitcoin software would need to be updated to be able to verify the new transaction+witness data format. With a soft fork, old wallets will see all new transactions as being valid regardless of whether they are actually valid or not. Pieter Wuille (sipa) makes the case that this will not result in any actual dangers to most users because miners will be selfishly honest and will only create blocks that are valid under the new rules. With a hard fork, old nodes would still fully validate transactions according to the rules that they were written for, and would not see new transactions or would mark them as invalid. They would remain on a separate and fully validating blockchain that lacked economic and had nearly zero mining. I think it is preferable to indicate the consensus rules have changed by moving mining and economic activity off of the old rules rather than to write the new rules in such a way that old nodes are relying on their trust in miners instead of verifying the rules themselves. I think that with a soft fork, nodes and software will not get upgraded as fast because the old code will still be dangerously mostly compatible, whereas with a hard fork, people will notice that they're not getting new blocks and upgrade because that's the only way they can maintain functionality.
Note that the FAQ linked to by OP addresses some of these objections. I present the objections here so that people can evaluate for themselves how well the FAQ addresses each one.
@_date: 2015-12-30 22:16:40
Are you referring to Rusty? I know he is employed by Blockstream to work on Lightning. I was under the impression that there were others. Still, perhaps I should delete Lightning from the comment. (Edit: I rephrased the original comment.)
I also have to admit that I don't really understand Blockstream's business model. I suspect sidechains are part of it, but I don't see how they can justify $21m in venture capital based just on that. I don't really know what their products are.
@_date: 2018-08-28 08:50:11
I pay even less without a contract. Under $0.03/kWh currently means $0.0265/kWh for me; I just rounded up for readability.
@_date: 2018-08-28 02:07:45
Miners are not mining at a loss. The analysts claiming otherwise are overestimating the costs of mining. I pay under $0.03/kWh for electricity, as do many other miners, but the analysts' models typically assume electricity costs around $0.10/kWh. With total operating costs (including labor, rent, and power) around $0.05/kWh, and with revenue from an S9 at around $0.10/kWh, we're still making a decent operating profit. Whether we are making enough operating profit to break even on new capital investments is unclear, but continuing to operate our existing facilities is a no-brainer.
@_date: 2018-08-11 22:28:25
You mean, they make them for a fraction of the cost that they sell them for. The opportunity cost of a miner is what matters here, not the production cost. If a manufacturer can make a machine for $200 in a week and sell it for $600, isn't that better for them than making a machine for $200 in a week and earning $800 from it over two years?
@_date: 2018-08-11 23:53:26
Until they max out their production line capacity, and have to invest $100,000 to increase it.
@_date: 2015-12-31 02:38:24
I do not consider SegWit to be a blocksize increase. It's a one-time limited capacity increase, yes, but not a blocksize increase. It has the same storage and transmission costs as a blocksize increase, and has most of the throughput improvements of a blocksize increase (assuming people use the new transaction formats), but at its heart, it's just an accounting trick to get around the need to increase the blocksize.
@_date: 2018-08-13 04:21:13
Let me know if you have questions.
@_date: 2018-08-13 04:19:20
First, the article is discussing the DAG of transactions, not a DAG of blocks or block-esque shares.
Second, the current p2pool design creates a share chain that is a linear representation of all shares. It is possible (and common!) to have orphans. Orphaned shares don't get rewarded. I've wanted to add support to p2pool for uncle shares a la Ethereum for a while now, but I haven't gotten around to it yet.
@_date: 2018-08-10 06:44:45
As a smaller miner, they're not putting me out of business. I think you're confusing "small" with "inefficient." I pay less per kWh than Bitmain does, so I can make a profit at a higher network difficulty than them.
@_date: 2015-12-29 13:42:26


Agree. Have an upvote.
(I've had a few chats with large-scale mining companies recently. They agree that they should be hiring their own Core developers, and are accepting applications. If you want to get paid to work on Bitcoin Core, send me a note and I'll tell you how and with whom to apply for a job.)
@_date: 2018-08-13 09:30:50
The DAG I'm talking about is the natural DAG of transactions.
Transactions have inputs and outputs. Each input refers to an output. That is, each input-output pair is an edge in the DAG. Each transaction is a node.
If your database contains the same set of transactions as someone else's database, then it's the same DAG. The transactions themselves define the connectivity.
We know that every database will have the same set of transactions in Bitcoin because Bitcoin has blocks and PoW mining. This is why Bitcoin rules and Iota drools.
But there's still a DAG. And knowing that it's a DAG, plus being able to check for double spends, means that we can do some cool stuff with validation.
@_date: 2018-05-04 04:49:18
Or just used one or two 120V PSUs. Actually, 110V-&gt;220V transformers capable of &gt;= 1500 W are not particularly cheap, and a new power supply would probably be cheaper.
@_date: 2018-08-28 06:12:27
Central Washington state. My utility company owns two large dams on the Columbia river. However, they are not currently accepting new cryptocurrency customers, and are talking about on raising rates for cryptocurrency miners by 207%.
@_date: 2014-01-13 19:06:54
I've only bought and sold Bitcoin miners with PayPal, and I still had my account frozen.
@_date: 2015-12-31 01:59:38
BIP103 starts at 1.0 MB in 2017 and increases 17.7% per year. It would take until 2021 before BIP103 reaches 2 MB. BIP103 *is* still at 1 MB.
@_date: 2018-08-13 10:44:57
Bitcoin's blocks and PoW is the mechanism which ensures that all nodes have the same DAG.
I'm not suggesting adding anything new to Bitcoin. This DAG already exists. Block explorers are tools built to help people explore the DAG. Most Bitcoin daemons don't bother to keep the DAG explicitly encoded in memory, but all of the information is built into the blockchain already.
What my article says is that when you think of the transaction set mathematically as a DAG, you get to prove that valid topological orderings exist without needing to compute those orderings. This saves us work.
Anyway, if you have more specific questions, I'd be happy to answer as best as I can. I do like trying to explain concepts that I find cool, even if they are rather complicated sometimes. Well, usually because they are complicated, to be honest.
@_date: 2014-06-19 10:49:12
Yes, we will power your machines.
@_date: 2014-01-13 19:03:09
I have received the same email. 
I wrote a reply to compliance in response to that email, and their reply is on the right. The emails I've received were signed "Julie" and "Robert" for the 1st (Dec 24th) and 2nd (Dec 27th), respectively. So, not a single overzealous compliance agent; there are at least two.
@_date: 2018-08-11 22:18:31
A year ago, miners were making $1.30/kWh on typical expenses of $0.06/kWh. Now that the price has fallen and the hashrate has increased, we're only making about $0.10/kWh on expenses of $0.06/kWh.
It has nothing to do with speculation. It has nothing to do with squeezing other miners out of the game. The only reason that hashrate is increasing is because it's still profitable to do so.
Changes in hashrate are limited by the production capacity for new miners and the development speed of new datacenter projects. Hashrate increases lag behind price increases by 3 to 24 months. The hashrate increase that we're getting right now is still being driven by the ramp up in BTC price from $700 in Jan 2017. The price has increased 10x since then. It takes quite a while to engineer and build out facilities to provide a 10x increase in mining.
I am an industrial miner. In Jan 2017, our mining activity was 0.7 MW. At that time, we had been working on an expansion project for about 12 months which we were working on as hard as we could. That came online shortly afterwards. By the end of 2017, we had 2.0 MW of miners running. We immediately started development of another project to add another 8 MW of capacity, but that project has been stalled as our utility company is unable to keep up with the load growth. Consequently, we probably will be unable to increase our capacity any more for at least 18 more months. This is how it goes in industry: changes in market conditions result in delayed responses in deployments.
@_date: 2018-05-04 04:49:59
That transformer only supports 100W. The S9 requires about 1500W.
@_date: 2014-06-19 10:32:01
Hi, Jonathan Toomim, ASICSPACE datacenter architect and investor here.
Sort-of. We have a small amount of capacity online already in the Portland area. It's sold out. We're building much larger facilities (0.5 MW to start, to be scaled to multiple MW) in the central WA area, where hyrdoelectric generation drives electricity prices near zero. The first capacity at the first of those facilities should come online at the beginning of August, in time for the August SP30 batch. That capacity is already all sold to our anchor tenant. Subsequent capacity should start coming online around September. We'll start accepting people's money around then.
@_date: 2018-08-12 12:35:36


No, they don't. I've lived in China for a total of 6 months. During one of those months I worked next door to BW.com. I spent another 1.5 months in Huaqiangbei, Shenzhen, the biggest wholesale electronics market in the world. I've lived in the USA for longer than that. 
Shipping costs are different, new products are usually announced first in Chinese, and very rarely a model is made which is only available in China (e.g. Antminer S5+), but generally speaking, the devices that make up the bulk of mining in China are the same devices sold for the same prices as you'd get if you're in the USA.


Um, you can just go onto shop.bitmain.com and buy a thousand machines right now if you have the cash.
@_date: 2014-06-19 12:21:30
Cheaper rates, shorter terms, better design, different name, faster growth. Is there anything else you might want?
There's room for both us and Hashplex in the market. Even with both us and Hashplex growing as fast as we can, I doubt we'll be able to host all the demand people will want to throw at us.
Time will tell if our plans become mirrored in reality.
@_date: 2014-06-19 10:20:15
Hi, Jonathan Toomim, ASICSPACE datacenter architect/engineer and investor here.
We have insurance against theft and fire. If our building burns down, you get paid for your loss. We'll have physical security too, though.
Yes, no photos. Sorry. I wish I could share them with you, because we've got some neat tricks up our sleeve, but we can't share them with you, because we've got some neat tricks up our sleeve. 
Also, it's not built yet, so there's really not that much to look at. So I'd be like, "Look, it's our datacenter!" and you'd be like, "But it's just an empty building!" and I'd be like, "On the contrary, it's chock full of potential!" and you'd be like, "Pfft!" and I'd be like, "Get off my lawn!" So I think it's better with no photos.
The PUDs in central WA provide fiber optic internet to basically every building in the county. Bitcoin mining's bandwidth requirements are ridiculously low. We'll get a couple of redundant internet feeds of different methods and be done with it. This isn't web hosting; we don't need gigabits of bandwidth. We could easily get that if we had a non-bitcoin customer who wanted it, though.
As for cooling, we've designed our cooling system to be between "sufficient" and "excessive" for 1 MW of machines on a day in the 99.9th percentile of hotness and humidness. A day muggy enough that our cooling system might be insufficient should come about once every 10 years, according to historical data. Global warming might make those days more common, so that perhaps a few years from now, you might have one day per year during which your hashing performance could fall 10% due to thermal throttling.
Anyway, you don't have to trust us if you don't want to. You can keep your miners on a rack in your friend's apartment with a powder extinguisher next to it if you prefer. We have a few customers lined up already, so a few months after launch you should have a few well-known customers (e.g. on bitcointalk) who can vouch for us.
By the way, backup generators and UPSes are a really bad idea for bitcoin mining. You will never ROI on the price tag of the UPS or the generator only looking at mining during the times when primary power is off. Generators and UPSs are for webservers and stuff like that, where downtime costs thousands of times as much as the hosting fees themselves. This is especially true for UPS, since those machines typically cost about $20,000 for 20 minutes of 200 kW capacity. If you use that 6 times a year for a full 20 minutes per blackout with SP30s at current difficulty, that's 2 hours of hashing per year at around 400 TH/s, or about $700 per year you get out of a $20,000 investment. 
@_date: 2014-08-12 13:00:51
It's Bitfury. They open a large datacenter roughly every two months. This one is in Iceland.
Quoth Guy Corem of Spondoolies Tech on bitcointalk:


What we see now is what George Kikvadze calls "Summer Surprise". It's their Icelandic DC.
I believe we can expect a continues cadence of 20 PH/s from BitFury every 2 months. 
@_date: 2014-06-19 09:53:01
Hi, Jonathan Toomim, ASICSPACE datacenter architect and investor here.
We are a hosting provider, not a mine. We don't own the miners, we just own the datacenter infrastructure. Our goal is to make it feasible for people to make small-scale investments in bitcoin mining while still getting the datacenter economies of scale possible with a good purpose-built bitcoin mining datacenter.
There are two types of centralization threats. 
One threat is centralization of pools, like with ghash.io. In this case, if signs of abuse are evident, it's feasible for miners to simply switch pools or start solo mining, and fix the problem. They'd probably be a little slow on the uptake, so they might not all jump ship from an abusive pool as soon as the abuse happens, but there's at least one check there.
The more serious threat is the centralization of ownership. If one entity owns the majority of the hashrate, then they can keep their hashrate on any number of pools as long as they want, then suddenly switch over to their own attack pool the moment they want to double-spend something. The best way to protect against this threat is to keep the small- and medium-size miners in the game. That's what we're trying to do.
Because we don't own the equipment, we can't choose for you what pool to use. We can't force you to use p2pool or eligius or ghash.io or anything else. That's the whole point.
@_date: 2014-06-14 20:57:31
Coinbase will buy it.
@_date: 2014-01-13 19:10:27
They haven't yet retracted the ones they sent to me. I'll post here if they do.
@_date: 2014-06-20 00:30:36
A converted storage unit with raised, perforated floors? That would be a strange thing to find.
It used to be a lumber processing plant.
@_date: 2014-06-19 12:19:17
Oh, it occurs to me that you can see photos of our old datacenter near Portland on our other website:
It's nothing special, and it won't be anything like what we're building in WA, but it's something. You can also visit it if you want, but we'll be shutting it down within a month or two, once we have WA online.
@_date: 2014-06-19 10:37:19
Hi, Jonathan Toomim, ASICSPACE datacenter architect and investor here.


Much like freedom, the price of security is eternal vigilance. The price of eternal vigilance is the salary of a security specialist. Those people aren't inexpensive.
We don't want people SSHing into their miners and then scanning our LAN for other people's miners that are still configured to use the default passwords. Yes, there are ways to separate the miners to prevent such attacks from happening, but those methods have to be implemented by humans, and humans make mistakes.
We would like to be able to give every person unrestricted and secure access to their own miner, but until we have finished building something to do that, we aren't going to promise it to anyone.


Not going to happen. If that's your position, then move along; there's (literally) nothing for you to see here.
@_date: 2014-06-19 10:46:06
Specifically, hydro. There is a small portion of our power that comes from wind power, because the Washington legislature decided to mandate that each utility in the state add a certain amount of wind or solar power to their system, and the laws were worded in such a way that it applies even to counties and utilities that were 100% hydroelectric.
You can move electricity long distances if you're willing to accept some energy loss, but it's much easier, cheaper, and more efficient to just move the load to be near the generator. That's what we're doing. There are about five large hydroelectric dams on the Columbia River within 100 miles of our facility, each of which produces more electricity than the entire bitcoin network uses. They've got a dam on the Wenatchee river (a tributary) that used to have 20 MW installed on it. However, after the dams on the Columbia were finished, they removed the generators to reduce maintenance costs because they now had way more power than they could sell.
So it's hydroelectric power generated from water that would have gone over the spillways and not through the generators if we didn't use the power. I think that's about as green as electricity can get.
@_date: 2014-01-13 22:26:57
PayPal just restored my account access. I think this is a direct result of me posting on David Marcus's Twitter feed.
@_date: 2017-02-26 21:44:09
Here is a description of the standard CreateNewBlock() algorithm.
First is a step called transaction packaging. For transactions that depend on other unconfirmed transactions, you group them together into a package of several transactions so they all confirm together. This step isn't super-important, as most transactions have only confirmed inputs, so I'm going to gloss over it.
Second, you take all of the transactions (or packages), and sort them by (fee/bytes).
Third, you go through the sorted list, and add transactions (or packages) to your block template until you run out of room.


600 seconds on average.
@_date: 2017-02-12 09:27:27


Breaking even is hard because if it were easy, everyone would be doing it. If everyone does it, it gets hard. There are no free lunches to be had with mining. Breaking even will always be hard.
However, if you don't have cheap electricity, and you're competing with people who do, breaking even won't be hard; it will be impossible.


Being able to mine profitably with your own home computer is a nice dream. Unfortunately, it is not possible to realize it in a sustainable fashion with a proof-of-work system. PoW burns capital and electricity to make money, and industrialized installations both use less capital (e.g. from economies of scale in the deployment of cooling and electrical infrastructure) and cost less in electricity (from careful location choice). Getting rid of ASICs and going back to GPUs or CPUs won't fix the problem. Switching to Proof-of-Stake will, though.


It is impossible to make industrialized mining impossible. Any computation you can do in your home can also be done more efficiently in a datacenter. 
@_date: 2015-05-16 20:46:09


Yes, but not on rivers in the mountains. In order for hydro to work well, you need a river flowing down a fairly steep grade with steep banks made of bedrock. Cities tend to be located on rivers in plains regions, where there's plenty of good farmland around, and where the river is navigable without having to deal with rapids. In those places, you get a lot of deposition of sediment, which makes for great fertile topsoil, but it doesn't make it easy to build a dam.
@_date: 2017-02-14 18:56:22
This thread and video explain it pretty well:
@_date: 2017-02-04 23:39:36
A lot of the animosity towards Coinbase on this forum is politically motivated. Coinbase came out in favor of hard forking to increase the blocksize about a year ago, and the general sentiment on this forum towards them shifted at that time.
@_date: 2017-02-07 00:25:19
These Monday sweeps have been going on for quite a while -- [two months]( at the minimum. It's likely that someone wrote code to clean up their UXTOs on their exchange a year ago or earlier, back when blocks weren't chronically full, and didn't think about how much it would cost them in fees when blocks filled up.
@_date: 2017-02-28 02:24:39
So long as everybody else cleans up the mess they leave, everything will be mostly okay. However, it would be better if Bitfury didn't leave a mess for everybody else. Although Bitfury's new algorithm sounds like it might be an improvement because it gets more transactions into each block, it does so by making the blocks that clean up the mess get fewer transactions into each block. Bitfury isn't helping with this.
@_date: 2017-02-01 22:57:32
My attitude at least is the same both times.
@_date: 2017-02-13 18:38:00


They were [not okay with BIP101]( because BIP101 scales to 8 GiB, not because it starts at 8 MB. I heard from a couple of people (Samson Mow of BTCC and Wang Chun of F2pool) that when they signed the open letter in support of 8 MB blocks, they meant that they supported 8 MB maximum, and when Gavin came forward with BIP101 (starting at 8 MB and doubling every 2 years until 8 GiB), they felt like he had betrayed them and were very angry with the way he disregarded their letter. (To me, it seemed like a translation issue -- when I read [the English version]( of the 8 MB letter, I thought they were asking for the limit to be moved to 8 MB immediately, not that they were asking for a limit not to exceed 8 MB.) 
During my census, I didn't ask specifically about something that started at 8 MB, but judging from what I heard, I think many of them would have been okay with it (though it wouldn't be their preferred option). KNC would definitely have been game for it. KNC was actually fine with BIP101.
@_date: 2017-02-12 23:35:08
The problem is that your disk is slow and you don't have enough RAM allocated for caching the UTXO database. If you have enough cache, it should finish syncing in around 12 hours. Try running with `bitcoind -dbcache 4096` to use up to 4 GiB of RAM. There may also be an option in the GUI for this, but I haven't used the GUI for ages so I can't say for sure.
It's unfortunate that you need to do this manually. The developers really should autodetect the amount of available RAM or at least use a more sane default value during syncing.
@_date: 2017-02-26 22:19:55


Yes, entirely.


Yes, that's how long it takes now, and that's how long it will always take. Difficulty increases if blocks are mined faster than an average of 600 seconds each. Difficulty falls if blocks are mined slower than an average 600 seconds each. Every 2016 blocks, the difficulty is adjusted in order to make the average block time as close to 600 seconds as possible.
Historically, we've seen the difficulty increase over time because miners have kept adding hashrate. However, if miners don't add hashrate, the difficulty doesn't increase as more blocks are mined.
@_date: 2017-02-07 19:30:06


I wish this is what you had written.
@_date: 2017-02-07 01:55:19
If you follow the inputs for a few steps you eventually come to this address:
That address has received a total of 738,191 BTC to date, and started engaging in fan-in fan-out behavior in February of 2016. Someone on bitcointalk [noted]( that xmine.org, a cloud mining ponzi scam, moved their money through that address, and thinks it belongs to an exchange or mixer.


Fan-in fan-out can be a useful pattern if you receive money from a large number of people and also have to send money to a large number of people, as exchanges and mixers do. 
For the fan-out, 1-input 10-output transactions are much more efficient than ten separate 1-in, 2-output transactions. A 1-in-10-out tx will take around 440 bytes, whereas ten 1-in-2-out transactions will take about 2,580 bytes. (Each input uses 180 bytes, compared to 34 bytes per output, so having a single input for ten outputs saves a ton of space.) In that 10-out transaction, you might have 9 outputs for customers with typical values around 0.01 to 10 BTC each and 1 output for the remainder (to be used in later fan-out transactions). 
@_date: 2017-02-06 02:24:21
100 connections is an excessive number. This node has far more connections than the average full node. A more reasonable number is 10 connections. Given the unreasonably high connection count, this data should not be taken as representative of the requirements for a normal full node.
High connection counts are wasteful. Most of the network traffic for a full node is in sending and receiving INV messages. INVs are notifications that a node has a new object (a transaction or block) in their inventory. INVs need to be sent once per peer per transaction, and INVs use about 100 bytes each after TCP/IP and Ethernet overhead is added in. If you have 100 peers, that means 10 kB per transaction of overhead. If you only have 10 peers, then you only have 1 kB per transaction of overhead. That is better.
If you look at the rx/tx ratio for this node, you can see that he is receiving about 2 GiB/day and sending about 40 GiB/day. He is transmitting 20x as much data as he is receiving. In a balanced p2p world, each node would send as much data as he is receiving. Given the presence of SPV wallets and leeches, a true full node is expected to upload a bit (maybe 2x) more than it downloads. The OP is way beyond that.
@_date: 2015-05-13 02:17:44
There is, but the amount of energy that you can harness is severely limited due to thermodynamics.
Basically what you would be building is a heat engine. In a heat engine, heat flows from a high temperature to a low temperature, and in the process, some of the energy is used to do work. This is how steam turbines work, or gasoline engines. Thermoelectric generators are also basically heat engines.
In a heat engine, the amount of work you can extract from a given heat flow is limited by the temperatures used. The theoretical limit for efficiency for a heat engine (the Carnot efficiency) is equal to (1 - T_L / T_H), where T_L and T_H are the cold and hot temperatures in Kelvin.
For a silicon chip, you might have silicon die temperatures of up to 125Â°C. The chip package would have a lower temperature, probably no greater than 100Â°C. If the cold temperature were 25Â°C, that would mean your efficiency limit would be (1 - 298K/373K) = 20%. Current typical thermoelectric generators are actually only a fraction of that, with 5-8% being common (though probably for higher temperatures). Of course, this power wouldn't be at a usable voltage, so you'd have to add at least one DC2DC converter stage (typical efficiency: 80%) before using the power. This means that adding a thermoelectric generator to a device would be able to save 4% to 7% of your electricity bill at best. 
Thermoelectric generators are bulky and expensive. It's very unlikely to be able to deploy them for power regeneration in such a way that they pay for themselves before they break.
@_date: 2015-05-21 00:43:55
They have an ulterior motive. I posted something relevant on bitcointalk earlier, so I'm just going to copy-paste it here:










@_date: 2017-02-13 18:22:42


[April 2016]( if I remember correctly.
@_date: 2017-02-13 09:42:57
With 4 GB of RAM dedicated to the task, should be able to cache nearly the full UTXO database, which means very few disk accesses will be needed. His sync will probably finish a few hours from now.
@_date: 2017-02-01 22:52:20
My main criticism, from **[3 weeks ago](






@_date: 2017-02-26 22:34:53
Soft forks need to be enforced by miners. A majority of miners needs to enforce the soft fork rule, or else bad things happen.
SegWit repurposes an opcode that previously meant "This transaction is valid no matter what" to mean "This transaction is valid only if accompanied by a valid SegWit signature." Any transaction containing this opcode will be considered valid by non-SegWit nodes, but will only be considered valid by SegWit nodes if properly signed. This means that if you send money into a SegWit address, and most miners and nodes are not running SegWit, then I can spend your money whenever or however I want.
If someone creates an improperly signed SegWit transaction, then miners who enforce SegWit will refuse to follow any block that includes that transaction, and will build their own chain without that block or that transaction. If the SegWit miners are in the majority, then this SegWit chain will eventually overtake the non-SegWit chain. Non-SegWit miners will see the SegWit chain as being both valid and longer than the non-SegWit chain, so they'll follow the hashrate majority. However, if the SegWit miners are in the ~~majority~~minority, then they will never overtake the non-SegWit majority, and there will be two separate chains forever. SegWit and non-SegWit nodes will be unable to agree on the chain state, and Bitcoin will be split. Furthermore, on the longest chain, any attempts to use SegWit addresses would result in anybody being able to spend your money.
Right now, SegWit-supporting miners are a minority, so the result would be a chain split with completely unsecured SegWit addresses on the longest chain. Most people consider this a bad outcome.
@_date: 2017-02-14 18:50:54


A full node is not required to use LN, even without SegWit. You do need to watch the multisig address on the blockchain to check for transaction malleability attacks, but an SPV wallet logging in once a day is sufficient for that.
However, there are several other parts of LN become a lot more complicated without a transaction malleability fix (a la SegWit), so even though it's possible to do it, it's hard enough to make it work that it might not ever get implemented.
@_date: 2015-05-16 19:50:27
Really good renewable power is usually geographically constrained, and it's rarely located where population centers are. Instead of moving the power to the load (which is expensive and lossy), Bitcoin mining gives us the opportunity to move the load to where the power is generated. Places like Iceland (geothermal), Washington (Columbia river hydro), northern Sweden (more hydro), and Labrador, Canada (also hydro) have rich power generation capacity, but they're too far north for a lot of people to want to live there.
You might want to check out my company, the [Toomim Bros Bitcoin Mining Concern]( We're a hosting service located in central Washington State, USA. Our electrical company owns two large dams, and generates way more hydro power than they have demand for, so they sell us their power at $0.025/kWh. Of course, nobody really wants to live in central Washington. We moved out there so you don't have to.
We've built a facility designed for hosting bitcoin miners at a fraction of the price of traditional datacenters. There's more power available to us, too, once we get the capital together to take advantage of it.
@_date: 2017-02-05 20:48:27


Actually, it's not to hard to prevent it from happening. All you need to do is include some coins mined after the fork, or some coins that were mixed therewith. Someone could even build a web service like Shapeshift that did this mixing and/or splitting for you.
@_date: 2016-01-01 14:43:56
An effective *capacity increase*. 
I think that in the interest of not confusing people, we should try to avoid using words in a fashion that is ambiguous. "Blocksize increase" is usually used to refer to an increase in the MAX_BLOCK_SIZE constant, which SegWit is definitely not. It may also be construed as a way of describing the sum of the header size, the transaction merkle tree size, and the merged-mined witness data merkle tree size, or to describe the transaction capacity, as you seem to be doing; however, it can be argued either way whether those uses are covered by the term, and I think that doing so is counterintuitive. In the interest of not confusing people, I think it is better to use words in which nobody gets confused. 
In particular, I think it was clear given the context that meant a MAX_BLOCK_SIZE increase, not an "effective blocksize increase." This is probably why he said "blocksize increase" and not "effective blocksize increase." It is also desirable to respond to the terms and the meaning that people actually use rather than the terms and the meaning that you think they should be using.
This is the **[second instance]( in 24 hours in which I have seen you use words in a counterintuitive way. In both instances, it seems as though these uses of terminology may have been intentional and ideologically motivated. If that is the case, please stop. Let's try to come together and try to speak the same language, otherwise we will never be able to achieve consensus.
@_date: 2017-02-03 16:39:02
It looks like you're spending mining earnings. 
Your transaction has 25 inputs of around 3 mBTC each all from two addresses (your mining address plus your change address). At current fee market rates, it costs about 0.15 to 0.3 mBTC to spend each input, which means that you're going to end up spending 5-10% of each input on fees if each input is only 3 mBTC. 
You should configure your pool to use a higher payment threshold, like 10 mBTC or higher. This will make you get payments less frequently (about once every 10 days instead of once every three days), but you will spend a lot less on fees when it comes time to actually spend your earnings.
@_date: 2017-02-07 17:42:01


No, I didn't. I suggested that a ponzi cloud mining scam sent their funds to this entity, because I suggested that this entity is either an exchange or a mixer.


It's not wasted. If you have a fragmented wallet with a lot of UTXOs, it will cost a lot of fees to spend your money. If you defragment it into a small number of UTXOs in advance of spending the money, you pay almost nothing extra versus simply spending them money straight from the fragmented wallet.
The only waste appears to be with poor fee selection and poor timing, but not with the transaction pattern itself (as far as I can tell).


And yet it looks like they've been doing these transactions since Feb 2016.
@_date: 2017-02-13 09:40:06


Only in the worst case, not the typical case, and only versus *transaction* size, not versus block size. 
1. An 8 MB block with normal transactions takes 8x as long as a 1 MB block with normal transactions. 
2. An 8 MB block with eight transactions of 1 MB each takes 8x as long as a 1 MB block with one transaction of 1 MB. 
3. An 8 MB block with a single 8 MB transaction takes 64x as long as a 1 MB block with a single 1 MB transaction.
4. An 8 MB block with 80 transactions of 100 kB each will verify slightly faster than a 1 MB block with a single 1 MB transaction.
In practice, transactions are limited to 100 kB in size each unless you're a miner, which makes this quadratic issue only relevant in the context of malicious miners.


No, quadratic is not exponential. There is a big difference between x^2 (quadratic) and 2^x (exponential).
@_date: 2017-02-01 23:05:27
Can you provide evidence for the claim that they're running Core, and not Unlimited?
My understanding of the way pools work is this: you've got the full node process (Core or Unlimited) serving RPC getblocktemplate requests to the poolserver process, and the poolserver process serves stratum requests to mining hardware. The poolserver also acts as a stratum client to other poolservers (usually run by competitors), and when the poolserver notices that someone else is mining on a block that the local bitcoind doesn't yet have, the poolserver will construct its own empty block based on the header they received as a stratum client from a competitor. This is called spy mining.
Based on this model of pool operations, they would normally get the header and version bits from their own local bitcoind (e.g. Bitcoin Unlimited for BTC.top), but when they're doing spy mining, they'd get the header and version bits from a competitor. If they didn't write code specifically to change the versions of headers grabbed from competitors, it would look like they're running their competitor's bitcoind somewhere, when in reality, they're just stealing headers from their competitors and not covering their tracks properly.
@_date: 2016-01-21 09:48:22
Changing the PoW function is pretty crazy. I was disappointed that Guy was taking these threats seriously.


so do not worry, i have no intention of merging such a thing
Matt Corallo and I touched on the topic later:


Luke may create his Keccak hash function fork; he already has the code. But it's not going to be called Core.
@_date: 2016-01-18 02:32:42
This is called First-Seen-Safe RBF, or FSS-RBF. It's a decent idea, but it's complicated to implement correctly. Opt-in RBF was seen as an easier alternative to implement.
Child-pays-for-parent was another proposal to do the same thing from the other side (recipient spends the unconfirmed transaction with a large fee to make it clear faster), but that too was stalled by implementation complexity. Luke-jr actually implemented that, but his implementation did not make it through code review. It may have not been performant enough.
@_date: 2017-02-12 23:42:33
Internet connection speed is irrelevant. He doesn't have an SSD and he isn't using a high enough dbcache value. The default dbcache value is not reasonable.
(you had the same comment.)
@_date: 2016-03-25 04:00:44


I expect the per-block sighash limit will be replaced with a unified validation cost function long before we get to 11 MB. However, if that's not the case, I would recommend either (a) increasing the block sighash limit or (b) setting a limit in IsStandard on the sighash per transaction like this:
    tx_sighash_limit = block_sighash_limit * tx.size / block_size_limit
I prefer (b), but if someone can come up with a compelling use-case for large SIGHASH_ALL transactions, then (a) would also be acceptable to me.
@_date: 2017-02-12 02:06:52
There are proof-of-stakeâbased voting systems that are also manipulation-proof.  is close to achieving that, but it doesn't show the signatures for auditors to verify and it doesn't have any mechanisms to disprove vote quashing. A better system would use a blockchain to record votes.
@_date: 2017-02-07 00:03:51
It appears that there is a large entity (possibly an exchange) that collects their UTXOs into a few single large UTXOs every Monday. We had a similar discussion about this **[one week ago]( and there will probably be another discussion next week too.
@_date: 2017-02-27 00:35:48
Looking at block ** it appears that they are building blocks with the following properties:
1. The vast majority of the transactions  (3250 of 3316) have 1 input and 2 outputs, and pay around 65 Sat/byte in fees.
2. A handful of transactions are very large and comprised mostly of inputs. The largest one takes 98 kB and only pays 10 Sat/byte. The cheapest one is 2.4 kB and pays 1.25 Sat/byte. These transactions appear to use around 25% of the total block space.
3. The block contains 4986 inputs and 6577 outputs, adding 1591 entries to the UTXO set.
4. The average transaction size is 293 bytes.
The transactions in 1. and 2. above are very different from each other, so it is unlikely that the two groups were selected by the same algorithm. Instead, it appears that BitFury is using a reserved-space algorithm for assigning block space, with 25% allocated for UTXO-reducing transactions and 75% allocated for small UTXO-increasing transactions.
In total, the 1-input 2-output transactions add +3250 items to the UTXO set. The UTXO sweep transactions only remove -1659 transactions from the UTXO set, leaving a net increase of +1591 UTXOs. This is undesirable, as UTXOs are unprunable and slow down block verification for all time. 
Typical blocks change the UTXO set size by anywhere from -2000 to +2000, and average around +200 or so. At first glance, it might seem that block  UTXO change of +1591 was within typical variation; however, judging from the algorithm Bitfury appears to be using, we can discern that large UTXO surplus will be typical for their new algorithm. Since inputs are about 5x larger than outputs (~160 bytes vs 34 bytes), the 25%/75% size ratio that BitFury seems to be using will always produce blocks with large UTXO surpluses. On average, an idealized version of this strategy will produce about 34% more outputs than inputs.
Selectively confirming transactions that include 1 input and 2 outputs is **not sustainable**. Imagine that each time you bought something in a store with cash, you could only pay them with one coin or bill, though you would still get change normally. At first it would be okay. If you start with a $100 bill, you could buy a jacket for $40, and get 3x$20 as change. Then you might buy a burger (-$5: 1x$20 -&gt; 3x$5) and a t-shirt (-$15: 1x$20 -&gt; 1x$5). You now have $40 left in your wallet (1x$20, 4x$5). You go to the store to buy groceries, and the total comes to $35. Unfortunately, you can't get your transaction to confirm because it requires 4 bills to get to $35 with your wallet, and Bitfury only allocated 25% of their block space to transactions like the one you now have to make. **BitFury's experimental block creation strategy makes change eventually unspendable.**
Block ** is the second block that Alex Petrov mentioned, and looks pretty similar.
@_date: 2017-02-14 03:10:34


"Sophie, I heard you're pregnant! When is the release estimate?"
@_date: 2016-03-01 07:34:11


This has not been possible with any of the serious big-block proposals. BIP101 and BIP109 both limit the amount of data hashed per block to 1.3 GB, which takes about 10 seconds to validate on a typical CPU. This is unlike SegWit or the current consensus rules, which both have a worst-case with 19.1 GB of data hashing, or about 2 minutes 30 seconds of validation time.
@_date: 2017-02-12 02:33:07


ASICs are not the reason why there's mining centralization. Economies of scale and non-uniform pricing of electricity are the reasons.
I pay 2.7Â¢/kWh for electricity. As long as we're using a proof-of-work consensus system, only about 5% of the world will have access to competitive electricity rates (â¤ 5Â¢/kWh). It doesn't matter if we're using GPUs, CPUs, or ASICs: people like me who have industrialized mining operations will outcompete hobbyists.


ASICs that are specific to that cryptocurrency are significantly more secure than general-purpose devices, especially CPUs. With algorithms for which general purpose devices are competitive, we get problems with botnets. Botnet operators have perverse incentives. Botnet operators don't pay electricity fees, so they're willing to waste more value in electricity than they earn in Bitcoin. Also, they are not usually invested in the long-term viability of the currency, so they are more likely to short the currency then use their hashpower to attack the network.
@_date: 2017-02-02 17:26:27
Spy/headers-only mining doesn't find blocks as commonly as it used to, since the improvements in block relay from 2016 (xthin, compact blocks, Falcon, FIBRE) significantly reduce the transfer time for full blocks. It's quite possible that this was BTC.top's first block ever mined with spy mining.
Generally, each pool writes (or adapts) their own poolserver code, so this bug of failing to change the version bits on spy-mined blocks likely wasn't in the code for other pools.
@_date: 2017-02-12 23:41:22


 
The existing proposals to reduce the size of transactions or the number of transactions that need to be done on the blockchain are all very complex. People have been working on implementing them for years, and none of them are finished.
@_date: 2016-03-05 09:45:35


 
Mostly, because I don't like the discounting function. I don't like the 4x adversarial case, and don't like how it disregards the fact that the actual bottleneck for Bitcoin's scalability, block propagation, doesn't care if a byte is for the base or the witness.
@_date: 2017-02-26 22:36:05
That is [forwards compatibility](
@_date: 2017-02-07 09:04:54


Yes. Demand for block space is relatively inelastic -- people rarely decide not to send transactions because fees are too high -- and supply is completely fixed, so the consequence is that relatively small fluctuations in block space demand cause large fluctuations in transaction fees. On a supply/demand graph, this would mean that the demand curve has a steep slope, and the supply curve is completely vertical.
@_date: 2017-02-07 18:32:34


Yes, that makes the mixing service hypothesis more likely. Mixers recirculate the majority of their holdings, and the fan-in step is crucial to their privacy goals.
@_date: 2016-01-25 13:49:46
Mike Toomim has nearly nothing to do with Bitcoin Classic. He just helped build consider.it.
@_date: 2016-01-01 17:17:38
On the other hand, when this was tried against Coinbase, Coinbase shrugged it off pretty easily. Anti-DDoS technology for websites is pretty mature and robust, and exchanges have a lot more money than mining pools. BTCC is mostly an exchange, like Coinbase. I don't think they're going to have much trouble.
@_date: 2016-03-01 02:17:40
Oh, excellent, they updated the graph title. (I asked a blockchain.info employee about that graph a few hours ago, since the old title was not very descriptive.)
Now that we know it's the **median** confirmation time, it's clear why it's not showing much delay. The median confirmation time will only show delays if more than 50% of transactions are being delayed. If we had 51% of transactions making it into the first block, and the other 49% of transactions took 100 hours to confirm, the graph would still show confirmation times around 10 minutes.
Since the only transactions that get delayed are the ~10% with the smallest fees, and those transactions tend to get delayed by hours, this graph will not reflect the delays we are seeing.
An open question: does this graph include transactions that never confirm (e.g. spam from October)?
@_date: 2016-03-06 21:34:09
Nighttime on a weekend.
@_date: 2016-03-01 08:22:33
I would like to see more technical description of this alleged spam attack. Does anyone know of an analysis?
@_date: 2016-01-08 23:26:24
User support for the different proposals can be seen here: 
Miner support can be seen here: 
I'm working on a project that I hope will be acceptable to both.
@_date: 2016-03-11 08:52:00
Adam Back mentioned to me the existence of code in an email on Dec 14th, but now that I'm actually looking for it, it seems that the only code that exists is the `uint32_t GetMaxBlockSize()` function in the BIP.
@_date: 2016-03-03 06:04:11
@_date: 2016-03-23 09:24:14


No, that's not true, and addressing that is the central point of the parent post. 
DDoS resistance is one of the reasons for the number of cloud nodes. Showing "support" for the fork is another one of the reasons. Helping propagate Classic blocks post-fork is another. I think that DDoS resistance is a valid reason for cloud nodes. I think that showing "support" is not.
After the DDoS attacks began, I **[encouraged]( people  to to help **[soak]( up the attack bandwidth by setting up Classic nodes at cloud services. I was not the only one to **[make]( such arguments.
@_date: 2016-01-01 18:25:36
Oh, he's on hackernews? I had no idea. 
I always have a j in my usernames. He generally does not have an m in his. That's the privilege of being the eldest brother.
@_date: 2016-03-05 01:26:26


It is only a 2D knapsack problem if you've disabled the IsStandard() check **and** are a miner who is actively trying to create attack blocks. It's not even theoretically possible to hit the bytes hashed limit with an unmodified bitcoind. The 1.2 GB limit is 10x to 100x higher than actual block hashing requirements.
If a transaction fails the IsStandard() check, that means it will be rejected if you try to send it over the p2p network. The only way nodes will accept a non-Standard transaction is if it's already in a block. All attack transactions fail the IsStandard() checks, since IsStandard() requires transactions to be less than 100 kB (among other things). Thus, in order to send out an attack transaction, you have to mine it yourself. 
People have been talking about the 2D knapsack problem like it's some insurmountable obstacle that prevents you from creating new block templates. That's not true. It's actually quite simple, easy, and fast to create a block template when you are facing a 2D knapsack problem. Bitcoind's CreateNewBlock function has been doing just that for years. The only difficulty is if you want your algorithm to create the *optimal* block template in all circumstances. Currently, the algorithm is this: you add the transaction with the greatest fee/kB as long as that transaction does not make you exceed the block size limit, the sigops limit or (for Classic) the bytes hashed limit. If it exceeds at least one limit, then you skip it and try the next transaction. This results in optimal behavior if the block size limit is the only limit that gets hit. If the limit that gets hit is sigops, and sigops/kB is similar among all transactions (which it is), then this algorithm produces nearly optimal behavior. 
In the case of sighash, where it is only possible to get close to the limit if you edit the source code and mine your own transactions, and where you have to try to make your transactions violate the sighash limits, this 2D knapsack problem only affects attackers. In that case, the 2D knapsack problem makes it so that a miner who performs an attack will usually be unable to simultaneously collect the optimal amount of fees in whatever remaining space there is in the block. That is, it makes attacks slightly more expensive. I am okay with this.
Miners who are not performing an attack are unaffected. Miners who do not edit the source code to disable the IsStandard() checks are unaffected (and also unable to perform the attack).


This is incorrect. If you have the bytes hashed limit, then the worst case scenario for hashing is *constant* versus block size. The 1.3 GB limit is high enough to not have to worry about the knapsack problem with IsStandard() transactions up to a blocksize of about 11 MB. After that point, we might have to either increase the limit or make the IsStandard() checks more strict in order to avoid the 2D knapsack problem. (I prefer the latter option.)
@_date: 2016-03-01 22:30:50
There are two types of computations that need to be done when verifying a block. The first is elliptic curve digital signing algorithm (ECDSA) verifications. The second is hashing parts of the transactions to provide digests that the ECDSA verifications use.
Normal transactions have the vast majority of their computational time taken up by ECDSA verifications. These are called "SigOps". A 1 MB block is permitted to have 20,000 (incorrectly counted) of these operations, and a 2 MB block with BIP109 is permitted to have 20,000 (correctly counted) of these operations. Typical 1 MB blocks have about **[5000]( sigops. A mid-range CPU can perform 5,000 or more verifications per second per core, so the worst case validation time due to sigops is around 4 seconds, and typical is 1 second or less. These sigops can be executed in parallel, and they also are usually executed when the transaction arrives in mempool rather than when the block arrives, so they are not usually a huge delay.
A normal block would only have a few megabytes of data that needs to be hashed, like maybe 5 MB. Since even a laptop CPU can hash at around 130 MB/s (very fast!), this hashing only takes a few milliseconds, and can be ignored. 
In the typical case, sigops comprise the vast majority of processing requirements, and bytes hashed can be ignored.
In the worst case, bytes hashed account for the vast majority of computational requirements, and sigops can be ignored.
There's a design flaw in the signature verification scheme. This flaw means that the amount of data that you need to hash in order to verify a transaction can be proportional to the **[square of the transaction size]( (or specifically, the number of inputs times the total transaction size, IFF the SIGHASH_ALL flag is used). Thus, a 200 byte transaction might require 200 bytes of hashing, but a 1,000,000 byte transaction that is 99% inputs can require up to 1,200,000,000 bytes of hashing. And there's a dirty trick you can pull (which I prefer not to repeat in public forums) that will increase the bytes hashed about 15x for a 1 MB transaction. Even though hashing is a fast operation on CPUs, when you have to hash 19 GB of data, it will take several minutes to complete. This means that large *transactions* can be dangerous. Large blocks can be dangerous if they allow these large transactions to be made without any checks on their verification complexity. 
BIP109 and BIP101 both place checks on a transaction's complexity. The worst transaction that a non-miner would be allowed to create in BIP109 would be 100 kB and would only require a few MB of hashing. It should not be possible to make a 2 MB block that even comes close to the 1.3 GB hashing limit with regular transactions that can go over the p2p protocol. If the attacker is a miner, on the other hand, they can create worse transactions, and the 1.3 GB limit comes to play. This limits the worst-case scenario to about 10 seconds. Note that block 364292 was this worst-case scenario, and Bitcoin survived.


No, the 1.3 GB limit is only hit in the case of attack transactions. Normal 2 MB blocks will be around 0.5% to 5% of this limit.
@_date: 2016-03-27 02:09:56


I think the argument can be made. The point of cloud nodes as I see it mostly is not to have a bunch of nodes on the network that won't go offline. Truth is, if the cloud nodes get DDoSed into oblivion, nobody really cares. The main point of the cheap cloud nodes is to provide enough targets for the DDoS attacks so that the attacks become less effective at knocking off the handful of economically relevant nodes. 
It's a lot easier to run a full node and weather a DDoS attack if you are in a herd of 2,000 nodes than if you're in a herd of 200 nodes. If the attacker divides his bandwidth evenly among the targets, then you'll get hit with 10x less bandwidth. If the target concentrates his bandwidth on a few nodes at a time, then the probability of getting hit is 10x lower.
However, I agree that it would be better if the nodes were more spread out among hosting providers.
@_date: 2016-03-05 03:34:14
4 MB is a little bit sketchy without the relay network. Might be okay, might not; depends on what you think "okay" is. I personally think that 4 MB would be okay, but there's too much personal interpretation in that statement for me to be able to say that 4 MB is "safe". "Safe" is pretty hard to define.
@_date: 2016-03-27 15:58:40


They could, but they didn't. That's why I said "the use of VPS nodes *has been helpful* as a defense against the DDoS attacks."
P.S.: It sounds like you wish the DDoS attacks were more successful. Is that the case?
@_date: 2016-01-03 14:17:03
That's interesting.
 already exists. What you are proposing is different from that, of course.
@_date: 2016-03-24 16:39:40


No, that is not a lie. I believe it to be a true statement. If it were an untrue statement, then that wouldn't make me a liar. It would just make me wrong.
I'm not wrong, though. The use of VPS nodes has been helpful as a defense against the DDoS attacks.


Yes, that's the point. When the DDoS attacks started on Feb 27th, there were about 1240 Classic nodes online. That number dropped to 990 nodes that day, indicating about 20% got knocked offline by the attacks. People responded by adding more high-bandwidth targets for the DDoSer, thereby reducing the amount of attack bandwidth per target. During subsequent attacks over the next month, far fewer nodes were knocked offline. It's hard to tell how many due to poor SNR on the graphs, but it looks to me like typically 100 lost out of 2000 nodes during the March attacks, and those only stayed offline for hours at a time instead of days.
Before the DDoS attacks started, the possibility of a DDoS attack was only in the backs of our minds. It's something we knew was fairly likely, due to the previous attacks on XT full nodes, but it wasn't a priority. After the attacks started, it was a certainty. In a sense, the DDoS attacks *caused* the increase in full node counts.


He could, but he didn't. DDoS protection is a cat-and-mouse game. We won that round. 
The next round might ignore the cloud nodes and focus on end-users. If that's the case, the VPS nodes can still be useful. Home users could configure their nodes to not accept incoming connections, and to only connect to other nodes reporting the Classic version string, or only to the cloud nodes, or only over Tor, or something like that, in order to avoid getting detected by the DDoSer's crawler. Or the Classic home nodes could simply report a Satoshi version string and operate in stealth mode.  There are a few other options too. Many of these options would reduce the connectivity and propagation performance of the network if too many nodes used them, but having a mesh of high-bandwidth VPS nodes immune to attacks and with a large number of incoming connection slots can keep the network relatively healthy even when the home nodes are being shy and hiding behind firewalls or funny connection rules.
@_date: 2016-01-13 23:19:29
 &gt;I came across [some promotional materials for a new upstart called "Bitcoin Classic"]( that is filled with lies.
It sounds like you want to blame Bitcoin Classic team members for inaccuracies present in news articles. That doesn't sound very fair.


We haven't yet decided on all of the parameters of the blocksize increase. All we've decided upon is that we're starting at 2 MB. I asked the website editors to change the language to make that clearer.


I hereby propose it as a hardfork for Bitcoin. (Oh, did you mean *formally* proposed in a BIP? We'll get to that soon, once we've finalized the forking mechanism and parameters of the increase. We're still working on those.)


We know Core is very likely to oppose these changes. That's why we started our own project. I intend to submit the hardfork changes to Core when we're ready so you guys can give it your official stamp of disapproval if you so choose.


Agreed. We devs just write code. Users and miners decide what to run. 
@_date: 2017-02-12 03:19:06
It's an industrial rate in central Washington State. 
@_date: 2016-03-23 09:38:04


I'm not sure which of the statements in my parent you're asserting is a lie, so I don't know how to defend it. I'll just have to guess. If I miss it, please clarify which of my statements you consider to be false so I can add supporting evidence.
After the DDoS attacks began, I **[encouraged]( **[people]( to to help soak up the bandwidth by setting up Classic nodes with open ports at datacenters. 
Another prominent user, hellobitcoinworld, made **[an appeal to respond to the DDoS attacks with more cloud nodes](  that reached the front page of I'm not saying that the only motivation of people who paid into cloud nodes was providing DDoS resistance (clearly it wasn't), but we do know that DDoS resistance was one of the motivations. We knew from the DDoS attacks on XT nodes (apparently conducted by the same DDoSer) that we should expect DDoS attacks, and with the DNS reflection/amplification attacks that we were seeing the only real defense against the attacks is redundancy and bandwidth. The cheapest way to provide those tasks is with lots of VPS nodes.
I have never encouraged the use of cloud nodes as a way of biasing node version counts, though I know that others have. I do not think that node counts are more than slightly important for deciding a hard fork, since node counts are easy and cheap to manipulate with half-nodes. (There is no way to test whether a node is a unique full node, and it is very inexpensive to set up enormous numbers of non-validating nodes that appear indistinguishable from full nodes.)


My company mined this one: 
@_date: 2016-03-01 01:58:10
Instantaneous network hashrate can not be directly measured. It can only be statistically estimated based on the network difficulty and the average time per block.
 has graphs of the time per block (second graph down).
@_date: 2016-03-24 20:11:40


That's your point, not mine. It's not fair to call me a liar because the point that I made isn't the point you want to make. 
I'm not even arguing against your point. I made no statements about the relative priority of the various motivations people have for running nodes in the cloud. I only note that among those motivations is DDoS protection, and that this motivation is valid, and that these nodes have been helpful in DDoS protection. 


There is no node-count election. The only vote that happens is in the block version numbers. 
Some people have been using node counts as a PR tactic to try to convince miners to switch to Classic. My opinion is that those people are misguided. Others have spun up Classic nodes out of a motivation to help the network (e.g. DDoS resistance). Both groups of people have helped the network.
@_date: 2016-01-21 09:37:01


Except that it won't be called Core. Most of the developers will want to stay with the economic and mining consensus, and so Core will eventually follow the 2 MB hard fork. I think there are probably only three or four who would seriously consider a hard fork as a response.
@_date: 2016-01-20 10:11:39
Actually, mostly Travis Kriplean. My brother Mike mostly worked on statebus, the backend web framework.
@_date: 2016-01-01 14:58:17


"Effective blocksize increase" isn't too bad. Presuming that "blocksize increase" is the same thing as "effective blocksize increase" is what I'm objecting to. Perhaps you were just misreading Bitcoinopoly, or perhaps you were just abruptly disagreeing with his terminology, I don't know.


I'm just following Greg Maxwell's terminology on that. I think he was being very careful in how he worded things in order to avoid ambiguity and confusion, and I appreciate his effort.


Interesting term. That's an improvement, thanks.


Sorry, it maybe comes from being American. Politics in the USA are full of calculated use of [language and framing in order to direct debates] ( I took enough classes as an undergrad to know how important these kinds of effects can be, and my cognitive science background makes it hard for me to not notice when these kinds of effects are occurring and potentially becoming significant. In this case it was probably unintentional.
@_date: 2017-02-07 00:28:03


~~The fees are probably random because the transactor is trying to produce a round-number output from a bunch of random mismatched inputs.~~
Edit: I misread the parent post.


Or that the code that made these transactions was written before transaction fees were significant, and that the author didn't think about how much money they would be wasting.
@_date: 2016-03-01 02:39:24
There are a lot of transactions with fees that will never confirm. For example, there were a lot of transactions published for the first time in October that had 15000 satoshi fees for 14780 bytes (+/- 30 bytes). These transactions were filtered out by miners as having too low of a fee/kB, so they will never confirm. However, they have been republished twice a day. **[This one]( was in one of my nodes today, for example, and I last restarted that node about a week ago. Blocktrail first remembers having seen it January 7th, although I expect it has been floating around for longer than that.
@_date: 2016-03-01 08:16:26
That's because it's the median, not the mean. If 49% of transactions are being delayed by 100 hours, and 51% of transactions make it into the next block with no delay, then the median shows no delay.
@_date: 2016-03-24 20:27:27
Tor is slower and has more overhead. It also takes more time to set it up. I expect it would be effective as DDoS protection, though.
I am not confident that the Bitcoin network would work with 1 MB blocks if more than about 40% of nodes used Tor.
With DDoS attacks, there are two opposing strategies that you can take: you can either hide from the attacks (e.g. Tor or stealth mode), or you can aim for strength in numbers. 
If you hide your node, then the DDoS burden that falls on the remaining nodes increases. This means that those nodes will be more likely to cave in and hide themselves. This can eventually snowball until only a handful of nodes are left. The cost of maintaining the DDoS at that point will be much less, making it feasible for a DDoSer to require that all nodes hide themselves or be DDoSed into oblivion. I find this scenario distasteful.
On the other hand, if you put everything in the open and simply scale up node count and performance, the cost of maintaining the DDoS attack at a scale large enough to make a dent becomes prohibitive. Bandwidth in major datacenters is extremely cheap, whereas DDoS attacks are relatively expensive (due to the risk of getting thrown in jail and the engineering required to get around the countermeasures put in place at the trunk level). In my opinion, this is the winning strategy.
@_date: 2017-02-12 02:08:03
Hashrate reflects investment dollars in mining. Clearly, that's not the same as dollars invested in the currency, but it's not a terrible approximation thereof.
@_date: 2016-01-09 17:08:06


I often hear people talking about "full node decentralization" as if it's a thing that matters. It's not. It doesn't matter how many honest full nodes a transaction block passes through on its way to you. It only takes one honest full node to enforce the rules of bitcoin. Since any full node that is dishonest or following different rules can send incorrect data or hide data from you, you need to connect directly to a full node that you can trust to give you data honestly and to follow the same rules you do. No other full nodes matter on the network. The only full node that you need is the one you trust.
The minimum safe number of honest full nodes on the network is one: the one you trust.
Practically, it can be better to have more than one full node that you connect to. That way you can request information from several nodes in parallel and check the results against each other. This means that instead of trusting one full node completely, you're trusting, say, eight full nodes to not all be part of a conspiracy to mislead you by omitting data, or part of a conspiracy including miners to mislead you by including invalid blocks.
If you don't trust strangers (like blockchain.info) to give you all of the data honestly, then you should run your own full node. If you don't, it may be possible to trick you into making you think you have not received payments that you have, or have received payments that you have not, at least for a few minutes. All large and high-volume businesses should run their own full node. If you're just an end-user, that doesn't really matter, since you're not likely to be receiving many payments from strangers who expect you to provide them with a good or service anonymously within a few minutes.


Most people are vehemently in support of a blocksize increase. Most people support very large blocksize increases like in BIP101. This can be seen in  
On  you can see people's reasoning. The top con for BIP101 is "Is too ambitious given the current state of block propagation and the difficulty of crossing the Great Firewall of China." It's only the third con that mentions full nodes: "Exponential increases in the block size makes it really hard for full nodes to catch up in terms of memory in future which leads toward centralization in Bitcoin." 
8 MB blocksize increases are currently vehemently opposed by miners. Miners (except BitFury) don't care about node decentralization and running full nodes at home. They care about block propagation. 
@_date: 2016-03-25 02:21:29


I was not aware that this was being done. Source?
Edit: by "host", do you mean "machine" (i.e. the IP meaning of the word "host") or do you mean "hosting provider"? I initially interpreted your message as meaning the former, but in retrospect it seems you meant the latter.
@_date: 2016-03-04 01:06:51


Except it doesn't fix the problem. The worst-case block with SegWit is the same as the worst-case block without SegWit: 19.1 GB hashed and verification time around 3 minutes. BIP109 makes the worst-case 15x better than SegWit.
SegWit creates a new transaction format that doesn't suffer from the O(n^2 ) bug, but it does nothing to address the bug in the current transaction format.
@_date: 2016-01-10 04:20:18
No, that's a bad idea. Google is absolutely awful at Chinese.
@_date: 2016-01-01 19:56:30
No, definitely not.
@_date: 2017-02-13 02:06:41


It's not possible to measure how many mining machines are on a given IP. It's not possible to measure how many calculations per second are being done by an IP. 
Here's a quick overview on how mining works: Each time you calculate a hash of a block template, it has a small probability (currently around 1/145000000000000000000) of being a valid hash. If it's not valid, you discard that invalid result, change the block template slightly by incrementing a counter inside the block and try again. If it is valid, you can send that new block to other nodes on the p2p network and they will recognize it as valid. Those p2p nodes will then forward it to the nodes they're connected to, and so on, until everybody has seen and accepted the new block.
There are two consequences of this mechanism that make it impossible to restrict the hashrate per IP address:
1. It is not possible to distinguish between someone who actually mined the block and someone who is just forwarding the block. All you know about someone who is trying to send you a copy of the block is that they heard about the block before you did.
2. It is not possible to know how many computations were done by the person who found the block. It could have been that person's first computation ever that just happened to be extremely lucky, or it could have been the result of a million ASICs running in parallel for a week. 
Using probability, you can make a guess about the amount of ha, but they're not very good or useful. A datacenter like mine uses about as much electricity as 500 houses, and we find a block a little more than once a month on average. If you watch 500 IP addresses (including ours) for a month (and if we decide never to change it), and if nobody else publishes a block during that time, then you won't be able to tell whether each of those 500 addresses has 1 mining computer, or if 499 have 0 mining computers and the last (us) has 500 mining computers. You also can't tell whether we have 500,000 mining computers, and were only using that IP address to publish blocks for a few hours before switching to another IP address.


It is not possible to identify the hashrate of a miner if the miner wants to keep it secret. (Miners can easily change their mining payout addresses for each block they mine. Most miners currently don't because it's convenient and they don't care about the privacy.)


Unfalsifiable hardware fingerprints are not possible.


Just because someone is an expert doesn't mean they can draw red lines with transparent ink. 
@_date: 2014-06-13 00:23:07
29,656 BTC is worth about $18m right now. Approximate trading volume spikes so far due to this news being released:
Bitfinex: $20m
Bitstamp: $17m
BTCe: $9m
Already, this news has caused about 3x as much money to change hands as will change hands during the actual sale.
(The CNY exchanges haven't shown much response yet.)
@_date: 2016-01-01 14:08:57


Many of the doomsday scenarios are adversarial. An intelligent adversary would (e.g.) detonate his EMP immediately after a halving.


It doesn't become more profitable until after the difficulty adjustment. Any hashing on the old (high) difficulty would effectively be a donation to the network. I think many miners would do this (miners are invested in Bitcoin as a long-term project), but I don't see how a purely selfish miner would want to hash at an unprofitably high difficulty.
@_date: 2016-01-03 13:26:13


Yes, I've seen that apparent opinion in the data. There are several users (e.g. jstolfi, Georg Engelmann, Steve) who support BIP101 and BU, and strongly oppose everything else, including 2-4-8. However, they also strongly oppose the status quo.
Mike and Travis and I have been discussing ways of being able to identify and visualize these types of voting patterns. We're working on it. In the mean time, you can mouse over each individual and get to learn each one's voting patterns. Travis will probably make a feature to explore a single individual's voting patterns in the next few weeks.
The best thing to do might be to talk to those people and request them to change their votes to make better use of the proportional non-binary voting system we've implemented. However, I am reluctant to do that as a moderator because I don't want to bias the results with things that are, like, just my opinion, man.
@_date: 2016-03-01 01:56:04
I think this is right. The "confirmation time" graph on blockchain.info might just be a graph of "time to go from N confirmations to N+1 confirmations" rather than "time to first confirmation". 
I have asked a blockchain.info employee about what this graph actually represents, and he said he would try to track down an answer.
**Edit**: the graph title has been updated. It's median confirmation time. That will be equivalent to block time until more than 50% of the transactions had to wait for more than 1 block.
 has some interesting stats on the percentage of transactions that are new vs. old. 
@_date: 2016-03-03 21:03:09
Gavin didn't test across the Great Firewall. I did. In my testing, large (9.1 MB) blocks worked fine as long as they weren't crossing the Great Firewall.
@_date: 2016-01-04 13:40:32
Partially incorrect. My brother wrote the visualization code, and yes, he refuses to post here in I ran the actual tests, wrote 90% of the bitcoind code that we used to collect the data, and coordinated with the other volunteers like and I gave the talk in Hong Kong. My brother Mike did not, and stayed in Washington while I was gone to take care of our Bitcoin mine.
@_date: 2016-01-03 14:22:05
Thank you for your opinion. I disagree. Bitcoin is a democracy in that the hashrate majority wins if it chooses to. It's also socially a democracy in that the miners listen to users about what code they should run. It's not a very well-organized democracy
It is not a democracy in that the coders are an insular group that decides what code should get published and pushed as the official Core position, and refuses to listen to years of support for a blocksize hard fork. However, that last bit can be changed.
Note: if your criticism is that Bitcoin is a plutocracy and not a democracy, then I think that is a valid criticism and I apologize. However, I note that the principle of plutocracy might actually be correct for governing how a currency should be run. That would be government by the stakeholders.
@_date: 2016-01-01 17:15:48
One of BTCC's project managers told me over lunch about a previous DDoS attack they had experienced. 
They received an email from an anonymous hacker who demanded something like 1 BTC immediately, and warned that if they did not pay that DDoS attacks would commence. Further, the extortionist hacker warned that the price would go up if they didn't pay immediately.
BTCC did not respond. A brief DDoS attack commenced.
The attack was larger than BTCC had expected. I think he said something like 10 Gbps. During the attack, they got a few fretful calls from their DDoS protection provider, who said something like "This thing is huge! You guys aren't paying us enough for this!" So BTCC paid them more.
After the attack, they got another email from the extortionist: "Now you have a taste of what we're capable of. Please pay us 10 BTC immediately or more attacks will follow." 
BTCC did not respond. Another attack commenced, and lasted several hours.
During this attack, BTCCs servers experienced some performance issues, resulting in a partial loss of functionality. They upgraded their servers.
After this one, they got another email from the extortionist: "We will keep these attacks up until you pay! The price is now 30 BTC. You had better pay up before you go bankrupt! Mwa ha ha!"
BTCC did not respond. Attacks recommenced. 
More threats were issues, more attacks performed. BTCC stopped noticing many of the attacks, as they usually failed to disrupt their networks for more than a few minutes after the upgrades they rolled out.
Eventually, they got a different email from the extortionist, saying something like, "Hey, guys, look, I'm really a nice person. I don't want to put you all out of business. What do you say we just make it 0.5 BTC and call it even?"
BTCC did not respond.
Eventually, they received the final email from the extortionist: "Do you even speak English?"
@_date: 2016-01-20 10:14:23
The acid was a misquotation. Mike was just mentioning that Berkeley, where he went to school, is an interesting place, well-known for its cults and acid.
As for pot, well, it's legal here in Washington. I can't stand the stuff, though.
@_date: 2016-01-09 16:28:32


I don't think you are understanding my position. I oppose SegWit being pushed through on its current schedule in its current form. I think it should be done as a hard fork, without any byte discounting at all, and should come long after a hard fork blocksize increase.
I think the 0.25x byte discounting in SegWit is effectively a subsidy for projects like Lightning and sidechains. Those projects have more complicated signature scripts than typical transactions, so they benefit more from the signature script discount. I don't like that. Lightning and sidechains should compete with on-chain transactions on their merits, not on their subsidies.
Also, I think that a 2 MB blocksize cap should be the first increase, not the last increase. I think it is urgent that we increase the blocksize to at least 2 MB within a few months, and that it is important that we schedule further increases so that we can keep ahead of demand. One way to do this is with a 2-4 proposal, in which the blocksize limit automatically scales over the course of the next two years.
@_date: 2016-01-14 07:39:15
75% support means 3:1 approval.
95% support means 19:1 approval. 
The 95% threshold is a 6.3x higher bar to meet than a 75% threshold. That is a *massive* difference. Using a 95% threshold as the decision criteria strongly biases the system towards inaction. 
You probably like that. I don't. I want Bitcoin to be able to adapt, grow, and progress in a reasonable fashion. I don't want to see Bitcoin become surpassed by some altcoin like Litecoin or Doge simply because we chose a threshold that makes progress unfeasible.
Furthermore, if we used the 95% threshold for a contentious and divisive issue, it might encourage the supermajority to engage in vote manipulation. If, for example, 10% of the hashpower opposed the fork, it would be quite easy for the majority faction to intentionally orphan and 51% attack all of the blocks mined by that 10% minority. I find that scenario distasteful, and prefer to avoid it. The higher we set that threshold, the stronger the incentive to perform this attack becomes. At 95%, it seems pretty strong to me. At 75%, it's weak enough that I doubt it will be an issue. 
Using a high threshold like 95% gives excessive power to minorities. BW.com, for example, controls more than 5% of the hashrate, so they alone could block such a fork. Most people here don't even know who runs BW.com, and have no idea what they stand for. Do we really want to rest the fate of Bitcoin in the decision of such a small entity? (Note: BW.com actually supports the hard fork. It's just an example.)
Using 95% for a non-controversial fork is a good idea. When there's no doubt about whether the fork *will* be activated, and the only question is *when* it will be activated, then the optimal activation threshold is relatively high: the costs (in terms of old nodes failing to fully validate) of a fork are minimized by activating later, and the opportunity cost (in terms of the risk of never activating, or of delaying a useful activation) is small. For a controversial fork, the optimal activation threshold is much lower: the opportunity cost (the risk of making an incorrect democratic decision) is much greater. 
In order to mitigate the cost of early activation (while full node support may still be low), we have a few tools available. First, there's the grace period. We can give everyone some time after the fork has been triggered (via 75% vote) and before the new rules are active. In the current version of Classic, that time is 1 month. Second, we can use the alert system and media channels to warn everybody of the coming fork and encourage them to upgrade. Third, we can coordinate with miners to not switch enough mining power on to cross the 75% threshold if it appears that full node adoption is lagging behind.
With these things in mind, I think that the 75% threshold is about right for this kind of thing, and so I vote to not change it.
@_date: 2016-03-01 06:02:05


That refers to the relay network protocol, not the relay network itself.
Each relay network is a centralized system with a single point of failure. It does not organically self-organize. Currently, there is only one such system. Saying that the relay network is decentralized is like saying a bitcoin network in which there were only one active miner is decentralized -- sure, it might be theoretically decentralizable, but in practice it is not.
In practice, due to the lack of self-organization, it takes a lot of work to set up and keep a relay network running, which is why there is only one.


I have some ideas for how Blocktorrent can self-organize into an intelligently routed network. Mostly, it involves preferring to send/receive large volumes of data to/from peers with good observed latency and bandwidth, and broadcasting compact-but-important (meta)data to hundreds of peers globally without waiting for requests.
@_date: 2016-03-11 06:56:50


~~It was implemented months ago.~~


No, it was specified to not activate until Jan 1st, 2017
@_date: 2016-01-03 18:46:39




Sipa and I also just had a conversation about it on IRC:
@_date: 2017-02-12 08:43:00
Bitcoin mining ASICs are not very expensive to buy, but they cost a lot to run. I can buy an Antminer S7 for $300 right now, but at the USA average electricity rate, it costs over $1000 to run one for a year. During that time, it would make less than $1000 in revenue (assuming zero difficulty adjustments). Not profitable. At my electricity rates, it costs around $303 per year to run one. Possibly profitable. Tell me again how electricity costs are less important than ASIC capital costs as a force for centralization.
Ethereum is mined with GPUs. That's general purpose hardware. Ethereum mining appears to be about as centralized as Bitcoin is. I know of several Ethereum mines within 100 miles of me that have over 200 GH/s each -- that's about 10,000 GPUs in a single datacenter. Tell me again how using GPUs will save us from industrialized mining.
Actually, don't. Your rants are verbose and rather incoherent, and also frequently counterfactual, so I don't think I'll be reading any more of them. Sorry.
@_date: 2016-01-25 14:16:04
The point of bitcoinclassic.consider.it is to gather information about what users of Bitcoin and Bitcoin Classic actually want in a Bitcoin client. There's not really much point in the Classic developers falsifying that data. If we write code that isn't what people want to run, then people won't run it.
If people think that a fun way to spend their time is by defacing the tool that we use, then we will have to squelch unverified users.
If people want to continue and do a similar attack using Mechanical Turk, then we'll have to add a requirement in the verification process to link to a forum, social media, or reddit account with a post about Bitcoin that is at least 2 months old, or something like that. 
In any case, consider.it is designed to be auditable. I think Travis will be working on rolling out interfaces to the data to make independent audits easier. As it is, most consider.it users have voluntarily included cross-referenceable data in their account names, so if you suspect manipulation, you can take a random sample of the opiners on any particular issue, track them down, and ask if their opinion on the website has been manipulated.  
@_date: 2016-01-09 04:12:34
No, SegWit provides about 1.75 MB of actual capacity, assuming 100% adoption of SegWit and a transaction mix (P2SH multisig vs P2PKH) that reflects the current mixture. The 4 MB number is what happens if you fill a block with 100% 15-of-15 multisig transactions -- that is, 4 MB is what you can do if you're specifically making your blocks to try to attack the network. That means that with SegWit you get the negative network security effects of larger blocks without most of the capacity of larger blocks.


Large blocks take longer to propagate, especially across the Great Firewall of China. 8 MB blocks are likely to strain block propagation more than 4 MB blocks.
@_date: 2016-01-08 23:22:21
For BIP101, the rule is actually to start at 8 MB, doubling every 2 years, *with linear interpolation* between the doubling points.
@_date: 2016-01-01 18:05:40


I don't understand this point. Are you saying that you would soft-fork later to reduce the effective blocksize limit? That's possible with or without SegWit. You can always soft-fork to reduce a blocksize limit. Are you saying that you like SegWit because it gives us the option of not deploying any hardfork blocksize increases? Well, yes, but not having SegWit also gives us the option of not deploying any hardfork blocksize increases. How does a 4 MB SegWit help with that?
And how does a 4 MB SegWit enable anything that a 1 MB or 2 MB SegWit doesn't?
If the 4 MB limit is actually an unresolved flaw, in your opinion, why did you put your name on a plan with unresolved flaws?


@_date: 2016-01-25 13:48:58


 
False. He just helped build consider.it. You are confusing him and me.
@_date: 2017-03-22 01:11:09


LN is not some magic fix. It won't increase capacity instantly by infinity. According to my estimates, it will increase capacity by around **[2.5x]( to 5x for the types of transactions that are currently being done. 100x is not a realistic expectation.
LN's main benefit is that it allows new use cases for Bitcoin, not that it makes the existing use cases massively more efficient. I like Lightning, and I want to see it happen, but I think it's important to keep our expectations for it realistic.


Fact check: LN transactions only pay fees to miners for the channel open and channel close transactions. LN transactions should pay less to miners in fees than normal transactions. There is no synergy for miners running an LN relay hub, and fees on relay hubs are expected to be very small. Ultimately, LN will take fees away from miners, and will give fewer fees to relay hubs than the miners would have gotten. This is not necessarily a bad thing, but your argument that miners will earn more with LN by running LN hubs is false.
@_date: 2017-03-05 02:10:55
@_date: 2017-02-15 00:25:11
Not necessarily. It's possible that their strategy is to *delay* SegWit until a HF blocksize increase gets deployed (as per the HK agreement). As LN isn't ready to deploy yet anyway, delaying SegWit does not necessarily delay LN as well. Or maybe those miners prefer their transaction malleability fix to have a different form, such as Flexible Transactions or SIGHASH_NOINPUT. Or maybe they just think that LN threatens their fee revenue my moving transactions off-chain, I don't know.
@_date: 2017-02-01 23:08:16
More likely, they're running BU but they just stole another pool's block header (for headers-only spy mining) and forgot to change the version bits. Headers-only mining skips the bitcoind process and only relies on the poolserver. 
If they were relying on the poolserver to report the Unlimited version bits while running Core, it's likely the poolserver code would also have modified the version bits from the spy mining mode. However, we didn't see that, so it's more likely that they were not modifying the version bits in their poolserver at all.
@_date: 2017-02-14 18:48:18
As far as I understand it, SPV wallets should be able to use Lightning without SegWit just fine. You can make a channel that has a cool-down period of a day or a week, and then as long as you check the channel state more frequently than that (by checking to see if your counterparty has published the channel-open transaction), you'll be fine.
The issue that SegWit solves relates to transaction malleability -- a malicious second or third party can take your channel open transaction and modify it such that it's a valid Bitcoin transaction but with a different txid than the one you were expecting to use. If they do this, then any precomputed channel-close transactions will become invalid, and you won't be able to close the channel until you sign a new transaction.
@_date: 2016-01-04 13:48:39
Me; I'll take it off your hands.
@_date: 2016-03-01 03:00:42
Of course a 1 satoshi/kB transaction should never confirm. I'm just curious whether blockchain.info's statistics call such transactions "without fees" or "with fees", since there are a lot of them and they have extreme values for confirmation times. Whether those transactions are included when computing the median influences where the median lies in the confirmation time distribution, which in turn would influence the sensitivity of the median metric to events that have selective effects on the lowest few deciles.
@_date: 2016-03-12 01:02:41
17.7% per year is way too low. Jan 1 2017 is too late. Both of the basic numbers in BIP103 were completely unacceptable to me and many others, so rather than saying "We like BIP103 except with all of the stuff that makes BIP103 BIP103 changed", we just created different proposals that we liked better.
BIP103 also doesn't include any protections against sighash attacks. This makes it relatively unsafe.
@_date: 2016-03-05 23:37:31


Because SegWit is not a good short- or medium-term solution to the scaling problem. SegWit has a lot of good stuff in it, but the element that allows for immediate increases in capacity -- the 0.25x discounting function for witness data -- is not a good thing.
SegWit provides 1.375 to 1.75 MB of capacity in the near future (for 50% to 100% wallet adoption). It also comes with a 4 MB adversarial condition. This means that miners and network engineers have to design their systems to be able to handle 4 MB blocks without bogging down, but we only get to actually use about 40% of that capacity.
This 4x adversarial case will make it very difficult to increase the blocksize limit in the future. With a 1 MB base block size, it's 4 MB, but with a 2 MB base block size, the adversarial case would be 8 MB. 
On the other hand, with a simple block size increase hard fork, we get 2 MB of capacity with a 2 MB adversarial case.
The 4x adversarial case is a direct result of the 0.25x discounting function that the current version of SegWit uses. The way this works is that SegWit says that one byte of data in the base block (what we currently think of as *the* block) is equal to one byte, but that one byte of data in the witness block is 0.25 bytes. This is an accounting trick, not compression: each byte in the witness block still has to be sent over the network as one byte, and it still takes up one byte of disk space; the only place where we are counting it as 0.25 bytes is in the accounting for the block size limit.
I don't like this accounting trick. I think it has harmful effects on the network's ability to scale. I also see it as a sort of tax subsidy for Lightning Network and sidechains. I don't like that subsidy. I would rather see those technologies compete on their own merits, not because we changed the fee rules to be biased towards them.
@_date: 2016-03-22 20:02:08
The use of VPS nodes has been helpful as a defense against the **[DDoS attacks]( that were directed against Classic.
I encouraged people on home internet connections or who couldn't afford to be DDoSed to not open ports on their firewalls for Classic nodes in order to make them less likely to be crawled by the DDoSer. This has the side-effect of making them not visible to people like Alex Petrov or to sites like CoinDance.
@_date: 2017-03-20 03:53:00
Please stop polluting and wasting electricity. Global warming is a thing.
If you're going to mine, buy some modern hardware. An Antminer S1 uses 20x as much electricity as a current-gen device for the same hashrate (and for the same vote).
@_date: 2016-03-01 02:22:52
The chart's title was updated recently. The chart is **median* confirmation time for transactions with fees. Since the confirmation times should be a very heavy-tailed distribution, using the median (which cuts off the tail) will miss most of the delays.
@_date: 2017-03-04 15:25:16
UASF is a fork in which miners and economic nodes play chicken with each other.
@_date: 2016-01-04 22:37:51




The miner that chooses to create a large block, that is, especially if it's a large *attack* block (i.e. full of unpublished transactions).
Yeah, another time.
@_date: 2016-01-09 21:26:32


You seem to be under the impression that I think 2 MB is "enough", or 2 MB is what I want. It's not. 2 MB is just the number that has consensus among miners. I would much rather have substantial headroom, like BIP101's 8 MB. I think that increasing the blocksize to that level would make miners and pools whip their networks (and algorithms) into shape pretty quickly. I am annoyed that they expect volunteers like me to fix their block propagation for them. However, I recognize that I only have about 0.2% of the hashrate under my indirect control, so I defer to consensus when it differs from my personal opinion.
If SegWit is pushed through in its current form, it may be impossible to convince miners to accept any hardfork blocksize increase, because such an increase could present unacceptable block propagation risks due to the 0.25x discounting and 15-of-15 transaction problem.
The maximum data size for a block with SegWit is slightly less than
    max_data_size = MAX_BLOCK_SIZE * 4
The typical capacity for a block with SegWit using 100% SW transactions is
    max_capacity_100 = MAX_BLOCK_SIZE * 1.75
The typical capacity for a block with 50% SW transactions is
    max_capacity_50 = MAX_BLOCK_SIZE * 1.375
If we want max_capacity_50 == 2 MB, then 
   
    MAX_BLOCK_SIZE_SW = 1.454 MB
That would mean
    max_data_size_SW = 5.818 MB
Several miners have expressed the opinion that 2 MB is the maximum that is currently safe. 5.818 is quite a bit larger than that. As such, I am not sure if there will be consensus for any blocksize increases if SegWit is merged in its current form. However, I don't know how much weight miners put into adversarial conditions.
@_date: 2016-01-01 19:00:20
Oops, I misinterpreted the red. That's totally my fault.
I guess I either need more friends, or fewer friends.
@_date: 2016-01-09 01:45:50


I don't see that as a given at all. Do you think that subsidies for miners falling will result in users choosing to pay more for their transactions?


No, empty blocks demonstrate that block propagation is slow, and that CreateNewBlock is slow, and that SPV mining is more profitable for the first few seconds of mining.
@_date: 2016-01-04 13:43:25
The GFW is about 1/3rd of the problem. The other 2/rd is that Bitcoin's current built-in block propagation mechanism is really inefficient.
@_date: 2016-01-17 06:51:17


Because they might see now that their roadmap isn't as popular as they thought it was. In particular, miners don't like it much.


An economic and hashrate supermajority currently supports a 2 MB hardfork. It seems that they also support Classic.


The first hard fork will be scary. The ones after that will be much easier.
Many altcoins have had hardforks before. It's not like hardforks are some huge unknown. Typically, in altcoins, they get deployed with about 1 month of advance notice. When they're carefully coded, they generally go pretty well. When the developers get too ambitious and try to stick too much into the fork without enough testing and review, they are more likely to have problems. Of course, that's true for soft forks as well as hard forks.


Sounds like a good opportunity for the more even-keeled people to make a lot of money. 
I'm interested in the long-term future of bitcoin. Variations in price during the fork don't interest me much. I'll wait it out. I expect many others will too.
@_date: 2016-01-01 18:34:30
No, that's not what I want people to infer. I'm trying to say that an anti-DDoS strategy is an important attribute for censorship resistance. If we want to be able to speak our opinions freely, we need to be able to protect each other against DDoS. 
Furthermore, we need to be aware that fear of DDoS may be causing publicly expressed opinions to differ from privately held opinions. So far, the DDoS attacks have been pretty focused on pro-fork parties. We ought to calibrate our sensors accordingly, so to speak.
I actually made a comment elsewhere saying that I explicitly do not think that any Core developers would be involved in such a thing.
I can think of several other parties (non-devs) that I think *would* be willing to perform this kind of attack, though. It seems to be a tool that would be favored by someone who favors heavy-handed moderation practices, for example, though ultimately I believe the culprit to be someone else (or possibly a few other parties). Mircea is a suspect, as he made some rather explicit threats around the time the other client was first published, claiming that he had deeper pockets than anyone who supported that other client, and some other stuff.
All I'm saying is that there has been a pattern of companies or organizations making a public comment in favor of a fork (e.g. Slush, that other client, Coinbase, BTCC), then getting DDoSed within a day or two. After the fourth (or more?) such instance, I think we can infer that there's likely a link.
@_date: 2016-01-17 02:31:17
Probably removed. We haven't gotten to Core's master branch, and are currently working off of 0.11.2, and that doesn't have RBF yet.
@_date: 2016-01-01 14:22:09
Segregated Witness will increase the amount of data that miners have to download per block to 4 MB in adversarial conditions. [Perhaps you should think a little more about what happens in adversarial conditions](
It was my understanding that you have no interest in participating in a system that lacks certain censorship-resistance qualities, and that without being able to profitably mine anonymously (e.g. behind Tor) anywhere in the world, Bitcoin would lack those qualities and cease to be interesting to you. Are you saying that you think that we can mine profitably over Tor in China with 4 MB blocks with no relay network in adversarial conditions? Or are you saying that with SegWit's 4 MB adversarial blocks, this is somehow acceptable or possible, but with a hard fork blocksize increase to 2 MB, it is not?
I hope you don't think I'm being too adversarial in my dialogue. I'm just trying to figure out how the different positions I've heard you take can be logically consistent with each other.
@_date: 2016-01-20 10:15:40
Would you prefer a benevolent dictator? Honest question.
@_date: 2017-03-04 01:27:02


More to the point, the way soft forks work is that miners *enforce* them. If you don't have well over 50% of miners enforcing a soft fork, then the result is a chain split and consensus failure, and one currency becomes two. 
@_date: 2017-02-07 00:18:38


Exchanges and payment processors collect a large number of small inputs from their customers, and eventually have to consolidate them. If they're going to consolidate them, why not consolidate into a round number of bitcoins in the outputs? It's not hard: you just keep adding inputs to the transaction until the inputs plus the minimum fee is greater than the desired output (e.g. â¥ 25 BTC), and then any surplus beyond your target is just additional fee.
If you look at the inputs for these big UTXO-sweeping transactions, you'll notice that the input creation dates are broadly distributed over December and January. This pattern would make sense if it were a poorly-configured exchange that receives UTXOs from customers at random times (based on their customers' choices) and consolidates a portion of them once a week in a big cron job. That pattern is stupid and expensive for an entity this large, but not necessarily malicious.
@_date: 2017-03-18 05:29:11
is probably referring to the May 2013 fork. The May 2013 fork was the same as the March 2013 fork, except that the March fork was unplanned, aborted, and rolled back whereas the May 2013 fork was planned and successful. Whether or not either fork was a hard fork is a point of contention, though.
Gavin described the March 2013 fork **[as a hard fork]( and others at the time also described the May 2013 fork as a **[planned hard fork]( but luke-jr **[amended the BIP in 2016]( to remove the hard-fork label.
So who's right? There is one major difference between a classical hard fork and the March/May 2013 forks, and that is that the 2013 forks were *non-deterministic*. The classical definition of a hard fork is an instance in which the consensus rules on block or transaction validity are loosened. The Bitcoin 0.7.2 effective consensus rules stated that blocks or reorgs that required more than 10,000 BDB locks were invalid, whereas blocks requiring fewer locks (ceteris paribus) were valid. The 0.8.0 rules stated that blocks requiring any number of locks could be valid. Alternately, one could describe the 0.7.2 consensus rules as considering blocks with large numbers of inputs (needing a lot of locks) as being *sometimes* valid, whereas 0.8.0 considered those blocks *always* valid. The 0.8.0 rules were strictly looser than the 0.7.2 rules, which suggests to me that it was indeed a hard fork.
The point of contention seems to be the non-deterministic nature of the forks. The number of locks required by 0.7.2 depended on the exact contents of the chainstate database, including page alignment, and therefore varied from node to node. Therefore, different nodes could disagree on which blocks were valid and which weren't. It seems to be the opinion of luke-jr and sort-of also **[gmaxwell]( that the non-deterministic nature means that it's not a hard fork. Personally, I disagree with luke-jr and gmaxwell's opinion, and think the fork is best described as a non-deterministic hard fork. 
Even though it was a non-deterministic fork, it's worth mentioning that *most* of the 0.7.2 nodes rejected the block 225430 generated by 0.8.0, and if it weren't the case that a majority of miners at the time were using 0.8.0, consensus would have followed the chain without the large/high-lock-count block 225430. Indeed, the way that the March 2013 fork was resolved was by getting miners to switch back to 0.7.2, after which most miner rejected the high-lock-count block 225430 and non-deterministically returned to the small-block chain. Effectively, even though a single 0.7.2 node might have accepted a block, the consensus rejected it. You can compare this to this hypothetical rule: "If a block is larger than 1 MB, generate a random number in the range \[0,1\] based on a local random pool; if that number is less than 0.1, the block is valid." With this rule, it is practically impossible for a large block to get the 51% hashrate majority needed to remain part of the longest chain. Indeed, one might even say that the non-deterministic node code results in a *consensus* rule against the large blocks. This would make the elimination of the non-deterministic limitation a relaxation of the consensus rule, and consequently a hard fork.
If you look at the May 2013 fork, it's a bit less muddy. Version 0.8.1 included a special rule to reject blocks containing inputs from **[more than 4,500 distinct transactions]( until May 15, 2013. The 4,500 transaction limit was designed to deterministically prevent the 0.7.2 undocumented non-deterministic limit from being hit. The elimination of 0.7.2's limit on May 15 from the consensus chain, in my opinion, qualifies as a hard fork.
If you take bitcoin-qt 0.7.2 and try to sync the blockchain, it will usually fail, but rarely it will succeed. Does this mean it was a hard fork? I think so, but we will probably never have consensus on this issue.
**tl;dr:** A case can be made that the 2013 forks were not actually hard forks due to the non-deterministic nature of the effective consensus limit on the number of transaction inputs. However, a case can also be made that refusing to call the 2013 forks hard forks is politically motivated revisionist history.
@_date: 2016-03-05 03:32:22
There is plenty of room for both if the discounting function in SegWit is changed.
@_date: 2016-01-09 22:11:52
I have been dancing around these questions because I haven't been sure if you're asking my personal preferences (which is for BIP101) or for what I think is the best course of action for the community (which is a compromise around 2-4 MB).




No, I did not say that I think 8 MB is too much. Technically, I think the strain of an 8 MB blocksize cap would be tolerable for the next few months, and could be eliminated with some protocol changes and performance improvements. All I said was that 8 MB would put strain on the network.
8 MB is not an option because most other miners are not as optimistic as I am.


Check  I don't see consensus there for a SegWit softfork. All I've seen for consensus is a bunch of developers sign a document in support of it.


It would do that. It just wouldn't be a good way of doing that.
@_date: 2016-01-09 18:39:37




Not my website; it was built by and Kevin Miniter (whose reddit username I don't know).


BIP000? What's that?
@_date: 2017-03-20 16:03:17
Keep in mind that pooled mining is a thing. If big blocks make it too big for small miners to run their own full node and solo mine, they will just mine to a pool instead (or rent a VPS somewhere with better bandwidth). Consequently, there's little incentive for a selfish miner to actually perform this attack.
You don't get to knock off 5-10% of the *hashrate* offline with a big block attack. The most you can do is shift the hashrate around. Furthermore, each time you do this, you make the remaining competing pools larger, which means you can't keep doing 5%; the chunks that you attempt to shave off would be larger each time, until they are too large to shave off. 
Miners have an incentive to not let the consensus blocksize limit get too high. They are better off keeping the limit low enough so that fee pressure is significant. Performing any of these big block attacks would require consensus blocksize limits in the 20-2000 MB range, and I don't think miners are stupid enough for a supermajority to decide to raise the consensus limit past the point where casual users (much less other miners) can no longer afford a full node.
@_date: 2016-03-03 22:05:19
These problems have been coming or a long time. Smart investors have been expecting them to happen, and have factored that into their purchase prices. The fact that the price has at most made a small dip in the last few days indicates that the problems are only slightly worse than expected.
@_date: 2016-01-03 17:47:40
Special thanks to Marshall Long for helping to promote this tool to a few early users.
@_date: 2016-01-09 19:17:04


First, it's jtoomim. Second, toomim is the older brother. He got first pick of all the internet usernames, so he just used our last name without his first initial.


That's me. It was just China, though. I talked with Bitfury and KnC a little bit in Hong Kong, and then more later via email.


Oh, you mean like the status quo option on  The one with an average vote vote of Âµ=-0.89.
@_date: 2016-03-04 00:47:01
Well, I need to sell some bitcoin soon to pay for our electricity bill, so ThePriceIsGoingTohHit$600WithinAWeekISwear.
@_date: 2016-01-03 14:20:29
The idea is only as good as the implementation.
@_date: 2017-03-20 20:52:31


1: I'm not supporting a 16 MB block size. Someone else put out that number, not me. I just responded to it. While I'm totally okay with 16 MB, I don't consider myself to currently be an advocate of that position. We won't need to get to 16 MB for several years, and can reevaluate the limits several times before then.
2: I've only tested up to 9.2 MB. That was 15 months ago, without libsecp256k1, the new GBT, xthin, compact blocks, or any of the other optimizations that have happened since 0.11.2. Back then, a 3993 kB block took 2237 ms to validate on my node (a $500 desktop computer), and a 9236 kB block took 5114 ms to validate. In other words, a block 2.772x as large took 2.286x as long to validate. Technically, that's better-than-linear scaling. However, on a long enough time scale (to allow for UXTO set size growth), then it's probably `O(n log (n))`. Except for a few tests with Classic on 2 MB, I haven't done any testing since. With libsecp256k1, I expect the validation would happen about 2-5x as fast.
I don't care enough to reply to the rest of your comment. Nothing personal, I'm just not too emotionally invested in this any more.
@_date: 2016-01-04 20:31:52


Latency usually results in a loss for the miner. 
I don't have time for this conversation right now, sorry. I can give you more detailed responses in a few days.
@_date: 2017-03-04 03:48:09
Most of Bitcoin's rules are enforced by all nodes. Miners redundantly enforce most Bitcoin rules, so a group of miners that breaks one of those rules would not compromise consensus. However, soft forks have the unique property that the new rules apply to everyone, *including non-upgraded nodes,* as long as a majority of miners enforce them. Thus, miners critically enforce recent soft forks. If miners fail to enforce those rules, the result is a consensus failure.
The BIP66 consensus failure happened because a majority of miners weren't doing their job. When they weren't doing their job, they didn't get paid. Thus, they're motivated to do their job. However, the point remains: it's critical that a majority of them do their job, or else consensus fails.
The user-activated soft fork idea is technically equivalent to the BIP66 scenario, with the caveat that you're doing it *on purpose*. The miners who aren't enforcing the soft fork in that scenario would be doing so not by mistake, but by intention, and it's not clear that they would immediately switch over if a majority of full nodes and a minority of miners forked off without them.
Consider the following two scenarios:
1. 75% of miners and 25% of economic full nodes are following a soft fork
2. 25% of miners and 75% of economic full nodes are following a soft fork
In Scenario 1, the non-upgraded miners create blocks which 75% of nodes consider to be valid, but which get orphaned after 1 or 2 blocks. The upgraded miners create blocks which 100% of nodes consider to be valid and which don't get orphaned. Non-upgraded miners quickly upgrade because otherwise their non-orphaned revenue is zero. Upgraded and non-upgraded nodes always see the same chain with the occasional exception of the most recent 1 or 2 blocks.
In Scenario 2, the non-upgraded miners create blocks which 25% of nodes consider to be valid, and which do not get orphaned. The upgraded miners create blocks which 75% of nodes consider to be valid, and also which do not get orphaned. Both chains grow in parallel. The chain that rejects the soft-fork upgrade grows 3x as fast as the chain that includes the soft fork. Congestion in the upgraded chain is 4x as bad as it was before the fork, whereas congestion in the non-upgraded chain is 1.33x as bad. Both chains will be usable, but the non-upgraded chain will be more usable than the upgraded chain. The exchange rates for tokens unique to each chain will fluctuate based on which one they think will prevail; that calculus might be partially motivated by the usability of each chain. Miners will (probably?) ultimately follow whichever chain has the higher exchange rate. Until the difficulty adjusts, there will be no incentive to mine on the chain with the least hashrate, as the difficulty will be identical on each chain; only the exchange rate for each chain will affect profitability. Thus, if market confidence in a chain with a 40-minute block time flags, it might simply die off. Or it might not. Maybe the chain that started with the majority hashrate will die off. Maybe not. It's pretty uncertain. In any case, it's a mess. Also, since the flag day is known in advance, double-spend attacks associated with the fork are likely.
Even though Scenario 2 follows the classic definition of a soft fork (i.e. it strictly narrows the set of allowable transactions and blocks), it has many of the properties of a hard fork (consensus failure and the possibility of a persistent and stable minority chain). I don't think the "user-activated soft fork" deserves the name "soft fork" at all, so instead I think I'll refer to it as a **Frankenstein fork**, since it has features of both soft forks and minority-hashrate hard forks rolled into one. 
The Frankenstein fork obviates nearly all of the benefits of doing soft forks in the first place. It also seems that it might qualify as attempting to alter the Bitcoin protocol without overwhelming consensus. If you think scenario 2 has acceptable risk, why not just convert SegWit to a hard fork, move the witness merkle root hash into the block header (thereby cutting fraud proof sizes in half), and do all the other hard fork cleanups that we've had on our to-do list for ages?
@_date: 2016-01-09 06:25:54


Yes, but not as a short-term capacity increase method. SegWit should be modified to have a cap for the coins+witness data to be 1.5 to 2.0 times the current cap. That would resolve the issue with the lost headroom for adversarial cases, while still providing the main benefits of SegWit (e.g. transaction malleability fixes, Schnorr signatures, fraud proofs, etc.).


SegWit is only 1.75 MB if 100% of transactions use it. I do not expect that to be the case for quite a while, perhaps a year. For the next few months, I expect SegWit transactions to comprise only 50% or so of the total, which would make the effective capacity only about 1.375 MB. 
There are a few other objections that I have to SegWit as currently proposed. I would like to see a few things about it modified before we roll it out. In particular, I think it would be much better deployed as a hard fork than as a soft fork. The soft fork version seems like an ugly hack, and if we deploy SegWit that way, we'll be stuck with ugly code for the rest of Bitcoin's lifetime.
Some of my objections can be seen here:
@_date: 2017-03-20 02:33:17


A 100 mbps internet connection costs around $30 to $2000/month. A 2 TB HDD costs around $60. A server with a quad-core CPU and 64 GB of RAM costs around $700. That is enough performance to handle block sizes of at least 20 MB, and probably more nowadays with the recent software advancements. So that's what you can get to for around $1000. Considering that an Antminer S9 costs about $1300 up-front and uses around $100/month in electricity, but only has 0.0003% of the total network hashrate, and considering that solo mining with less than around 0.1% of the network hashrate (or $300k worth of S9s) is ridiculous anyway because of variance, I don't think $1,000 is a significant deterrent for anyone who might want to solo mine.
What kind of blocksize are you thinking of that would require $250,000 in hardware to be able to keep up? 2 GB? 2 TB? Do you think that kind of attack scenario is realistic? If mining pools create blocks big enough to force competing pools off the blockchain, do you think that casual non-mining full nodes will survive? If casual non-mining full nodes don't survive, do you think the exchange rate for Bitcoin will stay high? (Ethereum's exchange rate didn't seem to do so well when Ethereum was subject to a major, disruptive, and explicit spam attack. Would Bitcoin be different for some reason?) If such a miner spam attack causes the exchange rate to tank, do you think miners would still actually do it?
@_date: 2016-01-04 13:47:59
The point of the data was to simulate adversarial conditions. It's trivial to make blocks that the relay network chokes on: you just include 100% transactions that have not previously been published. Under adversarial conditions, the demands for latency and orphan rates are diminished (adversarial conditions are not typical conditions), but you still need to be able to get the blocks in less than 20 seconds or so. 
I don't think the data show 600 kB blocks to be unsafe. There were instances in which UpdateTip could not be completed for a 600 kB block on a Chinese node because one of the parent blocks (which was much larger, probably 9.3 MB) hadn't been successfully downloaded yet. That can make it look like that node had a very long (e.g. 5 minute) verification time. That situation is easy to identify by looking at the time-to-download and the time-to-validate separately.
@_date: 2017-03-18 06:03:20
The community needs to be aware of facts and truths, but does not need to be aware of baseless accusations, rumors, and potential libel. 
@_date: 2016-01-01 16:46:08
Uh, I mean you could ask people about their opinions, and stuff. Maybe even request a non-binding coinbase vote from the miners, or put something up on  or  I don't see how allowing a vote to determine the value of a constant in a softfork proposal is a hard fork.
@_date: 2016-01-17 07:13:05
That is a concern. Time permitting, I believe would like to make the raw data available so that other people can build their own views upon it.
I recommend that people run independent surveys to verify some of the more important results of consider.it. Obviously, it's not going to be practical to verify everything, but as long as questions are sampled with some randomness, it should be possible to detect at least some vote manipulation. In national elections, press will often conduct exit polls to get an idea for what the voters are voting for. Obviously, if the exit polls say that Clark Kent has 55% of the vote but the official results say that Lex Luthor won 60%, there may be an issue.
We hope that other people will help us watch out for signs of voter fraud and will bring them to our attention. We will have to deal with issues as they happen.
@_date: 2017-03-22 00:33:53


Nothing except fee incentives, the desire for Bitcoin to succeed, common sense, and the ability of Bitcoin users to reject any chain that goes excessive.
The incentives to miners for choosing the consensus limit are quite a bit different from the incentives for choosing the size of an individual block. An individual miner does have an incentive to make his own blocks as large as possible without getting orphaned. However, an individual miner also has the incentive to make the consensus limit for blocks relatively small in order to:
1.Keep transaction fee pressure significant, and keep the market fee/kB rate somewhat high to maximize short-term revenue;
2.Keep the blockchain size small enough for regular Joes to be able to access it, in order to keep faith in Bitcoin high and the BTC/fiat exchange rates high;
3. Keep other miners from being able to perform bandwidth-assisted selfish mining attacks against them; and
4. Avoid consensus failures from having a bare majority (instead of a large supermajority) during blocksize limit increases.
In that respect, the incentives for miners are actually pretty closely aligned with those of regular Bitcoin users. It's even possible that miners might prefer somewhat smaller blocks than users because of  and  although it's also possible that miners will prefer somewhat smaller blocks than users because  affects miners only indirectly via the exchange rate and hits users more directly. Overall, though, the block size limit preferred by miners and by users should be pretty close to each other, probably within a factor of 2.
Ethereum has had a block size limit determined by miner votes for two years, and it has worked quite well so far.


The game is broken because developers don't have the right incentives to make that decision. Developers have potentially unlimited liability if something goes wrong, but they do not get the same benefit if something goes right. Miners, on the other hand, suffer pretty directly if something goes wrong and benefit just as directly if something goes right. 


Because you said single-cent fees. 1Â¢ is a lot less than 5-10Â¢.


You mean how he's offering 110% of the 12.5 BTC reward instead of offering 100% of the ~14.3 BTC reward+fees? I wish he were personally subsidizing BU mining, but he's not.


This is *not* my position, nor is it the position of most BU-advocates or BU miners.


Changing the block size limit changes what kind of damage spam does to the network, but it doesn't change the fact that the network is vulnerable to spam. When the block size limit is small relative to demand, spam (or mixing services) displaces "real" transactions and causes denial of service to people who aren't rich and/or don't have (or understand) RBF. When the block size limit is large relative to demand, spam doesn't displace "real" users but it does increase the resource requirements.
I would also venture that $4000 is a lot to pay for sending out a 16 MB block. With 5000 full nodes, $0.25/GB for storage (i.e. SSD), $30/month per CPU core, and $50/300GB for bandwidth (i.e. home connection), a 16 MB block costs the full node network around $33.62. So the spammer is paying around 100x as much as the damage he does. I don't find this scenario particularly concerning.
Bitcoin users are currently paying about 40x as much on transaction fees as they're paying to run full nodes, even assuming that all full nodes are expensive VPSs. I interpret that to mean that the block size limit is currently much lower than the optimal value.


This is not an accurate model for fees. There is a fee floor based on the default minimum fees baked into bitcoin clients. Furthermore, the demand curve for blockspace is characterized by very low demand elasticity, which means that small changes in supply or demand will make very large changes in the market feerate; this is why transaction fees increased 5x in the 5 months since October 2016 even though demand should only have increased 1.4x during that time (based on Oct '15 to Mar '16). Basically, blocks are either full (in which case some percentage of uses of bitcoin get priced out) or they aren't (in which case people pay the default minimum fee or something close to it).
This conversation is taking up too much of my time, so I probably won't reply to anything else in this thread. Sorry.
@_date: 2017-03-06 01:07:16
Currently, it takes 261.8 TH/s to get 1 BTC per week if you ignore transaction fees and pool fees.
Doing that with 20 Antminer S9s will cost about $32,000 in up-front costs for the S9s and power supplies, and will use about 27 kW of power. If you pay 5Â¢/kWh, that will cost around $985/month or $227/week in electricity costs.
@_date: 2016-01-10 04:17:43
Yes, this is a good idea. I think you should post a bitcoin address for people who are willing to pay for good translation work.
In particular, I would like to see **[this post of mine on SegWit]( translated, as I don't think the objections to SegWit have had enough exposure in the Chinese language. I don't know if you're willing to do translations in both directions, though.
@_date: 2016-01-01 19:03:08


Maybe. Slower than looking it up from RAM, though, at least last time I tested while doing work on CreateNewBlock. And my testing suggests otherwise even from SSD.
In the old CreateNewBlock code, the UTXO lookups were done on all of the transactions in the mempool (which might be 50 MB or 400 MB), whereas the sig verification was only done on the transactions that actually went into the block (1 MB max). In this scenario, in my testing, UTXO lookup would take up about 4 seconds (warm cache) to 30 seconds (cold cache) on a 400 MB mempool, indicating 10 (SSD) to 100 MB/s (RAM) of throughput. On the other hand, scriptSig verification took about 800 ms for 1 MB of tx data, indicating about 1.2 MB/s of throughput. Even with libsecp256k1 and loading UTXO from SSD, that would not be faster for scriptsigs than for UTXOs. That's with the current tx mix, which is heavy on p2pkh and light on sigops. With a subsidy and more complex scripts like Lightning multisig (without Schnorr), that could change to be even more expensive for sigs.
Let me know if you want to see the actual data. I should have the enhanced debug logs from that testing around, and if not, I still have the code.
@_date: 2016-01-21 09:37:45
Yeah, my thoughts exactly.
@_date: 2016-03-24 16:28:46
"Use Tor or get DDoSed so hard that your ISP shuts off your internet" is not a very good UX for Bitcoin. I prefer the scenario in which DDoS attacks are diluted to the point of cost-ineffectiveness due to a surplus of targets and/or upstream packet filtering. 
I prefer Tor to be an *optional extra* layer of security, rather than a base requirement.
@_date: 2016-01-09 23:49:13
You're welcome.
@_date: 2016-01-01 14:04:46
Yes, I am quite worried about this scenario. Not exactly the way you describe it, though.
One issue is that a reduction in hashrate could result in a reduction in consumer confidence in the currency, which could result in a reduction in the exchange rate, which reduces hashrate further, etc etc. 
Another issue is that hashpower that goes offline may be available for rent to the highest bidder. Currently, 51% attacks are hard to do because you have to buy or manufacture enough hashrate for the attack -- you have to pay the capital costs of the attack. If 60% of the hashrate went offline for economic reasons, that might mean that 50% would be available for rental. Having hashpower available for rental means that attackers only need to pay the operating costs of an attack, not the capital costs. Running a miner for, say, 2 hours costs much less than purchasing a miner for 2 hours. Currently, those numbers are about $300 to purchase 1 TH/s versus $0.20 to rent 1 TH/s for 2 hours. Having dark hashpower available for rental makes 51% attacks approximately 1000x more affordable.
@_date: 2016-01-01 17:18:27
Samson Mow: "  We try not to speculate about who attacks."
I, on the other hand, will. As you are aware, this is most likely a retaliatory attack for BTCC's (and Samson's) comments about being willing to use a client other than Core:
 
Edit: since I have received criticisms twice on this comment, I wish to clarify that this opinion is due to there being a pattern that has played out several times before in which a company or project comes out in support of a hard fork, then gets DDoSed within 48 hours. This includes Slush's pool, the BitcoinXT nodes themselves, some p2pool nodes, Coinbase (after they announced testing with BitcoinXT about one week ago), and now BTCC. One such occurrence is not sufficient grounds for the claim I made. However, I think that after five such occurrences, such a claim is worth making. It's not proven, but I think that saying "most likely" is justified.
Edit2: I do not think that any Core developers were involved in this. I think this was done by an unrelated party who has taken this upon himself as a crusade.
@_date: 2017-03-20 03:45:29
The same as is needed for 1 MB blocks. 1 GB is enough. 
Currently, a node with 1 GB of RAM and an SSD can validate a 1 MB block in a few hundred milliseconds. A 16 MB block would take the same node a few seconds to validate. Given that the block time is around 600 seconds, a few seconds is less than 1% of a block time and is plenty fast enough for a full node.
A mining node might want to have 16 to 64 GB of RAM in order to reduce their orphan rate by around 0.25% (i.e. 1.5 seconds), but any miner who is large enough to be able to handle the variance of solo mining can afford 64 GB of RAM. Solo mining is pretty nutty unless you have more than 0.1% of the network hashrate (i.e. enough to mine one block a week on average). It currently costs $300,000 to buy 0.1% of the network hashrate, and about $200 to buy 64 GB of RAM.
@_date: 2016-01-13 23:28:47




@_date: 2016-01-01 18:58:59
Noted. I have edited my comment to clarify that I do not think any Core devs are responsible.
@_date: 2016-01-14 22:10:13


Sounds pretty reasonable. Code would be mildly more complicated. I'll think on it, and maybe put it on consider.it later.
@_date: 2017-03-22 01:13:56
LN is not some magic fix. It won't increase capacity instantly by infinity.
With LN, you go from one on-chain transaction per financial transaction to two larger on-chain transactions per account per billing interval. A billing interval might be on the order of a month. If a typical Bitcoin user does 15 transactions per month, and if LN channel open/close transactions are 1.5x as large as regular transactions, then LN would only increase capacity by about 5x.
At 1 MB and 500 bytes per tx, with LN channels lasting an average of one month each, Bitcoin could only support around 4.3 million users, assuming Bitcoin was used for nothing except LN. In 2012, there were 883 million VISA users worldwide. To get to that level with LN, Bitcoin blocks would need to average 204.4 MB in size. To get to VISA-level transaction volumes (~2000 tps, 400 bytes/tx) without LN, blocks would need to be around 480 MB in size.
LN will make scaling easier, but it's not a fix. We will still need to increase the blocksize a lot. Most of the value of LN will be in enabling new use cases for Bitcoin, such as micropayments (e.g. the distribution of ad revenue). However, for the semi-infrequent large-value transfers that Bitcoin has traditionally excelled at, Lightning will be at best a moderate improvement, and at worst an awkward system with a complicated UX.
@_date: 2016-01-09 23:18:32


As a hard fork now, I think the best compromise option is something that starts at 2 MB.
If the current SegWit proposal is passed, then I no longer know what the best compromise proposal would be. Some miners may actually want a blocksize decrease to reduce the adversarial case to 2 MB; I don't know. I suspect that the result will be that there is no consensus around a blocksize increase as long as the 4x adversarial condition is possible, which would mean that we would probably be stuck at 1.75 MB capacity (assuming 100% SW adoption) for a year or two.


I don't know, but probably not. My personal wish will be for a blocksize increase beyond that, but I don't expect it to have enough support to make it a reality.
@_date: 2016-01-01 17:24:28
Not doing a poll now and using 4 MB is kinda like jumping out of a plane without checking to see if you have a parachute or not, don't you think?
@_date: 2016-01-17 07:01:41




This is incorrect. I did not censor the discussion at all. I deleted and edited no posts. It's true that I did not wait for a vote on the issue. I made an executive decision that the PR was ridiculous and trolling, so I simply closed the pull request. 
Subsequently, since my executive action was questioned, a vote was held on the PR. It was **[voted down harshly.]( As a result, I did not reverse my decision, and the PR remains closed.
@_date: 2016-01-14 07:55:49


Nit: A "supermajority" is any threshold that is **[greater than 1/2]( It encompasses a broad range of thresholds, including **[55%]( and 60%. Due to the word's inherent ambiguity, it should usually be used with a numeric qualifier (e.g. "75% supermajority") or in contexts in which the exact threshold is already known by the audience.
"Supermajority" is a slightly less ambiguous term than "consensus", but only slightly.
P.S.: Thanks for taking the time to carefully explain your position. We need more of that.
@_date: 2016-01-16 08:55:55
Core developers are welcome to contribute their code to Classic if they want. Or they can keep writing for Core, and we will include the changes in Classic that they make that our users want us to include. It's a simple git pull for us. Core is also welcome to join the fork, either before or after it happens.
@_date: 2016-01-21 13:22:31
But still defaults to enabled.
@_date: 2016-01-09 03:09:50
I think a hard fork can kick to 2-4 MB is a better idea. I think that will buy us time to fix block propagation, and will give us some experience with rolling out hard forks that will make a longer-term fix easier for everybody to accept.
@_date: 2016-01-03 17:10:45
A fraud proof just says "Here is the portion of the data that violates the rules." You still have to know what the rules are. With a soft fork, you don't know the new rules, so you would get what looks like an invalid fraud proof.
@_date: 2016-01-01 17:38:36
Can you elaborate on how you think that Lightning will be helpful in adversarial conditions when miners are using 4 MB SegWit blocks to attack each other? Or can you describe another way in which SegWit makes block propagation of adversarial blocks safer?
Or do adversarial conditions not actually matter that much for block propagation?
Or do you think that 4 MB is safe enough in adversarial conditions for block propagation?


I presume you're referring to the transaction malleability fixes, which make Lightning transactions easier. Those fixes could also be rolled out in a SegWit version that had a 1 MB tx+witness cap, or a 1.8 MB cap, or a 2 MB cap. Why do you not object to the current proposal that has an effective 4 MB cap? That seems like an unnecessary danger to me, and seems like the kind of thing you would dislike.


I don't care what color your thoughts are. I'm just looking for clarity and consistency. 
@_date: 2016-01-14 23:50:45
I agree, "majority" is the only logically defensible rule in this case. Some supermajority for forking as a threshold can make practical sense, but any supermajority can be simulated by a simple majority if push comes to shove.
As for moderation policy of this subreddit, I think that anything that has a *reasonable chance* of getting majority support should be allowed.
@_date: 2016-01-16 08:52:12
I do not offer cloud mining. I run a **[hosting service](
Our customers purchase hardware on their own and send it to us. We give the hardware power, cooling, and maintenance. It mines to a pool they choose. We do not ever touch their bitcoin. Just their hardware. (Which is more valuable, of course.)
As for why you should trust me? Eh, better to verify. **[Read the code](
You may also find this informative when doing research on who I am:
@_date: 2016-03-05 09:57:44




Then I am confused why you brought up "The extra dimension to the knapsack problem," since the 2D knapsack problem only affects block template creation. It does not affect validation at all. 
Perhaps you're confusing it with **[O(n^2 ) transaction validation time]( The solution to the  O(n^2 ) validation cost problem that Classic uses is the limit to the number of bytes hashed. It prevents attack blocks that are worse than about 10 seconds. The solution that SegWit offers to the O(n^2 ) problem is to create a new transaction format that does not have the O(n^2 ) validation costs. Unfortunately, SegWit does not prevent people from continuing to use the current transaction format to generate attack blocks, so it doesn't actually fix the problem. It just prevents the problem from getting any worse.




SegWit does not include a bytes hashed limit. That is only in Classic (BIP109). With Classic, the worst case scenario is constant vs block size -- 10 seconds. With SegWit, the worst case scenario is proportional to the square of the base size -- 150 seconds, same as without SegWit.


Which is why it is important to make it impossible to create very nasty blocks, as Classic does but SegWit does not.
@_date: 2016-03-04 02:16:51


Correct, and BIP109 makes the situation better by adding a new limit that can never be hit except in the case of explicitly malicious behavior by miners. (The 1.3 GB limit was chosen because there is no way to exceed it by mistake, and only miners who edit the source code to disable the IsStandard() checks can even get close.)


To be honest, I preferred a limit around 260 MB per block. That would have allowed a 2 MB block with (20) 100 kB transactions (the IsStandard() limit) with only one output each (which is the worst reasonable case in terms of hashing requirements). But Gavin decided to use block 364292 as the worst permissible scenario, and that block had 1.2 GB of hashing (due to using a single 999 kB transaction with one output. I think that's reasonable, though not optimal.


It's a sub gave to me because he thought Classic should be renamed. It is not being used.
@_date: 2016-01-01 14:50:48
Yes, I agree, which is why I've been arguing for it.  But that's not the point I'm making here. I'm just surprised that Peter Todd would support this proposal without such a cap.
(I'd prefer a 1.8 MB cap, though, as that seems to be the current practical limit of SegWit given current transaction types, and any higher would be a substantial effective subsidy on complicated p2sh scripts. But whatever, 2 MB is close enough.)
@_date: 2016-01-04 13:53:51
Not exactly. The algorithm is more important than the hardware. 
We did notice that our Intel Atom machine with 100 Mbps internet was *awful*, which we didn't really expect. So node speed matters. However, this may be due to a bug that we fixed at the last minute, and that bugfix may have never been uploaded onto the Atom machine, so don't think about it too much.
In any case, bandwidth utilization for most of these nodes was around 1% to 10% of their link capacity, or something like that during the block propagation period. For the Chinese nodes, it was much lower. We can fix this by using a **[different algorithm]( that (a) doesn't rely on single connections, (b) uses UDP instead of TCP, and (c) does not wait for the full block to be received before beginning uploads.
@_date: 2016-01-20 10:11:05
That's because it's awesome.
@_date: 2016-01-13 23:44:55
Not all of the parameters are chosen, but some of them are. The work-in-progress PR (subject to change) is here:
I think the ACKs are to something that starts at 2 MB, and the principle of using democracy to guide future development. But that's just my guess. There is still some ambiguity in it all.
@_date: 2017-03-14 18:43:02


That's not how it works. By that reasoning, AMD controls about 90% of the Ethereum hashrate, because they make the GPUs that almost everybody uses. 
Bitmain makes the hardware and sells it. They don't control how it's used.
@_date: 2016-01-01 17:11:58
(For the duration of this comment, I will be using "blocksize increase" to refer strictly to a hard-fork increase of MAX_BLOCK_SIZE, because I don't like typing in caps with underscores.)
And yet there are differences between a blocksize increase and a segwit-style increase that makes it useful to distinguish between the two categories.
1. Segwit-style increases have a lot more code complexity which affects all clients, including SPV wallets. Blocksize increases do not. People have been using the term "blocksize increase" for years to refer to a class of modifications that would have the simple code of a blocksize increase. Segwit does not have that simplicity.
2. Blocksize increases have been under public discussion for years, and people understand them pretty well. Segwit has been under public discussion for less than one month.
3. A blocksize increase does not change the relative costs a byte in different parts of a transaction, and can be considered to be economically neutral in terms of fees per byte. Segwit does not have that property as currently proposed.
Et cetera. As a result, I think that the use of the term "blocksize increase" or "effective blocksize increase" to refer to Segwit is a bit misleading. It seems like you see that people have been demanding a blocksize increase for years, but Core developers have not given it to them. Instead of giving people what they have been asking for, Core is now giving them something that is similar, but not the same, and trying to say that it is the same.
This actually reminds me of the time when Gavin proposed 20 MB, and the Chinese miners replied "(no more than) 8 MB", but the "no more than" part was lost in translation, so Gavin responded with "starting at 8 MB", and F2pool responded with "We don't trust you any more."
@_date: 2016-01-01 15:29:48


I think Bitcoin tends to attract people with idiosyncratic paranoias, turning Bitcoin into an echo chamber which amplifies each person's paranoias. Perhaps an irrational fear of the manipulative use of diction is one of mine.
@_date: 2016-01-08 23:40:37


No, we will continue to see empty blocks for as long as the block reward is large. This means at least until 2020 (when the block reward drops to 6.25 BTC), and possibly longer.
Currently, a full block contains about 2,500 transactions, and gives a 25 BTC reward. In order for empty blocks to not be mined frequently, the transaction fees would need to be comparable to the block reward. That means transaction fees would need to be about 25/2500 = 0.01 BTC or $4.50 per transaction. I don't see that happening.
(Technically, we need for the cost of mining to exceed 25 BTC per block, but not exceed 25 BTC + tx fees. Or we need the transaction fees to be motivating enough that nobody bothers with VFM/SPV mining.)
@_date: 2017-03-21 18:38:46


But no miners are actually using the default setting. Having talked to most of the miners and pool operators myself, I can also say with reasonable confidence that nobody is seriously considering 16 MB as the first step. Notably, most of the Chinese pools and miners stated that they will not consider anything above 8 MB. (Gavin and many other English-speakers notoriously misunderstood that letter to mean that the Chinese miners preferred a block size increase that started at 8 MB, but that was definitely not their intention.) I expect the first step to be something **[around 2-4 MB]( Currently, everyone is using EB1AD6, which is 1 MB (unless 6 blocks have been mined on top). ViaBTC recommends **[2 MB]( as the next step, which is probably what will happen. Sure, maybe in 4 years we might want to make a move to 16 MB, but we can talk about that on reddit before making that switch.
The point of BU is to get away from developer-set default settings for the blocksize. The default setting of 16 MB is meant only to make it easy for non-mining full nodes to stay with the longest non-DoS chain; it is not intended for miners to actually use that setting, nor is it a candidate for consensus at the moment.


You're moving the goalpost again. I don't think miners or most BU advocates want single-cent fees. I personally think 5Â¢ to 10Â¢/tx is about right for Bitcoin's current level of development. Based on historical transaction demand, I anticipate that right now we have enough demand to fill around 1.25 MB per block on average, so I expect that 2 MB would be large enough to keep transaction fees around 5-10Â¢ for the rest of 2017, 3 MB for 2018, 4.5 MB for 2019, and a similar growth rate after that. ~~Notably, 6 MB at 20Â¢/kB (or around 8Â¢/tx) gives $12,000~~ $1,200 ~~per block in transaction fees, which is basically as much revenue as miners are currently making. This would allow us to make the transition from mostly-subsidy to mostly-fees for miner revenue at the 2020 halving without a large loss of network hashrate and the possibility for hashrate rental 51% attacks.~~
Even if we had 10 MB blocksize limits, it's likely that many miners would choose not to accept transactions with 1Â¢/tx fees simply because of the marginal orphan risk incurred by those transactions on the 12.5 btc block reward exceeding the fee revenue generated by those transactions. 5-10Â¢/tx (like we had before the congestion began) is fine.
@_date: 2016-01-09 04:13:17
@_date: 2016-01-01 18:43:10
~~Is this a comment from you as a moderator, or as an individual? I'm assuming the latter for now; please let me know if that is incorrect.~~
This is the ~~fourth~~ fifth time that a major entity has been DDoSed within 48 hours of making a public statement in support of a fork. If you want, I can go through the Bayesian analysis of that to justify a "most likely" assessment. Doing that formally would require a few hours of research, though, to get the DDoS rates for companies that have not made statements in support of a fork, which I do not feel inclined to perform rigorously unless you think that is really important.


No, this is not accurate. The regular DDoS extortion comment I made elsewhere was just a piece of information about how BTCC usually responds to DDoS attacks, since many have wondered if BTCC would cave in like Slush did as a result of this attack. The link I gave was to a thread in which Samson Mow confirmed that my [paraphrasing]( of their comment was accurate and not a fabrication. I can see how that would have been confusing to you, and should have posted the more direct link. Edit: done.
@_date: 2016-01-17 07:27:56
Your replies are combative and often not directly related to the posts you are replying to. I would appreciate if you attempted to engage in civil discourse.
@_date: 2019-03-15 05:08:22
That statement is equivalent to 1 &gt; 2, which is wrong.
@_date: 2016-01-01 19:47:51


This was being done on mainnet with the mainnet UTXO. No, this was not using adversarial random access (which shouldn't punish SSDs or RAM too heavily), but it also wasn't using adversarial hard-to-verify scripts, either. My understanding is that adversarial conditions worsen script validation performance much more than UTXO validation performance.
Can you refer me to any benchmarks done by yourself or others that show UTXO performance along the lines you are describing? I would be interested in reading about this.


Agreed. I want to see a proper validation cost metric. I am saying that according to the data and analyses I've seen, script validation is about 10x to 100x slower than UTXO validation, and so I expect a proper validation cost metric would weigh script bytes more heavily than tx bytes, which SegWit seems to have backwards. I defer to better data if you can present it.
@_date: 2016-01-01 18:56:01


scriptsigs are computationally more expensive than UTXOs. scriptsigs affect miner profitability (via validation time) more than UTXOs. UTXOs require more memory than scriptsigs. I think the case can be made in either direction. In terms of fees (i.e. payment to miners), I think that verification time is the more appropriate metric.
@_date: 2019-03-15 07:44:55
There is no "2". You only think there's a two because you're used to Ï, and Ï's value is 1/2 of the correct value of the circle constant, Ï. 
[e^(iÏ) = 1](
@_date: 2016-01-25 13:53:46
Mike Toomim is not a developer of Bitcoin Classic. He just built part of consider.it, a platform which Bitcoin Classic has been using to visualize user and developer support for various proposals.
I know he has the same last name as me, but he is not the same person. I, Jonathan Toomim, am the Bitcoin Classic developer. He is not.
@_date: 2019-03-16 16:21:07
They're an exchange. They earn money when people exchange currencies. What's the big deal?
@_date: 2016-01-04 01:48:56
Slashdot reference.
@_date: 2019-03-26 05:51:55
No, it doesn't. It has about 46 exahashes/sec, not 60 EH/s. Perhaps you're looking at an estimate of the hashrate that's based on a very short time interval, like 1 day? There's huge statistical fluctuation in short intervals. You should really be using data averaged over 2 weeks at least, like 
@_date: 2017-03-20 15:52:38


Probably n log(n). Still a few seconds, and plenty fast enough for non-mining nodes.




Pruning solves that. With pruning, 16 MB blocks would usually add around 20 GB to the pruned DB size per year. Also, it's possible to store the blocks on an HDD but the chainstate/utxo set on SSD. I do this on all of my servers. But most full nodes don't need the full blockchain history, especially not on an SSD.
Non-miners don't need to use an SSD at all. It's only miners that are latency-sensitive on the order of milliseconds or seconds.


I believe in making Bitcoin as helpful as possible to as many people as possible. If we make a change that allows capacity for 5 new users of Bitcoin while pushing off 1 old user of Bitcoin (onto SPV wallets?), I consider that a win. 


Just responding to the context. The grandparent post was talking about 32+ GB of RAM, which I think means that he was talking about mining nodes. Only mining nodes need that much RAM, because only mining nodes are particularly latency-sensitive. (RAM helps for initial chain sync too, but it's not necessary there.)
@_date: 2016-01-25 14:48:21


"Mental manipulation" -- you mean by arguing publicly for one side? I don't see how that's a problem.
Or do you mean by posting a misleading, incomplete, or incorrect proposal?  In that case, I would post a corrected one, and ignore the results of the incorrect one. It's also worth noting that people should vote against any misleading, incomplete, or inaccurate proposals, as per our voting guide.


What if the Democrats conduct a voter registration drive in predominantly African American neighborhoods? What if the NRA publishes a note in their magazine encouraging people to vote against Proposition 99?
Again, it's not as if a machine is interpreting the results. They're being looked at by humans like me. If something looks funny in the results, we devs will apply the salt shaker before acting. We may even disregard the vote entirely. Condiser.it is just there to advise the devs.
@_date: 2017-03-04 15:32:47


An equally valid statement:


Ultimately, a UASF is a game of chicken in which each side blames the other if an accident is the result. "You were supposed to swerve!" -- "No, *you* were supposed to swerve!"
@_date: 2019-03-14 19:31:00
[No. You're making excuses for pi.]( Mathematics should be as elegant and simple as possible. 
We don't make circles using a diameter; we make circles using a radius. The length of the radius is the fundamental thing that determines the circumference of a circle. So why would we define the circle constant as a ratio of the diameter to the circumference?
@_date: 2016-01-01 15:13:34


Not a bad idea. You could even do a one-off vote right now.


That was Alex Petrov of Bitfury. To clarify, he was expressing discomfort with going beyond 2 MB before we know it's safe. If you look at his actual vote on the 2-4-8 proposals, it was ["moderated ok. require changes asap"]( (i.e. changes to improve performance, before we get close to the 4-8 MB range). I do not think that qualifies as "strong opposition."


That's an amusing way to describe your motivation.
@_date: 2016-01-14 22:06:30


"Mob mentality" is another way of saying "democracy you disagree with." 
Unless you're referring to the **[tyranny of the majority]( Yes, the tyranny of the majority is a common concern in democratic systems, and is the reason why the USA has the Bill of Rights and a system of checks and balances. Those are important features. In Bitcoin, there is a fundamental protection against the tyranny of the majority built into hard forks: if you disagree with the new version, you can ignore it. (Soft forks intentionally break this safety valve. You can't ignore a soft fork.)
Currently, the largest supermajority requirement used by the USA is for amending the constitution, which must be ratified by 75% of all states. Nothing in the USA can withstand a 75% supermajority. In most other contexts, a supermajority only means 66%, and even that is rarely used. 75% is already a very high bar. 95%, from a political perspective, is ridiculous.




You mean like NoXT? The game theory doesn't support that as an effective method at an activation threshold of 75% or above. If you have 26% of the hashrate available for an attack, and another 25% opposes the fork honstly, you could cause a hard fork to happen with less than 50% actual support, but you could also completely block it from happening by voting against it. If you have an actual hashrate majority opposed to the fork, why not simply oppose it?


What you're saying is that if it's at all contentious, it effectively shouldn't happen. I disagree. I do not believe in **[liberum veto]( or minority rule for Bitcoin.


If you think Bitcoin is being hijacked by a &gt;= 75% supermajority of miners, you can stick to the small-blocks branch. That's fine with me. That branch will have value proportional to what other users think of its utility. If it's being hijacked for the worse, then the old branch will retain a lot of value, and potentially more value than the new branch. If that's the case, then the miners who try to hijack Bitcoin will have trouble paying their electricity bills. If you're wrong, then you'll still have your bitcoin on the large-blocks branch.


Prevalent among a small minority of users (who happen to mostly be developers) with whom a supermajority of users disagree. Who gets to decide that 5-10% is the right threshold? Is it the 10% that oppose a blocksize increase? So we should use 5% as the threshold because 10% want to use that threshold to prevent something they don't want from happening?
Minority rule is wrong. If you want to use a cryptocurrency with different parameters from the vast majority of other Bitcoin users, you are free to do so. There are these things we call "altcoins" for that. 
If you want to transact with a majority of users, then you have to use the same rules that the majority of users decides to use.
@_date: 2016-01-25 14:02:12
Not if you filter to only verified users.
@_date: 2017-03-02 07:16:37




No, this is a minor problem. There has been one such transaction in the history of bitcoin for which quadratic scaling proved to be a significant problem, and its creation was a mistake. Furthermore, sighash scaling is a problem with increasing the maximum *transaction* size, not the maximum *block* size.
Importantly, SegWit does nothing to mitigate the severity of the existing attack, which can stall nodes for about 3 minutes by requiring nodes to hash about 18 GB of data. This attack has been known for nearly 4 years, but has never been performed on mainnet, which suggests that current incentives do not support its abuse.
Both BIP101 and BIP109 included new rules that mitigate the existing attack to be no worse than around 13 seconds (1.3 GB of hashing), making them about 14x better than both SegWit and the status quo in terms of the worst case sighash scenario, even for BIP101's 8 MB blocks.


Large transactions can already be generated without running into sighash problems by using SIGHASH_ANYONECANPAY instead of SIGHASH_ALL. SegWit just provides a second way to create large transactions without running into quadratic hashing issues. While that is helpful and appreciated, it is not a game-changer. 
@_date: 2016-06-30 18:19:00
This article is an astroturfing maneuver intended to make it easier for MGT to raise money. 
As a miner in Washington State, I can tell you that there are several mines within 100 miles of me that are of similar size or larger. Most of them prefer to remain anonymous and secret because they're privately funded, and therefore not looking for funding from the public.
@_date: 2016-06-08 04:57:02




This is incorrect. The 2MB HF only needs 75% adoption, and it only needs that adoption among miners (plus the passive consent of users). On the other hand, SegWit will need 95% adoption among miners, and in addition will need users to migrate their wallets over to new software.
The rest of your message appears to be a straw-man argument, so I will not bother to respond to it.
@_date: 2016-03-03 21:58:09
Because he asked them, silly.
@_date: 2016-01-16 08:55:16
At Blockstream, it's mostly just Rusty.
@_date: 2016-01-08 23:32:39
Fee revenue needs to be about $3000 per block to pay for our current mining infrastructure. 
With $0.16/kB (the current fee levels), we can pay for mining with 20 MB blocks. 
In order to pay for mining with 1 MB blocks, we would need $3.00/kB in fees. 
Generally, for most businesses, it's easier to increase revenue by increasing volume than it is to do so by increasing fees. I think it will be much easier to increase volume 20x while keeping fees constant than to increase fees 20x while keeping volume constant. 
Low, default fee settings are sufficient to pay for mining if volume is large enough.
@_date: 2019-03-22 13:08:20
If you switch between pi and tau, no physical objects need to be changed. It's not like metric/imperial. The only thing you're changing is the labels. One quarter of a turn is still 1.5708 radians; it's just that we call it tau/4 instead of pi/2.
If you want to write code that uses tau, all you have to do is something like
    
    from math import pi
    tau = 2 * pi
at the beginning of your file.
The hardest thing to change is your mind.
@_date: 2019-03-15 10:01:15
Question | Formula
Distance fallen over time| y = 1/2 gt^2
Spring energy versus displacement| U = 1/2 kx^2
Kinetic energy| E = 1/2 mv^2
Area of a circular sector| A = 1/2 Î¸r^2
Area of a full circle| A = 1/2 Ïr^2
All of these equations have the same form because they're all derived from integrating a linear equation. The integral of *kx dx* equals *1/2 kx^2*. Using *c = 1/2 k* instead so that you get *cx^2* may seem superficially more elegant, but it hides a coefficient of 1/2 that's real and actually there.
Chances are, you memorized the formula for the area of a circle (A = Ïr^2) separately from the formula for the area of a sector of a circle (A = 1/2 Î¸r^2), but in reality, they are the same formula. For a full circle's area, what you're doing is evaluating the area of a sector whose angle is one full turn, Î¸ = Ï, so A = 1/2 Ïr^2. Using Ï instead of Ï turns that into A = 1/2 (2Ï)r^2, which got reduced to A = Ïr^2 before it was taught to you. This makes it seem like a separate formula, but it is not.
One formula for all circumstances is simpler and more elegant than one formula for the special common case and another formula for all other cases.
@_date: 2016-06-18 15:44:10
It means that the signature isn't valid. This is a forgery, and not even a particularly good one.
@_date: 2019-03-14 18:39:03
I only celebrate June 28th.
@_date: 2016-06-18 07:53:55
While funny, it's not accurate. It's not a bailout; it's a reversal of a theft that occurred due to a bug in code. If we're using the government analogy, this is like the FBI tracking down a bank robber and returning the money.
Another inaccuracy: if this is done as a soft fork (as I expect it will be), then the action will be made by the miners who choose to run the soft fork code, not by the devs who write it.
@_date: 2016-06-28 01:55:19


Nothing can be safely done without new limits similar to BIP 109. Even doing nothing is unsafe. 
**[CVE-2013-2292]( is a vulnerability. A 3 minute verification time is a problem for Bitcoin. Bitcoin is (slightly) unsafe until that vulnerability is fixed. 
The only ways to deal with it are to forbid legacy-style transactions (which is untenable) or to introduce new (absurdly high) limits on them similar to BIP109. As SegWit does neither, it does not fix the vulnerability. The worst-case verification time with SegWit is still 3 minutes, versus about 10 seconds for BIP109. Whether or not we increase the base block size away from 1 MB, we should still fix CVE-2013-2292.
@_date: 2016-03-11 06:50:20
It starts at 1 MB. It is extremely slow. It starts on Jan 1st, 2017 with 1.177 MB, and wouldn't exceed 2 MB until Jan 1st, 2022. 
Most of the miners do not like it.
It was not very popular among users, either:
@_date: 2016-06-08 03:00:09


SegWit is not 2.0 MB. Even with instantaneous 100% adoption, it would only be **[1.8 MB]( Conversion over to SegWit-based wallets and addresses is not likely to exceed 50% for at least 1 year, so it will likely be less than 1.4 MB for 2016 and much of 2017. Less than 1.4 MB isn't acceptable to me. Less than 1.4 MB is also not "essentially the same size as a 2 MB HF."
@_date: 2016-06-08 22:37:33


No, I did not say that. I said that less than 1.4 MB is unacceptable. That does not mean that 1.4 or higher is acceptable to me. I consider 1.4 MB to be lame and anemic.
I think I'm going to block you too. Engaging with you has been unpleasant and a waste of time.
@_date: 2019-03-15 11:18:54
That's a terrible SAT question. All of the potential answers are equally justifiable.
1. g is the ratio of the velocity of the object divided by the time it has been falling for. It's also a measured physical characteristic property for a typical location on our planet's surface. v=gt
2. k is the ratio of the force exerted by a spring divided by the displacement of the spring from the spring's neutral position. It's also a measured physical characteristic property of any specific spring. F=kx
3. m is the ratio of the force exerted on an object divided by that object's acceleration. It is also a measured physical characteristic property of any specific object. F=ma
4. Ï is the ratio of the circumference of a circle divided by its radius. It is also a measured physical characteristic property of any circle. C=Ïr
All four are proportionality constants. What makes Ï unique on that list is that it's a constant that is the same for any circle, whereas the other proportionality constants take different values for different objects. But the equations take the same form regardless.
(Btw, I ninja-edited the immediate parent post to add 1.5 paragraphs and change the table formatting.)
@_date: 2016-06-30 22:43:50
The Chelan PUD likes to subsidize local industry with low power rates. Bitcoiners don't provide many jobs.
@_date: 2017-08-30 16:53:29
No, it's not wallets. It's block rate:
We're getting about one block every 8 minutes right now. That increases capacity, which makes transactions confirm way faster and cheaper.
@_date: 2017-08-18 21:46:14
The accelerated method is going to be difficult and/or expensive on the no2x because of the low block rate and congestion. It might be a good idea to consolidate your wallets into a single UTXO before the fork by sending all your money into a single address in a single transaction. That way, you only have to pay the pre-fork fees on most of the transaction kB, rather than the post-fork fees, which will be very high on the no2x chain (but much lower on the 2x chain).
@_date: 2017-08-28 17:34:41
Since Slush is one of the only pools not signaling NYA, it stands to reason that the miners who choose to mine with Slush will disproportionately be people who prefer not to mine NYA. People who prefer to mine NYA probably mostly left Slush a while ago.
@_date: 2016-06-30 18:23:42
Disagree. It's medium-small.
It's hard to say exactly how big their planned facility is, because it's unclear what type of machine they're planning to use. Chances are they're planning on using Antminer S9s or similar. 10 PH/s of S9s would only use 1 MW of power. I know of two facilities in my small town (pop: ~20,000) that have 2 MW each, and my facility should have 2.25 MW of capacity by the end of the year.
On the other hand, if they're planning on getting 10 PH/s of S7s, that would be about 2.8 MW for 10 PH/s. That would qualify as medium-scale, in my book.
Lots of the Chinese mines are on the order of 10 to 30 MW each. I think HaoBTC has about 20 MW at around 0.3 J/GH, for example.
@_date: 2017-01-16 04:55:19


The fee market.
This is what happens when you have a market with completely fixed supply (1 MB blocksize limit) and low [demand elasticity](
@_date: 2017-01-25 21:54:21


Yes, there's a non-zero centralization pressure. Yes, inflation can help to counter this. My opinion is that the centralization pressure is not quantitatively significant for either Bitcoin or Ethereum even at the 20 tx/s level. In my opinion, the factor that is limiting Bitcoin's capacity is not centralization pressure, but rather the fear of centralization pressure. That fear factor is less relevant for Ethereum than for Bitcoin.
Thus, Ethereum has better scalability for simple payment transactions mostly because it doesn't have the excessively low that Bitcoin does.


Ethereum can encode transactions included in a block using a 4-byte or 6-byte truncated hash just as easily as Bitcoin can. There just isn't yet enough reason to do so. 
Compact blocks give a proportionally larger benefit for Bitcoin than for Ethereum because the constant light-speed latency is proportionally smaller for Bitcoin than for Ethereum. However, this does not affect the additional orphan cost per byte of transaction size, which means that it is not relevant to scaling. Ultimately, the tradeoff is simply one of baseline centralization incentive (for a 0-tx block) versus transaction confirmation times. 
Speaking of which, I'm sorry for implying that the 14 second block time is a reason why Ethereum can have higher transaction throughput than Bitcoin. It's ultimately irrelevant. I mentioned it because most people think of transaction throughput as the product of the block rate and the block size. If I were being more accurate, I would have lumped the adaptive gas limit and the block time into a single point. I just couldn't think of a way to do that in a succinct and readable fashion in my original comment.


I mean that they put an accurate, but very small, price on storage space. Bitcoin currently puts a negative price on storage space. It's incentive-compatible because the real cost of storage space is very small, and smaller than the price charged in terms of gas.
@_date: 2016-01-01 18:24:47
A direct increase gives us 2 MB of useful space with 2 MB of adversarial condition space. A SegWit increase gives us 1.6-2.0 MB of useful space with 3.9 MB of adversarial space. Does that loss of headroom not matter to you?


My understanding is that the main benefit of Segregated Witness on Lightning is from the transaction malleability fix. That does not require the 4 MB adversarial limit at all. 1 MB would be enough. As I understand it, you would prefer no blocksize limit increase at all. Why not a SegWit with 1 MB?
The other benefit of SegWit on Lightning is that SegWit changes the accounting rules for transactions to subsidize complicated p2sh scriptsigs, such as the ones used by Lightning. This subsidy comes as a result of the 4 MB adversarial limit, which is a result of discounting SegWit scriptsig bytes to be counted as 1/4 as much as a regular tx byte. Do you think this subsidy is a desirable attribute? Is this why you are willing to overlook the 4 MB adversarial condition?


Yes, I read it. I'm curious if you think 4 MB is actually an unresolved flaw.
@_date: 2018-10-24 02:30:44
"When used as numbers, byte vectors are interpreted as little-endian variable-length integers with the most significant bit determining the sign of the integer."
(That's from 2013)
The scriptsig field is a byte vector which was hacked to store that plaintext message, and that's why the order is reversed.
If you think that this is actually a second copy of the "The Times 03/Jan..." message which is a reversed Easter egg, then tell me: Where's the other copy?
@_date: 2017-01-31 16:16:31


Bitmain isn't the reason why we have miner production centralization. Economies of scale are the reason.
A substantial portion of the cost of a single mining ASIC is R&amp;D costs. The amount invested in R&amp;D also substantially influences the efficiency and performance of each mm^2 of silicon. The more chips you produce, the less the R&amp;D per chip. Thus, companies who produce large numbers of chips can afford to invest more total in their chips (and make better chips) while also spending less per chip. Having one ASIC manufacturer (like Intel or Bitmain) for a given market that is much larger than the competition is almost guaranteed in this situation. 
Your "solution" of changing PoW functions also wouldn't have the effect that you're thinking. Bitmain has already sold almost all of the hardware that they've produced. They've already made all the money off of those sales that they will ever make. If you change the PoW function, you won't be punishing Bitmain itself; you'll be punishing Bitmain's customers. **Bitmain itself would actually *benefit* from a PoW function change**, as the whole mining industry would have to replace all of the miners that you just made obsolete. Bitmain already has the design expertise and manufacturing infrastructure in place; all they would have to do is design a new ASIC for the new PoW function and push it through their production line.
The true solution isn't to change the PoW function. The best options we have are either to switch to Proof of Stake or to accept that the level of decentralization that we can get from PoW is good enough for our needs. (51% attacks aren't exactly a common problem with Bitcoin, having been performed exactly 0 times so far.)
@_date: 2017-08-21 19:30:35
Where in the NYA does it say the point was to not fork bitcoin? I thought the whole point of the NYA was to increase the blockchain capacity. It seems that if the purpose was to avoid forking Bitcoin, it would be silly for the agreement to include a hard fork. 
Here's the [main text}( of the NYA:








Doesn't seem to mention avoiding a fork. Does mention causing a fork. Does not mention exclusivity.
@_date: 2017-08-20 21:52:23
I edited my comment around the same time as you posted that one. Check the parent for the answer. For clarity to others, if you delete your question, I will delete this response.


@_date: 2017-08-26 08:00:40
And Coinbase, and Bitfinex, and Bitfury, and Blockchain.info, and Bloq, and Bitcoin.com, and BTCC, and Coins.ph, and F2pool, and DCG, and Circle, and Jaxx, and Purse.io, and Shapeshift, and ...
@_date: 2017-01-25 16:38:35
1. Ethereum transactions cost about 0.2Â¢ each. That's a cheaper alternative than Bitcoin, and also much faster.
2. In the United States, an ACH transfer costs 25Â¢. That's cheaper than Bitcoin right now, although much slower.
3. PayPal, VISA, and MasterCard all offer consumer protection, which Bitcoin does not. Much of the fees that those services charge are due to the costs of occasionally needing to mediate chargeback requests. It's not entirely fair to compare dissimilar fees for dissimilar services.
4. On the other hand, Western Union and Moneygram fees are simply worse than Bitcoin right now for a comparable service.
Overall, Bitcoin's fees are better than some services, and worse than other services. It is not "still significantly lower than the alternatives", but merely better than *some* of the alternatives.
@_date: 2017-08-30 22:27:10
But this was a period of fast blocks due to a difficulty reduction due to slow blocks during the previous adjustment period due to design flaws in a competing blockchain's difficulty adjustment algorithm. It's an oscillation.
@_date: 2017-01-16 08:40:12
Multibit HD is very different from Multibit Classic. The message you're describing ("do not send BTC to this address as you will be unable to retrieve them") sounds like the message that HD gives you during the migration assistant. Make sure you downloaded Classic 0.5.19.
If you are indeed still using Classic, and the Send button is grey, that may be because it stalled during the block download/update process. You can try View -&gt; Messages to see what's going on, or you can try closing and restarting MbC to see if it starts to work better with different peers.
@_date: 2017-08-26 01:43:34
Is it really so much to ask for people to answer data with data?
@_date: 2017-01-31 15:40:26
Note: I am an industrial-scale miner. I have purchased hundreds of thousands of dollars worth of Bitcoin miners. 
But I'm sure you know more about this than I do.
@_date: 2016-06-30 19:40:21
Nah, Wenatchee is for chumps. (The Chelan PUD hates Bitcoiners.)
@_date: 2017-01-25 22:04:31


No, this is a common misconception. The number of full nodes on the network is entirely irrelevant to security. In terms of decentralization, there are only two things that matter:
1. That YOU either can run a full node of your own, or have access to a full node that you can trust, and
2. That miners are numerous, small, and distributed enough that they cannot practically organize or be manipulated to perform 51% attacks.
It doesn't matter how many other full nodes a block passes through before reaching your full node. Your full node can verify or reject that block with perfect accuracy by itself.
There is a third reason why the number of nodes almost matters: DoS resistance. If someone wants to shut down a cryptocurrency network by DoS attacks, it can be helpful to have a large number of nodes in the network in order to dilute the attack. However, these nodes do not need to be full nodes, as they don't actually need to verify transactions or blocks; they just need to forward them. The hardware requirements for these dumb "half-nodes" are minimal for everything except bandwidth, so DoS protection of full nodes is very cheap if that's your only priority.
@_date: 2017-01-31 13:17:33
If you do batches of transactions like this every Monday, and the difficulty changes every 13.5 days on average, you will eventually have coincidence of the difficulty adjustment and a batch of transactions.
@_date: 2017-08-20 06:56:06






Of course. Water doesn't disappear. Conservation of mass and energy etc. Technically, dams make a little more water get lost from evaporation and seepage into the water table, but those are generally insignificant in most dams on large rivers.
When water flows slow and deep, there's less friction because the average molecule is farther from the banks of the river, and the velocity gradient in the water is weaker, so the average water molecule mostly "sees" adjacent molecules moving at the same speed, and rarely sees obstacles like rocks.
Because there's less friction, the water gets to its destination with more non-thermal energy. Energy in a fluid comes in three forms: potential energy (i.e. being high), kinetic energy (moving fast), and pressure. If you have a frictionless fluid moving in a pipe of varying height and diameter, you'll find that [the sum of those three energy types equals a constant]( In a dammed river, that means that the dam causes the water to flow slow and deep to the dam, thereby avoiding friction, accumulating potential energy, and arriving at the dam with more height for the surface of the water (or more pressure at the riverbed). You then put a turbine and generator in the dam, and as water crosses the dam, you convert the extra water height and pressure (the "head") into electricity.
The theoretical power generated by a dam equals the head (the height difference of the water on the two sides of the dam) times the flow rate times gravity times the density of water. If you have 10,000 cubic meters per second of water flowing across a 10 m dam, and if gravity is 10 m/s^2, and if the water's density is 1000 kg/m^3, that means you get 10,000 m^3/s * 10 m * 10 m/s^2 * 1000 kg/m^3 = 1,000,000,000 kg*m^2/s^3 = 1 GW.
@_date: 2017-01-16 04:44:01
That is correct. The author made a special update to allow fee adjustments when it became clear that blockchain congestion was going to be an issue and when rhetoric about fee markets started to become popular. The author didn't forsee how much of an issue it would be, though -- the fee slider maxes out at 50 sat/byte, which is less than the 80 sat/byte fee currently recommended by  for a 1-block confirmation.
@_date: 2017-01-31 14:32:32


This may be because there was an unmet demand in the mining ecosystem for BU-based pools. Perhaps a lot of small- and medium-scale miners wanted to support BU, but couldn't do so with the existing pools, so that when new pools were formed, a substantial chunk of the network hashrate switched to them. Old pools may be especially reluctant to switch to BU because they think it might be viewed as a betrayal, and cost them a lot of hashrate from the Core supporters amongst their customers, but new pools are more concerned with attracting new customers than they are worried about pissing off non-customers.
@_date: 2017-08-27 19:17:29
Fee estimation is based on the recent past. Transaction confirmation times depend on the near future. When the future is unlike the past, the fee estimation algorithms make mistakes.
If you send a transaction in the early morning (European time) on a Monday, your wallet will look at the network traffic Sunday night, see it was low, and decide that a low fee is necessary. Then, as the Monday daytime traffic ramps up, the requisite fee will increase, and the fee recommended to you by your wallet may turn out to be insufficient.
If you send a transaction late on Friday night, then your wallet will see the high Friday daytime traffic, and will assume that a high fee is needed. As traffic slows down for the evening and the weekend, the requisite fee will fall, and you will end up overpaying.
Ultimately, when blocks are congested with e.g. 10% more transactions than there is block space, you need to pick a fee that is higher than e.g. 10% of other transactions. The hard part is that everybody else, including that 10%, wants to do the same thing. Everybody and their wallet is trying to guess what everybody else is trying to guess that everybody else is trying to guess (etc) that the optimal fee will be. Economists call this scenario a [Keynesian Beauty Contest]( There is no optimal strategy for this problem, because as soon as other people start to use one strategy, the best strategy to use becomes one that takes their strategy into account.
Most wallet developers consider unconfirmed transactions to be a worse user experience than high fees, so they use default fee settings that make the unconfirmed scenario unlikely. In any case, it's a trade-off. Unless you can predict the future behavior of other wallets better than the other walelts can, the only way to achieve confirmation certainty is to overpay. 
Ultimately, it's not a solvable problem. It's a competitive guessing game. No matter what, fee estimation algorithms will result in some people's transactions being left unconfirmed, and other people's transactions paying too high of a fee.
@_date: 2017-01-31 16:30:03
It is strange that you're alleging that Bitmain is favoring Bitcoin Unlimited buyers when both of Bitmain's own pools, Antpool and BTC.com, are mining with Bitcoin Core.
@_date: 2017-01-25 15:22:00
This ends either when people like you stop using this peer-to-peer currency, or when we fork to increase block capacity.
@_date: 2017-01-25 21:25:05


The absence of bugs is what prevents attacks, not safety margins. The attacks were ineffective after the bugs were fixed, and stopped shortly afterwards. Are you criticizing the current Ethereum network, or the old obsolete one in which gas costs were calculated improperly?
Ethereum has a much larger attack surface than Bitcoin because it is Turing complete. If you want to criticize Ethereum for having a large attack surface due to its (overly?) ambitious goals, that is an entirely valid criticism. However, that is not related to Ethereum's scalability features.


Valid, but again, the current clients are *way* faster than Olympic. On my laptop, Parity is around 2x-10x faster at processing new blocks as Homestead cpp-ethereum is (depending on the type of block, and whether it was a DoS block or not). I wouldn't want to do 25 tx/s on mainnet with Olympic-era clients, but with Parity and modern versions of Geth, I think we could handle it just fine.


That's why I said 100 ms. A 14 second block time is 14,000 ms, so 100 ms is 140x faster than the actual block time. If I were neglecting catchup and propagation times, I would have said that Ethereum can handle around 9520 tx/s. But I didn't say that, because that would be silly.


If you put it into the formula that ignores the adaptive gas limit, then yes, you get a figure of 13.6 tx/s. But the gas limit is adaptive. After a few hours of transactions at the limit, and without manual intervention to the contrary, and the gas limit would increase substantially.
Vitalik has **[suggested]( a 5.5 million gas limit as a good target for the near future with an acceptable safety margin. That gives a limit of 18.7 tx/s.
I think the 15 tx/s number is just a rough estimate of the limit because there is no hard limit, and nobody actually knows what the safe limit is, and people can't agree on what a safe-enough limit would be. So I suggest we not nitpick it, and we definitely shouldn't worry about 3 significant figures of precision.


A typical DAO transaction is something that Bitcoin simply isn't capable of. If you're comparing the effect of aluminum vs. steel on vehicle weight, you shouldn't compare a steel car to an aluminum airliner. Steel simply isn't practical for flying. Comparisons should be for two transactions that accomplish the same purpose on different platforms.
Even so, your DAO transaction only costs 5x as much as a simple payment transaction, which is comparable to the scalability advantage that Ethereum has. If 10% of transactions were 100k gas, Ethereum would still be able to handle several times as many transactions as Bitcoin.


An entirely valid position. Enjoy your $0.25-$1.00 transaction fees.
@_date: 2017-08-28 00:48:42
In addition, the sum of BTC + BCH post-fork is quite a bit greater than BTC was pre-fork, so even people who aren't mining are benefiting from this fork.
@_date: 2018-10-12 16:45:29


That's where you found the message, and that *is* the coinbase message. It's the same message and the same place.
@_date: 2017-01-25 15:59:05
On the Olympic testnet, Ethereum **[actually reached 25 tx/s]( with old (slow) client code. This exceeds your calculated theoretical limit of 12.7 tx/s, so one might suspect that your calculations are wrong. Here are a few issues I found with your numbers:
1. The gas limit is adaptable. As usage increases, so does the gas limit. Each time a block uses most or all of the current gas limit, the gas limit for the next block increases. The developers have [declared]( that a gas limit of 5.5M is currently acceptable for the main network. However, that includes a large safety margin. My parity-based Ethereum full node processes transactions at a rate of around 400 Mgas/second. My hardware is probably 2x as fast as most other nodes. If we assume that the typical node should be able to process a block in no more than 100 ms, that would mean blocks could be 0.5 * 400 Mgas/s * 0.1 s = 20 Mgas in size while only using 100 ms of processing time on medium-speed full nodes. Based on that, we could theoretically handle around 68 tx/s.
2. The ethereum block time is 14 seconds, not 15 seconds. It changed at the Homestead fork on [March 14, 2106](
3. Bitcoin transactions usually require more than 1 input. In the most recent block ([449953]( the average transaction had 2.06 inputs and 2.07 outputs. Inputs take up a lot more space than outputs (about 100 bytes vs 23 bytes). Consequently, 7 TPS for Bitcoin is not realistic. On Ethereum, due to the use of accounts instead of UTXOs, the minimum, median, and mode transaction gas cost is exactly 21,000, whereas with Bitcoin, the use of UTXOs makes the average transaction size about 2x as big as the minimum transaction size.


With SegWit, we might get 5.5-7 tx/s. With Ethereum and no soft or hard forks, we can already get more than twice that, and probably about 10x that.
@_date: 2017-08-30 21:48:58
Both 8 minutes and 14 minutes are anomalies.
@_date: 2017-01-28 01:06:41
parity --warp just works.
@_date: 2017-08-19 23:08:56
Not necessarily. Facilities like mine are renewably powered. I use hydroelectricity, for example. When we extract power from the water flow in the river, we reduce the amount of friction that occurs in the flow of water in the river by exactly the same amount as the electrical power that's extracted. Rivers without dams flow fast and shallow; rivers with dams flow slow and deep. Instead of directly heating up the river (and eventually, the ocean), we're pulling the energy out and using it to heat the air.
Of course, the greenhouse effect of CO2 from the non-renewably powered Bitcoin mines is a much larger source of global warming. I'm just trying to point out that the type of electricity generation that is the problem, not the electricity consumption.
@_date: 2017-01-08 13:46:03


If you look at just the last week, and align the Bitcoin exchange rate to the CNY/USD rate, there does appear to be a slump in Bitcoin shortly after a jump in CNY/USD:
Enjoy your rest.
@_date: 2017-01-31 19:27:42


That allegation can be disproven by [visiting their website]( and placing an order.
@_date: 2017-08-20 20:41:12
I allow my users to choose which branch of code they want to use, with the recommendation that they use the same client as is used by [90.3% of the network hashrate]( My approach forces users to read about the issue and make a decision, but it does not make the decision for them.
You force your users to use the same client that's used by the &lt;= 10% hashrate minority. Your approach allows people to download Bitcoin Core without being aware of its impending incompatibility with the majority of the Bitcoin ecosystem, and actively prevents users from using btc1, even while they're compatible. This is a political activist approach, and removes freedom from users.
@_date: 2017-08-21 00:26:34


No, I don't. I've just been maintaining my own fork in my own github.
I started doing that when I found a bug in p2pool that caused it to mine blocks that were about 500 kB on average, even when bitcoind was configured to mine blocks that were 999 kB. I fixed that bug and a bunch of other bugs, and the results have been in my repository for a few months.
@_date: 2017-08-18 18:48:52
You want to make sure that the transaction doesn't confirm on the congested chain. This is harder than making sure that a confirmation doesn't get reverted. Waiting until the transaction has been removed from the mempool on the congested chain will help. Otherwise, there's a chance that subsequent transactions you make will cause the root transaction to confirm via Child Pays For Parent (CPFP) on the low-throughput chain, especially if you aren't careful to keep your fees low.
You can accelerate this process by sending a transaction with a low fee to address A, then waiting for confirmation on the high-throughput chain. After it's confirmed on the high-throughput chain, you open a wallet bound to the low-throughput chain, and double-spend that unconfirmed transaction with a high fee to address B. If this works, you'll have your funds in address A on one chain and address B on the other. If it doesn't work, you'll have all your funds in one of those addresses on both chains and you can try again with different fees on each chain.
@_date: 2016-06-08 05:10:47


Most people use SPV wallets. An SPV wallet will connect to multiple nodes, and will select the heaviest/longest chain among those reported by that set of nodes. As such, most users will follow the heaviest chain even if only half of full nodes upgrade. A person who wishes to follow the 1 MB chain and who uses an SPV wallet has to actively reject the 2 MB chain, such as by controlling the full nodes that his SPV wallet connects to, or by switching to a full node wallet.
@_date: 2017-08-26 00:46:41
Can you respond with something other than ad hominems, please? Ad hominem attacks do not qualify as civilized debate. They qualify as trolling.
Incidentally, I don't even have a Bitcoin Cash wallet set up yet.
@_date: 2017-08-26 23:00:36
Dishonest and sensationalist headline.
Headline: "Jihan Wu of Bitmain **Confident** that Bitcoin Will be Valued $100,000 in 5 years"
What Jihan actually said: "Bitcoin price going up to $100,000 is **quite possible**â¦.in 5 yearsâ.
Saying that something is possible is very different from being confident that it will happen. Joshua Althauser, you should be ashamed.
@_date: 2017-08-26 00:19:33
I downvoted because of 1). The size of the mempool is the size of the queue of transactions waiting to be confirmed. Just because it's full doesn't mean it's getting spammed. 
Imagine a cash register line at a 24/7 supermarket. If you have five cashiers (each of whom can clear 20 customers per hour) and you have an average of 90 customers per hour enter the line, the line will be mostly empty. If you have one of those cashiers leave, then the line will get huge, growing at a rate of 10 customers every hour. After a few days of this, the line will have hundreds of people in it. That's what we're seeing with Bitcoin. The hashrate fell 20%, block times increased 20%, and block space supply fell below demand.
If you check the rate at which transactions are being added to the mempool, you'll see that it's about the same as or slightly lower than it was 2 weeks ago when the mempool was empty. The only thing that has changed was the rate at which transactions are being removed from the mempool and confirmed into blocks:
You're right that it wasn't a coincidence, though. Bitcoin Cash was created by people who didn't like Segwit, so they timed the release of Bitcoin Cash for Segwit's triggering date. Bitcoin Cash took a few weeks for the difficulty to adjust to the point that it had revenue parity with the Bitcoin Core chain. Segwit also took a few weeks for triggering to turn into activation.
As for 4), just because people disagree with you does not make them trolls. Trolling is when people specifically use polarizing comments and work hard to incite negative emotions. The term for people who are level-headed and diplomatic about how they disagree with you is "civilized debate". It's healthy, and it's something that we haven't seen on Reddit for a while.
@_date: 2017-08-24 19:01:01
SegWit only prevents one type of ASICBOOST ([c2]( There are a few other types of ASICBOOST that are slightly less efficient that can still be done with SegWit.
@_date: 2017-01-12 07:05:28








@_date: 2017-08-30 16:52:32
That's because the Bitcoin block time went below 10 minutes again:
We're currently at 7-8 minute blocks. 
@_date: 2017-08-18 22:40:05
I don't know of one. I'd be happy to review and maybe contribute to one.
@_date: 2017-08-27 21:55:12
If all transactions are able to confirm later, that means that supply exceeds demand at that later time.
@_date: 2017-08-20 21:58:43
And also has been true of p2pool pretty consistently for the last 3 years.
@_date: 2017-08-26 01:42:26
I believe that Bitcoin Cash should have done a better job designing their difficulty adjustment algorithm. I think the developers of Bitcoin Cash are largely to blame for the slow transaction confirmation times. 
In the parent post, I was just noting that the OP was correct in saying that it was not a coincidence. I'm not blaming Segwit for it.
@_date: 2017-08-18 14:42:09
If you think there will be two networks, and really want to split your coins on the two networks, here's two ways in which you can do it:
1. Get a satoshi or two of a coin that was mined after the hard fork, or comes from a transaction that for another reason will only be valid on one branch of the fork. Create a transaction that includes that satoshi as well as your other coins and sends the coins to a new address. This transaction will only be valid on one chain. This is the most secure way to split your coins, but until someone sets up a faucet of single-chain coins, it might be inconvenient to do.
2. Send your coins to yourself with a very low fee. If you get the fee right, it will confirm on only the 2x branch. Given that the hashrate on the no2x branch will likely be around 8% of the current hashrate, and the block time will be on the order of a few hours per block, it shouldn't be hard to find a fee that precludes inclusion in the no2x branch. Fees on no2x will probably be significantly above 400 sat/byte ($2/tx), while fees on 2x will likely be around 1/10th that or less. Once the transaction confirms on one chain and not the other, wait a few days for your transaction to be cleared from the mempool, and you should be set. 
@_date: 2017-08-26 10:10:40
[Satoshi Nakamoto, April 12, 2009](


It sounds to me like Satoshi did not anticipate fees being determined by some humans in charge of determining a blocksize limit (or, in economics-speak, a supply quota), nor did he envision fees being anything more than minimal. He expected minimal fees -- probably on the order of a few cents per transaction --  to be enough to pay for miners.
@_date: 2017-08-18 04:44:02
"Replay protection" is also known as "wallet backwards-incompatibility".
@_date: 2017-01-25 16:10:38


No, they're not, go read up on the paid uncle mechanism to reduce mining centralization pressures, the efficiency improvements of accounts versus UTXOs, and the adaptive gas limit. Ethereum was stress-tested over a year ago at [25 tx/s]( on much slower code.
The gas limit is currently 4 million, but adapts based on actual usage. At 4M, Ethereum can handle 13.6 tx/sec. My own full node can process 40M gas in 100 ms, which would give 136 tx/sec. The safe limit for the current network is probably in between those two points, but it's certainly better than Bitcoin's current 3.5 tx/sec, and probably more than 10x better.
@_date: 2017-01-31 03:01:08
It looks like these transactions are happening once a week on Mondays. Perhaps they are an exchange cleaning up deposits from customers and loading them into a cold-storage wallet?
@_date: 2017-08-20 05:32:28
blockocean is referencing this statement from 3 years ago:
"Malleability is a fundamental feature of the system" -- nullc
In all fairness, nullc probably meant "feature" in the sense of "attribute", not "design goal." nullc did not say malleability wasn't a bug, and it is possible for something to be both a feature and a bug at the same time.
@_date: 2018-10-12 16:48:24
It's only reversed because the value is stored in little endian order, and the OP was treating it as if it were big endian. It's the same message.
@_date: 2017-08-30 21:04:24
Fees are down because the block time dropped to 8 minutes average for a day or two instead of the 14 minutes we had before that. 
@_date: 2016-06-08 05:31:02
This is boring. You seem to be more interested in throwing insults than discussing technology, and your grasp of the intricacies of the technology is lacking. I'm blocking you.
@_date: 2017-08-20 22:18:18
'segwit2x' was btc1's name for BIP91 (plus some other stuff 3 months down the line). It was activated once bit 4 exceeded 80% over a 336 block window. So yes, it is in the *active state*. That's how we got segwit in the first place.
@_date: 2017-08-20 05:25:53
Oddly enough, neither I nor Roger Ver were at the NY Consensus 2017 meeting, and neither of us are signatories.
@_date: 2017-01-16 15:14:20
Multibit Classic 0.5.18 and earlier have a hardcoded fee. Multibit Classic 0.5.19 allows you to change the fee, although the maximum value is 50 satoshis/byte.
@_date: 2017-08-19 23:16:38
We built our facility in a region that has an excess of hydroelectric generation capacity. Our county is predominantly rural (population around 400k), but it has about 1.8 GW of hydroelectric generation capacity plus a bit of wind, and only around 0.5 GW of domestic demand.
Unfortunately, dam construction, even moreso than Bitcoin mining, benefits from economies of scale. Owning your own dam is generally not feasible.
Yes, we're professional. We currently have about 1 MW of miners, and are working on expanding to 10 MW.
@_date: 2017-08-28 09:18:23
And when it's daytime over the Pacific Ocean, who is awake to use it?
No point in arguing. If you take a look at the data, the difference is plain to see. It's like night and day. So to speak.
@_date: 2017-01-31 13:28:08
An exchange might do 1,000,000 BTC in volume per day, but it might also never hold more than 100,000 BTC at a time. Bitcoins in cold storage cannot be spent without manual intervention, so if customers were paying directly into cold storage, our example exchange would need manual intervention every 1/10th of a day just to be able to process withdrawals.
If you don't have a cold storage and just use a hot wallet, then you have more money at stake should hackers find a bug in your system that allows the hacker to withdraw more money than they need to. The motivation behind cold storage is to have a hot wallet to handle your day-to-day withdrawals and deposits (99% of the transfer activity but 5% of your holdings) while still keeping most of your long-term holdings secure against hacking.
@_date: 2017-08-26 00:06:20
An attack, you say? Bitpay disagrees.


@_date: 2017-08-26 02:23:02
It sounds to me like the point he made was this:


I hope that's not the "valid point" you think he was making. That kind of argumentation is a great way to create a circle-jerk.
I post in this sub specifically because I dislike circle-jerks. I think the best thing is when people of differing opinions get together to discuss issues civilly and eloquently, because that's the kind of discourse that has the greatest chance of people learning something new.
@_date: 2017-08-26 03:31:15
Bitpay was in favor of larger blocks long before May 2nd, 2017. Here's a couple of pieces of evidence that predate the Bitmain cooperation by over a year.
[Stephen Pair (CEO of Bitpay), Nov 12, 2015](


[Adaptive blocksize cap based on demand, Jan 17, 2016](


@_date: 2017-08-31 03:00:52
No, it's a lot more complicated than that. If I'm correct about what they patented, then they patented a way of regulating current flow and potential difference across a complex integrated circuit in a fashion that allows for multiple chips to use exactly the same amount of current while keeping the voltage supplied to each chip within reasonable tolerances, and to allow this to continue to work even if different chips in the string are operating at different clockspeeds or not operating at all.
Keep in mind also that this case being about the string mode configuration is purely my speculation. It very well could be something completely different. It's best not to judge without knowing the facts of the case.
@_date: 2017-08-24 17:54:31


Can you please describe to me the use case for filling a block with e.g. [67-of-67]( (3.6 kB) multisig transactions? 
These aren't typical 2-of-3 multisig transactions. A 2-of-3 multisig transaction only has around [254 bytes of witness]( These have around 8000 bytes of witness. They are monstrous beasts created with the sole purpose of making bloated blocks. Another term for that is spam.
Unless you can tell me how 100-of-100 multisig transactions are the real killer app for Bitcoin that's taking it to the moon, of course.
@_date: 2017-08-24 06:14:55
Funny how that 3.7 MB block only has 468 transactions, 467 inputs, and 471 outputs. If every Bitcoin block were 3.7 MB like that one, our transaction throughput would be about 75% *lower* than it was yesterday.
The only way you can get 3.7 MB blocks like that is by having a small stripped size in your block. You get that by having a small number of transactions, and a small number of inputs and outputs per transaction, but an artificially huge witness program in each input. The transactions in that 3.7 MB block have 8 kB of witness program data (i.e. signatures) even though they're only signing a single input. The only known application for transactions like this is spam.
@_date: 2017-01-31 14:15:41


If you look at [this transaction](
), you'll notice that it's 95 kB, just shy of the 100 kB limit. This is one of the dozens of large transactions on [1EPKSuFjTH3xbCK3b5ebaPbe8iUGvKvF8a]( one of the addresses that the author cited. Each one of those big transactions sweeps around 560 inputs into 1EPKSuFjTH3xbCK3b5ebaPbe8iUGvKvF8a on 2017-01-23 20:30 Â± 1 hour, with typically only a minute between each transaction. Each of those outputs got spent in low-byte high-BTC transactions to different addresses over the next few days. Overall, what is being done in this set of transactions is that they're taking around 16,000 inputs from around 7,400 unique addresses and concentrating them into a couple dozen other addresses.
(The de-HTMLized text of the blockchain.info web page for [ has 16190 total lines. If you remove duplicates, there are 7539 unique lines. Based on visual inspection, almost all of those lines are input addresses. Maybe 200 are not.)
To me, this appears to be exactly the type of pattern that a large-volume business like an exchange would have. This entity may have had 16,000 payments from 7,400 customers which they hand to sweep into large-value outputs in order for them to be useful for their hot wallet. If people think this is spam, then we should probably ban all bitcoin exchanges and large businesses.
@_date: 2017-01-25 16:19:44


It's not magic. It's engineering.
1. Paid uncles (GHOST protocol)
2. Adaptive gas limit
3. Account-based transaction model (which is more efficient in the typical case, and equally efficient in the privacy-maximizing case)
4. 14 second block times
5. The absence of perverse incentives in the fee model (in Bitcoin, a transaction that reduces storage requirements is more expensive than one that increases storage requirements (outputs are cheaper than inputs))
Ethereum hasn't "solved" scalability yet. However, Ethereum is about 10x ahead of Bitcoin on that topic. Bitcoin's scalability progress has been glacial. Lots of people come up with good ideas for Bitcoin, and they generally only have been implemented in altcoins. (Exceptions: fast block relay/Xthin and libsecp256k1, which only succeeded because no forks were required.)
@_date: 2017-08-26 09:30:46
No, I'm saying that transaction volume hasn't increased, but block times have. If there's a spam attack, there should be higher transaction volume. Whether or not a spam attack results in transactions that "look real" or not is beside the point if there are no extra transactions.
The problem is that transactions are not being removed from the mempool by blocks as fast as usual. That's all there is to it.
@_date: 2017-08-26 06:46:20
I think there may be a misunderstanding of how I meant the word "agent":
1. a person who acts on behalf of another person or group.
"in the event of illness, a durable power of attorney enabled her nephew to act as her agent"
synonyms:	representative, emissary, envoy, go-between, proxy, negotiator, broker, liaison, spokesperson, spokesman, spokeswoman; informalrep
"the sale was arranged through an agent"
2. a person or thing that takes an active role or produces a specified effect.
"universities are usually liberal communities that often view themselves as agents of social change"
I meant sense 
@_date: 2019-03-15 08:07:59
Ever use a compass? Or a wheel with spokes? Or think about the area of grass you can reach with a scythe from one spot?
It doesn't matter at all whether it's r\*tau or d/2\*tau in the end, you never get rid of the tau ï¿½ï¿½
@_date: 2017-08-24 18:58:21
He probably doesn't know what he means.
@_date: 2017-01-29 18:59:22
The Bitcoin mining network uses around 500-1000 MW. That is about the size of a single power plant. 
There are [about 40k]( bank branches in the USA. The USA has 4.5% of the world's population. If it has 10% of the world's bank branches, then the world would have 400k bank branches. If each one uses 10 kW of electricity, that means bank branches alone would use 4,000 MW. Non-branch operations of banks (e.g. backoffice operations, datacenters) probably add another 50%, for around 6,000 MW total usage by banks.
It's not really a fair comparison. For one thing, banks do a lot more than transfer money. For another, about 93% of the power consumption of Bitcoin is currently due to the *issuance* of new bitcoin, rather than the processing and confirmation of transactions. This would be like looking at the cost of mining gold in 1848 and claiming that it was part of the cost of processing gold transactions in 1848. The bitcoins that are being issued now are a long-term investment, and the cost of minting them should be amortized over the full lifetime of the system. (If only we knew what that was.)
Due to the way Bitcoin encodes data into each block, that power consumption metric is not dependent on the number of transactions in a block. The mining computation is performed using a 32 byte summary of the set of transactions in each block (the "merkle root hash"), and that 32 byte size is constant no matter how many transactions are included. If we scale Bitcoin blocks from 1 MB to 10 MB each, then we can fit 10x as many transactions in each block, and each transaction will use 1/10th as much energy. If we can use technologies like Lightning Network to perform 9 off-chain transactions for each on-chain transaction, then each transaction uses 1/10th as much energy.
@_date: 2017-08-26 23:05:01
Not all of Antpool is running hardware that belongs to Bitmain. I have several customers here in my datacenter in Washington State that mine on Antpool, for example.
@_date: 2017-01-16 04:49:55
0.00018000 btc/kB is not enough.  recommends 80 sat/byte, which is 0.00080000 btc/kB. The slider limits this to 0.00050000, unfortunately. 50 sat/byte would usually be enough to get a single transaction into a block with a small delay (maybe an hour), but since you're using CPFP to pay fees for multiple transactions at once, you would need to include a higher fee than normal so that the package of transactions has an average sat/byte that's high enough to be included in a block.
Note that while a fee of 0.00018 btc would be nearly ideal for a 226 byte non-CPFP transaction, if you send everything that's left in your wallet, you're going to end up with a transaction with a lot of inputs (basically, every unspent output you have left), and your transaction will likely end up being several kB in size. Because of this, you can not rely on the bug whereby Multibit calculates fees based on the size of the transaction in kilobytes rounded up to the next kilobyte (i.e. a 226 byte transaction rounds up to 1 kB), as e.g. a 5500 byte transaction only rounds up by 9%.
Might be time for a bug report to the author.
@_date: 2016-11-29 03:47:35
I meant more in the sense of using up bandwidth and (to a limited extent) storage. One of the main limits on the maximum safe block size is block propagation latency causing a weak force promoting mining pool centralization via increased block orphan rates. Large miners (especially large mining pool cooperatives that use headers-only mining, as we have in China) have an incentive to create large blocks that propagate through the network slowly. If someone were to spam the network with SegWit 3.8x transactions with high fees, it would have the effect of taking mining revenue away from non-Chinese pools/miners like Slush and Bitfury and giving that revenue to Chinese pools like F2pool, Antpool, BW.com, ViaBTC, and BTCC.


I do not believe that we currently have a significant amount of spam on the network. There just **[aren't any of the telltale signs]( of spam attacks in the current transaction mix. I think we have legitimate demand that exceeds the available supply. SegWit would help with that, although I think a block size hard fork would help more.
@_date: 2017-08-26 23:03:56
Your math is off by 1000. It's 312 PH/s, or 0.312 EH/s.
@_date: 2017-08-20 20:07:31


This is false. My fork [allows]( mining with Bitcoin Core as well as btc1, though a command-line switch is required. 


This is misleading -- it's only the first share after a new block is published that will include anywhere near 1 MB of new transactions, and even then, the 1 MB includes the full transaction size whereas the share itself only includes the 32 byte txids. This makes the share around 125 kB for a 1 MB limit. Most shares are around 20 kB.
Interested parties are suggested to read the [github issue]( on the subject. TL;DR: With a share size limit less than the block size limit, p2pool will be unable to consistently create blocks at the block size limit. A share size limit of less than 1 MB results in blocks mined by p2pool usually being less than 1 MB. In my opinion, this harms Bitcoin. It also substantially harms the revenue of anyone mining with that code. My fork gets roughly 1 BTC more revenue per block than p2pool master or veqtrus's code.
In addition to segwit and segwit2x support, my branch [reduces stale shares by about 60%]( [reduces CPU usage by eliminating an O( n^2 ) issue,]( [fixes a bug]( that can cause shares to not be broadcast, and [reduces RAM usage]( by about 4x for the same transaction load, among other things. All told, my branch has about the same hardware requirements as veqtrus's branch or the p2pool master branch, but handles a bit over 2x the transaction volume, and consequently gets around 2x as much revenue from transaction fees. If you wish to use my code:
    git clone 
    cd p2pool
    git checkout 1mb_segwit
Using pypy is strongly recommended with my branch. Installation instructions for p2pool on pypy can be found [here]( My branch has more improvements to RAM usage than it does to CPU usage, so with 2x the transaction volume, my branch uses about 50% more CPU and 50% less RAM. Pypy uses about 3x less CPU and 3x more RAM than CPython, so switching to pypy usually works better, as long as you have about 4 GB of RAM in your server. Further improvements to RAM usage should also be easier than CPU, though I have ideas for both.
@_date: 2017-08-30 20:52:13


No, it was just a change in hashrate and block times. Block times went from an average of 14 minutes down to 7.5 minutes. Now they're starting to creep back up to 13 minutes. If that continues, the mempool will start getting congested again.
@_date: 2017-01-31 14:32:08
Bitmain probably supplies 80% of the miners for all BU-powered pools. Bitmain also supplies about 80% of the miners for all SegWit-supporting pools.
(Note: Although Bitfury is not Bitmain-supplied and supports SegWit, Bitfury is not a pool. They are a large solo miner.)
If you go to  you'll notice there is no checkbox for "I am planning on running this Antminer S9 on a BU pool" that magically reduces the price. If you buy one or a dozen S9s, you get to choose for yourself what pool you want to run it on. Jihan does not make that decision for you, nor does he have the ability to influence you in any way.
@_date: 2017-08-30 23:39:26
It's not a 25-30% advantage. It's a reduction in power consumption, not an increase in hashrate. Currently, electricity costs are equal to about 10% of revenue for most miners (i.e. $0.05/kWh out of $0.50/kWh), so it's more like a 2.5% advantage in profitability.
And you can still do ASICBOOST with Segwit. It's just [one particular optimization]( of ASICBOOST that can't be done any longer. 
@_date: 2017-08-20 05:49:20
1 satoshi per byte is equal to about $45,000,000 per TB. On Siacoin, people are currently offering storage at around [$1/TB/month]( I know those services aren't equivalent, but still, you can buy 45 million months of storage on Siacoin for the same price you'd pay on Bitcoin. 1 satoshi per byte is actually quite expensive for storage.
@_date: 2016-11-28 02:40:57
Someone was wrong on the internet. I couldn't help myself.
@_date: 2016-11-28 05:01:58


No more than the size of the transaction. The worst-case scenario is where 100% of the block size is taken up by UTXO creation. With a 2 MB block size limit, this would be at most 105 GB per year. That's about 1/20th of a 2 TB $50 hard drive per year in the absolute worst-case attack scenario (in which 100% of all transactions are UTXO spam) sustained over a full year. That's about $2.50 a year per full node. That would definitely qualify as annoying should it ever happen. 
Of course, there's another problem with that scenario: If 100% of transactions are spam for a full year, Bitcoin is already completely dead.
@_date: 2016-12-05 14:35:11


And I wish I could fly faster than the speed of sound. However, it's not worth the price, so I haven't bought my own personal supersonic jet.


No, it's a bad idea. It's much better to not err at all. 
There are significant costs to overestimating and to underestimating. We should choose the value that maximizes the expected utility. 17.7% per year is not that value.
@_date: 2017-08-20 21:21:39
Can you explain a mechanism by which dishonest miners can earn more than they would earn if they were honest by using stale blocks with my code? All I can see is a way that dishonest miners can earn 99.5% as much as honest miners by mining stale blocks.
On the other hand, with the old code, there's a mechanism by which dishonest or custom miners can earn more by modulating the difficulty of the shares that they mine. The block-stale punishment mechanism only applies when the last share on the share chain is for a stale block. A selfish miner who sees that their share has become block-stale can reduce their share difficulty by as much as 30x in order to quickly pump out a share on top of their own and make both of their shares no longer block-stale. 
Note that the block-stale punishment mechanism doesn't prevent people from mining on completely invalid blocks, or mining on the wrong blockchain, as described [here]( It actually adds a selfish-mining vulnerability there due to the share-difficulty modulation vulnerability.
@_date: 2017-08-26 00:58:59
Sorry, I thought you were accusing the *miners* of performing an attack. Bitpay is not a miner, merely an agent of economic consensus.
Bitpay's motivations have nothing to do with being bought out by miners. They have to do with needing revenue. Bitpay makes revenue from processing Bitcoin transactions. The more transactions that Bitcoin can process, the larger the potential market for Bitpay. Bitpay just wants Bitcoin to grow. If Bitcoin can't grow, then Bitpay will have difficulty making their company profitable.
The reason that miners support Segwit2x is related. If Bitcoin can't grow, then the value of the coins that the miners are mining will not reach its potential. Bitcoin with small blocks might make it to low earth orbit, but not geosynchronous orbit, much less the moon.
No large Bitcoin exchanges, payment processors, or miners oppose Segwit2x. All of the large companies that have stated their opinion are in support of Segwit2x. Bitcoin companies with revenue know that their revenue depends on Bitcoin having enough capacity to serve its users. Bitcoin companies without significant revenue are much more inclined than big companies to oppose Segwit2x -- I'm guessing they're more worried about not spending money than they are about making money. Take a look at  and toggle the "Enable weighting" button to see the data I'm basing this assertion on.
@_date: 2017-08-28 20:59:53
I'm mostly not suggesting that NYA miners left Slush. I'm mostly suggesting that non-NYA miners flocked to Slush. Anybody who was mining on f2pool or btcc or antpool or whatever before NYA who disagreed with NYA would have probably moved to either Slush, CKPool, or p2pool. Of those, Slush is by far the largest and has the lowest payout variance.
@_date: 2017-01-31 03:02:16


Or maybe it's just a weekly cron job.
@_date: 2016-12-05 00:40:16






Xthin and compact blocks have nothing to do with CPU usage. They only affect bandwidth. As such, they do not affect the cost of the server. As this is a discussion of whether CPU might be a bottleneck, Xthin and CB are not relevant to this discussion.
@_date: 2016-11-27 14:10:10
This isn't exactly true. You can do it on mainnet too. The transactions are contrived and useless for anything except spam/DoS attacks, but we have seen quite a few of those on mainnet. As these witness-heavy transactions will be more fee-efficient than normal transactions for spam, it is likely that we *will* see large blocks on mainnet during spam/DoS attacks.
@_date: 2017-08-26 01:34:49


Actually, it's the result of a settlement over an antitrust lawsuit against Visa and MC:
@_date: 2017-08-28 21:51:06


You misunderstand. You *can't* signal whatever if you're mining on 93.6% of the Bitcoin pools (weighted by hashrate). Almost all of those are voting for NYA, so anyone who wishes to not signal NYA will run to Slush. People who are fine with NYA will likely stay on whatever pool they're on.
The pool determines the block signals, not the miner. Slush is unique in that it allows voting on their website for how each block should signal. No other pools (except p2pool) have that feature. When a miner disagrees with his pool, standard procedure is to switch pools.
I run a large-ish Bitcoin miner hosting business. My clients send us hardware and occasionally ask us to change pools on their hardware for them. We got a few requests to change pools for political reasons during the XT, Classic, BU, Segwit, and BCH debates/votes. I've seen a few pool changes to or from Slush recently among our customers, but I don't know if those have been for political reasons or for revenue and reliability reasons.
@_date: 2017-01-25 20:56:57


Paid uncles do not need to come at the cost of higher inflation, that was just the decision made by the Ethereum developers based on their own economic preferences. Paid uncles could be done on Bitcoin by reducing the block reward for later blocks. For example, for each 1 btc in uncle payments, each of the following 10 blocks could have their block reward reduced by 0.1 btc.




No, neither of those two points are correct. 
Ethereum's inflationary economic model is unrelated to the chosen block time. 
Ethereum is technically capable of using compact/Xthin block propagation methods, but since most blocks are currently only about 1 kB in size, it hasn't been a priority to develop it. As Ethereum transactions are fundamentally smaller than Bitcoin transactions (about 125 bytes each vs 450 bytes average), block propagation is less of a concern.
Ultimately, Casper and PoS will completely solve the orphan cost from block data propagation, so it's not clear that this will ever be relevant.


The perverse incentive regarding block size has a very low magnitude in Bitcoin. In Ethereum, it's even smaller, as selfish mining is 1/8th as useful with Ethereum as it is with Bitcoin due to the existence of the GHOST/paid uncle mechanism. 
It also won't be relevant after the switch to proof of stake, since orphan rates will fall nearly to zero when block creation is no longer a Poisson process and instead happens at regular intervals that are significantly greater than the block propagation and validation latency.


It can be parallelized insofar that one account isn't touched by two transactions. The probability of that happening for any random pair of transactions is small. The question parallelism isn't whether any lock conflicts will exist, but whether enough parallelism exists to keep your cores and SSD fully utilized. For real-world data, the lock conflict on a single account would likely reduce available parallelism by less than 1%.
Speaking of SSDs, it is unlikely that CPUs will be the bottleneck. Parallelism doesn't really matter when you only have one disk database. The usage of accounts versus UTXOs makes database caching much more effective in the common case of address reuse.
The main efficiency improvement of UTXOs is that you simply don't need a change output in your transactions, and you never need to deal with aggregating multiple previous change outputs. At steady-state, Bitcoin transactions using the standard wallet model will have 2 inputs and 2 outputs, whereas it's only 1 input and 1 output with Ethereum. That's a 50% (or 100%) difference, which is much larger than the effect of parallelism would have.




The Ethereum community has a different philosophy to Bitcoin about blockchain size. Ethereum does not value keeping the blockchain small very highly. For example, the transaction fee is only 0.2Â¢, which is about 1/10th what it was for Bitcoin even a few years ago when Bitcoin blocks weren't close to being full.
The concept of UTXO bloat and dust also just doesn't apply to Ethereum very well because of the account model, by the way. A series of very small payments to a single address will be unspendable dust in Bitcoin, but not Ethereum. 
Also, the Ethereum blockchain is a Turing-complete digital computer system, whereas Bitcoin is at best a conditional payment platform. The larger state database and blockchain size is largely the result of being used for its qualitatively greater capabilities than Bitcoin. The use of Ethereum as a simple payment platform currently accounts for relatively little of the database size.
@_date: 2016-11-14 04:30:42
The **[Columbia river](
@_date: 2017-08-21 20:54:08
We disagree.
Furthermore, I think you're trolling, so I'm not going to waste any more time on you.
@_date: 2017-08-18 04:42:03
No2x won't trade. You can't trade without blocks. You don't get blocks without hashrate.
(Exception: people who already have no2x coins in an exchange will want to sell them to something that they can withdraw.)
@_date: 2017-08-26 10:25:13
In the last two weeks, we've had [482054]( - [480195]( = 1859 blocks, not 2016. That's 157 fewer blocks than theoretical (600 sec/block), and 263 fewer blocks than we've usually had in 2016 ([570 sec/block]( Each block can remove up to 1 MB from the mempool, so 157 fewer blocks means up to 157 MB less data removed from mempool. The fact that we only have around 90 MB in the mempool shows that transaction rates have actually gone down, not up.
@_date: 2017-01-31 17:30:46


That's an interesting rumor. I'll make sure to keep it a secret from the Illuminati. It's an open secret that the Illuminati want to make sure that they don't have any competition when it comes to monopolies on financial systems, and would crush Bitcoin if they knew how powerful Jihan's pet project was becoming.
But seriously, there's another explanation for why miners are voting for Unlimited. Maybe they're doing it *of their own accord*. Maybe *some people other than Jihan* actually prefer Unlimited, and are willing to put up their own hashrate in favor of it.
*Non sunt multiplicanda entia sine necessitate.* Entities must not be multiplied beyond necessity. And the corollary: don't posit the existence of a conspiracy to explain data that can just as easily be explained without the existence of a conspiracy. If you do, it reveals your desire for there to be an evil conspiracy more than it says anything about the data itself.
@_date: 2017-08-26 09:41:06
Something in which blocks are larger and fees are lower...
@_date: 2016-11-25 06:45:51
Funny you should say that. Newegg uses Bitpay.
@_date: 2016-11-28 03:02:19
The current type of global pruning (all but the UTXO set) is default-disabled. The signature-only pruning (keep all TXOs and blocks and skip signatures on IBD) has not been enabled yet AFAIK. I don't know if it is planned to make it enabled by default, but I thought that was the intent. Until some form pruning is enabled by default, these kinds of spam attacks will use more storage than current spam attacks do. Afterwards, they will use less storage, but they will still use more bandwidth.
@_date: 2017-08-21 20:41:26


Mining the Core chain is explicity *not* supporting Segwit2x. Core is Bitcoin without Segwit2x, and with no other differences. Core and Segwit2x are in opposition to each other, and are actively vying for the label Bitcoin. The survival of Segwit2x and Core each depend on the relative hashrates and block rates at the time of the fork.
Bitcoin Cash and Segwit2x are not in opposition to each other. BCH is just doing its own thing. Whether a miner hashes on BCH or not does not influence Segwit2x's survival or popularity, nor does it influence BCH's survival or popularity.


It's not a game. It's just a few anti-Segwit2x people looking for reasons to criticize the agreement and its signatories. If other signatories of Segwit2x thought that mining BCH was a violation of the agreement, then that would be a different matter. But it's not. It's just bystanders who have no standing.
@_date: 2016-12-10 14:10:11


Yes, they have.
"51% attack" refers to a type of attack that requires a 51% threshold for guaranteed eventual success. It does not refer to an attack that is performed by 51% of the mining hashrate. Specifically, a 51% attack is where a majority of the hashrate chooses to ignore (and therefore orphan) any blocks that violates some arbitrary rule that the hashrate majority has decided upon. That rule could be something like "was mined by a non-approved miner" (enforced monopoly), "spends coins from Jonathan Toomim's account" (account censorship), "includes a transaction with an insufficient fee" (transaction fee cartel), or "violates OP_CHECKLOCKTIMEVERIFY semantics" (protocol change). While the last one might not seem like an attack from your perspective, it's an attack from the perspective of anyone who opposes that protocol change.
Soft forks in the past have had the following deployment rules:
Once signaling for the softfork reaches 75%, the new rules of the softfork are enforced for any miners that are signaling support for it. This means that if you say you are Jewish, then you will be punished if you don't keep kosher (your blocks will be orphaned), but gentiles are still allowed to eat pork.
Once signaling for the softfork reaches 95%, the new rules of the softfork are enforced for all miners. Even gentiles must keep kosher. Any blocks that violate the rule are orphaned by the 95% supermajority.
These rules comprise 51% attacks against the non-upgraded minority performed with 75% and 95% of the hashrate, respectively.


This is not accurate. Upgraded miners will not build on blocks produced by old miners, because old miners will include transactions that are invalid according to the new SegWit rules. It only takes one person to create and publish a transaction with a high fee that doesn't follow the SegWit rules. Such a transaction will stay in the mempool forever, and every non-SegWit miner will try to include it in a block, thereby invalidating the block according to non-SegWit miners. There will be many minority branches, but they will always be very short (usually 1 block long). A non-upgraded miner will ignore the SegWit rules, which means that it will create blocks that are invalid according to the rules of SegWit. Non-SegWit blocks will be orphaned by the â¥95% of miners who are following the SegWit rules.


SegWit requires that miners and all full nodes download extra data for each transaction. If I created a new rule that said that everyone had to download an extra 1 GB of data in order to validate every 1 kB of transactions, would you consider that to be an attack? With SegWit, it's only about 1.2 kB (typical) to 70 kB (adversarial condition) of extra data for every 1 kB of transactions. While you personally might not consider that an attack, other people might.
@_date: 2018-06-15 18:40:48
"Support the propagation" is a strange phrase.


They will only see that you are not propagating a block that someone else is propagating if someone else propagates that block to them. If someone else propagated that block to them, then it doesn't matter that your node didn't propagate that block, because they already got it.
Because your peers may be running different consensus rules than you, they will need to make sure that enough of their peers are running the same consensus rules as them. This means (in the case of a fork) that they will check protocol version strings and feature bits to make sure that they're connected to enough peers with the same consensus rules. Consequently, if they think your node might be rejecting a block that they consider to be valid, they will just route the block around you. This is what happened during the Bitcoin Cash fork.
Information cannot be contained. Your node cannot stop the propagation of valid blocks according to someone else's ruleset. Your node cannot force the propagation of invalid blocks according to someone else's ruleset. (Doing so will just get your node banned by those nodes.) 
The network doesn't care if you run a full node or not, nor does it care what rules you're following. You should only run a full node because you wish to fully verify that the data you get from the network conforms to the rules you choose to follow.
@_date: 2016-11-16 18:51:48
Mining has always been centralized since the advent of pools. Incidentally, we have Slush to thank for that. (Not that I blame him.)
If ASICs were the source of centralization, then we would see altcoins with GPU-based PoW be decentralized. But that's not the case. With ethereum, the top three pools comprise **[62%]( of the network hashrate, and the top pool is 23.4%. With  zcash, the top pool alone is **[39%]( of the **[total]( network hashrate.
The problem is that miners prefer to use big pools, because those are the pools with the best UIs and with the lowest variance. The bigger a pool gets, the more people want to use it.
@_date: 2016-11-27 14:01:36
15-of-15 multisig is an example, and the closest one to being reasonable. You can also write a witness program that is arbitrarily complicated or bulky, such as by using 2+2+2+2 instead of 2 * 4. (Bitcoin transactions actually contain simple programs written in a language called "**[Script]( to evaluate when money can be spent, and you can make those programs as long as you want.)
@_date: 2017-01-15 16:37:45
You probably didn't include a large enough fee. For future transactions, you need to go into the preferences page and increase the fee, probably to the maximum value on the slider. Edit: Fee adjustment is only available in version 0.5.19.
The transaction you sent might confirm today, since it's the weekend and the Bitcoin network is likely to be less congested. If it does not confirm in the next 24 hours, it is likely that it will never confirm. If that's the case, you can do Tools -&gt; Reset Blockchain and Transactions to force your wallet to rescan the blockchain and forget all transactions that aren't in the blockchain.
@_date: 2017-08-20 02:06:53
You can split coins pretty easily. You can either use the fee-based method described by TaleRecursion, or you can include an input from a transaction that is only valid on one chain. This includes any coins mined since after the fork, as well as any transactions that indirectly touch them. The fee-based method will probably be easier at first.
@_date: 2016-11-30 07:42:41
About right, yes.
@_date: 2016-11-28 03:31:14
In terms of hard-to-validate blocks, SegWit is just as bad as we have today (3 minutes, 19.1 GB of SHA256 hashing per block), whereas the BIP109 2 MB proposal would be 14.7x better than we have today (10 seconds, 1.3 GB of hashing).
BIP109 restricts the amount of hashing that can be performed by legacy-style transactions, which means that it is an actual fix for **[CVE-2013-2292]( SegWit does not, so it is not a fix for the vulnerability.
I don't know if Bitcoin Unlimited is planning to add a fix for CVE-2013-2292 in the same way that Bitcoin Classic did.
@_date: 2016-12-05 13:41:02




I didn't suggest putting it in the consensus rules. I'm just noting that the existing mechanisms for determining fees (e.g. propagation costs affecting block orphan rates, or the default minrelaytxfee setting) have always set a fee floor that is much higher than this level based on actual storage costs. 
Personally, I like the idea of using a miner cartel to set minimum fees, but I don't think anything additional is necessary to make Bitcoin unattractive as a bulk storage system.


Or like Ethereum, perhaps. Ethereum has a [working]( vote-based adaptive gas limit.
The main issue I had with BIP100 was that it set block size based on the 20th percentile vote. I think a median-based vote (which is what Ethereum uses) makes a lot more sense.
Note: BIP100 and the ethereum vote-based adaptive gas limits are both examples of a miner cartel for setting block capacity, which is almost equivalent to having the cartel set fees directly. Both approaches seem fine to me. I even think it would be fine to have both at the same time.


Ethereum was designed with the assumption that there would be attackers. The gas mechanism in Ethereum is pretty brilliant, actually. In Ethereum, even invalid transactions pay fees. A transaction that uses more resources than is allowed still pays the fee, but the state changes it was supposed to produce (e.g. moving money) are voided. This is unlike Bitcoin, where there are plenty of resources that an attacker can consume without paying any fees.
The issues that Ethereum had that made it vulnerable to attack were that a few operations (e.g. SUICIDE function calls, mathematical exponentiation, and most things requiring disk lookups) were being charged about 100x to 1000x less than the actual computational cost of the operations. That is, several constants in the consensus rules for Ethereum had inappropriate values. Each one of those constants was changed to a more reasonable number within 1 month of the issue being published. After fixing the constants, Ethereum has been resilient against attack.


I am not an advocate of BU primarily for this reason. Although I do agree that miners will not do that, I don't like designs in which miners can do that.


Me too.
I think that a hard cap on blockspace as the primary mechanism by which the blockspace is limited is a bad idea, though. Most uses of Bitcoin can be characterized by extremely low demand elasticity -- that is, when you need to spend your money, you *really* need to spend it, and would be willing to pay very high fees if you must. A hard blocksize cap is equivalent to zero supply elasticity. When you combine low demand elasticity with zero supply elasticity, the result is extremely variable fees and frequent supply shortages. 
I prefer systems in which the primary supply limitation is more elastic. The inherent block orphan cost from including additional transactions is one potential mechanism for this, and as long as we have a substantial block subsidy, I think it would be sufficient to keep fees from falling below a safe and reasonable level. Miner cartels are another potential mechanism for flexibly setting supply. Algorithmically determined supply quotas (adaptive blocksize limits) are another one.
The orphan cost limit already exists, and miner cartels could be implemented without any consensus rule changes. As such, I do not think that a hard block size limit as the method for artificially and manually determining the supply for the fee market is necessary. That's why I want to raise it so that it's no longer in the way.
@_date: 2016-12-10 13:01:53
I think the "fork" part of "hardfork" implies the possibility for more than one branch being used. Soft forks soften the impact of the minority branch by continuously performing a 51% attack against the minority branch, but hard forks do not. As such, I think that the capacity for the original system to continue functioning is precisely what a hardfork is.
@_date: 2016-12-05 09:06:17


In one of his posts describing the rationale for the 17.7% figure, he cited one of the Akamai State of the Internet reports as the source for his figure. It was so long ago that I can't find post(s) in which he made that citation, sorry.
As he was basing his argument for 17.7% off of that paper, and as that paper's numbers do not represent what he thought it represented (I'm pretty sure sipa doesn't think we should be running full nodes on our cell phones), his argument is invalid. 
It's also true that he was being conservative. Several sources for bandwidth growth data were available at the time, and sipa conservatively chose to use the source with the lowest figures. I don't think he read the report closely enough to notice the methodological issue, so from his perspective he was probably just erring on the side of caution when he chose the Akamai report.
Erring on the side of caution is still erring.
@_date: 2016-11-27 13:46:51
Much easier to do [in German](
(The link is to a 5,229 word German sentence. When translated, it begins, "When you begin to read this sentence, let go of all hope that you will ever get to the end, because in this sentence, among the tellings of stories and people...", etc., and turns into a treatise on how the author of that sentence is using the reader's interest in the never-ending sentence to guide the reader through a journey of discovery of the meaning of Earth, humanity, artistic intelligence, and Chuck Norris.)
@_date: 2016-12-04 21:56:15
Because sipa was using data from Akamai that used the available bandwidth of their website visitors, and Akamai did not distinguish between smartphone users and desktop/laptop users. Smartphone users went from nearly 0% to a quarter or half of all users during that time, and smartphones are much slower than laptops and desktops, so Akamai's measured growth was artificially stunted.
See also 
@_date: 2016-11-13 07:05:11
WA state is already a center for bitcoin mining. 
There are at least 6 MW of Bitcoin mines in my town in central WA. (At least 4 big mines. There may be a few more small ones that I don't know about.) My town has a population of about 20,000. That gives a per capita hashrate of around 1 TH/s per person. The worldwide average is 0.0002 TH/s per person.
Note that MGT Capital/McAfee's mine is [2.6 to 5.2 PH]( which is about 0.26 to 0.52 MW of capacity. That's about 1/10th of what my town has.
@_date: 2017-08-20 21:44:33


This is factually incorrect. You can mine with Bitcoin Core on my fork just fine. You can also mine with btc1. This is unlike your code, which only works with Bitcoin Core.
A previous version of my code was incompatible with Bitcoin Core, but I addressed that about a day after it being pointed out to me. The commit that enables mining with Bitcoin Core is a53bf4a.
Edit: It looks like you amended your comment to say this:


That's better and more true. However, it's still true that your code is requiring users to run software which may or may not be incompatible with the network in ~2 months regardless of how they configure their node.


It also requires the absence of the segwit2x "softfork" being reported by bitcoind. If you try to run your p2pool branch with a bitcoind that supports segwit2x, it will fail. The commit responsible for this incompatibility is [this one]( I ran into this issue in my own testing of your code. When I was merging your work with mine, I wasn't able to get my code to run with btc1 unless I either removed that commit or I added segwit2x to the SOFTFORKS_REQUIRED variable. I chose to take the latter approach and to add a command-line override for the SOFTFORKS_REQUIRED check. Have you tried testing it with btc1?
@_date: 2016-11-25 06:24:59
~~This looks like the email that Newegg sends after 4 hours.~~ Newegg will continue to wait on the payment for a total of 24 hours. If your payment hasn't cleared within 24 hours, they will cancel the order. If the payment clears after 24 hours, they will issue a gift card for the amount of the transaction.
I had this happen to me too with 3 purchases made on a single day, around a month ago. (My transactions were a bit bigger than yours, though -- around $110,000 total, instead of $469.) I used a transaction fee that was too little on that day, but was was fine the week before. It all worked out in the end, as my transactions confirmed in the middle of the night, when transaction volume was at its daily minimum. There's a good chance yours will too, although the network is a bit more congested now than it used to be.
Edit: Okay, it looks like it has been more than 24 hours, and I'm wrong about the 4 hour thing. Sorry. If the transaction eventually goes through, you'll get a gift card.
@_date: 2017-08-26 10:35:33


I included the URLs in all of the screenshots of graphs that I took. They came from [statoshi]( and [data.bitcoinity.org]( but you can see similar data from [blockchain.info]( if you want.
Specifically: 
From July 1st to July 15th, an average of 2.73 tps were accepted to mempool, but the mempool only averaged 5,394 transactions during that time, with 5,479 more at midnight of 7/15 than there were at midnight of 7/1. 
From Aug 23rd to Aug 26th, an average of 2.65 tps were accepted to mempool, and the mempool averaged 65,482 transactions during that time, with 53,989 more at midnight on 8/26 than there were at midnight on 8/23.
We have a lower transaction acceptance rate, but far more transactions stuck in mempool (and growing). This means that the transaction clearance and confirmation rate is even lower.
You can verify that data by zooming in on those specific date ranges (starting and ending at midnight) in statoshi and looking at the txAccepted numbers on the bottom-left of the graph.
@_date: 2017-01-16 20:22:38


Not quite. It rounds up to the nearest kB. A 226 byte transaction is considered 1 kB, so it gives a 0.2 mBTC fee (88 sat/byte). If you have a 4900 byte transaction, it would give that a 1.0 mBTC fee (20.4 sat/byte). This behavior is a bit awkward and counterintuitive, and it means that you need to be extra careful with large transactions as they will tend to get less sat/byte in fees than small transactions. If you ever need to sweep the full balance of your wallet, that will probably be a very large transaction (in bytes), so make sure to double-check the fees before you send.
@_date: 2016-11-29 13:41:05
Spikes in fee rates are the result of low demand elasticity combined with a supply quota. Fees went high because people are *willing* to pay a lot more in fees (at least for a short period of time) if there is no other way to access their money.
Judging from the link you gave, it seems that you think that the rapidly ballooning mempool is a sign that there was a massive spam attack. It's not. Consider the following scenarios: 
1. You add 1.60 kB of transactions to mempool every second, and every 10 minutes you remove 1 MB of transactions from the mempool (1.666 kB/sec average). 
2. You add 1.70 kB of transactions to mempool every second, and every 10 minutes you remove 1 MB of transactions (1.666 kB/sec).
3. You add 1.60 kB of transactions every second, and every 11 minutes you remove 1 MB of transactions (1.515 kB/sec).
How big will the mempool be in each of those scenarios? 
1. Tiny 
2. Huge
3. Huge
A very small change in block rates or transaction creation can make an enormous difference in mempool size.
@_date: 2017-08-20 22:00:49
If you write code that refuses to mine if bitcoind signals an unknown fork as being required, then it is your responsibility to make sure that your code is aware of all required forks of all major clients.
If you don't want to be responsible for that, then you should either allow users of your code to override your checks, or you shouldn't include them in the first place.
@_date: 2017-01-27 15:27:31
Not true. You can fully sync the Ethereum blockchain in about 2-4 hours using parity, or about 4-8 hours using geth, without any pruning or warp speed mode. With parity --warp, you can sync the blockchain in about 5 minutes.
Last time I did a full blockchain sync, it took 90 minutes. This was around September, I think.
There are some issues due to the recent spam attacks. If you run an old version of geth or without --cache-1024 (or something similar), you're going to have a bad time. However, if you're using parity with an SSD, it should be pretty fast.
Note that ethereum has more potential disk accesses per transaction than bitcoin due to the way smart contracts work and due to the spam attack, so having an SSD is more important and nearly mandatory if you want a high-speed sync or if you're mining.
@_date: 2017-01-25 21:29:49


No, I am not. I am neither in favor of nor opposed to SegWit. It is not my preferred approach to scaling, and I strongly dislike the 4x discount function as well as the soft-fork deployment with the auxiliary block merkle root tucked into the coinbase transaction, but I am neutral about whether SegWit is a good or bad thing overall.
All I'm saying in the parent post is that a block capacity increase fork (be it a hard fork or SegWit) will reduce fees in the short term.
@_date: 2017-08-20 20:55:09


No, BU never had more than 50% of the network hashrate over a 2016 block window. It had more than SegWit before Segwit2x, but typically only 40%. 
Segwit2x, on the other hand, has had around 89-93% support for over a month. They're not even close to being comparable.
@_date: 2017-08-27 21:39:35
If you only average over a few hours, then (a) your algorithm will be heavily influenced by miner luck and short-term variations in block times, and (b) you run into bigger problems with the day/night cycle.
If you try to predict the day/night cycle, then you'll do fine, unless everybody else is also trying to predict the day/night cycle, in which case you'd have to predict their predictions, unless they're trying to predict the predictions of the predictions, etc.
When demand exceeds supply, someone has to lose.
@_date: 2017-08-19 23:20:30
Don't forget to blame Bitfury, Bixin, F2pool, ViaBTC, BTCC, BTC.TOP, btc.com, Bitcoin.com, Bitclub Network, Genesis Mining, 1Hash, me, and all of the other miners except Slush and ([maybe]( Kano.
@_date: 2016-12-05 09:45:27


Only with that attitude.
@_date: 2017-01-31 16:29:18
The thing I dislike about your assertion is that it is not disprovable. 
It seems that you're asserting that there is a special channel by which miners can get substantial volume discounts (beyond the 1.5% listed on their website) that only exists for miners who support Bitcoin Unlimited. 
Without access to Bitmain's full financial or production records, it is impossible to prove the absence of such a channel -- we'd need to document every sale that Bitmain made, and prove that it went through the existing non-politicized channels. However, if it actually existed, it should be straightforward to prove the existence of such a channel -- we would just need one instance of someone who bought Antminers from Bitmain and who got a discount or access to greater availability because of their politics. Can you provide that?
I also find it interesting that you're alleging that Bitmain is favoring Bitcoin Unlimited buyers when both of Bitmain's own pools, Antpool and BTC.com, are mining with Bitcoin Core.
@_date: 2017-08-26 06:36:22
If the mempool were being spammed, wouldn't we see an increase in the rate at which transactions were being added to the mempool?
We're not seeing that. What we are seeing is a huge increase in the time between blocks, with up to 24 minutes per block for hours at a time.
Number 1 is wrong.  and  are correct.  is not really significant, and accounts for about 5% of the total backlog. It's mostly just 
@_date: 2016-12-04 23:42:54
That's not Jihan. That's JihadWu, a parody account.
I, for one, am downvoting the OP. I will also be reporting it, as the title inaccurately ascribes the tweet to Jihan.
@_date: 2018-06-15 16:08:04
Your node does not broadcast a "block XXXXXXX is invalid!" message to its peers. It simply silently chooses not to relay that block, and stores the hash of that block in its database in the list of invalid blocks so that it doesn't download it again.
Consider: if my node were to trust your node when your node said that block XXXXXX were invalid, then my node would never download and verify block XXXXXX. This means that your node could attack the network by slowing or stopping the propagation and verification of completely valid blocks that (for some reason) your node didn't like. Consequently, my node can't trust your node's claims of invalidity, so your node doesn't even bother to make them.
@_date: 2016-11-27 14:18:03
No, these transactions were created to simulate a post-SegWit spam attack. They are not typical or representative.
@_date: 2017-08-21 19:13:42
Once the fork happens, I think they're obligated to put close to 100% of their hashrate into Segwit2x. Until then, though, I don't think it really matters.
@_date: 2016-11-29 03:19:28
Are you questioning whether it's a minority and not a majority, or whether the minority that prefers the status quo (1 MB cap) actually exists?
For point 1:
and specifically
 
It's hard to get a single representative poll of block size opinions, since members of different forums have systematically different opinions due to the policy of censoring pro-hard-fork posts on bitcointalk and this forum, but those are the polls that I'm aware of.
For point 2, Luke-jr has repeatedly gone on the record saying that 1 MB is too large:
I also had a conversation with another prominent Core developer a day after Scaling Bitcoin HK (where the Chatham rules of non-attribution probably still apply, annoyingly) who said that he also thought that 1 MB was too large, but that he knew that position was politically unviable so he was supporting SegWit as a compromise. I don't have that on record, though.
@_date: 2016-11-28 03:46:42
Ethereum does this, and it seems to work. With Ethereum, each block can vote either to increase or to decrease the "gas limit" (which is the Ethereum equivalent of a block size limit) by a small amount. These votes can either be manually specified by the miner with a command-line option to set a target gas limit, or they can be automatically determined by the miner's software based on how full recent blocks were.
There is no technological obstacle to what you are asking for. It would be politically difficult to achieve with Bitcoin, though, as a substantial minority of people do not want the block size limit to be increased at all.
@_date: 2016-11-27 15:42:19
@_date: 2016-11-27 13:48:50
You don't make the transaction 92.5% witness data by making the non-witness section smaller. You do it by making the witness section larger.
There is an absolute minimum to the amount of non-witness data that a transaction can have. Typical transactions (2 input, 2 output) have about 2x as much non-witness data as the absolute minimum, as transactions with the absolute minimum (1 input, 1 output) just aren't very useful.
On the other hand, there's no absolute maximum on the amount of witness data that a transaction can have, except for the blocksize limit or the 100 kB per-transaction soft cap. So these 3.7 MB SegWit blocks just bloat a small number of 1-in 1-out transactions with a huge amount (~11.6 kB) of witness data.
@_date: 2016-12-04 23:49:22
Actually, that problem comes from having an 8 MB transaction, not from having an 8 MB block. There's a simple solution to this problem: limit all transactions to 1 MB in size (or even better, 100 kB in size, the largest currently allowed by the IsStandardTransaction() check). There are also a lot of other better solutions to the problem if you feel like writing more than one line of code. If you don't feel like writing even that one line of code, then this caveat from my previous post applies:


@_date: 2016-11-27 16:51:27
Both attackers (segwit and non-segwit) need to include a fee in their transaction that is greater than the transactions they wish to displace. With SegWit, the attacker can make a 3.8 kB SegWit transaction that will get included with the same absolute fee as a 1.0 kB non-SegWit transaction would need to have, or as much as a 1.8 kB "natural" SegWit transaction. The 3.8 kB spam transaction would only prevent 1.0 kB of non-SegWit transactions from being included in the block, but it would still use 3.8 kB of bandwidth and (until signature pruning is implemented) storage. Basically, SegWit gives the spammer around 3.8x as much hardware resource consumption bang for his spam buck.
Without SegWit, a spammer has to pay a fee for 1 kB of transaction space if he wants to use 1 kB of block space. He doesn't get way to make his transactions cost 50% to 75% less than a real transaction of the same size.
Your point is valid in that the fees that everyone would need to pay for blockspace are likely to be higher with SegWit than with a straight blocksize increase, but it remains to be seen whether that effect will be greater or smaller than the 4x discount that intentional spam gets. It's anyone's guess whether 1 kB of spam would be cheaper to send with SegWit or with a straight 2 MB increase. It's pretty clear that fees for everyone else would be cheaper with a straight 2 MB increase, though, and it's also clear that fees with SegWit are relatively cheaper for spammers than for non-spammers.
@_date: 2016-11-29 13:25:50
Demand tends to be less on the weekend and during nighttime over the most populated regions. Unless it's spam, in which case demand is the same 24/7.
(Although the most recent backlog was more due to fluctuations in block rate than fluctuations in demand.)
@_date: 2016-11-27 16:03:01
Actually, I was referring to the sentence structure, not the compound words. In German, all dependent clauses have a very well-defined grammatical structure that forces the primary verb to be in exactly one place (at the end of the clause). Furthermore, dependent clauses are always separated by commas, unlike English. (If you see someone writing English and using a lot of unnecessary commas, they're probably German.) The net effect of these features is that sentences in German tend to be chains of dependent clauses, whereas English sentences tend to have more nested dependent clauses. These grammatical features make it easier to string together extremely long sentences while avoiding ambiguity and minimizing the stack depth encountered when parsing them.
And now, for kicks, the same paragraph using German-style grammar:
Actually was I to the sentence structure referring. In German have all dependent clauses a very well-defined grammatical structure, that the primary verb forces, in exactly one place (at the end of the clause) to be. Furthermore are German dependent clauses unlike English always by commas separated. The net effect of these features is, that sentences in German chains of dependent clauses to be tend, whereas English sentences more nested dependent clauses to have tend. These grammatical features make it easier, extremely long sentences together to string, while ambiguity avoiding, and during parsing the stack depth encountered minimizing.
@_date: 2017-08-20 21:29:12
More than Ethereum had 2 months before its ETH/ETC split hard fork.
Upgrading nodes is easy. Node operators are even lazier than miners. They won't upgrade until they need to.
But that's not the point. My position is that p2pool users should choose for themselves which branch they want to use. veqtrus's position is that veqtrus should choose for p2pool users which branch they want to use. If veqtrus's guess is wrong, then he is responsible for causing his users to lose revenue.
@_date: 2016-11-28 03:22:42
Since a block's capacity is no longer measured in bytes, spammers can use more bytes for the same block cost with SegWit. It doesn't displace 4x as many bytes of non-SegWit transactions, but it does force people to download 4x as many bytes. This kind of spam attack could also have the side-effect of increasing the incentive for mining pool centralization, especially if performed by a miner.
No, it's not a UTXO bloat attack vector, but it is a bandwidth usage attack vector. Bandwidth is very expensive or limited in some regions (e.g. China), whereas storage has the same cost to everybody. SegWit trades off one type of attack for another.
@_date: 2016-11-27 13:57:23
The effect on the UTXO set size of SegWit is pretty minor. It doesn't incentivize transactions that eliminate more UTXOs than they create; it just punishes both the creation and the elimination of UTXOs equally. SegWit has pretty much the same effect on UTXO set size as just leaving the block size cap at 1 MB. (Actually, the 1 MB cap is even better for UTXO set size, since creating/destroying UTXOs usually has to compete with signature/witness data.)
The UTXO argument is a red herring.
Edit: as throwaway36256 pointed out, destroying UTXOs is currently much more expensive than creating UTXOs. SegWit reduces this discrepancy. This post is substantially inaccurate, and should be downvoted into oblivion.
@_date: 2017-08-21 19:57:10
It says that they 






It is a matter of interpretation what "support" means. My interpretation is that when either of those two forks is happening, the miners need to mine on the chain that includes those upgrades, and the non-miners need to have their platforms support or prefer those chains, and that when neither fork is happening, the signatories are free to do whatever they want as long as it does not harm the goal of activating the fork on time. As BCH is separate and independent of Segwit2x, I don't see how mining BCH harms the goal of increasing Bitcoin's base blocksize limit to 2 MB.
@_date: 2016-12-05 11:20:04
Probably a time zone bug.
@_date: 2016-12-05 08:58:31


There *is* something wrong with being too conservative: it limits the number of people who can use Bitcoin, and it causes unnecessary transaction delays.
@_date: 2016-11-27 15:29:26
Correct. SegWit does help normal transactions, but it helps spammers more.
@_date: 2018-06-15 16:04:02


Fact check: Coin.dance says there are [2130 BCH]( nodes right now on public IPs with open ports. In contrast, BTC has [9625 nodes]( with public IPs. This means that BCH has 22% as many public nodes as BTC, even though it only has 13% of the market cap and 6.9% of the transactions per hour. This suggests that the proportion of BCH users who run a full node may be higher than the proportion of BTC users. Given, full node statistics are easy to manipulate by running a node on a VPS, but even if 50% of the full nodes on BCH are fake and 0% of the BTC nodes are fake, they would still have about the same number of nodes per user. 
@_date: 2016-12-05 14:15:12
That isn't the tweet that the OP linked to.
@_date: 2017-08-21 17:04:58
Can you point out to me what part of the NYA prohibits miners from putting some or all of their hashpower on alternate blockchains when it is not needed for segwit2x? I know the Hong Kong consensus agreement had a provision like that, but I'm not aware of one in the New York agreement.
@_date: 2016-12-05 11:11:11
Yes, so let's encourage a minimum fee that reflects the inherent costs in the system. We don't want people to be using Bitcoin just because it's free storage.
Let's say there are 5000 nodes. Let's say that a 100 GB disk costs about $100. (Actually, a 250 GB SSD costs $60, but whatever.) That means that 1 kB of storage costs about $0.000000001 per node, or about $0.005 for all 5,000 full nodes. As long as fees are higher than $0.005 per kB, then users are not getting free or even subsidized storage. Fees are currently 20x to 100x higher than that threshold.
If you just want highly replicated and robust storage, there are much cheaper ways to do it than Bitcoin. Like maybe IPFS


**[On Ethereum via IPFS]( That's the kind of application that Ethereum and IPFS were designed for, so it's hardly abuse. Unlike Bitcoin, Ethereum's transaction/gas fees are intended to be a reasonable estimate of the actual cost of including a transaction in a block, so it's much cheaper to do this on Ethereum than on Bitcoin. And since they're using IPFS, they're not storing a significant amount of data on the Ethereum blockchain -- basically, just the 32 byte hash of the database is on the blockchain, while the database itself is stored on IPFS.
But sure, I get your point. You want big businesses to get off your lawn.
@_date: 2016-11-27 15:19:58


I didn't say that these blocks won't reflect real-world usage. We've seen spam attacks on Bitcoin before. We'll probably see them again. It's likely that we'll see 3.7 MB spam attack attempts on mainnet with SegWit.
@_date: 2016-12-10 12:59:22
A non-kosher miner majority has an economic incentive to exclude kosher miners. If 90% of miners are non-kosher (accept blocks larger than 1 MB), then they can either keep their blocks smaller than 1 MB in order to keep the kosher minority happy (in which case they get 90% of all the block rewards), or they can build bigger blocks and exclude the kosher minority, in which case they get 100% of all block rewards.
Caveat: This analysis ignores any possible effects on the coin's exchange rate. In the second scenario, you have two branches of the blockchain, and it's possible that both branches are active. If the kosher branch has a significant exchange rate, then it's possible that the value of the kosher branch will come at the expense of value in the non-kosher branch.
@_date: 2016-11-27 13:36:05
The 3.7 MB blocks have **[467]( or **[468]( transactions for the ones I looked at. There's a list of these blocks **[here](
In order to make very large blocks with SegWit, you have to use fewer transactions. Inputs and outputs are 4x more expensive with SegWit than witness data, so in order to use a lot of total data, you have to keep your inputs and outputs to a minimum, which means fewer transactions.
Smaller blocks (~2 MB) can have more transactions, but as you go above 2 MB, the number of transactions per block necessarily has to decrease.
@_date: 2016-12-05 09:42:02


This statement is based on misunderstandings of Gavin's solution. 
For example, the main argument that I've seen alleges that Gavin's solution changes block creation from a 2D knapsack problem into a 3D one. By adding a limit on the number of bytes hashed (the sighash limit) to a block, miners now have three constraints to satisfy when assembling a block instead of just two (block size and signature operations). The argument is that this makes it impossible to choose transactions to fill a block while maximizing fees. 
This argument is wrong, because Gavin's code also makes it impossible for a miner to be able to hit the sighash limit. The block sighash limit is just there as a sanity check. The practical limit is in **[AcceptToMemoryPool]( where a node will reject any transaction that uses 1/n of the block's sighash limit, where n is the number of transactions that could fit in a block. 
As it is not possible for a miner to hit the sighash limit, miners do not have to worry about factoring it into their calculations when choosing transactions that maximize their fee revenue.
The 100 kB limit proposal that I made earlier also does not have this issue.
@_date: 2018-06-15 15:51:18
Your node will only reject a block that is accepted by other nodes if your node is running a different ruleset than other nodes. That can happen due to malice (e.g. a miner attack), a difference in politics (e.g. a fork), or a bug (e.g. a non-deterministic ruleset). It will not happen due to a lack of data.
If your node is out of sync, your node will check the block's header for the hash of the previous ("parent") block, notice that it does not yet have the parent, and then request and download the parent from other nodes. If that block's parent is also missing, the process gets repeated, until your node gets to a block that it knows (e.g. the genesis block). Your node will then attempt to verify the blocks in their original sequence.
A block is only rejected if it is found to violate one of the consensus rules. If your node does not have enough data to verify a block, then it is neither accepted nor rejected until that information is present.
@_date: 2016-12-05 14:31:27


A mining cartel does not require mining to be centralized.


A Bitcoin mining cartel would be an atypical cartel, since Bitcoin is an atypical industry. The cartel's rules can be enforced with technology and does not require any centralization. If the cartel decides that the minimum fee is 20 satoshis per byte, then miners (or full nodes) can orphan any block that includes a transaction that pays less than 20 satoshis per byte. There is no technical limit on the number of miners who vote on the fee. No additional centralization necessary. 
@_date: 2017-08-26 01:00:21
No, I downvoted because he made an assertion that was not backed up by data, can be refuted by known data, and which attempted to explain a phenomenon that has a more parsimonious explanation given other known data. I frequently upvote posts I disagree with if they argue their point well, and I downvote posts when they are factually incorrect or incoherent.
@_date: 2017-08-20 20:50:06




Actually, no. The "block stale" punishment mechanism that I disabled will cause p2pool to orphan the last share found once a new block hits the network, regardless of whether that share was published before the block or after the block. In practice, the majority of the shares it orphans are shares that were published before the block. It orphans about 5% of all valid non-stale shares.
Exception: when the share belongs to a miner with a large percentage of the hashrate, that miner will sometimes mine another share on top of the otherwise-orphaned share, and this will bypass the block-stale punishment by other miners. In other words, the block-stale mechanism punishes small miners more than large miners. It reduces fairness. That was the main reason why I got rid of it -- it biases p2pool in favor of larger miners like myself.
@_date: 2017-08-30 16:55:01
By waiting until the block time went from 12 minutes down to 7.5 minutes:
OP probably thinks it was manual fee setting that did the trick, but it was really just that he chose a good time to send transactions.
@_date: 2016-11-27 12:58:34
To make a 3.7 MB block with SegWit, you need to the  construct transactions that are at least 92.5% witness data, and then fill the block with nothing but those transactions. Typical transactions are about 60% witness data and 40% non-witness data. These transactions were atypical. You only get 75 kB of non-witness data (i.e. source and destination addresses) in that 3.7 MB block. This was only enough for **[467 transactions](
The only known use case for blocks like these is DoS attacks. This means you're basically making 12 kB transactions comprising of one input and one output, when a normal one-in one-out transaction would usually only take around 225 bytes.
@_date: 2016-12-05 00:16:05
SegWit allows for a block that takes 3 minutes to verify. Gavin's BIP101 and BIP109 allow for blocks that take at most 10 seconds to verify.
SegWit's "solution" is not a "hacky solution" because SegWit does not actually solve the problem. SegWit provides a new transaction format that does not suffer from the O( n^2 ) issue, but SegWit does nothing to restrict the abuse of the old transaction format.


This is factually incorrect: Gavin's solution only prevents the use of large signatures that *do* have a quadratic component, while permitting larger signatures without a quadratic component. This is unlike SegWit, which *allows* the use of large signatures (up to 1 MB in size) with a quadratic component as well as larger signatures (up to 4 MB) without a quadratic component. 
It's already possible to make a non-SegWit transaction with no quadratic components using SIGHASH_ANYONECANPAY.
@_date: 2018-06-15 18:57:58


Not quite. First, it's important to understand what a hash is. A hash is a unique fingerprint of a block. It's also a very short, globally unique identifier of that block. It's like a name, except that it's guaranteed to be unique. The hash can't "not match its own ledger". It can only refer to a block that is either present or missing in its local database.


The latest block contains the most recent transactions, but it does not contain the full state of the ledger. The full ledger is currently about 2.8 GB of data. A block is only about 1.1 MB of data. To get a full copy of the ledger, you have to process all 527,612 blocks in the blockchain *in order*.
The hash refers to a block that can be one of four things: (1) not downloaded;  (2) downloaded but not yet connected to the blockchain; (3) known valid; or (4) known invalid. If you get a block that has a parent that you don't know, then you download the parent. If you don't know that one's parent, then you download that. Etc. Until you get back to a block whose parent you *do* know and have verified. Then you connect that block to its parent, and then connect its child, and so forth, until you've connected all of the blocks into the blockchain.
@_date: 2016-12-04 21:42:27
Pieter Wuille's use of 17.7% per year was based on data on annual bandwidth increases from Akamai which included both mobile web users and desktops. As mobile users are much slower and were increasing in proportion, this significantly reduced Akamai's estimates of performance increases. A more reasonable estimate for bandwidth improvement is **[30% to 36%]( per year.
@_date: 2016-11-28 03:22:17


I don't think this was my position. I think that UTXO bloat is not a mining centralization concern, as even small miners can easily afford to store the UTXO on SSDs or even in RAM, and non-miners do not need fast access to the UTXO and can even keep it on a spinning HDD fi they want.


The OP seemed to think that the ability to do â¥ 3.7 MB blocks was an advantage of SegWit. My posts were intended to show that it is a bug, not a feature. It's not a critical bug, but it is a bug.
I think it is an interesting attack vector, yes. It can be used to enhance bandwidth-limited selfish mining attacks, for example, so I think it's worth thinking about it. I do not think it is a major concern, merely a moderate concern. Bitcoin can survive 3.7 MB blocks, although they might be annoying.
@_date: 2016-12-05 00:36:58


Note: I'm not currently arguing about desirability. I'm arguing about causality and possibility. My argument is that you *can't* reverse those numbers. There's simply no substantial incentive for most people to run a full node. If there's no incentive to do it, then most people won't do it, no matter how small the cost of running a full node is
You could try to reduce the blocksize to 100 kB in order to reduce the cost of running a full node, but that will have the effect of making fewer people use Bitcoin, which will result in fewer people running a full node. Even at 100 kB, it will still be less convenient to run a full node than a light client, so people will still prefer light clients.
I think that Bitcoin block sizes should stay small enough so that just about anybody who wants to run a full node can afford to do so. I do not think that Bitcoin block sizes should stay so small that there is no incentive to run light clients, as block sizes that small would kill Bitcoin.
@_date: 2016-11-30 07:47:40
It does explain the graph, actually. You just need to realize that the mempool size is a cumulative measure of backlog, whereas the (tx_added - blocks_made) difference is a differential measure for the backlog. When transactions are added faster than they get cleared, the backlog (the mempool size) will grow continuously until it hits the upper limit and transactions start getting evicted.
It's like we have a bucket with an open tap and an open drain. If tap water is added to the bucket slightly slower than it drains, then the bucket will be mostly empty. If water is added to the bucket slightly faster than it drains, then the bucket will keep filling up until it starts to overflow.
@_date: 2018-06-14 09:24:27
Bitcoin miners are currently earning about $0.13/kWh in revenue (assuming an Antminer S9 or equivalent efficiency). The hashrate will not drop significantly in response to exchange rate fluctuations until revenue falls to about $0.08/kWh or less.
@_date: 2016-11-14 04:31:04
Not as cold as my heart.
@_date: 2016-12-05 15:17:41




Not if cartel members make an effort to orphan blocks that do not conform to the cartel rules. (This would be equivalent to a soft fork.)


That's fine. I like both options.


I like having the cartel's decision be enforced by a a consensus rule just fine. That's not the only way it could be implemented, but it's probably the way I would implement it if I had the choice.
I think we're pretty much in agreement here, but I'm calling the thing a cartel and you're calling it a consensus-rule-based-voting system. It's the same thing, and we both seem to like it.
@_date: 2016-12-02 17:58:32
Interesting. A few observations:
1. 6 MB of transactions alone cannot explain a backlog that peaked at something like 45 MB of transactions. 6 MB is about one hour's worth of Bitcoin transaction traffic. It's hard to argue that 1 extra hour of traffic was the sole or primary cause for 4 days of backlogs and delayed transactions.
2. It is likely that most of the 17-output transactions were performed by one entity. However, that alone doesn't mean it's spam. There are plenty of trading bots, gambling games, exchanges, tumblers, and other legitimate digital services on bitcoin that generate transactions of a particular type, and this 17-output entity could be a new one of those. A transaction is only spam if it has no reasonable purpose other than bloating the blockchain, using people's bandwidth, and displacing other transactions. To give good indicators of this, you can either look at the outputs themselves and show that they could not be used for anything reasonable (i.e. are dust), or you can do a taint analysis and show that the coins are being recycled.
3. The coincidence of the end of this 17-output burst with a reduction in congestion seems strange. The volume of transactions with 17 outputs should not have been enough to *cause* the end of congestion, but perhaps the 17-output transactions were only the tip of the iceberg, and there was a larger scheme at work which had 17-output transactions as just one component. Or maybe it was just coincidence. That's plausible, since the onset and end of the congestion period also coincided pretty well with a difficulty adjustment and a period of bad miner luck.
4. The linear ramp-up is weird. That's the kind of code I would write if I were trying to stress test something to see how much it could handle before failing. It's possible that this was designed to "test" the bitcoin network, but I think it just as likely that someone was beta-testing a new bitcoin application (gambling site?) to see how well it would scale, and eventually shut it down when they saw how much it was costing in fees, or when they saw bugs due to congestion.
@_date: 2016-12-04 23:29:08
"2k dollar" server? I'm talking about a single 2 GHz core being able to handle 1 GB of transactions every 10 minutes right now under typical conditions, and you think that costs $2k? More like $200.


There are a lot of adversarial conditions that prevent 1 GB blocks from being viable right now. There are no adversarial conditions that prevent 10 MB blocks from being viable right now (unless you write really bad code). With modest hardware and software improvements, 1 GB blocks can be viable with adversarial conditions in a decade or less. 
If you wish me to back those claims up with math, you'll have to name the adversarial condition you're most concerned with. (I don't feel like writing 5000 words detailing all of the known adversarial conditions and what their numerical limits on capacity would be.)
@_date: 2016-11-29 12:31:56
And I'm sure that everybody who tries to use Bitcoin will know intuitively what pruning is, why they might need it, and how to enable it, without asking their son or grandson why they are unable to save any more videos of their pet cat in their iMovie.
Speaking of continuous resources like data-transfer, can you say whether SegWit's 4x discount function can make it cheaper for a spammer to use them? Even if it's not in blocks, maybe a spammer could send out transactions with fees that are low enough to be unlikely to be included in a block, but high enough to not be rejected by nodes and make it into mempool and get forwarded to other nodes? I know that the minimum fee for mempool is automatically adjusted, but there should be some times of the day (e.g. before the daily peak) during which a marginal transaction will make it into mempool, but would still be likely to get kicked out later. Once that transaction gets accepted, you could then kick it out yourself with a transaction with a 1 satoshi better fee, and repeat. With SegWit, you could make those transactions 3.9x bigger than a non-SegWit transaction or 2.2x bigger than a natural SegWit transaction for the same fee. If you perform this attack carefully and time it right, you might be able to exhaust a lot of bandwidth without actually spending a single satoshi on fees.
Of course, this attack won't work against people who are using blocksonly mode, and blocksonly mode is enabled for anyone who wants to enable it--presumably anyone who is low on bandwidth! It's not like knowing about 0-confirmation transactions is useful to anyone anyway.Nobody will have their Netflix interrupted by pesky transaction spam storms unless they're fully aware of the cause and the potential solution and have decided the stutters are a small price to pay. 
Although once the spammer decides that he's willing to pay the fee and let his transactions get included in a block, Netflix stuttering once every 10 minutes can only be avoided by switching to an SPV wallet. (Which, to be honest, is okay with me.)
@_date: 2018-06-15 19:13:45
A block received over the network does not have to be a child of the most recent block. It can be a child of a block from a day ago, or from a week ago, or an hour ago. Such a block can still be valid. Multiple competing versions of the blockchain (that end on different branches) can exist at the same time. The rule for deciding between all of these valid options is that your node will follow the chain with the most mining work done on it.
Your node isn't asking itself "Am I in sync with the network or not?" Rather, your node is simply asking "Do I know this block and all its ancestors?" If it doesn't know that particular block or its ancestors, then it downloads those.
@_date: 2018-06-16 04:52:05
A miner would only do it if they expected to gain more by defrauding someone than they would otherwise earn in direct mining revenue. It's a theoretical attack. It has never happened on Bitcoin before.
@_date: 2016-12-04 22:45:55
Current CPUs can safely handle block sizes up to around 100 MB. libsecp256k1 gets around 10,000 signature verifications per second per core translates to verifying 100 MB of transactions in about 50 seconds on a single core, or about 5 seconds on a 10-core server CPU. Note that these signature verifications can be done when the transaction first arrives at a node, and do not need to wait until a block arrives -- that is, the transactions in a block can usually be preverified. Given that blocks should arrive an average of once every 600 seconds, a single core should theoretically (without safety margins) be capable of verifying around 1 GB of transactions every 10 minutes. 
This is roughly consistent with benchmarks of initial blockchain syncing, in which a 6-core CPU can sync 75 GB in **[410 minutes]( or 336 seconds per GB per core.
BIP103 would reach 1 GB block sizes in 2059. Given that a single core can handle that much data right now, I think is unlikely that CPUs will be the bottleneck in the future. If CPUs are a bottleneck in the future, that would mean that CPUs 42 years from now are slower than they currently are, which would probably mean that the apocalypse arrived. If that's the case, then it's unlikely that we would have enough demand to fill 1 GB blocks, so it's unlikely there would be a problem at all. Except for all the death and destruction, of course.
@_date: 2016-12-05 00:08:28
Mining centralization has nothing to do with the costs of running a full node. I operate 0.1% of the Bitcoin network hashrate. I spend about $14,000 each month on electricity for our miners, and average about $10 a month on our Bitcoin full nodes (which we don't even have to run in order to mine). The actual causes of mining/pool centralization are that (1) large pools provide lower payout variance than small pools, and (2) large mining operations have lower capital and operating expenses per hash than small operations due to economies of scale. My company pays 2.7Â¢/kWh for electricity, for example. Only other industrial miners can compete with that.
The reduction in node counts is due to the fact that you don't need to run a full node to use Bitcoin. Node counts started dropping once a few good light clients hit the market, because no matter what you do, light clients will always be more convenient than full nodes, and they aren't significantly less secure. (Yes, I'm aware that there are theoretical attacks against light clients that are possible (though very expensive, as they require valid proof of work but invalid blocks). I'm also aware that there are no documented cases of those theoretical attacks having been performed.) Why should use an extra 1.5 GiB or 75 GiB of storage and an extra 10 GiB of bandwidth each month if you don't benefit from it?
There is nothing that we can do to reverse either trend (except switching to some form of proof-of-stake, which, for Bitcoin, is implausible). Let's not prevent Bitcoin from progressing in the ways that actually are possible because you want to do the impossible first.
@_date: 2016-12-05 14:07:10


Validation cost for non-mining full nodes is nearly zero. Validation cost for mining full nodes is much higher per miner (as it affects orphan rates) but they are already priced in. Storage and bandwidth for non-mining full nodes are the (only?) significant externalities in the existing system, so they are what should be considered when making centralized planning decisions like setting a low block size limit.


The choice of which transactions to include in a particular block is still decentralized. That allows censorship resistance to be preserved. The only vote-based decision is what the minimum fee (or maximum block size) should be.


"Cartel (noun): an association of manufacturers or suppliers with the purpose of maintaining prices at a high level and restricting competition."
Voting is the mechanism by which the association of suppliers (i.e. miners) would choose what level to set the prices at. Without the cartel, competition would drive the price down to the marginal cost of including a transaction (which is nearly zero) instead of being sufficient to cover the total cost of including a transaction (which includes the vast majority of all mining costs, as electricity usage is independent of the block size).
Cartels are a bad thing in most industries, since most industries have significant marginal costs and relatively low fixed costs. Bitcoin has relatively low marginal costs and significant fixed costs, so a properly regulated cartel for Bitcoin mining would be appropriate.
A cartel would choose a transaction fee floor or block size limit which maximized mining revenue. This means that they would have to maximize the priceâ¢volume product. This is pretty similar to the incentives for how all other aspects of the world economy work. Bitcoin miners would not be in competition with each other, but they would be in competition with banks and other cryptocurrencies, so the Bitcoin cartel would need to make sure that its fees are both sufficient to cover all costs and also low enough to be competitive with other monetary systems. In other words, the cartel would choose sane prices.
@_date: 2016-11-02 22:53:46
The datacenter is cheaper than the stuff they fill it with. Bitfury has a 100 MW datacenter too, and Bitfury also has a 0.07 J/GH ASIC, but they don't have the full 100 MW filled with 0.07 J/GH ASICs. If they did, Bitfury would have 1.4 EH/s, roughly the current network hashrate.
Bitmain can't produce S9s as fast as they can sell them. I don't see how having a 140 MW datacenter changes things substantially in the short or medium-term, and in the long term, the hashrate will be well over 2 EH/s, and closer to 3 or 4 EH/s.
@_date: 2017-01-31 14:52:44
Bitfury has a 16 nm chip. Avalon has a 16 nm chip. BW.com has a 14 nm chip. All three chip designs are slightly (~30%) less efficient than Bitmain's 16 nm chip, and are not quite competitive in terms of price.


ASICs *are* complete chips. For reference, it costs a few million dollars to design and fabricate the first batch of a 14 nm or 16 nm chip.
@_date: 2018-06-15 16:13:42
All full nodes are equal on the protocol level. Your node will treat all blocks the same regardless of whether they came directly from a miner or if they were filtered through a dozen non-mining full nodes before reaching you.
Block validity and priority is determined by math, not by social networks. The block stands on its own merit, no matter who sent it to you.
@_date: 2016-11-28 06:52:24
Only if the more powerful miners have at least 51% of the hashrate. If they have 51% of the hashrate, then they don't need an adaptive gas limit in order to get rid of other miners.
(The adaptive gas limit algorithm converges on the median miner's preferred gas limit.)
@_date: 2016-12-05 08:55:33


The chart says "default dbcache," which I think means 100 MB. This conflicts with your statement that it has "enough disk cache that more or less everything should be in ram". Is that label inaccurate or is your comment? I'm assuming the label is accurate for the rest of this comment.
100 MB is not enough for everything to be in RAM, given that the serialized UTXO set is now 1.5 GB and the in-memory is about 4x larger (due to the cache system using a less memory-efficient representation of each UTXO). 
To me, it looks like that chart shows the early blocks (before 250,000) having a small enough UTXO set such that everything does fit in RAM and each transaction lookup takes about 30 Âµs, and around block 370,000 many lookups are hitting the SSD and levels off around 130 Âµs on average. My reference to the near-O(1) lookup speed was assuming that all UTXOs have to be looked up from disk, since UTXO lookups from RAM are never slow enough to be an issue. 
Your benchmark machine seems to be loading UTXOs fast enough to handle 7692 transactions per second with only 100 MB of cache. I fail to see how this is such a severe issue that we can't increase the limit on transaction throughput from around 3.5 transactions per second to around 35 transactions per second.
@_date: 2017-08-31 01:04:21
Quoth LightningASIC:


I suspect he's referring to the string mode hashboard configuration, where you put a bunch of ASICs in series and feed 12v into the first one, which consumes 0.7V and gives 11.3V to the second one, etc. Bitmain first did this in the Antminer S5 around January of 2015, but Bitfury had been doing it for a while before that (though I think they called it chain mode or something like that). However, depending on when Bitmain filed for the patent, Bitfury's implementation might not qualify as prior art.
@_date: 2016-12-04 23:33:23
Of course we should listen to him, he's a smart guy. And **[when he's wrong]( we should correct him, the same that we do for all other smart guys.
Pieter also does not claim that CPUs are the bottleneck in Bitcoin's scalability, by the way.
@_date: 2016-12-04 21:59:14
The more relevant question is "Why isn't this being downvoted?"
@_date: 2016-12-04 22:13:23
The UTXO set size is currently **[1.5 GiB]( (after 8 years of operation) and is growing about 100 MiB per month. If we had full 10 MB blocks (without doing anything extra to incentivize keeping the UTXO set size small), then that would be about 1 GiB per month of growth.
Technically, UTXO lookups are a O(log(n)) operation, as they are stored in a tree (via leveldb) on disk. However, the top levels of the tree are generally cached, so in practice UTXO lookups *from disk* are usually O(1) or close to it.
The UTXO set cannot be pruned, and needs to be kept on disk. If your application requires block verification times to be much faster than 1 second (e.g. for mining), then you need to keep the UTXO set on an SSD. Otherwise, HDD storage is sufficient. 1 GiB of SSD storage (enough for 1 month with 10 MB blocks) currently costs about $0.25. 1 GiB of HDD storage currently costs about $0.05. That's roughly the cost of one Bitcoin transaction. Each month. Oh, the horror.
@_date: 2016-11-28 02:39:15
With SegWit, spammers can still make a block that takes 3 minutes to validate, the same as without SegWit. They just wouldn't use a Segwit-style transaction to do so.
It's not "crippled", merely "not enhanced".
@_date: 2016-12-12 18:20:30
For 8 to 12 hours, you will need something like an Intel Core i5 quad-core 3.0 GHz CPU or better.
It would be nice if Bitcoin Core would take a lesson from Ethereum on this. I just did a sync with Ethcore parity yesterday using --warp mode on my dual-core laptop, and it took about 5 minutes.
@_date: 2018-06-15 15:47:18
Full nodes give you read-only access to the bitcoin ledger. Mining nodes have read-write access. 
@_date: 2016-11-27 13:30:13
If you look at these blocks, you'll notice they don't actually have very many transactions. For example, the 3.7 MB SegWit block  only has 468 transactions, with 467 inputs and 481 outputs. 
In comparison, the 975 kB block  on mainnet has 2,408 transactions, 4,515 inputs, and 5,176 outputs. 
A block made with a flat non-segwit 3.7 MB blocksize cap would be able to handle around 9,139 transactions, 17,133 inputs, and 19,642 outputs.
Despite being 379% as large, this testnet segwit block only got 9.8% as much work done. This is not an accident or a coincidence. The only way to make a very large block like this with SegWit is to make a small number of transactions with a small number of inputs and outputs but an artificially huge amount of signature/witness data. SegWit's 4x discount for witness data incentivizes transactions that are larger than normal due to complicated scripts and signatures.
The only known use for transactions with bloated witness data like this is spam. The 3.7 MB blocks were made to test the network's resilience to spam, not to test SegWit's functionality or transaction throughput.
@_date: 2016-11-28 05:29:04
Yes, Ethereum had some DoS attacks that were due to bugs in the gas cost calculation. These bugs were not related to the adjustable gas limit mechanism. The hard forks fixed these issues, and Ethereum has not had any DoS attack issues since the second hard fork, and very few issues since the first anti-DoS hard fork.
The first response in mitigating these DoS attacks was asking miners to reduce their gas limit targets. This measure was effective at reducing the severity of the DoS attacks by about 5x within about 4 hours. If Ethereum had not had an adjustable gas limit, the DoS attacks would have been much worse.
@_date: 2016-11-27 16:35:04
Good point.
@_date: 2019-02-12 18:17:24
My facility isn't secret. 
@_date: 2019-02-12 00:32:38
JPMorgan's analysis is wrong. I mine Bitcoin in the USA for less than half of their estimated cost.
@_date: 2019-02-12 19:56:01
It's very useful to a mining farm admin. It's just useless to attackers.
@_date: 2019-02-12 19:37:33
That chart is an accurate indication of the revenue that miners receive. It does not tell you the operating costs for miners, which is the subject of this thread.
@_date: 2019-02-22 04:20:17
This article is misleading. Block size is averaging about [1.05-1.10 MB]( nowadays. There was one day (Feb 11) in which block sizes were close to 1.3 MB, but that's what we call "[peaking]( not "averaging."
@_date: 2015-08-19 09:58:47
Slush is doing the same thing that we're doing. They have one server running XT, and one server running Core, and they're letting their users decide.
@_date: 2019-02-12 19:51:37
smarttrader does not understand what's going on. Having the ability to SSH into machines on your own LAN is a standard feature for miners. The S15 disabled that feature. Lightsword's exploit allows you to reenable that feature without Bitmain's permission. I consider this a good thing, as it empowers owners of their hardware to be able to use it as they see fit.
Third parties being able to ssh in and change the miner settings is still blocked by a simple firewall, which every sane person who mines already has.
@_date: 2019-02-12 18:15:56
No, on April 1st the rates in Grant increase to $0.033, and in 2020 they increase to $0.05. They don't triple until 2021.
There are many other energy sources that are comparably priced or better. The Central WA options were easy and convenient, but far from the only option in that price range.
@_date: 2015-08-17 05:58:46
Hi. I'm a medium-scale miner who recently switched over to Bitcoin XT. 
If you read BIP101 (Gavin's blocksize increase proposal implemented in XT), you'll see that it's an immediate increase to 8 MB, followed by doubling every 2 years for 20 years. The final blocksize cap should be 8192 MB in 2036. I think this growth rate might be enough for all of bitcoin's demand for the next 20 years, and it might not. In any case, it sounds technically feasible for us to be transmitting 8 GB of data every 10 minutes in 2036 in the bitcoin network. With the hundreds of millions of dollars already invested in bitcoin mining, although it wouldn't be easy, we could still do it today.
I think the fundamental problem that the XT release addresses isn't the blocksize increase, but rather developer gridlock. Up until now, all important decisions about Bitcoin's future have been made by a select few. Bitcoin has been an oligopoly of the developers. For the most part, those developers have done things pretty well. Recently, however, they have gotten gridlocked through their consensus process, and haven't been able to agree on something that the vast majority of bitcoin users agree upon. 
One of the things that some of the developers have been afraid of is fear itself. They've been worried about the effects of a hard fork, and worried that a hard fork might shatter confidence in the stability of Bitcoin. The truth is, hard forks are not a big deal. Bitcoin has gone through several of them already and survived, the most recent of which was BIP66. P2pool had a very rushed but successful hard fork a while ago in which about 1.5 weeks passed between the first release of the new version and &gt;95% penetration of that new version and rejection of all old versions. The network consensus mechanism requires 75% of miners to be using XT before changes to the protocol are made. That would mean there would need to be 3 miners in support for every miner against before anything happens. With that level of support among miners, it's likely that the fork would complete successfully once initiated.
If Bitcoin XT reaches 75% saturation, that indicates that miners overwhelmingly support the blocksize cap changes. Since Bitcoin XT does nothing controversial until that 75% threshold is reached, I don't see what the big fuss is about. It will only enact a currently-controversial policy if the controversy about the policy is over.
The basic structure of the Bitcoin protocol and network enables democratic actions like this. I much prefer rule by the miners and users of bitcoin over rule by the five or so people who have git commit access to the first Bitcoin client's source code repository. I don't trust any one group of people to make all the right decisions regarding how Bitcoin clients should be programmed. When the people in charge of making a client make mistakes, I want the option to switch clients to something more in line with my principles. If the client that I want to use does something that other clients think is harmful to Bitcoin, I want their clients to ignore mine. Consensus on the contents of the ledger will be determined by proof of work. This is what the core of Bitcoin is, not Bitcoin Core.
@_date: 2015-08-17 02:55:30
Individual workers with the same bitcoin address are not possible as far as I know. If you try to use them using _worker or .worker notation, your address will fail parsing, and p2pool will end up paying into the node default bitcoin address.
@_date: 2015-08-25 13:41:23
Large blocks suffer propagation delays and higher orphan rates. Miners have an incentive to keep their block sizes as small as possible.
The exception to this is selfish mining attacks. However, those require that the miner be very large, like 30% of total hashpower.
@_date: 2019-02-13 01:30:43
Only if you have access to the LAN that contains the machine.
@_date: 2015-08-17 21:37:32
Miners in China also have the option of only producing smaller blocks. They can use soft limits on blocksize for each node based on its internet connectivity.
Most Chinese pools are already using SPV mining. With SPV mining, you only download the header for a new block before switching over to mine based on it. With SPV mining, download speed doesn't matter as much. SPV miners would be able to handle very large blocks from other miners without much performance problems. 
Once a China miner finds a block, he has to publish it and get other miners to use it. This requires upload bandwidth. However, if a China miner has poor upload bandwidth, he can limit his blocksize to 250 kB or 2.5 MB or 25 MB or whatever he wants to make sure that his blocks don't take too long to send. The miner has complete control over the size of the blocks he creates. The block size cap is the maximum size allowed, but it does not force a miner to create large blocks.
@_date: 2017-11-16 01:20:06
Let's say you have 5 million users, each of whom opens and closes a channel once a month on average, and who make zero other bitcoin transactions. That's 10 million transactions per month. Channel open/close transactions are about 500 bytes each, so that's 5,000 MB per month. At 144 blocks per day, 30 days per month, this would be 1.16 MB of lightning channel open/close transactions per block. This leaves nearly zero space for non-lightning transactions.
@_date: 2015-08-23 20:07:48
That might be true.
@_date: 2019-02-12 02:12:52
The average cost of electricity isn't relevant. The cost to the average miner is what matters. Miners, as a class, pay lower-than-average rates.
The JP Morgan analysis assumes that miners pay about $0.07/kWh, because that's the average industrial electricity rate in the USA. But that's silly. Miners are like aluminum smelters. They don't say, "My home is New York City, so I'm going to build an aluminum smelting plant here regardless of what the power costs are." Instead, they say "My home is New York City, but I want to build an aluminum smelting plant, so I'm going to move to upstate NY near Niagara Falls to take advantage of the dams and excess hydropower there."
Electricity is the primary cost for smelting aluminum, so aluminum smelters have sought out the cheapest reliable power they can find. They pay around [$0.029/kWh on average]( The average Bitcoin miner pays something similar.
@_date: 2017-11-06 02:41:33
As with previous instances of large blocks, almost all of this block's space was taken up by one entity. This entity sent 36 large (33.7 kB) multisig transactions, each of which had 201 inputs and one output:
The following block had 14 of these 33.7 kB multisig transactions:
All told, that's 1685 kB in 50 transactions from a single entity.
This is likely the same entity that caused the previous bursts of large blocks:
Edit: Not all of the transactions in this burst from this entity were 33.7 kB, but 33.7 kB was a very common size. Some of the larger and smaller transactions in those blocks also had 201 inputs and 1 output and apparently came from the same entity and batch, but the multisig degree differed somewhat, and so the sizes differed. The total transaction volume in this burst exceeded 1.68 MB, and was probably around 2 MB.
@_date: 2015-08-23 18:32:44
maaku7 is saying that one thing that matters is the amount of time it takes to send a block, not the just amount of money it costs to maintain an average throughput level. Latency is affected by the number of hops required for traversal as well as the average bandwidth of each hop. He's right that latency is more important than bandwidth. However, matters to miners is latency for uploading blocks and bandwidth for downloading blocks and transactions. The blocksize cap is irrelevant for upload latency, as miners will always have the ability to reduce the size of the blocks they create in order to improve latency for propagating their blocks. It's only downloading blocks where the DoS concerns and the blocksize limit are relevant.
By the way, it's jtoomiM, not jtoomiN. I expect I miss a lot of emails from people who misspell it. (My email is probably more pleasant to read as a result.)
@_date: 2015-08-26 21:08:19


Miners will follow the economic majority in the event of a fork. They have to sell coins to pay for electricity. BIP101 measures one vote as one hash because it's much easier to add to Bitcoin than trying to program a proof-of-stake system just for e-voting.
Polls indicate that users strongly support larger blocks. 


What if someone decides to buy up 200 PH/s of Bitcoin miners and use it to perform 51% attacks on the network? Bitcoin is already predicated upon the assumption that miners will usually act in their rational economic self-interest, and will not use their hashpower in a way that does not produce revenue for them. Not being in the 75% supermajority after the fork has been triggered results in lost revenue. Switching away from the 75% supermajority after it has been triggered would create unnecessary chaos, and would reduce the real value of the miners' holdings. If you can't swallow the assumption that miners won't do that, then you probably shouldn't swallow the assumption that miners wouldn't collude to perform 51% attacks either.


The problem with that is "get a reasonable consensus". Consensus is a slippery word, and different people use different meanings. When used by the Bitcoin Core team, it means that any one person with commit privileges to the git repository can block the changes. This biases the process heavily towards inaction. Who chose the people in the Core team? They chose themselves. They are not elected. The only reason they have to be responsive to the desires of the users of the software is that if the code they write (or omit) no longer reflects the will of the users, then the users can switch to another piece of software. That is exactly what has happened here.
People have been creating different proposals and giving arguments for them for years. The problem is that several of the developers have a small-block vision for Bitcoin, and their politics are determining what code they let in and what proposals they listen to. Consensus among a small self-selected group is not a good system for making political decisions, and in this case, it has kept the process of investigating the different options stagnant at the initial stages.
@_date: 2019-02-08 10:05:48
Belief has nothing to do with it. The bump is due to the following factors:
1. The BTC exchange rate recovered somewhat, from a low of $3,200 on Dec 15th to $3400 today, an increase of 7%.
2. Deployment of AsicBoost on S9s has become more common (though not yet universal), allowing 20% lower power usage for the same hashrate.
3. A new generation of mining hardware has been released and is entering mass production. The new hardware has 25% to 60% more hashrate on the same amount of power as an AsicBoost S9.
4. Hardware has been changing hands. Machines have been transferred from facilities with unprofitable electricity rates to facilities with profitable rates.
@_date: 2015-08-17 13:38:19
If 75% of miners use BitcoinXT or another Bitcoin client that follows BIP101, then a fork will occur in January 2016 (or possibly later). At that point, you will have 3 BTC on the XT chain, and 3 BTC on the Core chain. If you try to spend some of them, the transaction should be accepted on both chains unless the Core chain is too congested. If that's the case, then you may have to pay a higher fee on Core than on XT, and manage two different wallets. This will only last for a short time (days? weeks?) until consensus is reached on which chain people think is most important.
@_date: 2019-02-12 02:15:03
JP Morgan's analysis was based on direct costs, not socialized or externalized costs. JP Morgan's analysis was based on the assumption that the average Bitcoin miner pays the USA average industrial rate. This assumption is false.
@_date: 2017-11-07 02:13:18
Blocks with Segwit are full when they hit 4 million weight units (4000 kWU). The typical block size for a full block with Segwit so far has been around 1.04 MB. That number is likely to increase slowly over time, but right now it's 1.04 MB. Segwit2x will instantly double that, so the typical size for a full block would be around 2.08 MB and rising.
The average transaction right now uses 3.85 WU per byte, which is why we're limited to 1.04 MB. Normal Segwit transactions use about 2.2 WU per byte, which would allow about 1.8 MB per block. However, it's possible to craft special Segwit spam transactions that use only 1.08 WU per byte, and that allows for spammers to put up to 3.7 MB in a single block, or 7.4 MB with Segwit2x. That only applies to special spam transactions (specifically, 63-of-63 multisig transactions with many inputs and only one output), so that 7.4 MB figure is the *worst case* scenario and not an indicator of actual everyday throughput.
@_date: 2017-11-23 04:00:18
That's the least important part, unfortunately. Also, many manufacturers of chips will give out free reference designs for PCBs that use their products, so it's not unusual.
@_date: 2017-11-23 08:32:19
I think you're getting Haarlem and Amsterdam confused with Harlem and New Amsterdam.
@_date: 2017-11-01 22:32:26
Block sizes are 1.04 MB, but block weights are 3.99 MWu. 
People don't use Segwit because it's largely a [tragedy of the commons]( The person who chooses to use Segwit gets a small benefit, but most of the benefit of a person using Segwit is to other people who may or may not be using Segwit. Consequently, few people use Segwit and Segwit is ineffective. I consider this to be a design flaw in Segwit, not a problem with humanity.
There are people right now who have a very good reason to use Bitcoin but can't afford to use Bitcoin because fees are too high. For example, people in Venezuela are stuck with a currency undergoing hyperinflation -- exactly the type of thing that Bitcoin was created to solve -- but they can't make use of Bitcoin because the fee for a single transaction is about 5% of their $40 monthly income. I consider that poorly served market to be an avoidable failure of Bitcoin.
@_date: 2015-08-23 14:05:14


If you don't like the VPS idea, then you can do a colocation. Send a server off to a datacenter with an encrypted filesystem. Don't give the colo company the password or key. It's a bit more expensive that way, and will probably cost you about $50/month instead of $10/month. 


It's 1.25% of one of our expenses, not 1.25% of operating profit. It's not even 1.25% of total expenses, since we've got really cheap electricity ($0.025/kWh) and we spend the same order of magnitude on labor as we do on electricity. We only spend about 3x as much on internet as the average house, and most of that is just for redundancy. We could pare it down to about 0.6% of our electricity costs, but currently we overpay because we're willing to pay 100% more to gain 0.1% in uptime.
When budgeting costs and doing forecasts, I don't even pay attention to how much we spend on bandwidth. It's a drop in the bucket.
Since everyone will have to pay bandwidth costs, it won't make mining unprofitable. It might make the eventual hashrate equilibrium a little bit smaller, but probably only 1% or so smaller.
Having bandwidth be a significant cost would have the effect of making bitcoin mines optimize their design in part for high bandwidth. Putting a financial incentive behind handling large amounts of data is pretty much necessary for becoming a large payment processing network. Insofar that bandwidth costs will increase as a portion of total expenses, I think that will be good. However, bandwidth costs are going to remain very small for all but the tiniest of miners.
@_date: 2015-08-16 14:47:06
For the +8191/8191 thing, all that matters is the total hashrate you have on the mining address, not the hashrate of each individual miner. 
A lot of Antminers have trouble with p2pool because of the frequent restarts, which cuts into their hashrate unless a custom (non-Bitmain) version of cgminer is used. You can get one such version for S4s here:   In particular, I advise against using p2pool with Antminer S5s, as p2pool sometimes results in S5's cgminer crashing, and all but the most recent firmware for the S5 have a bug which causes the S5 fan to stop immediately when cgminer crashes even though the ASICs continue to produce heat for a few minutes after that. I've run my own S5s on p2pool for several months without an issue, but one of our customers had two S5 hashboards burn out because of this issue.
I don't know about Coincraft and p2pool. We don't have any in our facility.
Spondoolies gear works perfectly with p2pool with no tweaking. About 95% of the machines we have on our p2pool nodes are Spondoolies. Spondoolies uses Minepeon, but the reason Spondoolies works with p2pool is because of the cgminer driver Spondoolies uses, not because of Minepeon.
Openwrt is like Minepeon. You'll have to be more specific before I can tell you if there would be issues with p2pool.
Thanks for the tip.
@_date: 2015-08-27 14:18:00
Jihan is referring to the fact that the cost of bandwidth per tx is less than the revenue that the fees bring in. About that, he is right. Bandwidth costs are very small.
However, orphan rates depend largely on the speed of the rest of the network. Unless you plan on paying to upgrade every other miner's internet connection, you will get elevated orphan rates for large blocks. 
So no, this two-sentence tweet does not mean that everything in the 16-page pdf I cited is wrong.
@_date: 2019-02-12 19:55:06
Not really. Being able to SSH into a mining machine is a standard feature of most mining machines, but Bitmain disabled that feature in the S15. This vulnerability allows the owner of the machine (or anyone else on the same LAN) to reenable that SSH interface. Firewalls prevent attackers from reconfiguring these machines without the owner's permission, same as for previous generations of hardware.
@_date: 2015-08-23 12:45:56
I'm a 0.25% miner. I run a 1 PH bitcoin mining hosting company in central Washington.
We spend about 1/80th as much on internet connectivity as we do on power. That gives us two 100 Mbps symmetrical fiber lines, of which one is just a backup. On that single fiber, we're running two full nodes, two p2pool nodes, about 1 PH/s of miners, and a few other things. With 400 kB average blocks, that uses about 1% of our available bandwidth. 
If we scaled up to 8 MB average blocks (40x higher transaction volume than today), that would use about 20% of our bandwidth, while still only costing 1.25% as much as we pay in electricity. 
Currently, transaction fees are about 0.1 BTC per block. If transaction volume were 40x higher, that would be about 4 BTC per block. This increase would more than compensate for the increased bandwidth and storage usage.
Edit: 0.25% miner, not 0.4%.
@_date: 2017-11-06 09:57:25
I see, you're thinking in terms of kilobytes.
If it was spam (i.e. if their intent was to harm the network), then the size of the spam was limited only by the fees they could afford to pay. Since fees are based on weight units and not kilobytes, by using Segwit they would be able to afford to send more kilobytes of transactions for the same fee.
It's worth noting that I do not think this is spam. I think it's more likely that this is the work of a large organization like an exchange.
@_date: 2015-08-26 14:35:10
Continued here: 
@_date: 2019-02-12 19:43:57
Yes, I'm factoring everything into that, including labor, land/building lease, and water.
The difference between your energy charge and your all-in cost is probably mostly due to the demand charge. The demand charge might be what you're calling the "delivery charge". The demand charge is a monthly charge based on your peak power demand. The energy charge is a monthly charge based on your average power demand. The demand charge has units of $/kW, rather than $/kWh, and many customers will overlook it because they don't understand how to deal with the units. If you have a 24/7 constant load like mining, you can convert $/kW to $/kWh by dividing your demand charge rate by 730 hours (the average length of a month). For example if you have a $4/kW demand charge, that is equivalent to a $0.005/kWh increase in your energy charge for a 24/7 load.
@_date: 2019-02-12 19:32:46
Yes, this is definitely a thing, and I'm pretty unhappy about it. Please don't get any ideas. I prefer not to mention things like this because the more it gets talked about, the more people will do it.
@_date: 2015-08-01 02:15:58
That's a valid reason to run a slow full node, in my opinion.
I see a lot of people buying Raspberry Pi nodes or other underpowered machines with the thought that it would help the network, even though they aren't actually doing anything with the node directly. My rant was directed against them, not against full-verification wallet users.
My suggestion for you then is to limit the number of connections, and to disallow incoming connections. I think it is best to set up your node so that it is asked to send data infrequently, but so that it sends data quickly.
@_date: 2015-08-23 13:02:09
1.25% is the floor on what we can pay for internet connectivity. It's just two business-class internet connections from a local ISP in our neighborhood. We pay about $65 per connection per month. We spend more money on water than we do on internet. Even with 8 MB blocks, we'd spend more on water than on internet.
Keep in mind that running a full node is optional. Small miners can use pools, where the pool runs the full node. The bandwidth requirement for pooled mining is about 0.1 KiB/s per miner, and is not related to block size at all.
They can also run their full node on a VPS in a datacenter with much lower bandwidth costs than we have. You don't need your miners and your fullnode to be in the same place. 100 ms latency does not make a significant difference in stale and orphan rates when new blocks are found once every 600 seconds.
Sharing a 40 Gbps line is much cheaper per Mbps than having a dedicated 100 Mbps line.
@_date: 2015-08-23 13:55:57
It's easier to prevent BIP101 from happening by continuing to use Core than it is to trigger the fork with a minority supporting BIP101. In order to cause a premature BIP101 fork, you need &gt; 25% of the hashrate to lie, plus another 25% to support Core. Furthermore, you need the 25% supporting Core to not switch to BIP101 when seeing a BIP101 majority.
Spoofing hashrate to cause a premature fork guarantees chaos when no chaos is necessary. The main reason to spoof hashrate is not to defeat big blocks, but rather to profit off of chaos by using the exchanges with insider information. People with control of a lot of hashrate are not likely to do this, as they have a vested interest in seeing Bitcoin work well.
The problem I have with 9,500 out of 10,000 is that it makes it very unlikely that the attempt will succeed. Sometimes, the worst thing you can do is nothing. I think that allowing blocks to fill up with non-spam transactions will result in a lot of damage, and we should do everything we can to prevent that from happening. We're running out of time to do this. Setting unnecessarily high thresholds may reduce the probability of chaos during a fork, but it will increase the probability of chaos during congestion.
@_date: 2015-08-24 19:48:12


If my blocks get orphaned because they're too large, then I'll make them smaller. The protocol-level blocksize cap is to limit the download requirements for participating in Bitcoin; upload requirements for a miner have always been under direct control via bitcoin.conf.
Also, there is a lot of work in progress or recently completed to speed up block propagation for large blocks. [IBLTs]( for example, or Matt Corallo's relay network.
@_date: 2017-11-07 00:58:44
"Replay protection" is just a euphemism for "wallet backwards incompatibility." The only way to add "replay protection" is to change the transaction format so that a transaction signature that was valid before the fork is invalid after the fork and vice versa. This requires all wallet software to be rewritten or updated for the new transaction format. This would mean that anybody using old wallets like Multibit Classic would be unable to access their coins after the fork without extracting their private keys and importing them into a new wallet. On the other hand, given the way Segwit2x is actually written, Multibit Classic will just work, following whichever chain has more miners.
If you don't add "replay protection" for a system-upgrade hard fork, it results in a much smoother upgrade process for SPV wallet users.
If you want to split your coins after the fork, it's really easy to do so. Just send everything you have to yourself using a very low fee. Enable RBF if supported by your wallet. It will confirm quickly on the 2x chain, but it won't confirm at all on the 1x chain. Then, with a wallet on the old chain, resend it to a different address using a much higher fee. (If you didn't enable RBF, you might need to wait 24-72 hours before performing this step before it succeeds, but there's no harm in trying earlier.) Assuming your fee was high enough and that the 1x chain is still being mined, your transaction will eventually confirm on the 1x chain.
@_date: 2015-08-17 02:59:34
There are custom cgminer versions available for the S5 that don't lose as much hashrate with p2pool, but I've never been able to find the cause of the instability of some S5s on p2pool. I think it might be a hardware issue, as the batch 3 S5s (early January delivery) worked stably on p2pool but later batches (6, 7) do not.
@_date: 2015-08-24 09:31:30
Not at all. Even today with current revenue, we can afford to spend more on bandwidth. Perhaps not 80x as much, but definitely 10x as much. 
We would only need to spend 80x as much on bandwidth if average block sizes were about 8000x what they currently are. With the current fees of 0.1mBTC/kB, that would be about 320 BTC per block in fees. With that much additional revenue, we could afford the bandwidth and the hashrate.
In general, in the long run, revenue from larger blocks scales about 10x faster than bandwidth costs for a miner of our size. (In the short run, bandwidth costs don't increase for us at all, since we currently have a large surplus of bandwidth.) If bandwidth costs ended up becoming significant, there are many optimizations we could perform to reduce them without impacting transaction volume capability, such as moving our full nodes 300km (8 ms) away to Seattle, where bandwidth is much cheaper.
@_date: 2015-08-23 23:14:26
We're working on them. We have about 360 TH/s on our XT p2pool node, and 34 TH/s on our Core p2pool node. With 360 TH/s, we expect to find one block a week on average.
@_date: 2015-08-17 21:40:21
@_date: 2017-11-14 03:38:27
Every side also has an even greater incentive to *not* spam the network. Spamming is very expensive. 
@_date: 2015-08-27 02:49:20


BIP101's block sizes will not make it hard to run a full node on a tight budget. If you have a slow home internet connection or a slow computer, you might not be able to do it at home. However, it should always be possible to run a full node on a VPS for about $10-$40 per month. 
A 100 Mbps internet connection can handle 8 MB average blocksizes pretty easily. A 20 Mbps can probably handle whatever blocks we actually get with an 8 MB cap. After that, BIP101 says 2x blocksize every 2 years. Nelson's law of bandwidth growth is basically equal to that. I don't see what your problem is.
@_date: 2015-08-17 13:47:13
I don't like the fact that only miners have a voice either. However, this was implemented this way because measuring mining power as votes is very easy, and trying to measure the economic importance of bitcoin holdings any other way is much harder to implement. 
If you want a voice, you can rent hashpower on a website like Nicehash.com or Miningrigrentals.com and use that to vote for XT or Core. It will cost you some money upfront, but you should get 95% +/- 10% of it back over the course of the mining contract.
If you want to use Bitcoin without running a full node, you're free to do so. If you want to use Bitcoin without mining, you're free to do so. If you want to be a fully-fledged citizen of Bitcoin, then you need to participate in the infrastructure of Bitcoin, and you need to mine. This isn't an ideal system. It's better. It's a functional system.
@_date: 2015-08-18 07:32:01
If 8 GB turns out to be too much, there are several mechanisms available for keeping them under control. For one thing, miners sending out large blocks will experience high orphan rates if the blocks are larger than the network can actually handle. For another, miners and nodes can put a soft cap on the size it will accept for a block, with a higher limit if another block has been mined on top of it. This rule would disincentivize large blocks while preventing soft forks longer than a couple blocks.
I also think you underestimate how much extra bandwidth and storage capacity miners have right now. My mine spends about 1% as much on internet connectivity as it does on electricity. Furthermore, we only use an average of 1% of our allotted bandwidth for our two full nodes, and we don't even touch our backup lines. If we spent about the same amount on power and bandwidth today, we would be able to handle 10 GB blocks, at least as far as bandwidth is concerned. 
@_date: 2017-11-16 13:47:19






Simulations are much easier than the actual implementation. 
Sure, you can set up a single payment channel and send thousands of transactions per second, but that's not the point. The point is that transaction routing on a real lightning network is computationally expensive even when you have the whole peer connectivity graph stored on a single machine's RAM. If the connectivity graph is not present in RAM, and if you have to do network requests in order to figure out who is connected to whom and how much is stored in each channel, then the routing problem gets a lot harder.
In particular, the question I have is whether payments can successfully be routed when a person tries to spend most or all of the money that they have locked up in channels. [69% of Americans have less than $1000 in savings.]( That suggests that most users would be (nearly?) completely depleting their payment channels on a monthly basis. In order to be usable for these users, it would need to be possible for users to find routes for the last dollar in their channels; being unable to make a payment due to a lack of routes even though you have enough money in your wallet is not a very good UX. All Diane's simulation was able to show was that it's possible to find routes for the first dollar in their channels. That's easy to do, and not very interesting.
My suspicion is that the routing problem was easy when all of the channels could be used in either direction for any amount, such that the first possible path was also an acceptable path, but as time went on and many of the channels became unavailable for further use, the pathfinding algorithm had to do more searching and used more resources. That is, the computational complexity of the pathfinding algorithm might increase as channels become more unbalanced. 
@_date: 2017-11-14 03:18:36
That is a graph of the transactions that *don't* make it into blocks. Transactions that don't make it into blocks are irrelevant. Spam is only effective if it displaces real transactions in blocks. 
@_date: 2017-11-23 03:54:17
Actually, no, it's pot growing in Haarlem, The Netherlands.
@_date: 2015-08-17 13:29:00
You mean competition over bandwidth? Right now, we're competing over access to cheap power and access to good ASICs, and bandwidth isn't a concern. This gives China a pretty strong advantage in the mining sector. Adding another variable to the mix, like bandwidth, should make things a bit more even, with different niches in different regions. North America might have cheap power and cheap bandwidth but expensive capital costs, for example, whereas China would have cheap power and cheap capital costs but expensive bandwidth.
No matter what, competition will be intense. I think that increasing the block size limit makes miners compete over something that actually matters: the ability to include large numbers of transactions into blocks quickly. That's much better than simply competing over the number of hashes that can be calculated per block, in my opinion.
@_date: 2017-11-21 15:33:38
The people who are using Bitcoin are fine with the fees as is, but there are other people who are not fine with the fees who have consequently chosen not to use Bitcoin. 
@_date: 2015-08-23 19:07:38
Users who do not mine are not full-fledged citizens of the Bitcoin network. If you are an American, you might be allowed to open a bank account in Switzerland, but you are not allowed to vote there. Much the same with Bitcoin. Only miners can directly vote on what transactions to include and what protocol changes to make.
This is an undesirable and non-ideal aspect of the Bitcoin protocol, but it's also one of the aspects that make Bitcoin possible. I prefer a non-ideal system that works in reality over an ideal system that doesn't even work on paper.
Non-mining users can vote indirectly with their money. Miners have to sell coins in order to pay expenses like electricity, rent, and labor. If no non-mining users want to buy those coins, then the miners will have to switch coins or shut down their operations.
@_date: 2019-02-12 02:31:14


Nothing written down, just conversations I've had with other industrial miners over the years, and the research I've done myself on potential locations for my facilities.
Miners generally don't publish lists of the best places to mine because power availability in those regions is limited, and prices respond to demand. If a bunch of other miners come to that region, prices go up.
@_date: 2015-08-16 15:34:17
My reply:








@_date: 2015-08-16 10:36:56
Thanks! We're pretty excited about Bitcoin XT in part because it is a great example of the unique combination of democracy and do-ocracy of which bitcoin is capable. 
@_date: 2015-08-17 12:25:22
Fourth, you also need to have lots of transactions in order to have big blocks. Lots of transactions means lots of fees. If we are actually filling 1 GB of transactions every ten minutes, that would be a ton of miner fees. 
Miners have to build their facilities mostly in remote regions where power is cheap but bandwidth is expensive. Those transaction fees will offset miner bandwidth costs nicely.
People who just want a fully-verifying node could do so in a Linode or Amazon Web Services instance, which run in hub cities where bandwidth is cheap but power is expensive. The bandwidth and storage costs of running a full node without mining will still be significant, but it shouldn't be prohibitive.
For reference, it currently costs about $10/month to run a node capable of handling 20 MB blocks. 
@_date: 2019-02-08 10:07:28
$0.05 is *not* "really cheap". Cheap is sub-$0.03/kWh. Really cheap is sub-$0.02/kWh.
@_date: 2015-08-23 15:43:11
That's what Matt Corallo's relay network is for. When you find a block, you just publish the header plus the hashes of the transactions, so you don't need to send the full block. You have to have the same amount of average bandwidth, because you have to have the transactions already for this to work. Most large miners already use it.
The above notwithstanding, if blocks took longer to propagate, then we'd make them smaller. Every miner has control over the size of the blocks he creates. If a miner gets high orphan rates, then he has a financial incentive to reduce the size of the blocks he creates. Upstream bandwidth is entirely decoupled from the protocol-level blocksize cap.
The purpose of the protocol-level blocksize cap is to prevent large miners from DoSing non-mining full nodes and small miners by publishing large blocks which everyone else is "forced" to download. It's not the only mechanism possible to prevent this attack, it's just the one that we've been using since 2010. 
What would probably be a better anti-DoS mechanism is if miners followed a rule in which they would not mine *directly* on top of very large blocks. If, say, someone published a 10 MB block, my node might choose to ignore it at first. If a 500 kB block was mined on top of that, my node might then choose to recognize it. If the big block was 100 MB instead of 10 MB, my node might require 4 blocks on top of it before accepting it. Such a feature could be used to proportionally and adaptively disincentivize excessively large blocks without hard consensus rules requiring a hard fork to add or remove.
@_date: 2015-08-17 09:18:12


Better yet, contribute to the alternate subreddits.
@_date: 2015-08-17 13:30:40
Not surprised. A lot of ASIC miners don't work well with p2pool. I expect we'll see some standard stratum pplns pools support XT soon.
Anyway, your contribution is appreciated.
@_date: 2015-08-18 07:50:06
No, you are not guaranteed that transactions will occur on both branches. However, it should happen that way in most cases unless Core gets congested. In any case, I think collapse for the whole system is an unlikely result. If a transaction occurs on one chain but not the other due to congestion, I expect most people will judge the network with higher throughput and usability as having greater value. People would then start to care less about the slower network, and they would stop asking for symmetrical transactions, etc. People could also put different real values on coins in the two forks.
The only reason why I think a collapse would be possible is because people would be afraid of a collapse. The only thing we have to fear is fear itself.
@_date: 2017-11-26 08:13:34
You just replied to a bot.
@_date: 2015-08-23 13:36:57
I like to think ahead. One of the things I'm worried about with Bitcoin is what happens if the reductions in block subsidy results in &gt; 50% of the hashpower going offline. In that case, it will be very cheap for a malicious entity to rent or purchase enough hashrate just long enough to perform a 51% attack. 
Off-chain transactions like Lightning Network reduce miner fees, which makes the eventual problem of paying for the mining network bigger. However, if we scale up the on-chain capacity by about 100x to 1000x, we can pay enough fees to support the current mining infrastructure indefinitely. This is one of the main reasons why I support BIP101.
Mostly, though, I just wanted to illustrate that a 0.25% miner like me can afford the bandwidth of BIP101's blocks even without any technological advancement. In 2016, 8 GB blocks would be expensive but economically feasible. In 2036, 8 GB blocks probably won't even be expensive.
@_date: 2017-11-02 18:18:25
Yes, it's about block size. Blocks are full when they reach 4 MWu. Right now, we're reaching 4 MWU at 1.04 MB. 1.04 MB is lame. Segwit is not a very effective way of increasing the block size.


No, Bitcoin is king of the crypto hill specifically because it was first. I think it would be easier to fix the currency that already has widespread adoption and is usable except for one constant in the code than it would be to try to get the whole ecosystem to switch over to another currency.
@_date: 2019-02-13 15:07:20
You're basically describing the software that already runs on those machines. cgminer has a "load-balance" mode for splitting mining hashrate up between multiple pools. But you can't install anything on that machine or change the configuration of that machine unless you have access to the LAN on which the machine resides. The "exploit" only works if there are no firewalls between you and the machine. This means that the "exploit" only really helps the owner of the machine.
It's not so much an exploit as it is a feature that Bitmain disabled in this generation which can be re-enabled by clandestine means. Using this "exploit", you can re-enable SSH access to that machine. But keep in mind, pretty much every other generation of mining hardware had SSH enabled out of the box, and very few owners change the password on these machines (it's usually "root" or "admin"). Having SSH access within your LAN is just part of the security model.
@_date: 2015-08-26 14:34:42
I'm continuing a conversation with that was started in the [8/23/2015 discussion]( thread.
Quoth adam:










Maybe come to the  conference or watch the live-stream they're trying to setup and participate via IRC.


My response:


I do not think that fees will drop if supply increases with demand holding constant. I've made that argument at [this link]( but I will make it again below.
I think we're currently at or close to the floor of the demand curve in terms of fees -- that is, I think that fees per kB are currently low enough that people don't care about them and don't pay attention to them. The main thing that determines fee/tx is the default setting in the bitcoin clients that people use.
Fees reached their peak in 2013 when the average blocksize was about 150 kB. In December, total fees reached US$20k/day, or about 25Â¢/tx. Shortly after that, version 0.8.2 of bitcoin-qt was released, which dropped the default fees from 0.5 mBTC/kB to 0.1 mBTC/kB. Fees have never reached the same level since. 
During the stress tests, total fees reached about $10k-14k/day, or about 7.7Â¢/tx. Now that the stress test is over and blocks are no longer congested, fees are about $5.6k/day or 4.9Â¢/tx. Thus, when blocks were actually full, the average fee size increased by about 57%. Block transaction volume was about 58% higher. Unconfirmed transaction volume (and congestion) was higher than that.
These data suggest to me that the fees per tx do not change much with moderate block congestion. (This might be improved somewhat with better client support for changing or automatically determining fees based on congestion.) Furthermore, the degree of overcapacity (2.5x vs 6x) seems to be completely irrelevant. Fees were higher when we had 6x overcapacity than they are now. Fees appear to only change based on on limited supply if the blocks are actually full. If we wish to increase fees by limiting block sizes, we would have to ensure that most or all blocks were full. In order to achieve the $2/tx to $20/tx needed to pay for the current 300 MW, 400 PH/s mining network with 1 MB blocks, that congestion would need to be extreme.
I think that artificially limiting blocksize to drive up fees is a terrible idea, since:
1. It ensures that some or most demand will not be met in a timely fashion.
2. The amount of demand that is met is fixed vs. time and does not scale to meet spikes in demand.
3. It does not inspire confidence in the capacity of the Bitcoin system.
4. It worsens the user experience considerably (long and unpredictable delays in transaction verification--block delay projections depend on how many competing transactions are added with higher fees and many other factors).
5. It promotes non-revenue-generating off-chain transactions as a method of fee-dodging and worsens the problem of paying for mining. Off-chain transactions do not generate fees in proportion to how much value the blockchain provides to the transaction as well as on-chain transactions do.
6. It promotes the creation or use of altcoins with larger or more frequent blocks. 
7. It makes bandwidth costs for miners irrelevant, and doesn't encourage miners to improve their connectivity. The capacity of Bitcoin transaction processing infrastructure will not improve, but power consumption for mining may continue to increase. 
8. The environmental cost per on-chain transaction will increase beyond the current 62.5 kWh/tx.
My preferred method for increasing fee revenue is this:
1. Scale up volume aggressively. Aim for 100x growth in 15 years. That would correspond to 36% growth per year. (Bitcoin demand has been increasing about 50% to 100% every year so far.)
2. Keep the default fee/tx in real (USD) terms somewhere between $0.05/tx and $0.25/tx until miner revenue comes 50% from fees, then start to reduce the default fee/tx in order to keep total real miner revenue close to constant.
If volume can't be scaled up fast enough, then average fees will need to increase. It will be much easier to scale up revenue if Bitcoin has high transaction volume in e.g. 2025 than if we scare users away with limited supply and high fees. 
@_date: 2015-08-23 19:11:39
A valid point. If I wanted to be more verbose, I would have said "50% plus some margin for a reasonable degree of statistical certainty." I think I'm usually more than verbose enough, so I left that bit out.
It wouldn't be too big of a mess, though. Each BIP101 miner who creates a large block would see it get orphaned. BIP101 miners who followed that block would see their blocks get orphaned too, until they were overtaken by the non-BIP101 miners. After some time (probably a couple of days), all BIP101 miners would either switch back to non-BIP101 or manually soft-limit their blocksizes to &lt; 1 MB. People who don't mine would be minimally affected, except for the news articles spreading FUD and causing the exchange rate to plummet. Eventually, people would see that everything is still working for non-miners, and the price would recover.
@_date: 2019-02-14 04:37:48
My customers are people who pay me to host their machines for them.  I had a few ask me to ship their hardware to the middle east a while ago because they were able to make more profit there than mining with me.
@_date: 2015-08-26 22:25:34
Bitcoin block size has been roughly doubling every year for the last few years. Before that, growth was even greater. If the 100% annual growth trend continues, blocks will run out of space in a couple of years following Adam's schedule. We would get:
* 2015: 0.5 MB blocks, 1 MB cap
* 2016: 1 MB blocks, 2 MB cap
* 2017: 2 MB blocks, 2 MB cap, mild congestion
* 2018: 4 MB blocks, 4 MB cap, mild congestion
* 2019: 4 MB blocks, 4 MB cap, 50% unmet demand
* 2020: 8 MB blocks, 8 MB cap, 50% unmet demand
@_date: 2015-08-01 03:24:14
Even if the reason they were sent was to deny service to others? The current transaction volume appears to be the result of a deliberate attempt to gum up the blockchain, which sounds like an attack to me. If you attack a friend during a sparring match, it's still an attack. If the purpose of a message or transaction is simply to take up space, it's spam. That's how I see it. In this case, I see "stress test" and "spam attack" both to be accurate descriptions. 
@_date: 2017-11-14 22:55:11
There are four problems with your plan:
1. A 1070 would get around 1 GH/s. In comparison, a single ASIC miner gets around 13500 GH/s, and the network has about 500,000 of these machines, for a total of around 6,000,000,000 GH/s. Your GPU would help to clear an expected value of 0.00007 extra transactions per day.
2. After two weeks, the network difficulty would increase by 0.00000000016% as a result of your mining, which would completely negate any transaction throughput benefit you might have conferred.
3. By mining free/low-fee transactions, you are choosing not to mine high-fee and medium-fee transactions, slowing their confirmation. The minimum fee for rapid confirmation would rise slightly, and people who wanted rapid confirmation would start paying higher fees.
4. Your 1 GH/s of computation would use about 200 W of power, whereas 1 GH/s from an ASIC would only require 0.1 W of power. You would be using power and trashing the environment for nothing.
@_date: 2019-02-15 00:56:54
Yeah, it's really unfortunate the way things played out.
I remember in 2017 a ton of my friends caught the crypto bug hard. They were all excited about free (as in speech) money and the ability to do ICOs and the libertarian dream. Then blocks on BTC and ETH got full, and fees went through the roof. Did they switch their enthusiasm to BCH, LTC, or ZEC, all of which had capacity for several times as many users as BTC? No. What happened was they got disillusioned by crypto entirely and gave up on the dream.
BTC could have been so much more if this community had just chosen a trajectory that enabled growth. Instead, the community decided to keep capacity low in order to push people away from doing on-chain Bitcoin transactions. That killed Bitcoin's momentum and caused the entire cryptocurrency community to stagnate.
@_date: 2015-08-16 14:04:08
Pool: stratum+tcp:/74.82.233.205:9334
Worker: [your bitcoin address]
Worker: [your bitcoin address]+8191/8191
Password: x
Don't include the square brackets. The +8191/8191 at the end is optional, but it will reduce the amount of share variance you get. Our node is the largest p2pool node in existence, which means that the default share difficulty is very high. You should use +8191/8191 if you're going to point less than about 15 TH/s at our node. It won't affect the average revenue, it doesn't affect the block variance (which is usually more significant), and it puts more load on our server. The password is ignored.
Our new IP address after the change should be 208.84.223.121, so you might want to add stratum+tcp://208.84.223.121:9334 too. You could also add Windpath's node (check  for details), as his is optimized a bit more for remote users and has a nice interface. You should also check your latency to our IP address (ping 74.82.233.205), as high latency will adversely affect p2pool revenue.
Keep in mind that p2pool is a very small pool, so variance will be high. Average payouts should be high over a long enough period of time, but p2pool should only be used by patient ones.
@_date: 2015-08-25 15:16:59
If you have 30% of total hashpower, you can do a selfish mining attack with or without large blocks. In fact, a large-block selfish mining attack may be less effective, since selfish mining attacks generally rely on careful timing of block publication, and it's hard to carefully time the publication of very large blocks.


If you're referring to the bandwidth costs being prohibitive for small miners, I suggest you read my comment from a few days ago. 
@_date: 2015-08-23 14:49:31
I'm not saying that big blocks will kill LN. I'm saying that small blocks will kill Bitcoin. 
At 300 MW (roughly our current mining network), each block uses 50 MWh, or about $4,000. At 800 transactions per block (current average with 0.4 MB blocks), that's about $5/tx needed to supply that hashrate. At 80,000 tx/block (40 MB blocks), that would only be $0.05/tx. If we can scale Bitcoin volume up 100x while having a minimum (or average) fee around $0.05/tx, we should be able to handle 300 MW indefinitely.
If we don't provide at least $2,000/block in fees+subsidy, it will become quite cheap to perform 51% attacks. Lightning Network is really cool in a lot of ways, but it makes this problem worse.
I don't know where your 13k/sec bandwidth figure comes from. Do you mean 130 kB/s? That's about what our nodes are currently using with 400 kB blocks. At 4 MB blocks, that would be 1.3 MB/s. Yes, I think that some people are making the argument that miners won't be able to afford 1.3 MB/s. I'm making the argument that we can afford it and that miners would get more in revenue than they would spend on bandwidth.
@_date: 2015-08-17 21:24:51
China has too large a share of all bitcoin mining right now. If the block size increase reduces the amount of mining in China by making bandwidth more important, that will improve centralization. 
Having too much Bitcoin infrastructure in one country is a problem because it makes Bitcoin susceptible to political attacks. That is particularly true of China due to China's history of censorship and financial controls. One of the big reasons why the internet is slow between China and the rest of the world is the Great Firewall/Golden Shield project.
However, I think that the blocksize increases will leave mining and running full nodes in China still feasible. I believe your 12% figure is for residential broadband speeds. Running a full node on a home internet connection is not the best way to do it. Most full nodes will be run on virtual private servers like Linode, Rackspace, Amazon AWS, or the like. Those datacenters have much better internet connectivity than residential areas.
It's worth mentioning that a miner can run a full node in an urban datacenter with high bandwidth in Shenzhen or Hong Kong and mine to it with miners on a remote farm in Sichuan. Residential internet connections do not need to be the limiting factor.
@_date: 2015-08-18 04:32:25
When the hard fork happens, all of the assets you had before the hard fork will exist on both forks. You can spend assets symmetrically on both forks as well. There's not much for you to lose by the hard fork as a regular user. 
The people who have the most to lose is the miners who choose the wrong hard fork. They will be mining blocks which may end up as orphans. This is a good reason why BIP101's consensus rule should be miner supermajority, not stakeholder supermajority.
I find it interesting that you describe the XT plan as dishonest, but you don't label the noXT plan as dishonest. The XT plan very clearly states its intentions and follows it. The noXT plan is the one that intentionally misleads.
@_date: 2015-08-23 13:17:49
BitcoinXT will not break from Bitcoin Core unless it has at least 75% support among miners. Polls suggest that support for big blocks among non-mining users is similar to or greater than support among miners. As such, I don't see how Bitcoin XT can be accurately described as forking without agreement from the economic consensus. If &gt; 75% support XT and &lt; 25% support Core, then if anything, I think Core would be the altcoin.
I think the definition of "altcoin" should include a sense of permanence. Altcoins are intended to operate in parallel with their parent coin indefinitely. Hardforks are intended to be resolved as quickly as possible, with only one dominant branch.
@_date: 2015-08-17 09:15:21
Thanks for that relevant quote. If the China pools don't join XT, and if a Core version isn't released with Gavin's BigBlocks patch which they use, then BIP101 will not succeed, and XT will behave in essentially the same way as Core. 
@_date: 2015-08-16 15:31:09
We've been able to do S4s on p2pool without any major issues. You just have to change the cgminer version for optimal hashrate. You can use smit1237's firmware to prevent the cgminer version from getting replaced each time the miner is rebooted.
With S1 through S4, if you don't change cgminer versions, you'll lose about 3-6% of your hashrate. With the S5 on most firmware revisions, you'll lose about 3-6% of your hashrate and also run the risk of damaging your hardware.
The Technobit miners actually use a bunch of different mining ASICs with different cgminer drivers, so even that isn't enough information to tell you if it will work well on p2pool.
With most of these machines, the only issues you are likely to have are a modest reduction in the effective hashrate. If your goal is to make an early vote for Bitcoin XT and encourage other pools to switch, then you might think this is an acceptable price to pay. Exception: S5s.
@_date: 2015-08-16 16:42:24
No, I didn't know. I was just hoping for bragging rights.
@_date: 2017-11-22 16:23:29
They may be different people, but it's obvious that their system design was "inspired" by Bitmain's. Whether or not Bitmain's ASIC design was reverse engineered and/or copied by Halong Mining is not obvious, but the copying of the tube, control boards, hashboard, and cooling design is obvious even from a couple of photos.
@_date: 2017-11-16 23:38:35


That is not an accurate representation of Fyookball's claim. Fyookball ("the very first article") claimed that a decentralized network could not be sustained. He did not claim that it was impossible to build.
The main objection that [Fyookball raised]( was that channels would get depleted quickly in a real lightning network unless it was centralized because large networks would require a lot of hops, and a payment depletes the channel at each hop along the path. This means that 1 payment would deplete e.g. 20 channels. Once many channels have been depleted, routing gets hard for payments that are larger than the typical channel has in available funds. This happens quickly, since the probability of success for a given path is *p^n* where *p* is the probability that a single channel can handle the payment and *n* is the number of hops -- e.g. *0.8^20 = 0.01*. In order to disprove Fyookball's claim, she would have needed to show that the simulation could run indefinitely without payments becoming unroutable over time. She did not do this.
She also stacked the initial conditions in favor of making routing easy. One of Fyookball's objections was this:


Diane Reynolds initialized every single channel with the same amount, which was 10x as much BTC as the biggest payment, and used 14 channels per user. In other words, she completely ignored that point of Fyookball's. The biggest payment anybody made in this simulation was 0.7% of the poorest person's wallet value.
@_date: 2015-08-16 12:40:05
@_date: 2015-08-23 17:24:18
I've presented another anti-DoS mechanism in  A hard protocol-level blocksize cap is not the only way to protect against DoS attacks using large blocks. It's just the oldest way.
Selfish miners generally prefer small blocks over large blocks. In order for selfish mining to work, a selfish miner has to be able to publish several blocks in a short time period, and they have to be able to do it in response to the competition getting close. With large blocks, you lose control over the publication time. You have to start broadcasting the large block early enough that it's finished before the rest of the network catches up. Without the ability to control the publication time for large blocks, I think the potential gains to be made by excluding low-bandwidth miners would be balanced or outweighed by the higher orphan rates. I'm not sure about that, though. Do you know how to do the math to compare those effects?
Large blocks have higher orphan rates because they propagate across the network more slowly than small blocks. In a worst-case scenario, a large-block miner might completely clog up the network, limiting the block rate to the rate at which they could be downloaded by 51% of the mining network. In this case, a miner that creates 8 kB blocks will have a lot more blocks incorporated into the chain than a miner that creates 8 GB blocks. The solution to dealing with large-block spam is for miners to simply not download those blocks, and at the extreme point, that's just the natural result of dealing with congested pipes. A selfish large-block miner cannot DoS more than 51% of the hashrate at once.
If the high-bandwidth pools are in the hashrate majority, then they can beat bandwidth-constrained miners. If 10% of pools do not have enough bandwidth to keep up, they have a few options:
1. The 10% of miners using those pools with poor connectivity can switch to pools with better connectivity.
2. The pools can switch to SPV mining, only downloading and verifying the headers before beginning to mine on top of it. If the pools notice a significant portion of large blocks with errors in them, they can blacklist nodes that repeatedly publish invalid blocks. Nodes can set a flag to indicate whether they have verified the block they are forwarding. Mining nodes can give priority to blocks that are verified over blocks that aren't. This would further slow down the propagation of large blocks and increase their orphan rates further.
3. The pools can stop being such cheapskates about their bandwidth bills, and upgrade their systems to be able to handle the BIP101 capacities. This should not be very expensive for medium-sized miners (0.01% to 1.0%), as I have argued elsewhere.
4. The pools can give priority to downloading small blocks. Large blocks will tend to lose orphan races. Selfish miners with unlimited bandwidth will have to balance the higher orphan rates against the higher fees and potentially the selfish mining benefits.
5. The pools can choose to only download (or build a chain on top of) large blocks if they are the parent of another block. This would be like selfish mining, but against the large-block publisher. It would be an attempt to intentionally orphan large blocks in order to disincentivize their creation.
Ultimately, these 10% of pools may fail. If that happens, the remainder of the Bitcoin mining network will have higher performance. I am not certain this is a bad thing.
@_date: 2017-11-22 16:26:36
No, I did not. They look like a bad deal. Also, my datacenter is at capacity already.


Citation needed.
@_date: 2017-11-07 07:31:40
That's not proof of spam.
@_date: 2017-11-14 03:37:18
This was not caused by an increase in transaction volume, but by a decrease in block rate and a reduction in the transaction clearance rate.
Over the last few days, about 60% of the network hashrate left the BTC blockchain to mine on BCH.
This caused the block interval on BTC to increase, exceeding 30 minutes per block on Nov 10th-11th.
As 1/3 as many transactions were getting confirmed per hour, this caused congestion and an increase in fees.
Users responded by sending fewer transactions. The transaction creation rate fell slightly from roughly 3.50 tx/sec during the week preceding congestion (Nov 4th-9th) to about 3.45 tx/sec during the congestion.
@_date: 2015-08-19 09:19:23
Okay. I was asking if you were Chinese. I thought you might be, since you were citing the China example and you speak English as a second language.
@_date: 2017-11-22 14:16:35
Big miner here. We're generally pretty flexible about form factor. Most of us just stack the machines on a bunch of shelves with the exhaust air all going one way. Whether the machine has a 4U 19" rack-mountable chassis or a tube design doesn't really matter, as they all fit on a shelf.
@_date: 2015-08-23 18:56:23
The issue is convincing people to switch to the new system. I don't think you'll be able to convince the early adopters to switch. Vendors who sell stuff will place more value on the scarce resource, so they won't want to switch. Et cetera. There's a reason why 1 DOGE is worth nearly nothing.
You will have two separate forks. One will be scarce, the other will be less scarce and inflationary. Which do you think will be valued higher by the economic majority?
If the rest of the world wants to collude against the early adopters to devalue or ban the use of Bitcoin21 in favor of BitcoinUnlimited, so be it. Bitcoin21 holders will continue to use it amongst themselves, much as they currently do. Every now and then, someone will get sick of the rampant inflation in BitcoinUnlimited and will switch to Bitcoin21. Both branches can coexist.
@_date: 2017-11-22 14:12:35
They claim 0.075 J/GH in their literature. However, their video shows the miner getting about 15.5 TH/s on 1470 W, which is **0.095 J/GH**. In comparison, a typical Antminer S9 gets about 13.7 TH/s on 1460 W, or 0.106 J/GH. 
Form factor and design looks like a blatant ripoff of an Antminer S9. In fact, I'm not even sure that the device in the video *isn't* an S9. The only differences that I can see are that the case is slightly different (smooth, no rails, and a different decal), and the hashboards have the power connectors on the right side of the PCB (like the L3+ and D3) instead of being on the left side (like the S7 and S9). The PSU also looks like a clone of the Bitmain APW3++.
The cgminer text in the video only shows 4.010 TH/s ([0:31s]( bottom line). I'm curious how they get from 4.010 TH/s reported by the miner to 15.5 TH/s reported by the pool without some black magic.
This all looks rather scammy to me. I personally will not be risking my money for a 10% reduction in power consumption with a 2-month later scheduled delivery date from an unknown supplier.
@_date: 2019-02-09 15:07:55
We're talking about miners here, not the general population. Miners have sought out the world's cheapest electricity. Miners will relocate their businesses to wherever power is cheap.
@_date: 2015-08-27 02:53:31
Thanks, reading it now.
@_date: 2015-08-24 10:23:48
So far, the impression I've gotten from the Bitcoin users, companies, and miners is that about 85% support a dramatic increase in the blocksize cap.  The 8 MB immediate target seems to be agreeable to most of those. 
Support for XT is more contentious at about 55/45.  People are less convinced of the need for a new governance model than they are for larger block sizes.


People want Bitcoin's on-chain capacity to scale. They want to Bitcoin to go to the moon. The ethos of Bitcoin that I've seen is that it's decentralized, deflationary, cheap, and fast. 
Decentralized does not mean de-industrialized. It means something something like how the steel industry works. Anybody can mine iron ore out of the ground. Anybody can smelt their own steel. Most people choose to use large steel mills instead, because they're much more efficient. A large percentage of all steel is produced in China, and steel mills tend to be located in areas with cheap energy and good transportation. Within the hotbed regions for steel production, the mills are owned by different companies and the production processes differ. The end results differ. While production is geographically somewhat centralized, it is functionally moderately decentralized.
Steel has a few well-known and well-defined grades based on composition. Several different organizations exist for defining those standards, like SAE, ISO, DAE, and JIN. The standards organizations define steel grades according to what they think users will want. Users get to decide what steel grade they want, and the producers have to make what the users want or go bankrupt. Because several different groups of people are writing the codes to define steel grades, it is decentralized. Standards organizations do not have the ability to manipulate their standards (e.g. by banning trace elements found in a competitor's mine) because the users will choose grades defined by other organizations if they're a better value.
Right now, the centralization of development in Bitcoin is like having only one standards organization. This disempowers the users, as they only get to choose what kind of steel they want if they want the same kind of steel that the one standards organization thinks they should want. In this case, the Bitcoin Core developers have decided that users should get small blocks, but users want big blocks. Thus the source code fork and creation of XT.
Tell me, Adam, do you think we should set the block size cap so that the average block is completely full, or should the average block have unused capacity? If it should have unused capacity, how much? 
Personally, I think that the blocksize cap should be between 2x and 10x the average blocksize to allow for diurnal variation in transaction volume and demand spikes without backlogs. I think that larger than 10x caps would probably be okay too, as I think that high orphan rates would make miners reluctant to create blocks that the network can't handle quickly.


I don't follow what you're inferring here. Can you elaborate?


I agree. It would be best if you and the rest of the core developers joined the existing consensus among miners and users in support of large (&gt;= 8 MB) blocks. 
@_date: 2015-08-23 16:35:08
If they're ticked off that early adopters have so many coins, they won't use Bitcoin. They'll continue to use fiat instead.
What you're describing is a wealth redistribution. What system do you think presents more value to stakeholders in the system: a system in which a person's stake can't be revoked without his permission, or a system in which it can? In order to revoke the stake of the early adopters, you need to create a new system (a blockchain fork) in which stakes can be revoked, and then you need to convince the early adopters that it is in their interest to switch to the new system. 
@_date: 2015-08-23 15:29:22
Some people are arguing for lifting the 21M coin cap, yes. There are two classes of arguments that can be made against raising the 21M cap:
1. Raising the 21M cap would require a hard fork, and hard forks are dangerous.
2. Raising the 21M cap would be really stupid, as even a hint of doing this would devalue the currency.
You're making a classic slippery slope argument. If we get rid of reason  why not to do it, there's still the other reason not to do it, and  is the only reason that's needed.
Just because gay people are allowed to marry in the United States doesn't mean people should be allowed to marry horses or goats, if for no other reason that non-human animals are not capable of legally providing consent.
@_date: 2015-08-31 21:08:55
May I shamelessly advertise our services? We do hosting for miners. Our prices are about 12Â¢/kWh all-in. We seem to be pretty popular among European miners due to our lower cost and the absence of VAT and import taxes.  for more info.
@_date: 2015-08-23 15:55:48
I said (and meant) increasing the block size, not the block size cap. The more people who are using Bitcoin, the greater demand will likely be for it.
The "deal" I was referring to in "blocksize dealbreaker" is Bitcoin's inherent value proposition. I don't think many people will want to use Bitcoin only if block sizes are kept small. Blockstream might be among those, and maybe the "fee market" zealots, but that's probably about it. If, on the other hand, the 21M coin cap was lifted, I think a lot of people would feel betrayed, and would consider that a dealbreaker for using Bitcoin, and would consequently liquidate their holdings and switch to something else.
@_date: 2015-08-17 21:14:25
It is a functional political system. Waiting for consensus on mailing lists where any one entity can block the process is not a functional political system. 
In order for any system to work, the politics of governance for that system need to be functional as well. I think democracy among the miners is a better system than consensus among core developers for several reasons. One reason is that democracy among the miners is fundamentally how the Bitcoin protocol operates. Another reason is that consensus politics don't work -- there will always be someone who objects to any controversial action, and often (like with the block size debate) the worst possible outcome is to do nothing.
@_date: 2015-08-19 11:30:40
I did not say that. It's possible he changed his mind, and now thinks that changing the blocksize is against what makes Bitcoin Bitcoin. It's also possible that he thinks that the Core developers are what make Bitcoin Bitcoin. There are other explanations possible. I just think that this 2013 post is interesting in the current context.
@_date: 2017-11-07 01:20:54
I'm not sure there's evidence of 0-fee transaction spam or 200 AWS nodes either. All I see is evidence that blocks are filled with high-fee transactions, which means that any transactions sent with low fees are not getting mined and are sitting around in the mempool for a while.
Zero-fee spam is ineffective.
@_date: 2015-08-23 15:49:35
The cheapest power in the world is renewable. We, for example, use hydropower to get our 2.5Â¢/kWh rates. I want to outcompete miners who are running on coal, because coal emissions kill about 300,000 people per year worldwide.
Weeding out the miners who have very low bandwidth will cause the rest of the bitcoin network to work faster and at higher capacity, which I also consider a good thing. People in Africa will have to use SPV wallets, which I consider a bad thing, but I think that is a minor problem and outweighed by the benefits.
Non-mining operations like Coinbase can move to where the bandwidth is more easliy than mining operations, actually. Bandwidth is far cheaper in major hub cities like San Jose, Tokyo, Hong Kong, and Frankfurt.  . These internet hub cities tend to be in large population centers, and the electricity prices there tend to be pretty high. Electricity prices are usually lowest near hydroelectric dams in remote areas, where supply is high and demand is low. Because of their location, bandwidth tends to be limited.
@_date: 2015-08-18 04:09:13
When Gavin and Mike created XT with BIP101, they were calling for a referendum. Using Core is a vote against BIP101. Using XT is a vote for BIP101. Using NoXT is neither.
Running NoXT is MAD. Mutually assured destruction. It's holding bitcoin hostage unless you get your way.  It's sabotage. It's like being the whiny emo kid who threatens murder-suicide the moment a potential breakup is discussed. 
Running XT is a reasonable voting mechanism. Unless 3x as many miners use XT as use Core, XT will do nothing. By the time a fork happens (as designed by Gavin), the fork won't be contentious any longer. If you oppose the existence of XT, you are opposing democracy.
If you think BIP101 and the blocksize increase is really that contentious, then you shouldn't be worried. 75% (3:1) support is a pretty high bar. When is the last time you've heard of a political election with &gt; 75% support that wasn't a sham election for a dictator? The 75% threshold will only be reached if a rapid switch to enable larger block sizes is clearly a good idea for the network.
@_date: 2015-08-17 12:14:58
Third, raising the block limit does not equate to having bigger blocks. Miners can soft-cap their blocksize, and are incentivized to soft-limit their block sizes to what other miners can quickly handle. If a miner creates a large block, they get more in transaction fees, but they pay for it in higher orphan rates. Large blocks take longer to propagate through the network, so they run the risk that other miners will find a competing block before the block finishes propagating.
@_date: 2015-08-19 06:56:09
Nope, that was Slush.
@_date: 2019-06-29 22:23:54
Tl;dr: If you have access to electricity below $0.07/kWh, you may consider mining. Otherwise, you'll spend more total money on hardware, electricity, and labor than you'll make.
Keep in mind that the mining block reward is getting cut in half around May 2020. If you're making $0.14/kWh in revenue now, the difficulty will probably adjust to the point where you're making $0.10/kWh or less by December, and by May you'll be making only $0.05/kWh. If you're paying, say, $0.10/kWh for electricity, you can see how this can be a problem.
A used S9 costs about $350 right now. If you have ASICBoost enabled, and if your electricity costs $0.10/kWh, you'll make [about $70 this month]( -- that's $155 revenue with power costs of $85. You can expect the mining difficulty to increase about 10% per month as long as the exchange rate is above $10k, so next month you'll make $15 less, or $55. The month after that, you'll make about $40, then $30, then $20, then $10, and then you'll need to shut off your machine. Over 6 months, you'll probably make around $230 in operating profit. Meanwhile, your S9's value will have dropped by 50%. If you resell your S9 in 6 months, you'll probably get about $150 for it after shipping costs. All told, you'll end up with about $30 more than you started with, and you will have put in about 10-20 hours of labor for that, giving you an effective wage of about $1.50/hr. And that's not taking into account cooling or noise effects.
If mining for slightly-better-than-breakeven revenue sounds like a fun hobby to you, then go for it. But if you actually want significant income, don't bother unless your power is cheap.
I wrote an extensive post on what people can do to mine at small scales [here]( if they don't have access to cheap electricity from their utility company. tl;dr: It's possible to make solar work for mining, but it's definitely not easy nor a quick buck.
P.S.: I run a 2 MW [mining datacenter]( in central Washington state on hydro. We have around 1500 miners. We currently pay $0.033/kWh. That might give you an idea of what you're competing with.
@_date: 2015-08-24 20:15:10
As long as people are afraid of XT's existence, bitcoin will be at risk. 
The only thing we have to fear is fear itself.
@_date: 2015-08-17 12:00:30
First, I think that there is currently a huge amount of headroom in network bandwidth. At my mine, we spend about 1/80th as much on internet connectivity as we do on electricity. For that, we get two 100 Mbps fiber connections, one of which serves as a backup, plus a 4/2 Mbps wireless link as a second backup. For the main internet line, we average about 1% of our pipe's capacity. That 1% is enough to run two full nodes, two p2pool nodes, and about 1 PH/s of miners. If we paid 80x as much on bandwidth and used closer to 100% of our pipe, we would have enough capacity for 8000x what we're currently using with 1 MB blocks. Coincidentally, that's about where BIP101's block size increases will end, at 8192 MB in 2036. So we're basically 20 years ahead.
Processing transactions should be the core of a miner's business. Bandwidth costs should be a significant portion of total expenses for a miner. Right now, they aren't. We aren't going to compete with Visa or PayPal unless we start engineering our systems to have plentiful bandwidth.
@_date: 2015-08-23 21:21:48
Currently, fees are determined by a gentleman's agreement and nothing more. There is no satisfying argument for why that gentleman's agreement will continue except to say that it has held up pretty well so far. That's why I think it's a reasonable scenario that transaction fees will continue to be about $0.10/tx, as that level is about the point where users don't notice them most of the time.
On the other hand, a gentleman's agreement will be unlikely to be enough to force higher fees. In order to have enough miner revenue with 1 MB blocks, fees would have to be on the order of $4/tx. People won't be okay with $4/tx just because it's the default option. That will need to be enforced.


No, I think you have that backwards. Hashrate causes the network to be strong. In order to run a 51% attack, a miner has to be in control of 51% of the network's hashrate. That means they have to be running 51% of the mining hardware in facilities like mine. If people shut off or lease out mining hardware to the highest bidder because it's no longer profitable to mine for one's self, 51% attacks become much easier.
Maybe I linked the wrong post. Try this one. 
@_date: 2019-02-13 17:34:24


If an attacker can convince the owner to run a particular piece of software on a different computer on their LAN, then the attacker can steal their hashrate without this exploit. If the attacker has access to their LAN (either directly or via installed software), they can just use the web interface to change the pool settings for that machine.


Mining hardware usually only has root accounts. Everything runs as root.


The only way to install a tool on one of these machines is to SSH in first. Because the S15 does not allow SSH access, you can't do that unless you've already used this exploit yourself.
This hack that Lightsword demonstrated is not a security risk. It's basically equivalent to jailbreaking your phone. It gives you power that the manufacturer didn't intend or want you to have, but it can only be done if you already control the device.
@_date: 2017-11-22 15:58:21
Personally, I hate the PCIE power connectors. The molex pins are only capable of about 8 amps each, and increasing amperage by increasing the number of wires is less elegant and more expensive than just using bigger wires and better connectors. I would much rather have a different connector, like maybe a pair of ring lug terminals. 
I think a larger form factor would be better. When installing S9s, we use the rails to join them into stacks of about 4 so that we can install a group of 4 as a cohesive unit. Trying to manage stacks of 4 without the rails will be unpleasant, as machines will want to slide off and make a mess. Even better would be if each machine was the size of 4. I think Spondoolies had the right idea with their SP50 design, which was a rack-mountable 10U unit (19" wide, 17.5" tall) . It's unfortunate that the SP50 never made it into production.
On the other hand, tube designs are usually pretty good in terms of airflow and cooling efficiency. They generally waste very little airflow. Good airflow designs in rackmountable form factors are possible too (e.g. Spondoolies) but they seem to require a bit more forethought and engineering than the tubes in order to get it right. 
I have no objection to the ethernet port.
@_date: 2015-08-23 13:29:57
The "correct" percentage, from several perspectives, is 50%. 
That's the threshold needed for a hard fork to persist. 
That's the threshold for majority rule in political systems.
One might argue that changing the protocol of bitcoin should require greater than 50% hashrate support because it can do all sorts of nasty things like change the block reward or otherwise violate the economic principles of bitcoin. However, it's worth remembering that if you can get 50% hashrate support, you can do much worse things without a fork at all, simply by denying transactions from being incorporated and blocking other miners access to the blockchain. With a hard fork, the conditions are much better, because people who don't like the rules set forth by the new majority can still mine to the minority fork by continuing to enforce the old rules.
Although 50% might be the most justifiable number from a logical perspective, it probably is not the optimal number.
Personally, I think that 75% is a better choice for BIP101 than 50% is, since having only 50% would result in a fork that isn't very clean, both due to economic effects and blockchain effects. With 75% (3:1 support), I expect the fork to happen quite cleanly. I would probably also be okay with 66%. Larger supermajority requirements make it harder for changes to occur. If Bitcoin is not allowed to evolve through orderly and semi-frequent hardforks, it will be supplanted by something else, such as an actual altcoin.
@_date: 2015-08-23 18:45:04
If you move to Kansas City (or one of [many other cities]( you'll have access to Google Fiber. 1 Gbps for $70. My home town also has 1 Gbps internet for $40. Bitcoin full nodes in your home will be practical there. It should never be the case that only datacenters have enough bandwidth for Bitcoin. It might be the case that a lot of home internet connections will not have enough bandwidth for Bitcoin. 
Mining in the People's Republic of Congo? You might have trouble with that, unless you're mining diamonds.
A full node in every home is not a reasonable goal for Bitcoin, in my opinion. It is not reasonable with 1 MB blocks either. The blockchain is already 47 GB. Each time someone uses Bitcoin, that size increases. It's not feasible to get every single user to download the whole blockchain.
It's also not necessary. The blockchain needs to be stored in enough places to keep the miners and the high-bandwidth hub nodes honest. Small leaf-nodes need to be possible for enthusiasts to run in order to verify that nothing fishy is going on. The rest of Bitcoin users can run SPV wallets, and only fully verify the cryptography relating to the addresses they interact with (plus a random sample of other addresses to establish that the server has a full and verified copy of the blockchain).
@_date: 2015-08-23 15:17:37
How do you assess support from users without a vote? BitcoinXT is a call for a referendum. It's asking the users for a vote of no confidence in the Bitcoin Core leadership. If the users do not support BIP101, they will vote against it by continuing to run Core. If there isn't a 3:1 supermajority favoring BIP101, then asking for a vote does no harm. The only ones who have anything to fear from this democratic "hostile takeover" are the ones who would be in the &lt;= 25% minority.
The main flaw of BIP101 in my eyes is that it uses hashpower as a proxy for voting power. This is not ideal, as it means that people who use bitcoin but who don't mine don't get a vote directly. However, I think it's better than ideal, because the ideal solution would be too complicated. Ultimately, I think the 75% threshold, coupled with the fact that miners tend to be more conservative than normal users and that informal polls indicate that most users support &gt;= 8 MB blocks, will ensure that at least an economic majority will support BIP101 should a 3:1 supermajority of miners support it. 
It's easy to implement a vote by hashpower. Hashpower votes also reflect something at the core of how bitcoin works: it's a distributed ledger where consensus is determined by proof of work. If you want to have a vote in that consensus, you should mine. 


No, it just got read by a bunch of people, and it seems that the majority disagree with you. This is how Reddit works. Incidentally, this is why the moderators of this forum have been moderating so heavily: the majority of redditors here hold opinions which the moderators disagree with.
@_date: 2015-08-23 13:50:47
There is a tradeoff on where you set that threshold. The tradeoff is between chaos and stagnation. If you set the threshold too high, then the ability of Bitcoin to evolve and improve is compromised. Unfixed flaws will build up over time. If hard forks do not happen frequently enough to relieve that pressure, there eventually will be an altcoin that surpasses Bitcoin by being nimbler, faster, more efficient, and more flexible.
Keep in mind that 75% is a 3:1 ratio. Most of the remaining 25% will be people who think that the blocksize increase is not a dealbreaker. Most people will switch over to support BIP101 once they see that 75% support it. I would be surprised if we had more than 5% of users who were so opposed to 8 MB blocks that they refused to follow the supermajority. If we had a 5% drop in one day in bitcoin price as a result, that would be unfortunate. However, I think that would be more than offset by the immediately 700% higher block capacity, which should drive the price up much more than the blocksize-dealbreaker crowd's influence.
@_date: 2015-08-26 17:08:43
BIP101 and any clients that implement it will only fork if they have a 75% supermajority of miners. "Consensus" is an ill-defined word, but 75% support is consistent with many definitions of consensus. If that level of support is reached, and if you insist on using the altcoin terminology, then it's more likely than not that the BIP101 branch would deserve the name "Bitcoin" than the non-BIP101 branch.
@_date: 2017-11-14 23:31:09
A GPU would be about one six-billionth of the network hashrate. A drop is about 0.05 mL, so six billion drops is about 0.3 ML (megaliters), 300,000 m^3, or 0.0003 km^3. That's comparable to a [small lake]( The ocean, on the other hand, has a volume of about 1,300,000,000 km^3. A single drop is a greater fraction of the size of a small lake than a small lake is of the ocean.
@_date: 2019-02-12 19:53:02
Miners fetch work from pools. Miners initiate the TCP connections, which allows their TCP connections to penetrate the firewall.
@_date: 2017-11-07 05:09:48


Yes, there obviously are plenty of people who are willing to pay $2 per transaction. That's fine. The problem is that there are other people who can't afford that, and who have legitimate reasons to use Bitcoin.
Bitcoin was created as a response to the 2008 financial crisis, and was intended largely to protect people from rogue banks and governments ruining people's lives through irresponsible actions. Venezuela's currency was trashed by irresponsible government action, with hyperinflation of about 500% per year. Venezuelans started to use Bitcoin as a stable alternative. When grocery stores ran out of food and other essentials, many Venezuelans started ordering stuff from Miami using Purse.io. However, that stopped when fees rose. Venezuelans earn on average $40 per month, so it's hard to justify spending 5% of a month's salary on fees for a single transaction.


I previously answered that question in [this comment](
tl;dr: Segwit doesn't have strong incentives for an individual or a company to use it. It's a case of the tragedy of the commons. There are a few other reasons, too.


Yes, I agree. That is one of the unfortunate aspects of Segwit -- it increases the hardware performance headroom we need by 3.7x while only increasing typical throughput by 1.04x to 1.8x. One of the reasons we need the 2MB blocksize limit is that it increases typical throughput by 2x while increasing the hardware headroom by 2x.
It's also important to not use the worst case number as if it were the best case capacity. This is a frequent point of confusion for people, so it's worth repeating. You can only get to 3.7 MB or 7.4 MB with specially constructed spam.
@_date: 2019-02-19 03:53:30
Here's a chart of Satoshi's post times on Bitcointalk:
@_date: 2017-11-22 14:20:33
I think it's far more likely that Halong Mining stole Bitmain's designs. These machines look nearly identical to an Antminer S7 or S9, and the purported efficiency improvements are pretty small (about 10%) and within experimental error, so it's quite possible that these machines are just S9s but with a slightly different chip voltage and/or frequency.
@_date: 2017-11-22 16:20:54
Actually, batch 1 of the S9 had *better* efficiency than later batches of S9s. Batch 1, 2, and 3 were actually 0.099 J/GH at the wall, as per spec, but every batch after that had about 5-10% higher power consumption. Bitmain increased the voltage on their chips for later batches in order to get better yields and reliability, since the RMA rate on the first few batches of S9s was very high. However, despite the increased power consumption, Bitmain never increased the power specs on their advertising material, and still advertise 0.099 J/GH even though they haven't shipped a miner with that efficiency for about a year.
They did something similar with the Antminer S7. The first few batches of S7 had no voltage regulation on the incoming 12 V line. If the PSU supplied more than 12 V, then the ASICs would get slightly more voltage each; if the PSU supplied 11 V, then each ASIC would get 9% less voltage. This was a very efficient design, as voltage regulators are lossy devices, but it gave Bitmain a headache with tech support because bad PSUs could cause the miner to crash. With later generations of S7, they added a buck-boost voltage regulator to make the voltage delivered to each ASIC constant regardless of the PSU type. This improved reliability with poor PSUs, but it also caused about 6% of all delivered power to get wasted as heat in the voltage regulation stage. When Bitmain released the S7s with this change, the efficiency dropped from about 0.25 J/GH to about 0.28 J/GH, but of course they didn't update their spec sheets and kept selling them as if they were still 0.25 J/GH.
In any case, the 0.106 J/GH figure for the S9 is the actual at-the-wall consumption for real devices measured by a third party (me), whereas the 0.75 J/Gh figure is their chip-level claimed efficiency, and their at-the-wall efficiency is 0.095 J/GH but not actually listed on their website.
@_date: 2015-08-23 18:27:06
A &gt; 30% miner can do a selfish mining attack whether or not large blocks are involved. I suspect that using large blocks makes it more difficult to perform selfish mining attacks, since selfish mining usually relies on carefully timing the publishing/propagation of blocks based on the progress of the rest of the network, and large block propagation is hard to coordinate.
@_date: 2017-11-22 22:18:08
I think you're referring to GMO, who is currently *designing* a 7 nm, with production expected for "[1H 2018]( -- i.e., around the beginning of summer. Actually, they *are* [planning on selling them](
@_date: 2015-08-23 14:22:55
And I said:








@_date: 2015-08-26 15:34:17
Bitcoin block size has been roughly doubling every year for the last few years. Before that, growth was even greater. If the 100% annual growth trend continues, blocks will run out of space in a couple of years following Adam's schedule. We would get:
* 2015: 0.5 MB blocks, 1 MB cap
* 2016: 1 MB blocks, 2 MB cap
* 2017: 2 MB blocks, 2 MB cap, mild congestion
* 2018: 4 MB blocks, 4 MB cap, mild congestion
* 2019: 4 MB blocks, 4 MB cap, 50% unmet demand
* 2020: 8 MB blocks, 8 MB cap, 50% unmet demand
@_date: 2015-08-24 08:48:27
Only if those miners were running at an operating profit margin of 0%. That is not typically the case. People stop buying miners when the operating profit is no longer sufficient to pay back the capital costs over a reasonable amount of time--that is, network hashrate growth usually stops while everyone is still making a profit on their day-to-day operations.
That said, I really wish Bitcoin's subsidy schedule had smaller and more frequent block reward reductions. Oh well. We'll have to make do with what we have.
@_date: 2015-08-26 20:37:24
My vision for Bitcoin is to be the decentralized cryptocurrency that tears down borders and unites the world. It's the currency with the support of the economic majority. If you want to keep the support of the economic majority, then you need to be able to support the load of the economic majority. 
If Bitcoin demand continues to scale, we will have a choice to make: either we can deny access to on-chain transactions to millions of people, or we can scale Bitcoin infrastructure and make it difficult for hobbyists to run a full node on a Raspberry Pi in their basement. I prefer the latter scenario. As I see it, a few years from now, hobbyists can either buy medium-high-end hardware with a decent internet connection (100-1000 Mbps) or rent a VPS in a datacenter if they want to keep up with the Bitcoin blockchain. If they really want to run a full node on a Raspberry Pi, they can use an altcoin.
Keep in mind that the protocol-level blocksize cap is very different from the average blocksize. Miners have an incentive to keep their blocks small in order to reduce orphan rates. See  for more information on that. Miners will keep their blocks to a size that can be propagated through most of the bitcoin network, reaching the vast majority of miners (by hashrate), within about one minute or less. Even in 2036, when the BIP101 blocksize limit reaches 8192 MB, typical blocks will only be as large as typical hardware of that day can handle.
For non-mining full nodes, the technical requirements are much lower than for mining full nodes. Non-mining nodes have to keep up with transaction and block volume, but do not have any significant latency constraints on how they do so. Mining nodes, on the other hand, need to be able to handle new blocks within seconds.  If most people have 200 Mbps of spare bandwidth, then they should be able to handle the full 8 GB.
Small blocks mean high transaction fees. We shouldn't force the cost per transaction grow to $1 or higher for everybody just because we want to let 1% of users run a full node on a $30 computer.
@_date: 2017-11-06 08:11:06
Hmm, that's not the conclusion I would have expected. Can you explain your reasoning?
@_date: 2017-11-07 07:18:45
The high variability in fees is caused by Bitcoin users having low [demand elasticity]( Low demand elasticity is something you see in things like food or health care -- when you're hungry or injured, you'll pay whatever the market is charging, even if the price is exorbitant. However, if you get overcharged, you'll change your behavior so that you are less likely to be price gouged in the future. Maybe you'll store a year's worth of non-perishables in your basement or start a garden. Maybe you'll get health insurance or move to a country with nationalized health care. In either case, the response comes later.
Even when fees are high only for short periods of time, they can still have lasting deleterious effects for Bitcoin's user base. High fees and slow/irregular confirmations cause people to change their habits and their finances so that they don't depend on Bitcoin. For example, if a Venezuelan is using Bitcoin for 12 months to buy groceries from Florida, then starts to see transaction fees rise to the point where it costs them 20% in fees just to spend their money, they will avoid holding Bitcoin so that they don't get put into the situation where they have to either go hungry or waste money on fees. People find another way. Once burned, they won't likely come back.
Bitcoin has reached a balance point. There are some people who leave Bitcoin permanently once they see that fees are high 10% of the time; others can tolerate fees being high 30% of the time; etc. Each time the fees get high, some people give up and leave, which causes the fees to go down again, which makes people think they can use it again. When the hashrate goes up or down, that tips the balance, and it takes a few days for people to adjust.
When blocks get full and fees rise, people have four ways to react:
1. They can send their transactions with a high fee.
2. They can send their transactions anyway with a low fee, and hope it confirms eventually.
3. They can wait until the congestion passes (e.g. the weekend)
4. They can stop using Bitcoin entirely.
There are enough people who choose  to have the fees spike above $1 for short periods of time, but there aren't enough to keep fees above $1 indefinitely. If everyone was   or  then the mempool would never clear --
 backlogs would just keep building forever. The fact that the mempool ever clears at all indicates that a lot of people are 


No, it is not fair to blame miners for that. There are two main reasons why Segwit activation took so long:
1. There was concern that Segwit would not do very much to increase block capacity. (In retrospect, those concerns seem justified.) Miners met with a few Bitcoin Core representatives in Hong Kong, and everyone there agreed that Segwit's activation would be combined with the inclusion of a 2 MB hard fork in Bitcoin Core. That hard fork code was never included in Bitcoin Core. Since Core did not deliver on their half of the compromise, the miners withheld their half and chose not to activate Segwit until it was coupled with a 2 MB hard fork as promised.
2. There was a large proportion of the Bitcoin user base that never liked Segwit. If you read the other Bitcoin forum, you'd know that. Miners read the forums and noticed that Segwit was controversial. Miners read the criticisms and decided that the controversy was justified. When miners and big businesses got together in New York and decided to push Segwit through despite the lack of consensus, the result was a bunch of Bitcoiners forking off with Bitcoin Cash.


This doesn't work at all. It just creates an incentive for miners to fill their blocks with spam.


That also doesn't work at all. Miners can put whatever timestamp they want into a block header. There is no reliable way to enforce timestamp accuracy.
@_date: 2015-08-24 09:18:41
I think you meant to link part 2 of that series,  not part 3. If you read that graph more carefully, you'll see that average real transaction fees hit their peak of ~$0.40/tx in the winter of 2013/2014. There was no stress test during that time. The stress tests happened in the summer of 2015, and this graph doesn't even cover that time. Winter 2013/2014 was when the exchange rate ramped up to $1100 and volume ramped up a bit too. When the exchange rate ramped up, it took some time before the developers of bitcoin-qt released a version that reduced the default transaction fees in XBT in order to have about the same BTC rate. This explains the large boost in real tx fees -- the change in the gentleman's agreement, not supply and mostly not demand.
What matters is total transaction fees per block, not average fee per transaction. You can see that graph here:
If you compare that to the number of transactions per block, you'll see that about half of the variation in total fees can be explained by the number of transactions:
The other half is due to increasing transaction fee habits. You may also notice that even after the stress test ended, the total fees per block remained higher. I think that's because people increased their fees per kb during the stress test, and never turned it back down afterwards. This is consistent with my interpretation that people are currently paying a fee level that they don't notice.
I think it's a desirable goal to maintain fees that are low enough that people don't notice or care about them. I think this fee level can be maintained if we increase the blocksize and on-chain transaction volume by a couple orders of magnitude. I think we can enact this volume increase economically. Even if we can't increase volume enough to solve all our mining problems, however much we increase volume will reduce the amount that we need to increase fees per tx in order to power the mining network.
@_date: 2015-08-23 12:54:58
I'm a 0.25% miner. I run a 750 kW, 1 PH bitcoin mining hosting company in central Washington.  for more information on us.
We spend about 1/80th as much on internet connectivity as we do on power. That 1/80th gives us two 100 Mbps symmetrical fiber lines, of which one is just a backup. On our single active fiber line, we're running two full nodes, two p2pool nodes, about 1 PH/s of miners, and a few other things. With 400 kB average blocks, that uses about 1% of our available bandwidth. 
If we scaled up to 8 MB average blocks (20x higher transaction volume than today), that would use about 20% of our bandwidth, while still only costing 1.25% as much as we pay in electricity. 
If we spent as much on bandwidth as we did on electricity, we could probably afford a pair of 10 Gbps or 40 Gbps connections. (80x 100Mbps = 8 Gbps, but networking tends to get big economy of scale benefits.) That would be enough bandwidth to download and upload an 8192 MB block in about 10 seconds. BIP101 doesn't have 8192 MB blocks scheduled to be allowed until 2036. We could afford them today, at least for the bandwidth.
Currently, transaction fees are about 0.1 BTC per block. If transaction volume were 20x higher, that would be about 2 BTC per block. This increase would more than compensate for the increased bandwidth and storage usage, which would have nearly zero marginal cost in our case. If blocks were 4000 MB each (10,000x larger), we could afford to reduce transaction fees 10x while still getting 100 BTC per block in fees. Fees scale faster than bandwidth/processing costs for miners with more than about 0.01% of the network hashrate.
@_date: 2017-11-06 10:09:30
That article has a lot of misunderstandings in it.
1. Spammers are not obligated to use Segwit or Schnorr/aggregated signatures. If signature aggregation would make their transactions smaller, spammers can always choose not to use signature aggregation and to continue to use the legacy transaction format.
2. Fees are per weight unit, not per transaction. A 100 kWU transaction costs 50 times as much as a typical 2 kWU transaction to send, or the same amount as 50 2-kWU transactions. It doesn't matter if you're filling a block with Segwit spam, legacy spam, or Schnorr spam; you pay the same amount per block (or per fraction thereof) regardless. If your goal is to deny other people the ability to get their transactions confirmed, the cost is the same whether you use Segwit, Schnorr, or legacy transactions. That said, if you're using Segwit and especially multisig Segwit, each byte is cheaper (fewer kWU per kB), so you can spam with more bytes for the same price in fees. If your goal is to fill up people's hard drives or saturate their internet connections, Segwit makes that cheaper and Schnorr has no effect.
3. His analysis does not clearly show it was spam. All it clearly shows is that one entity had accumulated a large number of UTXOs over a short period of time which they were consolidating. While spammers might generate large numbers of UTXOs like that, entities like Coinbase would too.
@_date: 2017-11-06 10:31:48


Just clarifying here: your argument is that Segwit helps protect against spam attacks because it allows spammers to afford more transactions, which takes so much time and effort that it's not worth sending all those extra spam transactions?
As for the Schnorr stuff, I replied to that the first time you linked to that article.
@_date: 2015-08-16 10:09:01
Thanks, Gavin and Mike! We're behind you 94.6%!
(That's 351 TH/s out of 371 TH/s.)
@_date: 2015-08-16 15:33:44
A comment by gressen on the [voat cross-post](


@_date: 2015-07-28 07:06:13
It's statistical noise, also known as luck. The hashrate estimate graphs you're looking at are based on the amount of time it took to find the last X blocks, where X is something like 504 or 2016. Since finding a block is a stochastic event (i.e. you have to get lucky), the rate at which blocks are found will vary due to chance. As such, you will always see about 5% variation on a 2016-block estimate of hashrate, and about 10% variation on a 508-block estimate. The amount of variation due to luck in an estimate will be proportional to 1/sqrt(X), so quadrupling the number of blocks you look at halves the uncertainty/variation due to luck that you will see.
@_date: 2015-08-16 12:47:09
To be honest, we're being a little bit biased. One of those nodes has a much faster CPU than the other (3.3 GHz Core i3 vs. 2.0 GHz P4 Xeon). We're making the faster node the one that runs XT, which in turn makes mining revenue a bit better for the XT users (lower latency and slightly lower DOA rates). I think we're justified in doing so, since I already know that over half of our p2pool hashrate has chosen or will choose XT, and our slower node definitely can't handle that much traffic. 
On the other hand, if more than 25% of our p2pool hashrate chooses Core, we'll probably upgrade that node.
@_date: 2015-08-18 06:23:35
You are correct, I was confusing hard and soft forks. 
I don't think XT is threatening MAD. I think XT is proposing a relatively orderly hard fork with a large supermajority of mining favoring the hard fork. NoXT is proposing a disorderly hardfork when no hardfork would otherwise exist. Can you describe how you think that the XT hardfork would be MAD?
Another reason why I think NoXT is harmful is that I don't see how it would actually help the cause of small blocks. The amount of hashrate needed to cause a minority hardfork using noXT is greater than that needed to prevent a fork with Core.
Let: 
n = noXT hashrate proportion
c = Core hashrate proportion
x = XT hashrate proportion.
In order to produce a hardfork when XT has a minority of hashpower, you would need these things:
n &gt; 0.25
n + m &gt; 0.50
If this is truly the preference distribution, I wonder what the advantage of this voting arrangement is over simply
m &gt; 0.25 + margin for luck
or even
m &gt; 0.50
since x needs to exceed 0.75 to do anything anyway.
@_date: 2019-02-17 21:45:48
Most wallets use method 
@_date: 2015-07-31 23:54:19
If you run a full node but limit your upload bandwidth, you aren't helping the bitcoin network much, and in fact might be hurting it. 
Each node in the network is one more place that transactions have to be sent to. The larger the bitcoin network is, the more hops it takes for a transaction to reach every node. If you limit your bandwidth, then not only are there more nodes, but the average node speed is lower. This means that both the total number of hops and the amount of time per hop will be greater. This slows down everything.
Same thing applies to block propagation, although the Corallo relay network helps a lot with that.
If you're going to run a full node, great! Try to keep it fast.
@_date: 2015-08-27 01:54:37
I addressed objections similar to these in detail in [*this comment*]( Some brief additional points:
A fee market will exist without a hard cap on block size, motivated by the higher orphan rates of large blocks. This fee market will be tied to the actual production costs of including transactions, and will be set to the intersection of both supply and demand curves instead of based on inflexible supply like a hard cap. 
It will be much easier to pay for mining with large on-chain transaction volume coupled to modest fees than with small on-chain transaction volume coupled to large fees. People don't like large fees, and they tend to not use services that have them. If we scale on-chain volume 100x, then we can pay for the mining network with current tx fees.
@_date: 2015-08-19 09:32:14
No, it's always the same many. There are a huge number of VPS and hosting services out there. Most ISPs offer VPS services, for example. This site lists a few of the options that are out there with English names: 
If you really want security, then you can build your own server and do a colocation. That's pretty cheap, and the blocksize increase won't really increase the cost of colocation significantly until the blocks get past around 20 MB average each. 
If you really really want security, then you build your own server room somewhere. That will cost more and limit your location options if you need a 100 Mbps line instead of a 10 Mbps line, but security tends to be expensive. If your application demands that level of security, then you can probably afford the server room.
The bandwidth costs to keep up with the blockchain are significantly smaller for wallet full-nodes than for miners. Miners have to get and send new blocks quickly in order to avoid having their mined blocks be orphaned. Wallets just need to not be more than a couple minutes behind. Miners need to not be more than a few tens of seconds behind. Increasing the blocksize increases revenue for miners, so the net effect for miners should be to increase mining profitability.
@_date: 2017-11-22 14:27:22
$65 cheaper than an S9, but they ship at least 2 months later, during which time the S9 would have made around $800.
Efficiency looks like it might be 10% better than an S9 (0.095 J/GH vs 0.0106 J/GH). That's somewhat impressive for the first release from a new start-up if it's true, but not a game-changer in terms of the economics.
@_date: 2015-07-31 23:39:00
When the hashrate drops, the cost of conducting a 51% attack also drops. If the hashrate were to drop by more than 50% all at once, that would mean there would be enough hardware and enough datacenter capacity offline for someone to perform an attack. It would be difficult for a person to collect all of that, since the idle hashrate would be mostly distributed among smaller miners who don't have the economies of scale and the large cheap power contracts. As such, there would not be much grounds for a drop in user confidence unless the hashrate reduction were quite large. Of course, the fear of a drop in user confidence might be enough to reduce user confidence. Insert FDR quote.
@_date: 2015-07-22 01:13:49
I run a mining company and hosting service in North America ( We pay $65/month for a symmetrical 100 Mbps fiber internet connection with 8 ms latency to peers in Seattle and 30 ms latency to San Jose. We've gotten 8 MiB/s of actual throughput on this line. We also have a $40 line at our house with the same speed; the additional $25/month is just for 24/7 tech support. We will run into CPU performance problems with p2pool before we hit any bandwidth issues. I expect to be able to scale up to 10 MiB blocks without spending more than 1% on our node hardware of we spend on mining hardware.
When I was in China (Shenzhen), bandwidth and latency were pretty great as long as you only accessed Chinese IPs. If you attempted to contact sites abroad, latency and bandwidth would suffer and packet loss would set in due to the Great Firewall. Since the GFW is intended to protect against viewpoints opposed to the Communist Party's goals, and since Bitcoin does not really relate to those goals, I expect it would be pretty easy to get a bypass to the GFW for bitcoin infrastructure. The gov't does allow companies to do this when they can demonstrate a need.
@_date: 2015-07-31 23:33:41
Your transaction fees are low on both of those transactions. The first one has a fee of 0.1 mBTC for 1111 bytes, or 0.05 mBTC per kB (using integer division on the number of kB). The second transaction has a fee of 0.05 mBTC on 226 bytes, or 0.05 mBTC per kB. Both of these fees are lower than the recommended minimum, which is 0.1 mBTC per kB.
I think it's likely that your transactions will confirm eventually, but it will probably take several hours to do so. The bitcoin mempool is currently very congested due to the small blocksize and a spam attack/stress test that some entity has been conducting for the last few days.
Useful links:
That shows you what kind of fee you need to include if you want to be pretty sure you'll get your transaction confirmed in one block.
The blue line in the top graph shows the amount of backlogged transactions over time.
@_date: 2015-07-28 06:40:47
There are two breakeven points for mining. The first is the point at which your revenue begins to exceed your operating costs. The second is the point at which your revenue minus operating costs is sufficiently positive to be able to earn back your capital costs in a reasonable amount of time. The second breakeven point is what determines whether people invest in new mining hardware. The first breakeven point is what determines when people turn hardware off. 
Let's look at the Antminer S5 in a large hydroelectric datacenter like mine ( as an example. The S5 uses 0.59 kW. Adding in electricity costs, rent, cooling, labor, and maintenance costs, and an S5 might cost about $0.06/hr to operate. (Currently, the lowest we charge is $0.065/hr, but I'll admit to having a comfortable profit margin in there.) An S5 currently makes about $0.13/hr in revenue, for a total profit of about $0.07/hr. An S5 costs about $450 with PSU and shipping. At current rates, it would take about 9 months for it to pay for itself. 
If the block reward halved right now, then the S5 goes from making $0.07/hr profit to $0.01/hr. It's still worthwhile to keep it on, but only just barely. No sane person will buy new miners because there would be no chance of it paying off in a reasonable amount of time.
Once the block reward halves, a few things will happen:
People who have expensive electricity will shut their miners off, causing the hashrate to drop some. Since fewer megawatts will be consumed by miners, there will be fewer bitcoins sold by miners to pay for electricity, driving the price up a little. Both of those factors will improve the profitability for the miners that remain.
@_date: 2015-08-23 16:11:16
Hashpower rental and purchase of obsolete equipment is the justification for 51% attacks at lower than $2000/block. Bitcoin is secured against 51% attacks by keeping the capital costs of acquiring enough equipment to perform an attack high. If the profitability of running miners decreases to the point that the least efficient 50% of miners are rendered obsolete and taken off the network, then the cost of acquiring miners or renting them for a short period of time plummets. In order to prevent that from happening, you have to keep those miners on. At 300 MW (our current network hashrate), that turns into about $4000/block. At $2000/block, about half of them would be turned off, which makes these rental attacks somewhat feasible.
It takes a lot more bandwidth than (blocksize / 600 seconds) to run a bitcoin full node, by the way.
LN is not pertinent unless you rely on it to scale Bitcoin instead of enlarging blocks.


There will be no fork with XT unless 75% of miners vote for it. I would call 25% "narrowly spread disagreement".


I think Mike Hearn should have dictatorial leadership of his Bitcoin client. That's no problem, in my eyes. The problem is if Mike Hearn has dictatorial (or consensus-based) control of the only accepted Bitcoin client. I think that there should be several different options for clients. I think that no single development team should have complete control over the Bitcoin network. That's what we have right now. Consensus should be de-facto, determined by which clients the users and miners of bitcoin decide is the best client or clients. Consensus of support should be determined on the network by the use of feature support flags. If BitcoinXT plus BitcoinXL plus BitcoinJGarzik plus everything else comprise 75% of hashpower and all of them announce support for BIP101 blocksizes, then we should allow BIP101 blocksizes on the network.
One of the core principles of Bitcoin is that it's supposed to be a decentralized network. Right now, Bitcoin development is about as centralized as it can get, with only one Officially Sanctioned Client. Any attempts to make a change that isn't sanctioned by the Council of Five, the five devs with git commit access, is blocked. Using BitcoinXT is a vote that all clients are equally valid. Thus, when it comes to the core principle of decentralization, Bitcoin XT is more core than Core.
@_date: 2015-08-17 12:09:42
Second, historical trends for internet bandwidth costs have been about a 10x reduction every 5 years. That's 2.5x every 2 years. Network bandwidth growth has been faster than most other aspects of computing for quite a while, and unlike Moore's Law, is not butting up against any fundamental theoretical limits any time soon. Even residential bandwidth in the USA has been increasing about 2x every 2-3 years, although backbone bandwidth exceeds that.
@_date: 2019-02-12 19:34:23
I'm not ZobraS, and I can't provide proof, but some of my customers have been doing this since 2015.
@_date: 2015-07-16 16:35:44
If you think this is likely to be a scam, I suggest you downvote the OP Coinyoo.com story.
@_date: 2015-08-18 07:05:36


The reason for calling for a referendum with XT is the same as calling for a referendum in other political contexts. It is done when there is insufficient faith in the government (i.e. Core devs) to act the way the citizens (i.e. miners and/or users) prefer. The process of trying to negotiate a hard fork to change the blocksize limit has been going on for years, and has been stuck for quite a while due to the consensus rule that Bitcoin Core uses for making decisions. Consensus rules only work for very small organizations when making non-controversial decisions. Whenever there's controversy, consensus ensures inaction. If the political process for Bitcoin Core is broken, one reasonable path for action is to change political processes. 


You are correct. I am a medium-scale miner, so I said "Using XT is a vote for BIP101" habitually intending that as a node for mining.
The hard fork mechanism used by XT was also implemented by p2pool during the BIP66 activation period. We saw then that 95% was too high a threshold, and forrestv ultimately had to release a new version that made the hard fork happen around the time that 75% of the hashrate had upgraded. It was not the most orderly of forks, as a few people lost a couple days of revenue, but it succeeded, and it took less than one week after announcement to complete.
The BIP66 fork had nothing to do with the threshold used there. That was due to software which was not honestly reporting its capabilities, just like noXT does. I think Bitcoin is stronger for having done BIP66, even though there was a brief soft fork. Do you disagree?
@_date: 2017-04-15 17:03:51
Again, variance can account for that. If you observe that the total support for SegWit in the sample fluctuated from 83% to 73%, that is equivalent to observing that the hashrate for the pools that oppose SegWit increased from a total of 17% to 27% in the sample.
@_date: 2015-07-13 20:52:35
If my math is correct, this would increase eurozone inflation by about 7.2% per year. The euro money supply ([M3]( is about [â¬10 trillion]( â¬60 billion per month / â¬10,000 billion = 0.6% per month. 
@_date: 2019-02-13 01:29:48
It allows people who have access to the LAN containing an S15 (e.g. the owner) to SSH into the machine and modify whatever they want. Assuming that the owner has a firewall (e.g. NAT box), it does not allow malicious third parties on the internet at large to access the machine.
@_date: 2015-07-12 16:03:44
The Bitcoin network has been subject to a spam attack/stress test for the last week or so. Bitcoin has held up pretty well, and the stress test has not caused many problems for regular users. The only effect you should be aware of is that transactions may be severely delayed if you do not include a sufficient transaction fee. I recommend choosing a wallet that allows you to specify the transaction fee. (Electrum does.)
@_date: 2019-04-27 01:46:24
@_date: 2017-04-06 01:05:48


That refers to the 21 mining chip, which hardcoded parts of the coinbase transaction in silicon to ensure that all users of 21 Inc's mining chips also used 21's pool. That sentence does not refer to Bitmain.
@_date: 2019-04-15 02:59:54


A [custodial wallet]( is one in which you don't own or control the private keys. They are custodians of your funds. Electrum always controls your *private* keys locally, but it does not generally keep your *public* keys secret. Electrum keeps your funds secure, but does not maximally protect your anonymity.
Public keys are so-called because it's generally assumed that everyone in the public can or will know them. Using public Electrum servers is totally fine for all but the most paranoid or clandestine of users.
@_date: 2016-02-29 20:08:33
**Edit: Check comment below. I am no longer convinced of the validity of this comment, or the validity of the parent, due to uncertainty about what this graph actually represents.**
Try this graph, which adds some averaging to reduce the noise:
Confirmation time has gone up by about 13% from around 8.1 minutes to about 9.1 minutes. This indicates that some transactions are getting delayed. Confirmation times are currently slightly worse than they were during the October spam attacks. It has been like this for about a week.
The way it works in these situations is that a transaction that is likely to not get included in one block is also likely to not get included in the next. Thus, what is probably happening is that most transactions are getting into the first block, whereas a small subset of transactions (the ones with the lowest fees/kB) are getting delayed by several blocks. Some are even getting delayed by hours.
My opinion is that if e.g. 5% of transactions are delayed by several hours, Bitcoin has a usability problem. That seems to be about where we are now.
It will probably get worse.
Yes, the 5-ish% of transactions that are being delayed have the lowest fees. People who sent those transactions could have avoided having their transactions by using a higher fee, thereby causing the second-worst transaction to get delayed instead.
Unless we increase the network capacity or users start sending fewer (or smaller) transactions, this problem will not go away. Throwing bigger fees at the problem does not make it go away.
@_date: 2016-02-25 00:30:21
Actually, it may never go through. If demand (i.e. the transaction generation rate) is monotonically increasing, and supply (block size) is constant, there will be one point at which demand exceeds supply for the first time, and after that point there will never be excess supply again. After this point, the backlog of unconfirmed transactions will continually increase. 
    backlog = old_backlog + newly_generated - newly_confirmed
Fortunately, the transaction generation rate is not monotonically increasing. In fact, it's cyclic, as people generate fewer transactions at night and on weekends. There is a good chance that will see his tx confirm at night or over the weekend. However, it's by no means certain, and this type of situation will likely get worse over time if the blocksize limit stays at 1 MB.
@_date: 2016-02-29 20:18:18




Not true. SW does discount P2PKH transactions as well, as long as they're done using a SW-compatible wallet. SW does not benefit P2PKH capacity as much as it benefits multisig, but both do benefit.
@_date: 2019-04-27 03:42:16
Please don't misunderstand. My point was that the accusations of shillery and sock-puppeting are being handed around in both directions by whomever thinks that this kind of argument is convenient, and they're petty and meaningless. People use double-standards for evaluating evidence; when something suggestive is released against their own tribe members, they pooh-pooh it, and when something suggestive is released against their enemy tribe members, they jump on it, even when it's the same low quality of anecdotal evidence.
As I said later in the thread, these kinds of character assassinations are dumb and pointless. [We should be judging comments by their intrinsic merit, not by the reputation of the character associated with them](


@_date: 2017-11-16 04:03:55


I think there are many reasons why people might want to open and close channels, and not all of them are related to the ability of the routing algorithm to keep channels funded. People may want to switch wallets, or perform on-chain transactions, or switch cryptocurrencies, or they might have multiple wallets, or the counterparty for their channel might go offline or need to settle for any of the above reasons. People are probably going to have more than one channel per wallet (maybe two on average?), and more than one wallet per user (maybe two?). I think that once all of those factors are taken into account, one month per channel-open per user is a reasonable ballpark estimate.


No settlement transactions after 0.04 lightning payments per user? Once 4% of users had transacted, that left 0.4% of channels unbalanced? And 1.25% of those payment attempts failed? And the simulation crashed before they reached 0.05 payments per user? In total, they had 50 times as many channel open/close transactions as they had actual payments? And the simulation ran at less than 10 tx/sec? I'm glad that someone tried to simulate it, but I'm not sure I'd call that evidence of it being "not near as bad as you think."
@_date: 2019-04-27 00:11:09


I recognize how Roger Ver's statements equating BCH and Bitcoin are misleading, and I do not support them.
However, the animosity towards BCH and the big block movement came long before Roger Ver's statement. People were calling BCH a sh*tcoin and a scam from day zero, because a lot of Bitcoin maximalists saw (and see) BCH as a threat. Because these people felt threatened by BCH, they jumped at any opportunity to make it look bad, and criticizing Roger's words and actions has given them more leverage than anything else so far.


None of those have any significant support in the community and aren't a threat to BTC. (Except Bitcoin Unlimited, which is an implementation and a development team, not a currency.)
@_date: 2015-08-19 10:32:06
If this hard fork succeeds, that will be an example for the future of Bitcoin. People will realize that anyone can publish a new Bitcoin client with new features, and if their idea is good enough (and if majorities of the miners and users support it), then it will become the future Bitcoin, a Bitcoin 2.0 or 3.0.
Larger blocks were part of the original vision for Bitcoin. Satoshi himself envisioned changing the max block size once we started getting close to it. 
It's now 5 years later. We haven't just gotten close to the limit. We've hit it several times, with all blocks saturated at the limit for weeks at a time.
Bitcoin Core can continue to operate after the hard fork if enough people continue to mine with it and use it. It's only if nearly everybody abandons it for XT that it will have trouble. There is no technical reason why XT succeeding means Core has to die. There are good social reasons for it, though, such as XT being technologically superior.
If you want to say that Bitcoin 1.0 will be destroyed by Bitcoin 2.0, then I suppose that is a semantically valid statement. However, it will only happen because Bitcoin 1.0 will be obsolete. 
@_date: 2016-02-25 01:27:09
Due to the daily cycle of bitcoin (and all human) activity, we can expect transaction delays to be common during the daytime in Europe and North America for a while. For now, the backlog will generally get cleared sometime after the sun sets on the east coast of the USA. 
Eventually, the daily average transaction generation rate will exceed the block capacity. Once that happens, the backlog will start to grow without bound, and the lowest-fee transactions will likely remain unconfirmed forever.
@_date: 2017-04-03 22:47:52
It looks like syscoin [also uses]( `aa21a9ed`, so the hypothesis that it's a syscoin idiosyncracy doesn't hold much weight.
can you enlighten us?
@_date: 2017-04-15 12:15:08
83 to 73 sounds like it could just be variance. Do we have any evidence showing otherwise?
My guess is that people are watching  which shows a 24-hour average (576 blocks). The amount of variance over a 576 block interval is around Â± 5%. A swing from 83% to 73% is about the size of the confidence interval for a 24-hour estimate of the SegWit support.
Edit: Support is back up to 76% now. Either Jihan turned some of his miners back off, or perhaps 73% was just a low point caused by variance.
@_date: 2016-02-23 06:09:59
A 15-of-15 multisig transaction with SegWit is several times larger than a "1-of-1" P2PKH transaction without SegWit. SegWit reduces the cost premium of multisig transactions, but it doesn't make multisig transactions cheaper or smaller than plain P2PKH transactions.
@_date: 2016-02-29 07:39:53
The first version is being written in python and will interact with bitcoind via localhost:8333. It will work with all Bitcoin node versions. We're doing it this way to make it easy to debug, benchmark, and tweak the protocol and algorithms. Performance should be pretty good, though, because the algorithm is not computationally expensive.
The second version will be written in C++ and will be a relatively self-contained subsystem of bitcoind, and should be quite easy to merge into Core, Classic, XT, and BU.
@_date: 2015-08-18 04:18:14
It's fine that noXT was written. It's an interesting idea. 
The problem is when people actually use it. NoXT is a destructive piece of software. Its only purpose is to disrupt the normal bitcoin mining consensus mechanism for hard forks, and to increase the likelihood that a contentious hard fork occurs. 
If XT were a destructive piece of software, then I think that using XT would be an assholish thing to do. However, XT was carefully written to only create a hard fork if there is 3:1 support for the fork. The changes implemented by that fork are something that will enhance bitcoin functionality, and which has widespread support. That does not sound destructive to me.
There's not really much wrong about building an atomic bomb. The problem is when you use it to demolish cities.
@_date: 2017-04-07 06:12:06


I have a lot of Bitmain's ASICBOOST miners. They're called Antminer S9s and T9s. They support non-ASICBOOST mode. In fact, that's the only way we are allowed to run them -- to use ASICBOOST, you need special mining software which the general public doesn't get.
@_date: 2017-04-15 16:59:12
Litecoin has a 4x faster block interval, which means that 1 day's blocks on Litecoin is equivalent to 4 days' blocks on Bitcoin, and consequently has half as much variance as a 1-day estimate in Bitcoin would.
@_date: 2016-02-29 00:49:52
Without fees? It wouldn't. Bitcoin will collapse. We would probably need to change PoW functions in order to avoid hashrate rental attacks.
We can pay for the current amount of mining (about 350 MW) with about $3000 per block. This could be achieved with fees around $0.15 per kB and 20 MB blocks. This is one of the main reasons why I want to increase the blocksize and scale bitcoin's on-chain capacity: I think that if we don't, it would compromise Bitcoin's security.
@_date: 2017-04-04 03:57:59


I would prefer to do this by just halving or eliminating the witness discount.
My main objection to SegWit is the 4x discount function with the associated 4 MB (4x) spam attack adversarial condition and modified economic incentives. While 4 MB is not beyond what the network can handle right now, this adversarial condition can get a lot more severe if we ever do a hard fork to increase the blocksize -- with a 4 MB base block size, this would result in a 16 MB spam block being possible. Ultimately, SegWit results in a reduction of our safety margin headroom, since we only get around 1.4x (assuming 50% adoption) to 1.8x more transaction capacity while seeing a 3.9x more network capacity required in case of spam attacks.
If SegWit did not have a discount for witness data, it would not have the 4 MB spam attack condition and would also not increase the amount of base+witness data that could be mined in a single block. I would have supported such a fork. I would also support SegWit with a discount of 2x, which I think is much more balanced between the benefits of discouraging UTXO use and the drawbacks of lost bandwidth headroom.
@_date: 2017-04-06 05:29:24
The full sentence is more specific about the *specific* effort they made:


incompatible with any mining system and, in particular, changed the
design at one point to accommodate mining chips with forced payout
@_date: 2016-02-26 04:43:57


It costs about $200 to buy 1 TH/s of hashpower right now, plus a similar amount to pay for the datacenter capacity needed to run it. It costs $0.057 right now to rent 1 TH/s of hashpower on Nicehash for 4 hours. That's about $400 vs. $0.06, or a factor of 7,000x.
Sorry, that's not the 10,000x that I remembered. Antminer S7s were more expensive the last time I did this calculation.
@_date: 2016-02-25 00:10:13
Edit: Okay, looks like I was wrong.
@_date: 2016-02-20 17:40:02
We might even call that a "bug."
@_date: 2016-02-29 20:13:26
This is not a representative time period. The first part of the last 30 days had unusually fast confirmation times (due to fast blocks from new hashrate and slow difficulty adjustments). This makes it look like the last half of the graph is slower than it actually is. Confirmation times are indeed higher than they typically have been, but the effect size is only about 1 minute instead of 2 minutes.
Confirmation times are now about as long as they were during last year's spam attacks (which happened in late July, August, September, and October), yet we do not currently have any spam attacks running. Delays are currently worse than all but the first of those attacks.
@_date: 2016-02-27 05:30:25


Turns out I was **[partially wrong]( about the hop count issue as well as the multiple validation issue. In actuality, it is pretty easy to form direct peering relationships with XTB, and the validation step is dramatically accelerated.
There's also another advantage to XTB over RN that I didn't mention before. With XTB, it is possible to request an XTB from several peers simultaneously. This provides some protection against the case in which one or more of your peers is particularly slow to respond, which is a very common case when crossing the Great Firewall of China.
One of the main issues with the RN is that you can't rely on it, as it is not a part of bitcoind and is controlled by a third party. Thus, when I made my recommendations for what is a safe block size in December, I based it off of tests that did not include the relay network. If XTB had been available at the time of my testing, I would have used them, as they are likely to be much more reliable than the RN is.
I expect RN will still be faster than XTB, but not by much. When properly configured, I expect miner-to-miner XTB communication might take 2 seconds instead of about 1 second for a 1 MB block with RN, compared to about 20 to 60 seconds for the current p2p protocol.
@_date: 2016-02-25 01:18:33
/un1kof Looks like the transaction confirmed, right around when the sun set on the east coast of the USA and around when Europe went to sleep.
@_date: 2017-04-04 04:28:32


The protection against UTXO bloat is better done with an additive element based on the UTXO set delta, rather than a multiplicative element based on the signatures. What we want to do is reward input count and punish output count, not reward input size. Division of byte count is simply the wrong mathematical operator to use.
Switching Bitcoin over to a universal transaction cost function used to be a topic of discussion, but people stopped talking about it when SegWit became The Thing To Do. I prefer the universal cost function approach.


No, I'm not worried about 4 MB. I very clearly stated that in the parent post I'm worried about the 4x multiplier making further blocksize increases unnecessarily dangerous. Did you actually read it all before replying, or are you just replying on autopilot? I know you have a personal distaste for me, but replying in an aggressive and careless fashion harms Bitcoin by worsening the cultural split that we have. Can we try to not mischaracterize each others' positions, please?
@_date: 2017-04-07 01:34:16


Empty blocks are the result of spy mining. Bitmain uses spy mining intensively because they have several large datacenters in IIRC Yunnan with terrible internet connections, and for some reason they are running full nodes on the LAN with bad internet instead of using stratum to span the weak internet link.
The analyses that I've seen from 2015 and 2016 on block propagation latency to various pools have shown that Antpool is typically the worst at getting new blocks. This is consistent with their higher rates of 1-transaction blocks.
@_date: 2016-02-28 23:33:27


Actually, blocktorrent should work better than TCP based protocols like XTB in the presence of packet loss even without forward error correction (FEC). The issue with packet loss is that it tricks TCP into thinking that it is experiencing congestion, so TCP slows down dramatically. In my tests across the Great Firewall, this will often result in 10 kiB/s of actual bandwidth between two computers with 100 Mbit/s connections. With UDP, we don't have that issue, and we can use latency instead of packet loss as a sign of congestion. Furthermore, with blocktorrent we can usually make the retransmit requests happen from a peer with lower latency, which also helps with dealing with packet loss. If we add FEC, we can reduce the amount of round trips needed for retransmit requests, which will help too.
@_date: 2019-04-26 22:40:25
I know, right? What did Peter think he was doing when he posted as Contrarian__ by mistake here?
@_date: 2016-02-28 00:57:58
I stop buying miners when it becomes clear that they either won't make an operating profit after the halving or will not pay for themselves before the halving. If a machine is unprofitable to operate after the halving, I will turn it off or replace it with a more efficient model.
(I have a 750 kW farm and hosting facility in central Washington.)
@_date: 2016-02-25 01:12:31


We are now at the point where the daily peak of transaction generation exceeds confirmation rates. We are apparently not yet at the point where the daily average of the transaction generation rate exceeds the confirmation rate. That time may come soon. People need to be aware and prepared.
There have been several other transactions that people have complained about being delayed for a long time, for example **[this]( and **[this]( Fortunately, the second one just confirmed (though it took 590 minutes to do so). I am guessing that Europe and eastern USA being in darkness is why it confirmed now.
@_date: 2015-08-23 17:30:37
The current fees of 0.1 mBTC/kB are small and reasonable to most users. I think a scenario in which fees remain about the same is a reasonable one. Fees could go up or down. If they go up too high, people won't want to use Bitcoin, and transaction volumes will go down.
I think that increasing transaction volume is easier and better than increasing fees per kB, so the scenarios I draw for the future reflect that.
Letting the hashrate fall by more than 50% due to economic factors is dangerous for bitcoin. See  for an explanation of why. Currently, it costs about $4,000/block to maintain our 300 MW mining network. At current block sizes, that translates to about $4/transaction. Eventually, we'll have the choice of either letting Bitcoin become easy to 51% attack, increasing fees to $4/transaction, or dramatically increasing the on-chain transaction volume. I think the third option is the best one.
@_date: 2016-02-29 23:57:20
Fee revenue for miners is still extremely small compared to the 25 BTC block subsidy. The average amount of fees per block has been around 0.5 BTC for the last 20 blocks, or about 2% of total revenue.
If block sizes increase, then miners can include a greater number of transactions, each of which has a smaller fee. Whether this results in greater or lesser total fees is purely speculative right now, and depends on a lot of different variables (like demand and the actual block size).
I personally think that miners' best bet for achieving long-term survival is increasing the block size to about 10 MB over 4 years while keeping fee/kB about the same as it is now. I think that it will be easier to increase the volume of transactions 10x than it will be to increase the cost per transaction 10x.
One of the issues with increasing the fee per tx is that doing so would make Bitcoin less attractive as a currency to investors, speculators, and users, which in turn would likely result in a decrease in the exchange rate. This would reduce the real value of the 25 BTC bitcoin block reward, which will have a much larger effect on miner revenue than direct fees, at least for the next 4.5 years. Thus, the idea of keeping block sizes small in order to maximize revenue through fees is likely to backfire.
@_date: 2016-02-27 09:33:42
Sure, a miner can connect to maybe 50 peers with XBT, of which maybe 10 are other miners. Since there are less than 100 major miners in the world, that means that you should be at most 2 hops away from the block origin. It's still a p2p decentralized protocol. It's just not an unoptimized and randomly connected one.
@_date: 2017-04-10 01:52:30
A current-generation machine runs at 10 TH/s per kW. Currently, that translates into revenue of 25Â¢/kWh without transaction fees, or about 28Â¢/kWh with transaction fees. My company pays 2.8Â¢/kWh for electricity, but I think the average miner might pay as much as 5Â¢. If ASICBOOST saves 20% of that, then the average miner would save around 1Â¢/kWh, which would improve their profit margin by about 3.5%. While that is helpful, it's worth noting that several of the larger pools charge more than 3.5% in total pool fees, so miners can currently get more benefit from using a cheaper pool than they can from getting ASICBOOST.
In the future, the landscape might change, and perhaps electricity costs will start to eat up more of the total revenue. When that happens, ASICBOOST might start to be more important. Even then, it will be less important than the difference between running your mine on coal power (about 4-5Â¢/kWh) and running on cheap hydro (around 2-3Â¢/kWh). Still, it's possible to envision a scenario in which only the miners who have both cheap hydro and ASICBOOST can survive.
@_date: 2016-02-29 02:58:12
Well, I've written about blocktorrent in a few different forums, and I've seen it discussed on the Bitcoin Core slack when he was around, so I expect he's heard of it. And yet I haven't heard any interest from him. 
I'm just trying to explain his apparent lack of interest.
@_date: 2016-02-26 04:22:40
It's refreshing to hear of someone who isn't concerned by the possibility of a 51% attack actually happening. Somewhat surprising, but refreshing.


Industrial sabotage, for example. You could reverse your primary competitor's payments before the goods are delivered. You could try to make it look like your competitor is doing the 51% attack. Make your competitor look untrustworthy to their suppliers, and effectively freeze their supply chain.


They get sent from COutPoints, each of which refers to a transaction and an output index number, and each of which has one associated address. 


Doing so would violate Reddit rules. Doesn't matter. I can tell you what addresses are Satoshi's, or f2pool's, or any of thousands of others, because many addresses are publicly associated with individuals. If I had transacted with you in the past (e.g. if I had sent you money), then I would know at least some of your addresses, and possibly many of them from cluster analysis.


Except that it would cause other miners to lose revenue and potentially shut off permanently, allowing you to cement your control over the blockchain.


There are no laws against 51% attacks as far as I know. Also, you could hide your identity fairly well if you first established a record of high-volume trades in which you hedge your bets on separate exchanges in order to avoid risk from simply making random trades. Then when you know the 51% attack is going to go your way, you make the same trade on multiple exchanges instead of opposite trades.
Or maybe you have a lot invested in an altcoin. Or maybe you represent a bank consortium, or a government, or for some other reason you just want to see Bitcoin fail.
I'm just saying that it's probably better if it's extremely expensive to perform these attacks. A 10,000x reduction in the cost to perform a 51% attack would bother me, and a lot of other users of bitcoin.
@_date: 2017-04-07 06:08:13
Whatever. I don't like witchhunts, and that's what this seems to have turned into.
For the record, I think that Antpool's design is crap and they should be ashamed of how many 1-tx blocks they mine. I'm just annoyed that you're blaming those on ASICBOOST when they're actually caused by apathy, poor design, and poor internet. 
You're hearing hoofbeats and thinking of zebras. There may be zebras out there, but this one is just a horse.
@_date: 2016-02-29 23:38:29
If there's one thing the fearmongers can't handle, it's **[misleading or false data](
@_date: 2015-07-16 16:08:14
This sounds likely to be a scam. Be cautious.
Any entity offering to sell hardware for bitcoin or litecoin only well below market prices should be treated with caution.
It appears that this company has a history of posting fake pictures on their website. These include fake pictures of their staff as well as fake pictures of their mining facility.
Don't give them a satoshi without doing due diligence. The more people that fall for scams, the more scams there will be.
@_date: 2016-02-25 02:23:36
You can see the cyclic behavior a bit in the [Transactions Accepted Per Second]( graph on statoshi.info.
It looks like there are actually two humps per day.
@_date: 2016-02-08 17:22:22
Yes, they are doing it. No, it is not a way of bypassing the GFW or the Chinese government; it's just because China does not have the world's cheapest electricity.
@_date: 2017-04-06 01:23:28
You mean with Maxwell's proposed modified PoW mode BIP, right? You're not referring to SegWit blocking the ASICBOOST clone from functioning, are you?
@_date: 2016-02-25 22:14:09


Yes, and that's a problem.
Currently, fees are about $300/MB. The operating cost of mining is about $3,000/block. Total mining revenue (including the subsidy) is currently about $11,000/block. We don't have to pay $11k/block to keep the network secure; it just takes about $3k. We can do that with $300/MB and 10 MB blocks. It's not far out of reach, but we have to make changes to get there. 
Moving transaction volume over to Lightning is not a change that will get us there.


I believe we need to increase miner revenue. We can either do it by increasing transaction volume, by increasing fees per transaction, or some combination of the two. For most industries and products, it's easier to increase volume 10x while keeping price constant than to increase price 10x while keeping volume constant, so I think that the best approach for Bitcoin is to focus on the volume side. 
Another reason to focus on volume rather than the cost per tx is that we do not have a good way of controlling the cost per tx. The best we have right now is the block size limit. Unfortunately, capping the block size below demand results in highly variable and unpredictable fees due to demand inelasticity and daily fluctuations in demand, and it limits economic growth and pushes users to altcoins or private blockchains. An economically superior method would be to set price floors, which would allow the bitcoin industry to choose the price that maximizes their revenue in much the same way that oil producers do. However, cartels are kinda icky, whether their setting supply quotas (blocksize limits) or price floors.
I think it's better to just scale up the supply to the point where you have 30k transactions  per block with $0.05-$0.10 fees each.


Straw-man. I am not saying we should postpone Lightning at all. What I am saying is that relying entirely or primarily on Lightning is dangerous. My opinion is that we should scale on-chain capacity now, and we should try to make sure there is enough on-chain capacity for whatever demand people have for it until we get to block sizes that are sufficient to pay for mining in the absence of the block subsidy. When Lightning is ready, we should make use of it for the use-cases that it's uniquely suited for, like instant payments and microtransactions, while still keeping regular on-chain transactions affordable and numerous.


This is not true. If the subsidy plus available transaction fees are not enough, then the less efficient miners will shut off their hardware. Two weeks later, the difficulty will be adjusted down, and the block rate will go back to what it was before. We'll have less security, and it may become possible to perform a 51% attack by renting hashpower instead of buying it.
The marginal cost of including a transaction in a block is nearly zero. The cost of mining a block is the same regardless of how many transactions are included. Thus, miners acting individually will include as many transactions as they can fit into a block.
Bitcoin transaction fees suffer from the tragedy of the commons problem. All miners would be better off if everyone else followed a fee floor. However, each miner acting by itself will benefit more if it disobeys the fee floor. The game theory here is equivalent to the prisoner's dilemma.
Lightning compounds the problem by making individual on-chain transactions compete on fees with a once-a-month or once-a-year Lightning "subscription". This makes it difficult for miners to collect fees from users that are proportional to their use of the blockchain. It would also provide a barrier to entry into bitcoin: the first transaction would cost as much as 1 month of transactions. I don't think that's a very good pricing model if we want to pay for mining with transaction fees.
The alternative is that we continue the block subsidy forever, and that violates the 21 million coin limit. I don't like that scenario.
Or we just let the hashrate crash, and deal with frequent 51% attacks. I don't like that scenario either.


The system is Bitcoin. I don't want to see that die and get replaced; that would be disruptive. I would much rather see the system evolve continuously without anybody's life savings (e.g. my own) getting reduced to dust.
@_date: 2016-02-29 00:45:59


This is not accurate. The amount of PoW security that you get in this scenario depends on how much the rest of the world pays in fees, which you have no control over. This results in the **[freerider problem]( The fees paid would be equivalent to donations according to game theory.
@_date: 2017-04-02 22:08:33
I believe what he means is that the miner who got hacked was using Antpool as his primary, ViaBTC as his secondary, and BTCtop as his tertiary choice, and those got changed to F2pool. Backup pools are pretty standard in the industry, and almost everybody uses a total of three pools. A lot of hardware (including all Bitmain hardware) gives room for 3 pools in their GUIs.
It's not a coincidence at all. The person who got hacked has a preference for BU, so they chose 3 pools that signal for BU.
@_date: 2016-02-25 00:08:25
It's a medium-large transaction (668 bytes) with a medium-low fee (0.0001 BTC). When blocks get congested (as they are now), transactions like this one will fail to get confirmed. There is a moderate probability that this transaction will never confirm. If the transaction generation rate is continually increasing, and the transaction confirmation rate is constant, we would expect the behavior to flip from enough-space-for-everybody-without-delays to pay-enough-fees-or-never-get-confirmed very quickly, and that may have just happened.
It's possible that during the nighttime the generation of new transactions will slow down enough for the backlog to get worked through. Knock on wood.
@_date: 2016-02-28 23:59:12
Faster than XTB for typical blocks.
Better hooks for using weak blocks.
Better GFW crossing due to the use of UDP.
Much faster than XTB or the relay network for adversarial blocks (ones with transactions that weren't previously published).
Should be able to handle enormous blocks with acceptable propagation times.
The implementation is not finished yet.
@_date: 2019-04-03 01:31:33
There are bots that do arbitrage between exchanges. If you place a buy order on Bitfinex, the exchange rate on Kraken will go up immediately. Consequently, the argument that this was a single $100m order doesn't seem logically sound to me.
@_date: 2019-04-27 06:15:35


Okay. I'm sorry.
@_date: 2017-04-15 02:56:22
In PPS mode, pool.bitcoin.com (Roger Ver's pool) does not pay transaction fees. Miners get their share of a flat 13.75 BTC reward regardless of the actual amount of transaction fees in the block. As the average block reward is close to 13.75 BTC with transaction fees, this is essentially an unsubsidized but free pool. During times of particularly high transaction fees, this pool can make a profit on PPS.
In PPLNS mode, pool.bitcoin.com will share transaction fees with the miners if the total ~~exceeds~~ is less than 13.75 BTC per block, and will give miners a share of 13.75 BTC if the total otherwise would be ~~less~~ more than 13.75. This amounts to a mild ~~subsidy~~ fee, likely on the order of 1-2%.
Despite these great rates, pool.bitcoin.com only has 1.6% of the network hashrate.
Edit: zowki informs us that the fee page had an error in the description which has been fixed. I have edited my comment and amended the screenshot link. The old screenshot is available at m2MIe.png, if you're curious to see the difference.
@_date: 2017-04-06 02:28:36
I see. Thank you for your informative comments. I look forward to seeing more detailed evidence from the reverse-engineered covert implementation.
@_date: 2019-04-26 22:46:36
It's named Bitcoin Cash because it's a fork of Bitcoin that's intended to be usable as cash. It shares the same genesis block and first 8 years of blockchain history with BTC. The reason why it's not simply called "Bitcoin" is that it does not currently have support of the majority of users.
Do you think that ETC is intentionally named "Ethereum Classic" in order to confuse people? Or is it named that way to give people the (accurate) impression that ETC shares most of its history and code with ETH/Ethereum, but only differs on one major philosophical issue that arose after Ethereum had already been in existence for a few years?
@_date: 2017-04-02 22:20:23


Maybe that's the witness commitment for the merge-mined syscoin, which enabled segwit recently? Seems weird that it would be in the Bitcoin coinbase transaction rather than the Syscoin coinbase transaction, though.
@_date: 2016-02-28 18:42:29
Blocks weren't as congested then. They're more congested now. 
Each block, the miners will include the 1 MB of transactions with the best fees. If people are making transactions with better fees per kB than your transaction at a rate of 1 MB every 10 minutes, then they won't include yours. 
Your transaction will probably get confirmed once it's nighttime over Europe. Maybe sooner if you're lucky.
@_date: 2017-04-23 16:06:59
No, the 19x claim is not true. Currently, ASICBoost is around a 2-4% advantage. 
The main difference between the $2m estimate that Guy made in the above link and the $100m upper bound estimate that Maxwell made comes from the assumption that Maxwell made that electricity costs equal revenue. This is currently a wildly inaccurate assumption, as miners make up to 28Â¢/kWh while paying around 2-5Â¢/kWh.
Differences in electricity rates in different regions are much larger than the 20% efficiency improvement due to ASICBoost. For example, I pay around 2.8Â¢/kWh for my mine, but many other industrial miners pay around 5Â¢/kWh. The 5Â¢/kWh miners will be forced out of business long before ASICBoost turns into a practical requirement; even if they had it and I didn't, they'd still be paying effectively 4Â¢/kWh compared to my 2.8Â¢/kWh.
Andreas claims that power consumption is always the limiting factor. This is certainly not true. It costs substantially more money to fill a datacenter with miners than it does to build the datacenter in the first place. It also costs a little more to buy the miners than it costs in electricity to run them for the expected lifetime of that hardware generation. For example, an Antminer S9 uses about 1.3 kW and costs about $1400 with power supply and shipping. If you pay around 4Â¢/kWh, then over the course of two years, you'll spend $911 on electricity, or about 65% of the hardware cost. Add in labor, building rent, maintenance, failure risks, etc., and electricity ends up being about 1/3rd of the total costs.
Keep in mind also that this optimization has been patented independently at least three times, by Spondoolies, Cointerra/Timo Hanke/SDL, and Bitmain. It won't be a technology that is used by only one company, once it actually becomes financially lucrative enough to actually use.
@_date: 2016-02-27 09:18:56




That's usually true, but due to the way TCP congestion control works in situations with high packet loss (e.g. GFW crossings), it can be much better to have a short low-latency hop for the part with packet loss in order to minimize delays due to retransmission and to allow the congestion window to regrow quickly. For example, a two-hop path from Los Angeles to Seoul to Beijing will typically have better throughput than a one-hop path from Los Angeles to Beijing.


And they could use UDP instead of TCP, and they could send different parts of each block to each peer, with enough information to let peers **[safely share partial blocks](
@_date: 2019-04-05 23:56:32
Maybe you should have mentioned that, then? E.g. "daily tx per block"
@_date: 2017-04-06 01:04:18


This is not correct. SegWit blocks can be mined with the S9 (which is Bitmain's hardware that uses the ASICBOOST clone) just fine. I think you're confusing it either with the fact that (a) SegWit's design was influenced by 21.com's hardware, which hardcodes parts of the coinbase transaction, or that (b) Maxwell is proposing a modified version of the Bitcoin protocol that is incompatible with Bitmain's ASICBOOST clone, but still compatible with true ASICBOOST.
@_date: 2016-02-27 02:25:14
I do not suspect Matt has any unethical intents. I'm mostly just repeating Matt's own criticisms of the RN. 


I found two people who offered to help build relay networks about a month ago. I haven't checked in to see if they're still working on it and/or if they're having any trouble. I should do that. Thanks for the reminder.
@_date: 2016-02-25 00:23:38


Yes, it is possible to broadcast a perfectly valid transaction that never gets confirmed.
Bitcoin has run out of capacity due to the 1 MB blocksize limit. Transactions with low fees are currently being delayed. Unless transaction volume decreases or the blocksize limit increases, these low-fee transactions may never get confirmed.


Bitcoin Core 0.12 nodes will **[forget it after 72 hours]( Bitcoin Core 0.11 nodes will remember it usually until they are restarted, which could be a very long time. BitcoinXT nodes will forget it eventually too. Bitcoin Classic 0.11.2 nodes will remember it for a long time, but Classic 0.12 will forget after 72 hours. 
You may be able to double-spend the transaction by creating another transaction that uses the same inputs. However, doing so is difficult, as it generally requires crafting the transaction in raw format, and no GUI wallets have this functionality as far as I know. In other words, unless you're a programmer, I don't know of any ways in which you will be able to accomplish this.
@_date: 2016-02-25 20:27:40
Not quite true. We can do it with high-end hardware available today, but we would need to fix some of our algorithms (e.g. block propagation, better parallel use of cores, better UTXO lookup) first. The computer would cost **[about $2000]( today, and would require about 1 Gbps of bandwidth ($40/month in some cities, $4000/month in others). I think in 10 years, it's likely that hardware like this will be affordable, and the algorithms will be fixed. 
I'm not willing to bet on it, though, and I don't think we have to. I think we should be aiming for 10 MB blocks within a few years, and I think that is entirely within our grasp.
@_date: 2016-02-29 04:34:08
coinjaf, do you regularly check my profile to make sure you reply to all of my posts with negative comments? It sure seems like it.


No data yet on performance, it's true. I'll have to have something to test before data can be collected. However, it has had a fair amount of independent review, and about 1000 lines of code so far.


No, it would take a week or two, not an afternoon. 


I'm sure he knows a lot of things I don't. I'm sure I know a lot of things he doesn't.


I would say more "discounting" the problem than ignoring it.
@_date: 2016-02-25 05:38:17
**[Some full nodes will forget it after 72 hours, others will not.]( His wallet probably will not forget the transaction unless he forces it to forget it. Note that it's only Core/Classic 0.12 nodes that forget after 72 hours, and those nodes currently comprise a small minority (about 10%) of the total nodes out there.
**[The coins will never be permanently lost]( but they may be unspendable until he fixes his wallet or forces it to forget the transactions. 
@_date: 2016-02-29 03:03:51
It is what I've been spending most of my coding time on since Feb 19th.
@_date: 2016-02-26 03:06:11
If more than 51% of the hashrate goes offline, then it becomes very cheap to rent hashrate and/or buy used hardware to perform 51% attacks. 
Bitcoin's security model is predicated on the premise that 51% attacks are expensive to conduct because they require large capital expenditures. Bitcoin mining rewards (fees and subsidy) have to continually pay the operating costs necessary to make sure that the capital costs of an attack exceed the rewards. If enough of the hashrate goes offline due to non-profitability, that means that there will be a glut of used hardware on the resale market or being leased out on sites like Nicehash.com. A used hardware glut would drastically shrink the CapEx cost of an attack. A glut on Nicehash would eliminate it entirely. At that point, instead of needing to spend hundreds of millions of dollars to perform a 51% attack, one would only need to spend about 10% more than the normal block rewards (some thousands of dollars) in order to conduct such an attack.
This would make Bitcoin insecure and not very usable.
Furthermore, there is potential for a downward spiral to happen in this situation. If a lot of hashrate went offline due to non-profitability, it may cause some people to become (justifiably) worried about Bitcoin's security. This could cause those individuals to sell off some of their coins. That would reduce the exchange rate, which would reduce miner profitability further, which would cause hashrate to fall even more, which would cause more concerns about security, etc. It could turn into a combined market and hashrate crash that feeds off of itself.
@_date: 2016-02-29 21:48:39
Go ahead and do zero-conf. You're in a similar situation to a restaurant that serves food first and asks for payment later. 
If someone tries to defraud you of $10, you have a high probability of identifying them and getting them in trouble with law enforcement. That serves as a strong disincentive to people to attempt these kinds of attacks in-person for small transactions.
If you do $1000 in bitcoin sales in a month, you've saved about $20 on credit card processing fees. That's ignoring chargebacks. In order for Bitcoin to be a wash, you would need to have $20 in fraudulent transactions per month. That is unlikely. I would expect a fraud rate on the order of 0.1% for an in-person small transaction, not 2%. 
If you have surveillance cameras installed, that would make the probability of getting attacked without recourse even smaller.
@_date: 2017-04-06 06:10:25
Not really, no. We saw that someone gained access via SSH or HTTP to miners in one or more farms, and the configuration of those miners was changed to go from viabtc, antpool, and btc.top (but not bitcoin.com). The hack was probably performed by someone getting unrestricted access to the LAN of one or more megafarms that had machines pointed at those pools. Maybe they had an unsecured wifi issue, or maybe they had a disgruntled employee, or maybe their miner management software had a vulnerability. The pools themselves were not hacked, nor was there any indication that the individual miners had a bug. 
Individual mining machines should never have ports exposed to the internet as a whole; they should always be carefully firewalled. This appears to have been a failure at the firewall level.
It's worth noting that this is not the first time this kind of thing has happened. Spondoolies's datacenter got hacked in 2014 or 2015, causing them to lose a few hours' hashrate, and similar things have happened to competitors of mine in the hosting business.
@_date: 2016-02-26 03:22:16


Done. It's now mentioned in two places in my original post instead of just one place.


No, you can do a lot of things with a 51% attack. You can reverse recently confirmed transactions made by another person. You can blacklist certain addresses and prevent any funds from them from ever being spent. You can get 55% of the hashrate and use that to get 100% of the block rewards by orphaning any block from a different miner. You can simply short bitcoin on an exchange and sabotage the whole network. Once you have more than 51% of the hashrate, you can do pretty much whatever you want.
@_date: 2019-04-05 21:14:42
No it didn't. The record is 12,239 transactions in a block, set back in 2015:
These transactions were not economically useful. They were "anyone can pay" transactions that were 62 bytes each.
@_date: 2015-07-16 16:47:03
Almost certainly a scam, in my opinion.
They have stolen and photoshopped photos in the past for their website. After they got caught, they removed their photos. There are now no actual photos of their facility or their staff on their website. All that they have is a bunch of cheap (but shiny!) marketing stuff.
Most of the threads on bitcointalk are for advertising campaigns that they were running. They ran a social media campaign that was effective, and they ran an avatar/signature campaign on bitcointalk (where they paid well-known forum members to show cloudthink.io in their signatures). Cloudthink.io is behind on payments to the contributors of these signature campaigns.
They also offer to sell bitcoin mining hardware. They are offering prices far below what the market supports -- $200 for an Antminer S5 when the actual market price on ebay is about $400-$500
They claim to have a custom mining ASIC that is more efficient than anything else on the market. Designing and fabricating a 28nm ASIC costs around $10 million or more, including about $4 million just for the lithography masks. If they had enough money for this, you should be able to find out where they got it from. You should also be able to see photos and verify the identity of the CEO and key members of the staff.
They only accept Bitcoin and Litecoin payment, both of which are irreversible and anonymous payment methods. They do not accept any fiat payment methods. If they were truly a cloud mining company, they would have large power bills which they would need to pay with fiat. They would have a bank account. They would be able to accept wire transfers at very minimum.
@_date: 2017-04-06 01:08:46


No, it isn't changed by SegWit. SegWit blocks can still be mined with Bitmain hardware. A previous SegWit design candidate was incapable of being mined by 21's hardware because 21 hardcoded the payout address into the coinbase transaction of the blocks it mined.
Maxwell is proposing adding a new rule to target and exclude Bitmain's ASICBOOST clone, but this rule is not included in SegWit.
@_date: 2016-02-28 01:08:30
@_date: 2016-02-28 03:38:54
 is the original write-up.
It's a protocol that's inspired by bittorrent, but does not actually use bittorrent. The basic idea is that you get different parts of each block from a different peer and reassemble it, and you use the merkle tree structure of a block to ensure data fidelity. Because each chunk of data can be independently verified, you can use UDP instead of TCP and you don't have to worry about transmission order or reliability, which should make it all way faster. Plus some other stuff.
@_date: 2016-02-26 06:55:47




I said price-out *uses*, not *users*. I have heard from several developers that they think that many low-value uses of bitcoin (e.g. buying coffee or putting inscriptions in the blockchain) should be made financially untenable because they think that it bloats the blockchain and the UTXO set. I have heard some of them refer to these low-value transactions as spam. I think some of them think that the line between a spam tx and non-spam is something with a value around tens or hundreds of dollars. That particular line was drawn for me by a person who is a developer, but not a bitcoin developer, so it may not be indicative of the attitudes of others.
In any case, if demand for on-chain transactions causes congestion, then what happens is inevitably the pricing-out of some uses. If demand exceeds supply, then some transactions simply won't make it into the blocks. No matter what, some transactions lose out. That is pricing out of some uses of bitcoin. That is fundamentally what the fee market is when the fee market is driven by a block size cap.


On the contrary, Core is refusing to give **[miners]( and **[users]( what they have been asking for. 


Don't forget sidechains. Lightning and sidechains are the two pillars of Core's Bitcoin-settlement-layer roadmap.
Sidechains are the way the employer of most of the full-time Bitcoin Core developers is supposed to actually be making its money. It's worth noting that sidechains have little use unless the Bitcoin protocol is difficult to change and limited in capacity. Delaying any hardfork attempt by 16 months (in addition to the 1 year delay that has already elapsed) accomplishes both of those goals at the same time.
Blockstream preferentially hires people who share Blockstream's vision of Bitcoin as a settlement layer, which means that the set of full-time Core developers is biased to be people who share that vision.
The fact that Blockstream has a financial interest in a particular outcome for Bitcoin does not mean that they are malicious or that the outcome is the wrong one for Bitcoin. It just means their goals and plans should be carefully examined to make sure that they are in everyone else's best interest in mind, and to make sure that they don't push anything self-serving through. I think that the 0.25x discounting function in SegWit (which disproportionately benefits sidechain pegging transactions and lightning "subscriptions") and the obstructionism to increasing the blocksize (which manufactures an immediate need for people to switch over to sidechains and Lightning) are two such actions. 
If Lightning and sidechains are superior technologies, they should be able to stand on their own merits and win. There is no need to constrain block space so that the only affordable way to use Bitcoin is indirectly through them.


Which is before 2020. That's the first halving that has a substantial risk of knocking a lot of hashrate offline. The halving this year will not be a big problem for miners unless the exchange rate drops to about $200 shortly before it. The timeline of 3.25 years or even 5 years works fine. However, if we delay that timeline by spending the next 16 months with an effective capacity limit of around 1.35 to 1.75 MB (which is all that SegWit provides), we may have trouble making it to $3000/block in fees by 2020.
Note: you said "$3,000 per MB", but I presume you meant "$3,000 per block."




I can't tell you what people are using Bitcoin for right now. Probably mostly speculation, probably some international remittances, probably some retail purchases. I don't know what the uses are now. However, I can tell you how much use there is, and I can tell you that use has been growing steadily for years. 
The problem with Bitcoin that causes me to not know what it will be used for next isn't that there are no uses, but that there are too many uses. Speculation, financial contracts, peer-to-peer lending, black market activities, legal marijuana sales, low-fee credit/debit card replacements, Newegg/Purse.io/Starbucks purchases, remittances... Any one of these uses could provide more demand for block space than there is supply. I don't know which one is going to be the next killer app. Maybe none. Maybe all. But the demand seems strong enough that I'm really not worried about it disappearing.
@_date: 2015-07-28 06:19:11
This company looks super scammy to me. They seem to be mostly PR and marketing, with no evidence of a physical presence. No photos of the facility, no photos of the principals, but plenty of fake testimonials and shiny website plugins. Much of their website appears to have been stolen from actual hosting companies.
@_date: 2017-04-18 01:13:21
There is a good reason for that. Most payment methods, including PayPal, are reversible. Bitcoin is irreversible. If I pay you with a reversible method in exchange for Bitcoin, then petition PayPal to have the transaction reverted, it can be ... problematic. The large fees for buying via credit cards and PayPal are largely to balance out this risk.
Also, there's the risk from the fact that selling bitcoin without a Money Services Business license is illegal in most jurisdictions.
One way of getting Bitcoin without dealing with all of those regulations and requirements is to mine it. However, mining is slower, and doesn't have a guaranteed amount of revenue. (Though you also might make a profit.) Note: I run a Bitcoin miner hosting company and am a reseller of miners, so I might think this is a better option than most other people.
@_date: 2016-02-24 23:04:08
IPv4/6 have a lot of differences. The address size is different, the MTU requirements are different (fragmentation is performed at the endpoints instead of intermediate routers), etc. These are incompatible changes.
A bitcoin hardfork could have as many changes as IPv4/6 does, or it could have fewer changes. Hardforks also come in different flavors in terms of SPV compatibility. An address format change would break SPV compatibility, for example, whereas a blocksize hardfork does not. An address format HF would be similar to the IPv4/6 change.
It's also worth noting that IP does not have a blockchain, and does not have any strong economic incentives for consensus like mining. Thus, IPv4 and IPv6 can coexist simultaneously in a way that is not economically feasible with Bitcoin.
So the answer to your question is "sorta". There are substantial similarities and substantial differences.
@_date: 2016-02-23 06:06:58
And the end of that fun quote, where he sums it all up:




@_date: 2017-04-06 02:20:36
Segwit would block a particular optimization that Maxwell proposed (using modifications to the right side of the merkle tree in order to find collisions more easily). However, I don't think Bitmain's clone ASICBOOST method actually modifies the right side of the merkle tree. The stratum protocol does not allow the stratum client (the miner) to modify any part of the merkle tree except for the coinbase transaction in the left branch, and all mining hardware that has been released in the last 3 years (except for the 21 computer) uses stratum.
It's possible that this right-side optimization is being used in private farms, but doing so would require a large manufacturer to have designed and fabbed a special ASIC that can only be used with special poolservers. I have not seen any evidence that this currently exists, but it might be there. In any case, all of Bitmain's known hardware uses stratum, and most of their money comes from selling their Antminer S9s (which use stratum), so either I misunderstand something or their ASICBOOST clone in the S9 does not use the optimization that Maxwell is proposing that we prevent.
@_date: 2016-02-18 23:54:39
When a block is mined by someone, Antpool needs to switch to work on top of that new block. The way they do this is by first downloading the header of the new block (about 80 bytes) and building an empty block on top of that. They can't include transactions in this block because they don't yet know what transactions were included in the block they're mining on top of yet. This is called validation-free mining (VFM), or "SPV mining".
Then they download the block and validate it. Once they've done that, they can include transactions in the blocks they build.
If about 10% of their blocks are empty, that means they're mining on empty blocks for about 60 seconds before switching to full blocks. It shouldn't take more than 5 to 20 seconds to download and verify a 1 MB block, even in China, so I believe the majority of that 60 seconds is from not flushing stratum jobs. That is, the pool sends a job to the mining hardware for an empty block, and the mining hardware works on that job for about 40 seconds before requesting more work and getting a job for a full block. 
But let's assume I'm wrong, and assume the worst case scenario where it actually is taking Antpool 60 seconds to download and verify a full block. If that doubled with 2 MB blocks, then we might see 20% of Antpool's blocks be empty. (We're assuming that after the fork, the blocks become 100% full immediately, which is again a worst-case scenario.) Even in that case, we would have way more capacity from Antpool with 80% of the blocks at 2 MB than with 90% of the blocks at 1 MB -- 77% more, in fact.
Of course, they should still fix their pool infrastructure to reduce their percentage of empties.
@_date: 2016-02-27 23:26:32
I can answer this question. There are a few reasons:
1. The relay network is not reliable. It is not part of the actual reference implementation of Bitcoin, and requires on external servers that are currently run by one person. That person has even **[expressed intent]( to stop supporting the network soon.
2. The relay network is not scalable. It has substantial per-peer memory requirements that would cause it to cost a lot more money to run if it were used by more than just miners.
3. The relay network does not work in adversarial conditions. If a miner wants to perform slow-propagation-enhanced selfish mining, it is trivial to make blocks which the relay network cannot accelerate. All the miner has to do is to mine blocks with unpublished transactions, such as spam that the miner himself generates. In this case, the relay network needs to transmit 1 MB of data for a 1 MB block, rather than just 4 kB. The relay network only works well in cooperative scenarios.
4. (a) Since it uses TCP, the relay network has some trouble crossing the Great Firewall of China quickly and efficiently due to packet loss. (b) Since it is (mostly) not a multipath system, it cannot route around one or two high-packet-loss link very effectively.
Note that 3 and 4(a) also affect Xtreme Thin Blocks just as much. 
How important these reasons are is up to interpretation. I personally think that even with these shortcomings, with the relay network, blocks up to 8 MB are *probably* okay (though I don't have firm data on this), and without the relay network, blocks up to 3-4 MB should be fine. 
However, I recognize that these issues are real. That's why I'm working on Blocktorrent. It should address all of these issues quite effectively.
@_date: 2019-04-14 21:54:55
Both is best. `maxconnections=8` will reduce both the amount of overhead from announcing transactions in your inventory and (to a lesser extent) the number of historical blocks uploaded; `maxuploadtarget=50` will cap the amount of historical blocks uploaded but won't affect transaction announcement messages.
@_date: 2017-04-04 05:01:30
I came to this thread to explain my reasoning and preferences as a miner to kryptomancer. I did not come here to engage in yet another flamewar. Goodbye.
@_date: 2016-02-28 23:39:51
He misses the point. The relay network helps a lot in some circumstances, but if the block contains transactions that the recipient does not have, the relay network becomes almost as slow as the regular p2p protocol. The relay network is also a centralized system with a single point of failure. 
Blocktorrent's design is intended to give us great performance in all conditions, including adversarial ones. It should perform much better in the worst case and about as well in the best case.
@_date: 2016-02-25 19:25:41
I mostly like Lightning Network, but I have some serious objections to the way it's being pushed. My main objections are twofold:
1. It takes mining fees away from miners. This has the potential to result in a collapse of the bitcoin mining ecosystem after two or three more halvenings, which would make short 51% attacks on the network affordable, especially if enough rentable hashrate were available.
2. It is unproven technology. It may have unforseen issues (e.g. routing of transactions) or may provide an inferior UX. We should not handicap the existing system with an arbitrary and low blocksize limit in order to provide incentives to develop Lightning. Let's not break the current system in anticipation of something better.
@_date: 2017-04-15 03:07:13
"Evidence" is **[here]( 
TL;DR: pool.bitcoin.com currently pays out 110% of 12.5 BTC instead of paying transaction fees.
@_date: 2016-02-29 04:26:21
Antpool is already using UDP for GFW crossings. Kevin Pan wrote it. It works. They wrote their own algorithm to do it, and they have not made it open-source or published a description of it. Blocktorrent is more sophisticated and efficient than their protocol, though, and should perform much better.
The GFW comes down equally hard on UDP and TCP streams, as far as we can tell. The problem is that TCP gets confused by packet loss and interprets it as congestion, but UDP does not.
@_date: 2016-02-28 23:59:47
He thinks that bitcoin blocks should be **[kept small because we need a fee market](
@_date: 2016-02-26 22:36:55
My data did not include the relay network at all.
It will be difficult to make sense of it if you haven't watched my talk on it.
@_date: 2016-02-25 20:12:07


Actually, you can. It's not super easy to do it, but with 5-10 years of work and some modest hardware advancements, we could get there. But that's besides the point. The question isn't whether to rely only on blocksize increases or not. The question is whether blocksize increases should play a significant role in the scaling of Bitcoin. I think it should. The Bitcoin Core developers think it should not.
I would like to see a Bitcoin in which blocks are large enough that everyone can make an on-chain transaction for about $0.10 if they want to, but where most people prefer to use Lightning most of the time for small transactions because it's faster and better-suited for retail transactions. I do not want to see a Bitcoin in which everybody uses Lightning for almost everything because that's the only affordable way to use Bitcoin. I do not want to restrict the growth of Bitcoin over the next two years with an unnecessarily low block size limit while Lightning is being developed. And most importantly, I want to make sure there's enough on-blockchain capacity and transactions so that miners get paid enough in fees. Excessive reliance on the Lightning Network can compromise the long-term viability of Bitcoin.
You mention Peter Todd. You might not know this, but Peter Todd would rather we reduce the block size. He thinks that 1 MB is too much. He would rather have congestion and lower on-chain capacity than let the block sizes get too big, let the blockchain and UTXO continue to grow at the current rate, and let mining orphan rates be what they are. 
Visa averages **[1971 transactions per second]( 600 MB blocks would be needed to handle that volume. Once we have a better block propagation algorithm like IBLTs or Blocktorrent, the hardware requirements for handling 600 MB blocks in a timely fashion would be about 1 Gbps ethernet, an SSD capable of at least 400k IOPS (or enough RAM for the UTXO), and about 32 cores at 2.4 GHz (10 second worst-case validation time). Such hardware is expensive right now, but by the time Bitcoin demand scales 1000x, it will likely be affordable. But whatever, I'm not saying we *should* scale with blocksize alone, just that we *could*. 
My personal goal is to get us to 10 MB blocks by 2020 so that miners can get enough in fees to survive the next halving. If a lot of miners fail to survive the 2020 halving, it can **[seriously jeopardize]( Bitcoin's security.


This annoys me too, but I don't think it's fair to blame one piece of software for the actions and stupidity of its supporters. 


maaku's objection to the consensus was that he was not okay with the compromises that were being offered. Specifically, he did not like the reduction of the SegWit discount, and he did not want to commit to a hard fork blocksize increase at all. This suggests to me that a lot of the current developers of Core are too attached to their philosophical goals for Bitcoin to be able to compromise and give users what they want.


This is incorrect. Gavin wrote most of the **[2MB fork code]( for Classic. (See the stuff around Jan 19th.)
Gavin has more total commits than anyone else except for Wladmir and maybe Satoshi (who didn't use git):
    git shortlog -s -n
      3259  Wladimir J. van der Laan
      1102  Gavin Andresen
       963  Pieter Wuille
       635  Philip Kaufmann
       533  Jeff Garzik
       331  Cory Fields
       288  Matt Corallo
       245  s_nakamoto
       226  Jonas Schnelli
       210  Luke Dashjr
       190  Gregory Maxwell
    
He ranked  in 2014. In 2015, he had 22 commits in Core, or about 40% of what Matt Corallo had, though Gavin spent most of his time on projects other than Core (e.g. 31 commits in XT). "Gavin doesn't write much code" is a common objection raised by people who want to marginalize him, but it's not actually true.
Note: Number of commits is not a good metric of the amount of contributions a programmer has made to a project. Neither is Lines of Code (LOC). There are no good metrics for code contributions, just a few different crappy ones.


The decision is blocksize + LN (in that order) vs. LN + blocksize vs. LN alone. 
I think that keeping blocks small while LN is not ready is a bad idea, as it will push users onto altcoins or private blockchains instead. I think that keeping blocks small after LN is ready is also a bad idea, as it will cause mining hashrate to fall, which in turn could make 51% attacks about 1000x cheaper via dark hashrate ("rental") attacks.
Lightning Network is cool, but let's not handicap what we already have just to make sure there's enough incentive for people to develop and switch to LN once it's ready. To borrow a metaphor from Alex Petrov, that's like jumping out of a plane in a wingsuit, then burning your parachute because you think it will give you more incentive to learn how to land without one. 
@_date: 2016-02-28 23:36:56


No, Blocktorrent also lets you reuse data that you already have in your mempool. When you receive the merkle tree skeleton from a peer, that gives you the hashes of the transactions you will need for the block. (Actually, you can often guess these without downloading all of the data, but that's an optimization for later.) Once you know the transaction hash, you can check your mempool to see if you already have it. If you don't have it, you request it from one or two peers.


No, it won't help with initial sync, as that is limited by your CPU and disk accesses. Bitcoin will already download different blocks from different peers during initial sync, and there's no benefit from improving the granularity size with blocktorrent in that case.


It's pretty much guaranteed. GFW-crossing performance is one of the main design goals of blocktorrent. UDP transmission is the main technology that provides for that, since the issue with the GFW is that the packet loss causes TCP's congestion window to shrink to nearly zero size.
@_date: 2016-02-25 20:42:21
Note that transactions will be delayed even when blocks aren't 100% full because blocks can never be 100% full. The question that matters isn't how many transactions get included in the block; the question is how many transactions get excluded from the block due to the block size limit.
This is a big deal because SPV mining is common in China, and SPV mining will produce empty blocks for the first 5 to 60 seconds of each mining round. Having average blocksizes around 850 or 900 kB is about as much capacity as the current Bitcoin network actually has.
Yesterday, **[several]( **[transactions]( were **[delayed]( The third one of those took 11 hours to confirm (7 hours before the author posted, plus 4 hours after).
Blocks are full.
@_date: 2016-02-23 06:06:36
And the end of that fun quote, where he sums it all up:




@_date: 2016-02-26 05:00:26


It's my estimate of the global average, based on 350 MW and $0.05/kWh. China's rates are pretty close to that, with most of China in the $0.04 to $0.05/kWh range. Some parts of the world (e.g. mine) are even cheaper.


The fact that we're having block space shortages now suggests that getting new users and uses for Bitcoin is not a problem. Bitcoin transaction volume has been doubling every year for several years now. 
In any case, what the current developers are trying to do is to keep the block size limit low in order to price out uses of on-chain Bitcoin transactions. The developers want a "fee market", which is another way of saying they want fees per transaction that are high enough that a substantial portion of users avoid using a Bitcoin transaction at all.


Just let it grow naturally. Let people find their own uses for it. Bitcoin is inherently useful. It's not like the Federal Reserve has a marketing department trying to come up with and sell different ways that their "customers" can use money.
Sorry, that's kind-of a non-answer. Finding use-cases for Bitcoin is not what I consider to be my job. I consider my job to be making sure Bitcoin has the capacity to handle those use-cases when other people find them. So far, I have not been succeeding.
If bitcoin had more capacity, it's plausible that it might get used by banks as a tool in wire transfers, with the potential to reduce the $35 it currently costs. For example. But that would take a lot more than 1 MB every 10 minutes. 
Build it, and they will come. 


In the context of failure, it's actually all of Bitcoin that is at risk, not simply Lightning. If Lightning compromises Bitcoin miners' ability to collect fees and pay for mining, then Bitcoin is the system that is at risk of collapse.


\[Citation needed\] -- I was under the impression that there were still some things (e.g. routing protocol, wallet integration) that hadn't been finished. But I haven't been following their work super closely, so I could easily be wrong.
@_date: 2016-02-28 01:07:50


Do you mean a heat pump (converts electricity or work into heat flow) or a heat engine (converts heat flow into electricity or work)? I think you mean a heat engine.
The efficiency of a heat engine with a 20Â°C temperature swing and a 30Â°C exhaust temperature would be ridiculously low. The Carnot (theoretical) limit for the efficiency of such a device would be (1 - 303 K / 323 K) = 6.2%. A real device would probably have efficiency of 2% or less. It would also have very low power density, so it would be quite massive and expensive.


Assuming the 2% figure above, it would take about 70 Antminer S7s (which use enough power for about 50 houses) to regenerate enough power for 1 house.
@_date: 2017-04-06 05:50:58
And now they know of another mining system that SegWit is incompatible with, but instead of changing SegWit, they want to ban this mining system.
Notably, having been patented in August 2015 and active in silicon no later than 2016, Bitmain's optimization appears to predate SegWit.
@_date: 2016-02-27 00:56:01
Xtreme Thin Blocks are a great improvement in block propagation for normal full nodes. However, they are likely to be slower than the relay network. There are a few reasons for this:
1. The relay network should have smaller data packages, since it uses two bytes per cached transaction instead of six. 
2. The relay network does not require bitcoind to validate the block in between each hop. Since validating a 1 MB block can take up to 1 second, and since it typically takes 6 "bitcoind hops" to traverse the p2p network, and the total block propagation time budget is around 1 to 4 seconds, that's kind of a big deal. **Edit**: XTB have an **[accelerated validation mechanism]( and also have the ability to add more blocks-only peers.
3. The relay network has an optimized topology, and will only send each block at most halfway around the world, whereas the topology of the bitcoin p2p network is random and can result in the data traversing the globe several times over the course of the 6 hops.
4. The XTB shorthash mechanism can be attacked by intentionally crafting transactions with 64-bit hashes that collide. This would force additional round trips on each hop, delaying block propagation time through the network by several seconds.
5. The XTB system requires more round-trips than the relay network. In the best case, XTB requires 1.5 round trips, whereas RN only takes 0.5.
On the other hand, XTB has a couple advantages over the relay network:
1. The relay network is a centralized system run by one person. If Matt Corallo is asleep and a server goes down, the miners suffer. If Matt Corallo decides to **[simply stop supporting the relay network]( the miners suffer. If Matt Corallo decides that he doesn't like one particular miner and denies him access, that miner suffers. If one of Matt's servers gets DDoSed or hacked, the nearby miners suffer. Thin block propagation is p2p and decentralized, and does not suffer from these issues.
2. **Edit** It is possible to request an XTB from several peers in parallel. This provides some protection against the case in which one or more of your peers is particularly slow to respond, which is a very common case when crossing the Great Firewall of China due to its unpredictable and excessive packet loss.
Fortunately, miners can use both XTB and RN at the same time, and improve upon the reliability of the RN while also improving upon the typical-case speed of XTB.
@_date: 2016-02-29 00:42:49
If blocks are kept small, there is ~~no~~ less need for enhanced block propagation. If you'll permit me to extrapolate a bit, he probably thinks that blocktorrent is a solution to a problem that does not exist.
@_date: 2016-02-27 01:27:37
I added a link to show that at least one of those concerns is not FUD. If you put more effort into your comment, I'll put more effort into a response.
@_date: 2019-04-26 23:00:24
Sure, you're free to fork the code and call your fork whatever you want. But you don't have the right to choose the name of other projects. Projects are named by their creators, not their detractors.


And pretty much all of those people are members of this subreddit and hate Bitcoin Cash.
If a majority of people hated Bitcoin, would that justify renaming the project as Buttcoin? Or does the name that its creator chose take precedence?
@_date: 2016-02-26 07:23:17


This is a tautology. All of the transactions that are not included in a block will have lower fees than the rest. If everybody raises fees, then there will still be the same number of transactions that don't make it into blocks. Raising fees is not a solution to limited capacity: it just changes whose transactions get delayed.
@_date: 2016-02-29 03:03:27
Not unless miners have a collective bargaining system in place. The marginal cost of production for block space is nearly zero -- i.e., just the incremental orphan risk of larger blocks. Nearly 100% of the cost of producing a block is for the block's header and proof of work, which is a constant overhead and independent of the number of transactions included. Miners will choose to include transactions in the block as long as the marginal revenue of doing so (the transaction fee) exceeds the marginal cost. Since the cost is approximately zero, miners acting individually and selfishly will choose to include as many transactions as they can. If a miner acting individually decides to not accept a transaction below a certain floor in an attempt to incentivize users to include higher fees, then that just turns into a fee that was donated to the miner of the next block.
@_date: 2016-02-29 11:23:32
You mean ÂµTP? Not currently, but I like the idea it has of watching latency as a sign of congestion instead of packet loss. 
Right now I'm just implementing the basic protocol in pure dumb UDP with the intent of dealing with congestion and flow control after I have a working prototype and can collect data on what kind of congestion/flow control is needed. It's pretty simple to switch the UDP socket with a different transport socket later, or to encapsulate it in a socket that includes custom congestion control logic.


A lot of the advantages and optimizations of blocktorrent come from using the merkle tree structure as the index of chunks instead of hashing every X bytes. This allows you to avoid downloading transactions that you already have in your mempool, and also gives a great way to make use of weak blocks by referring to a weak block's internal node (which would imply a 2^n sized set of transactions) plus some diff. It also makes it feasible to make almost every single UDP packet fully independent and verifiable, which makes packet loss almost irrelevant in terms of how much it delays the transfer. Without that idea of using the merkle tree, the idea of using bittorrent with bitcoin for block propagation might not seem very compelling to Bram.
@_date: 2016-02-29 11:12:46
Blocktorrent will use neither WebTorrent nor Bittorrent. It's bittorrent-inspired, not bittorrent-based. Blocktorrent is a protocol and algorithm designed and implemented from scratch for Bitcoin block propagation, and only takes ideas and half a name from Bittorrent.
@_date: 2016-02-25 03:07:32
Most of that 12.5k baseload is near-zero-fee stuff from the **[October spam attack]( (the one with the 14780-14820 byte transactions) that simply never got cleared out. 
@_date: 2016-09-11 09:50:03
A $1000 laptop can get about 4 MH/s on the CPU while using about $0.01/hr worth of electricity. Currently, this would be expected to make $0.0000001/hr in total revenue, for a profit of -$0.0099999/hr.
A $1000 ASIC miner can get about 8,000,000 MH/s while using about $0.10/hr worth of electricity. This would be expected to make $0.2362/hr in revenue, for a profit of $0.1362/hr.
You can semi-viably mine Monero with a CPU right now, but not Bitcoin.
@_date: 2016-02-25 00:38:29
Glad to hear it. 


Use a higher fee.  can give you an idea of what fee to use.
@_date: 2018-11-14 23:10:16


That depends entirely on whose blocks end up getting orphaned. Once blocks start getting orphaned, the difficulty will fall (as orphaned blocks don't count in the difficulty adjustment algorithm). Since BCH's difficulty adjustment algorithm is a moving average over the last 144 blocks, adjusted every block, after 25% * 2 * 144 blocks = 72 blocks (or maybe 12 hours) it will likely be more profitable to mine BCH than BTC, as long as you can avoid having your blocks be orphaned.
@_date: 2018-11-14 22:22:15
No, pool.bitcoin.com will be using PPS, which means that users will be paid per hash regardless of whether that hash contributes to any blocks that are part of the consensus blockchain. Any revenue shortfalls will come out of bitcoin.com's own wallets.
@_date: 2018-11-14 22:19:53
During this event, pool.bitcoin.com will be making payouts in BTC according to how much you would have earned if they had still been mining on the BTC chain. However, they will be using the hashrate to help defend BCH against a 51% attack attempt by Craig Wright, nChain, CoinGeek, and Bitcoin SV.
@_date: 2016-09-22 23:55:50
You don't need the blockchain on an SSD unless you're a miner or pool operator. Even if you're one of those, you don't need the whole blockchain on SSD. 
What I do with a few of my full nodes that have both an SSD and an HDD is put ~/.bitcoin/blocks on the HDD and put everything else (especially chainstate) on the SSD, then link them all into the same place with symlinks. Currently, this takes about 1.8 GiB of SSD space and 90 GiB of HDD space.
@_date: 2016-09-22 23:58:17
SSDs are a good idea for storing the blockchain (or at least the UTXO set) if you're mining or running a poolserver. Doing so can give you an extra 100 ms of hashing every 10 minutes, or 0.1% better performance. 
@_date: 2016-09-28 04:04:52
I think this belongs in not @_date: 2017-04-06 06:30:22
Most of those are good points. A few comments:


The specific optimization that Bitmain is alleged to be using (modifying the right side of the merkle tree in order to search for hash collisions in the last 32 bits of the merkle root) actually doesn't do anything if you're mining 1-transaction blocks, and neither SegWit nor gmaxwell's proposed BIP would reduce the incentive to mine 1-tx blocks. That would require eliminating the ASICBOOST optimization entirely, which does not seem to be on the table. And the main reason to mine 1-tx blocks -- spy mining, and the consequent ability to mine profitably without having fully downloaded or validated a new block -- has nothing to do with ASICBOOST at all.


Yes, it is incompatible with most commitment-based soft fork upgrades. It is not incompatible with most hard-fork upgrades.


While I wish that ASICBOOST (and Bitmain's optimization) had not been possible, they were. At this point, changing the rules has some nasty ethical issues. Basically, we're using keystrokes to change the amount of wealth and power that one particular entity has. It's very similar to the DAO hard fork. Changing the rules in a fashion that disempowers a specific entity is pretty close to government-sanctioned theft.
It might still be worth it overall to patch the issue, but it's murky water. While I supported the DAO hard fork, that was reverting a theft, which I think is more justifiable than blocking one company's in-house mining optimization. This is ... tricky.
@_date: 2018-03-17 11:58:35
Oh, man, David, you chose to use BigCorp's LN nodes? That was a big mistake, dude. They charge mega big fees. You should have gone with ScrappyStartup instead. Sure, they sometimes have routing failures, so you might be unable to make payments for a few hours a week, but at least you won't give 10% of your BTC to line FatCatCEO's pockets.
There will always be important tradeoffs to make when choosing whom to channel with in LN. Those factors include:
1. Fees
2. Server reliability and uptime
3. Channel liquidity
4. Hub connectivity
5. Trustworthiness
The invisible hand of capitalism will ensure that companies that do well in 2-5 will tend to do poorly on 1, and vice versa. If you don't mind spending a lot of money, then you won't have to spend much time researching whom to channel with.
Even then, you will still need to know something about the underlying tech. You need to know that you shouldn't open a channel with a small amount of money, since you pay BTC fees per channel opened and per top-up. 
David opens a channel with only 0.01 BTC, and spends 0.001 BTC in fees to open that channel. So he thinks he has 0.009 BTC left in his wallet. He makes a bet with his friend and wins, and his friend owes him 0.002 BTC. His friend tries to send this to him via LN, but the payment fails because the channel only has capacity in one direction right now. David can spend money, but he can't receive. Oops.
Instead, David tries to use LN a different way. He's heard so much about how LN enables true micropayments, and he wants to try it out for himself by buying access to a New York Times web article. It costs 0.000001 BTC, which is much less than his 0.009 BTC balance, so he thinks he should be fine. But the payment fails, and David is shocked. After spending half an hour searching StackOverflow, David finds out that the first payment with a new channel cannot be a micropayment, because that puts the channel into a state where the channel-close transaction would have an output below Bitcoin's dust limit. This would make that transaction invalid, and you can't put a channel into a state where it can't be closed. So your first transaction has to be a medium-big one.
Instead, David decides to just buy a computer on Newegg. He finds one that he wants that would cost 0.00826 BTC, which is less than his 0.009 BTC balance, so he thinks he should be fine. But the transaction fails, because it leaves him with too little money to pay the BTC fee for the channel close transaction.
Okay, so David chooses a slightly cheaper computer instead. This one costs 0.0075 BTC, which leaves him with 0.0015 BTC to pay the transaction fee on the channel-close transaction. Two months later, David wants to settle his account and close his channel. However, BTC fees have quadrupled since then, so he now need to pay 0.004 BTC in order to close the channel. Unfortunately, his wallet does not support adding an external fee source to a Lightning channel-close transaction, so he can't close the channel at all.
These are some of the issues that can happen with LN. You can make a wallet that hides the complexity of setting up channels from the user, but you can't hide the failure conditions. If LN can't send or receive a payment for something, the UI will just tell you that the transaction failed, or at best ask you to authorize spending more money on fees to set up new channels. The wallet won't be able to guess what you will want to do with LN, so if you don't make the decisions yourself about how to initialize your channels, the probability is higher that you will either run into one of these failure cases or overpay on fees.
@_date: 2018-03-16 16:12:33


While that is one good reason to run a node, that is not the only reason. 
A second reason to be online and monitoring a LN channel is so that you can receive a payment via that channel. In an old-school unidirectional payment channel, the recipient does not usually need to sign an off-chain transaction, but in the bidirectional channels of LN, the [recipient *does* sign a transaction upon receipt]( Since receiving a payment requires signatures, you cannot delegate this to a watchtower.
A third reason to be online is so that you can permit other people to route payments through you. 
@_date: 2019-01-23 07:12:15
@_date: 2018-11-15 01:35:24
Not if they win.
@_date: 2019-01-07 22:39:22
Good point, I forgot that genocide is an option for solving Bitcoin's capacity limit problems.
@_date: 2016-09-30 06:12:15
After 20 minutes have elapsed on testnet, the difficulty is allowed to fall to 1 for one block, after which it returns to the previous value.
Bitcoin nodes will accept a block from a peer as long as the block's timestamp is less than 2 hours in the future. A miner on testnet can therefore set its clock forward by 20 minutes, mine a block at diff 1, set it forward by another 20 minutes, and then mine another block.
The block after a special 20 minute block gets the difficulty from 2 blocks ago, so if you do two of these forward-jump blocks in a row, you get a special condition. The next block after two jumps forward will see that the block 2 blocks ago had a difficulty 1, so you can jump backwards and still make a block for free. If you go forward 2x20m, then go back 20m, then forward 20m, and repeat steps 2-3, you can continue doing this indefinitely.
This is known as timewarping.
Furthermore, if you are able to get the diff1 block to fall on a difficulty adjustment, the difficulty will stay at 1 for 2016 blocks. This means that the weakest ASIC miner can find hundreds of blocks per second.
Testnet3 is weird. 
If you want to prevent someone else from timewarping, you can set your clock to 1h50m in the future. This way, any blocks from the warper will be more than 2h in the future, and will be rejected by the network. You can then outcompete them with hashrate.
@_date: 2019-01-23 06:47:14


Give me an address and amount to pay, and/or a QR code. I will never allow your browser extension to access or touch my wallet directly. 
I do not use custodial wallets like Coinbase for anything other than converting to/from fiat. If your app requires me to use a custodial wallet, I will not use your app.
@_date: 2018-03-25 05:17:55
No, you will not lose funds. 
1. The Bitcoin Cash wallet uses a different address format. All BCH addresses issued by the Bitcoin.com wallet start with a "q". BTC wallets will be unable to parse that address and will refuse to send BTC to any address that does not start with a "1" (P2PKH), "3" (P2SH), or "b" (bech32). 
2. "Not enabled by default" means you have to go into the settings menu to enable it. The BCH and BTC wallets both use the same private keys. If you manage to manually convert the cashaddr address to a legacy BTC/BCH address, and then send money to it on BTC, you can recover your funds by manually enabling your Bitcoin.com BTC wallet. Alternately, you can grab your seed words from your BCH wallet and stick them into any other BTC wallet. I just tested importing BCH private keys from the Bitcoin.com wallet into Electrum, and it works fine.
@_date: 2018-03-29 14:43:18
The mod logs are here:
In them, I see the following entry:


The reason for this ban is alleged ban evasion. This means that BitcoinXio was claiming that is an account created by a user who was previously banned for a different reason, but who created the bitmaincash username (e.g. using the same IP address) in order to avoid this ban. I don't know if that's true or not, but given that the bitmaincash account is only 5 days old, and immediately jumped into conversations as if he was familiar with talking points on the typical discussions, and given that the username is a slur against a specific cryptocurrency that is popular in that sub, it seems plausible that the user behind bitmaincash is actually evading a ban.
if you're not banned in this sub, can you comment to explain the ban, and to specify which previously banned account you think bitmaincash is derived from? If you are banned in this sub, feel free to PM me your explanation, and I'll post it here in your place.
@_date: 2018-11-15 02:27:46
PPS hands out rewards based on the number of hashes you've performed and the current network difficulty.
    12.5 BTC * user_hashes / difficulty_to_expected_number_of_hashes(BTC_network_diff)
@_date: 2019-01-23 07:11:57
Most of these transactions were tiny. The average transaction size in that block was 81.7 bytes. Many (like [this one]( were 62 bytes. These were *unsigned* transactions. The scriptsig for these transactions was [just "1"]( A single byte. No ECDSA, no pubkey hash. No security. The output scripts that these transactions were spending were [literally empty](
@_date: 2019-01-07 19:34:08
144 blocks per day, roughly [2,300 transactions per block](
    144 * 2300 = 331,200 tx/day
[7.67 billion]( people on Earth today. 
    7,675,000,000 tx / 331,200 (tx/day) = 22735 days * (365.24 days/year) = 63.45 years
@_date: 2018-03-17 17:20:47
Your understanding is correct. As I said, it's a slight risk.
@_date: 2018-03-16 14:29:03
Most current wallets will never support LN. The amount of code needed to add support LN is greater than the amount of code that most current wallets have.
Most people will not like needing to keep their wallet online 24/7 in order to receive payments, so I expect custodial wallets will become somewhat popular. Of course, in order to run a custodial wallet, your custodian would need to have your private keys, which means you have to trust your custodian to not steal your money.
An alternative is to only connect your node when you are expecting to receive a payment. That's a very different UX from current wallets, though. You'd basically have to call anyone you wanted to pay who does this before you could pay them in order to ask them to turn on their wallet. Kinda like delivering rent to your landlord in person rather than leaving it in their mailbox. 
Alternately, you could rent a server in the cloud to be your 24/7 wallet. However, if you do this, you'd better choose a reputable VPS provider, since your LN wallet needs to have your private keys in memory at all times in order to participate in LN. This means that your cloud hosting provider could probably steal your money if they're savvy enough and ethically compromised. With unpatched bugs like Meltdown and Spectre, your money could be stolen even by another unprivileged client on the same server.
@_date: 2017-09-10 20:22:40
That 687k "extra" cost the sender as much to send as about 250k pre-segwit. Because the sender of these transactions was using multisig segwit, they got a huge (~2.5-3x) discount on fees. They were able to send around 1.1 MB of transactions for the price of around 400 kB.
Segwit helps miners clear bytes from mempool faster if they're segwit bytes. Segwit allows spammers to add bytes to the mempool cheaper if they're segwit bytes. Where non-spam is concerned, sure, it helps, but I don't see that as a net improvement where spam is concerned. 
@_date: 2019-01-06 21:56:33
If you start with -dbcache=8192 as a command-line option, it will use up to 8 GB of RAM and will sync about 10x faster.
The bottleneck is not downloading. The bottleneck is reading and writing transaction outputs to the UTXO database.
@_date: 2017-09-27 23:07:26
Not me. I will greet anybody who violates their promise to support Segwit2x with distrust and disdain. In my eyes, they are liars. They are people who promised 2x as a compromise in order to get Segwit, and reneged on 2x as soon as they got the part they wanted out of the compromise.
@_date: 2019-01-07 21:12:03
63 years is close to the average life expectancy. If each channel is closed at the person's death, and a person lives 63 years, that means that we would need 2x as much capacity as we have to cover the planet's population. And that assumes that there are no channel-top-ups, and nobody ever changes wallets. This means that you would have to load each channel at birth (or age 18?) with as much wealth as that individual would ever expect to have in their lifetime.


The solution is probably to increase the blocksize limit. Or to not have the world's population directly use Lightning. Or to increase average human life expectancy to at least 126 years.
@_date: 2017-09-13 20:48:24
Try -dbcache=4096 (if you have 4 GiB RAM free, 8 GiB total). It will probably reduce the time by 80%.
@_date: 2017-09-08 19:41:59


In other words, they will not be cost-competitive. Air conditioning is expensive to build and expensive to run. Shelves are cheaper than racks. Fire suppression is expensive and unnecessary -- you should just build everything out of steel so that nothing burns.
Not even Intel is making 7 nm chips yet. By the time GMO has 7 nm chips, so will everyone else. If they don't have a good ASIC design team, then they won't have a good ASIC, and they won't be competitive.
@_date: 2017-09-06 14:00:25


Sorry, but I've thought about this, and it doesn't work. Most home miners are better off buying a heat pump than a Bitcoin miner, at least as far as the waste heat effect is concerned.
Heat pumps typically have a coefficient of performance around 3 to 4. That means that with 1 kW of electricity, you can provide 3 to 4 kW of heating, since 2 to 3 kW of that is heat that's pumped into your house from outside. (Heat pumps can also be run in reverse as an air conditioner.) If your electricity cost is 10Â¢/kWh, that means your effective cost per unit heat is about 3Â¢/kWh. In order for a Bitcoin miner to be more economical to run than a heater, the net cost of running it must be less than 3Â¢/kWh, which means that revenue must be at least 7Â¢/kWh. Or, in other words, the net cost of electricity for the miner is 7Â¢/kWh.
That doesn't sound too bad. 7Â¢/kWh is clearly less than mining hardware is making right now. However, it IS bad. I'm an industrial miner, and I pay 2.8Â¢/kWh. 7Â¢/kWh is 2.5x as expensive as my electricity rate.
As bad as that sounds for the competitiveness of home mining, it gets worse. That 7Â¢/kWh net rate only applies in the winter when your house needs extra heating. People like me get to run their miners 24/365 in an attempt to recoup the capital costs. But space heater miners will only run maybe 12h/d, 90d per year, or about 1/8th as often. It's hard to recoup capital expenses with economics like that when the machines are priced such that someone like me can barely make a profit.
@_date: 2016-09-22 23:59:09


And the default setting, like it is for Parity (one of the Ethereum full node clients).
@_date: 2017-09-09 14:28:58
7 nm is [about a year away]( They aren't going to beat anyone by years; they're merely going to be a member of the pack.
@_date: 2017-09-05 08:18:21
It's not ideology or malice; it's economics. A block 100% filled with 5 sat/byte transactions only provides 0.05 BTC of revenue. That adds about 0.4% to the total revenue of a miner. If the miner has a slow internet connection, 1 MB of data in their blocks can significantly slow block propagation and increase their expected block orphan rate by more than 0.4%.
In the past, I've set a fee floor of 5 sat/byte for my mining hardware (via p2pool) when the CPU cost of processing those transactions was causing significant delays in switching blocks. The performance of bitciond has been improved since then, so a fee floor of 1 sat/byte might make sense for me now, but I have faster internet connectivity than many of my competitors.
@_date: 2017-09-05 16:07:37
6 B/tx times 3000 transactions is 18 kB.
In my testing, bandwidth across the great firewall of china using TCP can get as bad as 5 kB/s even with 100 Mbps pipes due to packet loss. If they were using TCP for any of their cross-FW links, 18 kB could easily take a few seconds. 
UDP mostly fixes this issue, but even with UDP and FEC you can have trouble when packet loss gets up to 50%. Still, if they're connected to FIBRE with those nodes, I would not expect 0.4% orphan rates. That said, I don't have their data. 
@_date: 2017-09-19 23:37:15
"Economics 101" does not imply that changes in Bitcoin mining will affect the exchange rate for Bitcoin. Bitcoin mining produces an amortized constant 1.25 new coins per minute regardless of the network hashrate. Changes in hashrate produce temporary fluctuations in the coin generation rate, but after 2016 blocks (2 weeks) the generation rate returns to 1.25 btc/minute.
@_date: 2017-09-08 19:42:43
They are claiming that the technology they will release at some point in the future (a year?) will be more efficient than the technology that was released a year ago. How shocking.
@_date: 2016-09-05 16:04:37
It's more likely that comments were just marked by the Automoderator script as needing human review and suspended.
@_date: 2017-09-08 07:33:09
Currently, there are 252M transactions in the Bitcoin blockchain. I have a $700 computer that can verify (sync) that in about 6 hours. This suggests to me that it IS at the consumer level, albeit at the medium-high end.
@_date: 2017-09-09 18:33:15
If nobody else does, then neither will GMO. It's not like GMO is building their own fab. They're going to use the same foundries as everyone else.
@_date: 2018-03-16 14:01:51
Currently, you need to run a full node as well as a Lightning node.
Eventually, you won't need to run a full node, but you will still need to run a Lightning node, and you will need to keep that node online 24/7 in order to receive payments or to allow other people to route payments through you.
@_date: 2017-09-10 23:53:06
But why did they have all those UTXOs in the first place? If they had received payments from users to generate each UTXO, then compressing them down at times of low fees makes sense. However, it looks to me like those inputs were all generated by a single entity -- or, more specifically, generated by the same software using the same fee estimation algorithm, which quite strongly suggests a single entity.
If that analysis is correct, then this is like lighting a fire and then taking credit for putting the fire out. 
@_date: 2017-09-21 16:02:25
Can Venezuelans afford high transaction fees? If someone is making $40/month in mining revenue, and each transaction has a fee of $2, what can they actually do with their "censorship-resistant money?" It's only revolutionary if it's affordable.
@_date: 2018-03-17 03:34:08
If your node goes down, then you can't receive or send any payments, and no LN payments can be routed through your node. 
Furthermore, as long as your LN node is down, you are at the slight risk of losing funds if someone you had a channel with decides to close a channel using an old transaction.
Yes, it matters which node you're connected to. If the node(s) you're connected to go offline, you can't receive or send payments, and if the node(s) they're connected to go down, you can't receive or send payments. Also, if you're connected to a node that has less capacity in their payment channels than the size of the payments you want to send or receive, you won't be able to send or receive those payments. You will always want to connect to people who are richer and more reliable than you are if you want to send or receive payments. 
This begs the question: why would rich people with dedicated servers want to open a payment channel with you? The answer, of course, is fees. Large hubs will only open channels with end users if they expect to earn more in LN fees from the channel than it costs them to open the channel in Bitcoin fees. 
@_date: 2017-09-09 23:41:10
3.7 MB is a worst-case scenario. You can only get to 3.7 MB with a spam attack, using specially-crafted transactions that are designed to take up as much space as possible using the minimum amount of "weight".
@_date: 2017-09-05 20:19:06
Jiang Zhou'er runs BTC.top, which [is not Bitmain](


@_date: 2018-03-29 15:00:03
If you have been using a VPN, then it's possible that your VPN has given you an IP address that overlapped with an IP address for another banned user, and caused BitcoinXio to incorrectly permaban you. If that is the case, your ban was probably a mistake. If you politely email the mods about this, they might be willing to reverse the ban.
Unless, of course, you are actually a new sockpuppet account. Why is your account only 5 days old? How were you active in the crypto community before then? What other forum accounts do you hold? Do you claim to have no other accounts that have posted on and if so, can you provide evidence for that claim?
@_date: 2017-09-08 07:37:12
His calculations are bad. They assume that you read the whole block once for every SPV node. That isn't a good assumption. A smarter way to code it would be to collect 100 or so bloom filter requests, then run all 100 bloom filters on each block at once. That way you only have to read the block once per 100 users. 
You can also set up servers that have different blocks cached in RAM, so that when a user wants a bloom filter to be run on a block, they first find a server that has each block cached, then ask each server in parallel to run their bloom filter.
@_date: 2017-09-10 00:12:47
It depends more on the electricity price than the climate. The COP=3 number I used is actually pretty conservative. Many heat pumps are capable of COP=5 when working over a lower delta-T, in which case the heat pump would have an even larger advantage. 
As it gets colder outside, the COP decreases and the duty cycle of the miner increases; both of these effects reduce the heat pump advantage. However, even in the most extreme case, you get COP=3, miner_duty_cycle=1.0, and you're left with the 7Â¢/kWh vs 2.8Â¢/kWh comparison I gave earlier.
In warmer climates, the advantage of heat pumps is much greater.
@_date: 2017-09-09 23:39:57
No, that's not accurate. The SegWit discount applies to signatures, but not to addresses (pubkeys) or coin amounts. Multisig transactions have the same amount of addresses and coin amounts as normal transaction, but they have a lot more signature data. Consequently, they get a much larger discount.
The 3.7 MB figure comes from a transaction that has a single input, a single output, and 8 kB of signature data in it. You don't get more transactions with Segwit multisig, you just get more bytes.
@_date: 2017-09-08 07:47:28
In 2012, there were [883 million]( VISA card holders. Today, there are probably about 1 billion. If each one opens and closes a Lightning Network channel every 3 months, that generates 2B transactions every 3 months, or 22 million transactions per day. At 500 bytes per channel open or close, that's 11 GB per day, or 77 MB per block.
LN helps, but it's not a panacea.
@_date: 2017-10-29 20:08:26
A platform with capacity but no users is boring. A platform with users but no capacity is painful. What we need is a platform that has both.
@_date: 2017-09-12 01:58:03
Heat pumps *are* capable of reaching the desired temperature for hot water heaters, and heat pump water heater units are commercially available. However, the typical duty cycle for residential water heaters is low, so the preferred designs tend to optimize more for capital cost than operating cost. That makes water heating even less viable as an application for Bitcoin miners.
It turns out that water cooling systems for computers are usually rather expensive. It's almost as if water and electronics don't mix. It's possible to make it work, but it's rarely cost-effective.
@_date: 2017-09-10 00:08:21
I hate to say it, but it looks like these blocks might have had a bunch of spam. There's a suspicious group of 64.3 kB SegWit transactions in both of these blocks:
Block  has 8 of these transactions, and  has 10 of them. All told, that's about 1155 kB of space used by one entity in two blocks.
Each of these transactions has 200 inputs and 1 output. At 64.3 kB per tx, that amounts to roughly 321 bytes per input. That sounds like a multisig tx, which is a well-known way to pack more bytes into the same weight with Segwit.
It's also possible that these transactions belong to an exchange or some other large entity that uses multisig. Still, it's weird, seemingly artificial, and clearly one entity that's doing this. Does anyone know of any exchanges that use P2SH or P2WSH deposit addresses?
Edit: more data [here]( thanks to @_date: 2017-09-10 16:59:19
It's not just that they're consolidation transactions. It's also that the parent transactions that are being consolidated appear to have been created by one entity, based on the pattern of fees used in those parent transactions. 
@_date: 2017-10-29 00:02:40
This *is* about blocksize. Fees are too high. Transaction throughput is too low. 
@_date: 2017-09-08 18:01:20
Neither of the two processes I suggested require "big node" solutions. Both are pure software fixes, and are entirely compatible with a distributed computing approach.
@_date: 2017-09-29 19:51:10
s/replay protection/wallet backwards incompatibility/


"Replay protection" is another way of saying "making the transaction format different so that all previous software has to be rewritten or modified".
@_date: 2017-10-19 15:53:32
Actually, a little over 15 hours (905 minutes):
@_date: 2017-10-29 00:19:19
15-20% is slower than the rate at which hardware improves. Hardware improves at about **[30% per year for bandwidth]( and about 40% for computational power. SSD storage also follow's Moore's law (40%/year).
@_date: 2017-10-30 22:24:11


Very few merchants accept litecoin.


No, Segwit's low usage indicates that very few wallets support Segwit. This might be because Segwit triples the number of transaction formats that wallets need to support.
@_date: 2018-07-22 06:40:29
I wasn't aware that the definition of "shitcoin" was precise enough to allow a calculation with two significant figures. 
I also wasn't aware that the definition of "shitcoin" encompassed all but the top 9 coins. But reverse-engineering your claim seems to indicate that, so I guess it must be true, right?
But seriously: dismissing cryptocurrencies like ZCash, Dash, Monero, Tether, and Siacoin as "shitcoins" and therefore of no significant value is not reasonable. Each of those cryptocurrencies has substantial technical innovations and trading volumes between $7m per day (for Sia, the smallest) to $2040m per day (for Tether).
But even if you were right and only the top 9 cryptocurrencies mattered, the combined market cap statistic would still be basically the same. The top 9 cryptos are and have always been the vast majority of the total market cap. 
@_date: 2017-10-31 21:00:24
It's entirely consistent with my narrative. My narrative, as of over 1 year ago (Oct 11, 2016), was that we would see [slow adoption of Segwit]( I also [predicted]( on Aug 16, 2016 that we would not see blocks get larger than 1.2 MB in the first month. Nine weeks after activation, the average block size is 1.04 MB.
People aren't upgrading to Segwit for a few reasons: 
1. They're lazy, and they don't like switching wallets or even upgrading wallets. 
2. It requires a lot of code to implement it and a lot of testing to make sure that nobody is going to lose money from a poor implementation, which means that few wallets support it already. 
3. Segwit requires people to act in advance in order to get savings in the future, which is a type of reasoning that humans are [notoriously bad at]( 
4. Most of the benefit of Segwit (or other transaction throughput increases) is to bystanders: using a Segwit transaction might reduce the fee for that user by 1.8x, but having blocks not be congested (e.g. if everyone used Segwit) reduces fees by about 10x.
On the other hand, a hard fork to increase the base block size would be transparent to anybody who is using an SPV ("light") wallet. It would reduce fees immediately for everyone without them having to do anything unless they were a power user running their own full node. I have always thought that to be an easier and more effective option than Segwit.
@_date: 2018-07-23 19:14:42
The Innosilicon model gives about 17% more hashrate than the S9i for the same amount of electricity, but about 34% less hashrate for the same initial capital investment. At $0.05/kWh, it would take 32.7 months of operation before the Innosilicon model was a better deal than the S9i.
@_date: 2017-09-05 08:24:04
Try p2pool. When installing bitcoind, choose an appropriate minrelaytxfee setting. If you're using [my fork]( of the code, p2pool will use whatever transactions bitcoind passes to it. If you're not using my fork of the code, then p2pool will create blocks that have all of the transactions passed by bitcoind about 61% of the time, and a subset of the transactions (and smaller overall blocks) [about 39%]( of the time.
@_date: 2015-09-17 05:38:59


No, that would reward all p2pool miners, including the p2pool miners that are using Core. The only fair way to do it is to reward 1GuDnEyYSE3Ra3pMar7311tx5poR5PGXR3, since that is the address that found the share that found the BIP101 block.
@_date: 2015-09-25 18:15:17
Free electricity? A used Antminer S3 might break even.
@_date: 2015-09-07 04:41:15
Miners that you own cannot be used as leverage for trades on exchanges. 
Simply creating a large block does not cause disruption. Either the majority of the mining network can handle your block in a timely fashion and it is accepted into the main chain, or it is too large for most and it runs a high chance of getting orphaned. If it gets accepted by most nodes but some miners are slow in downloading it (and don't do SPV mining), then they might get some orphaned blocks. Orphaned blocks happen every day and don't move the exchange rate markets.
With BIP101, the block size cap follows a very well-defined schedule. A solo miner who does not secure enough bandwidth to be able to handle blocks at the max allowed size is incompetent and deserves to lose their orphan races. Miners are in the business of transaction verification. Verifying transactions for a global payment network inherently requires a substantial amount of bandwidth. If you can't afford $10 to $100/month for the bandwidth needed to run your own full node with BIP101 blocks, then you probably also can't afford the payout variance from solo mining. In any case, I don't think we should limit Bitcoin's transaction throughput capacity to what the smallest, worst-connected miners can handle. If a miner can't keep up with the blocks and transaction volume, then they shouldn't be mining.
@_date: 2015-09-22 19:20:43


This is the crux of the issue. If they have a system for efficient micropayments similar to LN, then they might have a viable product until the actual LN gets finished. I guess I'm just skeptical that they have figured out a way to do it that requires that the satoshis be freshly mined. If it does not need to be freshly mined, then I think that it would be cheaper and more efficient to register as an MSB and sell people preloaded private key gift cards than to develop a product like this. I don't really see how making something freshly mined makes micropayments work better except insofar that coinbase transactions are not subject to the same priority rules and thus could be included in a block even if they do not come with a fee attached. 


Yes, but you can acquire it by taking that bundle of cash to Bitstamp.
@_date: 2015-09-07 04:09:49


I am not seeing how you can reasonably read OP's comment in that way. He said this:


He was referring to LN as a whole, not an individual transaction on LN.


Yes, I think that is essentially his point. Bitcoin-qt was ported over to litecoin-qt pretty easily. The only reason why the Lightning Network has been developed with Bitcoin in mind instead of Dogecoin is because the economic majority thinks Bitcoin has value even without Lightning and thinks Dogecoin is a joke. I might add, if Bitcoin gets significantly congested before LN is ready, then Bitcoin will lose market share to coins with greater network capacity; Dogecoin, with its 1 minute blocks, might not seem like a joke any longer. The economic majority could shift, and if it did those of us losers who hold on to Bitcoin will be out of luck. Once Lightning Network is ready and working, it will be deployed first on whatever coin is the economically dominant one. 
@_date: 2015-09-15 01:03:05
VISA is huge. It's not a realistic short-term goal to be able to achieve their levels in any way. Mostly, it's unrealistic to think that a majority of people will be willing to switch over to an alternate cryptocurrency in that time frame.
The goal with the blocksize increase is to increase the capacity of Bitcoin in order to exceed the demand for Bitcoin. Right now, with 1 MB block limits, we are limiting the capacity of the Bitcoin network well below what the hardware running the network can actually support. Furthermore, we are limiting the capacity of Bitcoin to about the same level as the demand for Bitcoin transactions. 
The BIP101 scaling algorithm is designed to immediately put into use the excess capacity that we have today with typical hardware, and then to grow according to typical historical technological improvements with bandwidth and storage in order to keep the cost of running a full node approximately constant. The goal of BIP101 is not VISA. It's just constant-cost growth.
Eventually, maybe in 20 years, enough people will be using Bitcoin to get to VISA's size. VISA has been around since 1958. They didn't grow to the size they are in a day.
@_date: 2015-09-25 07:21:19
I won't be updating this thread. Send an email to orders if you're interested.
@_date: 2015-09-17 05:30:48


Actually, NoXT is an attack on both XT and Core. By increasing the amount of declared support for XT, NoXT makes the probability of a hard fork higher. This hurts Core users because it increases the probability of a hard fork, and it hurts XT users because it increases the probability that the hard fork will be contentious.
NoXT is basically just trolling. I don't think 25% of miners are going to be trolls, so I don't think its existence is an issue.
@_date: 2018-07-22 21:56:07
Pruning has already been introduced. 
Start bitcoind with `prune=10240` in bitcoin.conf if you want to only store 10 GiB of historical blocks. This amount is in addition to the chainstate database (i.e. the UTXO set), so your total storage use with this setting will be about 13 GiB.
From bitcoind --help:
      -prune=&lt;n&gt;
       Reduce storage requirements by enabling pruning (deleting) of old
       blocks. This allows the pruneblockchain RPC to be called to
       delete specific blocks, and enables automatic pruning of old
       blocks if a target size in MiB is provided. This mode is
       incompatible with -txindex and -rescan. Warning: Reverting this
       setting requires re-downloading the entire blockchain. (default:
       0 = disable pruning blocks, 1 = allow manual pruning via RPC,
       &gt;550 = automatically prune block files to stay under the
       specified target size in MiB)
@_date: 2015-09-17 05:41:25
Yeah, it's kinda lame. Apparently only small miners currently support big blocks.
@_date: 2015-09-23 01:03:31
	














@_date: 2015-09-17 04:59:43
My company mined this block. 


That's a good observation, because it brings up some really interesting points.
I have a minimum fee set in ~/.bitcoin/bitcoin.conf:
    minrelaytxfee=0.00005
    mintxfee=0.00005
    blockmaxsize=990000  is 500000
I support BIP101, which allows for large blocksizes, but I have my pool configured to not create the largest blocks possible. You might ask, isn't that inconsistent?
Spoiler alert: it's not.
The blocksize cap is to determine the largest block that anybody (miner or not) should be forced to download. My nodes have enough connectivity right now to be able to download and process blocks 20 MB in size rapidly enough for our mining revenue to not suffer too heavily. It should only take us about 3 sec to download 20 MB, assuming we could find a peer that can upload that quickly (and assuming that Corallo's relay network or p2pool didn't handle it in O(1) time). If we needed to, we could upgrade our bandwidth a lot without breaking our budget.
The size of blocks you create, on the other hand, is all about upload and CPU usage. We've got enough upload bandwidth to be able to handle large blocks with low orphan rates. However, there are currently some serious performance problems when trying to create large blocks on p2pool. 
(The current implementation of CreateNewBlock() in Bitcoin Core and BitcoinXT (for the getblocktemplate RPC) is single-threaded and not well optimized. When trying to create 1 MB blocks, it takes my node about 1 to 2 seconds to complete. While this is a minor issue for traditional pools with 600 seconds per block, it's a much bigger problem for p2pool with 30 seconds per share. I've been looking at doing some work to parallelize sections of CreateNewBlock using OpenMP ( omp parallel for), but I haven't yet had the time to finish it with all the other tasks I have to juggle to run a bitcoin mine.)
Because of the performance problems that result from creating large blocks, I set my node's policy to only include transactions in a block if there's a significant fee to back it up. I don't want my miners to mine stale shares or to have my node get its blocks orphaned because I included a bunch of no- or low-fee transactions in my block out of charity.
In other words, [a transaction fee market exists even without a hard blocksize cap](
One of the reasons why I support BIP101 is that I think the hard blocksize cap is probably unnecessary. Creating large blocks is expensive for miners, especially if the blocks are larger than the network can quickly handle.
@_date: 2015-09-07 05:05:10
1. Please distinguish between "miners" and "pools". Miners with low bandwidth will use pools. The bandwidth needed for mining using the stratum protocol is about 1 kB/s, regardless of hashrate or blocksize. 
2. Bandwidth costs are nearly zero for all but the smallest of miners. I run a hosting company with about 0.25% of the bitcoin hashrate. We run two full nodes in our facility. We pay about 1% as much on internet as we do on electricity, and with 0.45 MB blocks, we only use about 1% of the bandwidth that we pay for.
@_date: 2015-09-26 00:45:19
An SP15? Wow, I didn't know those even existed.
@_date: 2015-09-23 02:32:53


That's not a very factual article. The authentication can be done with biometrics and private/public key pair cryptography without using Bitcoin at all, but it hasn't caught on because people like the ability to use pseudonyms and don't like their identity to be tied to any hardware.
The e-commerce point doesn't work because of the huge cost premium for mined coins with inefficient hardware. Same thing for digital content. Why would people sell a good online to someone who mined the coins using 21's hardware 1/5th the price that they can get for it if the person got their bitcoin from some other method?
There's also another issue. If 21 is planning on having these microtransactions appear on the blockchain, then they'll run into a problem with capacity. If 1% of the world uses bitcoin to replace advertising for 1% of their web page views, that will be about 10 million transactions per day. With current block sizes, we are limited to about 200,000 transactions per day. They have not revealed how they plan on addressing this issue. Micropayment channels would likely not work well because there are not expected to be a large number of transactions between each pair of entities. Lightning network might work, but only if the total number of users is small (about 1 million or less) or the block size is much larger, since even settling LN accounts once a month produces a lot of transactions if you have a lot of users.
Whatever. This conversation is boring. I probably won't reply any longer.
@_date: 2018-07-23 07:08:55
They are currently selling the 17 TH/s, 0.083 J/GH model for about [$1100]( For comparison, Bitmain is selling a 14 TH/s, 0.1 J/GH model for about $600.
@_date: 2015-09-27 07:04:04
Smart contracts? What about 21's device enables smart contracts per se? It seems to be an attempt at a micropayment enabler, not a smart contract enabler. Smart contracts require that you have a programmer write a bitcoin transaction that will only be valid when the conditions of the contract are completed. The 21 computer does not do that as far as I have read.
@_date: 2017-09-09 23:23:01
I hate to say it, but it looks like these blocks might have had a bunch of spam. There's a suspicious group of 64.3 kB SegWit transactions in both of these blocks:
Block  has 8 of these transactions, and  has 10 of them. All told, that's about 1155 kB of space used by one entity in two blocks.
Each of these transactions has 200 inputs and 1 output. At 64.3 kB per tx, that amounts to roughly 321 bytes per input. That sounds like a multisig tx, which is a well-known way to pack more bytes into the same weight with Segwit.
It's also possible that these transactions belong to an exchange or some other large entity that uses multisig. Still, it's weird, seemingly artificial, and clearly one entity that's doing this. Does anyone know of any exchanges that use P2SH or P2WSH deposit addresses?
Edit: more data [here]( thanks to @_date: 2017-09-20 16:25:38
Can anyone point to me the part of the NYA that prohibits supporting Bitcoin Cash?
For reference, here is the text of the NYA:








The Hong Kong agreement had a provision specifically excluding its signatories from using any Bitcoin client in production other than Bitcoin Core. However, NYA has no such provision.
In my interpretation, supporting parallel options like Bitcoin Cash is permissible under NYA as long as doing so is not an existential threat to "Segwit2Mb".
@_date: 2015-09-30 02:34:58
Not easily, no. You have to limit the number of connections. Otherwise, you can download 1 MB and upload 10 MB. 
Also, all that JSON in the communications protocol adds a lot of overhead. And currently, transactions are sent twice unless you're on the relay network. Actual usage is much more than (block_size * number_of_blocks * times_sent). 
@_date: 2015-09-27 02:21:45
Multibit Classic lets you do that. Don't know about Multibit HD.
@_date: 2015-09-18 00:34:29
As far as I understand it, all of the other code changes (double spend relays, getutxos, tor deprioritization, etc) affect the full node behavior of BitcoinXT, but do not affect the block creation behavior. I think a Slush block would be indistinguishable from a "true XT" block or a Bitcoin Core bigblocks patch block.
@_date: 2018-07-22 22:34:29


Market cap figures are only calculated for tokens traded on public exchanges.
@_date: 2015-09-07 03:19:06


In order to do this, a miner would have to both (a) possess or temporarily control a large portion of the total bitcoin mining hashrate, and (b) take a short position the value of bitcoin. These are mutually opposed actions. If you own or rent bitcoin miners, you are taking a long position on the value of bitcoins.
Long propagation times do not make blocks more dangerous or disruptive to the network. They just make them more likely to get ignored in favor of a smaller block that propagates faster.


Then the attacker wasted 25 bitcoins worth of hashpower creating what could have been a valid block. In all likelihood, that corresponds to several days of hashing. The attacked entity will start mining on that block as soon as the header is downloaded, and will stop mining on it as soon as the full block is downloaded 10 to 100 seconds later. Total cost to the victim is far less than the cost to the attacker.
@_date: 2015-09-06 22:55:55
How many bitcoin full nodes do you think bitcoin needs to be "decentralized"? In my opinion, bitcoin full-node decentralization is the least important type of decentralization. Non-mining bitcoin full nodes don't really do much. Miner decentralization matters, pool decentralization matters, and developer decentralization matters. The only serious concern that having a large number of full nodes protects against is Sybil attacks, and I think there is not a large risk of those being practical or very useful. Another thing about Sybil attacks: If the number of full nodes decreases because the cost of running a full node is higher, then the cost of running full nodes for a Sybil attack will also be high.
Full nodes basically just relay transactions to other full nodes and to SPV clients. If the number of full nodes decreases while the number of SPV clients increases, then the load per full node will increase. Moving full nodes into datacenters with 10+ Gbps bandwidth should scale the capacity per node faster than the load per node.
@_date: 2015-09-07 10:03:24


The current fee levels average about 0.32 mBTC/kB or 7.7Â¢/kB. At 100 MB blocks, that would be 32 BTC or $7,700 per block, which is greater than the current block subsidy of 25 BTC.
@_date: 2015-09-07 10:28:52
The source for my 0.32 mBTC/kB is dividing typical transaction fees per day (23 to 30 BTC/day) by the number of blocks per day (about 144) and then by the average size per block (470 kB). It's actually a bit more than 0.32 mBTC/kB, and probably more like 0.4 mBTC/kB. 
@_date: 2015-09-26 00:45:56
Sometime between 4 months and never.
@_date: 2015-09-26 02:44:59
Sorry about that. We were pretty swamped last fall, and we didn't have someone to deal with billing/orders/sales until around March.
@_date: 2015-09-17 07:49:57
@_date: 2015-09-24 21:47:03


Yeah, I don't really like the fractional miner thing either, as it locks our customers into only moving the machine if all of the owners choose to do so as a group. At least they get to choose their own pool, though, which I think is the most important aspect.
@_date: 2015-09-17 05:50:32


Nope. Each node on p2pool is trying to create a block with a coinbase tx that will pay out to all of the miners on p2pool in proportion to how many "shares" they have found in the last 3 days. The rest of the content of the block will vary from p2pool node to p2pool node. Some nodes might be trying to create a 1 MB block, and others might be trying to create a 100 kB block at the same time. 
Each miner on each node is actually trying to create a slightly different block. Each p2pool block encodes the bitcoin address of the miner/user who mined it. If a miner has a semi-near miss (hash value is greater than the bitcoin target, but below the p2pool share target), then the p2pool nodes share that near miss (a "share") amongst each other, and the publication of that share changes the amount that the other nodes "owe" that user's bitcoin address in their next block attempt. 
@_date: 2015-09-15 04:34:51
Find a miner. Buy direct.
@_date: 2015-09-14 05:31:55
Paging Was the 20 MB proposal due to an arithmetic error? Peter Todd claims so.
@_date: 2015-09-08 05:05:27
Good point, luke-jr. In BIP100, the 80% threshold for changing the max block size is really just a 51% threshold in disguise. If 51% of the hashrate really wants to change the blocksize, they can do so by orphaning any blocks found by the 20-49% minority that is being obstructionist. This leads me to ask which system is better:
1. A system in which a simple majority vote (&gt; 50%) determines the next block size.
2. A system in which a large supermajority (&gt; 80%) determines the next block size, encouraging 51% &lt; x &lt; 80% majorities to orphan blocks of vetoing minorities in order to manipulate the vote so that they have &gt; 80%.
Even with less than 50%, a large minority voter might be able to manipulate the vote by attempting to intentionally orphan other miners' blocks in a manner akin to selfish mining.
@_date: 2015-09-14 02:30:26
It's fine if companies like Blockstream or Fidelity have an interest in seeing Bitcoin go one way or another, and if they expend resources in seeing that vision come to pass. We just need to keep their financial interests in mind when evaluating their opinions and arguments.
@_date: 2015-09-27 22:40:58
Sync from scratch. 
@_date: 2015-09-22 05:46:24
I hate to agree with a Buttcoiner, but he's right. Pruned nodes cannot serve SPV wallets, since they don't have the full transaction history for any given address. They don't store blocks, so they can't serve blocks to other full nodes. They can't replay blocks, so if there's a reorganization longer than a few blocks (a fork of some sort), they have to start over from scratch. Currently, you can't even run a wallet off of them. The only thing they can do is verify new transactions and new blocks, and mine. 
@_date: 2015-09-07 08:55:23


Pool operators absolutely should run a full node. If they don't, then they can't include transactions in their blocks, and they lose out (currently) on about 1% of the available revenue. As block sizes increase and block subsidies decrease, that 1% will grow dramatically. For example, with 100 MB blocks and the current average fee of 8Â¢/kB, fees would be about $8k or 33 BTC per block. Miners can and will change pools over a 1% difference in revenue, much less a 2.5x difference.
However, bandwidth costs per Mbit/s and per GB are much lower for pool operators, because they can run their servers in a datacenter in a major city near large bandwidth hubs, where bandwidth is much cheaper than in the remote farms near hydroelectric dams that miners prefer. On DigitalOcean's VPS, for example, I can get 2 TB/month at 1 Gbps peak for $10/month, which is about 10x the bandwidth that I get for 1/10th the price I pay.
Many pools (including the big Chinese ones) use a hybrid system that combines SPV (header-only) mining with full-node mining. Immediately after they receive a block header, they start mining a new block on top of it but without any transactions included. In parallel, they download the transaction data for the block. Once the transaction data has been downloaded, they can switch to full-node mining and include transactions in the block they're trying to mine on top of it. This way, they get the best of both worlds: they have low orphan rates due to the SPV mining as soon as a block header is available, and they have the transaction fees once the full block has been downloaded. However, everyone has to deal with the bugs if they mine on an invalid chain, like with the BIP66 fork. The main cost of SPV mining is borne by the non-mining users of Bitcoin, which means that SPV mining is, in my opinion, evil.
In the future, once the block subsidy drops a few times and mining fees increase, SPV mining will likely become obsolete. The revenue from finding an empty block will not be enough to pay for the electricity used to mine it, so miners will idle their machines until they accumulate enough transactions to pay for the expected electricity cost of mining a block.
@_date: 2015-09-07 02:56:41


There is already a good amount of data showing that this is the case. Orphaned blocks are larger on average than non-orphaned blocks. This is because a &gt; 900 kB block takes about 16 seconds to reach 3000 nodes; a &lt; 100 kB block only takes about 4 seconds. 
Extrapolating from this, an 8 MB block would take about 130 seconds to reach 3000 nodes. That would correspond to up to a 20% orphan rate. I consider that significant.
Peter R did some math on this, and showed that block orphan rates are a sufficient incentive to create a fee market.
@_date: 2015-09-24 21:37:26
We have no idea. Spondoolies has not announced that, nor have they yet replied to my request for information.
@_date: 2015-09-12 03:49:20
If you have 1000 nodes on the network, and each node removes 50% of transactions at random, there is a 0.5^1000 chance it will be forgotten forever. That's about 10^(-301). If each node removes 99% of transactions at peak mempool conditions (i.e. total distributed transaction set is 100x allowable mempool size), that makes a 0.004% chance for each transaction to be forgotten. This is a first order approximation. In reality, performance would be worse, since transactions would tend to continue circulating through the network for a while after some of the nodes had forgotten them, causing more transactions to be forgotten. Still, I think it illustrates how few transactions will be forgotten under typical conditions.
If each node forgets the lowest-fee transactions, and all nodes have the same maximum mempool size, then a transaction that is forgotten by one node will be forgotten by all nodes.
@_date: 2015-09-24 02:27:42
Toomim Bros ( may be organizing a group buy of these items. Email orders if you're interested. We may be able to use the load balance option in cgminer to allow you to buy fractions of an SP50.
@_date: 2015-09-30 03:03:08
Nope. 
Industrial miner here. Once you plug a machine in, you almost never unplug it. It's just statistical noise.
@_date: 2015-09-17 05:12:12


I totally agree with you, and that's why we have been mining with p2pool since we started  Unfortunately, p2pool has a lot of performance problems, and will not be viable with large blocks unless a lot of optimization work is done. Currently, the author/maintainer of p2pool is not very active on the project. p2pool is also not viable with the most common types of mining hardware due to bugs.
@_date: 2015-09-17 05:18:17
Right now, share variance is only the main source of variance on p2pool for miners with less than about 3 TH/s. That corresponds to about $1,000 of hardware. 
Variance just means unpredictability. It does not mean unprofitability. If you only have $1,000 of miners, then Bitcoin mining is a hobby for you, not an occupation. If it's not your occupation, then you probably have another job, and you probably don't need your miners to turn a profit every single week. 
@_date: 2015-09-22 19:11:56
If the bitcoin price rises, then the value of any bitcoin you bought right now would also rise. If the bitcoin price rises, then difficulty will rise faster as well. You will still be better off having bought bitcoin.
@_date: 2015-09-07 03:22:46
No, OP is correct. The Lightning Network could operate on any cryptocurrency. It hasn't been implemented for any altcoins yet, but it hasn't been implemented for Bitcoin either.
However, the Lightning Network will be much more useful as an extension to Bitcoin than to other currencies due to the fact that the economic majority uses Bitcoin.
@_date: 2015-09-06 23:01:18




No, he's saying that large blocks will often get orphaned, so miners will elect to not create excessively large blocks out of self-interest. No altruism implied.
@_date: 2015-09-07 23:48:24
Transaction timestamps are not mandatory and not reliable. They are not used for anything important. People misconfigure their clocks all the time; nobody should assume that another computer's clock is set correctly. There is nothing to attack. If anyone builds an app that requires transaction timestamp data for something, and doesn't build a network of monitoring nodes on VPSs in order to measure those timestamps, then their app deserves to fail.
The only timestamp that matters to Bitcoin as a protocol is the block height at which the transaction was confirmed. Anything else is just there for statistics and curiosity's sake.
@_date: 2018-07-22 06:04:10
The top 10 coins have a combined market cap of $232 billion, which is 82.6% of the total cryptocurrency market cap of $282 billion. The top 100 have a combined market cap of $270 billion, or 95.7% of the total. Getting rid of the long tail of junk altcoins does not substantially affect market cap dominance charts, as the market cap of minor junk altcoins is tiny.
@_date: 2015-09-30 02:56:43


Oops, you're right. But there's still the overhead from transactions being sent twice unless the relay network or IBLTs are used.
@_date: 2015-09-17 05:40:44
If you view the share associated with the block on a p2pool node (any will do), it will show the bitcoin address associated with the block. This block was mined by the p2pool user with the address 1GuDnEyYSE3Ra3pMar7311tx5poR5PGXR3.
@_date: 2015-09-07 09:24:44


Not exactly, no. We use p2pool, which has a different set of performance problems than traditional mining. Block propagation times should be pretty good for p2pool due to the O(1) algorithm that p2pool uses, but GBT latencies are an issue. 


Yes, we noticed significant performance problems during the stress tests when the mempool got out of hand. This is one of the reasons why I strongly support large blocks -- mempool inflation appears to be about as effective an attack vector as mining large blocks is purported to be, but a lot cheaper for the attacker. I found minrelaytxfee=0.0001 to be an effective protection for the previous stress tests, but that only works if the congestion is due to low-fee transactions.
@_date: 2015-09-07 08:38:08
True. Hopefully, in the semi-near future mining fees will become more important, and then full verification will have the advantage over SPV mining of transaction fees.
I think the main problem with SPV mining is when miners don't have a slow full-verification code path implemented. If one pool mines a block on top of an invalid block, it sucks to be them. If a bunch of other pools follow and mine a chain off of it (like we had on BIP66), then it's an annoyance for everybody. It's not too complicated to fix--you just have to not continue to mine on invalid chains after your full node has caught up and reported the chain as invalid.  
@_date: 2015-09-22 06:19:08
It's stuffed full of profit. For 21.co.
@_date: 2015-09-17 06:39:25
No, ASICs are not the problem. Pooled mining is the problem. With pooled mining, individual miners band together in order to distribute risk and make their revenue more predictable. 
However, the way most pools are implemented, the operator of the pool server has complete control over the content of the blocks, and can use the miners who are on the pool in order to perform 51% attacks or selfish mining attacks.
With p2pool, each miner (or really, each p2pool "node", which is almost equivalent) creates their own blocks and independently chooses the content of the blocks, including which transactions to include and which consensus rules to follow. However, the revenue is shared. The mechanism by which this is achieved is pretty cool, technically. It's basically a blockchain on top of a blockchain. 
If every miner used p2pool, we would not have a problem with mining centralization or 51% attacks at all. Even if p2pool were 100% of Bitcoin mining, it wouldn't be able to do a 51% attack. Unfortunately, only 0.5% of miners use p2pool.
@_date: 2015-09-22 19:35:36
21 claims the ability to do micropayments. The reason the payments will be micro is because most of the users' money went to pay for the hardware. You won't be making a lot of money on 21's micropayment network if the main source of satoshis on the network is people mining with 21's hardware.
@_date: 2015-09-23 01:09:02


This is incorrect. The bitcoin are stored in a private key somewhere. Whoever has that private key controls the bitcoin, and controls the proof-of-identity associated with it. If someone sends bitcoin to your address, then sending money out of that address proves your identity. Of course, so does signing a message using that bitcoin address, or using a PGP key.
@_date: 2018-07-23 19:27:59
The Innosilicon model gives about 17% more hashrate than the S9i for the same amount of electricity, but about 34% less hashrate for the same initial capital investment. At $0.05/kWh, it would take 32.7 months of operation before the Innosilicon model was a better deal than the S9i.
@_date: 2015-09-22 19:08:43
A reflection attack with trolling amplification.
@_date: 2015-09-21 22:06:32
The choice of the 128 GB SD card is strange. It indicates that they are expecting to run a full node on the Pi. It also indicates that they are not expecting the block size to increase dramatically (otherwise it would fill 128 GB quickly). And yet they are indicating it as a micropayments server. This does suggest that they have some sort of blockchain-as-a-settlement-layer game going on. Well, either that or they just can't do the math, which I'm not excluding as a possibility yet.
@_date: 2015-09-27 20:32:31
It's repeatable. It also gets much worse if I try to have multiple wallets syncing simultaneously. I have one wallet that typically syncs in about 4 hours, and another that takes about 40 hours. If I try to have both open at once, it takes about 100 hours. If I try to add a third wallet, it takes longer than I've ever had the patience for. It does not seem to scale linearly with the number of addresses, as I would expect.
@_date: 2015-09-14 03:35:18
No. 20 MB was not a math error. It simply didn't have support from the big pools in China. DANKEST_KHAN is correct. 
All of the simulations and calculations Gavin has done has shown 20 MB to be feasible today with current technology. 
@_date: 2015-09-17 02:43:25
This block was mined with BitcoinXT. I should know, since my company mined it.
@_date: 2015-09-17 05:36:55
tombroth? Close. Our actual name is Toomim Bros Bitcoin Mining Concern. 
@_date: 2015-09-30 01:35:41


Currently, about $8/month.  That gives you a VPS with a lot more performance than you actually need for a full node. If you also use that VPS for other tasks, then you might be able to get the effective cost down to about $3/month.
On the other hand, it costs about $40/month to run a 1 TH/s miner like an Antminer S5. The total mining network consists of as much hashpower as 400,000 Antminer S5s, but only about 5,000 full nodes.
@_date: 2015-09-21 22:02:19
Actually, mining revenue *is* their business model. However, mining revenue from this device is not.
21 raised millions of dollars of VC funding to build an IoT device, because they knew IoT is what gets VCs excited. They then developed an ASIC which could be used in their IoT device and could also be used in their mining farms. They deployed this ASIC in their farms, and now they're selling this device. This device will flop, as there is no market for it. 21 will then pivot a little into a more pure mining company, and may actually turn a profit.
@_date: 2015-09-08 07:56:06
And that's why I upvoted you. I just thought that both you and luke could have expressed the concern more clearly or completely, so I tried to help.
@_date: 2015-09-17 06:26:33


Yup. That's true for all pools, though.
"Workload" is a misleading concept with mining. There isn't a set number of hashes that have to be done in order to find a block. Really, all you're doing is rolling a random number over and over again until you roll the right number. It doesn't really matter if you coordinate about what numbers to roll as long as you are all rolling different numbers. The input vector for the double SHA256 hash has enough bits of entropy to be able to do this with an insignificant probability of accidental collisions (two miners performing the same hash).
@_date: 2015-09-20 07:25:40
New clients sync a lot faster due to headers-first syncing.
Release notes for the version that introduced the feature: 
I'd strongly suggest upgrading to the latest version of either Core (0.11.0) or XT (0.11B).
@_date: 2015-09-22 21:02:26
They don't need to mine in order to provide liquidity. They choose to mine because that is the only portion of their business that will be profitable. The micropayments platform that they pitched to VCs is really just an excuse to build enormous datacenters with their own ASICs. That's how I read this.
@_date: 2015-09-30 02:57:34
Statistical error in hashrate estimation. These graphs are generated based on the amount of time taken to find some fixed number of blocks. That's a stochastic process, so the graphs are noisy. The amount of noise is inversely proportional to the square root of the number of blocks used to make the estimate. If you look at the graphs on  you can see the effects of different numbers of blocks for the window on the variance of the estimates. Anything below around 2016 blocks will have more apparent variation due to noise than due to changes in the mining network nowadays.
@_date: 2015-09-22 22:43:05


No, I've said the same thing many times before. Here's one instance in the same thread. 
Here's a post I made with the same thesis on May 20th on bitcointalk.


I don't see how having a continuous stream of bitcoin following a geometric progression that would produce about 0.25 BTC over eternity would be better than pre-filling a wallet with 1.5 BTC. Making it continuous payback doesn't make it inherently better. If it were, people would frequently take investments with negative interest rates that paid back the principal (minus interest) over time.
In any case, how does Coinbase produce a continuous stream of Bitcoin to meet their orders? They can either make a continuous stream of purchases of Bitcoin, or they can purchase it all at once and make a continuous stream of deposits.
@_date: 2015-09-06 22:48:08


You shouldn't store private keys on a VPS. Don't run a wallet on a VPS. The VPS hoster could see the private keys decrypted in memory or install a keylogger trojan into sshd. Looking at the other use cases for a full node, I don't see any opportunities for the VPS host to perform an attack that would have any utility to the host.
If you want to run a wallet tied to a full node that you control, you should do something like using a VPS to run a full node that runs your own Electrum server. The full node's purpose is to keep an exhaustive record of bitcoin transactions; creating and signing transactions for your wallet can be done elsewhere, like on a Trezor, an airgapped offline Armory netbook, or Mycelium running on your cell phone. 
In this sort of configuration, the VPS host company could manipulate your full node to give incorrect transaction history for your addresses. If they did this, they would not be able to steal your funds. By giving you incorrect information, they could prevent you from creating valid transactions by telling you incorrect information about the inputs for your bitcoin transactions, but they can't trick you into signing a valid transaction with different outputs than you want. That is, they can deny you access to your funds as long as you use your compromised full node. This attack would be easy to detect and bypass by simply checking your balance with one or two other (e.g.) Electrum servers or full nodes operated by random strangers using different VPS providers. 
If you just run a full node as a contribution to the decentralization of the bitcoin full node network, then compromising your full node could be used for a Sybil attack. However, Sybil attacks are difficult to perform, since you usually need much more than 50% of all the full nodes under your control in order to conduct them effectively, and a VPS hosting company would be better off creating hundreds or thousands of competing full nodes for an attack rather than trying to compromise yours.


I think you mean on-chain. Very few transactions occur off-chain right now. Generally, the only transactions that occur off-chain are transactions within a single exchange or web wallet provider, like Coinbase-to-Coinbase transactions or 


Fees are currently not influenced by limited block space. If we increase block sizes, fees will remain about the same. If we do not increase block sizes, then blocks will get congested, and latency will increase for transactions without larger fees. So with larger blocks, capacity increases, and we don't deny people access to inexpensive on-chain transactions.
The transactions that are currently done off-chain will stay off-chain because the main incentive for these transactions being off-chain (e.g. in exchanges) is instantaneousness, and that will not be changed by the blocksize cap.


No. Bitcoin block sizes have not been a limitation to growth rates in the past. For the vast majority of bitcoin's history, the block size cap has been 10x to 1000x the average block sizes. Blocks have generally only been full for about two weeks during a few stress tests in the last few months. The network swelling rate has been limited only by demand so far. Average block sizes are currently about 450 kB with a 1 MB cap, which means that if we don't increase the block size cap, we will see something new: chronically congested blocks and a limitation on bitcoin growth by the block size cap. 
You're arguing that increasing the block size will cause off-chain transactions to move on-chain. This is factually incorrect, as there are currently very few off-chain transactions. The more common and more valid argument is that letting blocks get congested will motivate people to move on-chain transactions off-chain. However, there's a problem: there are no working off-chain trustless decentralized systems for people to move to. The options for off-chain transactions are a few systems that don't yet exist (Lightning Network and sidechains), and a bunch of entities like Coinbase that require you to trust them. None of these options are good. In a few years, that might be different.
How many bitcoin full nodes do you think bitcoin needs to be "decentralized"? In my opinion, bitcoin full-node decentralization is the least important type of decentralization. Non-mining bitcoin full nodes don't really do much. Miner decentralization matters, pool decentralization matters, and developer decentralization matters. The only serious concern that having a large number of full nodes protects against is Sybil attacks, and I think there is not a large risk of those being practical or very useful. Another thing about Sybil attacks: If the number of full nodes decreases because the cost of running a full node is higher, then the cost of running full nodes for a Sybil attack will also be high. 
@_date: 2018-04-03 14:17:49
The minimum allowed payment amount is 546 satoshis for a legacy tx or 298 sat for a segwit tx. This is the point at which spending the payment would cost 1/3 of the payment amount if you pay the minrelaytxfee.
The default dust threshold is 3 satoshis/byte, or 3x the default minrelaytxfee setting. From the current version of the source:
    CAmount GetDustThreshold(const CTxOut&amp; txout, const CFeeRate&amp; dustRelayFeeIn)
    {
        // "Dust" is defined in terms of dustRelayFee,
        // which has units satoshis-per-kilobyte.
        // If you'd pay more in fees than the value of the output
        // to spend something, then we consider it dust.
        // A typical spendable non-segwit txout is 34 bytes big, and will
        // need a CTxIn of at least 148 bytes to spend:
        // so dust is a spendable txout less than
        // 182*dustRelayFee/1000 (in satoshis).
        // 546 satoshis at the default rate of 3000 sat/kB.
        // A typical spendable segwit txout is 31 bytes big, and will
        // need a CTxIn of at least 67 bytes to spend:
        // so dust is a spendable txout less than
        // 98*dustRelayFee/1000 (in satoshis).
        // 294 satoshis at the default rate of 3000 sat/kB.
    
Older versions of the code had a comment that stated the 3x ratio more clearly:
        // "Dust" is defined in terms of CTransaction::minRelayTxFee,
        // which has units satoshis-per-kilobyte.
        // If you'd pay more than 1/3 in fees
        // to spend something, then we consider it dust.
        // A typical txout is 34 bytes big, and will
        // need a CTxIn of at least 148 bytes to spend:
        // so dust is a txout less than 546 satoshis 
        // with default minRelayTxFee.
@_date: 2015-09-01 02:55:17
Hashingspace's goal is to pump their share value up as much as possible with frequent news articles like this one. They don't actually have much in the way of physical operations, at least at the moment.
@_date: 2015-09-21 21:02:25
If you buy this, you will lose money. Assuming their chip is at the top of their stated 50-125 GH/s range, 
1. You will spend $400 today.
2. You will get back no more than $1.6 per week for the next year, or $83 in bitcoin.
3. You will get back no more than $0.80 per week for the four years after that.
This ignores mining difficulty increases, electricity cost, and exchange rate changes. Mining difficulty will increase a lot. I would be surprised if you got $50 in the next year after taking difficulty increases into account. (I run a 750 kW mining company.)
21.co cited 0.16 J/GH for their mining chip. At 125 GH/s, that would be 24 W. This is probably the chip-level power consumption, not the system-level power consumption. I'd guess that with AC/DC and DC/DC conversion losses and the RPi's power consumption, the total system power consumption is likely about 35 W. At 24 W and $0.10/kWh, this device would use about $0.40 per week. That's actually pretty good. However, the huge up-front cost makes this a big loss.
@_date: 2015-09-27 18:53:47
It takes my laptop one or two days to fully synchronize a wallet with 500 addresses.
@_date: 2015-09-07 04:44:51
No, it doesn't say much. OP's point on LN with altcoins is a true point, but it is only tangentially related to the blocksize debate.
@_date: 2015-09-22 05:42:19


I did the math. The math came out negative. 
Effectively, 21's IoT device is charging a large (~300%) premium on bitcoin for the "benefit" of delaying your returns on investment. You could build a device that is just an RPi and 4 GB SD card that would emulate the behavior of 21's computer, except that the source of the revenue would come from a pre-loaded wallet instead of mining, and you'd get more out of the device. Essentially, 21's machine is like a gift card where you pay $400 for the card and only get about $80 out of it.


I hope not. Most consumers do not have access to inexpensive, low-CO2 electricity. I think it's better for Bitcoin mining to be concentrated in regions with plentiful and inexpensive renewable energy, like Iceland, Washington state, northern Sweden, and south Sichuan province. Trying to build a device that distributes mining among consumers is a losing strategy both for the environment and for the consumers' wallets.


No, they deployed their own farms because they needed cash flow. If you need liquidity, the best thing to do is not spend any money and to sell existing assets, not to buy more assets.
@_date: 2015-09-17 05:35:23


I think you are missing something about how mining works. The code run by full nodes follows the rule that the block chain with the most mining proof-of-work behind it that is still valid is the consensus block chain. Consensus over the state of the Bitcoin ledger is achieved through mining. 
@_date: 2016-10-11 15:01:27
I'm not digging in my heels. I'm migrating away from Bitcoin. I don't really care what you guys do with Bitcoin any longer.
Someone asked to hear a perspective on why ViaBTC might want to avoid SegWit, so I gave that perspective. That is all.
@_date: 2015-09-17 06:09:47


That's how most pools work, but not p2pool. With p2pool, each miner is creating a different block. The only thing that has to be the same is the coinbase has to reward the other p2pool nodes, and some other p2pool metadata has to be stored in the block. Each p2pool node has to mine according to the p2pool consensus rules, very much like how each Bitcoin node has to mine according to the Bitcoin consensus rules.
@_date: 2016-10-23 13:21:45
No, the SP20 is obsolete hardware and very inefficient (0.5 to 0.77 J/GH). You will spend more on electricity than you will make in revenue unless your electricity costs less than 5.2Â¢/kWh. Even then, you will probably never make your original investment back.
If you have cheap electricity, you might be able to make a profit with an Antminer S7 or Avalon 6 (both of which, at around 0.28 J/GH, are around 2x as efficient as an SP20).
If you don't have cheap electricity, your only reasonable options are the Antminer S9 or R4 ( --  0.1 J/GH). Both are currently out of stock.
@_date: 2018-04-11 20:38:46


Quadratically, not exponentially. During each clock cycle, you have to discharge and recharge the capacitors that form the gates of the transistors. The amount of charge that each capacitor requires is proportional to the voltage. The energy required by each capacitor for each charge is proportional to the charge times the voltage, which is equivalent to voltage squared. If you're increasing the clockspeed as well as voltage, then you get voltage cubed.
Exponentially means something like 2^x. Quadratic means something like x^2. There's a big difference between the two.
@_date: 2018-04-11 16:40:01


This is not accurate. When running with a pool that does not support ASICBoost, the DragonMint still mines at 16 TH/s, but 75% of the hashes that it performs use a block version that the pool is not expecting, so the pool only sees 4 TH/s of valid hashrate.
@_date: 2015-09-23 00:56:51


I don't have anything against them. I have something against their claimed business plan. I think it is either incomplete,  misleading, or stupid. Either they aren't telling us something about how their system works that would actually give their product positive value (probably about how they plan to implement micropayments), or they are tricking VCs into investing in a business that is essentially just mining, or they made a mistake somewhere.
You seem to think that this is a personal vendetta that I have. I don't get those. I'm the type of person who develops mathematical vendettas. For example, I think that fuel-cell cars are terrible, and corn-based ethanol for biofuel-powered cars is also terrible. My grounds for those positions are mathematical. Fuel cell cycles are about 20-30% efficient, compared to about 85% for battery electric vehicles. Corn based ethanol would require all of the USA's agricultural production to replace gasoline. The goals of these projects are excellent goals, and they have some good qualitative ideas, but they are quantitatively untenable. For fuel cells and ethanol, the source of the quantitative shortfalls are inviolate thermodynamic limits. In 21's case, it's more stuff like economies of scale, overhead percentage, AC/DC conversion efficiency issues, but the effect is the same.
21's published idea of a miner in every home is environmentally wasteful and economically parasitic. It redirects about 35% of the money that would go into the micropayment economy into 21's pockets as a premium for the hardware, and 45% of the remainder is wasted due to inherent inefficiency in the implementation. I don't see how they are adding value proportional to the price they're charging. I don't see how their micropayment platform could warrant more than about a 10% fee, much less an 80% fee.
@_date: 2018-04-11 18:54:54
ASICBoost allows you to reduce the total amount of computation needed to do a double SHA256 hash by reusing some of the early computations in the SHA256.
Each computation on an ASIC is a group of transistors. If you want to take advantage of an algorithmic improvement that reduces the amount of computation you need to do, you have two options:
1. Include the transistors for the no-longer-needed computations, but don't switch them on when the operating mode makes them unnecessary.
2. Don't include the transistors, thereby making each SHA256 engine smaller and allowing you to fit more engines on the same chip.
Bitmain chose option  HalongMining chose option 
@_date: 2015-09-08 06:00:38
If an opinion is controversial, relevant, and unpopular, what typically happens on reddit is that the replies and rebuttals to the comment get upvoted, which prevents the unpopular parent from being hidden. It's not a perfect system, but it's fairly functional.
@_date: 2015-09-08 05:10:40
I disagree, but I admit to not having much data beyond what Gavin has presented in order to argue so. Ultimately, it's an empirical question. Perhaps we just need to test this? If so, and if enough others are willing to support such a test, I could throw in some of my datacenter's resources for a test.
@_date: 2018-04-11 17:02:53
As I understand the way it works, it cannot. However, I do not have one, so I could be wrong.
@_date: 2018-04-11 16:16:46
It means that HalongMining has partnered with a manufacturer who has a more modern manufacturing process, and that their current advantage in performance and efficiency is most likely due to manufacturing technology rather than design quality. Once Bitmain's manufacturer (TSMC) catches up, this performance gap should disappear or reverse.
@_date: 2016-10-19 19:15:32
You don't mean megawatts per hour. You just mean megawatts. One megawatt already means one megajoule per second.
@_date: 2016-10-11 06:10:23
**[This might be part of why.](
It's also worth noting that 1.8 MB blocks only come when 100% of users have switched wallet software over to something that supports SegWit. In my estimation, it will take about 1 year before 50% of users do so, which would mean that after 1 year SegWit would be a 1.4 MB effective block size. One month after activation, I expect the effective block size to be pretty close to 1.1 MB. I consider that sort of effective block size increase to be ineffective. Perhaps ViaBTC does too.
@_date: 2015-11-09 04:13:31
There are politics involved, yes. One of my social/political motivations in running this test is to get people involved in running tests. I think that talking about hypotheticals is less useful than building it and measuring it. 
(I didn't expect to get such a strong response from people on this, so I'm a little overwhelmed now, but I guess I should have.)
@_date: 2015-09-17 05:25:18


 
No, you are incorrect. This p2pool has a copy of the share with which this block was mined, but getbashed.org did not find that share or that block. Every p2pool node has every p2pool share, just like how every full node has every block ever mined. If you look closer at that node, you'll notice that the 1GuDnEyYSE3Ra3pMar7311tx5poR5PGXR3 user/address that mined this share and this block is not mining on  at all. That user mines on my p2pool node.
@_date: 2018-04-14 22:18:25
You should thank Sergio Demian Lerner (SDL) for his blog post. I just linked to it.
No, I don't know of any miners who avoid mining Segwit transactions.
@_date: 2015-11-09 02:12:33
It has been tested by Gavin extensively in regtest mode. It has not been tested on testnet yet because, to be honest, testnet is a mess. This test is going to be messy. Performance will be much worse than in regtest mode, and also much worse than on mainnet. What I'm hoping to achieve in this test is **correct** behavior and graceful degradation in the face of adverse conditions.
We may set up a new testnet4 just for BIP101 performance testing (current one is testnet3, shared with Core, and dominated with non-BIP101 nodes), or maybe set up something more permanent/repeatable/public with regtest for replicating Gavin's performance testing. However, accurate performance testing is a lot harder to do than the correctness testing that we're doing now, and will take days or weeks to set up, rather than the hours that have been spent on this testnet test.
@_date: 2018-04-11 19:01:34
SegWit eliminates one ([c2]( of several forms of covert ASICBoost. It does not prevent the other covert methods nor overt ASICBoost.
@_date: 2015-11-06 21:27:26




Yes, I know that people have made that point before. That does not mean it's valid.
The point behind large blocksizes reducing the number of full nodes isn't that having a lot of full nodes somehow makes the network secure. The point is that if full node count decreases, that means that more people are choosing not to run their own full node and therefore are using SPV wallets.
It's true that SPV wallets are not as secure as fully verifying wallets. One of the common points of disagreement between small-block proponents (e.g. Peter Todd and luke-jr) and large-block proponents (e.g. Gavin, Hearn, me) is that small-block proponents think that SPV wallets are too unsafe for most people to use most of the time, whereas large-block proponents think that SPV wallets are imperfect, but safe enough for most users.
(The only exception to the idea that the full node count is unimportant is with Sybil attacks, where an attacker tries to manipulate the network by getting a monopoly on the distribution of information. In order to protect against a Sybil attack, all you need to do is connect to at least one full node that is part of the non-Sybil network. The more full nodes that exist, the more expensive and error-prone Sybil attacks become.)
P.S.: I couldn't figure out what you were referring to with that link. I didn't see any references to full node counts. I didn't read all 26 pages of that thread, though, so perhaps it's in a different post than the one you linked to.
@_date: 2016-10-19 19:14:34
Try redoing your calculation with hashrate rentals from websites like nicehash.com.
@_date: 2015-09-22 19:46:12


Seriously. It costs a lot less than $10 million to register as an MSB, but it cost 21 about $10 million to produce their custom ASIC.


That doesn't make the transactions free. It just makes the cost not represented as a fee. The transactions still take up block space. They still have to be mined. This just means that 21 will be subsidizing those transactions by not including other fee-paying transactions in the blockspace that they're using with their coinbase tx.


I've done both ways. One reason why I bother with mining is that it's a fun hobby. Another reason is that I want to support decentralization. Another is that I want to make mining more environmentally friendly by bringing as much mining as I can to the hydroelectric-powered Pacific Northwest. And lastly, when you buy hardware with 50 or more mining ASICs per control board, they get a lot more capital efficient and start to have a chance of actually making a profit. 21's device does none of those things, especially not decentralization (since it requires 21's pool).
@_date: 2015-09-08 07:54:31
It's not hard to use a reasonable timestamp. It's hard to enforce that the timestamp is reasonable. Thus, timestamps are there only for statistics and curiosity, not for anything where actual money is at stake.
@_date: 2015-09-17 20:59:46


That's almost true. In actuality, we've mined a few blocks with XT before this (we started around August 8th), but we didn't notice that with p2pool the block header and version is determined by p2pool, not by the version of bitcoind that you're running. So there should have been earlier blocks that followed the BitcoinXT consensus rules but were marked as version 3, and there were other blocks that followed the Core consensus rules but were marked as version 0x20000007 (is that the right number of zeroes? uint32, right?), but this is the first block that both follows BitcoinXT consensus rules and is marked as version 0x20000007.
@_date: 2015-11-26 21:58:42


BIP101 partially addresses the O( n^2 ) validation cost issue by limiting the bytes hashed with 8 MB blocks to the same amount possible with 1 MB blocks. It is still desirable to reduce the allowable bytes hashed in a separate soft fork; Gavin just decided it to be preferable to not combine a hard and soft fork into a single patch.
@_date: 2015-11-11 01:55:46
The block explorers have gotten all sorts of confused. They're not reliable because they seem to not pay attention to the chain-with-most-work rule.
While we were mining on BIP101, we saw the Core chain quickly pass us up with about 2,000 blocks mined in rapid succession with difficulty around 1.6. I got rather confused by that, but even though Core was over 1000 blocks ahead of XT, all of the XT nodes stayed together.
We had some difficulty goofs of our own, too. At some point, we crossed a difficulty adjustment border. A few other people mined some blocks when I was otherwise busy, and then when I was ready to mine, I didn't know the difficulty was different. I pointed a half SP10 at my poolserver, and then mined at up to 100 blocks per second. Many of the blocks arrived at other nodes in reverse order. It was interesting.
Later, I got the CPU mining thing worked out, and we mined about 6 full 9.1 MB blocks in a row. We had a reorg in the middle of it, though, so I think we were having some connectivity problems from insufficient node coverage. One of my nodes also wasn't following the rest for a bit.
When it functions properly, it seems that block propagation for 8 MB blocks may be around the 5s to 15s mark. It appears that a lot of that time is due to block validation; the network transit time appears to be pretty low in our configuration. TCP slow start does not appear to be as much of an issue as I was expecting.
I'm currently not mining on XT because I found that leaving it for CPU mining when possible was more convenient.
Edit: I'll post a transcript from  of today's testing on in a bit.
@_date: 2015-11-15 21:54:58
An Antminer U3 is cheap, around $30. It should earn about $1 per week right now at 60 GH/s. It will use a lot of power (about 1 J/GH), so you'll spend about as much on your power bill as you make in revenue.
The GekkoScience/Sidehack USB miner is a device designed and made by an enthusiast. It is a lot more efficient (about 0.3 J/GH) and has a lower hashrate (8-16 GH/s), so its power costs will be very small. Revenue per week is expected at $0.13 to $0.26. It costs about $40.
The Antminer S5 costs about $400 on the used market. You'll also need a power supply for it, which might cost another $80. This has a hashrate of 1152 GH/s, and an efficiency of 0.52 J/GH. It would produce about $17 per week at the moment, of which you would spend about half on power if your electricity costs $0.10/kWh.
The Antminer S7 costs about $1600. It has a hashrate of 4000 to 4800 GH/s, depending on the batch. It has an efficiency of 0.25 J/GH, which allows it to be profitable even with more expensive power.
@_date: 2015-11-06 22:35:56
Stale block ("orphan") risk is roughly equal to (propagation time / block time). If you reduce the block time, you get 2x as many blocks, but you get 2x the stale rate. If you increase the block size, you get 2x the stale rate. The two approaches are mostly equivalent in terms of how they affect stale rates.
The main difference is that with fast or O(1) block propagation methods, the propagation latency is not dependent on the block size. Once you have this in place, larger blocks at low frequency performs better in terms of stale blocks.
@_date: 2018-04-11 16:42:16
No. Without pool-side ASICBoost support, 75% of the hashes that DragonMint performs use a block version that the poolserver is not expecting, so only 25% of the hashes are considered valid.
@_date: 2015-11-09 12:52:47
Don't spend it all in one place. At least, not all at once.
@_date: 2015-11-09 02:05:06
I'm the one who started this. I did the mining to activate it.
**This is not a performance test.** This is intended as a quick test to ensure that the forking mechanism works, and to check for any unanticipated behavior in chaotic conditions. Currently, I'm guessing BIP101 nodes comprise less than 10% of testnet's network, which means they should have a lot of trouble propagating blocks to each other. It will be interesting to see what happens when BIP101 comprises a hashrate majority but a full node minority.
Performance of this network will be terrible. There's no relay network, and most of the servers that people are going to be poorly configured, medium-low powered, and not actively maintained. We'll probably do some block propagation tests (because it's one of the most important questions with big blocks), but we are not expecting the results of this test to be any way representative of actual network performance on mainnet.
Performance tests are better done with a separate testnet with &gt; 90% BIP101 nodes, or in regtest mode, rather than on testnet.
@_date: 2018-04-11 16:26:54
The Antminer S3 had an efficiency of 0.77 J/GH on a 28 nm process. 
The Antminer S7 had an efficiency of 0.25 J/GH on a 28 nm process.
Manufacturing process matters, but it's not the most important factor. ASICBOOST gives a 20% to 30% boost in efficiency, but you can get larger benefits from good layout and design, low threshold voltage transistors, and all of the other trade secrets that a good design firm with a few years' experience with SHA256d design would have.
HalongMining was able to get their first product to be slightly superior in efficiency to Bitmain's 5th-generation product. This is somewhat impressive. However, they did so using a manufacturing process that is about 2 years newer than Bitmain's 5th-gen product, and that is likely responsible for most of the delta.
@_date: 2015-11-09 03:14:17
Biggest, yes. Best, no. It's the test that is most likely to fail miserably because testnet is a mess. Gavin's performance tests on 20 MB blocks were much more accurate than this test will be in terms of reflecting optimized conditions. Gavin's tests may have a bias toward overreporting performance, and this test will have a much larger bias towards underreporting performance.
The way I view this test is as a way to get people involved in testing. It is not a conclusive determination of the capabilities of the network to handle large blocks.
@_date: 2015-11-09 04:20:57
I think it might be possible to make those changes on testnet just by adjusting your clock. If you fast forward your system clock by 20 minutes, the mining difficulty on testnet drops to 1, which means even a CPU can find a block in an instant. Do this repeatedly, and you can get to any block height you want. It's a time machine to 2036.
Once your clock is more than 2 hours out of sync with other nodes, you will no longer be able to communicate with them, though.
@_date: 2015-11-09 02:09:03


These are not real-world conditions. They are much much worse than real-world conditions. This is a poorly maintained network, poorly configured, with a lot of firewalled ports. Performance is going to be crap.
Should be fun.
@_date: 2015-11-07 22:08:32


And I don't like the idea of the mining network requiring about 300 MW of power just to support an average of 1.6 KB/s of financial transaction data.
I've [done the numbers]( for us, and my mining company (which has 0.2% of the network hashrate) should be able to make a profit just fine in a world with 8 GB blocks while operating our own full node. Companies about 4x smaller than us might have trouble affording a full node, so they might have to use someone else's pool.
@_date: 2015-11-09 04:11:42
A few things.
1. I wanted to make sure the forking code works as expected in chaotic, unfavorable settings, even if the hashrate majority has a node minority, as is the case on testnet3 right now.
2. I wanted to test out some new CreateNewBlock code on 8 MB blocks. Usually, I would do this in regtest mode, but I kinda felt like doing it the hard way this time.
3. Eventually, I want to do some block propagation time tests using a testnet, using something vaguely resembling the actual mainnet network. Those tests will take more work to set up and make meaningful, though, as the mainnet has a lot of work that's been done on optimizing block propagation, and testnet does not. We'll probably switch to a different genesis block for those tests.
@_date: 2015-11-09 03:47:46
Block explorers would be nice. I emailed blocktrail about this, because I like their interface the best of what I've seen. I have not emailed anyone else. If other people want to help, emailing others would be a good way to do so. Here's the message I sent to blocktrail which you can edit:






Everything you can get from a block explorer you can also get from the command line, though not as easily. Without the support of block explorers, we will still be able to learn some stuff. 
@_date: 2015-11-09 11:29:11


Now would be a good time to start ramping up the spam.
@_date: 2015-11-11 11:59:48
There are a lot of tests going on on testnet right now, and each of the block explorers is following a different chain. One chain is the v4 chain, for the BIP065 tests. That chain is/was the longest in terms of the number of blocks, but most of the blocks were mined at very low difficulty and so it is not the chain with the most valid work.
We haven't found any block explorers yet that are consistently following the BIP101 tests.
As for the block time, arrival time difference: that's mostly my doing. One of the "features" of testnet is that after 20 minutes, the block difficulty gets set to 1. This timer is based on the timestamp of the block, not the clock of the recipient of the block, which means anyone can mine a block in a second by setting their clock 20 minutes ahead. The limit of this behavior is 2 hours, because a node will reject a block if its timestamp is 2 hours ahead of the receiver's clock. To try to minimize this issue, I set my mining server's system clock to 1h55m ahead of real time. This mostly worked for us, and is effective at preventing others from mining free blocks on top of mine, but if a peer has a system clock that is more than 5 minutes behind, they will reject the block. That has happened on several occasions, and has made the tests messy.
FYI, I will not always be posting or reading about this because of the censorship on this forum targeted at one of the pieces of software that was used during this testing. I recommend that if you want a response, you post on or the-subreddit-which-shall-not-be-named, or tag me as (make sure to spell it right -- two 'm's, no 'n's). There are several posts on the-subreddit-which-shall-not-be-named with more data. Also, most of the live test activity is conducted on IRC in a similarly named channel.
@_date: 2015-11-08 01:05:52


No, that's how *rational* markets work. We're working with *homo sapiens* here, not [*homo economicus*]( In order for people to act rationally, they have to care. The evidence is pretty clear that, when it comes to transaction fees on the order of $0.05, most bitcoin users do not care.
@_date: 2015-11-09 04:25:15
It's applicable for some things. The main purpose I have in this test is to check the forking behavior and the network behavior when the hashrate majority runs a different version from the full node majority. Right now, the vast majority (90%?) of full nodes on testnet do not support BIP101, so it will be difficult for BIP101 nodes to reach each other after the fork starts.
Different questions require different setups, and the testnet3 setup with mixed BIP101/non-BIP101 support is definitely inferior for many things, like evaluating performance. For evaluating correctness, working in a chaotic environment like testnet with many eyes on the subject is, in my opinion, ideal.
@_date: 2016-10-12 05:23:50
Google translated from **[their blog](




@_date: 2016-10-12 03:29:57
You forgot about building Skynet.
The rules on this forum forbid me mentioning what it is that I'm doing, but if you google blocktalk toomim, you might be able to find out for yourself.
@_date: 2015-11-24 22:27:18
There is no way you can make a profit at $0.18/kWh. Hosting plans at my company are cheaper than that at around $0.12/kWh.
Small miners have a lot of overhead and are not profitable. They should be considered toys only. If you want to have a chance of making a profit, the cheapest option with a reasonable chance of success is a $350 Antminer S5. Even that won't make a profit on $0.18/kWh power, though.
@_date: 2015-11-09 04:08:02
This is not using a new genesis block. It's using the same genesis block as testnet3 has been using for a long time. (Couple years?)
A new testnet4 with a distinct genesis block will be needed to get even roughly accurate performance data on big blocks. That's not the goal of the current test. Right now I'm mostly just trying to verify that the hard forking process works as predicted in a real-world, chaotic, unfavorable setting.
@_date: 2015-11-11 11:12:39
We have been deliberately testing the hardfork process to see how it works by turning on and off BIP101-based hashrate, and occasionally hashing on the legacy (Core) chain. Sometimes other people have been hashing on the Core chain too, especially sometime in the last 10 hours. With the drastic difficulty adjustments that happen on testnet (e.g. 100000 difficulty for one block, then 1 for the next), and the fact that difficulty is dependent on the node's timestamp (which can and does get manipulated) rather than the block height, things are pretty chaotic. Most of the chaos is just testnet being testnet, and not related to the hard fork. Most of the rest of the chaos is intentionally caused by us because we want to make sure that the behavior is correct despite everything we throw at it.
@_date: 2015-11-11 11:36:30
More details on the reorg after the fork can be found on another post which might or might not be censored/automoderated by this forum. I'll try to post a link in a reply to this comment in 10 seconds.
I'm still tracking down what went on while I was asleep. If you think my understanding of what happened is incorrect, let me know. I'd appreciate the help.
@_date: 2018-04-12 01:06:42
Interesting. I suppose Bitmain just got lazy and decided not to do a 10nm design. They're probably skipping straight to 7nm. Not too surprising; they (and everyone else except KNC) also skipped the 20nm node.
No, the other ASICs (including blake2b, X11, ethash, and cryptonight) are all 16nm, AFAIK.
@_date: 2015-11-08 00:33:46
I don't think that is a valid assumption. A fee market can force people to increase fees above the current level, but it won't force people to lower their fees below the current level of convenience provided by the default options.
@_date: 2015-11-09 03:39:57
If you spell my name correctly (two 'm's, no 'n's), I'm more likely to notice when you try to page me.
@_date: 2015-11-07 06:16:37
BIP100 has not been programmed. It can't run on testnet until it's been coded. Right now, BIP100 only exists as html. As far as I know, jgarzik (BIP100 author) does not have plans to code BIP100 because Core would not accept it, despite a majority of hashrate voting in favor of it.
BIP101 has been codified in code in a fork of Bitcoin Core which cannot be mentioned in this forum. It is possible to run BIP101 on testnet now. However, last time I checked (about a month ago), there had not been enough blocks mined with a BIP101 client to trigger BIP101 activation on testnet. I was thinking about doing that myself, but I got busy with other stuff and haven't made it happen yet. Maybe I should do that tonight.
@_date: 2015-11-08 09:03:46
Are you referring to BIP101? BIP101's earliest possible fork date is Jan 11th. If it takes longer to get support for BIP101, that's fine.
It seems to me that a lot of people think that the bitcoind code isn't fast enough for large blocks, and that block propagation isn't fast enough for large blocks. In response to this, I've been working on speeding up some CreateNewBlock() stuff in bitcoind, and Gavin and Mike have been working on IBLT stuff and other block propagation code. It might take a few months to get all of this done. If people decide in March that we're finally ready for BIP101, that's fine with me. I think the best way to argue is with code, and writing good code takes time.
@_date: 2015-11-09 04:06:48
We've got enough spam for now, but thanks. Maybe tomorrow?
I might want to try to spam mempools more widely to crash some of the unmaintained Core nodes so we can have a slightly better testing environment, but there's no need for that yet.
Page me if you want some coin. I'll try to keep pre-fork coins and post-fork coins separate.
@_date: 2015-11-06 18:59:23


Correct. Specifically, you need to know the hash of the previous block, since you have to stick that into your new block's header. It's also useful to know what transactions were in that block, because without those, you can't safely include any transactions in your new block.


Incorrect. The Relay Network is just a system for propagating blocks faster. It compresses the blocks by not transmitting transaction data twice. Normal block propagation methods store all of the raw transactions (about 500 bytes per transaction on average) inside one binary blob and send that, resulting in 1 MB of bandwidth for a 1 MB block. However, most of these transactions already exist in the recipient's mempool because they were propagated through the network earlier. The Relay Network is just telling nodes what the order of the transactions is inside the block. The Relay Network encodes each transaction as the first few bytes of its hash (just enough for it to be unique and recognizable), and gets that 500 bytes per transaction down to about 10 bytes. This means that you can send a 1 MB block with about 20 kB. Upon reception, you can still reconstruct the whole block (with transactions), because each node already has the full ~500 byte transaction. 
@_date: 2015-11-08 10:39:40
FSS-RBF, First-seen-safe replace-by-fee (you sent a transaction without enough fee and you want to add a bigger fee)
CPFP, Child pays for parent (someone sent you a transaction without enough fee and you want to spend it while adding enough fee for both of them)
Unfortunately, neither of these have been merged into Core yet, as far as I know.
@_date: 2015-11-09 03:54:31
BIP101 was already tested pretty extensively in regtest mode by Gavin. It was not tested in testnet because testnet is actually a pretty difficult place to test things, despite the name. Nodes on testnet are poorly maintaned, and some of the rules (e.g. resetting the mining difficulty after 20 minutes) are kinda nasty. It will be chaotic, messy, and fun.
@_date: 2015-11-15 20:39:27
For fun or for profit?
@_date: 2015-11-08 17:29:14
Your assumptions are not realistic. 
Rational users will optimize their utility, which usually means not spending a lot of time worrying about things that do not matter.
SPV wallets cannot create dummy blocks or accurately estimate fees. If block sizes are large, most people will use SPV wallets.
I'm done with this conversation. I've got better things to do.
@_date: 2015-11-08 02:11:39
Rather than enforcing a protocol-level block size cap (an artificial supply ceiling), why not a fee floor (an artificial demand floor)? If you want to use the Bitcoin network, you have to pay at least $0.01/kB in fees for that transaction. Any miner that includes a block with a transaction with a smaller fee will be orphaned by other miners. That way, miners can negotiate with themselves about what an acceptable minimum fee should be. It's collective bargaining, or a miner union, if you will. 
@_date: 2015-11-09 03:49:59
I would prefer to be the only significant amount of hashrate during the initial stages of the test, as I intend to try to fork it, un-fork it, refork it, etc a few times, and that will be a bit more complicated if I have to coordinate with others to do so. I've got plenty of hashrate for this test without others' help.
We can do more distributed tests later, possibly after we migrate to another (BIP101 exclusive) testnet to try to test performance in a more reasonable scenario.
@_date: 2015-11-09 04:36:04
If the purpose of this test were primarily political, then you're right, block explorers would be very important. But politics is only about 30% of my motivation in running this test. Believe it or not, I wanted to actually play around with an active BIP101 network, with active participants other than just myself, just to see what would break, and how it would break, when we tried to do crazy things with it (like running a 90% BIP101 hashrate majority with 10% BIP101 fullnode minority). I also wanted to use it as a practical environment for testing out and developing new code in a large-block context, such as Mike Hearn's new [thin block propagation]( code, or my work on speeding up getblocktemplate.
I started the test by mining BIP101 testnet blocks last night. I am competent enough to use the bitcoin-cli RPC command line interface to get the information I need, albeit more slowly. A block explorer would be very helpful, but not necessary.
@_date: 2015-11-11 23:53:36
Part of me hopes that the Bitcoin price does not rise any until the block reward halving of 2016 in order to prevent too much more hashrate from being manufactured.
@_date: 2015-11-09 09:59:51
I want to see every problem BIP101 can have while we're on testnet or in regtest mode. Once we see the problems, we can fix them, or otherwise prevent them from happening again. That's the only way to make sure that it won't have problems when we deploy it on mainnet.
@_date: 2015-11-09 03:51:59
Yes, that's an interesting question. It will be time-consuming to set up that test in a way that would give accurate and meaningful results, though, so I suggest we do it later, after we get the basics covered.
@_date: 2015-11-09 03:59:13
Miner consensus! Hah, good one. No, just a brief and sudden miner monopoly. Testnet mining is not very competitive.
@_date: 2015-11-09 04:15:54
BIP101 probably won't be activating in January. That's just the earliest date that it would be possible to activate it. We'll activate BIP101 when we have support of 75% of miners for it, whenever that is, if ever. I'm vaguely guessing around March might be a good time, but whatever. Whenever.
@_date: 2015-11-07 23:24:34
In order for the mining industry to fail with BIP101, fees would have to be lower than they have been in the past when blocksize was unconstrained. The assumption that fees are determined solely by supply and demand is incorrect. Most people just use the default fee given by their Bitcoin client, as long as it's not too large for them. These default fees should be sufficient.
For example, in September, there was no stress test going on and average block sizes were 0.45 MB. That means that Bitcoin users did not have to pay a fee for transaction prioritization, but they still did. At the time, the average fee per kB was 0.00037 BTC, or 0.166 BTC per block. If we had 45 MB blocks, that would be 16 BTC per block. That would be enough for miners. Fees right now are [even higher](
If you look at the [entire history]( of Bitcoin transaction fees, the highest transaction fees per block of all time were in 2013, when blocks averaged only 0.15 MB in size. They dropped after that because bitcoin-qt version 0.8.2 was released in July 2013, which lowered the default transaction fee from 0.0005 to 0.0001 BTC. As people upgraded over the next 8 months, average transaction fees plummeted.
@_date: 2015-11-11 10:08:33
Thanks for taking the time to put in the corrections; I'm pretty short on it these days, so I might slip in some minor inaccuracies or oversimplifications in informal settings. I apologize for them.
@_date: 2015-11-11 10:52:01


I think that is factually incorrect. 
Full bitcoin clients follow the (a) valid chain with (b) the most work. That makes two independent requirements for determining the current best chain tip. Our BIP101-supporting bitcoin clients were seeing the best chain as being 584xxx blocks long even though they recognized the existence of a 6008xx block long competing chain because the 584xxx block chain had the most work.
Read [this link]( for another source stating that the actual requirement is the most work. "Longest" is often used as a shorthand version of saying "most-work", but number of blocks is not the metric. It usually doesn't matter, because on mainnet the amount of PoW per block doesn't vary much. On testnet it did, because testnet is nutty.


Block explorers are absolutely not reliable. They are excellent tools, but you should not trust them for anything revenue-critical. They are an easy target and vector for hacking attempts.
EDIT: This post was accurate as of when I went to sleep, but after I went to sleep someone apparently hashed on the legacy chain and overtook the BIP101 chain in terms of proof of work. I wrote the post after waking up and before checking the current chain state. Now block explorers are showing two different chains, one with v4 blocks (BIP065 activation), and one with v3 blocks and BIP101 blocks.
@_date: 2015-11-09 02:08:04
No, services and peers will choose whichever chain is the longest **and** is validated by their client. Unlike mainnet, I doubt there are very many SPV clients on testnet. Most peers will remain on the non-BIP101 fork. The BIP101 fork will only be used as long as I keep it longer than the non-BIP101 fork.
@_date: 2015-11-09 04:22:00
In 2036, yes. Can you imagine trying to test out a video game from 2015 on hardware made in 1995?
@_date: 2015-10-04 02:02:13


No, the attacker does not have to spend the same amount as is needed to protect the network. You only need about 1 hour of hashing to perform an attack, whereas you need to keep miners on 100% of the time to protect against an attack. You need to purchase or rent the miners. Rentals are not currently an option for more than about 5% of the network hashrate. When an attacker has to purchase hardware, security is best when CapEx is high and OpEx is low. When the


According to the efficient market hypothesis coupled with the assumption of a zero transaction cost and miners that are only financially motivated, perhaps. However, miners are lazy, and it takes a lot of work to change the pool configurations for hundreds of thousands of machines in thousands of buildings spread all over the planet, and even then many miners (like me) would refuse on idealogical grounds.
Rentals are only an option if more than half the mining hardware is connected to a rental system like Nicehash. That is currently not the case and will not be unless the profitability of mining honestly with that hardware falls below breakeven.


I think the problem might be that we're using different definitions of the word "value". When I see "value", I think you're meaning "what it's worth to us". That is, it's how much we stand to lose if our security is compromised. I think this would be a significant fraction of the total market cap for Bitcoin, or around $1B. You seem to be using it to mean "the degree of protection provided", and you seem to be asserting that "the degree of protection provided" is equal to "the cost of performing an attack" as well as "the cost of providing protection". I think the first two are equal, but the third one is not equal, due to CapEx/OpEx differences.


This is in conflict with the data that I've seen. Can you show me data backing up that assertion?


If multisig and lightning/payment channels became 30% of total transactions, I think we might see an increase around 15% in average tx size. (How big are multisig transactions, anyway? An extra 150 bytes?) I do not consider that to be relevant as a contribution to 10x or 8000x increases in block sizes.


Perhaps a bit lower, perhaps a bit higher. Will they be more than 50x lower? That's about as much as it would take for bandwidth, CPU, and storage costs to be comparable to revenue with current hardware.
I think miners will use a minimum fee/kB setting that excludes transactions, and that very small transactions will use micropayment channels (e.g. Lightning).
Current fees are set by a de facto consensus among developers about what the default fee settings should be for miners and wallets. The highest fees per tx we've seen happened in 2013, when block sizes were much lower than they are today. Then bitcoin-qt 0.8.2 was released which dropped the default fee per kB from 0.5 mBTC to 0.1 mBTC, and actual fees dropped by about 4x. I think this shows that developers can exert a lot of fee pressure if they need to in order to keep the network safe, at least as long as average fees are small enough that they don't bother most users.


Actually, I think transaction ECDSA sigs are already verified on first arrival, which is usually before they're seen in a block. Gavin is also working on weak block propagation, and might have that done and part of XT before the January earliest activation date for BIP101.
@_date: 2015-11-07 03:34:55


I probably should have emphasized the word "need" there.


By "regular users" do you mean "non-mining users" or "SPV wallet users"? 
What I was trying to say is that non-mining full nodes are pretty much irrelevant in terms of the trust-free security of Bitcoin. They also are pretty much technically irrelevant to miners, except insofar that they lightly filter transaction data before it arrives at miners. They are economically relevant to miners, because miners have to sell their coin to someone in order to pay for electricity and labor, but that's about it.
Full nodes don't enforce consensus rules so much as they ignore anything that doesn't follow consensus rules. "Enforce" makes it sound like they're cops, patrolling the streets for criminal blocks, which they scavenge and put in jail. Really, they're more like taxi drivers who will take you wherever you want to go as long as you're not carrying a weapon. Just because one taxi refuses to give a bloody axe murderer a ride does not mean that none of them will, so you can never assume that all of the axe murderers will never arrive in your lovely upper-middle-class suburb. Invalid blocks can get mined and propagated no matter how many honest full nodes there are, as long as there are dishonest ones as well.


If there is a consensus rule that really matters to you, you should verify it yourself, or you should enter into a contract with someone else to have them verify it for you. If all of the consensus rules matter to you, then you should run a fully-verifying node. You do it for yourself, not for the network. If it only matters to you a little bit, then maybe you can trust the [rumors on the street](
The idea of running full nodes because the network needs thousands of full nodes is misguided, in my opinion: it is much cheaper to run a dishonest full node than to run an honest one. There may be more to gain by running a dishonest full node than it costs. This means you cannot completely trust full nodes to be honest. You can only trust what full nodes can prove, and there is very little that they can prove efficiently. They can prove that transaction X was in block Y, but they cannot prove that transaction X was not in block Y without sending all of block Y.
I said that full nodes are irrelevant to the trust-free security of Bitcoin, but they are not irrelevant to the security of Bitcoin. This is true because SPV wallets trust at least a minority of full nodes to honestly answer any given question about the blockchain. Placing a small amount of trust in full nodes to not omit information allows SPV wallets to work. It's a small amount of trust, which provides a small window for attack. In my opinion, this small amount of trust is an acceptable tradeoff for the vastly improved performance and cost of participating in Bitcoin for almost all users. However, I think it's important to be clear that they do make Bitcoin slightly less secure, and that Bitcoin does not need SPV wallets, and Bitcoin without SPV wallets has no need and little use for non-mining full nodes.
@_date: 2015-11-11 12:13:00


Yes, a logical AND is what I meant by "independent requirements". Thanks for clarifying.


If the block explorer is using the Legacy (v3) consensus rules, then it should invalidate the &gt;1 MB blocks. One issue is that the bitpay [test-insight server]( is allowing BIP101 blocks (we saw them there earlier, about 24 hours ago), but it's not following the most-work rule. This is the incorrect behavior that I was referring to. The other block explorers are rejecting BIP101 blocks as they were designed to, which is correct behavior.


Because it's faster for reviewing a large amount of information than using bitcoin-cli. We use the command line when making sure of things. We have seen (as expected) that the results from the command line often differ dramatically from what is shown on the explorers. That's no big deal, it just slows us down. The explorers not showing our branch does not make them useless, though, because they still show what is going on with the other branches. A fork test is of no use unless there are at least two branches. (Lately, there have been three at once due to the v4 block testing, which is even better.)
There are some posts with bitcoin-cli results over on the-subreddit-which-shall-not-be-named. I just don't usually make posts on about this because the mods here have made it known that they often disapprove of such things, and I don't like taking time to make a thoughtful post and have it get deleted. Please tag me if you want a comment from me on something on @_date: 2015-11-11 22:03:12
Unfortunately, that scenario is dangerous. Bitcoin's security relies on the capital costs of acquiring enough hashpower to perform a 51% attack being large compared to the operating costs of the attack. If the active hashpower decreases a lot, that means that there will be a lot of used hardware for sale cheaply or on a hashpower aggregator like nicehash.com for rental. Renting hashpower for a short period of time is very inexpensive. 
It costs only 0.3 BTC to rent 1 PH/s for one hour, but to purchase that much hardware would cost about 1000 BTC.
@_date: 2015-09-07 19:29:15
Note that in order for this to be an effective attack, many or most (by hashrate) of the miners need to get the very large block quickly, and miners without the large block need to not be using SPV mining. At its strongest, this attack can only harm the subset of miners who have poor connectivity with higher orphan rates. At its weakest, this attack harms the miner who attempts it with higher orphan rates. 
@_date: 2015-09-30 21:58:25
Nope. Statistical noise. The number reported on that graph is not the actual hashrate, it's an estimate. The estimate is based on a small number of samples. The actual hashrate is closer to 430 PH/s.
@_date: 2015-09-08 09:27:13
I think the BIP100 voting gives added incentive to exclude certain blocks or pools. It provides an organizing principle for cartels -- namely, the control of block size caps -- and a valid (but bad) reason for them to exert their power.
@_date: 2017-05-26 19:41:54


Cointerra (Timo Hanke + SDL), Spondoolies, and Bitmain are all publicly known to have independently discovered and patented this optimization. Other manufacturers have probably also discovered it but neglected to patent it. From what Sam Cole said, it sounds like KNC falls into that group. Given Bitfury's R&amp;D budget and their chip performance, I would be very surprised if they missed it. I have no idea about Avalon, but they could always license it from one of the bankrupt companies (Cointerra or Spondoolies) pretty cheaply.
@_date: 2015-11-11 11:03:59
Yes, this looks to be the case. I see the large blocks we mined earlier today in my database when I check by hash, but not by block height. After we stopped hashing on BIP101 blocks, another chain overtook us. This is cool, because it lets us do more testing on forks activation.
@_date: 2015-11-09 10:41:31
Nope. BitcoinXT is compatible with Bitcoin Core until 2 weeks after 75% of all mining power is using BitcoinXT. At that time, BitcoinXT is like Core, except with blocks that can be up to 8 MB and doubling every 2 years. You can read about BIP101 (which is BitcoinXT's main feature) here:
@_date: 2015-11-06 02:57:51


Everybody right now. Look up Matt Corallo's Relay Network.
@_date: 2015-10-21 12:06:35


No, I'm not missing that point. I'm disagreeing with it, as I think it is not valid.
As a miner, you have three ways to increase the number of transactions you add to blocks in a day. You can either increase your hashrate to find more blocks, you can increase your network connectivity to reduce your orphan rate, or you can increase the blocksize to increase the number of transactions per block. A single miner doesn't care what the average blocksize is. They only care what their revenue is.
You can't model miner incentives accurately while assuming that the number of blocks found per day per miner is a constant. You have to evaluate the opportunity cost of increased bandwidth to figure out where miners will spend their money, and you need to account for the fact that bandwidth is very cheap compared to power.
@_date: 2015-11-09 04:18:06
Mike Hearn's policy is to merge features from Core when Core makes a release. Currently, that means that OP_CLTV will get merged into XT when Core 0.12 is released. 
There are a lot of changes in Core master right now that I would like to work on in my own work, so I might ask Hearn to start merging early, but I don't know if he'll listen to me. I'm a pretty junior developer for XT.
@_date: 2016-05-27 19:08:52


**[Only if you're trying]( and only if you violate the existing IsStandardTx() policy rule limiting you to 100 kB transactions. I roughly estimate it would take a &gt;= 260 kB transaction crafted for maximum validation complexity to reach the 1.3 GB limit. Transactions of this type serve no purpose except as attack vectors, as the way they are constructed is by writing the validation script so that it performs the same operation 200 times in a row.
**Any such timelocked 200-sig transactions would be unambiguously DoS attack transactions.** I have no compunctions about invalidating timelocked DoS attacks made in the past but scheduled for the future. Not that they exist.
Note that with the current 1 MB limit and no 1.3 GB sighash limit, you can create a block that would hang bitcoind nodes for about 3 minutes during validation. The Bitcoin Classic team decided that this was worth fixing, so we fixed it: the worst-case with Classic is about 10 seconds regardless of block size. Segwit's "fix" only applies to the new transaction format, which means that **Segwit still allows the 3 minute attack.** That's why it doesn't invalidate these hypothetical locktimed attack transactions. With Bitcoin Classic, we could have set the limit to allow the current 3 minute (19.1 GB bytes hashed) attack, which would have made it impossible that any timelocked transactions would be made invalid; however, we decided that fixing a severe DoS vector bug was more important than protecting against this locktime transaction FUD.


Technically, it limits blocks to 1.3 GB of hashing, but that also limits transactions to the same  for 1-tx blocks.
@_date: 2015-11-06 04:00:26


Only if the average blocksize stays small. With 0.2 mBTC/kB, a 62.5 MB block would have about 12.5 BTC in fees. 
We probably don't need 12.5 BTC in fees to maintain the current hashrate; for details, see the calculation in  In order to pay for the operating costs of mining with 500 PH/s, plus a small profit margin, 5 BTC should be enough. With hardware with improved efficiency (e.g. all Antminer S7s), 2 BTC should be enough. We can get that with 10 MB blocks. With higher fee/kb, higher mining efficiency, and/or lower electricity costs, we could achieve that at even lower blocksizes.
So perhaps we have a choice between which hardfork we prefer: increasing the block size limit to increase fees, or removing the 21 million coin cap. Miners have to be paid somehow. 
@_date: 2015-11-24 22:52:41
I don't see how this is related to Bitcoin. Radio (e.g. WiFi) works just fine for any Bitcoin use scenario I can think of.
@_date: 2015-11-09 03:42:45
I request that you not do this for the time being. I have plenty of hashrate for the tests that I want to conduct, and trying to coordinate hashrate changes with multiple people will be a logistical nightmare.
(One of the first tests I will be trying to conduct is forking the chain with a large block, then switching to mining on the small chain to overtake it, and then switching back. Trying to collaborate with other people to do this is much harder than just doing it.)
I run a 750 kW bitcoin mine. Most of the machines here do not belong to me or my company, but I've got plenty of hashrate which I do own for these purposes, at least for now.
@_date: 2015-11-08 16:27:29


The system is made of humans setting an option in bitcoin.conf. The $0.01 figure is arbitrary, and would be denominated in BTC.


Yes, but it's more of a direct intervention on the thing that we're trying to control. The problem is that we have difficulty incentivizing users to pay for hashrate and power directly. If we set up collective bargaining on behalf of the miners, we may be able to address that.


We can make it enforceable by putting each miner's minimum fee vote in the blockchain. Any miner who doesn't obey that vote gets their block orphaned.
@_date: 2015-11-07 04:53:02
My point is that running an honest full node doesn't help anyone except the person running that full node and SPV wallets that decide to trust it. Non-mining full nodes are otherwise unnecessary. A network in which all full nodes are miners is technically feasible and secure. 
I think you might have jumped into this thread halfway through and bypassed the context. The context was that Adrian-X thought that the relay network somehow compromised Bitcoin by allowing miners to talk to each other without talking to full nodes. There is no reason to be afraid of miners talking straight to each other and bypassing full nodes, as the purpose of non-mining full nodes is not at the core of Bitcoin's functionality. It's at the edge, where regular non-mining users interact with Bitcoin.
@_date: 2015-10-19 23:20:02


If needed, I'll rent a VPS in China, and set something up with point-to-point weak blocks, or write a proxy-based stratum poolserver-type thing so I can publish my blocks within China without having to worry about the latency of sending a 20 GB block across the GFW. The only data that needs to be sent across the GFW quickly is the 80 byte block header; the rest of the block can be sent across before the block is found. 
Or I'd build a clone of BlueMatt's relay network so I didn't have to worry about BueMatt going rogue.
I could almost just continue to use p2pool, as p2pool already has fast block propagation. I know Bitmain/Antpool has a p2pool node close to their main stratum poolserver. P2pool's performance is somewhat lacking with large transaction volumes, but I've already made a [&gt;= 40%]( [improvement]( on that, and have more improvements in the pipeline that would likely be done before BIP101 is activated.
More likely, the many megawatts of non-Chinese miners who use Chinese pools will switch to pools with better connectivity. I know from my hosting business that about 30-50% of miners in the USA use Chinese pools. They do so because there's currently no reason not to. If miners were told that BW.com+F2pool+Antpool+BTCC were a mining cartel conducting a 51% attack, I think they would stop having 51% of the network hashrate within a few weeks.


These are a few points where we disagree. I think the Bitcoin network can handle 8 MB blocks today without dangerous orphan rates or dangerously unequal access to tx fees, and probably can handle 20 MB blocks. I think that the block size limit is superfluous, and that the economic incentives already in place are sufficient protection. I think that if large blocks were a certainty, then miners would change their expense priorities to improve connectivity, and they would also probably invest in programmers to help reduce their stale/"orphan" block rates.
Anyway, I recognize that your positions are different than mine. If we were to solve the block propagation problem, would you support increasing the block size limit?
Did you have any thoughts about the blocktorrent idea I proposed? What about weak blocks as an option? Is your concern that we can't get these done by January (BIP101 activation date), or that they might not work well enough for 8 MB blocks at all?
@_date: 2015-11-07 09:05:06
VISA and Mastercard were not built in 5 years. I think it would be possible to scale Bitcoin infrastructure up to the point where it could handle that transaction volume in 5 years, but I do not think it is a good idea to try. Doing so would require a very large investment in hardware in order to be able to handle that transaction volume, which means that only a small number of entities (maybe 100 worldwide) would be able to afford to do so. Scaling over 20 years allows for technology to improve a lot more, both in terms of hardware and software, bringing down the cost of running a fully verifying node at that volume to the point where it will likely be affordable to run one in one's home in an industrialized country, and will almost certainly be affordable to a small business or startup.
It's worth mentioning that the above paragraph contains forward-looking statements which may be controversial. I'm happy to provide math to back those statements up, but I do not think they belong in an ELI5 thread.
@_date: 2015-11-09 03:44:33
I've been in the same boat as on this. I've wanted to do some testnet work with full-size blocks for a while, but I've been working on other things too and haven't had time. Last night I decided to make time and just do it, so now it's getting done.
@_date: 2015-11-09 04:29:36
BIP100 will probably never be implemented. I think we've already passed the critical date for Jan 11th activation, since BIP100 requires 95% voting for 12,000 blocks.
I don't like BIP100 as much as BIP101. I might support it if were implemented, but I'm not sure.
Evil is subjective.
@_date: 2015-10-07 13:16:31




An unproven but easily testable fraud rumor. It's not hard to describe a consensus rule violation in terms that only require downloading a few transactions. Unless you think there's some way to make the verification of the violation dependent on the whole blockchain somehow?


Come on, be realistic. People will notice a hard fork unless there are a dozen or fewer full nodes left. Hell will break loose. Bitcoin value will crash. The miner's millions of dollars worth of SHA256 hardware will depreciate overnight. 


Sure, "safe" is a fuzzy category.  The curve for the risk of this attack vs. percent full nodes will be monotonically decreasing. It will not be linear. Whatever. The question is, at what proportion of SPV clients, and at what proportion of full nodes that are miners and complicit in the attack, would the risk become economically significant, with accuracy of about one order of magnitude?
Given the nature of the attack has large costs for failure, there should be a tipping point where the attack becomes financially lucrative. Where do you think that inflection point would lie? 


No, it has to be a merchant that offers fast or irreversible transactions and more than (blockreward+fees) in liquidity every 10 minutes. Right now, that would be $36,750 per hour. If the merchant is doing that much business, they can probably afford to run a full node of their own.
@_date: 2015-11-08 16:51:01




Only if the use case is microtransactions or something like that. For most users, the difference between 98% certainty that it would confirm in 1 hour and 99.9% certainty that it would confirm in 1 hour is worth a lot more than $0.05. At the same time, $0.05 is not worth more than 20 seconds (at a wage of $10/hr), so users will often prefer to pay too high of a fee rather than check to see if such a high fee is actually necessary.
@_date: 2015-10-02 22:48:50
It sounds like you are trying to figure out how to pay for mining. You are thinking that the price per tx needs to increase. I do not think this is the case, as I do not think that price increases alone will be sufficient or necessary to pay for mining. I think this will become clearer if we look at actual numbers.
Currently, the Bitcoin mining network uses about 300 MW of power. Let's say the average miner gets power for 4Â¢/kWh, and pays another 3Â¢ on other costs like labor, land leases, and a very slim profit. That means that the cost of mining is about $3500 per block. In order to ensure that the mining network remains secure, we have to ensure that the block subsidy stays above about $3500, or around 15 BTC.
With average block sizes around 500 kB, we are getting about 1000 transactions per block. In order to fund mining with blocks this size, we would need to have fees around $3.50 per transaction. Currently, transaction fees are around $0.05/tx, so this would require a 70x increase in fees.
On the other hand, we could also keep transaction fees the same and increase block sizes 70x, to about 35 MB average block sizes (~70 MB cap). For a miner of my size (0.25% of the network) using today's technology, this would result in bandwidth costs using about 1% of our total budget. Basically, we'd have to pay about 2x as much on bandwidth as we currently do. (Smaller miners would typically use a pool anyway, and won't have to pay bandwidth costs.)
35 MB blocks with $0.05 per tx in fees sounds more reasonable to me than 0.5 MB blocks with $3.50 per tx in fees. I think that if transactions cost $3.50 each, people would not use Bitcoin, and would either create altcoins or use Western Union. On the other hand, if we keep the cost of each transaction approximately constant (and low), I think it plausible that demand will increase 70x over 5 years.
We can also reduce the cost of mining at 450 PH/s by improving the efficiency of mining hardware. Currently, that 300 MW is based on an assumption o 0.67 J/GH miners, which I estimate is what the average miner uses. The best miner on the market uses only 0.25 J/GH, and a few have been announced (but not shown) with efficiency below 0.1 J/GH. By improving the efficiency of mining, we could probably reduce the cost of mining over the next few years by 5x without reducing the hashrate.
With 5x improvement in mining efficiency over 5 years, we might our options be a spectrum between 7 MB and $0.05/tx to 0.5 MB and $0.70/tx. Both of those points sound plausible to me, but the end with 7 MB and $0.05 sounds both more plausible and more desirable.
A bitcoin network that is capable of high transaction volumes and low fees is inherently more valuable than one that is capable of low transaction volumes at high fees. I suggest we build that. While we might be able to fund Bitcoin mining with 7 MB blocks at current fee levels, that would require a lot of dominos to fall perfectly. I think we should aim for 15 MB average block sizes with $0.05/tx fees by 2020, when the block reward halves again to 6.25 BTC. At that point, if the price remains constant, we would have $1500 or 6.25 BTC in transaction fees and 6.25 BTC in block subsidy.


Difficulty has nothing to do with orphan risk, transaction volume, or blocksize. Difficulty is merely a function of hashrate. I think you might mean "equilibrium fee per kB".


We don't need a strong fee market (though it would be nice). The current $0.05/tx fee is determined mostly by defaults in the common Bitcoin wallets/clients. I think that can be good enough. A fee market would be better, of course, but not necessary.


No, the cost is not bandwidth, storage, and CPU. Those are so cheap as to be basically free relative to tx fees, even for 8 GB blocks today. The true cost is orphaning. The cost is lost block subsidy (for now), or the lost transaction fees from losing an orphan race by creating too large a block. It's an opportunity cost, not a real cost. It's similar to a game where I get a few people together and make them choose a number, then give them a die, and tell them to roll the die once. They add the number they chose to the die roll, and the person with the lowest sum gets the number they chose in dollars.
@_date: 2015-10-25 16:05:01
I think those are essentially the same question.
@_date: 2015-11-08 14:09:15
Awaiting code review. Additionally, delayed by mempool refactoring.
@_date: 2015-11-07 06:17:54
Litecoin has its own testnet.
@_date: 2015-11-09 14:50:44
I mined one 1.1 MB block already. The fork has happened. You may need to addnode some XT nodes to be able to get the new blocks, as the network fragmentation is severe from Core outnumbering XT.
Unfortunately, each time I restart my mining node to change the configuration (e.g. max block size), I lose the mempool, so I don't see much of a backlog any longer. If you restart your node, you'll retransmit all of your transactions, which is helpful to me.
@_date: 2015-11-15 08:35:24
If you're looking to mine as a hobbyist, there are a few USB miners out there that might be fun to play with.
If you're looking to actually make a profit, you should expect to spend $400 or more, and you should be prepared to lose a lot of what you spend. Mining is a gamble. Sometimes you win if you buy the right hardware at the right time and run it with cheap power (or use it to replace your heating bill), but it's much easier to lose money than to earn it.
@_date: 2016-05-22 06:31:52
Spam in your mempool and spam in blocks are very different things. 
All miners and pools now have set minrelaytxfee=0.0001 or higher, and that prevents all of this spam from being included in blocks. The spam attacker from October has been rebroadcasting these transactions twice a day since October, so the spam you're seeing is really probably a 1-day-old rebroadcast of a valid transaction that was first seen in October and which will never be confirmed.
@_date: 2016-05-13 08:21:28


I think you're confusing me with my brother.
@_date: 2015-10-05 12:37:19


Yes, that is intentional. I was trying to describe steady-state conditions. The capital cost of miner hardware will likely be paid off by the block subsidy before we reach steady-state conditions. My main concern is making sure miners don't permanently shut off a large amount of hashrate and redirect it to the hashing rental market, where it could be used for 51% attacks. 
As long as attackers are forced to pay capital expenses to acquire enough hashrate to perform a 51% attack, I think the network will be secure enough. If too much used miner hashrate is available, we could have a problem.
@_date: 2015-11-08 16:22:51
The block reward is irrelevant to the demand side of things. If people are willing to pay $0.05/kB when miners don't need it, they'll be willing to pay $0.05/kB when miners need it too.


Receiving five-cent donations is not a competitive industry. That is what transaction fees have been equivalent to so far. I see no reason why that needs to change.
@_date: 2015-11-09 04:18:48
Maybe later.
@_date: 2015-10-07 11:46:05
This is basically a hard fork attack against SPV clients.
SPV clients do not validate transactions, but they do validate proof of work. In order to perform this attack, the malicious miner cartel has to produce a lot of blocks with valid PoW but invalid transactions. That's a very expensive attack. 
Ultimately, all they can do is fool SPV clients about the current state of the blockchain. 


How about a news article or comment posted on linking to the invalid transaction and describing why it's invalid? That sounds like a type of fraud proof that we already have. It's slow, but given that anyone who ran both a full node and an SPV wallet could/should notice discrepancies, I think it's likely to work well enough. When SPV wallet users hear that a group of miners are trying to use a hard fork attack, they can hold off on attempting to spend money until they can either (a) spin up a full node of their own, (b) find a full node that they trust, or (c) wait until the attacking miners run out of fiat money and the honest fork overtakes it.
If a group of miners have enough hashpower to perform a hard fork attack against SPV clients, they also have enough hashpower to selfishly mine and get 100% of block rewards. I think this purported SPV hard fork attack is a red herring.
@_date: 2015-10-05 11:34:10
Irony and sarcasm are difficult to communicate effectively in text. It is usually better to avoid them in high-pressure situations like this. I would suggest apologizing to him.
@_date: 2018-04-14 16:25:08
Click the link.
@_date: 2015-10-21 02:43:53


No, power is the main marginal cost. Electricity costs miners about 100x as much as internet connectivity does. Let's say we have a miner that spends $100 per hour on power and $1 per hour on internet, and gets an orphan rate of 1%. This is about what we have today. They could increase their spending to $2 per hour on internet and maybe halve their orphan rate, or they could increase their spending to $101 per hour and increase their block-finding capability by 1%. A rational miner would choose to increase hashrate in this situation.
If we then increase the blocksize to 8 MB, then their orphan rate might increase to 8%. At this point, $1 would gain them 4% more revenue, and $3 would gain them 6% more revenue, so they would choose to spend about $2.8 on internet and $98.2 on electricity, and get an orphan rate around 2.82%.
Orphan rate might be approximated in this model as 0.01 * block_size/bandwidth_expense. Revenue would be (1-orphan_rate) * hashrate, or (1 - .01 * block_size/bandwidth_expense) * (100-bandwidth_expense). Doing a little calculus, this means that the optimum point would be where bandwidth_expense = sqrt(block_size). (There are actually two hidden constants there, which I set to 100 and .01, and so they cancel out in this example.)
In a more extreme example, with 100 MB blocks, it might cost $100 per hour to get a 1% orphan rate. Instead, miners might choose a 10% orphan rate at $10/hour.
This assumes the cost of internet bandwidth does not fall, and no work is completed on improving block transmission efficiency. These things will happen. For one thing, bandwidth is already cheaper the more you get (decreasing marginal cost), which would push down equilibrium orphan rates.
@_date: 2015-10-06 00:22:47


Plus all SPV clients, regardless of whether they have been updated or not. SPV clients comprise a majority of all wallets, as far as I know.


In the absence of a soft fork, it is difficult to create a transaction that will be confirmed once and then orphaned. If the transaction is valid, other miners will usually incorporate it into a block. Furthermore, it is difficult to predict what blocks will be orphaned, and difficult (and expensive) to intentionally orphan blocks. The probability of success is approximately equal to the percentage of the network hashrate that they control. Performing the attack successfully requires that the attacker risk the transaction unintentionally being validated. 
With a soft fork, it is trivial to create a transaction that will be confirmed once and then orphaned. The attacker does not need to collude with miners, as long as there are still some on the old version. The chance that the transaction will be unintentionally permanently confirmed is 0%. The cost to the attacker is 0. Attackers can afford to "overpay" for items with invalid transactions in order to scam vendors into releasing goods prematurely. 


From a UX perspective, I think this is bad design. Blaming the user for not being aware of how the backend works is usually not a good design policy. 
Have you ever tried to buy something on Craigslist or bitcointalk with bitcoin? Bitcoin scams are a very common thing, and this soft-fork 1-conf issue gives scammers a tool with nearly ideal properties. Zero risk of full confirmation, high chance of partial confirmation, zero cost, and indistinguishable to SPV clients? That's not good for newbies.
It sounds like you don't like 1-conf transactions. However, I think you must be aware that the majority of commerce conducted using Bitcoin uses either 0-conf or 1-conf. Sites like Bitpay, Purse.io, NewEgg, and TigerDirect all use it, I believe. Requiring 6-conf makes ecommerce checkout UI design much more complicated and less user friendly. People don't want to wait an hour just to hear if their order was placed successfully. If you tell your customers that the order was placed after 1-conf, and 1-conf is not 99.9% reliable, then you have to write code to inform your customer that the payment was orphaned, and you have to train your customer service reps on what that means, etc. Issues with exchange rate fluctuations also become more severe as the number of required confirmations increases.
It's true that in the absence of a soft fork, 0-conf and 1-conf transactions are not 100% reliable. They're only about 99.9% reliable. However, they're instant or nearly-instant payment, which means that they can compete with credit card transactions. The fraud rates for credit card transactions are on the order of 1-2%. This means that merchants can reduce fraud rates by allowing "insecure" 0-conf and 1-conf payments as an alternative to credit cards. With a careless soft-fork, you might create an attack vector that increases 1-conf fraud rates to 1% or 10%. At that point, Bitcoin would lose many of its advantages over credit cards and would probably no longer be used in ecommerce.
Soft forks will make some of the most common use cases of Bitcoin -- buying things with SPV wallets, and buying things with 1-conf transactions -- less intuitive and more dangerous. Each time a user runs into trouble with Bitcoin and gets defrauded, they will tell all of their friends about what a terrible sham Bitcoin is and how they should never use it. It does not matter if you think the fraud was their fault for accepting 1-conf with an SPV wallet or an old client. They won't know why they got defrauded; they'll only know that they lost money because Bitcoin's behavior was counterintuitive and unpredictable.




No, they do not define or represent economic consensus. They just follow it. 
Perhaps instead of "entirely wrong" you mean "not entirely right"?
@_date: 2015-10-28 03:42:39


 ... That's a doubling in 5 years. Nowhere near Nielsen's law.
It's also just their projection. I couldn't see what data they were basing it off of. I noticed that their projection is for much slower improvement than their data showed for the previous 5 years, and I didn't see any justifications for the difference. Also, their data from the previous 5 years seems to have conflated mobile and fixed line bandwidth, much like the Akamai results did.


Valid. However, many home users have traffic caps (e.g. 300 GB on Comcast). I would expect the caps to increase to not hinder the average home user. If non-bitcoin traffic increases, then I would expect the amount of traffic available to Bitcoin would also increase. In a sense, traffic is a more appropriate measure than bandwidth, although really what we want is permitted traffic, not total traffic, which is not readily measurable.




Even if we don't have fast O(1) block propagation done in 20 years, an end user does not need to propagate blocks quickly. They don't care what the block transmission time is across their network connection, as long as it's not more than 10 minutes. Really, the entry level only needs to be able to download transactions and blocks. Their connection does not need to be symmetric.
I do not think you need anywhere near 20 Mbps to run a full node right now. My full nodes average around 0.5 Mbps (symmetrical) of traffic when running both a full node and a p2pool server on the same machine. That's with 44 active connections, plus whatever the p2pool process has.
Scaling that up 8000x gives 4 Gbps. If you decrease the number of active connections to 11 and take out p2pool, you'd probably end up with about 0.5 Gbps.
If you're talking about miners needing to be able to upload a block quickly, then they can do that using the relay network or p2pool without having a 20 Mbps uplink. Miners can also afford to spend a lot more money or time on setting up a high-bandwidth link than end-users can. 
If the network can't handle 8 GB blocks quickly enough, then miners who create 8 GB blocks will get their work orphaned. 


You're saying "optimistic", but I think you mean the other thing. 
You're claiming that a peer needs to upload a block to 25 peers within 10 seconds, and that defines an "optimistic minimum bandwidth". I do not think this is reasonable, because it (a) assumes none of the low-hanging fruit in block propagation is used (no encoding blocks as a list of transaction hashes instead of full blocks, no use of weak blocks, no use of IBLTs, no use of torrent-style block propagation), and (b) 100% of block uploading would end up being done by 4% of the nodes. I do not believe this is accurate even for today. From watching the bandwidth use on my network, I've gotten the impression that my three nodes are only able to upload a recently-downloaded block an average of 5 times before all their peers have it. That's 5 uploads for 3 downloads. Statistically, the average node would upload 1 time for each download. (My nodes are well above average in their bandwidth. I've never seen our network connections saturate except when I bittorrented bootstrap.dat.)


No, that is pessimistic. If you just transmit the hash of the transaction instead of the full transaction, you can get about 15x. If you only use the first 10 bytes of the hash (like the relay network), you get about 50x. Those are not O(1) propagation, they're still O(n) propagation. With O(1) propagation, you would be able to send an 8 GB block in about 1 kB to 1 MB of traffic. That's 8000x, not 5x.
@_date: 2015-11-07 07:52:26


It is not possible to increase the block size without a hard fork. A hard fork is where something that was previously prohibited in Bitcoin (like creating a block larger than 1 MB) becomes allowed. A soft fork is where something that was previously allowed becomes prohibited.


With a 1 MB limit on the size of each block, we can only have 1 MB of Bitcoin transactions confirmed every 10 minutes. That corresponds to about 3.5 transactions per second on the whole Bitcoin network. With an 8 MB block size limit, we could have up to 28 transactions per second.
BIP101 starts with an 8 MB limit. It increases the maximum allowable block size by a factor of 2 every 2 years, ending with 8 GB blocks in 2036. This would correspond to about 28,000 transactions per second, which is approximately the level of performance of a major credit card processor. 
Miners always have the option to create blocks that are smaller than the limit.
There is debate currently about whether computers could handle transaction volume and block sizes that high. I think it is feasible and will not compromise the security or usability of Bitcoin, whereas I think that keeping Bitcoin limited to 1 MB would compromise both. Others, including the moderator of this discussion forum and about half of the Bitcoin developers, hold the opposite opinion.
@_date: 2015-10-25 16:08:14


We aren't using a full broadcast network. More people use SPV wallets than use full nodes. The proportion of SPV wallets will increase even if the block size limit remains 1 MB. 
If you're going to use the O(n) notation, you have to define what n is. 
@_date: 2016-05-07 08:24:44
This is another possibility, which I think is unlikely. 
I expect a lot of miners would be willing to continue mining for a few weeks in the event of a price crash in the hope that Bitcoin recovers. Miners have an interest in having Bitcoin survive, due to the fact that their hardware serves no other purpose. Also, a lot of miners (especially in China) are locked into long-term power contracts which they have to pay regardless of whether they use the power.
Overall, though, I expect relatively few miners to stop mining during this halving because overall, miner profit margins are still relatively fat. The 2020 halving may be difficult, but I expect this one will not be.
@_date: 2015-10-08 05:04:58
Here's one of my nodes:
    p2pool date; bin/bitcoin-cli getmempoolinfo; cat ~/.bitcoin/bitcoin.conf
    Wed Oct  7 22:00:00 PDT 2015
    {
        "size" : 581,
        "bytes" : 1098951
    }
    p2pool cat ~/.bitcoin/bitcoin.conf
    server=1
    rpcuser=[redacted]
    rpcpassword=[redacted]
    
    maxconnections=125 # 125 is the default, donât go below 8
    
    minrelaytxfee=0.00003
    mintxfee=0.00003
    maxmempooltx=50000
    blockmaxsize=990000 
        
    addnode=10.0.0.1
    addnode=10.0.1.2
I've got 581 tx in mempool, or a little more than 1 MB. That's nowhere near the 50,000 tx limit before my node starts to randomly forget transactions.
@_date: 2015-10-30 04:29:45
No block size BIPs have won. 
- BIP100 has a lot of expressed support among miners (via coinbase message voting), but it has not been implemented.
- BIP101 was implemented in BitcoinXT and in the BigBlocksOnly patch to Core, but it has not yet gained much support among miners. It is not clear to what extent the lack of support for BIP101 is a vote for the Bitcoin Core development team vs a vote against the technical aspects of BIP101 (e.g. the 20-year block growth pattern).
- Consensus among miners clearly supports an immediate increase to at least 8 MB blocks, which both BIP100 and BIP101 provide. The Bitcoin Core project has several developers who strongly oppose 8 MB blocks, making it currently impossible for any changes to support 8 MB blocks to get merged due to the consensus rules followed by Bitcoin Core. Most of the developers who support &gt;= 8 MB blocks have left Bitcoin Core in favor of BitcoinXT; for the most part, only Jeff Garzik remains.
The results of miner voting on blocksize changes can be seen here:


As for this, when this developer exodus happened, the moderators of this board (primarily theymos) decided that BitcoinXT was an altcoin, and banned discussion of it. A temporary ban was also placed on all discussion of blocksize-related issues. That ban was subsequently partially lifted. Currently, blocksize issues can only be discussed in this weekly thread on this forum.
@_date: 2015-11-11 02:26:45
It looks like my first comment got automodded away. Maybe it will show up later? Maybe not.
@_date: 2015-10-25 16:17:45


It is possible (but expensive) to build a system capable of handling 8 GB blocks with today's technology. For miners with more than about 0.1% of the network hashrate, the cost of building and running this system would be much less than the tx fee revenue it brings in. We're just hoping that the cost of doing so will fall over 20 years enough for non-mining full nodes to not be prohibitively expensive.
The goal of the 20 year schedule is to minimize the expected number of hard forks. If we use a 5 year schedule, then we have to perform another hard fork after 5 years. If we use a 20 year schedule and it proves either too optimistic or too conservative after 5 years, then we have to hard fork after 5 years. If it's right on the money, then we don't have to hard fork. Thus, the cost of making an incorrect guess is essentially the same as making no guess at all: we have to hard fork. (Actually, a soft fork would be an option if the original guess was too optimistic, which is even easier.)
@_date: 2015-10-24 18:41:53


They don't need to settle every month. Six months is possible too. I just thought that monthly settlement would make a bit more sense to accountants


I get 1.92 MB. (100000000 * 500/(6 * 30 * 24 * 6))
@_date: 2016-05-28 00:23:46
Someone has been doing that since October 2015. They rebroadcast the October spam attack transactions usually 2x per day. If you examine those transactions, you'll find that they're all 14,780 Â± 50 bytes and pay a 15,000 satoshi fee. Don't pay attention to those spam broadcasts, as all miners have been ignoring them since October by using the minrelaytxfee command line/bitcoin.conf option.
Cool graph, though.
@_date: 2015-11-08 13:25:48
Oh, I see.
@_date: 2015-11-09 04:27:46
We are currently using testnet3. It is quite easy to create another testnet for testing other things. We can create one testnet per BIP if we wanted to. We can create 10 testnets per BIP too, each one to test another aspect. We won't do that because most BIPs don't have much support. 
BIP101 is the only big block proposal that has been implemented, so it's the only one that can be tested.
It's also, you know, the best.
@_date: 2015-11-08 02:04:08
Can you put some estimated numbers behind your qualitative arguments? 
When I've done the math on the hardware/bandwidth costs to a solo miner of handling very large blocks with about 1/400th of the total network hashrate (my size), I've seen that the costs amount to approximately 1/100th of the current fee/kB level we see right now.
I fail to see how a 100x reduction in fee/kB is inevitable in a future in which the blocksize cap exceeds the demand when the historical trend (in USD) is flat for a past in which the blocksize cap exceeds the demand. You are arguing for a future based on a theory when the past doesn't match that theory.
@_date: 2015-10-24 18:16:25
We should adopt a scaling schedule that gives us enough time to detect and fix issues before they become crippling. We should not expect all problems to be fixed before beginning to scale at all.
@_date: 2015-10-21 19:46:08
The cost of putting a transaction in a block is the marginal orphan risk for a block. If your block gets orphaned, then you lose all of what you spent creating that block. The cost of creating a block is the power cost plus the amortized capital cost plus the network connectivity cost plus the rent plus the labor cost. Thus, wasted electricity is part of the marginal cost of putting a transaction in a block.
@_date: 2017-05-10 03:34:39
The cost of the hardware was not a component of the 2-4% calculation. Neither was labor, shipping, and handling. 
@_date: 2015-10-24 21:31:13
I have seen no such slowing trend in Nielsen's law. My internet connection now is about 50x (download) to 150x (upload) faster than the one I had 10 years ago. Gigabit internet is already being deployed in some regions.
Even if Nielsen's law collapses after a few years, bandwidth should not be an issue. With O(1) block propagation, or with close approximations like the relay network, a 1 Gbps connection should be able to handle 8 GB blocks with about an 8x margin for overhead. Nielsen's law predicts a 100 Gbps connection would be as widespread and cost as much in 2036 as a 30 Mbps connection does today. We don't need that to be true.
I agree that Moore's law has been falling apart, especially in the consumer space. (Server CPUs have been scaling better, since server loads are more conducive to many-core execution. You can get a 14-core Xeon E5 2683 for under $400 now, for example.) Moore's law is starting to get close to fundamental limitations -- e.g. the size of a silicon atom at 0.2 nm -- which is causing it to flatten out. However, we have more headroom in ECDSA verification speed than in bandwidth. My understanding is that current mid-range CPUs can do about 20k verifies per second with libsecp256k1, whereas 8 GB blocks would require around 53k/sec as a sustained average. If technology advancement stagnates completely in the CPU speed realm, we might have to use the $400 E5 2683, possibly in a dual CPU configuration in order to get enough headroom.


Only 20-year.
@_date: 2015-10-02 22:31:50
Many people claim that the fee per tx needs to increase in order for us to be able to pay miners to secure the network. I do not think this is the case, as I do not think that price increases alone will be sufficient or necessary to pay for mining. I think this will become clearer if we look at actual numbers.
Currently, the Bitcoin mining network uses about 300 MW of power. Let's say the average miner gets power for 4Â¢/kWh, and pays another 3Â¢ on other costs like labor, land leases, and a very slim profit. That means that the cost of mining is about $3500 per block. In order to ensure that the mining network remains secure, we have to ensure that the block subsidy stays above about $3500, or around 15 BTC.
With average block sizes around 500 kB, we are getting about 1000 transactions per block. In order to fund mining with blocks this size, we would need to have fees around $3.50 per transaction. Currently, transaction fees are around $0.05/tx, so this would require a 70x increase in fees.
On the other hand, we could also keep transaction fees the same and increase block sizes 70x, to about 35 MB average block sizes (~70 MB cap). For a miner of my size (0.25% of the network) using today's technology, this would result in bandwidth costs using about 1% of our total budget. Basically, we'd have to pay about 2x as much on bandwidth as we currently do. (Smaller miners would typically use a pool anyway, and won't have to pay bandwidth costs.)
35 MB blocks with $0.05 per tx in fees sounds more reasonable to me than 0.5 MB blocks with $3.50 per tx in fees. I think that if transactions cost $3.50 each, people would not use Bitcoin, and would either create altcoins or use Western Union. On the other hand, if we keep the cost of each transaction approximately constant (and low), I think it plausible that demand will increase 70x over 5 years.
We can also reduce the cost of mining at 450 PH/s by improving the efficiency of mining hardware. Currently, that 300 MW is based on an assumption o 0.67 J/GH miners, which I estimate is what the average miner uses. The best miner on the market uses only 0.25 J/GH, and a few have been announced (but not shown) with efficiency below 0.1 J/GH. By improving the efficiency of mining, we could probably reduce the cost of mining over the next few years by 5x without reducing the hashrate.
With 5x improvement in mining efficiency over 5 years, we might our options be a spectrum between 7 MB and $0.05/tx to 0.5 MB and $0.70/tx. Both of those points sound plausible to me, but the end with 7 MB and $0.05 sounds both more plausible and more desirable.
A bitcoin network that is capable of high transaction volumes and low fees is inherently more valuable than one that is capable of low transaction volumes at high fees. I suggest we build that. While we might be able to fund Bitcoin mining with 7 MB blocks at current fee levels, that would require a lot of dominos to fall perfectly. I think we should aim for 15 MB average block sizes with $0.05/tx fees by 2020, when the block reward halves again to 6.25 BTC. At that point, if the price remains constant, we would have $1500 or 6.25 BTC in transaction fees and 6.25 BTC in block subsidy. 
@_date: 2015-11-09 03:58:08




It's actually really just me. I activated BIP101 on testnet on a whim last night. I then asked for help getting BIP101 full nodes on testnet, because I later realized there aren't enough to relay big blocks to each other. For example, my XT full node had 8 outgoing connections, of which 0 were to other XT nodes. That would have made for a pretty boring test. It's still pretty bad, but I think now I have at least 1 natural XT connection.
@_date: 2017-05-27 00:55:22
Jurisdictions. Bitmain only has a patent in China. The other patents appear to have priority in non-China jurisdictions.
@_date: 2015-10-05 14:02:47
The Bitcoin Foundation is dead. I'm glad to see that 21 is stepping in to support Bitnodes in their absence.
Edit: The Bitcoin Foundation is not quite dead. However, they are limited in resources and may not be financially viable.
@_date: 2015-10-25 23:33:34
 (currently out of stock -- check again later)
 (shameless plug)
Mining is not guaranteed to make a profit. It's a gamble. Keep that in mind.
@_date: 2015-10-07 10:51:15
The key requirement here is "If a majority of miners do this." This falls under one of many different types of 51% attacks, and as such, it does not present any additional risk beyond what we already know. In my opinion, this presents a smaller risk than most other 51% attacks, since blocks mined by these rule-breaking miners will be rejected by honest full nodes, and non-mining full node operators have no incentive to cooperate with dishonest miners. As soon as someone notices that miners are trying to attack SPV clients this way, people will publish their findings and SPV users will point their clients to honest full nodes for data, and will not see the longer (but invalid) chain at all.
@_date: 2015-11-06 08:03:52


No, I think you're misunderstanding the design of Bitcoin.
Full nodes verify blocks before relaying them. There is no good reason to trust someone else's full node to do that verification correctly. If there is something you want verified, you should do it yourself. It is very simple and only moderately expensive to attack the network by creating a bunch of fake nodes that simply relay information regardless of whether it's valid.
Adding more full nodes doesn't help the network's security except insofar that it makes this fake node attack (a "Sybil attack") slightly more expensive.
Mining nodes are full nodes. The Bitcoin network does not need any non-mining full nodes. Non-mining full nodes contribute very little to Bitcoin security, since all they provide is a bunch of data which you can't trust to be accurate or verified.
If you want to use Bitcoin, you (or your software) has to verify any information that it uses. SPV wallets exist because it's possible to do limited verification on specific transactions without downloading the full blocks, due to the Merkle tree structure that holds the transactions. An SPV wallet can't prove that money in an address hasn't been spent without downloading the whole blockchain, and also can't prove that they have identified all of the money in an address, but they can prove that a specific transaction was a valid part of the blockchain.
Mining nodes are kept honest by the fact that if they don't follow the consensus rules which govern the blockchain, regular users will notice, and will reject those blocks. If miners want to use the coins they generate, they have to follow the same ruleset as the people they want to spend them with.


The average blocksize is what matters. Empty blocks are the result of inefficient block propagation and validation code. We can improve that a lot with some dedication to performance optimizations, but it will take time, and we will probably never get rid of them entirely as long as many new coins are being generated. When we think about the block size limit, we need to set it high enough that the average blocksize will be sufficient for our needs even when some blocks are empty.
@_date: 2015-10-02 12:47:44
With no block size limit (or a sufficiently large one, like with BIP101), an economic actor can rely on the supply adjusting to meet demand like most economic systems. Miners prefer not to create large blocks because large blocks result in higher orphan rates and lost revenue. That creates a natural fee market tied to the actual cost of producing the resource of block space.
BIP100, on the other hand, is a cartel of miners choosing an artificial limit on block space. The 1 MB limit is equivalent to a non-elected central government applying a production quota for the economy.
@_date: 2015-11-07 08:13:51
A full node is a computer that contains an entire copy of the Bitcoin blockchain, which is currently about 50 GB in size. A full node will check every transaction that has ever been performed with Bitcoin since it was created to ensure that it conforms with the protocol's rules, and stores all of these transactions forever. This takes a lot of storage space, bandwidth, and processor time. A fast computer running a full node will take about 1 day to synchronize with the network from scratch. 
Lightweight wallets, or SPV wallets, only store and verify the transactions that you use. They take about one to ten minutes to synchronize with the network, and only use a few megabytes of storage.
You may notice that theymos gave you a few links to articles that express his viewpoints. These links were to bitcoin.it and bitcoin.org. Keep in mind that both of these websites are owned by theymos (as well as bitcointalk and reddit.comand are moderated according to his viewpoint. While all of these websites contain some useful information, they are not independent or unbiased sources and should be interpreted accordingly.
For example, I disagree with a lot of the content in the "Economic strength" section that theymos linked to as "totally insecure". The argument that's made there states that if most users used SPV wallets, miners could do whatever they wanted and people wouldn't notice. I think that this is not true, since any attempt to do so would be observed by the minority that did run a full node, and they would notify the rest of the Bitcoin users through things like reddit.comIRC, and the news media. That is what has happened in previous crises with Bitcoin mining, like the version [0.8 fork]( which was detected and resolved within hours, and I see no reason to suspect that future events wouldn't play out similarly even with a small minority of users running full nodes.
@_date: 2015-10-22 01:34:54
Perhaps to some extent. If the cost of operating a full node gets large, then I would expect that the SPV wallet/full node ratio would also get large. This means that a Sybil attacker would need to serve a number of SPV clients that is proportionate to the number of full nodes that the attacker is emulating. Depending on the type of attack, the attacker may be able to reduce the machine load per request by fabricating data instead of retrieving it. Is this what you had in mind? Or were you referring to the fact that the attacker only has to store and (maybe) validate one copy of the blockchain?
@_date: 2015-10-03 02:12:23


 
My company pays about $0.025/kWh. 
You can get power for as little as $0.02 (typically $0.025) in Washington State and northern Canada. In Iceland, you pay about $0.035. I don't know about Sweden, but I think it's probably similar. In Georgia (Bitfury), you might pay a little more, like $0.05. Different parts of China have different rates, some of them as low as $0.02 or slightly below, e.g. in the south Sichuan province. Venezuela, UAE and a few other oil producing states also have rates around $0.02 for dirty power that nobody should use. If you're paying more than $0.06/kWh, you will get out-competed.


Currently, 1 MB limit (0.5 MB average block) is barely enough for about 200k users. With 200M users and the same number of tx per user per day, we would need about 1 GB blocks. 
@_date: 2015-10-05 04:43:07


It depends on if the 20% is rejecting blocks by the 80% or simply accepting (and possibly creating) invalid blocks.
If the bug is rejecting valid blocks, then you get a persistent fork with 20% of the hashrate. SPV clients will follow the longest chain if they canvas enough nodes to notice a difference, or if they are started up again connecting to a different node. Once the bug is fixed, clients will reorg and follow the 80%.
If it's creating and accepting invalid blocks, then you have a fork that only lasts as long as it takes for the 80% to overtake it, which will usually be within 15 minutes. Anyone relying on 1-conf may get some nasty surprises unless they're running a full node with the non-bugged software. Clients running versions with the bug will not have any invalid results with more than 6 confirmations, nor will anyone else.
With a 50/50 split, the results are much more serious for both types of bugs.
If the bug is rejecting valid blocks, then you get a potentially successful 51% attack attempt against all non-bugged miners and clients.
If the bug is creating invalid blocks, this could result in the invalid chain being longer than the valid chain, which would compromise SPV clients as well as making bugged clients report invalid 6-conf transactions.
I don't consider those differences to be negligible, but if you do, I suppose that's fine.
@_date: 2015-10-05 04:11:13
I think he means that a fork due to a bug in one implementation might cause a fork, but the fork would likely not have more than 50% of the network hashrate as happened with the SPV mining BIP66 issue. The bug would likely be only in 20% or 40% of the implementations (and nodes) instead of nearly 100%.
@_date: 2015-10-21 01:39:37
1) In my opinion, we don't need 5,000 full nodes to keep Bitcoin secure. ~~The number of full nodes needed to protect against a Sybil attack is not absolute; the more expensive it is to run a full node, the more expensive it is to run a Sybil attack.~~ [Contested -- see muyuu's post below.]
Running a full node provides very little in terms of consensus or security. All full nodes do is relay valid transactions and blocks and respond to queries about the contents of blocks.
Running a miner on a small pool, on p2pool, or as a solo miner helps a lot more.
Caveat: Companies or individuals that transact in high volumes with anonymous or untrusted customers should either run a full node or (if possible) find a full node operator that they trust. Most others can get away with using SPV wallets. (Luke-jr will probably disagree, and say that anybody who transacts with untrusted customers should run a full node, and most people who don't as well.)
2) A smaller, slower increase would limit Bitcoin's usability and growth. I think it will also result in less total miner fees, which will result in lower security of the Bitcoin network, although that depends on the exact shape of the supply and demand curves.
Edit: But wait, there's more!
3) According to the consensus rules used by the developers of Bitcoin Core, we are nowhere near consensus. Consensus means that there is a proposal that not a single developer of Bitcoin Core vetos. That is a very high bar to reach. Not only is there not currently agreement by 100% of developers that the blocksize should be increased at all (luke-jr for example is opposed to any increase, and thinks the blocksize limit is currently too high), but developers also are nowhere near to agreeing about how much of an increase there should be, or what the schedule should be. Furthermore, the developers are not in agreement about how such an increase should be implemented -- should it be on a regular schedule, or based on miner voting? I, for example, think that voting adds a ton of unnecessary complexity and unpredictability, and think that miners should be building their infrastructure to keep up with the blocksize schedule instead of the other way around. Voting systems create a chicken-and-egg problem: a miner won't vote for an increase until they have the infrastructure in place, and they won't spend the money to build the infrastructure until it has passed the vote.
We've been debating this issue for years now, and we haven't yet made much progress on the issue among the developers. On the other hand, bitcoin users overwhelmingly support a blocksize increase. As a result of this deadlock, two of the main developers (Gavin and Mike) have abandoned the Bitcoin Core project and forked off a separate branch. If I name this branch in this forum, my comment will likely be deleted, and I might get banned, so I won't say its name. (The mods of this reddit think it's a very bad thing.) This branch does not follow the any-dev-can-veto rule, and instead follows the Mike-Hearn-makes-the-final-call-and-if-you-don't-like-his-decision-you-can-make-another-fork rule. This is the same type of rule that is used for developing the Linux kernel, the Python programming language, the Windows operating system, the Mac OS X operating system, and ... well, pretty much all other software. Gavin coded up an implementation of BIP101 for this fork Bitcoin client, and a few people are using it now. However, this client does not have enough support among miners to activate large blocks, mostly because most miners and users think that a blocksize increase should come out of the developers of the Bitcoin Core version of the software, and should come out of the any-dev-can-veto consensus process.
The result is that nothing is happening.
@_date: 2015-11-06 21:03:42


Miners *are* the network of nodes. There is no technical need for any non-mining nodes for Bitcoin to work, and there never has been. The role of non-mining full nodes is economic: they support non-mining users and their wallets, and they distribute the load of SPV wallets. Non-mining nodes can be in between mining nodes in the connections network, or they can be on the side, it doesn't matter. 
In theory, it's possible to have every node connected to every other node. In practice, the number of connections per node is usually somewhere between 8 and 125. My nodes have around 45, for example.


Nodes don't have the power to decide for anybody except themselves and those who trust them. They never have had any other power.
Note that even without the relay network, many mining nodes will connect directly to each other through the standard slow Bitcoin p2p protocol. Also note that non-mining nodes can also connect to the relay network. The only thing that the relay network does differently is it makes it all happen much faster and more efficiently, in parallel with the main p2p protocol.
@_date: 2015-11-09 04:02:19
Please try to spell it properly! jtoomim, not jtoomin. Tagging an empty account isn't very effective.
@_date: 2015-10-25 00:02:18
LN transactions derive much of their value from the promise of a maturity in on-blockchain Bitcoin after some period of time. A coin that's locked away for 5 years may have a different value from a coin that's locked away for 1 month. 
Personally, I would be reluctant to commit to only using some of my value on the LN for such a long time. Bitcoin itself hasn't been around much longer than 5 years.
I think that if 100 million people were using LN, there would be a lot of demand for non-LN bitcoin transactions, either due to the transparency benefits, the requirement for synchronous communication with LN, or something else. I roughly estimate that about 10% of users would want to use one of these features an average of once per month. If so, that's another 11.6 MB per block of transactions.
@_date: 2015-10-21 17:15:42
The equilibrium point for orphan rates is determined by individual miner incentives and tradeoffs, not by the industry-wide costs. Network-level performance is the result of individual decisions. For the individual miner, the tradeoffs are hashrate expense vs. bandwidth expense vs. orphan rates. 
You can ignore the electricity costs if you want, as long as you don't mind making woefully inaccurate estimates of the orphan rate equilibrium.
@_date: 2015-10-24 00:06:43




The scenario described above doesn't work. The incentive to move forward is not challenged by large blocksizes and low block subsidies
Let's say some miners are adopting that strategy. Ten different miners have published blocks at block height X. These blocks vary from 10% full to 100% full. An eleventh miner is now mining a block. This miner could make a 100% full block at height X, but the chance that it would be part of the main chain is very small. Alternately, he could build at height X+1, which gives him a much better chance. If he does so, he can choose whichever block to build on that he wants. If he's selfish, he'll choose the smallest block, since that leaves the most fees for him. This means that large blocks will have a much higher chance to lose orphan races. This makes the strategy of intentionally orphaning blocks a non-starter unless the miner has a large enough proportion for selfish mining effects to be relevant.
@_date: 2015-10-05 23:26:02


I see it the other way around. With a soft fork, you have to trust the developers and miners, as they are the sole determinants of what chain you use. With a hard fork, all users have a choice about which fork they want to use. 
@_date: 2015-10-21 01:59:02
Here it is: 
@_date: 2016-05-27 22:55:13


Exponentially, no. Quadratically, maybe, but it depends on how they increase the block size. A linear increase is also easy to achieve without breaking anything.
Keep in mind that they have in mind SegWit-style transactions, not current-style transactions with monolithic tx+sig, for most of their roadmap. SegWit style transactions can be as large as you want without running into the validation cost problems. However, most people seem to understand that the block size hard fork would be an increase in the maximum amount of space for all transactions, not just SegWit transactions.
If you increase the block size limit for legacy transactions to 2 MB, but keep the limit per transaction to 1 MB, then the worst case validation cost increases from 19.1 GB of sighash to 38.2 GB of sighash. If you increase the legacy block size to 2 MB with a 2 MB max transaction size, then you get approximately 76 GB of sighash in the worst case. You could also add a limit to bytes hashed like Classic's and keep the sighash at 19.1 GB per block, and allow 2 MB transactions as long as they don't exceed 19.1 GB of sighash per block. All of these approaches would allow all hypothetical 1 MB 200-checksig timelocked DoS transactions to remain valid.
If you add SegWit on top of any of those, you get the same worst-case sighash results, but potentially worse bandwidth exhaustion attacks. For 2 MB legacy per-tx, 2 MB legacy blocks, that would mean that you would get around 3.4 to 3.8 MB of typical capacity with 100% SegWit transactions (1.4 MB to 1.8 MB in the witness block), 8 MB of data in bandwidth/storage attack conditions, or 76 GB of sighash in the worst case. 
If the 2017 hardfork has a sighash limit like Classic's, then the sighash worst case goes away.
Basically, as far as I have seen (though I might have missed it), Luke, Adam, Matt, and Greg have not addressed the sighash issue in their hardfork proposals. They seem to have asserted that it goes away once you have SegWit. While that has an element of truth to it (SegWit-style transactions do not have the n^2 validation cost bug at all), it misses the point that an attacker will use the weakest transaction format, not the strongest. The blockchain is only as strong as the weakest link.
@_date: 2016-05-28 06:52:45








Your tone is not civil, and you avoided answering the one question I directly asked you. I will not be continuing this conversation with you.
@_date: 2015-10-29 17:17:39


Not with that kind of thinking.
@_date: 2015-10-05 21:30:13
Luke, your description isn't quite accurate.
Soft forks can be deployed by only a miner majority, because blocks that conform to the old rules but not the new rules will get orphaned. SPV clients and old clients will not be able to distinguish between valid and invalid blocks until long after they've been orphaned. Transactions can be crafted which will only be incorporated into orphaned blocks, allowing for reliable, inexpensive attacks against anyone accepting 0- and 1-conf.
Hard forks can be deployed by only a miner majority, because blocks that conform to the old rules but not the new rules will be on a shorter chain, and will be ignored by new clients as well as SPV clients. Only old clients will use the short fork. These clients will see a different block height, different transactions, and very slow confirmation times for most transactions, and will likely notice that they are no longer part of the economic consensus. However, their clients and their network will still be functional on their forkcoin.
In my assessment, this means that soft forks have greater (and less transparent) disruption when contentious than hard forks. I do not think a soft fork should be attempted without a high activation threshold.
@_date: 2015-11-11 10:59:12
The issue with the identity crisis is due to the fact that testnet allows difficulty 1 work in a lot of circumstances (e.g. when hashrate falls to zero), whereas our fork was doing difficulty 100,000+ work (because we were hashing on it with GH/s miners). This let the Core fork overtake the BIP101 fork in terms of block count, but not proof of work.
Most of the block explorers were ignoring the BIP101 fork anyway because they were configured to follow nodes that reject large blocks.
This is not an identity crisis. It's an identification crisis. People who aren't running BIP101 full nodes are having trouble watching what's going on with the BIP101 testing. This is not actually a problem for the security of our chain; our nodes have been agreeing with each other except when they haven't been connected to each other.
EDIT: This post was accurate as of when I went to sleep, but after I went to sleep someone apparently hashed on the legacy chain and overtook the BIP101 chain in terms of proof of work. I wrote the post after waking up and before checking the current chain state. Now block explorers are showing two different chains, one with v4 blocks (BIP065 activation), and one with v3 blocks and BIP101 blocks.
@_date: 2015-11-09 16:17:04
Regtest is more predictable and more restricted. It's basically a private blockchain where mining difficulty is zero.
@_date: 2015-11-05 07:20:50
It's to solve a serious problem in relativity.
Specifically, it's to solve the problem that there is no such thing as absolute time. Time can only be defined in a meaningful way within one reference frame. When you have a bunch of servers communicating with each other, it's hard to make them all agree on what order events happened in. Server 1 might see A happen before B, but server 2 might see B happen before A. 
The blockchain is a way to get everyone to agree on the timeline for events. In the case of Bitcoin, those events are transactions. Bitcoin makes timestamps by creating a computationally difficult math problem that takes a while to solve. Each time someone solves the math problem, the solution marks a different timestamp. Bitcoin makes a bunch of arbitrary data piggyback upon that solution, like a list of transactions and a link to the previous solution. This allows the solutions to be linked together into a chain, and allows for transactions to be put into an order that everybody can agree upon.
Unfortunately, the way this computationally difficult math problem is used puts heavy constraints on what kind of problem it can be. It has to be fair to each person who tries to run it, so it has to be a cryptographically secure well-defined and well-studied problem with little or no optimization options available. It has to be able to summarize and depend on arbitrary data. It has to not change over time. And lastly, it has to be very slow to find a correct answer, but fast to verify a correct answer. All these constraints together mean that there are very few known and viable options other than the use of cryptographic hash functions like SHA256 or scrypt. 
@_date: 2015-10-31 18:44:40
Use the Coinbase Exchange, exchange.coinbase.com. You're already mostly set up for it. You just need to transfer BTC or USD from your Coinbase wallet to the Exchange wallet, then log in to the Exchange and start trading.
@_date: 2017-05-12 00:12:26
250 GB per month / Two transfers per block (upload + download) / 2x overhead (assuming you're using -blocksonly mode) / 144 blocks per day / 30 days per month = 14.4 MB per block. So 2-4 MB should be fine for now, right?
@_date: 2015-10-28 03:49:41


I expect the number of full nodes to roughly scale with the square root of the number of users. I think we have about 100k users right now with 5k full nodes. I think if we had 1B users, we would have about 500k full nodes, or one full node for every 2k users.
I don't see any problems with this, as I don't see any of the purported attacks against SPV clients as being practical. Also, those attacks (mostly variants of a Sybil attack) become more expensive as the network grows.
@_date: 2015-10-21 19:56:53




You're half-right. I was referring to the peer-to-peer protocol, which is the part that serves the network, rather than the RPC protocol, which you can only use if you have the RPC user/pass for that full node (and usually only from localhost). In the sense of providing security to the network as a whole, the JSON RPC interface does not help, because its use is effectively restricted to the owner of the node.
When I said "no API to query **a node**", I meant a random node, not a node that you own and control. I could have been clearer on that point.


 
It's both a bad thing and a good thing. My opinion is that the bad outweighs the good.






Here's a 1-step protection against that attack:
1. Fork.
@_date: 2016-05-27 19:15:13


Yes. The reason that Classic makes some possible attack transactions invalid is because Classic has a soft fork and a hard fork rolled into one. The 1.3 GB per-block hashing limit is a soft fork, not a hard fork. It is there to make invalid any block that would take longer than about 10 seconds to validate. 
Currently (and with Segwit), it is possible to make a block that would hang bitcoind nodes for about 3 minutes; the Classic team decided to fix that worst-case attack scenario, whereas the Segwit team decided to prevent it from getting any worse.
@_date: 2016-05-28 06:27:21


Do you consider a 3 minute block validation time to "fixed"? With the SegWit soft fork, you can still make a block that takes 3 minutes to verify. With Classic's dual fork, you cannot.
I'm aware that most of the SegWit proponents claim that SegWit's solution to the O( n^2 ) sighash problem is more elegant and more of a fix. In some ways, that's true, but ultimately SegWit's solution doesn't fix the vulnerability at all because SegWit does not disallow the abuses of the current transaction format. It doesn't matter how many ways you can craft a transaction with O(n) scaling as long as there is still one way that an attacker can intentionally craft one with O( n^2 ). All SegWit's solution does is keep the attack transaction size limited to a 1 MB legacy transaction instead of using the full 4 MB base+witness size.
Note that for "fixing the scaling characteristics", there is already a way to make extremely large transactions without running into the n^2 sighash problem. You just have to script your transaction with SIGHASH_ANYONECANPAY instead of SIGHASH_ALL. All Classic needed to do was disallow abuses of SIGHASH_ALL in enormous transactions with tons of inputs and possibly multiple CHECKSIG script ops. Capping the actual sighash validation cost achieves that goal pretty effectively, addressing all of the potential attacks, and with no collateral damage to non-malicious entities.
Note: I'm not against SegWit's new transaction format. I like it. However, we still need a sighash limit, because the ability to stall nodes for 3 minutes is too serious a vulnerability to leave in the code.
@_date: 2017-07-18 20:52:26
(910000 B/block) / (448 B/tx) / (510 s/block) = 3.98 tx/sec
(998000 B/block) / (448 B/tx) / (510 s/block) = 4.37 tx/sec
(920000 B/block) / (480 B/tx) / (570 s/block) = 3.36 tx/sec
@_date: 2017-07-10 06:43:32
Here's what I get on my poolserver:
    dd if=/dev/urandom of=tohash bs=1000000 count=100
    time sha256sum tohash
    
    real    0m0.391s
    user    0m0.364s
    sys     0m0.024s
        
That's 256 MB/s. Maybe your laptop is slow?
@_date: 2017-07-06 19:24:56


16 MB blocks with 5Â¢ per transaction (10Â¢/kB) can pay for about 1 EH/s of mining hashrate with current-gen hardware (assuming miners need to be paid 10Â¢/kWh and get 10 GH/J). By the time inflation gets close to zero, I think 16 MB blocks should be well within our technology's capability. We don't need transaction fees to be over a dollar.
@_date: 2017-07-24 18:28:17
Plus $130k/month in lost fee revenue from mining 256 tx blocks. That's a much bigger deal than the bugs.
The thing I don't get is they can get about $40k in savings without doing any of this transaction permutation stuff if you just settle for 2-way collisions instead of going for 4-way collisions. And that will keep working after SegWit, so you don't have to retool anything. It seems to me that they're missing the sweet spot in terms of revenue vs power savings.
@_date: 2015-10-22 00:21:50
Until we fix block relaying to be O(1), there will be a small advantage with very large blocksizes for larger pools and large solo miners, as well as for p2pool users. This centralizing force will be much weaker than the existing centralizing force of excessive reward variance for small pools and solo miners. My bitcoin mine has about 0.25% of the network hashrate. For us, if we used only Bitcoin Core (no relay network or p2pool, both of which we actually use), and without using any VPSs for bandwidth amplification, the bandwidth costs would be about 0.5% of our total expenses with an expected 1% first-hop orphan rate at 10 MB per block. Note: this is how much we already pay for internet. 
So, yes, there will be a very small centralizing force that will apply to miners that don't use p2pool or the Bitcoin relay network or IBLTs. For the next few years, it will be less than 1% in higher costs for solo miners with about 0.25% of the network hashrate. Miners of our size rarely solo mine anyway due to variance. We use p2pool, for example, which has near-O(1) block propagation already.
Once O(1) propagation is deployed in your system, orphan rates no longer increase with block size until your internet connection is no longer able to handle the p2p transaction relay volume. With our current internet connection, we could handle about 1 GB blocksizes (6 GB in theory, but I'm allowing a 6x overhead/safety factor). That's still paying about 0.5% of our budget on internet connectivity. Once we get to 8 GB blocksizes in 2036, yeah, we might have to upgrade our line. Well, if we don't all have 1 Gbps fiber by then.
@_date: 2015-10-06 02:50:48




That is one of the potential costs of code decentralization. You don't have any central authority who says which Bitcoin is the One True Bitcoin. If you want to know where the economic consensus actually lies on an issue, you have to measure it. You have to let the fork happen and see how it plays out. 
Unless a significant minority of people think that the old version has unique merit, it will die out entirely. People want to use the currency that other people use. In the unlikely case that both forks are considered by some to have merit, then both forks will maintain economic value. Pre-fork stakeholders will have a proportional stake in both forks. The currency will then be even more decentralized.
If the developers aren't certain that an economic majority would support a change, they shouldn't implement it as a soft fork. If they are certain, then there shouldn't be any problem implementing it as a hard fork.




The user determines which chain is valid by choosing which version of the code to use (or, in the case of SPV wallets, which full nodes to listen to). With a soft fork, there is only one chain, so users no longer have an effective choice.
I don't think the arguments in this comment mean that hard forks are inherently better than soft forks. I also don't mean that these types of decentralization are necessarily desirable for Bitcoin. I think that the ability of users to choose which fork they want to use and the ability of an economic minority to maintain an old-style chain if they choose is an unimportant feature. I just mention this because I think that your earlier statement -- that "hardforks centralize Bitcoin" and force you to "trust the [sic] some combination of miners and a spokesperson to determine which new set of rules you will follow" -- is the opposite of true, and that it describes softforks accurately instead of hardforks.
@_date: 2017-07-10 05:41:57
 &gt; However, based on estimates of actual transaction usage, this should result in blocks around 2.1MB in size, a 110% systemwide capacity increase. ... Please note around 63% of transaction data is witness data.
Let's check your math, taking that 63% number at face value:
`.63/4 + 0.37 = .5275`
So the typical SegWit transaction has only 52.75% as much weight as the typical non-SegWit transaction, given the discount. That means that we can fit more such transactions into a block:
`1 / 0.5275 = 1.8957`
About 90% more transactions per block, assuming that 100% of transactions use SegWit. That's pretty different from the 110% you claimed.
@_date: 2015-10-05 20:40:41
When the last thing you see is someone going to the hospital bleeding profusely, it's easy to make the mistake later on and think they died when they didn't. 
I heard about the Bitcoin Foundation's governance and budgeting problems, and I saw the one aspect of the Bitcoin Foundation's activities that I cared most about -- the funding of developers -- evaporate. Consequently, I thought they were dead. It appears that my impression was inaccurate. I apologize.
@_date: 2015-10-05 20:43:36
No, they're dependent on mining revenue to pay their $100k/month power bills and their employee salaries. 
@_date: 2015-10-20 00:12:10




This is a repost/continuing conversation from 
The key argument that gmaxwell mades is this:


to move the blockchain forward goes away.  An optimal rational miner
would be best off forking off the current best block in order to capture
its fees, rather than moving the blockchain forward, until they hit
the maximum. That's where the "backlog" comment comes from, since when
there is a sufficient backlog it's better to go forward.  I'm not aware
of specific research into this subquestion; it's somewhat fuzzy because
of uncertainty about the security model. If we try to say that Bitcoin
should work even in the face of most miners being profit-maximizing
instead of altruistically-honest, we must assume the chain will not
more forward so long as a block isn't full.  In reality there is more
altruism than zero; there are public pressures; there is laziness, etc.
I've thought about this a bit more, and I think it won't be a problem. In short, the reason is because a selfish miner will choose to orphan the block with more fees when there's a race, leaving more fees for the selfish miner to take himself. This means that a miner would never want to have the "bigger" block in a block race ("bigger" = more fees).
Let's say miner A finds a 10 MB block (100 MB limit) with 1 BTC of transaction fees at block height 100. A few minutes later, there are now 5 more MB with 0.5 BTC in fees available. Miner B has four options:
1. Mine on top of A's block at height 101.
2. Mine a fork against A's block at height 100. 
3. Mine a fork against A's parent at height 99 or below.
4. Do not mine yet.
Option 1 has an expected revenue of 0.5 BTC per block. That's what we want B to do, but it's not clear that the incentive is there.
Option 2 would put A and B in conflict. If B wins, he gets 1.5 BTC. Since there are two blocks in competition, a naÃ¯ve guess might be that B has a 50% chance of winning, which would suggest that B's expected revenue would be 0.75 BTC per block. However, I don't think the two blocks have an equal probability of being orphaned. A and B both would obviously try to mine on top of their own blocks in an orphan race, but other miners will not choose randomly between the two. Instead, miner C will choose whatever block leaves the most transactions for C to use. That means that C will try to orphan B. This makes it B against everybody else. Option 2 is not a good choice.
Option 3 isn't very interesting. If Miner B includes all of the transactions that are available, then that leaves no transactions for Miner C or A except what come after the block is found. Miners will only choose to mine on top of B's chain if they have reason to believe that B's chain is more likely to win in the long run. Given that it is currently shorter, that is unlikely. Mining non-full blocks at lower heights sounds a little better, but then B's getting no more revenue than the blocks he's orphaning, and still is working at a lower block height and is unlikely to have his blocks survive. 3 is not a good choice.
Option 4 is interesting. This is likely to be chosen a lot in the future, when the electricity cost of mining a block exceeds the expected revenue. If the difficulty drops, then the electricity cost of mining a block drops. This means that the equilibrium difficulty under this scenario will be similar to the revenue available from mining. This will not be dangerous unless total transaction fees are low.
(will you come out of hiding?)
@_date: 2017-07-10 06:10:55
Yeah, I think the "covert ASICBOOST" terminology sucks too. If extranonce grinding is covert ASICBOOST, then SegWit does not block covert ASICBOOST.
@_date: 2015-10-02 12:55:10
Bandwidth is much cheaper than electricity. If my mining company paid as much for bandwidth and storage space as we currently do on electricity, we could afford 8 **GB** blocks right now.
@_date: 2017-07-09 07:54:07
Neither. 
@_date: 2015-11-09 15:46:08
Hold onto it for now, we're going to go back to Core for a bit. Testing the reorg and merging of forks.
@_date: 2015-10-26 17:47:50
This is valid. However, it is easier to reduce the block size limit (soft fork) or average block sizes (miner policy) than it is to increase it (hard fork).
@_date: 2015-10-05 11:32:07


Dude, not cool.
@_date: 2015-10-21 16:38:03




That's why I said "valid transactions". They don't do anything other than relay valid transactions. They silently quash invalid ones. There is currently no API to query a node about the validity of a transaction or block.


The only decentralization that it would reduce is the number of non-mining full nodes. We have way more of those than we need anyway. It will not affect mining decentralization, since (and I'm speaking as a miner here) bandwidth costs are minuscule compared to power, labor, and rent. Big blocks would probably also improve software development decentralization. 
As for security, it would improve mining revenue once the block subsidy is halved and quartered, which is critical for sustaining Bitcoin's security. See 
It's true that increasing the blocksize will make it more expensive to run a wallet on a full node, so the utility of bitcoin for that use case will decrease. Currently, no more than 5185 people use bitcoin this way, while (I estimate) 100k-500k use SPV wallets. With an increase in the blocksize limit, we could increase the number of SPV wallet users to many millions. I don't see it as a good tradeoff to prevent access to millions just to keep things cheap for the &lt; 5185 full node wallet users.




Actually, "consensus" is a word with multiple definitions. Everybody agreeing ("absolute agreement") is only one sense of the word. That's the sense that the Core developers use for their decision making, but it's not the sense that we use when describing consensus on the blockchain. In that case, we use the "general agreement" definition.


There are multiple consensus processes in play. 
One consensus process is the blockchain, or the consensus among miners and full nodes about the state of the ledger. This is determined by the code that miners choose to use. Another consensus process is governing principle behind the development of Bitcoin Core, which is the client that most people consider to be the reference implementation of Bitcoin. As long as people continue to consider it that, then Bitcoin Core determines the features of the Bitcoin network, and the consensus process for Bitcoin Core effectively becomes the process that determines the state of the Bitcoin protocol and the state of the blockchain.
The consensus process for Bitcoin Core is well defined. When a contributor proposes a change to Bitcoin Core, s/he submits a github pull request (a patch), and then the main contributors (sipa, laanwj, jtimon, etc) review the code and respond with either ACK (approved), utACK (read but not tested, approved), or NACK. If one reviewer responds with ACK, none respond with NACK, and several others respond with either ACK or utACK, then the patch will be accepted by wumpus (Wladmir) when he gets around to it. This is an oligarchy (rule of the few) with free veto. This is the governance model we are using as long as we rely on Bitcoin Core to define the protocol.


Free-veto governance models ( have a notoriously poor track record for performance in political processes. They work okay as long as nothing controversial needs to be done, but they fall apart as soon as there is. As long as we are using a Liberum veto/consensus model for Bitcoin development, then Bitcoin's evolution will be hobbled. We will be stuck with the design that was created by the last benevolent dictator of the project -- Satoshi Nakamoto -- until another benevolent dictator comes along with an altcoin that has enough technical advantages over Bitcoin for people to switch over to it.
**When Satoshi handed over the reins for Bitcoin development to Gavin Andresen, one of the first things that Gavin did was set up the free-veto consensus model of governance for the Bitcoin reference client development. Consensus development sounds nice, but it's a recipe for stagnation. Gavin has since realized that this was a mistake, and abandoned the free-veto consensus system. I think it's time the rest of us did the same.**


Core development on the blocksize issue is like a car with tires stuck in the mud. If you step on the gas, something will happen -- the wheels will spin -- but that doesn't mean that progress is getting made. During the debates, some developers -- notably Luke-jr, who thinks that even 1 MB is too large -- have gotten more entrenched in their small-block positions, much like how spinning tires dig themselves deeper in the mud. 
The only actual code to increase the blocksize that has been written was written by someone who has rejected the free-veto consensus model. It took him about two weeks to write the code and test it. The Core team has debated the same issue for several years and still isn't close to a solution.
I think the conclusion is pretty clear: we need to replace the free-veto consensus model in Bitcoin development if we want Bitcoin not to stagnate.
@_date: 2016-05-22 01:01:31
You can calculate what fee would have been necessary for the past 3 blocks. However, calculating the necessary fee for the next 3 blocks requires predicting the future, which is not something humans or computers are generally able to do accurately.
In order to calculate the needed fee, you need to guess things like the following:
1. The number of blocks that will be found in the next x minutes, and the amount of time for each block (not just the average time);
2. Which of those blocks will be "SPV-mined" 1-transaction blocks;
3. The number, size, and fees of the transactions that other people (your competitors) will generate over the next y minutes, where y is the time until your first confirmation;
4. How many of those transactions will be included because of special priority given by the miner (e.g. BTCC gives transactions from their exchange priority on their mining pool).
Now, it's fairly easy to create an algorithm like  that estimates these effects based on past data and gives you a fee/kb number to use. However, if everyone uses the same algorithm, then it stops working. Let's say there's only enough space for 80% of the transactions being generated during peak hours. If everyone looks at the last 10 blocks and decides that a feerate (fee/kb) of 0.0001 BTC/kB was sufficient to be included within 1 block, and they all use that, then 0.0001 BTC/kB will be insufficient for 20% of transactions. If, during those next 10 blocks, they all decide that 0.0002 is sufficient, then 0.0002 would end up being insufficient. It devolves into a **[Keynesian beauty contest]( and outcomes for those are difficult to predict.
@_date: 2017-07-10 04:41:54


Getting a little off-topic here, but liquid cooling is not as good of an idea as it sounds. It increases capital expenses considerably, and it does not significantly change efficiency (which is ultimately a function of voltage and ASIC design), although it does improve performance (which is a function of clockspeed). Bitmain tried liquid cooling with the Antminer C1, and it worked, but it was more expensive per H/s than the plain air-cooled version.
All heat generated by the chip must be moved into air, whether you're using liquid cooling or not. With standard air cooling, you move the heat from the chip to a heatsink to air. With liquid cooling, you move heat from the chip to a waterblock to water to a heatsink to air. With phase-change immersion cooling, you move heat from the chip to 3M Novec fluid to a heatsink to air. The standard air cooling approach is the simplest by far.
Bitfury and other proponents of liquid cooling say that you can save a huge amount of money on cooling because you no longer need air conditioning to cool your hardware, but that's a strawman. Almost no Bitcoin miners use air conditioning. My datacenter uses standard air cooling and has a PUE of around 1.02. That's pretty much the same as Bitfury's immersion cooling datacenter.
@_date: 2015-11-07 21:41:20


This is not how blocks are transmitted today most of the time. The algorithm that most miners use today is this:
1. When a new block is found, grab the 80-byte header of the block using the stratum protocol from that miner. Start mining an empty block on top of it immediately.
2. Try to download the block with Matt Corallo's relay network. The relay network compresses transactions down from the 500-byte raw transaction to a roughly 10-byte compressed hash. The miner can usually reconstruct the block because most of the raw transactions were previously sent. When this works, it results in a 50x reduction in bandwidth needed to send a block. For a 1 MB block, that means about 20 kB. For a 2 GB block, that would mean about 40 MB of data, which would take a 1 Gbps line about 0.32 seconds.
3. If that fails, download the whole raw block. This may take much longer, such as 10 seconds with 1 MB blocks today, but you rarely have to do it. 
4. Validate the block. This step is currently fairly slow due to a lot of code that has not been parallelized yet. It may take 1 second for a 1 MB block.
5. Fill the new block template with transactions. This step is also very slow with current code, and takes about 1 for a 1 MB block, but it will be 5x faster or more within a few months. (I have a version in progress on my computer that is about 3x faster.)
6. Switch from mining an empty block to mining the new full block.
By 2032, I expect we will have some optimizations that make Matt's relay network look extremely wasteful. If we don't, I'll be very disappointed, given that these optimizations have already been described in detail.


I'm a miner. 2.7% is not significant in relation to our profit margins.
Also, if the average miner had an orphan rate of 2.7%, then the cost to the average miner would be 0%. Orphans reduce the mining difficulty, so a high orphan rate makes it easier to mine a block. The net effect of orphan rates is to reward the miners with low orphan rates and to punish the ones with high orphan rates. This means that miners have an incentive to maintain good network connectivity. For a global payment network, this appears to be a desirable trait.
The Chinese miners are complaining about this because their connectivity is worse than most other miners' bandwidth. This makes their votes against increasing the blocksize limit an instance of [regulatory capture](
@_date: 2015-10-02 12:42:15
Bitcoin is a democratic doocracy. Changes happen because code is written, downloaded, and executed. Proof of work makes the change real. Miner votes are the only votes there are. Everything else is an opinion straw poll.
Miners have to pay bills. They are beholden to exchanges and the market to sell their mined coins for fiat to pay electricity and rent. Miners can not afford to do anything which they know will significantly and permanently devalue the currency. Miners are financially bound to the interests of users in this indirect fashion and no other.
I wish there were a way that users could vote in a meaningful and binding matter. Other than things like  and renting hashpower on  there isn't yet.
@_date: 2015-10-07 13:42:38
I think it's more an issue of talking about a problem that isn't really a problem when other problems that are actually problems exist. This hard fork attack is silly, extremely expensive, easy to defend against, and not particularly effective.
@_date: 2015-11-08 01:46:49


Correct, most will care more about features like the security and privacy than about the fees. Very few people will chose a wallet based on it advertising low fees. 
Keep in mind, with versions of Bitcoin-qt before 0.8.2, it was easy to use lower fees than the default 0.0005. It just wasn't usually done because most people didn't care about it enough to do Edit-&gt;Preferences and change the relevant setting. Thinking that people would take 15 minutes to download and configure a special wallet that does the same thing as the 1-minute reconfiguring of the existing wallet that they are currently not doing sounds pretty silly to me. 
Even with the (incorrect) rational market assumption, there are costs other than bandwidth/propagation time with increased block size. Verification of a block and assembly of transactions into a new block both take a significant amount of time, and we will never be able to make them zero. Including a transaction into a candidate block will always have a nonzero cost to mining pool operators.
@_date: 2015-10-22 13:55:14


Do you know if this has been measured?
@_date: 2015-10-24 00:39:38
The Lightning Network will not be able to handle all transaction types, especially not at first. Some transactions require public audit trails. Other transactions might not be conducive to locking away value in the LN for many weeks or months at a time. Others still (e.g. multisig) might be theoretically possible but undesirably complicated to program. True blockchain space will always be in demand.
Even with LN, if you want a lot of different users to use Bitcoin, you still need a lot of transactions just as settlement for LN. If you have 100 million users, each of whom settles their LN balance on the blockchain every month with a 500 byte transaction, you get 11.5 MB of transactions every 10 minutes. 
(I'm not a Core dev, and I didn't directly address your question, but I give my $0.02 for free. Arbitrage opportunists are welcome.)
@_date: 2015-10-01 12:15:17
They are harvesting energy from radio waves. You can get femtowatts to nanowatts of power that way. For bitcoin mining, you need kilowatts to megawatts.
@_date: 2017-07-10 04:53:59
Let's say you're right, and crunch some numbers:
ASICBOOST is supposed to save 20% on power costs. Only S9s support it, and they get about 10 TH/s per kW. Right now it takes 1,000,000 TH/s about 1 hour to mine a block, so the amount of electricity needed for Antminer S9s to mine a block is 100,000 kWh. Most miners pay less than $0.05/kWh, so the cost of mining a block is about $5,000. Saving 20% of that would mean $1000 saved.
If the 240 kB blocks were the result of using Covert ASICBOOST, then Antpool sacrificed about 0.6 to 2.3 btc per block in transaction fees, or $1500 to $5750 lost.
In other words, if using Covert ASICBOOST means mining 250 kB blocks, then that would lose them money.
By the way, Bitmain just finished production on a batch of S9s a couple of days ago. (I got a notice of shipment on Thursday.) The increase in hashrate is just because (a) they kept some of the machines they made for themselves, and (b) many of their customers use Antpool.
@_date: 2015-11-26 12:59:46


You are welcome to hold that opinion. However, there are no blacklists in XT. 
This is a common misunderstanding. You are probably referring to the anti-DDoS protection that was added to XT. Normally, a bitcoin node will have 8 outgoing connections and accept up to 117 incoming connections. It is rare for all 117 incoming connections to be full. If that happens, it is probably a sign that someone is trying to DDoS bitcoin nodes by taking up all of the incoming connection slots. This is easy to do by using Tor, since Tor allows a single computer to use many different IP addresses and hold many connections open. The XT DDoS protection method is to give priority to non-Tor connections (which are harder to spoof) in DDoS situations where all incoming connection slots are full. Peers using Tor have the same priority and full privileges under normal, non-DDoS conditions.
Yes, there's a list in XT. No, it's not black.
@_date: 2015-10-25 00:43:10


Those data combine mobile devices and fixed lines into one metric. Mobile devices' share of internet usage has been increasing mostly over the last 7 years, and they're much slower than fixed lines. This may be why internet speeds have appeared to be stagnant over most of that time, with an uptick over the last few years as LTE deployment increases.
According to  wired devices were only 54% of total IP traffic in 2014.
Unless you think people should be running full nodes on their smartphones, I don't see how these data are relevant.
Full report here:
 
Another data source for global internet traffic can be found here: 
Looking at just the fixed-line stuff, traffic increased by 56% per year between 2002 and 2012.
In any case, I think that the growth rates are not the best way to think about it. I think it's better to think of the actual bandwidth that will be needed, 1 Gbps. The question is whether 1 Gbps (or faster) connectivity will be available and affordable to the majority of entities who would want to run a full node in 21 years. I think the answer is yes. Do you?


As for O( n^2 ) scaling, it's not. It might be O(n*m), where n is the number of full nodes and m is the traffic per node, but as you might be aware, the number of full nodes is actually decreasing. 
Gavin did a thing on alleged O( n^2 ) scaling and the many different meanings of that assertion. 
Even with 8 GB blocks and 50k nodes, we'd have about 5.3 Tbps of total traffic, which is half of what 2012's actual traffic was. With another 24 years of infrastructure development, I think Bitcoin's traffic won't be a significant load on the global backbones.
@_date: 2015-10-24 19:37:58
And BIP101 would not be? Is it the 8 MB in 2016, or the 2x every 2 years that you have problems with?
The problem I have with BIP103 and Adam's schedules is that they don't keep up with the demand growth rates that we have seen in the past, and would result in chronically congested blocks. They also do not keep up with technological progress (BIP103's 17.7% per year is much less than Nielsen's law, which states 50%), nor do they address the large difference between what current technology can handle and the current blocksize limit.
We've tested 20 MB. We haven't tested 32 MB. According to BIP101, blocks larger than 20 MB will not be allowed until around 2019. Do you think four years is too little time to detect and fix scaling issues that may manifest above that level? Or do you not believe that 20 MB has been tested well enough?
@_date: 2015-10-03 00:40:59


Sorry, you're right. I confused you while replying with someone else who did.


No, that is not correct. You have to account for capital expenses separately from operational expenses. The security is measured as the cost of attacking the network, not the incentive to run the network. If you have to manufacture a lot of expensive hardware in order to gain 51% of hashpower, then the security is high. If that hardware exists on the open market for pennies on the dollar, then the security is low. What we need to avoid is having a large percentage (e.g. 30%) of all potential SHA256 hashrate offline. If that were to happen, then a malicious actor could simply rent hashpower in order to perform attacks, and would only need to pay on the order of 0.1 to 1% of the cost it would take to buy enough hardware for the same attack.
Miner manufacturers will not continue to produce hardware until the operating expense of running the network equals or even gets close to the revenue. They will stop when they can no longer sell what they make, which will happen when the operating profit is insufficient to pay back the capital expenses over a reasonably short period of time. As such, the operating expenses will not reach 25 BTC or even get close unless exchange rate fluctuations cause a period of overconfidence among miners.
In order for the network to remain secure, we need to ensure a few things:
1. A supermajority (about 75%) of available hashrate remains online at all times
2. The cost of purchasing hardware to double the network hashrate is larger than the gain to be had by attacking Bitcoin.
Step 1. is really just a heuristic for 2., but it's a useful one.


I'm not following what you mean by equilibrium difficulty yet, but I'll take a few minutes and try to understand before I comment on it.


Can you include some numbers? I've done the math before, and I've found that the dwarfing would go in the other direction. For bandwidth, for example, my company would want to spend about the same amount on bandwidth (10 Gbit/s or 40 Gbit/s) as we are already spending on electricity (750 kW), instead of 1% as much. Thus, 50 BTC block rewards (25 subsidy, 25 fees) would be more than enough to cover our expenses. (Rent and labor wouldn't change much.) With 0.2 mBTC/tx fees and 500 bytes/tx, 8 GB blocks would contain 3200 BTC of fees. That's 128x as much revenue as we would need to cover the bandwidth costs.
@_date: 2015-10-05 12:48:10


That simulation assumed that all miners paid the same electricity rate. If memory serves, they used a rate of something like $0.07/kWh. This is not a very accurate number. My guess is that the average bitcoin miner runs on $0.04/kWh electricity. My company spends $0.025/kWh. I think what will happen is that the miners with expensive electricity will turn off their miners to wait for transaction fees, and the miners with cheap electricity like me will continue to mine 24/7, and won't let transaction fees pile up enough for most of the expensive-electricity miners.


I'm working on a detailed response to this question in another thread. I'll try to remember to tag you once it's done. It's definitely an interesting issue, and I don't think I've examined all of the contingencies yet, but so far I think it will work out okay.
@_date: 2015-10-02 09:31:03
It redirects money from your wallet to 21's, using a technology called "credit card."
@_date: 2017-07-03 16:59:24
In Bitcoin pruning mode, you have to download the full blockchain, verify every transaction, and then delete spent transactions from your database after they're verified. That's 150 GB of data downloaded and processed, leaving ~10 GB of data on disk.
In Ethereum, you just download the current state snapshot plus all of the block headers. That's â¤13 GB of data downloaded and processed, leaving â¤13 GB of data on disk.
(Note: while my nodes are showing 13 GB of data on disk, a freshly synced node will have less. By default, parity will store the full blocks that were downloaded after the initial snapshot is made. In other words, the Ethereum fast sync mode is about syncing, and it doesn't even prune after sync is done, whereas the Bitcoin pruning mode doesn't affect syncing at all, and only prunes afterwards.)
@_date: 2016-05-28 01:16:35


This kind of comment is not very constructive. I don't see how you interpret the statement you quoted as spreading fear, uncertainty, and doubt. I also don't see how you could think it would be untruthful unless you misread something. If you think that a comment that a poster made is non-factual, please state how you think it is non-factual and make a counter-argument. 
As an example in defense of my statement, Matt Corallo's proposal for a hard fork after SegWit included changing the discounting function in SegWit from 75% to 50% while increasing the base block size to 2 MB. The base block size is what determines the amount of space for legacy transactions, and the base block size combined with the discounting function determines the amount of space for SegWit transactions. In effect, Matt's proposal would increase the capacity for SegWit transactions a little, and for non-SegWit transactions a lot. I have not heard any serious proposals for a hard fork that would restrict the additional space to SegWit transactions.
Note that if I had said "block size fork" without the word "hard", then my statement would have been false, as the SegWit soft fork is called a block size increase by about 50% of people and not a block size increase by another 50%. But SegWit is not a hard fork.
@_date: 2015-10-04 00:06:05
I'm sorry, perhaps I don't understand what you mean by "like". I interpreted that to mean that you thought that my estimate of $0.04/kWh was lower than the actual price paid by miners for electricity, so I responded by noting the places where mining is actually done on a large scale right now and the electricity prices that they pay. For example, in central Washington state there are about 14 MW of miners that I know about (including a lot of Antminer's Hashnest machines), plus probably another 25 MW that I don't know about, all of whom pay about $0.020 to $0.025/kWh. Iceland has Genesis Mining, Advania, and a few other places. KNC is in Sweden, Bitfury is in Georgia, etc.
If instead by "not sure I like" you meant that you thought that $0.04/kWh is accurate but somehow harmful, then you should have said so. 


I did not respond to every single sentence in your comment, I just responded to the first sentence and the last sentence. Are you referring to the fact that I did not directly address the middle of your comment at all? 
@_date: 2017-07-10 06:51:50




No, because that is not an error. We only get 90% more transactions per block if 100% of transactions are using SegWit. 
The variation in market fee rates due to congestion is on the order of 20x to 80x. We're seeing transactions with 5Â¢ fees get confirmed right now, whereas a few weeks ago it required $1 to $3 to get confirmed. It really matters what other people are doing with their transactions.
@_date: 2015-10-26 00:55:46
That's a common and popular opinion, but it's not a correct one. When I've done the math for my customers, at least half look like they've broken even when doing all accounting in BTC. A lot of that has been driven by hardware resale values remaining higher than initial estimates. For example, the Spondoolies-Tech SP35 sold for about $2200 or 6.5 BTC new in December 2014. Last week, someone sold one on eBay for $2025, or around 7.5 BTC. After paying for electricity at my company's rates, an SP35 would have made about 4.5 BTC in operating profit over that time frame. 
Other hardware from around that time frame has held its value similarly well. SP20s sold for about $400 to $600 new, and are still worth about $400 used. Antminer S5s sold for $350 to $450 new, and are now worth $350 used. 
Of course, many other examples exist of purchases that have not made a profit by a large margin. For example, there's all of the people who bought KNC Neptunes at $13,000 each. Those shipped about 8 months late, and were worth about $3,000 by the time they were delivered. Or the Black Arrow Prospero X-3s, all of which were at least 7 months late, and some of which were never shipped to customers (stolen by Minersource, a distributor). Many of the ones that did ship burned out within hours of being powered on. 
@_date: 2015-10-06 02:38:16
I have to admit, at this point you have me quite curious. I have a lot of questions for you now. I'd appreciate it if you took the time to answer them, even if only with a simple yes/no.


(My understanding is that a stale block is not the first block to be created at a block height, but stale blocks are occasionally part of the main chain. On the other hand, orphaned blocks are blocks that are not part of the main chain, regardless of whether they were stale when they were first mined or not. Using those definitions, "orphaned" describes what I had in mind more accurately than "stale". I'm curious to hear if your definition of those terms is different. **Edit:** Oh, you're probably using the definition of "orphan" meaning a block without a parent, whereas the definition I gave above (and which I think is the more common usage) means a block without a child. I recognize the ambiguity. I'll have to see if I can come up with better terms. For now, I'll continue to use "orphan" to denote a childless block, as I think that is the most common usage.)
Orphaned blocks happen less than once per day on average, or about 0.5% of the time. Transmitting transactions on the hope that the transaction will be incorporated into one of those orphaned blocks first (0.5% likelihood) and then a competing double-spend needs to supplant the orphaned transaction in the replacing block (maybe 50% likelihood, or more or less, depending on the miner's first-seen and RBF policies). That means a success rate around 0.25%, unless the attacker controls a lot of mining power. 99.75% of the time, the transaction will not be orphaned, and the attacker will have to accept whatever good or service he purchased. If the good is worth less than 99.75% as much as the bitcoin he used to make the purchase, then he loses overall, and the attack is not profitable.
On the other hand, with a soft-fork assisted attack, the probability of attack failure as a loss to the attacker (fake transaction becomes real) goes from 99.75% to 0%. 
Are you claiming that the difference between 99.75% and 0% risk of attack failure is not significant, since they both are qualitatively lucrative to the attacker? Are you claiming that the difference between 0.25% and 100% attack success rate is not significant, since they are both qualitatively risky to the defender?


Do you think that SPV wallets should not be used for any commercial activity in which goods or services are traded for bitcoin, and should only be used for things like accepting donations? Do you think that Core developers should expend any thought or effort to prevent their changes from degrading usability of current SPV wallets? Do you think that Core developers should intentionally deprecate SPV wallet security to convince people to use more secure fully-verifying wallets?
Some kids might want to run a garage sale in which they accept bitcoin on their cell phone. Do you think this is a reasonable use of SPV wallets? Do you think it is important for Core developers to keep the security of use-cases like this in mind when making code and architectural changes?


You have a point here. It may get complicated in cases like eBay disputes, in which an attacker could show the purchase confirmation email stating that the order was paid to an arbiter even though the payment was an orphaned attack transaction. It would also require training customer service reps carefully, as a tier 1 CS rep might be easy to convince that the order was paid even though it wasn't. It might not be necessary to do much work on the payment platform UX for the customer against these attacks, but it definitely would require work on the payment platform UX for the merchant side.
@_date: 2015-11-05 05:30:51
We're at 100% of 1 MB capacity, because a 1 MB hard cap does not mean 1 MB average blocks. Immediately after a new block is published, pools switch to mining empty blocks. They don't start trying to mine a full block until they've finished verifying the new block and have finished composing a new block template, which can take several seconds. This means that a lot of blocks will be created empty even when there are enough transactions with fees to fill every single block.
@_date: 2017-05-01 22:13:41
I think that the activation of SegWit on Litecoin has caused maybe 2% of Bitcoin's market cap to flow to Litecoin via pro-SegWit Bitcoiners. This resulted in a non-significant downward force on Bitcoin (that was drowned out by other effects), but while $400m might be nearly nothing to Bitcoin, it doubled the market cap for Litecoin.
Activation of SegWit on Bitcoin may also increase the price, but the funding source would need to be different.
(Yes, I'm aware that market cap is not a conserved quantity, but it's a somewhat useful, though woefully inaccurate, approximation.)
@_date: 2017-07-10 07:08:57


No, 243.9 **MiB**/s, or 255.75 MB/s, [technically]( 
Anyway, in the context of networking and Bitcoin, base-10 sizes are more common than base-2 sizes. I don't know of anybody who says that the blocksize limit is currently 0.953674 MiB. 
I would think the benchmarks on a poolserver would be more relevant than the benchmarks on a laptop, since the delays on a poolserver are what contribute to orphan rates and centralization pressure.


That's 16 bytes out of 400 bytes, for the transactions that take an insignificant amount of hashing? Okay, sure.
There is also some overhead in my test example from reading the data off disk (or out of the OS's file cache), whereas in actual transaction hashing the data for a transaction would all be in either L1 or L2 cache.
@_date: 2016-05-06 20:13:32
We will tighten our belts. Those of us who have old hardware, high electricity prices, or both will have to shut down our operations. Those of us who have hardware with modifiable clockspeeds and voltages (e.g. Spondoolies hardware) will lower them in order to improve efficiency. This all will reduce the network hashrate.
As the network hashrate drops, the mining difficulty will drop as well (with a 2016 block, ~2 week delay). This improves the revenue and profitability for the miners who remain operating. Some equilibrium will eventually be reached, with a mining difficulty that's between 0% and 50% lower than the pre-halving difficulty. It's anyone's guess what that difficulty reduction will be. Mine is about 20%.
While the difficulty is adjusting, block times will be slower than normal. Instead of one block every 10 minutes on average, we might see one block every 12 or 15 minutes. This will make block congestion worse. In turn, this should increase transaction fees. In theory, this could help encourage miners to keep mining; however, in practice, the size of this effect will be insignificant, as the transaction fees per block will likely only increase from about 0.2 of 25.2 BTC per block to about 0.5 of 13.0 BTC per block, or from about 1% to 4% of the total block reward.
@_date: 2015-11-09 03:52:44
You are technically correct. The best kind of correct.
@_date: 2017-07-03 15:18:21
That site says that the Ethereum blockchain takes up 220 GB, but parity on my laptop is only using 13 GB. 
Sure, if you run your full node with non-default settings (or with the settings that were defaults before the 2016 spam attacks), then you'll see high resource usage. But there's no reason to do that. 
Ethereum includes a hash of the state tree root in every block, which means that unlike Bitcoin, you do not have to replay every transaction ever made in order to be able to have a trustworthy snapshot of the current ledger state. This means that an Ethereum full node can sync in five minutes to an hour. 
The state root hash also means that light clients on Ethereum are much more secure than on Bitcoin. On Ethereum, a light client can verify that money exists to be spent by a transaction (or can download and verify the previous state for an account) without needing to download the full blockchain. 
Keeping track of the state root hash (and the rest of the state trie) is computationally expensive, and it's the main reason why Ethereum's CPU usage for full nodes is higher. But what you get for that high full node CPU usage is the ability to securely use Ethereum without running a full node, which is pretty rad.
The state root hash is equivalent to the concept of [UTXO commitments]( but Bitcoin was never able to add UTXO commitments because they couldn't figure out a database structure that would make it easy to compute, and it would have been a big change to the bitcoind UTXO database design to make it compatible with UTXO commitments, so it was never added. But Ethereum designed it in from the beginning.
Who cares how big the full unpruned blockchain is if you don't have to download or store it?
@_date: 2016-05-27 19:22:31
The size of a transaction is measured in (kilo)bytes, not it bitcoins. A transaction that has 100 inputs of 0.001 BTC each will be much larger than a transaction that has 1 input of 1000 BTC. Your Bitcoin software usually handles this for you, so you often won't know the size of the transaction until after it has been sent.
If you view the transaction on a block explorer like **[this]( you should be able to view the size of the transaction and calculate the fee/kB.
@_date: 2015-10-21 23:28:52
Agreed. However, current relay methods are not sufficient to achieve O(1) transmission times. This whole discussion will become moot once a more robust technique gains widespread deployment.
@_date: 2017-07-07 02:00:52


How many of the ones who signed that paper even tried to make good on their promise? Adam Back didn't. Peter Todd didn't. Cory Fields didn't. Matt Corallo didn't. Luke Dashjr parodied his obligation with a blocksize decrease hardfork. Only Johnson Lau seems to have made a good-faith effort at writing hardfork code and convincing the rest of Core to use it.
@_date: 2015-10-03 16:08:04


I don't know what you're trying to say, but I think that statement as it stands is false. Perhaps you mean the investment in network security? Or the cost of network security? If the cost of something is greater than or equal to its value to you, you shouldn't buy it. 


Your "economic perspective" doesn't take into account capex and opex, availability of used hardware


It's simple because it doesn't require consensus. Different nodes will still agree to accept blocks eventually if others mine on top of them. They will just discourage blocks that take a long time to verify (and usually orphan them) when they are first mined.
I don't think the fix should be prioritized ahead of larger blocks, because I don't think it will get used. If it does get used, I think it would not be a very effective attack, since full nodes verify blocks before forwarding them. As such, a block like this would tend to propagate very slowly across the network. The counter to this attack would be to disconnect from the relay network and continue mining. Unless the attacker had a lot of hashrate behind them or crashed almost all of the mining full nodes, the slow block would eventually be overtaken and replaced. The longer it takes to verify this attack block, the more likely it is to get orphaned.


I don't agree with that assumption. I think transactions would stay about the same size, and we would just get more of them. From the data that I've looked at, this appears to have been the case historically, with mild deviations during the recent stress tests.


Yeah, I got the 22 hours number too for a single CPU with current algorithms. I don't see that as a problem. 8 GB blocks would imply 26,666 tx/second. That's about the transaction volume that would be needed to replace all credit and debit cards all over the world. If we had that level of transaction volume, we could afford to set up the server clusters needed to verify it. Keep in mind that 8 GB blocks would give about 3200 BTC or $768,000 in rewards per block for miners. It might cost $200k (200x $1000 CPUs/nodes) to build a server cluster fast enough to process 8 GB blocks with 2015 tech, but a mining company that was 0.1% of the network hashrate (one block per week) would be able to pay off a cluster like that after a few days of revenue.
We can only scale to about 32 MB blocks using a single high-end 2015-era CPU with current algorithms. Doing 32 MB blocks on an unknown 2014 CPU would take about 6 minutes according to one source, or about 3 minutes on a slow 2008 CPU according to [another]( (Note that verifying ECDSA signatures for transactions when the transactions arrive and not when the blocks arrive is a trivial optimization, so verifying ECDSAs in blocks will only add delays if they contain transactions that are not in the receiver's mempool.) According to BIP101, we would get a 32 MB block limit in 2020, so we have about 5 years before we would have to start to address the ECDSA scaling issue.
@_date: 2015-10-13 08:49:15
Mycelium local.
@_date: 2017-07-19 01:09:12
Um, no. The first two of those numbers are trying to explain the peak transaction throughput that you saw using either (a) the daily average block size, the unusually low transaction size, and the unusually short block time seen around May 25th, (b) the peak block size, the unusually low transaction size, and the unusually short block time seen around May 25th, or (c) the typical daily average block size, the typical average transaction size, and the yearly average block time.
So really, the typical capacity is around 3.36 tx/sec.
@_date: 2018-04-11 17:20:08
SegWit eliminates one ([c2]( of several forms of covert ASICBoost. It does not prevent the other covert methods nor overt ASICBoost.
@_date: 2017-07-10 04:28:58
Look at the block intervals for the last few blocks:
1. 7 minutes
2. 6 minutes
3. 4 minutes
4. 3 minutes
It looks to me like they may have just emptied out the mempool.
@_date: 2015-10-22 00:58:55
I'm done with this conversation.
@_date: 2015-10-18 01:10:37
Perhaps you want something like this?
(Point-to-point wireless bridge system)
@_date: 2017-07-05 14:08:46
Bitmain isn't as small as you seem to think. They probably make about 500 PH/s per month. At 70 GH/s per chip, that's 6.8 million chips per month. At $1000 per 13.5 TH/s S9, they get about $37 million in revenue per month from sales. NVIDIA's revenue is about $540 million per month.
Anyway, there's more money to be made in making mining GPUs than in making Bitcoin ASICs. Ethereum's mining reward is worth about $93 per second, but Bitcoin's is only worth $65 per second (including transaction fees). Why would NVIDIA or AMD leave their core competency to combat an entrenched opponent for a smaller market than they already have access to?
@_date: 2017-07-12 12:01:52
FIBRE stands for Fast Internet Block Relay Engine. It is a communications protocol that uses UDP with transaction compression and forward error correction to transmit blocks. It has nothing at all to do with fiber optics; that's just a play on words. It works just fine over copper.
@_date: 2017-07-10 06:03:02
Oops, you're right on the 0.37. I'll fix.
@_date: 2017-07-24 18:02:58
1hash has about 70 PH/s, which would use about 7 MW of power without ASICBOOST. ASICBOOST would save them about 20% or 1.4 MW, and the transaction-permutation variant would save them roughly 5% or 0.35 MW more over the basic extranonce-grinding [B1]( method (by allowing for 4-way collisions instead of 2-way collisions). Most miners pay around 4Â¢/kWh, or around $29.2k/(MWâ¢month), so using regular ASICBOOST (without the transaction permutations) would save them around $41k/month, and using transaction permutations (like what seems to have gotten them in trouble here) would save them an additional $10k/month.
Making blocks with only 256 transactions each sacrifices about 80% of the total transaction fee revenue per block, or around 1 BTC per block. With 70 PH/s (1.1% of the network), that's about 47 btc or $130k in lost fee revenue in addition to the lost $70k in block rewards from these two malformed blocks for the privilege of saving an additional $10k per month.
Let me know if I made any major mistakes on my math. If not, and if this is actually due to transaction permutation ASICBOOST, it seems like 1hash is losing more money from bugs than it's saving on power, and also losing more money on transaction fees than it's saving on power.
@_date: 2017-07-05 14:48:49
Actually, there are two things that matter: hashrate/(mm^2) (or hashrate/$) and hashrate/watt. Ever since they released the Antminer S5, nobody has been able to get beat Bitmain on either one.
@_date: 2017-07-05 07:30:22
The Avalon 7 uses [about 50% more power]( per TH/s than the Antminer S9, costs 15% more per TH/s, is slightly less reliable, and is a bit more of a pain to set up.
That's without ASICBOOST on the S9, of course.
@_date: 2015-10-25 17:13:51
Yes, SPV wallets are very slightly unsafe. Users who choose SPV wallets are sacrificing a small amount of safety for ease of use. Most do this knowingly, because they have decided that the degree of safety provided by running their own full node is not necessary for their use case. The ones who don't do it knowingly rarely have enough bitcoin at stake for it to matter.
For example, I run a business (a miner hosting service) in which about 75% of our receipts are in bitcoin. Even though we run a few full nodes for our mining operation, we don't use them as bitcoin wallets. We use a SPV wallets for most of our receipts. This means that an attacker who knew which addresses I use for my clients could perform a Sybil attack and lie to me about what my account balance is for a time. They could also construct blocks with invalid transactions (but valid headers and valid proof of work, an expensive attack) and make it seem that they have paid me when they haven't. I'm not worried about these attacks at all, since we have our customer's equipment as collateral, and we maintain a many-month relationship with our customers. We'd notice, and we'd have means of recourse.
Full node wallets are needed if you are receiving a large volume of payments from untrusted parties to whom you quickly, irreversibly, and pseudonymously provide a good or service. For most cases in which any one of those conditions is not satisfied, an SPV wallet will provide sufficient security.
@_date: 2017-07-10 04:55:18
This isn't actually SPV mining. With SPV mining, the blocks only have the coinbase transaction. These blocks have more than 1 transaction. This is more likely the result of mempool depletion.
@_date: 2017-07-17 14:43:11


I'm going to ignore this part of your comment.


The area you have circled in red appears to be roughly May 10th through June 10th. If you look at the block times during that interval, you will see that they were unusually short from May 4th through June 9th, reaching a minimum of around 515 seconds on June 1st. The average block time over the last year is about 570 seconds.
I haven't been able to find any graphs showing average transaction size directly. However, the average transaction size is equal to the number of transactions per block divided by the size of each block, and both of those charts exist:
If you take a look at those two graphs, you'll notice that the average block has been pegged at 0.90 to 0.92 MB from Feb 2017 to July 1st 2017 with little variation, but the number of transactions per block has varied from around 1600 to 2000 over that same interval. This indicates that transaction sizes have fluctuated significantly. In particular, on the week ending Feb 7th the average transaction was 480 bytes (0.90 MB / 1876 tx), but on the week of May 25th it reached 448 bytes (0.91 MB / 2031 tx). 
The spike in the graph you showed is pretty close to May 25th, where transaction sizes and block intervals were both at their smallest. This is sufficient to account for the higher tx/s throughput during that time.
@_date: 2015-10-04 22:24:05
No, I read your full comment and misunderstood it. It didn't make sense to me. I thought you were trying to say that my estimate should have been higher using some strange grammar. Sorry. I was also confused because I gave two estimates, one of which was 70x and the other of which was about 12x (which I thought you were rounding to 10x).
I don't think your estimate of $0.025/kWh average is plausible. How much do you think Bitfury pays? When I've looked up retail rates for Georgia in the past, I see retail rates around $0.08. With discounts for industrial use and high voltage feed, I'm guessing they probably get $0.06, though they might be able to get as low as $0.04.
@_date: 2017-05-02 19:58:03
Yep, totally speculation. That's why I prefaced my comment with "I think." 
My point was essentially that the OP's speculation of $40-50 billion in lost value due to the non-activation of SegWit is a speculative claim based on the questionable assumption that the same *percent* change would happen in Bitcoin and Litecoin.
It might seem that all this speculation is totally mad, but
 [most everyone's mad here]( Heheheh.
@_date: 2017-07-05 07:14:27


Nobody mining on a Bitmain rig benefits from ASICBOOST except (allegedly) Bitmain itself.
The real reason why miners aren't voting for the UASF (even if they're otherwise voting for SegWit) is simple: miners don't like UASFs as an upgrade mechanism. This isn't surprising, as chain splits are more expensive for miners than for users. 
Each block that a miner creates on the fork that loses costs them about $39,000. Creating a block on the fork that wins does not confer any advantage. Miners are therefore interested in minimizing the risk of chain splits by choosing upgrade strategies that ensure overwhelming consensus. The UASF is the opposite of that, and consequently, most miners oppose it.
@_date: 2017-07-03 08:41:47
People are switching over to other blockchains, mostly Ethereum.
@_date: 2016-05-22 16:42:52
Most miners use default settings. The priority block space default size was set to 0 in version 0.12 due to some complications in the new CreateNewBlock() mining code that Alex Morcos wrote. Consequently, very few miners have any of their blocks dedicated to transactions that spend old and large inputs but little or no fees.
A lot of miners like the idea of the priority space, but they aren't doing it right now because the software doesn't make it convenient and it may soon compromise mining performance.
@_date: 2015-11-09 04:00:51
We probably lost about $30 of mainnet hashing while activating testnet. I didn't bother to do the math on what the minimum hashrate would be to trigger the activation, I just kinda did it. Hardware time is cheaper than my time, most of the time.
@_date: 2017-07-07 07:23:54
They don't have the authority to force it, and nobody said they did. However, they do have the authority to write code and advocate its use, and they did not do that. There was no good-faith effort made by any of the devs present except from Johnson Lau.
@_date: 2015-10-22 00:31:28
Only if miners are perfectly rational, and don't have any additional costs like hard drives and processor power, and don't care at all about the size of the blockchain, and only if there are people who are publishing enough transactions with fees for miners to include.
What is your definition of "too large"?
In practice, fees don't fall to zero when supply exceeds demand, and blocks don't fill up when transactions are free. If you look at the blockchain fee history, you'll see that the time with the greatest fees was not when the blocks were full, but rather in 2013, when the default fee in bitcoin-qt was something like 5x higher than it is now.
@_date: 2015-10-21 23:16:52


What do I think the reason that the RPC protocol isn't open to all hosts by default? Several things:
1. Security. Some of the RPC calls are used to access the wallet. Obviously, that has to be restricted.
2. DoS safety. Some of the RPC calls consume a lot of resources. getreceivedbyaddress, for example, can take many minutes to complete if used on an address that isn't already in the node's wallet.
3. Performance. The JSON API is not efficient in terms of network resources. 


For everyday stuff? Mostly, sure. For seeing if my friend sent me the $20 that he said he would? Sure. For running an ecommerce website in which product is shipped out after at least 6 confirmations? I'd want to query several different independently operated nodes, but if they agreed, that would be fine with me. For transacting with the operator of the block explorer or RPC proxy? No. For running an exchange or a gambling website? No. 
"Trust" does not need to be a binary variable. Tools without 100% trust can still be usable in revenue-path systems. For example, most credit card transactions have about 1% fraud rates, so when a vendor charges a credit card, they should only assign 99% trust to that transaction. I would say that a block explorer or RPC proxy is about 99% trustworthy for correctly reporting a random unconfirmed transaction (or its absence), and about 99.9 to 99.99% trustworthy for correctly reporting random information 6 blocks (or more) deep. For a non-random transaction (i.e. one in which the block explorer or JSON proxy has a financial interest), that trust level would be much lower. Even then, a reasonable level of certainty for most uses can be achieved by querying multiple independent sources and looking for discrepancies. 
Bitcoin should always remain a system in which trust is not required. However, using a small amount of trust as a convenience in order to reduce the need for low-volume merchants to spend $10-$40/month to run a full node is, in my opinion, reasonable.
@_date: 2015-10-04 22:31:47




I think he's saying that forks would be shorter, not that they would be less frequent.
@_date: 2016-05-22 16:35:12
There are two mechanisms for setting minrelaytxfee. One is to set the minrelaytxfee floor via the command line or via the config file. The other is to set the maximum mempool size, and let bitcoind decide what the miinrelaytxfee should be. The latter method allows some spam into your mempool, but not into blocks. Normally, a few megabytes of mempool bloating doesn't bother people, so the automatic method is good enough for most, but since OP complained about it, I thought I should point him to the minrelaytxfee option.
@_date: 2015-10-06 00:35:03
Cloud mining companies are frequently scams.
The most efficient hardware on the market right now is the Antminer S7. I have 10 of them in my hosting/colo company right now and they perform as specified. They're about 2x as efficient as the nearest competitor. They're a bit expensive, though.
@_date: 2017-07-05 14:46:01
You usually can't speed up engineering by spending more. The years spent on engineering SHA256 engines is more important than the money spent. Unless they have a time machine, they can't do 1-2 years of engineering before breakfast. And if they did have a time machine, they would have just bought ETH last October, or Litecoin in January.
@_date: 2017-05-02 20:02:57


Most of that is in Ethereum. SegWit will do very little to address Ethereum's superiority in Ethereum's core competency (smart contracts), so it's unlikely that Bitcoin will get too much of that 40% bonus.
But even if all 40% of the total crypto market cap flowed back into Bitcoin (producing a Bitcoin dominance index of 100%), you would only get a 66% increase in the amount of money invested in Bitcoin.
So the point remains: in order for SegWit to increase Bitcoin's market cap by "$40-$50 bln" (to a total of around $70 bln), you would need new money to come into bitcoin from outside the cryptocurrency system entirely.
@_date: 2017-07-10 05:07:26


They have to justify to their investors having spent millions on acquiring Allied Control.


If you look at my other posts, you'll see that I agree with you. Electricity costs just aren't that significant for miners right now. The incentive to use it is pretty weak. 
Also, the difference between the Covert ASICBOOST optimization (which may allow for 4-way collisions) and the standard extranonce grinding ASICBOOST version (which can only get 2-way collisions while still being impossible to detect) is only around a 5% reduction in power consumption, or about 0.8% of their revenue.
@_date: 2015-11-11 11:36:38
@_date: 2017-07-08 12:11:33
Stuffing won't be necessary; we can just wait another 10 minutes to let enough transactions be created to get the block weight above 4M.
@_date: 2017-07-17 10:04:54
The capacity limit is not defined in terms of tx/s, but in terms of bytes/block. If the average transaction is around 500 bytes and the average block takes 600 seconds, then the 1 MB/block limit equals 3.33 tx/sec. If the average transaction is 450 bytes and the average block takes 550 seconds, then the limit equals 4.04 transactions per second. 
What you're looking at in that cherry-picked time interval is probably either a period during which the mining hashrate was increasing faster than usual (resulting in a shorter block interval) or a period in which people were sending smaller transactions than usual, or both. The average block interval has gotten as low as 510 seconds several times this year, but it is usually around 580 seconds:
@_date: 2017-07-05 13:33:42
Just like how in 1978, Texas Instruments and Motorola, both huge established companies with a ton of design experience, jumped into the field of microprocessors with their TMS9900 and 68000 chips, and [crushed]( the hopes and dreams of a small startup called Intel and their 8088.
Oh, wait, no. That's not what happened.
Or like how the small startups of 3Dfx and NVIDIA were crushed by huge established companies like SGI? 
Wait, no, that's not true either.
@_date: 2017-07-20 23:44:22
It's just variance. They had a good day.
@_date: 2017-07-12 12:04:21


Having tested 9.1 MB blocks on $500 nodes using the slow 0.11.2 codebase, I tend to concur.
@_date: 2017-07-06 19:15:29
In Hong Kong, the miners made an agreement with several Core developers to implement SegWit plus a 2 MB hard fork. All of the Core developers reneged on their promise. The miners got pissed. A lot of other people got pissed too. So dishonest devs get fired.
(F2pool also reneged with around 2% of their hashrate, but the other miners remained faithful.)
@_date: 2015-10-25 17:30:38


There aren't as many miners as there are non-miners.
21 years ago, 28.8kbps modems had just been released, and it cost about $2,000/month for a 1.5 Mbps line (a T1). Now, pretty much everybody has affordable access to a line much faster than 1.5 Mbps, and 30 Mbps has about the same availability as 28.8k modems did. If that trend holds, we'd expect 312 Gbps internet to have the same availability as 30 Mbps does today. To keep up with bitcoin, you would need no more than 1 Gbps. That's a pretty large safety margin.
@_date: 2017-07-10 05:35:31
I think freework's point is that the average transaction's hashing cost is pretty close to zero. Let's take a common but worse-than average example: a 2 kB transaction with 10 inputs using SIGHASH_ALL would require about 20 kB of hashing to verify, which would take about 66 Âµs at 300 MB/s. Verifying 1000 such transactions (a 2 MB block) would only take 66 ms out of the 600 second block interval. With a more typical 400 byte 2-in 2-out transaction, you're looking at around 13 ms of hashing per 2MB block. In comparison, the ECDSA signature validation time for these blocks (10k sigops each) is around 1 second.
Such hashing and validation can be done in advance for the typical block. 
@_date: 2016-07-01 06:02:07


Not quite. An immutable ledger is fundamental to the type of decentralized block chain that you're interested in. That's fine; there's a lot of utility to be had from having a system in which transactions are practically irreversible. There's also a lot of utility to be had in a system where consensus is occasionally determined by people agreeing on the blockchain they want to use when tricky cases come up instead of blindly following the software. For Ethereum, when deciding between an immutable ledger and a ledger that doesn't make a single hacker ridiculously wealthy, I prefer the latter.
But this is all rather off-topic.
@_date: 2015-10-03 13:54:47


We don't have to keep the total block reward at 25 BTC to keep the hashrate at 450 PH/s. My assertion is that we only need about $3500 or 14.6 BTC per block in order to keep the miners on. The other 10 BTC are currently paying off the capital costs. Miners are expecting the block reward halving, so they're trying to pay off their capital expenses before that happens. Thus, I don't see why we would need or want to keep mining revenue at 25 BTC when we should be able to operate fine with around 15 BTC or less (unless the hashrate grows a lot).


When you get a chance, can you provide a citation on that? From my understanding, it should be essentially linear, especially since most transactions can be verified in parallel. What element is nonlinear?


If we had 8 GB blocks, we probably would not be using a single multicore CPU from 2015 to process them. If technology does not advance fast enough, we would likely have clusters of computers dedicated to the task of verifying blocks. If we choose to approve BIP101, we'll have to figure that out sometime in the next 20 years. Either that, or we could use GPUs or FPGAs to accelerate the ECDSA verifications.


Yes, that attack has been known for a while. It hasn't been implemented yet because there's not a strong financial incentive for the attack. That doesn't mean that someone won't figure out a way to gain off of it in the future. 
Anyway, there is a simple hack fix for blocks built to verify slowly: miners can ignore them. We can add code to miners to make them not mine on top of blocks that take longer than X seconds per MB to verify unless they are not the last block in the chain and are part of the chain with the most PoW. A better fix is to simply forbid transaction types that would take excessively long to verify.
@_date: 2016-05-28 01:09:54
Oh, interesting. Those are different. 200 input, 1 output. Output size is around 1.5-2.0 BTC per transaction. Those look like they might be a cleanup operation from a poorly designed business application that accumulates small UTXOs. With 1.5 BTC output per transaction, I don't think the owner can really afford to have these transactions permanently fail to confirm, so I'm guessing he's aiming for the lowest fee that has a chance of confirming.
I'd call this bloat, not spam. Spam's purpose is to congest the blockchain, which means that it usually has a low (near-unity) ratio of bitcoin transmitted to fee paid. These send about 300x as much coin as they pay in fees.
@_date: 2016-07-24 06:27:52
Correct. The goal was to target companies that have a high power-to-jobs ratio. Chelan PUD provides electricity within the county at a price below the cost of production, and subsidizes that with revenue from exported electricity. They do this to try to stimulate the local economy. Consequently, they don't like datacenters and bitcoin mines taking advantage of their artificially low rates. They used square footage as a proxy/correlate for jobs provided. Bitcoin mines are the best example of that category, but traditional datacenters also qualify.
@_date: 2015-10-22 00:10:34
BRN is nearly O(1) time to the nodes that are connected to it, but most nodes are not connected to it. Yes, most miners are, but the statistics for block propagation times are for all nodes, not just miners, and it's hard to use data that haven't been collected during an argument. (Well, on second thought, I suppose it's easy, but I still prefer not to do it.)
I don't think block propagation latency and orphan rates are going to cause any problems for Bitcoin at up to 20 MB blocks with current technology. I think that the relay network and standard inv/getdata will be sufficient for many years if BIP101 gets passed.
What I'm trying to show is that even without them, using the worst-case algorithm for block propagation, stale ("orphan") block rates will not grow crazy-huge. Really, I'm just trying to explain to that miners won't want to make their blocks huge and won't want to make orphan rates huge. I'm probably wasting my time, though.
@_date: 2015-10-05 20:54:08
As a first-order approximation, yes. 
Furthermore, I expect that capital costs will become a much smaller portion of total costs once things slow down. Five years from now, people will be amortizing their mining hardware over 5 years instead of 1 year, so the percentage of total expenses due to capital costs will be smaller. 
Also, companies will already own the datacenter capacity which will simply be reused once the hardware upgrades arrive.
Right now, over a 1 year amortization, CapEx and OpEx are about 50/50. With 5 year amortization and reused datacenters, I think below 20% CapEx is likely. I decided that that was negligible, so I didn't bother to include it in my Fermi estimation.
A bigger problem with my estimate is that it doesn't address possible large changes in the Bitcoin exchange rate. An increase in exchange rate would result in increases in the hashrate, especially if it happens before the halving. The goal of a Fermi estimate isn't to be accurate; it's to give you a rough idea of the outcome for a scenario in order to assess the feasibility and likely challenges of a plan.
@_date: 2016-07-24 03:40:51
Washington State did not hike the rate for Bitcoin mining. Chelan County's PUD hiked the rate for Bitcoin mining. Chelan County had the best rates in the state, but only by a narrow margin. Two counties in Washington still offer electricity rates that are lower than $0.0389/kWh.
@_date: 2016-07-01 13:54:19
I have no problem with "censoring" hackers.
I don't think the slippery slope argument is valid. It's really hard to get a majority of miners and/or users to agree on a fork. I think it will only happen in cases in which there was a clear-cut error of enormous magnitude, like this one. In every other case, I think that the desire to keep the network predictable and rule-governed will win out.
@_date: 2017-07-05 07:36:49
AMD and NVIDIA are making GPUs for Ethereum and Zcash mining. They are not making SHA256 ASICs for Bitcoin mining. 
Even if they entered into the market, there's no way they would be competitive. Bitmain, Bitfury, Canaan, and the rest have spent years and tens of millions of dollars each on engineering R&amp;D to optimize their SHA256 silicon engines. Bitmain has consistently done a better job of that than anyone else since 2015.
It's silly to think that a company that specializes in making GPUs would know how to make Bitcoin miners better than a company that specializes in making Bitcoin miners.
@_date: 2017-06-20 04:54:00
Do you know how many of the supporters of the [NYC 2017 agreement](
) are miners?
By my count, it is only 10 of the 56 signatories.
@_date: 2015-10-07 12:32:26


Yes, if **everyone** is running SPV clients except miners, then we have a problem. However, if a few economically important non-mining entities run full nodes (e.g. Coinbase, Bitstamp, BitPay, blockchain.info, Wells Fargo, NewEgg), then it's less of a problem. Miners have a choice: Do I make coins that can be traded for fiat at an exchange, or do I try to trick SPV clients for a while? 
What is a safe ratio of SPV clients to full nodes? What is a safe ratio of mining full nodes to non-mining full nodes? I think both of those ratios would need to be absurdly high for this kind of hard fork attack to be profitable. 


The reason why I think there is sufficient security to defend against this kind of attack is that the miners are trading real revenue for the right to trick SPV clients about the state of the ledger for a short period of time. The cost is proportional to the duration of the trick. I think word would get around about the attack within a few hours to a few days at worst, and the miners would have to have some follow-up attack to actually get profit on the real blockchain or in the real world. 
Can you describe a scenario you have in mind in which the miners actually cash out? The only way I can see it happening is if a large financial services company like Coinbase or shapeshift.io uses SPV.
@_date: 2016-07-28 14:19:15
That only affects Chelan County, not all of Washington State, and it doesn't take effect until Jan 1, 2017.
@_date: 2017-06-18 17:52:05


But at the same difficulty. For each block reward on the UAHF chain that Bitmain gets, they sacrificed enough computation to get one block reward on the UASF chain or the plain Bitcoin chain. Bitmain only loses by mining on the UAHF chain unless the UAHF chain wins in the long run.
@_date: 2017-07-05 14:59:17
That's ridiculous. Designing a high-performance competitive bitcoin mining ASIC from scratch in a â¤ 16 nm full custom design is not a weekend project.
@_date: 2017-06-18 20:32:36
Nope. Changing the difficulty is something that would require a hardfork. Unlike a blocksize change, a difficulty change is a significant economic change which very directly (but temporarily) benefits miners, and users would not go along with a hardfork in which a miner gives itself more money.
There is a way to manipulate the block difficulty by [creating blocks with misleading timestamps]( However, in order to do this, you still need to mine 2016 blocks to get through a difficulty adjustment cycle, which is not possible to do in 72 hours.
@_date: 2017-06-14 22:57:40
For SegWit, not for the UASF.
@_date: 2017-06-16 19:48:10


So is 3 minutes, which is what is currently possible. However, that bug has never been exploited, because even 3 minutes is expected to cost the person mining such a block about 5 BTC due to orphan risk.
Personally, I'm a fan of the XT/Classic approach to this problem, which was to limit the amount of hashing to 1.3 GB, equivalent to about 10 seconds per block regardless of tx or block size. But whatever.
@_date: 2017-06-20 03:02:23
Um, no, it's how they implement the 2016 Hong Kong agreement that Core ignored.
Like, take a look. The [2016 HK agreement]( said:
1. SegWit first
2. 2 MB hard fork in 3-12 months
3. Until then, no Bitcoin clients in production other than Bitcoin Core
And the [NYC 2017 agreement]( said:
1. SegWit first
2. 2 MB hard fork in 3-6 months
3. No more Bitcoin Core
Also, saying that it's "the miners' way" is not very accurate, as 80% of the signatories to the NYC 2017 agreement were non-mining Bitcoin businesses.
@_date: 2017-06-15 18:45:17


Bitmain said last year that they would help activate SegWit if a 2 MB hardfork got included in Bitcoin Core. A few Bitcoin Core developers agreed to this. Bitmain followed this agreement in good faith, as did 98% of the hashpower that signed the agreement. The Bitcoin Core developers did not deliver the hardfork code in good faith, citing the betrayal of the 2% of hashpower as justification. Consequently, Bitmain decided not to adopt SegWit. After the 6 month window for Bitcoin Core's hardfork delivery passed with nothing except Luke-Jr's joke(?) hardfork [blocksize decrease BIP]( Bitmain decided to support a development team that doesn't break its promises, and switched to Bitcoin Unlimited.
While an argument can be made that Bitmain should ignore the HK agreement and subsequent betrayals and help activate SegWit now because you think it's the right thing to do, it's not fair to simply forget about those events when criticizing them. You need to at least explain why activating SegWit is more important than betrayals and hard forks before you can be justified in calling it an attack.
@_date: 2017-06-16 22:26:27
Block 364,292 was a 10 second block, not a 3 minute block, and it was a mistake. Even Wang Chun, who made the block, said it was a mistake -- he didn't realize that he should have used SIGHASH_ANYONECANPAY instead of SIGHASH_ALL when generating that transaction. While you can make a 10 second transaction by mistake (but only if you're a miner, since such a transaction is non-standard), the 3 minute transactions have completely absurd scripts and cannot be created by mistake.
The point I'm making is that it is harmful to the revenue of the entity who mines such a block. That's why we haven't seen the 3 minute blocks ever created, even though it has been public knowledge since 2012 how to make them.
I'm in favor of actually fixing the vulnerability with a soft fork to restrict the amount of hashing allowed per transaction. SegWit doesn't do that, unfortunately, and nothing else that's currently proposed does either. I'm just saying that if it doesn't get fixed, the only thing that it's going to affect is reddit conversations about scalability.
@_date: 2017-06-22 21:32:30
The problem is that there are economies of scale at play in both ASIC production, in datacenter construction, and in power production. All other PoW functions will have the same problem. Shall we switch to proof of stake instead?
@_date: 2017-06-09 21:40:37


I fixed Antbleed on my mining farm by adding this one line to my /etc/hosts file on my DHCP/DNS server:


I find it amusing how many people seem to think this is a big deal.
@_date: 2018-12-26 17:00:49
This is funny. The same tweet was linked to on but over there, the headline was that BSV is a dumpster fire, and over here, the headline is how Bitcoin is building genuinely cool tech while overlooking the fact that he also said "I have my disagreements with the bitcoin roadmap, PoW, etc."
We all filter for the things we want to hear.
@_date: 2018-12-04 07:07:06
SV is mining at a loss on the order of $100k per day. That's a drop in the pond for BTC markets.
@_date: 2017-06-16 14:55:55




Does too:
    static const unsigned int MAX_TX_BASE_SIZE = 1000000;
@_date: 2017-06-16 14:32:44
I was under the impression that this discussion was about the UAHF proposed by Bitmain, not BIP148.


BIP148 has very little hashpower support. BTCC and Bitfury are the only two I think are likely to go along with BIP148; if they both do, that's only 14% of the hashrate. Given that more hashrate supports BU/EC than BIP141/SegWit (much less BIP148), it is unlikely that BIP148 will be able to get the 51% hashrate support that it needs in order to successfully 51% attack the legacy chain.  Consequently, it is very likely that both BIP148 and the legacy chain will exist. The BIP148 chain won't have enough blocks to be usable, though -- you'll probably need to pay transaction fees close to 0.01 btc ($25) in order to get your transaction included in the BIP148 chain.
@_date: 2017-07-09 13:01:49
Let's say you have a message you want to send. Your message is 1 MB in size. You want to send a set of chunks of data 1 kB in size each to the recipient. You want to be able to send enough data for the person to be able to reconstruct the message even if one or more chunks of data is lost.
A simple way of providing some redundancy is to break up the message into 1 kB chunks, send all 1000 chunks, and then send one last chunk that is the XOR of all of the first 1000 chunks. This would be a parity chunk. If the recipient got all but one of the chunks, they could XOR together all of the chunks they received (including the parity chunk) and the result would be the missing chunk. Since XOR is a commutative, associative, and reversible operation, `m1 XOR m2 XOR m3 = p1` also implies that `m1 XOR m3 XOR p1 = m2`.
You can expand on parity chunks a lot, by e.g. bit-shifting some of the chunks in a particular manner before doing the XOR in order to provide multiple parity chunks and to allow more than one chunk to be lost while still being able to reconstruct the whole message.
But you can also go much further, and fountain codes do that. With a fountain code, you can generate an effectively infinite stream of chunks, none of which were found in the original message (but all of which were derived from it), but such that as long as you get 1000 different chunks from that near-infinite stream, you can reconstruct the message. With a fountain code, it no longer matters how many packets are lost; all that matters is how many packets are received. The sender keeps sending new chunks of data until the recipient has enough to reconstruct the whole message.
@_date: 2017-06-14 23:23:57
I think you misread Bitmain's announcement. They're planning on keeping the majority of their hashrate working on the â¤1MB non-BIP148 chain -- that is, the Bitcoin we're currently using. They will use a minority of their hashpower to mine a backup fork with a &gt; 1MB rule (a mutually exclusive hard fork with replay protection), but they will only publish that blockchain and switch the majority of their hashpower to it *after* BIP148 wins (if it does win):








So the options are either UASF-SegWit *and* BU, or to wait for SegWit2x and get SegWit plus a hardfork that is *not* BU, or to get nothing.
@_date: 2016-07-28 14:21:47
As a miner in Washington State, I think there's about 10% of the network hashrate within about 200 miles of me. However, I don't think many or any of us are shutting down our operations, as most of us pay less than $0.03/kWh for electricity and are consequently still profitable.
@_date: 2017-06-16 14:51:21


Nope: 
The O( n^2 ) scaling issue is versus *transaction size*, not versus block size. Specifically, versus legacy transaction size. This code allows for blocks containing at most two 1MB transactions, which would take twice as long to validate as current blocks. That would be about 6 minutes instead of 3 minutes.
@_date: 2017-06-25 10:04:25
Antminer S9: $1113 for 13.5 TH/s, or $82.44/TH/s
Avalon 741: $714 for 7.3 TH/s (plus $60 controller), or $97.80/TH/s
Reliability is comparable between the two models. The S9 will be profitable for longer due to its higher efficiency. The Avalons are also kinda a pain to set up since they don't have an integrated controller.
@_date: 2018-12-09 03:06:12


No, a chain split and a hard fork are not the same thing. Hard forks happen whenever there is a rule change that causes previously disallowed blocks to become allowed. Chain splits are where different nodes will disagree on what the best chain is. Hard forks create the potential for chain splits, but they do not guarantee a chain split.
For example, the Bitcoin Cash network had a [hard fork in May 2018]( to increase the default blocksize limit to 32 MB and to enable some new opcodes. This was not a contentious hard fork, so no miners followed the old consensus rules and [there was no chain split](
Chain splits are also possible with soft forks, if they are attempted by a faction controlling less than half of the hashrate.


A soft fork is supposed to be backwards compatible in that blocks created by the new rules are valid according to the old rules. A transaction that is valid according to the post-SegWit rules is also valid according to the pre-SegWit rules. A block that is valid according to the post-0.25-btc-fork rules is also valid according to the pre-fork rules. 
However, a miner who mines a transaction that follows the SegWit ScriptPubKey/RedeemScript format except without including a valid witness will have their blocks be orphaned by miners who follow the SegWit rules. A miner who miners a block that includes a 12.5 BTC coinbase subsidy will have their blocks be orphaned by pools and nodes following the 0.25 BTC rules.
If the 0.25 BTC miners have a hashrate majority, then the longest chain will have no coinbases larger than 0.25 BTC, and will be considered valid by both old and new nodes. However, if the 0.25 BTC miners have a hashrate minority, then the longest chain will contain 12.5 BTC coinbases and will be considered valid only by the old nodes; the new rules will exclude those blocks, so nodes following the new rules will follow a separate, shorter chain, and there will be a chainsplit.
@_date: 2017-06-21 19:30:16
The code isn't quite ready yet. Signaling on bit 4 is intended as a way to show that they're running software that supports SegWit2x. Coinbase signaling is just to say that they intend to run the software once it's ready.
@_date: 2017-06-18 17:10:49
Bit 1 and bit 4, not BIP.
@_date: 2013-12-01 22:55:27
What if he's wearing a hat? Would we need to strip-search each beggar to ensure that they're not hiding another address tattoo somewhere?
@_date: 2017-07-05 13:37:55
NVIDIA, AMD, and Bitmain all have their chips made by TSMC. NVIDIA and AMD could only compete with Bitmain if the designs they send to TSMC are better than Bitmain's.
@_date: 2017-06-15 18:26:03


That's a good question. Something I read at one point but can't find in their current UAHF contingency plan page is that they were planning on transitioning to a slightly new transaction format (e.g. a version bump) as replay protection, with the first few weeks after UAHF activation allowing both transaction formats. This idea may have changed, though, so it might no longer be valid. I would guess normal transactions will probably be allowed on the HF chain, but I can't be sure. Although the blocks will be slower due to the diminished hashrate, they will be at least 2x as big, so the transaction throughput won't be as bad as the UASF chain would be with the same hashrate.


The difficulty won't drop for 2016 blocks. As long as Bitmain doesn't mine 2017 blocks on their own, they don't get any advantage. If they only have 10% of the July 31st Bitcoin hashrate pointed at their HF, those 2016 blocks will take 20 weeks to mine.
@_date: 2017-06-22 00:21:22
The /NYA/ coinbase signaling that's going on right now is in no way binding. It's just a way of showing support.
Chances are, since signing /NYA/ doesn't really matter, we're seeing less support for SegWit2x than actually exists. A few pools have not yet added /NYA/ to their coinbase sigs (at least not on all servers) even though they've signed the agreement.


There's no incentive to lie. Miners don't want controversial chain splits, as they harm Bitcoin's value.
@_date: 2017-06-22 04:27:57
In order for that to work, you'd need at least 10% of the network hashrate to make an impact, which costs about $50 million in hardware. People with $50 million in hardware at stake are not inclined to try things which they expect to reduce the value of their physical assets.
@_date: 2017-06-20 17:38:05
Because most of the signatories to the SegWit2x agreement are businesses who run economic nodes. Miners are a small minority (about 20%) of them.
@_date: 2017-07-05 14:13:23
The early graphics chipsets absolutely were ASICs. They were fixed-function hardware that could only do the specific operations needed for rendering graphics -- texture sampling and bilinear interpolation, gourad shading, perspective correction, etc.
@_date: 2017-06-14 23:02:04
If the UASF loses 70% of the current hashrate, it will have as much transaction throughput as Bitcoin had in 2014, and that congestion will persist for 47 days.
(Blocks reached 0.3 MB/10 min in [December 2014]( With only 30% of the hashrate, the average block time will be 33.3 minutes, which makes the 2016 block difficulty adjustment period last 3.33 times longer than normal.)
However, it's unlikely that the UASF will get 30% of the hashrate. We currently see 30% of the miners signaling for BIP141 (SegWit), but we see only three miners who have expressed support for BIP148 -- Bitfury (6.3%), Bitcoin India (0.2%), and LightningASIC (â¤1%?), for a total of about 7.5%. With 7.5%, there will be as much transaction throughput as Bitcoin had in 2012, and the difficulty adjustment period will last 187 days.
@_date: 2017-06-14 23:25:11
Bitmain is only planning on doing the hardfork if BIP148 wins. 
Adding hashrate to BIP148 will just make things worse -- either BIP148 loses but has significant hashrate and we have a chain split with BIP148 as the minority chain, or BIP148 wins and we have a chain split with a hardforked blocksize increase as the minority chain. Chances are that it will be the former condition.
I believe also that most of the 30% of SegWit miners signed the [Consensus 2017]( scaling agreement, which I think is not consistent with the UASF idea. They plan to activate SegWit via miner signaling at an 80% threshold. As Consensus 2017 came later than the UASF, it's possible that some people who previously voiced support for BIP148 (e.g. Bitfury) may no longer support it.
Of that 30%, 9.5% is f2pool. Wang Chun has said that he thinks the **[UASF is evil]( It's pretty unlikely that f2pool will join BIP148. So that's at best 20.5%, even if everyone else goes along.
@_date: 2017-06-25 07:09:07
Avalon 741: 1150 W / 7300 GH/s = 0.157 J/GH
Antminer S9: 1450 W / 13500 GH/s = 0.107 J/GH
(Note: The S9 figures are based on what I usually meter S9s at, which is about 7% higher than the power consumption that Bitmain reports in their spec sheet.)
Thus, for 1 kW of power, an Avalon can get 6.34 TH/s, and an Antminer can get 9.31 TH/s, or 47% more hashrate.
If we use the Bitmain spec instead of my metered values, then the Antminer gets 57% more hashrate.
@_date: 2017-06-15 18:14:52
That doesn't make sense. If they want high fees, why would they be backing both SegWit2x and a UAHF to increase the blocksize? Bitmain's scaling plan is far more ambitious and aggressive than SegWit is:
    Time        Block size, Byte
    Now	        1,000,000
    2017 Aug	2,000,000
    2017 Sept	4,194,304
    2018 April	5,931,641
    2018 Aug	8,388,608
    2019 April	11,863,283
    2019 Aug	16,777,216
    After     	Depends on further research
Also, attacking the UASF would be mining BIP148 blocks that are empty and that orphan all non-empty BIP148 blocks. The UAHF plan is just to build something other than SegWit blocks for the people who don't want to be 51% attacked by BIP148 miners.
@_date: 2017-06-20 05:15:29
I expect about 30% of the nodes and less than 10% of the hashrate will not upgrade in advance of the hardfork. Full nodes have nothing to lose by being slow about upgrading, but miners have far more at stake. With 10% of the hashrate, the difficulty will not adjust for 20 weeks. The minority chain will not be viable with only 10% of the hashrate, and coins mined on that chain will drop in value. Since revenue is proportional to coin value divided by difficulty, a drop in coin value without a rapid drop in difficulty will motivate any remaining miners to leave, and the minority chain will die. When that happens, most of the remaining 30% will throw in the flag and upgrade. A few diehards (mostly luke-jr) will try to do a PoW change and keep the old chain alive, but they will be thought of by the Bitcoin mainstream more as an amusement than anything else.
@_date: 2017-06-25 00:15:41
Yes, the S9 gets about 50% more hashrate for the same power.
@_date: 2017-06-18 17:48:04
Your timing is off. Bitmain is planning on starting the hard fork as soon as voting for SegWit starts, not as soon as SegWit is activated.
Also, ASICBOOST reduces the power consumption, but does not increase the hashrate, so only their power company would be able to tell the difference.
Also, SegWit only prevents a specific optimization to ASICBOOST that makes 4-way collisions computationally feasible. It does not prevent the basic form of ASICBOOST in which you can get 2-way collisions via simple extranonce grinding. Although Bitmain has admitted to using 2-way ASICBOOST in testing (but not production), there is no evidence that I know of indicating that Bitmain is using Maxwell's optimization for finding 4-way collisions either in testing or in production.
@_date: 2017-06-20 02:07:25
SegWit does not disable ASICBOOST. All SegWit does is make it hard to find 4-way collisions using modifications to the right side of the merkle tree instead of simply finding 2-way collisions via extranonce grinding. People have made it sound like this is a big deal, but it's really not. It's not a 20% reduction in power consumption; more like 5-10%. Since electricity costs are about 4Â¢/kWh for most industrial miners, and revenue is about 50Â¢/kWh right now for S9s, this can account for about a 0.4Â¢/kWh reduction, or 1% of revenue. 
Jihan Wu and Bitmain have opposed the adoption of SegWit without a hard fork because they signed an agreement in Hong Kong in 2016 which stated that they would help SegWit through in exchange for Core doing a 2 MB hard fork. Core never delivered any code for a hard fork, so Bitmain chose not to support SegWit for the time being. Now that SegWit is being coupled with a hard fork again, as per the HK agreement, Bitmain once again supports it.
The ASICBOOST talk is just a PR campaign intended to demonize Bitmain and distract people from the fact that Cory Fields, Johnson Lau, Matt Corallo, Peter Todd, and Adam Back broke the agreement, and Luke Dashjr [violated]( the spirit (but not the letter) of the agreement as thoroughly as he could.
@_date: 2017-06-18 17:52:22
It would be a power penalty, not a hashrate penalty.
@_date: 2017-06-20 06:07:37
Why not both? Is it just too hard for you?
@_date: 2017-06-07 18:15:38
We might consider it if you sent us the specs (including airflow), but we're experiencing very high demand right now and are inclined to be even pickier than usual. It's better to do one thing well than to do a bunch of things with mediocrity.
@_date: 2017-06-25 10:43:07
@_date: 2017-06-06 13:38:57


Just a quick correction, here. A 20% savings in electricity does not translate to a 20x improvement in profit. Far from it. 
I'm a miner. An Antminer S9 (without ASICBOOST) currently gets about 65Â¢/kWh, or 65Â¢/hr for 10 TH/s. I pay about 2.8Â¢/kWh, or 2.8Â¢/hr for 10 TH/s. If I were to use ASICBOOST, I would pay around 2.24Â¢, for a savings of 0.56Â¢/kWh. This would increase my operating profit by about 1%.
So yeah, ASICBOOST would currently be a 1% advantage in profit for me.
@_date: 2017-06-18 18:02:41
Antminer S9s are always clocked at the frequency that maximizes their stable hashrate. The voltage per ASIC (which determines efficiency and maximum stable frequency) is not readily adjustable with the firmware they run. The power supplies (AC/DC converters) they use are generally sized for 1.6 kW, but the S9s only use around 1.25-1.45 kW *without* ASICBOOST.
@_date: 2017-06-07 03:28:20
That's an industrial rate in central Washington state. We have an excess of hydroelectric power from the Columbia River here. 
@_date: 2016-07-01 05:58:11
The Columbia river is particularly well-suited for hydroelectric projects. It has a fast flow, carves through a lot of canyons, and large elevation changes.
@_date: 2017-06-16 07:50:58
It's also possible that both will win. ETC is worth more now than ETH used to be before the split, and of course ETH is through the roof. Even days after the split, we saw ETC+ETH &gt; pre-fork ETH.
@_date: 2018-12-08 20:18:16
Reducing the block reward is definitely a soft fork. You lost. Pay up.
Unfortunately, you probably don't understand the difference between a soft fork and a hard fork, so the same reason that caused you to lose is also the reason why you can't see that you lost. Dunning Kruger at play.
A soft fork is a change where all new blocks are acceptable under the old rules, but some old blocks are not acceptable under the new rules.
A hard fork is a change where some new blocks are **un**acceptable under the old rules.
The rule on coinbase payout amounts is that the amount a miner pays themselves can be no greater than 
    50 BTC / (1 &lt;&lt; (block height / 420,000)) + tx_fees
Thus, for a 1-tx block right now, miners can pay themselves anything between 0 BTC and 12.5 BTC. If a miner wants to make a block that only pays themselves 0.25 BTC per block, that is permissible behavior. If a miner wants to only extend the chain if all blocks after a certain height pay themselves 0.25 BTC per block, that is also permissible behavior, and would constitute a soft fork attempt.
@_date: 2017-06-10 06:00:48
As someone who has owned Antminers for several years, and saw the UI elements for the "Minerlink" interface come into being but never really get functional, it seemed pretty clear to me that it was just a poorly thought out and incompletely implemented feature. Being able to remotely manage a farm of dozens or thousands of mining rigs is pretty much mandatory. It's disappointing that Bitmain's attempt at implementing that feature was insecure, enabled by default, and incomplete, but it would have been much worse if they had delayed shipping their miners until that feature was complete. 
It's much better to have a miner that has a remote shutoff vulnerability like Bitmain's machines did than it is to have a miner that is a fire hazard from a PCIE power connector running 30% above the specified maximum amperage like KNC did. 
@_date: 2017-06-18 17:58:45
Specifically, it would need to be empty blocks that try to orphan regular transaction-containing UASF blocks. Simply adding empty blocks won't change the transaction throughput on that chain until the next difficulty adjustment, which is too late. If they have more hashrate than the pro-UASF miners do (which is likely), then they can completely and single-handedly squash this UASF attempt. But they're not doing that.
@_date: 2017-06-20 18:53:38
It's an empirical question. We'll find out the answer soon enough.