@_author: brynwaldwick
@_date: 2017-03-27 20:24:32
This race to the bottom sounds desirable to me. This is the market competing to provide the lowest cost to users.
Over the long term miners will not include transactions that have fees lower than the marginal cost to include them on the blockchain. This will create an equilibrium fee pressure dictate by economic forces, not quotas.
I really don't understand how anybody could possibly advocate using controls to disrupt the competition of miners that you refer to here. This competition is healthy and it is the mechanism that has secured The Blockchain up until this point. Controlling that competition in any way goes against the fundamental spirit of bitcoin that I invested in and believe in To This Day.
@_date: 2017-03-28 19:49:48
You are correct that there is some threshold above which the block size will begin to have negative effects on these network properties you and I both value.
However I think we are so far away from that threshold with our current block size that it is an unforgivably bad engineering decision to keep it where it is to protect the network... When the network doesn't need our protection at these throughput levels but rather needs us to get out of its way.
@_date: 2017-04-14 03:51:44
"... WITH ENGINEERING", and then this would be a more accurate characterization of my beliefs, and his I'd imagine.
@_date: 2017-09-28 02:21:38
You aren't listening to them or to me though, or to Satoshi.




I don't trust you or myself or anybody else to determine the parameters of the bitcoin ecosystem. I trust a decentralized group of profit seekers. That was the design of the system from day 1 and you are fucking with it if you ignore miners. Your job is to provide software for them to run and support the software they decide to run.
@_date: 2017-09-26 18:54:23
You characterize NYA as a "backroom deal takeover" when it was intended as a way to unite the big block and Segwit sides of the community and achieve what you call consensus. Your doublespeak and selfish vision for this world changing technology is completely incorrigible and if this is how you will behave I am honestly very glad you'll be gone soon.






\- Just Some Bitcoin User
Make sure you don't let the door hit you on your silly ass, Greg.
@_date: 2017-09-28 02:08:52
Hi Adam,
This problem has long been brewing and you should not find it so difficult to understand why much of the community is fed up with the nonsense around bitcoin development and wants bigger blocks now.
97% of miners are signalling they want a larger block size limit. Their investment and their judgement and profit seeking and the direction of their hash power has been fundamental to the design of bitcoin from the first day it was operational.
Literally the only mention of the word "consensus" in the bitcoin white paper written by Satoshi Nakatomo is:


So you'll have to forgive much of the community for siding with miners, as well as drawing on practical technical experience with the internet that tells us that 2MB will still allow bitcoin to work on many computers worldwide. We need headroom in throughput above average transaction rate to grow the ecosystem. We want it now so that spikes in transaction rate are not met with ridiculous fees.
I think your time might be better spent to beg existing core developers to stay with the ecosystem and keep working on whichever implementation becomes dominant after November. You and I both value their contributions, and I hope they continue to contribute with excellent cryptography and contributions to the stability of the ecosystem -- but *not* as central economic arbiters deciding the hard-coded values of parameters that *nobody understands better than the free market*.
I really do hope you do that because I have the feeling the ecosystem is going to fork one way or the other and I hope you'll agree to work together to scale bitcoin against fiat. Please.
\- A bitcoin user (I buy it, I spend it, I pay friends back with it, and I code with it. It offends me when you say no users will agree to big blocks because I have been begging you and Greg and Luke and many others for them for some time. I want more throughput to maximize the value of the bitcoin ecosystem as soon as possible.)
@_date: 2017-09-28 02:38:25
The fact that Segwit2x gives "CEOs control over bitcoin" is a false narrative.
Many miners and many developers (including Adam Back from like 18 months ago saying 2mb then 4mb then 8mb would be reasonable) and many community members have been pleading for an increase in the block size limit. We need consistent headroom above average transactional throughput during this rapid growth phase of bitcoin and we do not have that during periods of increased demand for transactions unless we hard fork to bigger blocks soon.
I continually asked Adam and Greg for their estimates of the cost of the entire ecosystem upgrading to use Segwit to use the "transactional throughput increase" it provides. The most direct answer I got was "all this time-wasting talk of bigger blocks and trolling has already cost the ecosystem millions of dollars in wasted time".
I have heard legitimate use cases and spikes in transaction volume called "spam" one too many times. Like it's bad the system is being used or that people want to use the blockchain for microtransactions.
I've read endless posts about how all users hate the idea of a hard fork to bigger blocks and that miners are selfish and attacking bitcoin with another client when that is the main design of the system as described in this nifty paper [here](
By the way for fun... ctrl+f for "consensus" in that paper and read the damn sentence then reflect on the years of debate on this topic and the vilification of miners in this context.
I value the existing client's devs' contributions as cryptographers and system architects but I'm done with them acting as central quota planners and I hope they grow beyond not wanting to work on a system that doesn't kneel and bend to their will.
That is literally the power of bitcoin and I'm excited to see it unfold over the next few months.
@_date: 2017-05-14 16:25:56
Hi. Peter R was not allowed to present. If you don't understand supply quotas and how they effect economic equilibrium, marginal costs and marginal prices of goods, and how that is relevant to our debate then you need to have seen his talk which was blocked from the conference.
@_date: 2017-05-31 06:32:06
I think it is important that bitcoin can service all citizens of the world, including unbanked and marginalized. Lots of people who cannot afford the current fees and who likely will be unable to open their own Lightning channels with the current fees.
The main argument I'm trying to make here is that it is a matter of degrees. I don't accept that increasing the block size today (within reason) will have an adverse effect on the properties you list here, that you and I both value. I think that's just incorrect. So I want to scale the network as aggressively as possible given current technological constraints. Any suggestion of scaling in this way is shouted down as if it's some compromised shill trying to destroy bitcoin.
I respect a conservative approach w.r.t. scaling this budding global finance system... but not at the cost of system performance, especially when the system could still perform with more throughput given current technology &amp; storage costs.
@_date: 2017-05-16 07:50:52
Doesn't something like a "slow moving average times a factor" block size limit solve a lot of these attacks? This would effectively bound the size of rich blocks (from their paper), even given the exponentially distributed block arrival time of new bitcoin blocks.
The argument of many "big blockers" is not that there should be no block size limit... but rather the limit should be dynamic and be chosen by a "sensible" mechanism that maintains an extremely high degree of decentralization.
* One method for this is a completely free market approach. This is what BU represents, to me. It's chaotic and unpredictable and may not be desirable for a production system where absolute robust, predictable performance is needed... hard to say until it breaks for real, on a consensus level not talking just bugs. (As a side note, if bitcoin developers cannot raise the blocksize themselves I'd rather entrust the chain to the chaotic free market then centrally planned quotas)
* One method is a slow moving average. I like this idea most of all. I'm awfully sure proposed some ideas like this and got trolled to shit.
* Another method may be building some sort of stable incentive scheme into the transport layer of the network to balance against the tendency to "flood" for infinite fees... I am interested in research in this area.
But really some SIMPLE slow moving average that respects predicted technological progress in bandwidth, storage costs... network performance..., etc. Why isn't this already what bitcoin does? The inability to come to a solution like this that makes sense and is not complicated is very frustrating and my belief is wavering.
edit: optimized words
@_date: 2017-05-15 19:14:01
Interesting discussion thx. Doesn't orphan risk increase with larger blocks? Couldn't this be a "downward" force that could lead to some stable, bounded block size.
@_date: 2017-05-31 00:19:27
Why is it always slippery slope and semantic arguments taken to their logical extreme? I know we both want what is best for bitcoin but your debate and leadership style is so tedious and very damaging to the community.
Full blocks (when we are still very low on the block size / network performance curve for a large majority of ISPs throughout the globe), skyrocketing fees, and limited transactional throughput during a CRITICAL period for user acquisition and institutional uptake... have been Quite Damaging to bitcoin's value proposition.
This damage, especially in an environment of ever-increasing competition, is really hard to overstate. In fact I don't even have to state it... just ask the free market that we are all supposed to love and respect as a core principle.
I see it as problematic that every technical proposal is immediately politicized and viewed through lenses of motives, slippery slopes, and roundabout arguments that are hard to prove and don't take into account their own assumptions... and this is the core spirit of what Garzik's post here is about. We need more throughput for adoption of the bitcoin blockchain and that is the stated goal of the code being released. It looks pretty good to me.
Do you have specific technical concerns about why this code will break bitcoin? Can you put them in here? 
@_date: 2017-05-14 20:22:58
I think the marginal cost for a miner to include a transaction will set a minimum fee threshold in the absence of a block size limit. This is based on widely accepted economic theory.
@_date: 2017-05-31 21:47:43


That is a fairly viable conspiracy theory and an enjoyable piece of adversarial thinking, however it seems quite unprofessional to speculate casually on reddit about this. If you want to have a discussion about security model of the protocol and how this relates you should bring that up through proper channels. Otherwise you are just another useless troll.
You are not the sole arbiter of bitcoin's value proposition. In fact nobody is. The market is. Bitcoin was meant to enable more distributed business models. If many of those businesses are "conspiring" to change bitcoin, that change is probably good for bitcoin.
Now there are very many edge cases where what I just said could become untrue. That is why it's up to the public to use their best judgement, the community to continue to develop high quality code that maximizes bitcoin's value, objectively choose the best software, and thank everybody nicely for not speculating vaguely on reddit.
Honestly don't you have work to do?
@_date: 2017-05-14 14:22:34
Many individuals who wanted to present about bigger blocks or their ramifications at the Bitcoin Scalability conference were not allowed to participate, shouted down, or otherwise ridiculed. Many of the ideas that would have been presented were based on widely accepted, highly relevant economic theory and likely would have broadened and deepened the experience of those who were courageous enough to listen and learn from them.
My memory of those times makes your posting feel very disingenuous. And believe me sir I have applied about as much benefit of the doubt to this debate as I can muster...
I would like to see a clean refactor of the block format some day, because I trust your expertise that that will help grow Bitcoin long term.
I would also like to see a simple solution for more transactional throughput, and that is based on my technical experience and my experience using Bitcoin, programming Bitcoin, and running a node. To have those beliefs of mine referred to in any way as moving goalposts or obstructing progress (not by you but by relevant individuals who's judgement I really would like to support but cannot) of this open decentralized protocol... Has really been quite offensive.
@_date: 2017-07-22 00:32:42


Can you explain what you mean by this? Is it unconditional but with caveats?
Are the caveats too unlikely or too esoteric to go into? Genuinely interested in the limits &amp; edge cases of what you are trying to say here.
@_date: 2017-05-15 03:53:06
Indeed... It seems logical to me then that we want to engineer a solution where the nodes themselves place the upper bound on thruput. Maybe BU has some problems but it is the closest we've come so far to that approach and I think it's quite attractive.
Miners push for bigger blocks and more fees... Nodes as the transport layer put limits on the thruput.
@_date: 2017-05-31 01:09:15
But my core point to you is that a 2MB block size is well within the realm of physical possibility. If we are going to discuss "well then where does it stop" I would say at an intelligently engineered middle-ground between 1MB and nMB, where n is less than the possible network throughput of a well-endowed internet user. 2 &lt;&lt; n today. 
Frankly I am not convinced that it needs to stop... I think there might be a market equilibrium blocksize without a hard-coded limit... but that is a discussion for far down the road, with far greater user adoption, when bitcoin is far more decentralized than it is today because of uptake in industry.
I do know that staying at 1MB is a poorly engineered mistake that has little to do with pie in the sky altcoins and everything to do with where we sit on the block size / network performance curve... a place that is in my professional opinion Way Too Low.
I do not want to compete with pie in the sky altcoins and I respect your resolve to keep bitcoin sovereign and widely accessible, but I think you miss the forest for the trees and it's a bit myopic to shy away from increasing the max block size today.
Your read of that pull request is not the same as mine. I think it's kind of a joke that you don't have more respect for Jeff Garzik.
@_date: 2017-05-31 04:30:23
I am not asking for naively serving every customer ever like Snapchat. I'm simply asking to keep the system healthy and fulfilling its original value proposition, which is cheap, permissionless, near-instant, global transactions. That was the bitcoin that most investors signed up for and the bitcoin that the market wants.
We can have both with the way the internet works today.
@_date: 2017-05-14 17:35:16
We need to consider all perspectives if we're going to make this system robust. I could just as easily say that many perspectives of our technical experts disregard the mechanisms happening in the system where supply of block space meets demand. The most holistic and effective perspective is one that takes many narrow, varied, individual viewpoints into account.
@_date: 2017-05-15 04:05:49
Thanks for the paper I enjoyed reading it. I'm on mobile so I can't get too far into it but it looks like they are making some assumptions in their "proof"... Just to start, the attack where a miner forks a recent block in order to take the fees only souds feasible where there is not a huge swath of fee'd, incoming transactions... Something I expect during the no block reward future. It certainly is something that requires some study but in short I don't think this great paper has proved anything yet.
In a wider lens, I frankly don't think any of the mechanics of the block reward less future is relevant to the immediate needs to bootstrap Bitcoin. We need to violently maximize uses of the blockchain so that there are endless fee paying transactions in the future to make these attacks moot. Fear and over engineering for the future is creating problems IMO.
@_date: 2017-05-31 02:05:11
Well I build products on the internet that people use. That has given me context to know what a megabyte is in the context of modern internet traffic (even across a replicated peer network). It has also given me way more respect for users, customers, and network effects than you demonstrate, and it has taught me that the it is the engineer’s responsibility to engage users on their terms, not dictate the terms of the product or its use cases. I’ve watched countless use cases and institutions shy away from bitcoin after deeming its properties and future prospects unfit for their business. I’ve built apps with different blockchains and seen them slowly become pointless and useless when implemented with bitcoin vs. ethereum.
 
It was your/your team’s idea to put a capacity increase into segwit. I thought it was a bad idea from the beginning. It...
 
* Conflates the benefits of a block refactor with transaction throughput increase, which makes it difficult for the ecosystem to signal preferences for either feature
* Introduces another arbitrary, hard-coded parameter (witness data discount) into the code. What is the rationale for this?
* Increases the “spam surface” of a block without increasing the effective number of inputs and outputs that can be included. You’re telling me that an attacker could create 4MB blocks that only contain 70% more inputs and outputs than a vanilla 1MB block? That sounds pretty bad to my admittedly naive notion of how bitcoin works.
* Is being used as some carrot to push segwit through. I like the idea of segwit enough to know that it could be adopted on its own merit. Making it into a throughput increase with an arbitrary parameter is really clumsy. Why am I supposed to like segwit? Is it because it cleans up the block format? Is it because it offers a transaction throughput increase? I thought it wasn’t a scaling solution… The cognitive dissonance from the arbitrary witness discount parameter is real.
 
Again I like the idea of segwit and look forward to when it is activated… but you and your team are the engineers who decided that segwit blocks should be bigger blocks, not me. It seems bad to me.
 
What are the qualifications that make you fit to dictate a critical change to the way bitcoin operates (that we need full blocks in this stage of growth for the system to function)?
@_date: 2017-06-01 04:17:57
I am quoting from another post of mine in here.
The main argument I'm trying to make here is that it is a matter of degrees. I don't accept that increasing the block size today (within reason) will have an adverse effect on properties like fungibility, sovereignty, decentralization and immutability that you and I both value. I think that's just incorrect. And believe me I want all those things also though I am a big block shill... I am interested in bitcoin because I think it can change the world for every person.
So I want to scale the network as aggressively as possible given current technological constraints. Any suggestion of scaling in this way is shouted down as if it's some compromised shill trying to destroy bitcoin. It's really discouraging, when most of us just want the best for the chain... I only want to maximize the number of use cases there are for bitcoin.
As engineers I think we lack the proper context for any more philosophical discussion here... Are there stated system specs somewhere for the reference client that says what "decentralization" means as a function of hard drive space and memory?
We can argue around about rates and about what we are optimizing for. How much memory does the UTXO spam take up? How costly is that? What is the projected cost for a blockchain-worth of storage given 2MB blocks in 2020? In 2025? What is the projected cost for an assumed smooth increase to 16MB over that time?
Without being able to answer that right away I think neither of us have enough information to say what the optimal block size limit is. Or what the network can handle and still remain "decentralized".
(imo this is also good reason to let nodes signal a maximum block size they'll accept. what more elegant way to deal with some of the externality issues we have discussed before?)