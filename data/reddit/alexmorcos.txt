@_author: alexmorcos
@_date: 2015-12-17 17:58:55
I hope it is only temporary.  He would be sorely missed from the project.
@_date: 2016-03-05 05:10:14
That's an argument similar to one I've been making.  But on reflection, I don't see why that is the obligation of a few volunteers to present the HF that everyone can rally around.  That's obviously a hard thing to get right.  Why does Core have that burden?  I think the burden lies on those who most want to change the rules, and so far they haven't been able to convince the community of a particular proposal.
@_date: 2016-03-04 22:36:08


This is really depressing. Developers who contribute to Bitcoin Core do not control Bitcoin nor have responsibility for it.  Nor is it their obligation to help convince the community to agree to a rule change.  I think it would be reasonable to expect Bitcoin Core to implement rule changes that the community has accepted, or to expect the community to simply fork the code and do it themselves.   But we haven't seen that happen yet, first we're talking about 20MB or 40% growth, and then we're talking about pushing something through in 28 days without any concern for non-upgraded nodes.   The community hasn't accepted these changes.  What is it that Core should have done differently?
Do I have a personal obligation to help write consensus rule changes that increase the block size?  To help convince everyone they are a good idea?  Why?
@_date: 2016-03-02 15:04:50
0.12 is more conservative with its fee estimates than 0.11.  Requiring 95% of recent txs to have been confirmed within the target number of blocks rather than 85%.  Until miners start rebuilding their block templates more frequently, I think there will never be an answer for estimatefee 1.  
Recently I also noticed that a couple of empty blocks can cause estimatefee 2 estimates to jump to relatively high values as well. It's a good idea to sanity check estimates.
There is a new RPC API called estimatesmartfee, it is still under development and the API is subject to change, but I think it'll be used in the future to add a bit more intelligence to estimates.  Feel free to experiment with it. It's what Core uses internally.
@_date: 2017-03-13 15:41:40
This isn't data, but my opinion is that maybe the economic majority would like a small increase in block size via HF, but not at the expense of a contentious fork or putting the network in the hands of a much smaller less experienced development team.
It is clear that a significant fraction of the economic activity in Bitcoin is opposed to a HF right and in light of this the majority doesn't want to proceed with BU.
@_date: 2017-03-24 18:25:26
It requires long term rational actors.  And if miners foolishly believe they unilaterally know which chain is more valuable and kill the other chain, they have eliminated the possibility that their rational action is to switch back to the minority chain after the market eventually shows it is more valuable.
Also Bitcoin is new now.  We are setting a precedent for a culture.  That culture is important to it's long term success.
@_date: 2016-03-04 22:48:34
I don't know if it makes you feel better but some of my posts are disappearing as well.  (well never appearing in the first place).
@_date: 2016-01-13 20:46:54
Jeff that also sounds very reasonable to me as a moderation policy.
I disagree with the notion that the best way for Bitcoin to work is for the majority to just choose what the rules are, I think changing the rules should be very hard.  But I think every individual has the right to choose for themselves what their own bar for changing the rules is.  
Alternative implementations with different rule sets give them a tool to exercise this right.  Civil discussion of alternative implementations is a way to have debate within the community over what the rules should be and whether sufficient consensus is reached to change them and thus should be encouraged.
@_date: 2017-08-24 18:29:25
I hope it's obvious that Luke is alone in believing this is a good idea.
@_date: 2017-08-08 13:25:08
Did you even read your own comment?
You are already conceding that many of the improvements are going to happen on the legacy bitcoin core implementation. Do you not think there is a lesson to be learned from the fact that the vast majority of developers who work on the Bitcoin protocol all agree that we should not be doing NYA2X?
If your argument was this is just a few stuck up clowns and we'll eventually build a highly competent dev team for NYA2X, they'll just be different, that would be one thing.  But that's not your argument, because experience has shown that's not what happens.
Why is it that almost anyone that bothers to really understand how this technology works comes to the same conclusion that NYA2X (aka XT 4.0) represents the wrong way to evolve it?
@_date: 2016-12-30 21:52:42
Another potential obstacle to doing this is that any non-cash gift (other than publicly traded stock) over $5K is required to have a valuation attached to the form 8283 when you file your tax return.  So you will need to pay for an appraisal.
@_date: 2015-08-25 23:09:07
Sigh... It's not just the Blockstream devs.  Many other devs are against implementing BIP101 in core (although not necessarily against some blocksize increase)  But apparently repeating this ad infinitum has no effect.
@_date: 2017-11-07 01:37:25
I don't actually think that makes sense. With **optional** 2-way replay protection they could achieve exactly the same goal.  In fact if anything, I think without the RP, they are less likely to achieve their goal for two reasons:
* They are earning ill will from the community
* It becomes more technically difficult to express your opinion in the market which I think is beneficial towards the more technically literate and more likely to control their own coins No2x side of the fork.
The only actual reason I could see for no replay protection was if they wanted to try to mislead people from the beginning into thinking there was no need because there wouldn't be two forks.  But even that doesn't make sense because they were actively considering RP just a couple of weeks ago when it was clear to everyone that wasn't the case.
@_date: 2015-08-27 14:41:34
Only 2 of the commit access devs work for Blockstream.  I think other than Gavin the other 2 are either against 101 or at least in favor of finding a compromise that can garner wider support if possible.  So even if the Blockstream devs quit today, BIP101 wouldn't get merged into core.
@_date: 2015-08-29 17:15:09
That's news to me.  Pieter, Greg and who? 
Wladimir is employed by MIT DCI (just like Gavin). 
Jeff is mostly an independent contractor I think who does some work for BitPay
@_date: 2015-08-15 23:56:08
My hope is these workshops will represent a wide diversity of community members and help give everyone the same knowledge base of what is technically possible to do with Bitcoin now, where the pain points are in trying to scale it, and what we hope will be technically possible in the future.   We should combine that with an understanding on what the perceived tradeoffs are with different approaches from the various constituents.  Once we've accomplished that,  maybe as a community we'll be able to reach consensus on how to evolve our money.
@_date: 2015-08-19 15:00:03
Given that over half the hashing power is in China and presumably they are not limited to 2mbit/s internally, I'm not sure its the Chinese miners who are being protected.
@_date: 2016-02-16 04:06:22
Yes I would recommend against ever using a txconfirmtarget of 1.  (this is partially a side effect of miners not rebuilding blocks very frequently making it hard to have a sufficiently high likelihood of getting into the next block regardless of fee)
Actually the release of 0.12 will make the estimates even more conservative (and in general slightly more expensive), which will cause a target of 1 to almost never return an answer.  However there is also an improvement to do smarter estimation which among other things will find you the lowest target which does have an answer if there is not one available at your target.
Future improvements will include providing estimates beyond 25 blocks and taking into account current mempool state.
Conceptually what's happening is fee estimation is telling you what the lowest fee is such that 85% of recent transactions with at least that fee were confirmed within your target number of blocks.  With 0.12, this success threshold changes to 95%.
  
@_date: 2016-02-15 17:11:32
I also worried it would come across that way.  I don't think that was the intention though.  It was more by explanation of why 0.12 has taken a bit of time and there have been so many RC's.  A lot of effort has gone into the release.
@_date: 2016-02-21 20:11:50


I personally am in favor of a larger discount as well.  However I believe the original segwit discount was partly chosen to help achieve capacity increase.  A discount that was just targeted for the incentive value may have been closer to 2-3.  My personal choice would be after the HF to have 2MB base size and 2.5x discount so thats a 5MB worst case block and effective capacity of a little over 3MB.  Makes the HF worth it in terms of capacity bump and still has a significant incentive for limiting the UTXO.  However I think if we're going to ever actually move forward with evolving the protocol, I think we have to all be a little flexible in consenting to rules that aren't exactly what we would choose, otherwise we'll never be able to make any change.


This is also misleading. If the second limit is chosen to be one that primarily protects against attacks, then block creation code only needs to check it at the end and not optimize for it and heuristics can be used for policy.  This is exactly the same as the case with the sigops limit.  Even if the second limit was in play more often, block filling is always only a best efforts algorithm anyway, i don't think it would be hard to build something workable.
@_date: 2016-02-24 03:44:26
I also agree with what he actually said.
@_date: 2016-02-29 22:31:00
More or less, yes it does fix it.
This code could easily have been ready for 0.12 if we weren't spending all our time fighting about block size.
I'm not saying that to be obnoxious.  I'm sure blame can be assigned many places, but I think its useful for the community to understand how destructive the situation is.
@_date: 2016-02-21 03:08:27
It's not important if Mark is opposed or a couple other core devs are opposed.  What's important is if the community supports it.  And if the proposal has overwhelming agreement from the community then Core will be happy to deliver on it even if a few individual contributors wouldn't pick it as their first choice.  (It's not my first choice, I'd prefer a greater change in the block size)
@_date: 2016-02-08 21:49:48
Speaking for myself.  I'd be fine with a plan like this.  I still have concerns about setting a precedent for just raising the block size every time we have more transactions, since I don't believe we can scale that way forever.  However I think that needs to be balanced against the real world problems faced by businesses and users while we don't have the technology for other scaling solutions developed yet.  A plan along these lines demonstrates that we can use some of the limited technical headroom to provide short term relief (alleviating concern that the protocol can never change) while at the same time demonstrating that we must remain committed to building a decentralized future.
The key to implementing something like this is finding a set of rule changes that the whole community could consent to.  What scares me most is changing the rules against the wishes of a significant minority.
@_date: 2016-09-08 13:52:18
After [mempool limiting]( was merged for 0.12, nodes that had their mempool fill up would not accept any transaction which paid a fee rate below the new mempool min fee (determined by the fee rate of txs evicted by the limiting code).  The feefilter message just serves to advertise this mempool min fee to your peers so that they don't bother sending you txs which you wouldn't accept anyway.  It shouldn't really change behavior other than saving some useless network traffic.
Further discussion in the [BIP](
@_date: 2016-09-23 14:38:55
With the segwit soft fork the consensus rules will change.  So miners will be allowed to put more bytes from witnesses into their blocks than bytes from "base block".  This is one of the fundamental features of segwit.  
So to answer your question of "why would they?", well if they care about maximizing their fee revenue then they will try to pack their blocks with maximum fee.  Luckily, the default mining algorithm will already do a pretty good job of this for them.
@_date: 2017-09-19 12:26:45
I used to think that too.  Then I realized I don't think it's a good idea.  We should not be raising the block size every time we barely manage to have enough transactions to fill the existing size.  But at the end of the day, it doesn't matter what I think.  What matters is that this idea of raising the block size has remained contentious.  By some sort of happy accident we were able to get segwit activated which does effectuate a 1 MB increase and has the benefit of that increase being gated on adoption.  But any further increases are just not agreed upon sufficiently to happen right now. People can complain all they want that it's a few people blocking it, but I think if you believe that you might as well give up on Bitcoin.  No small group of miners, devs, or companies has the power to force through or stop a change if its overwhelmingly supported.  So I'm not worried that the NYA can force through 2x.  I do wish they'd realize that though and take steps to avoid the confusion that is bound to happen in November.  
@_date: 2017-09-18 16:52:33
Erik, I'm not sure why you think this won't be a 50/50 situation.  Both sides think their side has the majority.  Either one of us could be wrong.  But it is the possibility that you might be wrong that you need to consider.
I hate to say this, but it seems like you are taking the path of least resistance.  You know the 2x organizers don't want to add strong replay protection and you don't want to start internal strife by arguing for it.  But if you were to look at this situation objectively, I think you would clearly see that that is what you should be doing.  I believe that deep down you can not be truly confident that this will be an overwhelming victory for 2x.  You must sense there is significant uncertainty.  
@_date: 2017-10-04 11:51:11
If Core merged the 2X code and the community just followed them, I would be incredibly disappointed.  Luckily I'm very confident that would not happen.  Someone else would spin up an alternative implementation for people who didn't want a bunch of business CEO's (with or without Core) deciding the rules of the network without consensus.
@_date: 2017-10-26 15:16:11
you did the right thing reporting it.
I'm sorry, I'm unsure why you did not receive a response on thread, but I'll make sure someone follows up.
Personally I haven't written a whole line of code in over a month, been too busy wasting my time with the 2X fiasco.
@_date: 2017-10-04 13:39:53
No of course it's not.  But they didn't create a proposal and then try to build support for it.  They got in a room together and dictated this was the Bitcoin they'd be running.  The very first attempt (XT) was also done this way.  If they'd really tried to build support and agreed to only go forward if they go it, they might very well have succeeded.  And people like me might have gone along even if it wasn't what we would have preferred.
@_date: 2017-10-06 14:36:30
Please don't consider the standard brand as defined by Core.  Consider it defined by whatever it was yesterday unless you consent to it being changed. One factor that I think should be important in your consent is the question of what is everyone else doing, and if someone is changing BTC in a way that is leaving a significant part of the community behind, it better really be worth it to you.
@_date: 2017-10-24 03:13:15
Innovation is publishing research and open source projects, not marketing for get rich quick schemes.  Why is this being announced via Bloomberg Technology if it is innovation instead of a white paper?
You are right that we should wait to see what the actual project is.
But I'm tired of all fraudsters, scammers, and con artists jumping on the band wagon and trying to get rich quick with their altcoin or ICO or garbage technobabble.   
So maybe we're a bit quick to rush to judgement, but I think in this envirornment it's not surprising, we'll find out tomorrow I guess.
@_date: 2017-10-01 12:29:00
@_date: 2015-09-09 04:35:26
The article claims 8% of blocks were mined in support of BIP101.  That is off by an order of magnitude.
@_date: 2015-09-02 11:28:51
I view it as a counter to the BIP101 letter as it should have been done.  If industry had all signed a joint statement of support for increasing block size soon, that would have been great and productive.   The response would have been this, we hear you, please give us a little time, we are committed to the same goals.   Maybe I just need to get over my personal feelings about how offensive the BIP101 letter was to me, since thats what I'm asking everyone else to do, so we can move forward.  I just want to discourage such unyielding statements in the future.
@_date: 2015-09-02 11:21:47
I agree.  For the most part the existing development process hasn't been given a chance to see if it will work.   I know Gavin and Mike have been pushing for raising block sizes for years.  But speaking for at least myself, it wasn't clear to me until only quite recently how strongly and broadly this view was held among industry and community members.  And by that point the acrimony on both sides had stymied all forward progress.  We can argue about whose fault it was that the message wasn't heard, or we can acknowledge that the message is now received and see if progress happens.
@_date: 2015-09-02 11:23:22
I wouldn't describe myself as standing behind him.  But I would describe myself as sharing many of his view on the issue at hand.  I can't speak for every one of the other signers, but I do know a LOT of them share some of the same concerns and goals.
@_date: 2017-05-23 21:03:18
Core was invited.  I was asked by Barry to relay the invite to Core devs even though I warned him that most people from Core didn't think it was their place to show up at a meeting that was designed to make a binding agreement for the entire ecosystem.  I privately relayed that invitation to a number of Core devs (not including Luke) but did let a larger number (including Luke) know I had done so.  If Luke had wanted to attend, he would have been welcome, I am confident.  And he could easily have asked me at the time I mentioned the invite.  
But to be clear I don't think Luke or anyone from Core should have be party to making such an agreement.
@_date: 2017-05-24 15:31:12
ha ha, and I didn't mean to imply I told everyone but Luke.
I just relayed the invite to a few.  But yes, the invite and the request to sign on to the agreement were comingled.  It was clear to me at least Barry would have welcomed participation from Core Devs even if they wouldn't sign the agreement, but I perhaps relayed that inadequately, and I also don't think that made a lot of sense.  No one else going to this meeting would really have appreciated that.
@_date: 2017-05-23 21:05:19
See 
@_date: 2017-05-24 15:28:04
Core Devs were invited (at least I was asked to invite them).   But how does Core "agree" to anything?  Given the purpose of the meeting was an agreement, it didn't make sense for anyone to go.
@_date: 2017-06-26 17:46:59
yes if you download, compile, and run the master branch.
i'd take care using the master branch with actual funds, but it should be fine to get the fee estimates.
@_date: 2017-06-25 13:48:50
There is a new fee estimation algorithm in Bitcoin Core 0.15 which has already been merged to master.
It allows estimates for longer targets (up to 1008 blocks) and allows for a non-conservative mode.  The non-conservative mode is much better at reducing estimates as market conditions become less congested.
It is estimating 20 sat/B for 6 confirms and 5 sat/B for 25 confirms right now.
You can follow  on Twitter for latest estimates.
It should be noted though, that as the name implies, there is some chance it misjudges changing conditions and if you don't have CPFP or RBF as a backup option, you might want to consider using conservative estimates which are still much higher.
For a fuller description of the algorithm and changes:
