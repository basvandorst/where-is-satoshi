@_author: davejh69
@_date: 2015-12-05 20:05:09
Consumer electronics are still improving at a huge rate though- mining will follow the same technology path
@_date: 2015-12-21 23:44:15
Fees are slowing inching upwards at the moment, as are the mean block sizes, but there are still blocks being mined with no fees and there's a definite question about how many transactions are actually "real" as opposed to just shuffling things about. With that said, daily fees are running close to 40 BTC now; that's about $18k and there are around 220k transactions so we're at about $0.082
@_date: 2015-06-08 16:36:10
Whether this is really 21, Inc. or not isn't clear, but does anyone have a good theory why they're sending coins to the same destination wallet using multiple outputs per TX. There are frequently many outputs going to the same place and all that seems to be doing is making block sizes larger? For example: 
@_date: 2015-12-21 23:58:02
I usually run my own. blockchain.info has one but I'm not convinced it's accurate; it doesn't stack up with their (seemingly accurate) one for total daily fees in USD: 
@_date: 2015-06-10 14:56:23
This doesn't seem to work either - they amounts are pretty random and aren't being spent. This only seems to make it less efficient in the future.
@_date: 2015-06-10 14:55:07
I wondered about the testing idea but it seems to have been going on for a very long time. That's a very long "test" especially as it's using up block space.
Moving coins doesn't really match the transactions - they're moving from the same input wallet to the same output wallet.
The only thing I can think of is that they're somehow trying to obfuscate something but even that makes no real sense as both the input and output wallets are easily tracked.
@_date: 2015-12-05 20:02:56
Until the last month or so it's been pretty steadily increasing all year- early last year I'd estimated things would steadily increase as hardware replacement cycles slowed and process-related changes drove the GH/J numbers but if nothing else changed it would be reasonable to continue to expect about a 2x increase in total hash rate every year (more if the BTC price increases). The block reward halving will be interesting though
@_date: 2015-06-24 05:10:28
There are pros and cons. If we accept that there's a need for a hard fork then I think this is something that has some merits; the discussion of whether such a hard fork is warranted is a very different thing. Certainly I'm not convinced that there's been enough analysis done to understand the various pros and cons, which for me at least would leave me erring on the side of not wanting to see any changes until those things have been done.
The pros are pretty-much what I outlined in the video. There are potentially a few others but they're more minor. There are a few things that would concern me a little though:
1) The orphan rate will increase. This is to be expected but we'd really want to know by exactly how much. Propagation delays need to be factored in here.
2) The reduction in the per-block fee may accelerate a risk I noted a few months ago in which there may be an incentive for a miner to try to steal an earlier block reward by re-mining a previous block and then mining a new one at a point where fees become larger than the block reward. This is a long-term systemic risk right now irrespective, but reducing block times and reducing block rewards per block would risk making this happen sooner.
Not so much a risk, but certainly an issue is that for SPV clients we'd really want to stop talking about "number of confirmations" and switch to "minutes of confirmation" instead. At 5 minute blocks then 6 confirmations only give 30 minutes (mean) of confirmation security and not the 60 minutes that we'd see with 10 minute blocks. Similarly at 3 minutes then 6 confirmations would only give 18 minutes.
@_date: 2015-12-21 23:53:19
10k transaction backlog isn't that unusual. We have 220k transactions per day and they're not evenly distributed over a 24 hour period. That's a mean of 152 per minute though.
If we get a 30 minute inter-block period (pretty common) then that would account for about 4600 transactions, but they'll drain away as faster block arrivals happen.
I just did a quick check and can see about 6200 outstanding, but I can see that a lot of the older ones have no transaction fee so there's really no incentive for a miner to mine any of those (although some do get mined).
@_date: 2014-06-05 18:38:14
Almost 3 months on I took a look at this again. We're seeing something that looks like the same spike happening on the same schedule again. What I hadn't noticed before is the trend in the low points between spikes. That's a surprisingly smooth curve! 
@_date: 2014-06-13 17:38:43
While they may well be getting a larger fraction of the total hashing capacity there's absolutely no way that looking at the reported numbers day to day can tell you anything very interesting. A 5% or even 10% increase in apparent hash rate is well below the statistical noise floor
@_date: 2014-12-21 14:24:52
You'd have to hard fork the protocol to reduce block times from a nominal 10 minutes to 5 minutes. You'd halve the reward per block to 12.5 BTC meaning that we'd still see 150 BTC per hour.
The problem is that this requires a hard fork which is rather difficult to do. Doubling the block finding rate would increase the orphan rate a little, but would reduce the time to first block confirmations and would also reduce the periodic (and quite normal) long inter-block gaps that appear at times.
@_date: 2014-12-21 11:42:35
There's another question here which no-one appears to have asked. What happens long before we hit the limit?
The answer is that as long as we have less than 144 Mbytes of transactions per day (slightly more if the hash rate is expanding, but right now it isn't) then we'll see first confirmations on transactions starting to take incrementally longer. Fees will set the priority order in that larger inter-block times will tend to fill up with higher value transactions and lower-value transactions will take a lot longer to get their first confirmations.
Another secondary factor is that if a miner has a series of blocks then their correct economic incentive is not to select the transactions with the highest fees, but instead to select the ones with the highest fee per byte.
At &gt; 30% mean block size then the effects on mean time to first confirmations start to become noticeable; at 40% they become a lot more noticeable and at 60% the mean time is about 3 minutes higher than it was a 30%. It's more than 5 minutes higher at 70%.
Earlier this we we saw the mean for a single day at 43%, while early indications (over the last month) are starting to show an upward trend where previously they'd been falling as a result of improved network infrastructure
@_date: 2014-11-19 11:46:25
I did a similar calculation a few days ago based on an assessment that filling blocks up would cost about 0.4 BTC per block and with the current 1 Mbyte block size limit. The cost would be a little over 400 BTC per week.
We've already (quite recently) reached the point where the mean block utilisation is sufficiently high that block confirmations are starting to slow down for lower priority transactions, but the effect is quite small right now. 
Such an attack would artificially push up the speed with which higher transaction fees would appear, and would almost certainly speed the adoption of newer node/wallet software (miners would want to ensure that they mined the most valuable transactions, users would want to ensure that transaction fees were sufficiently high to get transaction confirmations).
@_date: 2014-11-19 10:59:53
Both are attempting to provide a means to offer characteristics that Bitcoin can't since at this point it's very hard to do anything with Bitcoin that requires a major hard fork.
This doesn't mean that either of these are actually the right way to go (and  I have concerns over incentives for miners on all of the proposals I've seen so far), but it seems likely that we do want to have a means to get faster confirmation and to scale the transaction processing rates (and this has implications for transaction fees so there's no simple answer there either).
@_date: 2014-06-25 10:44:34
One problem with this assessment is that it doesn't account for the jumps forward as a result of some particularly insane hash rate increases last year.
Another way to look at this is to see how long an average difficulty period is when we have different network expansion rates. At 1% per day this is 12.37 days and corresponds to a 13.1% difficulty increase. At 2% per day this is 11.19 days and a 24.8% difficulty increase (see 
As I'm typing there are 112267 blocks to find so that's approximately 56 difficulty changes away.
Even sustaining a 1% per day network expansion seems impossible though because we're near the limits of what the ASIC technology can do. To put this in perspective 13.1% expansion for 56 difficulty changes would increase the global hash rate by a factor of just under 1000x (986 in fact).
At the 1% per day level though we'd reach our block reward halving about 90 days earlier than the 2016-08-12 currently estimated by bitcoinclock.com - that's 2016-05-14.
In practice that 1% per day level isn't even vaguely possible for the next 23 months so I'd guess we're looking at sometime nearer early July 2016.
@_date: 2014-12-21 08:30:39
If you reduce block times you also need to reduce the reward to keep that block reward per day essentially constant. For security the use of 6 confirmations for an SPV client is essentially an approximation for "1 hour", so if you doubled the block finding rate then that 6 confirmation number would need to become 12.
@_date: 2014-03-13 22:46:22
What seems so odd is that it's every 7 months or so. After each spike there's around 5 months where the price drops before surging up again. Why 7 months and not 12, 4 or randomly scattered?
@_date: 2014-04-05 22:21:23
To put this in context, when the block reward halves from a nominal 3600 BTC per day to 1800 BTC per day in 2016 that's a lot of transaction fees that need to be generated to make up the difference. They're currently running at about 10-15 BTC per day so we'll need more than 100x more transactions or higher fees per transaction.
Last time the block reward halved the BTC price jumped up shortly after - that was fortuitous but it seems unlikely it will happen the same way next time.
@_date: 2014-06-26 12:05:42
I don't see any strong evidence that this happens. I'm not sure it would make much sense either. Removing hashrate just before a difficulty change hardly affects the next difficulty at all but means that you don't get any mining rewards at the lower difficulty.
Unless my maths is way off the only "right time" to add hashing is immediately and the only right time to remove it is when it's costing more to run than it's generating.
@_date: 2015-01-21 05:17:51
IBLTs are a very interesting idea, but the analysis I've seen so far suggests that it's far from clear that they're sufficiently well characterised. I'm also curious if anyone has worked up a good threat model for this particular change? Any change that results in nodes having to rely on reasonably "good" behaviour from others seems like it introduces scope for deliberate misbehaviour.
@_date: 2015-01-21 18:58:00
I looked at the proposal in more detail last night and IBLTs have the scope to give quite interesting speedups.
There are still issues though because our 20x increase in transaction volume would itself risk choking up the network under some circumstances. Mean transaction propagation to 90% of the network is around 3.3 seconds but that will increase with a 20x volume increase. Also for IBLTs I'm guessing that we'll need to see a higher propagation percentage than that.
I believe there will need to be some threat analysis done though because I can imagine scenarios in which an attacker might withhold transactions from other miners specifically to cause them to lag even further behind (adds a new twist to block withholding). I'm not saying that I can see a major risk here, but it's one that needs to be assessed.
I disagree entirely that larger blocks will mean more revenue though, unless the thresholds for transaction relay are pushed back up. Even if we were to scale the current transactions 20x then we would still see less than 10% of the total mining revenue coming from transaction fees (20x right now would still put us at less than 300 BTC per day). In practice the network needs to be closer to 50% coming from fees just after the next block reward halving.
If there's no block scarcity then, if anything, fees are much more likely to reduce down to their minimum level (why pay more if it makes no difference?) and so 20x the volume would be likely to see the average fee per transaction reduce dramatically. You can see this at work by comparing the following two charts (for the last 2 years):
Transaction volume has doubled but transaction fees have slumped.
@_date: 2015-05-14 22:55:37
It's pretty easy to construct a simulation for the Bitcoin network. It gets more tricky if you want to simulate large numbers of nodes but it will still run really fast. The trick is to avoid simulating actual mining and instead just simulate the timings between blocks being found.
A lot of the ones I wrote for various articles on hashingit.com are over on github; they might not be sufficient but they might give some ideas on how to build something more detailed.
@_date: 2015-01-21 18:34:56
2.4 transactions per second, or nominally 1440 transactions per block (mean).
@_date: 2015-01-21 00:24:16
The current max isn't 3, it's more like 2.4. The current set of mining pools are generating a mean maximum block size of just over 750k bytes, not the maximum 1M bytes. If all of the pool operators increased their soft limit them we could get to 1M bytes.
Incidentally this is a problem for Gavin's proposal. If the large pool operators decide that large blocks constitute a commercial risk (e.g. risking losing blocks to orphan races) then they will continue to mine small blocks.
@_date: 2015-01-21 23:30:38


This neglects that the security of the network comes as a function of the investments of miners. If the BTC price went to $1M then the daily reward (without fees) would be $3.6B and now the shortfall not made up by fees at the next block reward halving would be $1.8B. The $3.6B per day may seem fanciful but if it's protecting ~~$100T~~ $1T in transactions per day then probably not.
Incidentally I did spend some time looking at exactly the BTC and USD denominated rewards from mining a couple of month ago: 
The network did not react to falling BTC prices by compensating miners with higher BTC-denominated fees so that the USD-denominated costs were being met.


Current transaction volume is about 100k per day so if we had 16M per block then that's 23,000x larger. If we're seeing that many transactions then the network security would need to be dramatically more expensive than the $5k per block right now. The incentives to find a way to successfully attack a network performing ~27k transactions per second would be way higher than they are right now.
@_date: 2015-01-21 01:07:49
Almost certainly not. The vast majority of pools don't even mine 1M byte blocks now (pretty-much just Discus Fish/F2Pool). Many are using the soft default in the Bitcoin Core implementation at 750k bytes.
Large blocks with insufficiently high bandwidth to the entire network will result in much higher block propagation times and that means orphan races. Losing an orphan race costs 25BTC right now, vs a potential reward of &lt; 1BTC in fees.
Assuming the pool operators are rational then they'll do the maths and will set whatever limit makes sense for them (i.e. they'll want the fees collected to offset the incremental risk of losing an orphan race).
@_date: 2015-01-21 00:26:16
The problem is that the real number is much lower because many transactions are much lower. It hasn't actually been 7 TPS for a very long time (basically since about late 2010): 
@_date: 2015-01-21 01:15:15
I'm not sure why it would. The primary risk with orphan races would come from latency in propagating solved blocks. Right now the mean time for the network to propagate a block to 50% of the nodes is around 5 seconds, but the 90% level is more like 20 to 30s. Make the blocks bigger and that will make things slower.
Yes there are implementation and technology improvements that can be made, but most large pools are already connected via high speed infrastructure and there's no magic 20x speedup coming any time soon (nor anyone offering to pay for such a speedup AFAICT).
@_date: 2016-01-17 18:33:29
Depends on what you're looking for. Are you looking for retail transaction processing or things like bulk money transfers between banks? The volumes are much higher for the former, but the values are *huge* for the latter.
@_date: 2015-08-13 21:19:05
Many small APs and small Wi-Fi devices are designed with specific constraints (sometimes hardware constraints based on cost). This was especially true a couple of years ago. Yes, enterprise Wi-Fi can handle many more connections, but you'd be amazed how many devices just crash when you push them slightly too far.
Wi-Fi is a shared medium so slow devices or those connected at range consume disproportionate amounts of air time and thus reduce channel capacity- this is why many large Wi-Fi designs fail when too many users connect.
I agree entirely that fridges and toasters make no sense- I thought I'd said just that, but I may have misremembered what I'd said. Routers and STBs have an entirely different set of problems, often based on thermal characteristics, cost and regulatory issues associated with energy efficiency.
@_date: 2015-08-14 03:20:33
Totally agree that the IoT domestic kitchen doesn't work. Oddly it does work commercially- large supermarkets have been doing networked control and monitoring for 25 years. Mining in these devices would not make sense though for thermal reasons. Mobile devices make no sense either (thermal, power management, cost). I'm not sure what that really leaves where there's not an incremental cost for connectivity, etc? I already mentioned other problems with routers and STBs.
I'm not sure there's anything presumptuous about the maths. Mining makes almost no economic sense to any domestic user because it will invariably cost more in electricity than the mined coins. Buying coins makes far more sense in the vast majority of cases.
If 21 (or anyone else; I certainly have no axe to grind with 21) take any revenue from mining they're making the economic case far worse from the end user's perspective, while at the same time acting as a point of centralization in the network (as any large pool does). If they're not doing revenue sharing then they have cost, thermal, etc problems (typically multiply BoM costs by 2.5x to work out the incremental retail cost). Why will an average consumer pay for mining as a feature? Why would manufacturers add it and risk being more expensive, plus having to deal with increased support costs?
Mining is a zero sum game; everyone can't win. Much better that people who care, and make conscious informed choices about the pros and cons, do so.
As for Wi-Fi, there is a huge difference between an Enterprise grade Aruba device that an domestic AP. The team at Aruba (and others) do phenomenal amounts of stress testing and their entire value proposition is based on maximizing the ability to connect devices as well as possible; low cost designs don't do this. They are designed to fixed costs by engineers who don't have the time or facilities to do large scale testing. Some work very well: I have a Netgear AP that happily handles 30+ devices and never misses a beat; it replaces one I won't name (but which has shipped in the millions) that starts dropping devices as soon as device number 15 comes online). In 10 years of working with Wi-Fi I ran across far more bad APs than good ones.
@_date: 2015-08-13 21:12:11
The "former" tends to be inserted by other people :-) I decided to move on to PeerNova because I believe crypto-ledger technology is a much more interesting space. Qualcomm have some exceptional people- the Wi-Fi team have quite a number (and there are many more besides)
@_date: 2015-08-13 21:35:50
I can't speculate on what 21 may or may not do competitively with PeerNova, but my comments were made on the basis of the analysis I have done, and continue to do, on Bitcoin mining, not what I do for PeerNova (which is rather different).
As regards immutability I think it's premature to judge any idea without understanding what it might be. In particular though, not every problem requires the same set of constraints as Bitcoin.
As regards my LinkedIn, I'm more than happy for anyone to verify any aspect of it- please do :-)
@_date: 2016-02-24 16:19:56
Not too surprising given that the BTC:USD price went from $230 to $420 in the same time period (1.83x - quite close to double)
@_date: 2016-02-03 23:59:09
That's an execellent idea. I'll try to get to this sometime in the next day or two :-)
@_date: 2015-09-03 23:27:37
Many pools were already clearly using different limits (as evidenced by the largest blocks each created in any given week). I plotted the trend to estimate the peak practical block sizes earlier this year:  I'm more than happy to share the data via GitHub if anyone's interested 
@_date: 2015-02-20 12:32:50
In general it's safe to say that both users and wallets will drive their fees to the lowest level that they can get away with. This is a "classic tragedy of the commons problem". You can just imagine wallets being marketed as "works to reduce your fees" and uninformed users being delighted to see this happen. If you look at the BTC-denominated fees they are running at historical lows right now; almost no-one pays more than they absolutely have to.


This is a steady-state view, but not one that considers risks. If an attacker wished to cause chaos they could easily push a block that plays havoc with the fees unless the fees are weighted over a very long period of time (in which case up and down votes have very marginal advantage). The problem with the heavy weighting, however, is the statefulness requirement again (for SPV clients).
Trivial block reorgs are very common. We typically see a couple of orphan blocks per day, but over the last few years reorgs of up to 4 blocks have been observed


I definitely agree that something has to be done to sustain mining rewards of one form or another (I've done a lot of analysis on this over the last 6 months), but changes that affect the consensus code are pretty-much guaranteed to be rejected by the core dev team without an extremely detailed analysis. Even Gavin's proposed block size increase has caused a huge amount of debate, not least because it will prevent fees in their current form from ever becoming a basis for a commercial market.
In practice I suspect sidechains will end up being the way forward here and the main chain will see higher fees but larger transfers of value.
As regards SPV clients, yes they accept a lower level of security but transactions sent via an SPV client aren't second-class in that those SPV clients aren't additionally disadvantaged in terms of whether their transactions will be mined or not. Now we'd be seeing a new additional restriction on their capabilities.
The main problem here is that in order to accept any change at all there's going to be a huge analytical barrier. The design will need to be analysed for any theoretical weaknesses and consensus risks. That will be a huge undertaking. There will also be the problem that the design will be compared with "well why don't we just make the minimum transaction fee per kbyte 10x what it is now?" as that involves a much smaller set of changes and carries far fewer potential risks.
@_date: 2015-02-17 17:23:32
Input values can be huge for very small transactions because they're returning almost equally huge amounts of change. How would this work?
Large numbers of transactions already effectively pay miners by conventional fees too, so I'm unclear why this would be different than, say, imposing a larger minimum fee per unit amount of transaction data?
@_date: 2015-02-08 22:41:54
For the constant hash rate case then this is definitely something that can be derived analytically. The problems come where the hash rate is increasing or decreasing as we end up with a non-homogeneous Poisson process.
Things start to get tricky when we consider the impact of the difficulty changes because our starting conditions don't begin with a 10 minute block time, but instead something different.
The curves also reflect the individual hash rates over the 2016 block series.
I've not found any way to handle this analytically that doesn't end up with a large-scale numeric computation anyway, but if anyone can provide a pointer to an approach that will do this then I'd be more than happy to use that too :-)
@_date: 2015-02-09 09:05:31
Treechains seem to be missing a good description. I'd really like to see the design written up sufficiently that it could be compared with sidechains.
I've been (slowly) working on the reduction in block finding time idea for a while now, but it has some of the same problems in terms of potential consensus risks. It does have a lot of upsides though (reducing the requirement to pool mining, increasing the network capacity, reducing variance in terms of getting a nominal hour's worth of block confirmations, faster first confirmations, etc. I definitely need to quantify the impact on orphan rates though.
@_date: 2015-02-08 18:52:21
I applied the relevant spanner to wording in the article :-)
@_date: 2015-02-09 09:00:01
With smaller difficulty periods we'll see wild amounts of variance. Even at 2016 blocks the variance is actually really large (
@_date: 2015-02-09 14:05:36
Thanks for the link - I've just skimmed it but not yet had chance to read it properly.
The missing short rounds are something I've speculated about but not investigated, although an analysis of orphan block interactions is on my "to do" list.
Are the short rounds missing uniformly throughout the difficulty window or are they biased towards earlier in the window? I'm wondering if we see a short-term reduction in the hash rates immediately after a difficulty change as obsoleted equipment is taken offline.
A bigger reduction in short rounds would seem a likely result of network propagation effects. We see a non-trivial number of orphaned blocks (speed of light will cause some of this anyway) which indicate that at least for some fraction of the time we have the network working on non-useful problems immediately after a new block is announced.
It occurs to me that if the latter turns out to be a significant factor then we might actually use this to infer something useful about block propagation times?
@_date: 2015-02-18 21:28:18


It's not clear to me why transaction fees shouldn't pay for network security? If we say "transactions are free, but if you want to see security pay an extra amount" then that's essentially the same as saying "pay larger transaction fees". This is a problem of incentives for anyone making transactions; what's making the transaction pay larger fees of any description other than a protocol mandated minimum?
In the absence of any form of scarcity that generates a competitive market, or a protocol-mandated requirement for particular fees in order to achieve some particular result then it's not clear to me what will ever make anyone pay more than the minimum they can get away with?
For example, say blocks were magically to become 20 Mbytes in size then every outstanding transaction would fit in any "next" block so a miner would be insane to do anything than take every transaction possible, no matter how low the fees (of any sort). Any miner who ignores transactions in the current block is merely handing them to the next miner and the next block.
The problem of incentives is highly tricky. One problem with the idea of the next block determining fees for the block after is that this definitely changes the mining incentives too. In particular it may well incentivise forms of selfish mining because it has the potential to encourage any miner to try to maximise the reward for the next block and to then try to claim that block too.
@_date: 2015-02-19 09:15:34


In the case of Bitcoin mining there is actually a risk that a miner with a sufficiently large amount of hashing can slightly game the coin toss example. If a selfish miner withholds block announcements then they are able to work on a subsequent block at the expense of other miners. If the reward could be deferred for some significant period of time, however, then that particular effect can be ignored (no-one can mine a sufficiently long chain).
Back to the original question though: The problem of voting on block rewards is the same as that of transaction fees. The new reward has to be implemented in the form of an additional fee charge on future transactions (we're not generating new coins). This has the same problem as transaction fees: No user of the system will choose to vote up the reward and thus make their future transactions more expensive.
There's also a problem with having the fees decided by voting as opposed to having a known minimum fee at a fixed block height. When a user issues a transaction they will have to include sufficient funds to cover the transaction fee, but now also some future mining reward. If a transaction takes number of blocks to be included in a block then there's a real risk that it carries an insufficiently large quantity to meet the new reward level. This risk increases with block reorgs.
Finally the other problem here is that this would represent a massive shift in the consensus code. We'd now have a new consensus-critical characteristic and which would also require that SPV wallets would now have to maintain a large amount of state about previous reward calculations in order to determine how much to add to transactions to form new rewards.
@_date: 2015-02-08 18:03:22
Dynamic scaling certainly looks appealing, but I think this is an area that would need a lot of analysis. The biggest problem with any dynamic changes are that they risk becoming an attack vector if they allow a motivated attacker to change the network behaviour too quickly.
With that said, if this was something that changed over a long period of time then it may well work quite well.
There's another subtle problem though, if the size were to be set dynamically then the acceptable block size algorithm becomes another part of the consensus critical code. Each time more code is added and the complexity of this part of the software increases then there's an additional risk that a subtle bug could cause forking.
In this respect the sidechains efforts (and perhaps treechains if they happen) attempt to avoid some of the problem by allowing lower-value arenas to try out new concepts in a way that ought to carry less risk.