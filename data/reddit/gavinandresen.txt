@_author: gavinandresen
@_date: 2016-08-24 02:15:43
What country?
@_date: 2016-08-23 21:39:55
Why do you say the network cannot handle larger blocks?
Everybody who had studied the network agrees that it CAN handle larger blocks just fine. Is there a whitepaper  somewhere written by somebody who has actually studied the network I am not aware of?
@_date: 2016-08-24 02:58:33
Validate their own transactions?
Or validate ALL transactions?
I think it would be fine if only ten or twenty thousand people or companies in half the countries in the world were able to validate ALL transactions ALL the time. Bitcoin would have to be a couple orders of magnitude more popular to get there, though. Dedicate half your connection to Bitcoin and it should be able to handle 8meg block no sweat.
What threat model are you worried about where tens of thousands of people fully validating would be insufficient?
@_date: 2016-08-24 02:25:31
Restricting the block size to 1mb is doing two things:
1. Pushing companies and people to other chains. Bitcoin is at serious risk of becoming the MySpace of cryptocurrency because it is failing to scale.
2. Pushing people to centralized exchanges and wallets, because as fees rise it is cheaper to transact off-chain and it centralized off-chain solutions are much, much, much easier than decentralized solutions like Lightning.
Again, how do 'we' decide what is reasonable? Maybe somebody should appoint a Maximum Block Size Committee?
The market (aka the community) should be allowed to determine the maximum block size. Any other way of deciding (including letting developers who happened to get involved years ago decide) is against the Zen of Bitcoin.
@_date: 2016-08-24 00:17:16
How do you suggest we decide what is reasonable?
I HAVE done the research, your connection sucks compared to most of the rest of the world-- but why should the rest of the network be crippled because you have an irrational desire to validate every single Bitcoin transaction?
@_date: 2016-08-23 22:06:38
Do you still have a really slow DSL connection? You're not trying to run a fully-validating miner over it, are you?
I would dig up the research from Cornell and Bitfury on what the network could support LAST YEAR, but I'm typing on my phone.
@_date: 2016-08-23 23:32:00
2Mbps divided by 8 bits per byte time average 600 second block time equals 150 megabytes.
Divide by ... I dunno, ten? ... for extra safety margin and you can download 15megabyte blocks no problem.
If you are solo mining, just produce tiny blocks and they'll upload plenty fast.
@_date: 2016-04-13 23:16:27
Those two statement reconcile perfectly.
With a 95% threshold, you could rent a bit over 5% of hashpower and veto a change.  That's bad.
There's a good argument that even 75% is too high, because somebody could rent a bit over 25% hashpower to veto a change.
As for personal attacks: yeah, I'm human, I had a grumpy Monday, and I decided to rant. That's what Reddit is for, right?
@_date: 2016-04-02 23:26:43
Witness data is transmitted with transactions, in upgraded 'tx' or 'block' messages.  See BIP 144 
@_date: 2016-04-12 19:26:42
If the risks of those things were smaller and the likelihood of their success greater, then sure.
But they are more complicated (so more risky) and less likely to succeed, because they require lots more people to upgrade their software.
@_date: 2016-04-02 23:30:34
Uhh, this isn't correct:
"While Segwit is complex and introduces many changes, it is still about the same number of lines of code as the Bitcoin Classic implementation of the 2 Mb hard fork because that implementation still needs additional changes to mitigate the problems with quadratic hashing."
Segwit was a little more than 2,000 lines of last I checked.
BIP109 is significantly simpler; most of it's lines-of-code count is for the pseudo-versionbits implementation (and tests) for a smooth upgrade.
If you are not mining and you are not accepting bitcoin payments of more than a couple thousand dollars every ten minutes, then your BIP109 implementation can quite literally be just changing MAX_BLOCK_SIZE from 1,000,000 to 2,000,000.
@_date: 2016-04-12 19:40:47
I'll quote myself from THAT VERY SAME ARTICLE:
"If the number of transactions waiting gets large enough, the end result will be an over-saturated network, busy doing nothing productive. I don’t think that is likely– it is more likely people just stop using Bitcoin because transaction confirmation becomes increasingly unreliable."
Fewer people using Bitcoin is a Very Bad Thing, and I still believe it is urgent. Unfortunately, there probably will be no "oh my god, it is completely broken" moment where people wake up and realize that the irrational fear of the centralization boogey-man has stopped Bitcoin from growing and some less crippled alternative takes over as the "crypto-reserve-currency."
Maybe once the Core developers are done adding thousands of lines of code to support the new features they want they'll get serious about on-chain scaling, but, frankly, I've lost faith in their ability to make wise engineering decisions. They are piling on layers of technical debt with increasingly complicated solutions to problems that have simple solutions (see the discussions around what the Core wallet would have to do to support replace-by-fee for a good example).
And as for calling Peter Todd Chicken Little: there's probably a better analogy. What's a good name for somebody who makes mountains out of mole-hills, or who invents a Rube Goldberg contraption to brush their teeth because they're afraid their hand might fall asleep while brushing?
@_date: 2016-04-12 18:02:44
Peter Todd is wrong, as usual.  I usually don't bother explaining all the ways he is wrong, but I'll make an exception while I wait for a compile to finish:
"we end up with two different economically relevant currencies"    Debunked here:  
RE: renting hashing power to "force a fork" :  That is the way Bitcoin consensus works.  Majority hashpower wins, unless they try to force through a change that is rejected by majority economic power (exchanges, merchants, users).  The block size limit is a change that the economic majority has been SCREAMING FOR for a long time.
“It behooves us to make sure that any hard fork triggers in a scenario where it is very unlikely for any of this to happen . . . Triggering at 75 percent hashing power is not that scenario.”
75% plus a 28-day grace period is extremely safe.  Even Mr. Chicken Little Todd says in that same interview that there is only a 5% chance of anything going wrong (and I think he is overestimating that risk by an order of magnitude or two).
The risks of even more people deciding that Bitcoin is a failed experiment that cannot scale is much greater.
@_date: 2016-04-03 16:14:26
What kind of huge programs?
3-of-2000 multisig is given as an example, but that isn't a very practical example. 1,001-of-2000 would be interesting, but is not possible with MAST.
MAST is technically nifty, but should be part of a larger 'Script 2.0' redesign unless there is a really compelling use case.
@_date: 2015-12-19 15:10:54
He is making the common error that there is some magical barrier preventing transaction creators from mining themselves if a miner or group of miners collude to try to drive up fees.
@_date: 2015-12-01 14:10:09
Greg, can you please start to focus on SOLUTIONS?
The Bitfury analysis assumes no optimization of the p2p protocol AT ALL-- not even running with a lower -max connections to decrease bandwidth.
Your constant negativity is not helping make progress, which is sad because there is awesome progress being made all the time by you and Pieter and other contributors.
@_date: 2015-12-18 00:40:32
Would it help if I wrote a blog post describing how segregated witness, fraud proofs, and protocol upgrades would let full nodes, including miners, trade off validation or bandwidth costs for a minuscule reduction in trustlessness?
Or is your mind made up that it is impossible?
@_date: 2015-12-09 15:23:10
I was aware it was running on the Alpha sidechain.
The trick of interpreting a PUSHDATA *stuff*  scriptPubKey as a segwitness-enabled transaction is the part I wasn't aware of before, and that is what gives the smooth upgrade path for wallets. And that's really cool.
(I'm not aware if elements alpha does that, but I'd guess it doesn't, since it's not necessary because segwitness is built in to that chain from the start)
@_date: 2015-12-12 16:53:22
Where are you hearing signatures would be communicated 'in a more efficient channel'????
That is just incorrect.
@_date: 2015-12-31 03:40:08
If you want a ten minute head start, you can just not announce the block for ten minutes.
That is also known as selfish mining, and it only makes sense if you have a lot of hash power and are willing to mine at a loss for a few weeks until difficulty adjusts to the much higher orphan rate selfish mining creates.
@_date: 2015-12-09 15:26:53
It should be almost exactly the same amount of data (overhead could be as small as one extra hash-- the fraud proof data can be computed by any fully validating node, so no need to broadcast that).
I didn't realize at first the fraud proof data didn't need to be broadcast, so I think I made a comment here that bandwidth would increase.
I haven't had time to review the code yet; Pieter's presentation in Hong Kong was the first I'd heard details on the plan to get it into bitcoin main net.
@_date: 2015-12-31 03:37:02
Most of the time is hashing to create 'signature hashes', not ECDSA verification. So libsecp256k1 doesn't help.
@_date: 2015-06-22 20:24:43
I did reply to Pieter:
"But doesn't Matt's fast relay network (and the work I believe we're both planning on doing in the near future to further optimize block propagation) make both of our simulations irrelevant in the long-run?
Or, even simpler, why couldn't the little miners just run their
block-assembling-and-announcing code on the other high-bandwidth-side of the bandwidth bottleneck?"
... but I guess he got busy with other things because he didn't answer.
@_date: 2015-12-09 15:58:22
If you are running a block-creating-for-mining node, you must upgrade in either case.
If you are running a full node, it will start complaining at you that you must upgrade in either case (at least, the Core code will complain at you once it sees enough blocks with a version number it doesn't understand).  Yes, you can ignore the warnings if this is done as a soft fork, but if you decide to do that you significantly decrease your security against double-spending.
If you are using an SPV or electrum or web wallet (vast majority of users), this particular hard fork (increasing merkle depth and/or increasing max block size) is the same as a soft fork -- no effect.
So I disagree that there is any significant advantage in terms of ecosystem disruption for this particular hard fork change.
@_date: 2015-12-01 14:02:26
Stop with the false info, Peter.
Pieter's simulations showed that small miners WITH LIMITED BANDWIDTH WHO INSIST ON BUILDING BIG BLOCKS earned less than big miners (in the future when transaction fees become significant).
@_date: 2015-12-17 12:49:15
So why is litecoin mining, with much smaller blocks, even more centralized than Bitcoin?
@_date: 2015-12-08 12:57:51
You misunderstand, I think. Old transactions would continue to work as they always did.
@_date: 2015-12-30 01:12:31
Miners have already taken care of much of the problem themselves by mining empty blocks as soon as they see a valid 80-byte block header.
I think most developers "can't see the forest because they're down in the trees" -- there is, and will continue to be (if not limited by arbitrary protocol limits) innovation happening at every level in the ecosystem.
Some developers believe that all innovations to solve scaling or centralization problems must be invented by them or must be implemented before allowing growth. Show me another thriving system with that attitude and I'll reconsider my belief that the best path to success is to let smart people innovate.
@_date: 2015-12-10 20:43:11
I was about to say EXACTLY the same thing....
@_date: 2015-06-27 21:07:23
my bip101 code includes a fix.
@_date: 2015-06-05 21:07:42
So help test 
The "everything done must be perfect RIGHT NOW (but I'm not willing to lift a finger to help)" attitude is annoying.
@_date: 2015-12-09 15:51:15


... if the full node operator is unwilling to trust anything besides the genesis hash and proof of work.
If a full node operator is willing to trust it's peers a teeny-tiny bit, AND segwitness transactions become the norm, then it seems to me probabalistic checking of a UTXO set directly fetched from a peer and cross-checked using fraud proofs would avoid fetching the entire chain history.
Do you have a better plan, or do you think we should just stick with one megabyte transaction data forever?
@_date: 2015-12-02 01:32:37
"you can't start building a block until you receive..."
There is plenty of time to receive the blocks that miners are working on, while they are working on them. The current p2p protocol doesn't do that, but a future protocol could.
... and now you'll say "but attacker doesn't have to!"
... and I'll say "attacker must have high hash rate AND control new block propagation (e.g. if lots of hashpower SPV mines on top of the block header assuming the block is valid until they can download/validate then attack goes away) to gain advantage"
... and you'll bring up some other objection I've addressed before.
PS: It would be very helpful if you would take the time to write a blog post laying out exactly what you are arguing, and what you propose should be done to support more transactions on the network.
@_date: 2015-12-08 12:42:43
A transaction with witness data will be a dozen or so bytes per input bigger than an equivalent without, so bandwidth will be... uhhh... maybe 5 or 10% greater during new block relay.
I love segwitness and think it should be rolled out-- the benefits outweigh the costs.
But any solution that relies on both a consensus rule change and a change to wallets will take at least six months to a year to deploy, if everything goes well.
I think the most conservative approach is a hard fork that increases the limit we're hitting already AND roll out segwitness, ideally as part of the same hard fork (stuffing the witness Merkle data in the coinbase will just complicate the consensus-critical implementation for no good reason).
@_date: 2015-12-07 12:49:05
Yes, we should do both. Segregated witness won't magically make the very-short-term new block propagation concerns of miners go away. 
There also still needs to be a block size hard fork to fix the very broken signature-counting limits we have right now (see the recent "max sigop" attacks).
@_date: 2015-12-04 13:16:15
Safety is not Boolean, and your narrow black-or-white thinking and comments are not helpful.
@_date: 2015-06-09 02:10:09
I dismissed his analysis after "First, I only looked at blocks found after approximately 10 minutes, to avoid the time variance from influencing the result."
My wife is an actual scientist, and if she decided to throw out bunch of data because it influenced her results... umm... she would never, ever get a paper through peer review.
See  for a good analysis.
@_date: 2015-12-09 20:53:15
No, I unsubscribed from this subreddit in protest over censorship of reasonable discussion of the future of bitcoin.
@_date: 2015-06-23 03:05:24
I can't think of anybody with whom I trade some bitcoin value back and forth. In fact, I can't really think of anybody with whom I regularly trade dollars back and forth-- I tend to either receive them (from my employer or from acting as a merchant) or send them (when I want to hire people or buy things). That is just not a use case that happens for me. What use case are you thinking of, Greg?
@_date: 2015-12-30 13:36:46
I wrote this in an email yesterday about a similar proposal:
This is yet another "let's do a handstand and hop down the stairs because we're worried we might trip if we walk down" proposal.
@_date: 2015-06-04 12:29:35
Be patient, it takes time to write good code.
@_date: 2015-06-12 12:11:47
People keep on saying that, but it is simply not true.
It is O(m*n) for the entire network, where m is the number of nodes and n is the number of transactions.
It is O(n) for each of the m nodes in the network.
And that is as optimal as possible, BY DEFINITION, for "full nodes" (because "full node" means fully-validating, trusting absolutely nobody else to validate transactions for you).
All of the "bigger blocks mean more centralization" are worried about 'm' getting smaller. That's a fine thing to worry about, but, in my opinion, should not be the  priority right now.
And if it really was the  priority and a problem that needed fixing right away, lowering the block size would NOT be the right way to solve it.
All of the hair-on-fire OMG CENTRALIZATION!!! drives me nuts. Yes, bigger blocks may lower 'm'.  But even if bigger blocks made 'm' 50% smaller (very unlikely, in my opinion), that would have a tiny effect on security or centralization.
If there is a good argument for how it would have a MAJOR effect on either security or centralization I haven't seen it.
@_date: 2015-06-02 12:48:01
How much more expensive? Is $1 a day 'significant'?
We can't have productive conversations until questions like that get answered.
@_date: 2015-06-23 13:24:51
You need to go look at how gossip networks work; on average, you will download about the same amount of data that you upload.
See  for some theoretical work on the optimal lower bound.
@_date: 2015-06-17 11:46:16
the effect is very small unless your connection is REALLY crappy. And it is trivial to fix-- just put your block assembly and broadcast code on the other side of the crappy connection.
Or connect to Matt's relay network.
And the effect will go away completely when block announcements are optimized.
@_date: 2015-06-20 14:40:47
internet bandwidth has been growing at a rate of 50% per year; doubling every two years is about 40% growth per year, so the idea is to stay under the growth trend.
Rusty did some good research on US home internet bandwidth trends over the last decade or so that shows much lower growth-- I think we're about to see an explosion of bandwidth to the home, because the financial crisis is over and big companies are investing in infrastructure again. And people LOVE streaming video....
@_date: 2015-06-28 02:07:49
It restricts max transaction size to 100,000 bytes (consensus rule, not just IsStandard), which fixes the vulnerability.
I need to write a separate BIP for that change... it's on my TODO list...
@_date: 2015-06-16 02:22:55
didnt you get this email I sent you this past weekend?
... maybe there's a misunderstanding on what I'm actually working on. I'm coding a hard-fork that will only fork:
+ After March 1, 2016
+ If 75% of hashing power is producing up-version blocks
+ AND after some brave miner decides to actually produce a &gt;1MB block.
It will be very difficult for *anybody* to lose money. Even if we assume that some stubborn minority of miners decides not to upgrade (in spite of Bitcoin Core warning them first that the chain consists mostly of blocks with up-version version number, and then warning them that there is an alternative invalid higher-work chain), those miners will be the only people who will lose money. Ordinary, SVP-using users will follow the longest chain, and since at least 75% of hashing power will be on the bigger-block chain, there is no chance of them losing money. The big-block-rejecting-chain will very, very quickly be left behind and ignored.
There will NOT be two active chains, that is just FUD. Anybody running old code will have to willfully ignore the warnings to upgrade to stay on the old chain, and the incentives are so strong to follow the majority I can't imagine the 1MB chain persisting for any significant length of time.
@_date: 2015-06-04 03:24:04
(And you're also assuming new blocks will be announced the inefficient way they are today, which is a bad assumption. I will implement more efficient new block announcement protocol unless sipa or somebody else beats me to it).
@_date: 2015-06-16 19:40:11
I'm happy to tweak the parameters. The code I'm writing regression tests for right now is:
* 8MB max block size  (chinese miners were unhappy with 20 for not-entirely-clear reasons)
* Earliest fork date 11 Jan 2016 (miners and others want sooner rather than later)
* Activation when 750 of last 1,000 blocks are up-version (version 0x20000004 to be compatible with sipa's new versioning bits scheme)
* 2 week 'grace period' after 75% threshold reached before first &gt;1MB block allowed to be mined
* 8MB cap doubles every two years (so 16MB in 2018, etc:  unless there is a soft fork before then because 16MB is too much)
The code for all that is starting to get right on the edge of "too complicated for consensus," but the individual pieces are all straightforward. I'll write a BIP when I'm done with the code, and, as I've said repeatedly, I'm not stuck to any particular set of parameters.
@_date: 2015-12-19 15:07:25
I guarantee that a large enough decrease in the block size limit would make Bitcoin fail.
I believe the 1MB limit is severely limiting growth and investment right now, and that fearmongers like you who claim possible failure at a higher limit while never, ever quantifying safety or working on ways to mitigate risk are bad for Bitcoin.
@_date: 2015-06-09 02:18:05
I'm busy writing code....
@_date: 2015-06-05 10:55:53
That doesn't work, it would just increase their own orphan rate.
@_date: 2015-12-21 13:10:03
Have you talked to anybody who runs a poker site?
I implemented and deployed a just-for-fun multi person online poker system; your tech would have been nice, although we probably wouldn't have used it. Much simpler to have a central server, especially since players need a way of finding each other and since leader-boards were a big feature.
I believe the biggest problem in real-money poker is players collaborating with each other outside the game. Three players cooperating at a table can pretty easily take down a fourth. The big poker sites try to detect odd betting patterns that indicate unfair collaboration and will ban players that cheat; you won't disrupt them unless you have a solution to that problem.
@_date: 2015-06-15 01:25:08
You really like telling people what they ought to do, don't you?
@_date: 2015-06-28 21:11:40
My response:
If I don't see how switching from using the thousands of fully-validating bitcoin nodes to (tens? hundreds?) of Lightning Network hubs is better in terms of decentralization (or security, in terms of Sybil/DoS attacks), then I doubt other people do, either. You need to do a better job of explaining it.
But even if you could convince me that it WAS better from a security/decentralization point of view:
a) Lightning Network is nothing but a whitepaper right now. We are a long way from a practical implementation supported by even one wallet.
b) The Lightning Network paper itself says bigger blocks will be needed even if (especially if!) Lightning is wildly successful.
@_date: 2015-06-17 11:57:13
It is symmetric, so if 60% of mining is in China, and blocks propagate fast inside China, then THEY have a small advantage.
Unless one of them connects to the fast relay network, which is about 100 times more efficient.
Or most of them put block announcing nodes on both sides of the firewall, pre-send the blocks they are working on through the firewall, and just send the winning nonce and coinbase when they find a block.
@_date: 2015-06-04 14:05:00
Go for it. Code it up, just remember you have to use fixed-point math that is absolutely positively immune to overflow or rounding issues, because this is consensus-critical code.
@_date: 2015-06-04 17:54:10
The penalty is built-in: you give up fees if you create a 0-transaction block.
@_date: 2015-06-22 20:20:49
The wikipedia article on Moore's Law has a lot of good trends/analysis/references/etc.  :  
@_date: 2015-06-02 03:33:05
Yes, a multisig hosted wallet may be more secure than Bitcoin Core running on a malware-infested PC.
The wide range of wallets available (including hardware wallets) is a success story, even if it is sometimes painful when popular wallets fail in spectacular ways.
@_date: 2015-06-15 15:45:17
I've largely stayed out of the discussion of Jeff's proposal because I don't want to add to the noise. As I said in IRC, I'd be happy with something like Jeff's proposal if it can get consensus from Adam/Greg/Pieter/Wladimir.
I completely agree with Jeff on:  "Let the free market decide the long term shape of bitcoin’s transaction fee market, level of security and level of decentralization."
@_date: 2015-06-02 12:38:18
yes-- they will connect to the fast relay network or keep blocks small until other new block announcement optimizations are implemented.
@_date: 2015-06-04 02:45:54
How is that any different from bribing all the miners by sending a lot of fee-paying transactions?
And what is the attacker trying to accomplish?
@_date: 2015-06-04 12:26:45
if you have a crappy low-bandwidth connection then you will relay approximately zero blocks, because your neighbors will have already told your other neighbors about the block when your crappy connection is still busy downloading it.
Can you stop with the FUD? It chews up my time to respond to ridiculous scenarios like this.
@_date: 2015-06-23 03:19:04
I've averaged about two pull requests per month since January:
... but yes, finding a new salary source for me and Cory and Wladimir chewed up a good bit of time. Code review and testing is the current bottleneck for Core development, so number of pull requests isn't a good measure of productivity.
Another big chunk of time was spent testing 20MB and 200MB blocks, trying to get consensus privately to do something to raise the block size, and then, when those conversations went nowhere (Pieter and Greg got really busy with sidechains work), pushing the block size issue publicly.
I WISH I had more time to work on IBLTs...
@_date: 2015-06-01 14:37:18
Mining is a competitive business: it will ALWAYS be tough for miners, regardless of the maximum block size.
"If it was easy, everybody would do it" &lt;-- ... and then the difficulty increases and makes it hard for everybody again....
If you are worried that miners will have to invest a lot of money in infrastructure to support larger blocks, I think that worry is unjustified. If I was proposing gigabyte blocks then that would be a reasonable worry, but the costs of supporting 20megabyte maximum-sized-blocks are very modest.
@_date: 2015-12-09 15:43:26
Designing for success means setting limits high enough so that smart engineers and academics have room to innovate to solve problems as they come up.  Or "don't paint yourself into a corner"
I am not, and have never claimed to be, the smartest engineer or the most talented programmer. I am old enough, and lucky enough, to have worked with dozens of people smarter than me, and am continually impressed with the calibre of people working on Bitcoin and the amazing solutions they come up with. Greg's road map should make anybody who can understand the implications of things like "fraud proofs and partial validation" excited that Bitcoin has a very bright long-term future.
Lately, when I'm not distracted by reporters asking if Dorian Craig Szabo is satoshi or not, or catching up on the latest arguments in the Great Block Size Limit Debate, I've been reading a lot of academic work on gossip protocols and learning about network simulation tools and thinking about how to go about actually putting some of the academic work into practice (after seeing if somebody has actually already done that).
All that may turn out to be wasted effort-- a brilliant engineer or academic might be two days away from announcing a fantastically scalable, efficient, private, robust, decentralized, DoS-proof solution that we can just pick up and use.
That's OK, I'm learning a lot of stuff I didn't know before, and that makes me almost as happy as applying what I've learned to solve some tricky problem.
@_date: 2015-06-24 14:29:50
BIP101 is the 8MB to 8192MB proposal.
@_date: 2015-06-29 11:10:01
I assume a new, optimized network protocol.
@_date: 2015-06-30 10:56:58
Bitcoind supports batch rpc, so you could also bundle up all the getrawtransation calls into one round-trip (but parsing the raw to data yourself will be faster).
@_date: 2015-06-04 11:52:44
a) that is economically identical to just paying fees
b) there are faster, cheaper ways to discourage people from running full nodes. Like hire a botnet to DDoS full nodes. Or just rent a bunch of servers if you want to pull off a Sybil attack. Sending a bunch of transactions hoping to up the block size hoping THAT will drive nodes off the network is not an attack I would worry about.
@_date: 2015-06-04 12:07:22
All miners will consistently produce blocks at the maximum ? Why?
And if you assume all miners get together to do that.... All miners could get together to do much more damaging things, like pull off double-spends.
@_date: 2015-06-06 10:28:56
These are good questions for the Electrum, Libbitcoin and Insight developers.
"This software I'm using can't scale up to handle the load" is not a good reason to limit Bitcoin's growth.
@_date: 2015-06-23 02:12:38
Sounds like a challenge. If somebody can get sanity-checking of 8MB blocks running in less than 40ms, would you support raising the cap?
(is the merkle-building code parallelized? Building binary trees should be a nicely parallelizable process and multicore is a standard feature of modern CPUs...)
@_date: 2015-06-10 02:59:45
No. If it is very unlikely and there are no incentives to do it and the consequences if it did happen are mild then it makes no sense to plan for it.
Much better to spend time on things that are much more likely to happen and/or would have much more serious consequences if they do happen.
@_date: 2015-06-23 15:18:21
I'm inspired by Satoshi's "I have to implement it first to make sure I thought of everything."
@_date: 2015-06-23 01:29:06
The sky will not fall if the block size stays at 1MB-- there will just be a lot of disgruntled users pissed off that their transactions don't confirm as fast as they expect and they have to pay more than they expect.
If the block size must be raised eventually (and basically everyone agrees it must be raised at some point) then I think it is best to do it now, when there is time for thorough testing and a careful, planned rollout. And before there are a lot of unhappy users telling their friends how their money got stuck in the Bitcoin network.
@_date: 2015-06-23 02:17:11
His test is a good example where 8 connections are WORSE than 1, though (his orphan rate would be much better with just one connection).
Maybe it is time for a version of the Core code that is optimized for at-home users on low-bandwidth DSL connections, or maybe just shipping a bitcoin.conf file that has settings tweaked for low-bandwidth-connection environments would be good enough.
@_date: 2015-06-02 12:51:21
That effect is completely insignificant until the block reward gets a lot smaller.
And we will have very fast new block announcement code in place well before then.
@_date: 2015-06-05 23:38:16
We're at 400k average blocks, so you're going to be over 10minutes per block before we saturate at 1MB blocks.
So no matter what happens you'll soon need a software and/or hardware upgrade to keep up.
@_date: 2015-12-23 13:40:25
You haven't been paying attention.
Lots of miners have optimized their orphan rates by mining an empty block on just the 80-byte block header, so block size is irrelevant.
That is not ideal, but it does little harm and it is a smart thing to do until a better solution like 'weak blocks' is implemented.
@_date: 2015-06-01 16:36:03
That is why wallets don't let you send dust of spammy feeless transactions.
The question is, are wallets ready for ordinary reasonable-fee-paying transactions taking a long time to confirm (or never confirming)?
I was worried about this for Bitcoin Core, so I implemented the first version of the fee estimation code (it has since been improved by Mike Hearn and Alex Morcos).
But most people aren't running the Bitcoin Core wallet, because SPV wallets or web-hosted wallets are more convenient and/or more secure.
@_date: 2015-12-09 19:25:36
So you don't have a better plan, and think we should stick with one megabyte blocks forever?
@_date: 2015-06-12 12:20:25
Adam's 'extension block' proposal is a non-starter because it would require EVERY SINGLE wallet implementation to go through a very complicated upgrade.
ANY proposal that requires all of the various wallets to do anything that requires significant coding or, worse, significant changes to the way coins must be handled that will almost certainly be exposed to users-- like transferring them between the bitcoin blockchain and some 'extension block') isn't going to fly.
As for his other suggestion on a 100% consensus hard-fork: that would be lovely. But all of my attempts at compromise have run into "this isn't a problem that needs solving now, it will be controversial, lets just wait."
I think "lets just wait until it is a problem" is irresponsible. Consensus-critical code needs time for testing, review, rollout, etc.
So I'm starting the process; we'll have 9 months for the testing/review/rollout process, and maybe we'll even reach 95% consensus before the earliest possible date I'm coding into the fork (March 1, 2016).
@_date: 2015-06-23 13:33:44
Yes, that's exactly right.
@_date: 2015-06-04 12:14:45
6 blocks are found in 10 minutes pretty regularly. When that happens, mempools are emptied out and small blocks are created. 144 is large enough to smooth out the randomness of block-finding.
@_date: 2015-06-04 12:04:57
Has monero ever had enough transaction volume to see if it works in practice? Block explorer I looked at showed nothing but 1-2 transaction blocks.
@_date: 2015-12-16 12:41:49
It is feasible today, and gets easier as time goes on and we create better test cases. Try running btcd for a while, there are already good, interoperable implementations of the protocol.
@_date: 2015-06-29 00:18:25
Great results-- and those are single core numbers. If necessary, the structure of an IBLT should make it pretty easy to parallelize construction to take advantage of multicore CPUs.
@_date: 2015-06-23 15:15:55
If you read the article as advice to miners ("don't be dumb and produce big blocks unless transaction fees cover the increased risk of losing a block race") then it makes sense.
@_date: 2015-06-02 23:31:58
Meni: feel free to republish the comments I sent you via email...
@_date: 2015-06-04 03:19:15
By 'contributing node' you mean solo miner/mining pool?
Yes, if you are solo mining on a crappy DSL connection you should create small blocks....
Happily, you can-- nobody will force you to make big blocks.
@_date: 2015-06-02 12:35:58
that was a soft fork that only required cooperation from 50+% of hashpower
@_date: 2015-06-08 03:26:47
GiveDirectly is my personal favorite.
@_date: 2015-06-04 12:08:25
Yes, probably 1mb
@_date: 2015-06-17 18:29:06
If the advantages of mining in China (cheap electricity, labor, facilities costs) outweigh the disadvantages (slower block propagation that cannot be solved by writing code or moving around where they do things) then mining in China will happen.
If not, then mining will not happen in China.
Or, to put it another way: connectivity is one of the (many) variables that go into the profitability-of-mining equation. It is great that equation has lots of variables, because that means there are likely to be multiple places in the world where mining is equally profitable (maybe electricity is a little more expensive but internet connectivity is better in one place, and vice-versa in another place).
I have described two different ways for miners with a 5Mbps connection and 0.1% packet loss to overcome that disadvantage (connect to Matt's relay network and have 1/100th the bandwith requirement or run full nodes on both sides of the bad connection and pre-send the blocks being worked on or otherwise arrange for them to know what blocks will be worked on).
You seem lack imagination when it comes to the clever ways people find to work around real-world limitations like "I have bad Internet connectivity to the rest of the world."
@_date: 2015-06-03 15:08:26
I didn't have time yesterday, but here's the email conversation:
Interesting.  How do we decide what "T" should be ?
My knee-jerk reaction: I bet a much simpler rule would work, like:
    max block size = 2 * average size of last 144 blocks.
That would keep the network at about 50% utilization, which is enough to keep transaction fees falling from to zero just due to people having a time preference for having transactions confirmed in the next 1/2/3 blocks (see  ).
I think this simple equation is very misleading:
    Bigger blocks -&gt; Harder to run a node -&gt; Less nodes -&gt; More centralization
People are mostly choosing to run SPV nodes or web-based wallets because:
    Fully validating -&gt; Less convenience -&gt; Less nodes -&gt; More centralization
Node count on the network started dropping as soon as good SPV wallets were available, I doubt the block size will have any significant effect.
Also: Greg's proposal:
  
**Meni's reply:**
Hi Gavin,
 (1a). I don't believe in having a block limit calculated automatically based on past blocks. Because it really doesn't put a limit at all. Suppose I wanted to spam the network. Now there is a limit of 1MB/block so I create 1MB/block of junk. If I keep this up the rule will update the size to 2MB/block, and then I spam with 2MB/block. Then 4MB, ad infinitum. The effects of increasing demand for legitimate transaction is similar. There's no real limit and no real market for fees.
 b. I'll clarify again my goal here is not to solve the problem of what the optimal block limit is - that's a separate problem. I want to prevent a scenario where a wrong block limit creates catastrophic failure. With a soft cap, any parameter choice creates a range of legitimate block sizes.
You could set now T = 3MB, and if in the future we see that tx fees are too high and there are enough blocks, increase it.
 (2). I have described one causal path. Of course SPV is a stronger causal path but it's also completely irrelevant, because SPV clients are already here and we don't want them to go away. They are a given. Block size, however, is something we can influence; and the primary drawback of bigger blocks is, as I described, the smaller number of nodes.
You can argue that the effect is insignificant - but it is still the case that
Many people currently do believe the effect is significant, and
This argument will be easier to discuss once we don't have to worry about crash landing.
 (3). Thanks, I'll try to examine Greg's proposal in more detail.
**My reply**
Who are "you" ?
Are you a miner or an end-user?
If you are a miner, then you can produce maximum-sized blocks and influence the average size based on your share of hash rate. But miners who want to keep blocks small have equal influence.
If you are an end-user, how do you afford transaction fees to spam the network?
If you are arguing that transaction fees may not give miners enough reward to secure the network in the future, I wrote about that here:
   
and here:
   
And re: "there is no real limit and no real market for fees" :  see
  
There IS a market for fees, even now, because there is demand for "I want my transaction to confirm in the next block or three."
@_date: 2015-06-25 12:10:43
you REALLY haven't been paying attention if you think nothing has changed since then.
@_date: 2015-12-09 20:56:24
Have you looked at how long it took to finish bikeshedding and then roll out the checklocktimeverify soft fork?
Segregated witness is likely to take at least as long.  And then AFTER the network supports it, wallets need to start producing segregated witness transaction.
My blog post talks about all of that, and that is why SW is NOT a "short term solution" for blocks filling up unless you think "short term" is a year or more from now.
@_date: 2015-06-04 17:41:40
if you try to spam zero (or very-low) fee transactions most of them will be ignored and not get into anybody's memory pool.
@_date: 2015-06-04 14:06:10
Yes, the mempool backing up regularly will drive a healthy fee market.
That will happen naturally just because of the natural variations in how quickly or slowly blocks are found.
@_date: 2015-06-19 11:36:52
"Use freicoin"???
@_date: 2015-06-01 14:38:33
Fewer people are running full nodes with 1MB blocks now. Did you read my blog post where I say:
"Give the typical user three experiences: a SPV wallet, a full node processing one megabyte blocks, and a full node processing twenty megabyte blocks, and they will choose SPV every time."
@_date: 2015-06-08 16:09:30
I wrote about it in the second half of this blog post:
@_date: 2014-10-21 15:29:42
I'm most excited about all of the non-currency uses of the blockchain's ledger-ordering ability. I have no idea which ones will turn out to be successful, but I'm glad all of that experimenting is happening.
I'm most worried about scalability.
@_date: 2015-06-04 02:43:45
Median means if you are in lower 49% you have zero influence. Average gives everybody influence.
Imagine 73 blocks of 1.1MB and 71 empty blocks (maybe some miners are desperately trying to lower max size). Median would be 1.1, average is a much better measure of collective desire.
@_date: 2015-06-04 17:51:18
Yes, pretty fast. Compound growth is amazing.
@_date: 2015-12-21 22:35:57
Have you talked with any heavy online poker players about whether or not choosing a particular table/seat is important to them?
Honest question, I have no idea what they would say (I don't play online poker).
@_date: 2014-10-21 23:53:51
I didn't (he spells it weird, too many vowels and ss's).
@_date: 2014-10-21 17:05:44
I tend to agree with Daniel Krawisz: 
We might be wrong; I scratch my head at all sorts of stuff that people value (what's the deal with Hummel figurines????)
@_date: 2014-04-08 17:07:31
This commit is part of 0.9.1:
commit 6848d180b2a3b259e6e7d3b017db06da6b6c5980
Author: Wladimir J. van der Laan &lt;laanwj
Date:   Thu Mar 20 12:03:36 2014 +0100
    gitian: add statically built variant of bitcoind/bitcoin-cli
... which should fix the glibc conflict. You can help by testing as soon as we release binaries.
@_date: 2014-08-20 14:03:15
You don't have to store them once they are history.
I will be posting a 'scaling up' road-map soon.
@_date: 2014-06-12 13:35:14
Those are very smart, well-informed question.
Theoretically, if Bitcoin becomes a truly global currency it should be more stable than any regional or national currency, because unavoidable negative events in one part of the world (war! famine!) should be balanced out by unavoidable positive events in other parts of the world (peace! freedom!).
Only global unavoidable events should affect stability in that case-- and as economists say, "there ain't no such thing as a free lunch." Clever monetary policy won't save us from a worldwide depression if we're hit by a massive asteroid that causes crop failure all over the world.
As for poorer countries being left behind: maybe you could ask your dad how a country's monetary policy makes poor countries wealthier. Where does that extra wealth come from?
The only answer that makes any sense economically is that somehow monetary policy can make people more productive, but that seems unlikely to be significant to me. Innovation and ease of starting businesses increase productivity a lot; I can't imagine that making exports cheaper (and imports more expensive) by devaluing your national currency has a huge impact on productivity in the long run.
And we certainly have a lot of examples of countries becoming poor (or becoming more poor) by central banks screwing up and hyper-inflating.
@_date: 2014-12-06 15:53:14
I always thought it was spelled 'eff' ... Like 'effing kids these days, pissing on my front lawn'
@_date: 2014-10-21 18:32:54
In hindsight, it was too early to standardize on a 3D graphics format. All the big 3D content creation systems had their own way of doing things, and too much would get lost in translation if VRML was used as a universal interchange format.
I haven't been paying attention to the 3D graphics world, I don't know if that is still true or not.
@_date: 2014-08-03 00:02:48
Must be another Gavin, I have never had 25,000 BTC.
@_date: 2015-06-28 15:33:14
@_date: 2014-10-08 15:56:46
Bandwidth has been growing at 50% a year. Yes, even home internet bandwidth (streaming videos are very popular and drive demand for more bandwidth).
CPU at 60% a year.
Memory and disk even faster than that.
When those trends eventually slow down or stop then the maximum block size rules can be adjusted (it would be a "soft-forking" change if the limits are lowered). But I don't think anybody knows when that will happen. I do know that people have been incorrectly predicting the end of Moore's Law for decades....
@_date: 2014-08-11 19:02:23
That falls out from sorting by largest-fee-paid-first and transmitting the total size (in bytes) of transactions the miner decided to include (higher-fee-paying transactions push out the 1Sochi spam).
@_date: 2014-07-16 13:10:44
Yes, Chris and Trucoin have been around for a long time, they are not fly-by-night scammers.
I haven't been involved at all with Trucoin since they were forced to regroup by typical startup funding/regulation/etc drama a couple years ago... I'm happy to see them persevere and hope they succeed!
@_date: 2014-10-21 15:39:51
I think Bitcoin Core will go to version 1.0 when it feels like it is stable and won't need to change (besides minor bug fixes) for two or three years.
@_date: 2014-10-22 00:01:28
Polar Bears. Those things are frickin huge.
@_date: 2014-08-11 19:25:22
Nobody will sync from the genesis block; you'll sync 80-byte block headers, get the UTXO set as of some recent block from one or more trusted sources, check the UTXO hash against some other trusted source or sources (maybe find it in a block in the blockchain), and start from there.
Exactly how to do that all that has been argued about for a long time; when 'bootstrap.dat' gets too big to download in a reasonable amount of time maybe it will even get implemented.....
@_date: 2014-12-06 15:49:42
@_date: 2014-03-18 13:48:34
I have no account at BTC-E and have never interacted with them.
But I agree with monolithik: I think it is likely they will eventually either get shut down or disappear. I hope I'm wrong.
@_date: 2014-03-29 23:20:34
Huh? 'Accepted' is finalized.
@_date: 2014-10-14 22:16:55
Transaction fees.
Or maybe big merchants or exchanges that have an interest in keeping the ledger up to date will mine themselves.
Or maybe they'll band together to fund assurance contracts (see  ) to support mining.
@_date: 2014-08-11 20:11:42
No, they make no sense. I suspect many of them will turn out to be Ponzi schemes.
@_date: 2014-01-19 14:24:39
Bought my ticket to the financial cryptography 2014 conference with CheapAir and Bitcoin. Price was better than HipMonk, too.
@_date: 2014-10-21 16:24:14
That's a great question for somebody who has studied how other Internet protocols evolve (or not) over time.
My guess is that protocol change will speed up again when some of the startups grow up a bit and have the time and resources to participate in a more formal standards-making, protocol-evolving process.
And then a few years later it will slow down again.
@_date: 2014-10-21 15:20:20
I think in a year or so after the current influx of investment starts producing results it will be safe to stop saying "Bitcoin is an experiment."
Block sizes are already dynamic-- miners decide how large to make the blocks. If you mean maximum block size: I haven't heard a dynamic proposal that fixes the problem that people are worried about.
No, there is no canary clause for Bitcoin.
@_date: 2014-10-21 16:29:46
"Delayed a few weeks" wouldn't happen.  "Delayed 8 hours" might if the attack happened at exactly the right time of day.
@_date: 2014-07-02 20:01:44
Great decision-- more diversity in how "common good" things get funded is good.
@_date: 2014-10-21 17:06:51
Good question!  Maybe... but after another halving or three I think we'll get a pretty good sense of what will happen.
@_date: 2014-10-21 17:08:45
I think a close look at local politics shows how hypocritical people are; what Robin Hanson calls "near versus far thinking."
@_date: 2014-10-21 18:06:39
Bitcoin is a new way of doing money: think of it as "cash for the Internet."
@_date: 2014-10-21 15:30:02
No, not right now.
@_date: 2014-08-11 18:59:34
Thanks for starting the discussion, sandball!
RE: why canonical ordering: as others have commented, IBLTs just give you the set of transactions. The order of transactions in the block data affects the merkle root and block hash, so a canonical ordering is needed.
RE: 8-byte values having unreasonable overhead: mmm, maybe. I suspect we might be able to optimize away the IBLT checksum (because keys are 48 bits of hash of the transaction data) but I'd have to spend more time playing with IBLTs and thinking about that kind of micro-optimization.
RE: but what if a miner has a mempool very different from everybody else, or wants to include transactions nobody else would?  Then they'll have to transmit a bigger IBLT than everybody else (or, past some threshold, just resort to sending the whole block).
I think that is a feature, not a bug; the incentive is to go with the majority of hashing power, but it is not all-or-nothing: miners that want to set their own policy can compare their blocks against standard-selection-policy blocks and be sure to create an IBLT large enough to encode the differences.
@_date: 2014-02-04 16:31:32
"terrible"?  Compared to what?
@_date: 2014-10-21 16:16:52
1) A good place to start for Bitcoin Core: fork it on github and make sure you can compile/debug/etc. Learn how the RPC interface works. Then dive in and pick a RPC call and write some regression tests for it in Python (see  ).
2) Contribute to highly visible open source projects! I've heard that startups go down the list of contributors on github projects to look for people to recruit.....
@_date: 2014-10-21 13:48:57
Sure. There are lots of technologies that don't have the potential to make me wealthy that I use and evangelize (the web, email, my electric car).
Why? Because they make my life better, and it is nice to share things that make your life better.
@_date: 2014-07-14 22:24:14
Yes, it is, see Andrew Poelstra's explanation of how: 
@_date: 2014-10-21 16:20:39
Define "micropayments" ?
@_date: 2014-10-21 16:20:26
Yes, "good enough for now."
Suggestions on making it better are welcome, but I haven't seen any suggestions get traction and consensus.
@_date: 2014-10-10 15:57:20
So.... what do you mean by "core dev" ?  Anybody who has ever contributed a patch to any bitcoin-related project?  Anybody who has ever contributed a patch to the reference implementation?
When I say 'core dev' (and I'm trying not to say it, but still slip sometimes) I mean one of the five people who are able to pull code into the reference implementation github repository.
RE: removing developers: pull requests are judged on the quality of the code. We don't care if the person submitting the code is a saint or a sinner (although playing nice makes it more likely you'll get other people to help test and review your pull requests, which is a big part of getting code accepted).
@_date: 2014-09-11 20:24:01
Wanna bet?
@_date: 2014-08-20 02:01:18
That someone was Satoshi, and I am certain he knows how ip addresses work, you ignorant, lazy armchair troll.
@_date: 2014-05-10 12:48:05
Are you just trolling, or are you really so ignorant and immature you don't know that in the USA anybody can be sued for anything?
My apologies if you are neither and live in a country with a more reasonable tort system.
@_date: 2014-10-21 16:19:27
Yes, I think we do.
There is still at least a month or two of work before I'd be willing to write a patch to increase the maximum block size, and then probably a month or two more of arguing. So early next year at the earliest before even starting the hard-fork process (which must roll out to miners-- they will control when the fork actually happens).
@_date: 2014-10-21 17:07:11
That would be a great problem to have!
@_date: 2014-10-21 18:11:44
Koala is our tabby cat. Best cat we've ever had, except for when she insists on bringing half a dead chipmunk into the house.
Bunyip is our 7-month-old golden lab puppy. Very cute and friendly and big and almost completely house-broken.
They have not been featured on reddit before.
@_date: 2014-10-29 20:20:24
He didn't read the followup: 
@_date: 2014-10-21 16:21:28
You should've asked that last week when Jim Harper did his AMA... I dunno.
@_date: 2014-05-10 13:18:17
See the Foundation bylaws for requirements on Board members.
Being named in a dismissed lawsuit, or being trolled by clueless people on reddit who have never run anything bigger than a hair dryer does not disqualify you.
@_date: 2014-10-21 18:19:50
I think at the micro level it is fragile. A bug or vulnerability in the reference implementation could bring down the network for a little while.
At the macro level I think it is very robust; bugs get fixed.
And it is getting more robust all the time as there are more people able to fix the bugs, more implementations, etc....
@_date: 2014-06-19 17:32:51
Peter Todd is a very large part of the "Bitcoin Core moves forward too slowly" problem.
He needs to realize that changes to Core are not life-and-death; we can, and do, correct small mistakes.
And he needs to realize that just because he has over-riding priorities (DECENTRALIZATION AT ANY AND ALL COSTS!!!!!!!), the wider Bitcoin community might not set priorities exactly the way he wants. Consensus requires compromise, and as far as I've seen he is never willing to compromise.
And Kyle: if you want a feature in Core, then begging and pleading for it to get done is not the way to make sure it gets done. Do it yourself, or hire somebody to do it (just make sure they've worked on open source projects before so they know that it takes a lot of work AFTER writing the code is done to get consensus that a change is safe enough, and generally useful enough, to be included).
@_date: 2014-05-02 11:27:09
No, I meant I am a skeptic in general.
Incentives in Bitcoin are correct.
@_date: 2014-03-25 13:23:56
Really?  100% faith?
I don't even have 100% faith in myself (it is why I'm always banging on the code review and testing drums), I can't imagine having 100% faith in some other developer.
@_date: 2014-10-21 23:47:50
It feels like one hand clapping. Why do you ask?
@_date: 2014-10-21 15:10:20
See 
@_date: 2014-10-16 01:36:49
Ooh! Ooh! I know!  VAMPIRES!
Almost halloween, I'll have to dig up my pointy teeth and black cape.....
@_date: 2014-10-21 16:26:28
@_date: 2014-08-28 21:05:52
We've lost a brilliant mind and a wonderful person.
I hope the cryo-revival thing happens. Fifty years from now I really want to see the headline "Man revived from cryo-sleep has brainwallet worth $11 billion."
@_date: 2014-09-29 16:05:28
Mine too!
@_date: 2014-11-21 12:24:25
Status quo was not financially sustainable-- the pivot is to try to fix that and survive (same reason any startup pivots, really).
@_date: 2014-12-21 17:40:04
Yes, the brain chip giving the Foundation control of me was a condition of becoming Chief Scientist....
Totally worth it, I don't have to worry about what to have for lunch any more, that's all pre-programmed.
@_date: 2014-10-17 14:00:55


Because I want to get rough consensus on the fundamentals before discussing specifics, and because anything I post on the blog people misinterpret as "Gavin is saying this is the way it MUST be done."
e.g. see Mr. Todd's immediate attack because I mention that assurance contracts "may" be a way of funding blockchain security in the future.
@_date: 2014-08-21 18:14:19
No, new block announcements become O(1) (constant-sized) given some what-I-hope-are-reasonable assumptions about miner transaction selection policies being consistent (e.g. assuming that miners are rational and would prefer to put higher-fee-paying transactions in their blocks than lower-fee-paying).
The network as a whole is still O(n) because you're still transmitting every transaction across the network, but we can scale a long way before that becomes a problem.
@_date: 2014-10-21 16:25:29
Start at the edge, and work your way in.
Play with the RPC interface, write some code that plays with stuff in -regtest or -testnet mode.  (and see my answer here about writing regression tests to start contributing)
@_date: 2014-10-21 16:17:06
Dunno, I'm not working on side-chains.
@_date: 2014-10-21 16:28:08
To be honest, I can't remember if I ordered one or not!
I don't own one. I tend to wait for the 2.0 version of gadgets before getting them.
@_date: 2014-11-20 13:41:46
TWO currencies.... Huh. Makes me think:
"The curious task of economics is to demonstrate to men how little they really know about what they imagine they can design."
F.A. Hayek, The Fatal Conceit
@_date: 2014-10-21 15:47:27
I don't think cryptography is the weak link right now, so I don't think a cryptographic expert is the right place to spend money.
Bitcoin's failures haven't been cryptographic, they've been either old-fashioned fraud or plain old computer security failures.
@_date: 2014-10-21 15:17:36
How do you encourage businesses to do anything? Do business with businesses that reflect your values, and avoid ones that don't....
@_date: 2014-10-21 17:00:25
My wife REALLY didn't want to be "Michele Bell"
@_date: 2014-03-15 00:08:41
The bitcoin core code will keep track of the last few thousand transactions broadcast and then included in blocks, and based on how long they took to confirm will estimate a reasonable fee.
@_date: 2014-10-16 14:15:44
How much does it cost the network to store an unspent transaction output forever?
(there's a bitcointalk forum thread that does a back-of-the-envelope estimate, but I'm too lazy to go search for it right now).
Or better: what is the cost of storing a UTXO versus the cost of broadcasting/validating/storing a spend?
Some costs are so small they are not worth charging for.
@_date: 2014-10-21 16:30:36
RE: autograph: I never understood autographs. Seems like a really bad idea to give anybody who asks a sample of your signature....
@_date: 2014-12-21 17:32:46
Not an imposter, that's me.
@_date: 2014-10-21 17:09:59
I think the Great Biscuit Debate is too hot for me to handle; I've got enough controversy on my plate....
@_date: 2014-10-20 15:15:13
Did you ever consider maybe it is you, not them?
Personally, I find your attitude and approach off-putting. I find you very "you're doing it wrong and you're an idiot and you should do it my way" versus "have you considered doing it this way, maybe it would be better...."
Maybe that's just me. But maybe you should consider doing things a different way, maybe it would be better.....
@_date: 2014-10-16 21:04:44
I think I followed you right up to "If blockspace were unlimited..."
One of the main points of the blog post is that blockchain security and maximum block size may have very little to do with each other. Increasing/decreasing/keeping-the-same maximum block size won't solve the Tragedy of the Commons how do we secure the blockchain problem.
The first statement does is *not* saying there is a relationship between fees and people's desire for a functioning network. It just says that fees will not drop to zero, assuming economically rational miners and non-zero desire for people to make Bitcoin transactions.
That says nothing about whether or not those fees would be enough to secure the network.
@_date: 2014-10-21 16:46:43
Side-chains seem like they are much better thought out.
I'll confess I still don't understand how treechains would work.
@_date: 2014-10-17 15:58:10
See: 
... where I describe the short-term solution (which is already implemented).
@_date: 2014-06-30 17:15:25
Mining is a zero-sum game.
Unless you have some innate advantage you should not play zer-sum games. In the case of mining, my advice would be not to play unless you have early access to more efficient hardware or cheap/free electricity.
The only other reason I could see to mine would be if you want to speculate on the value of Bitcoin going up, and are living somewhere where you can pay for mining hardware (and electricity) but cannot work for or buy bitcoins directly.
The same reasoning applies if you are mining for alt-coins; yes, it is possible you get lucky and buy the right hardware at the right time and mine the right coin. But that's not likely, unless you have some special knowledge or hardware or talent.
@_date: 2014-10-16 15:19:27
Why do you assume that? I directly address that in the blog post:


@_date: 2014-03-23 12:21:04
Yes, start a better organization and out-compete the Foundation.
@_date: 2014-08-15 14:28:57
Mmmm.... pie.....
@_date: 2014-09-21 23:22:28
Bitcoin would be safe today even if it used the broken md5 hash everywhere it uses sha256.
Why? Because Bitcoin never uses single SHA256.
We'd be busy moving to a not broken hash function, of course, but there would be plenty of time to switch.
@_date: 2014-10-30 00:01:21
We have REPEATEDLY seen people running businesses that involve holding other people's bitcoins fail miserably.
That is the type of business Jim is referring to, and where well-crafted regulation would help. At the very least, they might encourage people to create services where the coins are never controlled by the service, thereby avoiding those well-crafted regulation.
@_date: 2014-06-04 12:37:17
Security is NOT all-or-nothing. Fingerprint plus PIN is much better than  PIN (or pass phrase) alone.
@_date: 2014-04-05 17:13:56
Google scholar search for "bitcoin"
@_date: 2014-04-10 13:55:30
Why would it be hard to get this rolled into core?
@_date: 2014-06-15 01:02:51
Our policy is to not release binaries until they have been reproduced by at least three people.
We're at two right now (Wladimir and myself), Official Announcements won't happen until we have at least one more gitian-builder.
@_date: 2014-02-19 03:21:28
... and merchants and exchanges. Mined coins that cannot be spent or exchanged are worthless, even if there are 51% of them.
@_date: 2014-12-23 12:58:38
I'd like to see a better protocol for distributed name registration-- maybe a sidechain will get it right....
@_date: 2014-10-08 15:49:50
Quoting myself: "There are other ideas for how to make Bitcoin scale, and whenever practical I like to choose “all of the above” for how to solve a problem, because nobody is smart enough to choose The One True Solution every time. So I won’t be surprised or disappointed if development wanders off this roadmap in a different direction."
So, yes, go for it-- implement off-chain solutions, too.
@_date: 2014-12-05 12:57:17
High value, coins-haven't-moved in a while transactions would still go through because most miners still set aside space for 'high priority' transactions.
If an attack did happen, it would be trivial for some of the big pools to switch temporarily to blocks sorted just by priority, which would make the attack almost completely ineffective.
@_date: 2014-03-07 03:06:03
Hey y'all:
Two wrongs do not make a right- leave Leah alone.
I think what Newsweek did is wrong and they should have worked to protect Dorian's privacy, but I also think reasonable people will disagree about where to draw the line between "the public has a right to know" and "we all have a right to privacy."
@_date: 2014-10-17 15:57:01
Personal initiative. All of the accusations of "handlers" or companies pressuring me to work on this are pure trolling.
@_date: 2014-10-15 17:52:20
Maybe I'm being dense, but I don't see how stealth addresses help the issue at all.
Because to be transparent the Foundation would publish the list of (stealth) addresses that everybody paid to.
I don't understand your question, maybe you can restate with a for-example.
@_date: 2014-03-30 22:09:07
Umm... You sure about that?
@_date: 2014-10-21 23:49:35
Yes, I think anybody with Chaos in the reddit nym should be subject to strict KYC/AML.
Everybody else is OK.
@_date: 2014-10-21 16:55:50
It was a lecture, from me to them, not a discussion.
@_date: 2014-04-09 15:53:24
Okey dokey:  
@_date: 2014-03-30 22:16:36
Absolutely right, stand well away from the chain, you don't wanna get hit by shrapnel when it blows.
@_date: 2014-09-02 01:58:07
Most of the miners, exchanges, and startups will fail.
There will be lots of headlines (again) on the death of Bitcoin.
I believe a few will survive and thrive and inspire the next round that will compete with them and mostly fail.
@_date: 2014-07-07 14:46:07
Invest in the cloud-mining company that is providing proof that they actually ARE mining, and aren't just taking in money, saying that they are mining, and paying out "dividends/profits/whatever" to old customers from new customers coming in.
In other words, make sure they're not just a gussied-up Ponzi scheme.
@_date: 2014-08-12 15:05:44
It will be anti-censorship-- because if you want to go against the standard, neutral policy of "choose highest fee or priority transactions" you'll need to generate a bigger IBLT.
I'm sure somebody will make a "slippery slope" argument that the opposite could happen; if an Evil Government convinced 50+% of hashing power to switch to some other standard non-neutral policy then the incentives go the other way.
To which I'd say: okey dokey, good luck with convincing miners to go against their economic self-interest to support some particular centralized organization's goals....
@_date: 2014-10-21 17:49:46
I think everybody should switch to talking in "bits" (millionths of a bitcoin)
@_date: 2014-10-21 17:00:06
I use Bitcoin for big purchases whenever I can, and charitable donations.
Day-to-day little transactions I still use credit cards and cash. I expect that will change when the big point-of-sale-systems start supporting Bitcoin.
Change 3 aspects of Bitcoin...
1. Have transactions refer to previous outputs by a hash that omits the scriptSig.
2. Use Schnorr signatures instead of ECDSA
3. Go back in time and have the Bitcoin-Qt wallet use hierarchical deterministic keys.
@_date: 2014-10-24 14:31:40
I can assure you, Sauron saw us long ago. Jim is right, without hard work from Patrick and Jim it would be much harder to use Bitcoin in the US.
You know the threads that pop up here with "my bank account was shutdown because... (I use localbitcoins.com) (I run a Bitcoin business)" ? Jim is working to solve that problem.
What are you doing besides bitching?
@_date: 2014-08-21 18:16:26
It isn't even either/or : miners are free to use it, to connect directly to each other, and to connect to the p2p network as they always did (and I would guess miners will do all three of those today; if another relay network starts up, then they'll connect to it, too).
@_date: 2014-02-02 15:22:36
Ecdsa private keys are, indeed, completely random. If you want to be unnecessarily stubborn you would check to make sure you didn't pick one of the very few values near 2^256 that are not valid private keys, but there is essentially zero chance your random number generator would pick one of those so no harm in just picking from 0 to 2^256-1
@_date: 2014-03-13 12:35:11
Yes, the "coin control" feature lets you do that.
@_date: 2014-10-17 14:03:11
There's a chicken-and-egg: we don't need IBLT until the block size is larger than 1MB (Matt's fast block relayer works great with 1MB or smaller blocks).
I haven't done the math yet, but we probably don't absolutely need IBLTs until we get 40MB blocks...
@_date: 2014-10-21 15:52:08
Obstacle / factor: getting to where people are earning Bitcoin directly, instead of having to jump through some hoop to trade the currency that they earn for BTC.
Bitcoin in five years:  uhhh.... 2019....  I left my crystal ball in my other coat pocket.
I think it'll either disappear and become an under-the-covers ledger system that Joe-ordinary-consumer never sees.
Or it will be the de-facto currency of the Internet (prices still quoted in your local currency, but payment in Bitcoin always accepted).
RE: senior school project: sure, email me questions. Please don't be too sad if I answer you with "sorry, can't, too busy this week."
@_date: 2014-08-22 16:08:12
O(1) size:  validating a block will ALWAYS be O(n) CPU time, because you have to check all transactions.
(well, unless you relax the not-going-to-trust-anybody-else-at-all constraint, in which case there might be a scheme where peers validate different chunks of a block and share summary results).
@_date: 2014-10-06 19:44:19
I'm proposing that we aim for the "Bitcoin Hobbyist running a full node at home dedicating 50% of his bandwidth to Bitcoin" -- do you agree that is a reasonable target? If not, what is?
If we can get rough consensus on that, then we can work backwards towards a reasonable maximum block size.
@_date: 2014-10-21 17:02:33
The more the merrier.
Diversity is good.
I wish I had more time, it'd be fun to learn Go.
@_date: 2014-08-21 16:39:23
Yes, this goes from new block announcements being O(nbytes) to O(nbytes/25) -- which is a fantastic more-than-order-of-magnitude speedup, but is still O(n).
My latest thinking is that to scale up many more orders of magnitude putting the IBLT idea on top of Matt's solution will be the best way to go.
@_date: 2014-05-27 19:02:58
Yes. I am not a lawyer, but worst-case scenario for a rogue telco is a hefty fine that might bankrupt the company.
Worst case for a rogue money transmitter:  see eGold, Liberty Reserve ...
(I agree with most of the article, but not "ignorance is bliss don't talk to lawyers").
@_date: 2014-10-21 17:54:10
That is what OP_RETURN is for, go for it!
@_date: 2014-08-08 22:24:34
Warning: half-baked thoughts:
  
If you start implementing any of this, let me know so I don't re-invent any wheels.
@_date: 2014-05-31 01:19:16
It is $150K/year, but the January/February/March paychecks were pegged to the price of bitcoin on January 1'st (policy was to peg once per quarter).
Those were very good months for Bitcoin, so Lindsay and I ended up getting big bonuses. And the Board decided that pegging every month was a better policy :-)
The 2014 form 990 will show me making $150K, unless the Board decides to cut my salary or give me a raise.
@_date: 2014-10-21 16:56:01
@_date: 2014-04-09 15:36:56
Yes, that is exactly why. Those instructions have been carried forward probably from the 'wxwidgets' days; it is probably time to drop them.
Testing installing over various old versions (withouth uninstalling first) on all supported operating systems is exactly the kind of thing anybody could do to help. I'd love to know the oldest version on OSX and Windows where you CANNOT just overwrite the old install (but I'm not going to spend a bunch of time doing that myself...).
@_date: 2014-08-21 20:55:05
The blockchain is self-validating, so don't worry. Worst case you waste time downloading and verifying a bogus bootstrap.dat
@_date: 2014-11-16 14:15:47
Anybody have a good model for how to screen members that won't devolve into an 'old boys club' ?
Or to ask another way: what existing, open, non-profit organizations have solved this problem well?
@_date: 2014-10-21 16:36:00
You didn't ask "Should the Foundation EVER hire a cryptographic expert?"
I think there certainly might be a time in the future where that would be a great use of funds.
@_date: 2014-10-21 16:02:24
I think it will get more formal over time, evolving towards something like the IETF model for RFCs.
@_date: 2014-11-21 12:30:49
What scammers and dodgy companies are you talking about?
A hard fact of life is 'most startups fail.' MANY Foundation member companies will fail-- if you have a way of telling in advance which are dodgy/scammy and which are just unlucky, a bad business idea, or poorly run I would love to hear it.
(Besides obvious scams like the recent MMMglobal membership that was just revoked).
@_date: 2014-06-10 02:55:49
We will have to agree to disagree. I believe one global non-political decentralized currency -- one universal means of exchange-- would be incredibly efficient, convenient, and powerful.
@_date: 2014-03-30 22:10:06
No, the Foundation does not oversee development.
@_date: 2014-12-09 17:51:30
Yes, that is "working on", and is appreciated, thank you very much!
@_date: 2014-03-19 20:40:29
Fixed. It was a bug in the release process-- I used the same virtual machine that I built rc3 in, and should have started with a completely clean build tree.
We hope to have gitian (deterministic) builds for the 0.9.1 release, thanks to lots of work from Luke Dash-Jr and Cory Fields.
@_date: 2014-10-07 20:18:13
This is *exactly* "people coming up with their own nomenclature."
@_date: 2014-04-09 16:07:08
I think sweeping funds is not necessary for... well, probably everybody.
IF you were clicking on bitcoin: links on untrustworthy sites (but why would you be paying an untrustworthy site?), AND you have a lot of bitcoins in your hot wallet (don't do that)... then there seems to be a small risk that one or more of your private keys might have been compromised.
@_date: 2014-10-31 12:20:05
Very well said (except it should be 'as he or she sees fit).
@_date: 2014-05-18 08:16:24
The Foundation's 33-page annual form 990 will be published on the Foundation forums soon.
So everybody will get to see what my salary was last year.
@_date: 2014-06-19 12:46:26
Lamport signatures are implemented via hashing.
Bitcoin ASIC hardware is insanely tuned to do the double-SHA256-hash of the 80-byte block header. Tweaking it to do Lamport signatures is probably a lot harder than we non-hardware-designers think (I doubt it'd be a firmware upgrade, but might be wrong, I'm not a hardware guy).
As other people point out: variance.
@_date: 2014-11-16 16:26:39
Easy to say... but HOW should the Foundation vet corporate members?
That is why I'm asking if anybody can point to a successful organization that can be a role model for the Foundation to follow. The models I can think up are either US-centric (e.g. you must be a member in good standing of the Better Business Bureau), arbitrary (e.g. some person at the Foundation judges you "doesn't smell like a scam"), or are not open (e.g. "you must be nominated by two members in good standing").
@_date: 2014-10-21 16:10:17
My concern is that a solution won't get consensus before we bump into the 1MB block size. That won't be a disaster, but I think it could hurt Bitcoin's reputation as a low-transaction-fee way to transfer value around the world.
RE: contributing: more code review and testing is always welcome!
@_date: 2014-10-21 16:52:37
RE: paying me directly: there is a well-worn path for getting a paycheck that is easy to explain to the IRS.
If I got a bunch of money from random people on the Internet, I'm not sure how I'd explain that income to the IRS. I _am_ sure I don't want to think about it, so I prefer getting paid in the mostly-traditional way.
Thanks for the compliment!
@_date: 2014-10-21 18:03:20
A1: see my Scalability Road Map post. The mini blockchain sounds like another way of doing UTXO commitments (but I haven't looked at it carefully).
A2: maybe I'll find time at some point to do a "mining economics" blog post and get it reviewed by some Actual(TM) Economists. I'd need to take some serious time to do a good job of it, though.
@_date: 2014-07-08 14:00:25
Your transactions might have been included because they were high priority-- were you sending hundreds of dollars (or more) worth of Bitcoin that you received more than a few days ago?
Feel free to implement a better data analysis algorithm, I don't claim that the current implementation is anywhere near optimal for estimating fees/priority -- just that it is better than what we have now.
@_date: 2014-10-18 01:09:04
No, miners cannot begin hashing with just the new block header.
Well, I suppose they COULD, but it would be a very bad idea-- they must validate the block before building on top of it. The reference implementation certainly won't build empty blocks after just getting a block header, that is bad for the network.
@_date: 2014-11-16 14:57:43
Since Ponzis are illegal and illegal businesses are already on the 'don't want you as a member ' list that is covered.
But innocent until proven guilty in a court of law and all that....  Is there a fair way of pre-screening members that won't cost the foundation more than the membership dues?
@_date: 2014-05-24 21:51:07
You mean like seed development of a more private wallet? Did that (Coinpunk)
Or maybe sponsor bitcoin.org so we get better developer docs? Done
Or pay three devs to work on Bitcoin core full time? Yup
Or handle hundreds of press inquiries per month, or figure out who has the real power in DC, or build a DoS-resistant website for members or fundraise and recruit and fill out all the necessary paperwork so all of the people doing all of the above don't end up in jail for failing to file their taxes?
I eagerly await the decentralized DAC that will replace the Foundation. But I predict I will wait in vain.
@_date: 2014-09-11 20:21:36
The Foundation sponsors Bitcoin.org, but has no control over the content-- the content is community-created and controlled.
@_date: 2014-03-29 22:30:17
FINCEN and HSGAC, probably.
@_date: 2014-07-10 17:07:02
Downvoted for spreading misinformation.
@_date: 2014-04-10 16:33:30
I see two risks if they are implemented incorrectly:
1. Somebody can 'unfreeze' the coins that went onto the side-chain without following the rules. Non-sidechain-using-bitcoin-users would not be affected.
2. Validating the new 'unfreeze coins that were tied up in an sidechain' transactions has some bug that makes it take a very long time, opening up a denial-of-service attack on the whole Bitcoin network.
Obviously any change here needs lots of review and testing, but that is true of pretty much all non-trivial changes to the core code.
@_date: 2014-06-19 22:02:14
Exactly. Security, privacy, scalability, open-ness, simplicity, ease of use, cost, ..... There are lots of concerns to juggle.
@_date: 2014-05-31 01:24:54
The Foundation has already hired PR staff (Jinyoung full-time, Amy part-time) and a community liason (Kevin, who just started a few weeks ago).
If the Foundation had a $400M endowment... that would be a very good problem to have.
@_date: 2014-01-07 02:21:45
"patches welcome"
@_date: 2014-10-21 23:54:52
Nope. I earn BTC and spend BTC (sell BTC if I can't spend directly).
@_date: 2014-10-21 15:45:05
I'm excited about the Trezor (and hope the Mycelium people get their hardware wallet working soon).
And watching the spread of Bitcoin ATMs, because getting BTC is still a bottleneck for ordinary people.
@_date: 2014-10-17 13:57:35
There are two solutions proposed:
1. Child-pays-for-parent (merchant can respend low-fee transactions with a higher-fee transaction to cover the cost of all). Luke DashJr has a patch implementing this that hasn't been pulled because it is hard to prove that it does not open up CPU exhaustion denial of service attacks.
2. A payment protocol extension letting merchants offer to pay the fees for their customer (price shown to customer is 1,000 bits, they pay 1,000 bits, merchant swallows the transaction fee and only gets 995 bits).
@_date: 2014-11-16 14:18:15
No. Everybody working at the Foundation could make more money working for a for-profit startup.
We work for the Foundation because we believe in its mission.
@_date: 2014-10-21 18:24:21
Andreas gave a good answer in his Canadian Senate testimony: distinguish between uses of Bitcoin where somebody is putting their trust ('signing authority') in somebody else, and uses where there is no extension of trust or possibility of fraud.
And give innovators some room to fail; it is nice to think that we could just pass a bunch of laws and keep Everybody Safe Forever From Everything. But that is not the way the world works.
@_date: 2014-10-20 14:47:40
If that site description comes from bitcoin.org (I have no idea where site descriptions come from), then submit a pull request to change it: 
@_date: 2014-02-19 03:17:38
No, he is not. Peter is trustworthy.
@_date: 2014-10-21 15:54:08
Use it.
Don't be too pushy about talking about it, but do let people know that you're enthusiastic about it. Think about who you're talking to, and tailor your message to what you know they care about (low fees? put bankers out of business? take control of your own finances?)
@_date: 2014-10-21 18:05:27
I think it will grow in pockets.
First physical thing... uhh, it was either Alpaca Socks (the alpaca farmer is ten miles away across the river from me here) or Red Sox tickets purchased from a friend.
@_date: 2014-10-21 18:15:26
More than $1 : sure, the network should be able to eventually scale up to handle every more-than-$1-transaction on the planet.
Less than that: I'm not so sure, we'll probably need a solution built on top of Bitcoin (e.g. micropayment channels established in advance of a bunch of tiny transactions -- Mike Hearn and Matt Corallo have that working already).
@_date: 2014-05-03 12:18:27
I like mikes
@_date: 2014-10-21 16:05:16
"Rough consensus" -- I've got to convince Wladimir and Pieter and Jeff and Greg that the change is good, and convince enough other developers who are watching so they don't raise a huge stink.
I don't always succeed; see "relay first double spend"   for a good example.
@_date: 2014-10-21 15:09:47
Biggest misconception: That Bitcoin will Topple Governments and the Powerful.  Governments will do what they always do -- they will adapt (well, the worst ones will fail, causing misery and suffering; maybe Bitcoin will speed that up a little, and mitigate the misery a little).
Far-out things only a few people seem to talk about: nothing comes to mind. Lots of people like talking about the far-out things.
TED talk: I actively try not to be the Celebrity Spokesmodel for the Bitcoin Project. Somebody else can do the TED talk.
@_date: 2014-10-21 16:13:06
I'm excited about the possibilities.
I think a lot of projects unnecessarily mix up the various services the blockchain provides, and try to make it do things it is not good at doing (like storing data). I think the best projects understand that they don't need to invent a new currency. They don't need to use the blockchain as their long-term data storage solution. And they don't need to use the p2p network as their communication mechanism.
They *should* use the blockchain as the world's most secure distributed ledger.
@_date: 2014-08-28 12:10:40
They do more than break even-- the most efficient farmers make a small profit. Just enough to keep them from switching to the second-most-profitable activity they could invest in (maybe raising chickens instead of farming).
In theory, at least. The real world is complicated by individual preferences (maybe farmer's wife hates chickens) and irrational biases like the sunk costs fallacy.
@_date: 2014-06-14 15:06:34
Excellent use of multisig and the payment protocol.
I'll try to find time to comment on the draft BIP....
@_date: 2014-10-17 15:11:09
"mel" : you're not helping promote reasonable discourse here, using innuendo ("don't want to start naming names") and then lies (making up fake quotes).
I also wish Mr. Todd had a less inflammatory communication style and more humility, but this isn't the way to convince him maybe sometimes he doesn't express himself clearly (and if we were all just as smart as he is we'd understand what he is saying the very first time he says it) or that perhaps not everybody shares his priorities or concerns.
@_date: 2014-10-21 18:13:11
Too many transactions to fit into 1MB blocks and you'll see transactions never confirming and transaction fees required to go into blocks rise.
@_date: 2014-08-12 15:00:41
Interesting idea!
bitcoind's memory pool already keeps track of when (at what time) a transaction entered the pool.
Probably the best thing to do would be for the IBLT to have a "time created" timestamp, and when peers reconstruct they ignore mempool transactions they received after that timestamp.
@_date: 2014-04-16 13:49:16
First: this is a bad idea; making unconfirmed transactions even a little more likely to get double-spent makes Bitcoin less useful, and the value of Bitcoin comes from its utility.
Second: I'm surprised they don't have a minimum undo amount. Without that, they will eventually go out of business because they have to make more in fees than the increased chance that their blocks will lose block races (because their blocks will take longer to confirm because they contain transaction signatures that most miners have never seen before and aren't in the valid signature cache).
As a miner, I wouldn't go near their pool for both of the above reasons.
@_date: 2014-10-21 18:36:14


Like hovercraft?  Big fan of hovercraft....
@_date: 2014-10-21 17:03:18
Haven't tried it-- where? You should send me an email and we'll go get some tea...
@_date: 2014-10-21 15:05:33
Jeff Garzik has a nice post about that:
  
@_date: 2014-11-21 00:20:58
Okey dokey. So give up and go home??? Tell everybody to wait six months until Lighthouse is done then cross our fingers and hope that actually works???
How have you supported core development?
@_date: 2014-06-15 18:38:39
Great idea.
@_date: 2014-03-07 02:58:48
I have regretted NOT talking to a reporter, because they then talked to somebody else who said something crazy or extreme.
@_date: 2014-04-09 19:11:34
I'm not allowed to express the opinion that driving cars around in circles for hours is maybe not so great for the environment?
I mentioned dogecoin because I LOVE their positive attitude and lots of their charity projects (just not so much the NASCAR thing...).
@_date: 2014-07-11 23:10:51
Who is "Cory Robertson????"
There is a stunning amount of misinformation in that article.
@_date: 2014-10-19 12:55:09
Someone already did. Nothing much happened.
You might as well ask: what if I set my web browser's user agent to some illegal information and then visit a bunch of websites? I can bring the whole internet down because server logs all over the world will be illegal!!!!!! 
Yeah, no, doesn't work that way. The legal system learned 'don't feed the trolls' long ago.
@_date: 2014-10-15 21:23:07
Subscribe to the "Bits and Bytes" member newsletter for what the Foundation is up to: 
Most of the Foundation budget goes to salaries; you can see the team here:   (scroll down)
I don't do budgets, so please don't ask me to give you a breakdown of exactly how much is spent on me/Wladimir/Cory (core development) versus other functional areas.
@_date: 2014-10-21 15:31:35
Do you mean the Bitcoin-Qt wallet?
Core development is moving away from adding things to the Bitcoin-Qt wallet, because there are lots of great wallet options (including privacy-focused wallets like the DarkWallet project).
@_date: 2014-09-06 15:14:54
I think this comment says it all about Peter's way of thinking.
Anybody who even SUGGESTS that an issue might not be black and white is painted as "huge risk of making things fail catastrophically."
I want to live in a world where people can express their opinions openly, with rational debate and a lot of respect for the idea that NOBODY has a monopoly on the truth.
@_date: 2014-10-21 17:01:35
I kind of like that idea. I might be the only one, though....
@_date: 2014-10-24 15:20:06
What makes your efforts (what efforts, by the way: "I support it in my own way" isn't much of an answer) less significant is when you engage in 'circular firing squad' behavior just because somebody is doing something that you think is a waste of effort.
We disagree about the correct strategy to make Bitcoin successful. Okey dokey, I get that. So instead of complaining, make some concrete suggestions on what you think will work better. Maybe you'll convince me you've got a better strategy.
@_date: 2014-10-17 13:53:14
I don't see how that changes the analysis in the blog post, which is making two major claims:
1. Transaction fees would not drop to zero, no matter the block size.
2. Block size has little to do with whether or not the future 0-subsidy Bitcoin will have enough fees to secure the chain.
Assuming that people don't work around the 10 minute artificial limit (I think they will, there are at least two approaches that should work, but lets assume they won't): that means less demand for transactions, which, indeed, should mean lower fees.
Again, I don't think that changes the analysis at all, it just adds yet another influence on the demand for Bitcoin transactions.
@_date: 2014-10-21 16:26:05
You're welcome.
@_date: 2014-10-21 15:21:42
We need regulatory clarity, ease of use, and no-single-point-of-failure security.
I think we're very close on all of those things.
@_date: 2014-08-30 20:09:16
I wouldv'e rather just donated... but I did both. Because Hal.
@_date: 2014-10-21 23:48:26
Ooh! only post right now with negative eleven points!  Eleven!
@_date: 2014-06-09 23:33:19
Yes, doesn't matter what I like, what WORKS is important.
@_date: 2014-10-21 15:43:32
It would be enforced the same way banning any activity a government doesn't like is enforced, with fines and jail sentences for anybody found doing the thing they don't like.
They would probably start by making it very difficult to exchange Bitcoin for the national currency via banks.
Unencrypted Bitcoin traffic would be pretty easy to block at the ISP level, but it is also pretty easy to tunnel it through Tor, which is harder to block. But talk to the Tor folks about that, they know a whole lot more about blocking internet protocols than I do.
@_date: 2014-10-21 18:12:14
Yeah, busted.
It really depends on the transaction amount and how much risk you're willing to take.
@_date: 2014-10-21 16:07:17
@_date: 2014-10-21 17:56:12
That wouldn't be a cryptographer, that would be a security expert. That IS something I wish the Foundation had the resources to do right now....
@_date: 2014-10-21 16:50:12
Really really interesting.
A lot will depend on how the SEC (or similar regulatory agencies overseas) reacts, I think.
@_date: 2014-06-19 16:24:51
RE: can't versus won't:
Fine, I won't talk.  Two reasons:
1) Lawyers I respect tell me not to.
2) 
@_date: 2014-06-27 18:58:04
It is weird; several people (including Mr. Todd) seems to think that economies of scale always lead to Unacceptable Permanent Centralization.
I believe that as long as there are no artificial barriers to entry competition will prevent Evil Monopolies. Because the bigger you get, the fatter and happier and lazier you get. Then some nimble little startups come along and steals all your business away.
@_date: 2014-10-21 18:34:38
Hmm? Current thinking is that hashing (like SHA256) is quantum-computing-resistant, so there'd be no issue.
@_date: 2014-05-02 11:29:09
Thanks! Spock rocks!
@_date: 2014-10-26 12:51:19
GiveDitectly is my favorite charity right now. They are data-driven, measure results, and are very efficient and effective.
And they will accept Bitcoin.
@_date: 2014-09-22 19:47:04
Consensus is 'bits'.
Except for anal-retentive coders who frequent reddit.
@_date: 2014-12-22 16:27:51
My proposed solution isn't vulnerable to manipulation by miners, and is designed specifically so that anybody with a reasonably good computer and internet connection can support the full network transaction volume.
@_date: 2014-10-21 16:45:39
I worry about that a little bit. I think the key is to make sure we keep 'permissionless innovation.'
I think lots of people confuse natural concentrations of power or wealth that come from either being smarter or more savvy or luckier or starting with more power or wealth or reputation with systems that prevent newcomers from competing with entrenched interests.
@_date: 2014-04-17 17:13:07
0.9.0 should report the transaction as 'conflicted'.
If you find a double-spend situation where it doesn't, please file an issue at github.
@_date: 2014-10-16 15:09:28
Okey dokey.
Then pay for it-- just create a send-to-self transactions once per year that pays 0.01% of its value in fees.
If you're worried about those nasty free-riding savers who refuse to do that... ummm.... I don't have a magical answer besides "maybe you want to use freicoin."
@_date: 2014-06-08 12:55:09
Once per day. Except for when I forget.
@_date: 2014-08-11 19:13:52
I think we might be talking past each other on github, I'll try here, maybe other people can chime in and help explain to you why you're wrong.
So: imagine there is a block race with your just-forward-the-header scheme.
Miners A and B send out their headers, which race through the network with O(1) speed.
Miner C gets both those messages at about the same time. What happens?
The answer: nothing happens, because Miner C can't start building on top of either of those blocks until they know which transactions they spend.
So Miner C tries to pull the transaction data for both of those competing blocks, and we're back O(n): whichever is smaller is more likely to have propagated through the network to get to miner C first.
@_date: 2014-09-01 15:09:49
Sure; please help test 
@_date: 2015-05-30 00:22:14
What? There will not be two chains....
@_date: 2014-07-23 18:46:00
Changes to 'IsStandard' rules take several months to roll out-- nodes have to upgrade to allow the new transaction types to be relayed and then mined.
But yes, assuming the commit is not reverted and assuming people upgrade, in a few months wallets and other services can start using lots of interesting new transaction types.
@_date: 2014-10-21 23:48:58
What kind of vehicle?
@_date: 2014-10-21 16:07:44
Do you want to understand the technology or just how to use it?
@_date: 2014-12-04 18:27:20
Not a dumb question at all. Yes, that is possible, but would require a change to the proof-of-work algorithm that requires EVERYBODY (miners, merchants, exchanges, end-users) to upgrade their software.
There are less disruptive ideas for fixing this (e.g.  lock-up-some-coin idea, or the proposal that pools pay winning shares more and ordinary shares less so withholding blocks is too expensive to be profitable).
@_date: 2014-06-09 23:43:02
M-of-N for Bitcoin means M can collude to steal ANYBODYS/EVERYBODYS coins.
The worst case is completely catastrophic.
M-of-N for contracts only risks the value of funds secured by all M. A failure is not completely catastrophic, and recovery is clear (never,ever trust those M oracles again).
@_date: 2014-10-21 23:55:41
For, as long as nobody's forced to pay for it.
@_date: 2014-08-01 23:54:02
When Bitcoin was a baby, it was important for everybody to run a node, so people behind firewalls could easily find somebody to connect with.
That is much less important today, because most people have decided the convenience of a wallet like Electrum or blockchain or coinbase is worth the decrease in security/privacy.
Miners and merchants and wallet services and a small fraction of super-security-conscious people are more than enough for a robust, stable network; I don't think we need more incentives to run full nodes.
Well-tested patches that make it less onerous to run a full node are still welcome, of course!
@_date: 2014-03-13 12:35:45
Binaries now available at:
    
@_date: 2014-06-15 18:27:59
No, he really isn't.
@_date: 2014-10-29 00:36:00
1J1L4sfjjhZjSJ9scMeVm81kCpCk1ktsYW will go into one of my wallets.
@_date: 2014-04-28 11:56:54
Side chains are a half-baked idea that a lot of people are excited about.
Tree chains are a half-baked idea that Peter is excited about.
Peter: how are transaction fees assigned to miners in any 'miners do not validate transactions, just provide proof of publication system?' How do you prevent miners from claiming too many fees if they are allowed to put double spends in their blocks?
Transaction fees are key to mitigating DoS attacks, which is why I don't see how tree chains can possibly work (might be just me not understanding your proposal fully, I have to admit I fall prey to tl;dr syndrome reading your stuff).
@_date: 2014-03-21 22:52:16
Mmmmm.... No.
There are lots of ways to skin that cat that don't involve building a brand new cat.
@_date: 2014-10-16 15:17:30
RE: resistance to censorship:
I'll quote myself from this thread on bitcointalk: 


















RE: assurance contracts not working:
okey dokey. Please don't continue to muddle the maximum blocksize issue with whether or not transaction fees will be sufficient to secure the network in the future. They are mostly orthogonal issues that should be discussed separately.
@_date: 2014-09-13 11:37:50
Okey dokey, I suppose I won't compromise and try to influence your opinion then.
@_date: 2014-10-21 17:10:43
Yes, see 
@_date: 2014-10-21 16:40:24
I haven't looked at Darkwallet's code.
I'm 95% confident that everything would be OK and nicely decentralized even with an infinite maximum block size.
But that's not good enough, so I'm not proposing that.
@_date: 2014-10-16 21:08:03


No. If the transactions are truly dust, then they won't be in the other miners' memory pools, and the rogue miner will have to transmit all that transaction data along with the new block, making their bloated block more likely to be orphaned.
@_date: 2015-05-31 19:35:23
I don't believe a 20MB max size will increase centralization to any significant degree.
See 
and 
And I think we will have a lot LESS centralization of payments via services like Coinbase (or hubs in some future StrawPay/Lightning network) if the bitcoin network can directly handle more payment volume.
The centralization trade-offs seems very clear to me, and I think the "big blocks mean more centralized" arguments are either just wrong or are exaggerated or ignore the tradeoff with payment centralization (I think that is a lot more important for privacy and censorship resistance).
@_date: 2015-05-31 13:09:52
And block propagation will get optimized so new blocks are just a nonce, coinbase txn and forward-error-coded list of transactions that is very small.
@_date: 2014-10-21 17:53:46
Me, personally? No. But "patches welcome." Although any network-facing code is really, really sensitive....
@_date: 2014-12-28 14:49:46
Bitcoin-Qt has always let you spend your own unconfirmed change. That is perfectly safe, unless you export your private keys to another wallet that might try to spend at the same time.
@_date: 2014-11-11 13:58:51
Peter is hung up on the decentralization/privacy aspects of Bitcoin-- he tends to ignore the importance of permissionless innovation and competition driving down costs.
And the novelty of a global payment method that lets innovators set up shop in regulation-friendly nations.
All of the above are 'real' Bitcoin apps.
@_date: 2014-10-21 23:52:44
Nope. Are you?
@_date: 2014-08-08 01:46:10
There was a secret meeting on Jekyll Island, a significant amount of cash changed hands, and then I gave them push access to the github repo.
No, really the process is "do lots of great work while not pissing anybody off too much and the existing people with push access might notice and ask if you'll help pulling changes."
If I recall correctly (and I almost certainly don't), I asked Jeff and Wladimir to help out. And then we asked Pieter and Gregory (and Mike Hearn, who said no because was too busy with bitcoinj).
If you see somebody doing great work who is calm and steady and communicates well and has a thick skin feel free to email any of us and suggest we ask them to help out, too.
@_date: 2014-09-22 15:08:58
I missed the Nazi reference when I wrote that-- my point was you CANNOT attach anything to an air guitar, just like you CANNOT un-invent an idea like Bitcoin.
Consensus was that the Foundation needed to respond to regulatory threats, so that is what happened...
(Consensus was right, I was wrong)
@_date: 2014-10-21 16:34:41
See  for my thoughts on alternative implementations.
The Foundation has already indirectly supported those projects-- I (and others) spent a lot of time last year creating testing infrastructure that is re-used across implementations.
Supporting more directly is a question of financial resources and priorities; right now, the Foundation doesn't have the financial resources to do more.
@_date: 2015-05-29 14:29:45


Ummm, no, that won't work. EVERYBODY will reject 2MB blocks until either a hard-fork switchover date or a soft-fork "supermajority of blocks have a version number that indicates support for bigger blocks."
@_date: 2014-10-21 18:25:39
I don't know nuthin about billing systems.
@_date: 2014-11-16 14:53:20
Are there any examples of that working? The Darkwallet project seems to be struggling under that funding model.
@_date: 2014-09-21 12:24:26
Disappear like go away or disappear like blend into the mainstream?
I think most successful movements get assimilated-- they don't completely replace what came before.
@_date: 2014-03-15 00:10:40
(Reasonable fee given the user's preference for how long they're willing to wait...)
@_date: 2014-10-21 18:09:00
I think once the mining sprint-to-11-nanometer race is over mining will be a commodity.
And I don't think it will be economies of scale that take over, I think it'll get more distributed.
After all, there should be economies of scale for CPU power, too, shouldn't there? Why do we all run our own computers instead of just connecting to CPUs in some big data center somewhere?
@_date: 2015-05-04 19:48:45
How would RBF help?
If all wallets just re-broadcast with higher fees, then you just end up with lots of network bandwidth usage from rebroadcasts and lots of transactions still waiting (all with higher fees).
@_date: 2015-05-27 14:41:22
I'm almost done.
I've done this as separate posts so they are easy to link to in arguments; "but what about UTXO growth" --&gt; respond with a link to the UTXO post.... "but what about twenty years from now when..." --&gt; link to the when the block reward goes away post...
@_date: 2014-08-20 11:36:21
If the miner is paid for taking the risk large versus small doesn't matter.
Peter, your 'we are doomed if we don't scale up in exactly the way I want (tree chains)' is getting tiresome.
@_date: 2014-08-10 23:08:48
Almost everybody will prune old, spent transactions and only keep a subset of the whole blockchain. You would keep the bits relevant to transactions related to your wallet.
Big merchants and services will probably just store the whole thing. A few thousand terabytes of data isn't a big expense for a moderate-sized company even today.
Nobody will download the whole chain; you just need block headers and the current unspent transaction output set to start. Easy-peasy.
@_date: 2015-05-30 12:52:31
Yes, approximately nobody will mine on the 1MB chain. Why would you, if your coins would be rejected by all of the big merchants and exchanges?
That doesn't mean all blocks will be bigger than 1MB, it just means that a miner that refuses to build on a bigger-than-1MB-block is being an idiot.
I don't think we have many idiot miners, making a profit at solo mining or running a mining pool is not easy.
@_date: 2014-07-15 23:15:12
Foundation members?
You do know that there are over 2,000 of those now, and the Foundation does not require any sort of background check to join (not sure if there is any infrastructure for background checking people from scores of different countries even if we thought checks were a good idea).
Or do you mean Foundation Board members or staff? 
@_date: 2014-10-21 18:18:07
How important for me, or how important to everybody-on-average?
Or how important will privacy be as a factor contributing to Bitcoin's success?
Me: 7
Everybody: I dunno-- 3? (I'm cynical about most people's willingness to tolerate a little extra cost or inconvenience for privacy)
Success: 4
@_date: 2014-10-21 15:26:50
Bugs in "Bitcoin" ?  "Bitcoin" isn't software, it is a protocol, so that's like asking "what do you think about bugs in HTTP?"
If there are serious problems with Bitcoin-the-protocol, then we'll fix them.   I'm pretty confident there are not any really serious, you-might-lose-your-bitcoins bugs in the protocol.
There will always be bugs in Bitcoin implementations. The best way to mitigate those bugs is with multisignature transactions secured by two different "software stacks."
@_date: 2015-05-29 21:38:56
Push them with a range of fees if you can.
That will give more data about the fee policies miners are actually running.
@_date: 2014-10-21 16:55:33
Wladimir and Jeff and Pieter and Greg have everything they need to carry on without me. I'm really not a single point of failure.
@_date: 2015-05-30 15:57:12
Maybe I should amend the blog post or write a follow-up...
0.3% was the WORST I could get, creating the absolute worst network topology possible and assuming that the miners being attacked are idiots (that they are also producing big blocks that take a long time to propagate).
"What if you are a miner several hops away from all the rest of the miners" :  Grab my code and simulate it yourself (the config files are simple text files). I'm not going to bother, because if a miner is stupid enough to not arrange to be directly or almost-directly connected to most of the other miners (preferably via Matt Corallo's fast relay network) then they deserve to go out of business.
@_date: 2014-11-11 15:26:53
Mea culpa, I shoulda read the original tweet.
My apologies.
@_date: 2014-10-16 01:40:48
The Foundation has in the past-- it helped fund CoinPunk development (which was merged into DarkWallet, if I've got my wallet-developers straight).
But the Foundation doesn't have the money to responsibly fund a couple more developers to work on a privacy-preserving wallet. Great software engineers are expensive....
@_date: 2015-05-31 02:55:27
If a large pool creates a slow-to-propagate block all they accomplish is increasing their own orphan rate. 
That actually INCREASES the profits of the other miners.
@_date: 2015-05-30 12:28:57
We know about what fraction of hashing power runs with the default 750K limit on blocksize (50-60%) and what the rest are deciding to do with respect to blocksize when there are plenty of fee-paying transactions waiting to get into a block:
 (graph requires javascript)
@_date: 2014-10-21 16:06:59
Why do you care about block times?
What problem are you REALLY trying to solve?
@_date: 2015-05-08 21:26:07
yes, the space set aside in blocks for high priority txns under default miming ruled weakens this attack.
And the transaction fee estimation code in the latest release does, too.
@_date: 2014-10-21 15:22:50
No, I haven't talked with the CIA or InQTel since my infamous talk.
I don't know if Satoshi will ever reappear.
Andreas did a fantastic job at the Canadian Senate! He should do the TED talk....
@_date: 2014-10-21 18:04:12
We'll see, it is a great experiment!
@_date: 2014-05-11 14:41:10
You might not need a Foundation, but I certainly need a place to direct journalists and regulators when they come to me with questions. I also need the Foundation to jump through the hoops necessary to get Windows and OSX code signing certificates.
Complain all you like about the Foundation being centralized, but before the Foundation I was becoming the center of Bitcoin. That would have been even more centralized and dangerous. 
@_date: 2015-05-31 16:40:07
It wasn't an arithmetic error, I should have counted bandwidth twice for relayed traffice because that's how your ISP counts (if you get 1MB then relay to your peers it counts as 2MB towards your bandwidth cap).
But I had also doubled all the numbers assuming that there would be ZERO changes to the P2P protocol, and transactions would be relayed twice across the network (once when they're broadcast, once as part of a 'block' message).
There is work underway (or already deployed-- see Mat Corallo's fast relay network) to fix the second, and since ALL of the discussion has been about 20MB versus 1MB, and since a hard fork that would actually allow bigger blocks is a long time away, and since transaction volume would have to scale up to make it worth miners' whiles to actually PRODUCE larger blocks...
I decided to stick with the 20MB proposal.
I've said REPEATEDLY that I'm completely open to specific-enough-we-can-write-code counterproposals.
If Mr. Chief Scientist of ViaCoin wants to propose starting at 10MB, okey dokey (but I'd prefer 11MB, eleven is my favorite number).
@_date: 2015-05-08 17:41:47
Yes, and if you are running off an SSD then the permanent SSD storage is also RAM....
@_date: 2015-05-24 11:52:45
have you done any calculations to figure out how much it costs the network to process a transaction? Or why 1cent?
@_date: 2015-05-30 15:49:38
I was graphing past the end of the stress test; graph updated with 26 blocks, which, according to when my node's memory pool drained, is when things got back to approximately normal.
18 of 26 miners producing default-sized blocks (70%), 5 producing small blocks (20%), and 3 producing max-sized blocks (10%).
@_date: 2014-10-21 16:06:36
I dunno. I'm starting to get tired of the title, maybe it should become "Head Cheese (Technology)"
@_date: 2015-05-23 16:51:54
The topology for that test was the two 30% miners each connected to three of the 10% miners (not the same three), and the four 10% miners completely connected to each other.
It is easy to create config files with as many miners and whatever connectivity you want to test, so you should see what happens when mining power and network connectivity follows a power law distribution....
@_date: 2015-05-26 16:58:52
Two points:
1. BIPs are supposed to be descriptive (here is what I did...) not proscriptive (here is what I think everybody should do).
2. Even if I thought a proscriptive BIP was the right thing to do, there is no chance a BIP about this would get consensus, so there is no value in proposing a BIP.
@_date: 2015-05-23 16:49:02
It is down to 4.5 seconds:
  
The real question is how quickly do blocks get to 50% of hashing power, and I don't think we have good numbers on that. Many miners are connected to Matt Corallo's "fast relay network" (and if they aren't, they should be) which propagates blocks much faster than stock bitcoind.
There are a couple of full-scale simulation networks just getting up and running, so we'll soon have a much better way of testing optimizations, bug fixes to the networking code (and bigger blocks). I haven't yet written the "We shouldn't schedule a block size increase because we haven't tested enough..." blog post; I'll talk more about why I think it is safe (and wise) to schedule the hard fork now as opposed to six months or a year from now.
@_date: 2014-10-21 16:01:46
Kind of, but not really. I'm mostly worried in the same way I'm worried about the stuff described in Phil Plaith's "Death from the Skies!" book.
I think we could do more to make 51% attacks even less likely, but don't have any concrete proposals right now.
@_date: 2014-03-10 15:20:37
Hmm... this is not me:  
What's your process for handling phishing / impersonation / etc?
@_date: 2014-10-21 15:36:29
I guess my advice would be to act like an entrepreneur-- try lots of things, expect most of them to fail, but keep your eyes open for opportunities.
I don't have specific advice for what to try, maybe other people can chime in with what has or hasn't worked for them.
@_date: 2014-10-17 18:41:42
@_date: 2014-03-19 19:26:01
The Foundation pays me and Wladimir.
Jeff is paid by Bitpay. Pieter and Gregory are still purely volunteers.
The Foundation doesn't pay a big-ass development team because that would centralize development, and the world's most successful open source software project (Linux) shows that decentralized development can work really well.
@_date: 2015-05-09 12:41:32
Interesting idea. You should write a patch (with tests) and see how much it saves in practice.
@_date: 2014-10-06 23:02:39
In my heart of hearts I still believe that going back to "no hard-coded maximum block size" would work out just fine.
But I might be wrong, and I agree that a reasonable, manual size is safer... so here we are.
@_date: 2014-10-21 16:38:24
RE: the protocol itself: maybe. I'm not a privacy expert, so I'll rely on people who are to put forward proposals to improve privacy. Right now, there are several not-yet-implemented-ideas to improve privacy that don't require any protocol changes.
@_date: 2014-10-21 15:57:30
Pieces of ethereum are interesting. I wrote about this on my tech blog a while ago:
  
@_date: 2015-05-30 13:44:00
No; I think miners should do whatever they think is in their best economic interest with respect to block size.
I am thinking about writing a patch so miners that don't express a preference "go along with the herd" -- produce blocks that are an average size. That would replace the hardcoded 750K limit.
@_date: 2014-04-02 01:16:12
Hal is awesome; I donated.
@_date: 2015-05-28 21:26:36
So where will I find this scientific analysis?
I'll certainly retract my accusation that you're trolling if you actually did do some careful peer-reviewed analysis. But all I found after a quick surf of the ViaCoin web site was a lot of marketing hype and approximately zero scientific analysis.
So I'll say it again: put up or shut up.
@_date: 2015-05-11 16:50:29
Interesting question... the total size of the UTXO wouldn't change, but memory pools wouldn't get as big as they do now (wouldn't have an hours' worth of transactions waiting in mempools if miners were unlucky in finding blocks), so the UTXO working set should be smaller.
@_date: 2015-05-28 13:01:13
@_date: 2014-10-21 16:49:14
One of the conditions for taking on the Chief Scientist role was that I wouldn't have to manage anybody but myself.
Does wonders for keeping the gray hairs away...
@_date: 2015-05-07 18:52:47
@_date: 2015-05-31 02:58:41
biconnect 1 2  connects nodes one and two, so blocks flow from 1 to 2 (and from 2 to 1-- that is why 'bi', I might implement a single-direction connect if there is a plausible attack scenario where it might help).
@_date: 2015-05-30 17:27:26
You mean like 
@_date: 2015-05-29 17:04:58
Starting at 16MiB as of 1 Jan 2015, increasing 40% a year, and it'll be twenty (or more) by the time the hard fork rolls out.
That DOES NOT MEAN that miners will suddenly start producing 20MB blocks; I expect (and will encourage them) to be conservative in increasing the size of blocks they produce.
@_date: 2015-05-31 19:41:02
I could make the same argument for why we shouldn't stay with 1MB blocks:
There are a lot of optimizations that can make huge blocks almost as quick to download as small blocks. But there is no reason to do them if we're stuck with a 1MB maximum block size-- why bother implementing a fancy Invertible Bloom Lookup Table solution for forward-error-correcting-coding of blocks if that only makes sense if much larger blocks are allowed?
@_date: 2015-05-31 19:29:50
See my response: 
@_date: 2015-05-30 15:58:12
Be specific. What risks?
@_date: 2015-05-27 21:08:53
No, we have better testing now, thanks to Matt Corallo, myself, and, lately, Suhas. If you know Python or C++ you can help make it even better....
@_date: 2015-05-27 16:42:39
Up to block 300,000. Here's one of the READMEs I created for myself for one of the test chains:
    Main bitcoin blockchain, up to block 300,000
    All transactions EXCEPT cf263594859fcf1ebb80d8ca659b283c1503ff7359d14129bdb541a2003b74a0 (from block 299,900) and its descendants.
    gen_megablocks -n=50 -d=/Users/gavin/mb/chain_50_1 -datadir=/Users/gavin/goodchain -to=300000 -skiptx=cf263594859fcf1ebb80d8ca659b283c1503ff7359d14129bdb541a2003b74a0
    cp /Users/gavin/mb/chain_50_1/coinbasetx.dat    /Users/gavin/mb/chain_50_1/regtest/coinbasetx.dat
    bitcoind $(for a in /Users/gavin/mb/chain_50_1/blk*.dat; do echo -n "-loadblock=$a "; done) -datadir=/Users/gavin/mb/chain_50_1 -daemon
The test chain combined 50 main-net blocks into one megablock (since we're running about 400K average per block, that gives about 20MB blocks).
@_date: 2015-05-23 00:26:21
I simulated two 30% miners and four 10% miner, all producing blocks that take 20 seconds to validate.
The 30% miners end up each getting 0.15% extra, at the cost of the 10% miners.
If the 10% miners decide to produce 1-second-to-validate blocks, though, then the tables are turned and the little miners have a slight advantage:
    $ ./mining_simulator --config m.cfg --runs 200
    Simulating 2016 blocks, default latency 1secs, with 6 miners over 200 runs
    Configuration: Two 30% miners, four 10% miners
    Orphan rate: 1.431%
    Miner hashrate shares (%): 30 30 10 10 10 10
    Miner block shares (%): 29.77 29.98 10.07 10.06 10.05 10.08
@_date: 2015-05-31 02:51:38
i believe keeping 1mb max blocks will increase centralization-- we will see more services like Coinbase that just update entries in their centralized database. We might see hub&amp;spoke payment networks, where the hubs are the centralization point.
We have a gloriously decentralized p2p network; we should scale it up, that is the simplest engineering solution and gives the most decentralization.
@_date: 2015-05-31 15:00:26
@_date: 2015-05-27 15:16:02
No, I don't think their business/personal projects are the reason.
I'll let them speak for themselves on their reasons; see, for example, 
@_date: 2015-05-11 16:16:48
Copy/pasted from  IRC:
Jeepers creepers, I can’t have a half-baked opinion on ANYTHING anymore, can I?
I do think 1-minute blocks would pass a cost/benefit analysis; benefits would be less variance in confirmations-for-whatever-level-of-security (e.g. wait 60 1-minute confirmations has much lower variance than wait 6 10-minute confirmations).
I think it would probably encourage mining decentralization. Easier to solo mine if you’re competing for 1.25 BTC 1-minute block instead of a 12.5 BTC 10-minute block.
… that might not be true, would have to run the numbers for disadvantage due to higher orphan rates....
I have no opinion on whether or not the adjust-difficulty-every-2-weeks should be changed if the block time was changed, but that’s something else to think about.
Disadvantage is that SPV nodes would have to download more headers, but that’s not a huge disadvantage and I think the benefits of an SPV node finding out about a re-org / double-spend quicker probably outweigh them.
One minute is not fast enough for real-time payments, so that’s neither an advantage or disadvantage.
Whether or not to change the 10-minute block time is a separate issue (with its own pros and cons) from whether or not the max block size should change. Status-quo with a faster block time would be 100K max blocks, each with one-tenth the current mining reward.
@_date: 2015-05-30 16:00:10
... you also ignore the bit where I say:
"I'm confident that we will have 20MB blocks propagating across the network more quickly than 1MB blocks propagate today, eliminating even that small 0.3% advantage."
@_date: 2015-05-30 03:17:19
731kb == 750,000 bytes, which is the default "soft limit" for bitcoind.
Sizes since this started are interesting:
    for i in {358594..358608}; do src/bitcoin-cli getblock $(src/bitcoin-cli getblockhash $i) | grep -i size; done
    "size" : 749030,
    "size" : 450929,
    "size" : 749182,
    "size" : 749174,
    "size" : 999800,
    "size" : 749048,
    "size" : 749149,
    "size" : 749141,
    "size" : 749202,
    "size" : 999974,
    "size" : 903483,
    "size" : 749075,
    "size" : 749150,
    "size" : 749209,
    "size" : 749194,
... so eleven miners running with default 750K block size. Two at max 1 megabyte block. Eligius at a 900K block (but eligius has an... interesting.. transaction selection policy).  And one miner produce a 450K block.
That is more miners producing default-sized blocks than I would have predicted, it will be interesting to see if that changes as memory pools drain.
@_date: 2015-05-29 17:23:41
... there would be more, but I asked the other core committers if it would be helpful if I asked more of the big services to publicly state their support bigger blocks, and they said "no, we believe you".
@_date: 2015-05-28 13:02:05
@_date: 2015-05-29 23:15:04
Some miners are very talkative, some secretive.
But it is always better to measure than to ask; even if you assume nobody will lie on purpose, people often make mistakes so the answers you get are likely to be wrong ("oh, you mean I'll NEVER produce 1MB blocks? I could've sworn I set -maxblocksize, let me check... oh, yeah, misspelled it....").
@_date: 2015-05-29 17:00:48
Yes, I stepped back from day-to-day work on Core exactly so I could work on bigger-picture issues like scalability-- doing things like talking to big merchants/exchanges, testing bigger blocks, writing a series of blog posts responding to objects, etc etc etc.
It is impossible to do everything; Greg, I think you've been trying to do too many things, and I wish you'd either trust my judgement on this a little more or spend the time to present a coherent alternative that we could all get behind (or tear apart or both).
@_date: 2014-06-18 16:09:41
Easy, just create a DAO on ethereum. DAO's can do anything.
@_date: 2014-12-06 15:36:58
To clarify: this isn't a full-time 40-hour-per week position. I hope it grows into that, but that depends on the health of the Foundation's finances and Sergio's willingness to put aside his other projects.
@_date: 2015-05-30 12:31:57
Larger blocks will not push out smaller miners.
  
@_date: 2015-05-25 00:13:25
yes, do the math. Last time I did it Hal Finney chimed in, so it has been a while.....
@_date: 2015-05-28 13:04:05
Please send me the analysis and methodical science behind your choices for ViaCoin's total supply, block time, and block size.
Otherwise shut up, you annoying troll.
@_date: 2015-05-04 18:56:25
Bitcoin Core already detects and warns about "large work other forks".
@_date: 2015-05-31 19:15:18
@_date: 2015-05-30 13:40:31
Bah! Description was backwards, fixed.
@_date: 2014-12-04 14:24:56
It turns out the most profitable strategy for miners is to solo mine or mine in a pool with a trusted group of people some of the time, but the rest of the time participate anonymously in a big, anybody-is-allowed-to-join pool -- but withhold "winning shares" (when you get lucky and solve a block for the pool).
The reason that is the most profitable strategy is because you still get paid for all the non-winning shares, and since the big mining pool solves fewer blocks, difficulty does not rise as much so you make more money with your solo mining (or trusted pool), too.
Ittay argues this will be positive for Bitcoin-- it will make pools get smaller as big solo miners decide they will mine on their own instead of joining a pool that might be suffering from a withholding attack.
@_date: 2015-05-09 13:11:13
That trades off increased bandwidth (to resend the UTXO) for storage.
And bandwidth is generally more expensive than storage.
Since it doesn't require a fork... go for it! Write a patch, test it, measure bandwidth and storage use before and after...
@_date: 2015-05-23 01:56:22
ASICs hash just the 80-byte block header; doesn't matter how many transactions are in the block (the block header contains a 'merkle root' that you can think of as all the transaction data smushed together in a secure way into just 32 bytes).
@_date: 2015-05-31 13:10:25
Eleven is my favorite number....
@_date: 2015-05-26 15:45:22
I've been pushing for the other core committers to come out and say:
how much?
... for months now. I think it is irresponsible to just say "not now" "some other way"  "not that much".
So luke-jr:  when? how? how much?
@_date: 2015-05-31 19:23:58
So you're saying we need to DECREASE the block size?
How small?
@_date: 2016-03-01 18:38:34
Really?  Tell me about how you have used OP_CHECKLOCKTIMEVERIFY ....
@_date: 2014-10-21 23:50:05
Time travelling alien AI.  But from the past, not the future.
@_date: 2015-05-29 14:40:42
I've been pushing them to make a counter-proposal, or, at least, a specific plan for how to move forward since February.
@_date: 2015-01-07 01:53:39
You know what version of block validation rules the miners are following because they put a version number in all the blocks they create (everybody is creating block.version=2 blocks right now).
The rules for a new block.version will be:
+ Nodes that understand the new version produce blocks with the new version number.
+ Until a supermajority of nodes are producing block.version=3 blocks, all blocks (version=2 and version=3) follow the old rules. Everybody can validate everything.
+ When more than 50 of the last 100 blocks are version=3, the reference implementation complains with the message: "Warning: This version is obsolete, upgrade required!"
+ When more than (say) 80% of the last 1,000 blocks are version=3, nodes that understand the version=3 rules refuse to build on version=2 blocks and validate under the version=3 rules (e.g. allow bigger-than-1MB-blocks).
@_date: 2015-01-07 01:21:46
Yes, see my megablocks branch in my repo:   
'make gen_megablocks' in the src/ directory might work (but your C++ compiler has to support 'auto').
@_date: 2015-05-28 13:49:34
Interesting idea.
I was just writing some code that creates histograms of block sizes over time, to get a feeling for what miners are doing, today, with respect to block size.
@_date: 2015-01-07 02:14:44
I think a lot.
I think that a payment network that support more transactions is more valuable than one that supports fewer. And I believe that miners will be rewarded more for securing a more valuable network rather than a less valuable one.
I find it implausible that the opposite would be true-- that they would get paid more for securing a network with less transaction volume.
@_date: 2014-06-10 03:02:21
We'll see... I don't think we are anywhere near a POW endgame; I think Bitcoin mining innovation is just getting started.
As always, I might be wrong-- so lots of different approaches are great, as long as they are marketed honestly as risky experiments that might fail (not a slam at ethereum, everything I've seen from Vitalik and company has been reasonable).
@_date: 2014-10-21 15:16:12
No, see 
@_date: 2015-01-24 04:26:38
How cheap is electricity in Venezuela, is it subsidized like gasoline? And any way to get enough hard currency to buy a mining rig, and any chance of a mining rig making it through customs?
@_date: 2015-01-07 01:29:13
If you create a version=3 block that doesn't follow the version=3 rules it is simply rejected by the rest of the network. No need for trust.
And the version number is part of the block header that is hashed to create the proof of work, so it can't be changed without redoing proof of work...
@_date: 2015-05-27 14:38:09
I'm not worried because the incentives are correct. Lots of people with lots of money want Bitcoin to keep working.
And technological trends (everything getting faster and better and cheaper and more decentralized all the time) are also in Bitcoin's favor.
@_date: 2015-05-28 21:32:01
That attack has been discussed a bunch of times before; it is a good reason to implement a dynamic block size (attacker just ends up wasting a lot of money).
The space many miners set aside for high-priority-any-fee transactions mitigates it, because an attacker will quickly run out of high-priority transactions so they can't completely crowd out all transactions.
@_date: 2015-05-29 21:50:11
I disagree that we'll learn nothing: we'll be able to figure out what policies miners are using for transaction selection and maximum block size.
We'll see how many miners run with the default 750K "soft" block size, how many are willing to produce 1MB blocks, how many produce very small blocks no matter how many transactions are waiting.
We'll also see how prepared wallets are, and how much people complain if their transactions take a long time to confirm.
I'm not actively promoting this experiment (I'm tempted to tweet about it, but I won't), but I will certainly be looking at what happens, and I hope you do, too.
@_date: 2015-01-08 21:43:20
Your username is "shill troller" .... do you really expect anybody to take your arguments seriously?
@_date: 2015-01-08 20:00:24
"blocks", not "blokes"....
@_date: 2015-05-04 17:32:00
Bigger blocks will mean it is less expensive to mix, because average transaction fee will be lower. So you'll spend less to mix, or mix more thoroughly for the same budget.
Total transaction fees will be higher, though (just like Toyota making more profits than Lamborghini, even though average price of a Lamborghini is higher).
@_date: 2015-01-23 22:52:01
If you are signing on a device that is connected to the Internet, and the attacker can replace the ECDSA signing software you are using (or whoever created the software is evil), then they can just send the private keys directly (or simply change the transaction so you pay the attacker the contents of your wallet).
The secret backdoor attack is only useful if the attacker is targeting off-line signing of transactions.
In that case, the attacker would have had to compromise the software or hardware of the off-line device when it was first set up.
Follow kinoshitajona's advice to mitigate the risk, and if you are really paranoid use multisignature transactions with signatures generated on completely different software/hardware "stacks." That you got from trustworthy sources, not from a USB stick you found lying on your driveway....
 
@_date: 2015-01-07 01:22:47
No, everybody is 100% behind it.
KIDDING! Of course there are a some people who think 1MB blocks are fine and dandy forever, but they are a tiny (but sometimes very vocal) minority.
@_date: 2016-03-13 22:06:47
Really? Far stronger?
I am not aware of any SPV wallets ever losing any money because they were SPV.
So the claim of 'far stronger' seems exaggerated. Marginally stronger, yes.
@_date: 2016-03-01 19:49:38
How is a soft fork not "bending the protocol" ?  New rules are introduced in a way that is invisible to old code that thinks it is validating all the protocol rules correctly.
@_date: 2015-01-20 20:11:09
Most pessimistic estimate for end of Moore's Law is about 20 years.
@_date: 2016-01-29 14:19:20
Growing by layers is one way to go, but it is risky-- if you fail at any layer, you fail.
I think it is much safer to grow like a garden. Some seeds fail, but others succeed, and the fittest grow and reproduce.
@_date: 2015-05-27 21:06:20
code is on github, I rebased my mega locks branch, feel free to run it yourself...
@_date: 2015-05-23 00:17:34
0.3% is essentially noise. Reward varies week-to-week much more than that just based on luck.
(updated blog post to make that clear)
@_date: 2015-05-04 20:56:11
Ok, let me ask another way: How is RBF any better than the transaction fee estimation code that is already in 0.10 (and the improved estimation code that I hope will be in 0.11) ?
Going to all the work of signing a transaction (which in this age of hardware and multisig wallets is substantial) only to be told half an hour or two days later "please sign again with a higher fee, your transaction didn't confirm" is a terrible user experience.
Especially because if everybody actually does that (as opposed to just quitting Bitcoin in disgust because it has a terrible user experience) you end up in exactly the same spot-- too many transactions to confirm, so you have to sign with an EVEN HIGHER transaction fee.....
@_date: 2015-05-23 16:39:08
The 0.3% is worst-case EXTRA profit for a single miner dominating the network.
That 0.3% comes from all the other miners, ASSUMING they are also shooting themselves in the foot and producing slow-to-propagate blocks.
So: the worst-case profit loss for a smaller miner will be an even smaller fraction of a percent (e.g. if there are ten little miners, each would lose on average 0.03%). And in the short term, while transaction fees don't matter, smaller miners can just produce small, fast-to-validate blocks and negate even that tiny disadvantage.
@_date: 2016-03-11 02:09:05
....except your node will complain at you if your clock differs from the network-adjusted time by too much.
And the 51% miners would have to be orphaning the 49% miners that are producing valid time stamps. So orphan rates shoot up and confirmation times double.
@_date: 2015-05-10 22:31:09
I think 1-minute blocks is a good idea. The best time to roll that out would be the next subsidy halving (makes the code much simpler).
We still need a bigger max block size, though.
@_date: 2015-05-04 18:58:26
Somebody else pointed that out, I added:
“Any change that requires a hard fork will open up Pandora’s Box and destroy the confidence people have in the stability of Bitcoin; if the one megabyte maximum block size limit can be changed, why not the total number of bitcoins issued?”
I should probably add an item about who, exactly will need to upgrade. Most wallets are SPV, and they will NOT have to upgrade (they never download full blocks, and don't know or care whether their transactions are confirmed in 1MB or 20MB blocks).
@_date: 2016-01-18 14:01:32
Can you be more specific about how a hard fork supported by at least 75% of hashpower, most users, and all of the major exchanges will 'wreck Bitcoin?'
I'll be writing a blog post in the next day or so about the incentives for consensus, and want to make sure I address whatever failure scenario is in your hear.
@_date: 2015-01-21 23:16:05
Me too.
@_date: 2016-01-13 14:11:28
Open the debug console and type getpeerinfo
If there are a bunch of bitcoinj peers, you should feel good-- you're helping people who don't want, or can't, connect to the network.
@_date: 2016-03-08 11:56:47
They are and they will.
@_date: 2015-01-08 21:42:11
If it is in miners' best interest to produce smaller blocks, then they will do that.
You seem to be confusing MAXIMUM block size with minimum block size.
Increasing the 1MB maximum gives miners more freedom/choice, not less.
@_date: 2016-03-02 21:08:48
No, they couldn't-- they don't know which of the 30,000 are safe to mine.
@_date: 2015-01-21 14:47:02
I'd be happy to address any concerns you have.
Even though "Gavin and most everybody using Bitcoin are too dumb to realize this is a bad idea" is not a good way to start a productive conversation...
@_date: 2015-05-31 16:43:46
I'll quote myself:
I agree with Jameson Lopp’s conclusion on the cause of the decline in full nodes– that it is “a direct result of the rise of web based wallets and SPV (Simplified Payment Verification) wallet clients, which are easier to use than heavyweight wallets that must maintain a local copy of the blockchain.” Give the typical user three experiences: a SPV wallet, a full node processing one megabyte blocks, and a full node processing twenty megabyte blocks, and they will choose SPV every time.
@_date: 2015-01-07 01:38:19
@_date: 2015-05-31 14:09:33
That was a soft fork.
@_date: 2015-05-10 12:05:06
There is-- it is called 'pruning' and is in the latest Bitcoin Core code.
@_date: 2016-03-25 16:04:24
Bitcoin Roundtable Consensus
On February 21st, 2016, in Hong Kong’s Cyberport, representatives from the bitcoin industry and members of the development community have agreed on the following points:
SegWit is expected to be released in April 2016.
The code for the hard-fork will therefore be available by July 2016.
If there is strong community support, the hard-fork activation will likely happen around July 2017."
@_date: 2016-03-28 12:32:36
Any intelligent discussion would be censored here-- the moderators seem to be unable to tolerate the idea that a stifling development culture for Bitcoin might lead to alternatives taking over.
I'm not saying that will happen, but it seems like an important topic to discuss.
@_date: 2016-03-04 23:20:49
Did you read the part where he talks about how you let the perfect be the enemy of the good?
@_date: 2016-01-24 14:37:54
Segwit will give you a choice between 80 bit and 128 bit security. 80 bit gives more scalability, 128-bit will give more security for applications where collision attacks might be an issue (like setting up a payment channel).
@_date: 2016-03-10 23:46:39
The attacker has to have more than 51% of hashpower, or their chain with the bogus transaction will have less work than the real chain, which your Classic node will get from another peer.
If you're imagining a Sybil attack where a low-hash-rate attacker prevents you from seeing the real chain.... That is detected by code that notices blocks aren't being produced about every 10 minutes, and you are alerted something is wrong.
This is an extremely low-risk change with big benefits for people who need to catch up with the chain.
@_date: 2016-03-02 21:19:08
No, that is not desirable.
And you're not wrong, you are exactly right.
@_date: 2016-03-04 14:10:51
Have you talked with any smaller miners?
EVERYBODY CAN HANDLE 2MB BLOCKS.
@_date: 2016-01-24 14:33:55
Yes, that was pointed out to me.
That is one of the purposes of communication-- to find out that you are woefully uninformed so you change your mind. We can't all be experts at everything.
@_date: 2015-01-06 23:01:08
That is what I will propose: allow bigger block once a supermajority of blocks created are version=3
(And start rejecting blocks that are version=2)
We already went through that once, transitioning from block version 1 to 2. The transition should be smoother, because Bicoin Core now warns that you probably need to upgrade if it notices a lot of 'up-version' blocks in the blockchain.
@_date: 2016-03-07 13:59:22
It is not either-or.
We have enough developers to work on both on-chain and off-chain solutions, but some developers have convinced themselves on-chain scaling will lead to 'only Google can process transactions.'
That is just plain silly, but people believe all sorts of silly things (like the sun revolves around the earth....).
@_date: 2015-05-06 17:39:31
It IS a bad thing if the things they are driven to are more centralized, subject to censorship, might fail and make them lose their money, etc.
@_date: 2016-03-06 23:32:18
Yes, see this graph for what happened the last couple of consensus rule changes:
 
Hash power moved from 75% to 100% very quickly (within 3 weeks with block version 2 to 3, and about a week for block version 3 to 4).
@_date: 2015-05-26 20:31:28
Like... what?
@_date: 2016-03-06 15:32:50
The limit makes transaction confirmation less reliable, because miners don't have the choice to include more high-fee transactions (they can't when blocks are full-- either because miners get unlucky and don't find blocks as quickly as normal or because somebody decides to broadcast a lot of higher-fee transactions as either an attack or just because they have a lot of transactions that they want confirmed).
Businesses and their customers want reliable transactions, and are willing to pay for more reliability. If they can't do that by attaching a higher fee, they will do that by paying a miner directly for some guaranteed space in blocks (or, like BTCC, will be in integrated miner/exchange platform).
Give ALL miners the opportunity to compete for transactions and the flexibility to include more high-fee transactions when the instantaneous demand is high and there is less pressure to centralize.
@_date: 2016-01-24 14:35:55
80-bit is the p2pkh addresses we are all using today.
Unless you need to worry about collision attacks, it is plenty secure.
@_date: 2016-03-07 15:55:34
Apparently not. As I said in the blog post, some developers seem to be against ANY on-chain scaling.
@_date: 2016-01-29 16:45:58
You mean like when the first stage in a rocket blows up?
@_date: 2016-03-11 02:15:32
Yes, but they do have to own the coins first to double-spend-steal.
If they are a majority of hashpower, though, they're creating thousands of coins per day so that shouldn't be a problem for them.
@_date: 2016-01-29 16:52:22
Yes, multiple layer2s.
But also multiple layer1s.  Why pick segwit as "The One True Short-Term Answer" ?  Why not segwit AND a hard fork ?  A hard fork is needed sooner or later.
And multiple development teams with different experiences, priorities, etc.  Groupthink is a real thing.
RE: getting back to IBLT/weak-block/gossip network stuff:  I have a feeling that's not going to happen, because there needs to be a conversation about how to avoid this unpleasantness when we get to the "lets do a flexcap (or something) plus cleanup hard fork" item on the longer-term roadmap.
That conversation will be easier if we can avoid inflammatory rhetoric like "packing everybody in a clown car."
@_date: 2016-03-01 19:47:34
You are confusing "block size limit" with "how big are blocks."
A 20MB limit would be perfectly safe-- if the network cannot handle 20MB blocks (and it can't right now) then miners won't produce 20MB blocks.
You will hear complicated arguments about "accidental selfish mining" and "advantages for larger miners" but they are just FUD -- ordinary selfish mining is a much better attack than "produce an expensive-to-validate-block and pray most people are not doing something smarter than single-threaded validation."
I have been listening very carefully to what they've been saying, have responded to every criticism raised, and have asked if I'm missing anything.  See  for links.
@_date: 2016-03-10 23:53:57
That is an Underpants Gnomes attack:
1. Majority of miners produce a 100+ block long chain with bogus million-dollar output.
2. Miners send million dollar transaction to unsuspecting Classic mode who happens to be catching up with chain.
3. ?????
4. Profit!!!
 
If I recall correctly, every node on the network starts warning something fishy is going on at step 2 ('invalid more work chain detected')....,
@_date: 2015-05-27 14:35:58
Thanks, fixed.
@_date: 2015-05-31 19:43:40
Because you keep misleading repeating that a bigger block size will mean increased block propagation and therefore more mining centralization, and because people keep saying "what about people mining with raspberry pi's in Greenland?"
@_date: 2016-03-03 19:10:32
There are people running around saying "Security Mindset!" while having zero clue what real-world security entails.
Security is not a boolean-- it is not "is this secure / is this not secure." The cost to mount an attack matters, as does the cost of alternate attacks that can accomplish the same goal. And the damage done by the attack matters a lot.
Designing around a worse case scenario is hopeless. It certainly didn't stop Satoshi; the only reason we have Bitcoin is he made reasonable assumptions about people's incentives and designed a system that does NOT assume a worst-case scenario but assumes that people respond rationally to incentives most of the time.
@_date: 2016-03-17 11:30:22
I'll double-check today, but there should be no change for SPV clients (I don't THINK they use "sendheaders" to get block headers-- if they do, I can think of a couple simple things that could be done).
However, the 'rip off SPV clients who are accepting 2-confirm txs' attack is very expensive and extremely unlikely to be practical. 'A security mindset' run amok, in my humble opinion.
I could be convinced I'm wrong-- could you work through the economics of the attack?  (Attacker spends $x and has a y% chance of getting $z...)
@_date: 2016-03-01 12:05:12
No, it wouldn't. Some developers are against ANY increase because 'it would set a bad precedent.'
@_date: 2016-01-23 15:47:19
Core devs ARE working on solid stuff.
As for copy-pasting... you need to learn how open source software works. A culture of building on the work of others is why open source software has been so incredibly successful.
@_date: 2016-01-08 02:24:34
You need to do some research, Patrick.
The NSA was involved with the SHA hash function design, but were NOT involved in RIPEMD.
So it is more likely the design Blockstream is proposing is backdoored.
HOWEVER: neither is likely at all, and you are just being an annoying troll. Grow up.
@_date: 2015-05-25 00:18:10
What can I say, I'm just a shiny happy person.
@_date: 2016-03-03 21:13:16
You can't phase out non-segwit transactions entirely, people lock up money in pre-signed old-style transactions (very secure cold-storage schemes use pre-signed transactions created completely off-line and held, possibly for years, before broadcasting on the network).  Phasing them out entirely would be equivalent to confiscating their money if they have lost or destroyed the private keys that signed those transactions.
I completely agree with moving towards a single combined block limit -- and think we should start talking about making that a dynamic ("flexcap") limit in a future hard-fork.
@_date: 2016-03-08 18:08:05
 
... for the full list.  Benchmarking framework, regression testing framework, fee estimation code are probably the most important.
@_date: 2015-05-26 20:24:28
This IS the consensus part of Bitcoin...
... but if consensus isn't possible, then it will come down to what version of code people (merchants and exchanges and miners and anybody running a full node) choose to run.
@_date: 2016-01-23 19:18:40
Because he implemented bitcoinj, which many more people are using to Bitcoin than Bitcoin Core.
Don't conflate 'Bitcoin' with one particular piece of software.
@_date: 2016-01-13 19:15:47
Err, I mean:  I agree completely, I would have complained loudly at the first block halving if the (crazy) 50-btc-forever people had been told to create their own subforum to discuss why they thought it was a good idea to do that.
@_date: 2016-03-02 21:23:46
Yes, that is what I'm saying.
Elsewhere on twitter, I said that I believe this scenario is unlikely. But it is a much greater risk (with MUCH MUCH greater potential harmful impact) than the risks of the simple fix, which is to just increase the block size limit so miners have the OPTION to include more transactions and relieve congestion.
@_date: 2016-01-08 02:58:11
Libsecp256k1 is awesome. I don't know enough deep crypto to comment on confidential transactions (or compact confidential transactions which are claimed to be even spiffier).
@_date: 2016-03-17 02:28:31
I'll have to double-check, but I'm pretty sure SPV clients don't send the 'sendheaders' message, so they won't know about blocks until they're fully validated.
@_date: 2016-03-13 22:20:22
It can easily scale at least two, maybe three, orders of magnitude on-chain with today's typical computer and broadband internet connection.
That takes us from probably a couple million people using bitcoin to maybe a billion.
Don't confuse measurements of today's code with what is possible. There has been almost no effort put into on-chain scaling (yes, libsecp256k1 is awesome, but signature validation isn't the scaling bottleneck), because the active core devs are more excited by complicated shiny new technology like lightning and segregated witness (which are also both potentially awesome).
@_date: 2015-05-04 21:02:30
We have been "exploring all options" for almost three years now. I'll post more about those other options tomorrow when I tackle "but SideChains/TreeChains/Factom/OffChainTransactionORama is a better way to do it"....
@_date: 2016-01-26 13:14:26
Old clients absolutely will know and care about the more-work chain, and will complain about it until they are upgraded to understand the higher block size limit. Proof-of-work is visible in the block headers, and headers-first downloading of branches of the chain means clients are fully aware of more-work branches before the perform full validation and discover a block is bigger than they think is legal.
Somebody could release a new client that looked for and then purposely ignored the higher-limit branch of the chain... But nobody sane would run it.
The incentives to go with the stronger branch of the fork (where 'stronger' is a complicated combination of hash power and number of businesses and users on that branch) are very, very strong.
To answer the original poster's question: no, there is no reasonable scenario where we end up with two persistent branches of the Bitcoin block chain.
@_date: 2016-01-13 21:07:23
What happened to "Satoshi didn't get everything right, and we've learned a lot since then" ?
If I recall correctly, the first pull request from me was to implement the test network.
I created the data-driven consensus-level tests for Script which other implementations use (and contribute back to when they find edge cases) to make sure they are consensus-compatible.
And I created the qa/rpc-tests integration test framework, which, I'm happy to say, has been extended to test consensus between different implementations (including different versions of Core).
Satoshi was absolutely correct-- back then we didn't have the resources (people, testing infrastructure, understanding of the issues) to contemplate multiple, compatible implementations.
Today we do.
@_date: 2015-05-24 12:14:35
a bigger max block size puts more control in the hands of the miners (since you can always create smaller blocks if that is in your best interest).
@_date: 2015-05-30 15:45:52
No, I mean "average" -- why are bigger blocks "gaming the system" but empty blocks aren't?
Median gives 51% of hashpower absolute control. Average means anybody who is producing blocks, regardless of hash rate, gets to influence it.
@_date: 2016-03-03 18:42:59
If the network cannot handle 20MB blocks, then the miners will not produce 20MB blocks.  They WANT the network to accept their blocks.
Why is that so hard to understand?
@_date: 2016-01-01 14:06:39
... Poor wording on my part. A couple hundred to a couple thousand full nodes spread across the world is plenty to be robust end secure (a dozen or three in each country).
Supporting SPV nodes is another reason for lots of nodes, but I haven't done the math to figure out how many SPV nodes one full node can easily support.
@_date: 2016-03-24 16:11:27
The system handles changing the alert key just fine, if you assume alerts are very rare events (they are):
                // Small DoS penalty so peers that send us lots of                                                                        
                // duplicate/expired/invalid-signature/whatever alerts                                                                    
                // eventually get banned.                                                                                                 
                // This isn't a Misbehaving(100) (immediate ban) because the                                                              
                // peer might be an older or different implementation with                                                                
                // a different signature key, etc.                                                                                        
                Misbehaving(pfrom-&gt;GetId(), 10);
 
@_date: 2016-01-01 03:48:25
No, the plan was for miners and businesses to run full nodes, with most people running SPV nodes.
I've never understood the full node fetish; there are hand-wavy arguments that we need everybody to run a full node 'for security' or 'in case of government crackdown' but as long as we have a dozen or three full nodes in a dozen or three countries the network will be nice and secure and robust from a number-of-full-nodes perspective.
@_date: 2016-03-03 18:41:35
Why would the price crash if you unleashed an expensive-to-validate-block on the network?
Worst case is it no transactions confirm for longer than usual.
But that happens all the time-- I doubt anybody would even notice.
Maybe if you did it repeatedly... but there are much more effective uses of that $10,000-every-ten-minutes.  Like use it to jam up the artificially-size-limited network with high-fee-paying transactions, as is apparently happening right now.
@_date: 2015-05-04 00:21:20
actually, it does change the protocol size....
.... But yes, it is intended as 'it is time to discuss this now.' I will be writing a series of blog posts in the coming week or two responding to objections I've heard.
@_date: 2015-05-31 13:17:45
Greg, are you getting my emails? I sent this last night:
Reddit comment:
"I've made alternative proposals. You've in turns ignored them, or recently I find not responded on the list while hitting me with adhomenem on a comments closed blog. It doesn't exactly make for a great working relationship, but I'm trying."
Ignored them?
I spent an afternoon working up a spreadsheet to get a feel for how your quadratic adjustment algorithm would actually work.
I asked, specifically, for you to dig down into the idea of a hard cap, and got no response.
My last two or three emails to you have got no response (I understand you're busy, I sympathize, I know startups are hard).
As for ad-hominem attacks:  the only time I've mentioned you on the gavinandresen.ninja blog is to link to a post you made on Reddit explaining your position. If there really is an ad-hominem attack, link please so I can fix it-- I do NOT want to attack you.
As for sitting silent while other people credit me for other people's work: please feel free to ping me if that happens, I'll be happy to set the record straight.
I was happy to see your four-point proposal buried in reddit comments.  Want to discuss privately or on bitcoin-dev or somewhere else?
If privately:
I completely agree that the size should be replaced by a cost that bundles up UTXO-set impact and sigops.
Do you have a link to the dynamic maximum cost target algorithm you're thinking of? Since I think a size increase needs to be scheduled NOW, I need something that can actually be turned into a pull request.
RE: agreeing on metrics for decentralization "to inform decision making" : ok.... so propose some metrics. I don't think we should delay scheduling a block size increase until we have consensus on what metrics matter, we'll just get more analysis paralysis.
RE: no controversial hard-forks:  do you mean no hard forks unless there are no Mirceau Popescue's screaming? I think that is impossible; at the very least we'll have sockpuppets and altcoin developers who want to see bitcoin fail screaming.
@_date: 2016-01-19 00:35:24
Nobody owns me.
Well, maybe my wife...
@_date: 2016-01-17 13:57:42
Maybe it was poor word choice on my part-- I've been warning 'the natives are getting restless' for probably a year now, but maybe Wlad isn't familiar with that English language idiom and thought it meant something else.
The 'we know what is best for Bitcoin' condescending attitude is very effective at alienating people.
@_date: 2016-01-30 23:00:40
Have you ever started a start-up?
I've started several; things are usually chaotic at the beginning. Usually all that chaos isn't put onto social media for all the world to see, though.
@_date: 2016-06-29 12:01:46
Would it help if I wrote a blog post explaining how a supply quota hurts producers more than consumers?
That is, how the block size limit and higher fees economically harms miners more than transaction creators, and REDUCED overall network security?
@_date: 2016-06-14 16:56:54
Can we please stop exaggerating and taking sides and over-reacting?
The libsecp256k1 code is almost certainly the most reviewed and tested code (more fantastic work from sipa), much more extensively reviewed than segwit.
Constructive reactions to my original post would be comments like "segwit is big, it is unreasonable to expect ACKs for the entire thing, I worked on the FooBar open source project and we found that reviewing in pieces with the lead developer making sure everything was covered by at least two code reviews worked well."
"Yeah, in my experience the only way to get code reviews it to pay reviewers for thorough, complete reviews."
"There's an awesome ReviewOMatic tool that my company used to keep track of what code has been reviewed and what hasn't, Core should use that."
@_date: 2016-01-14 22:08:30
Comes from computer network protocols-- an ACK packet acknowledges that a message was received and was good.
@_date: 2016-06-14 13:49:41
Mining is a zero-sum game.
Don't play zero-sum games unless you think you have some unique competitive advantage over the competition.
@_date: 2016-06-23 03:06:37
RE: jtoomim's tests:
I started with the assumption that EITHER miners are not idiots, and won't create blocks that orphan themselves because they take a long time to validate/propagate (if they are idiots, they'll go out of business).
OR that if there is an advantage to making blocks take a long time to propagate, that same advantage can be had by simply selfish mining (unless you make absurd assumptions that miners can control the network topology of other miners).
Block size limit != block size miners choose, and bitcoind can handle much, much larger blocks.
As for the email thread:  it is the one that started with "I think we're hurting bitcoin because the lack of a scaling plan is causing a huge amount of uncertainty" and ended with me asking for help with simulation/testing/etc (zero response, I guess you were too busy with BlockStream stuff). 
BUT: all that's in the past. What's the current plan for the hard fork that's on Core's road map?
@_date: 2016-06-22 18:29:38
Sure they were, we ran tests on the public testnet.
I don't remember if we specifically invited you to help out; we probably didn't, I asked you back in January of 2015 for help and was ignored (I'd still like your permission to publish that email thread, but both of my requests about that were ignored, too).
@_date: 2016-06-07 10:26:08
Is that attack economically feasible? Or will the attacking miner pay more in to tx fees than they gain in making competitors blocks take longer to propogate?
@_date: 2016-06-14 14:02:32
Why no segwit ACKs ?
I talked with another dev a while ago who estimated a couple of weeks of a couple of hours a day to review segwit (most of us find it impossible to do code review for more than a couple of hours).
I'd be curious to hear from potential reviewers-- any other reasons you haven't reviewed segwit? Lack of good code review is a chronic problem, any ideas for fixing it?
@_date: 2016-06-27 23:25:46
Happy to join consensus, tired of banging my head against a brick wall.
So as soon as Core comes out with (or even starts to discuss) a reasonable proposal, I'll help promote it.
So far, I've seen nothing but 'wait for the Lightning silver bullet-- until then, Schnorr and segwit will be plenty good enough, we know because we're Developers, we write code!'
@_date: 2016-06-27 18:41:32
But there have been zero public efforts to discuss or develop a hard fork.
Unless there is a secret protocol development that has been kept hidden from me.
@_date: 2016-06-08 01:30:37
I am not a lawyer...
... but why would you get in trouble if you can:
A) Point to transactions in the Blockchain that show when you purchased the coins.
B) Show that you paid income taxes on the cash used for those purchases (e.g. cash was withdrawn from a bank account that your paycheck was deposited in to).
Just pay any capital gains taxes if you bought low and you should be fine, even if you are audited.
Unless the cash came from unreported income, in which case it might be worth the peace of mind to hire a tax attorney to file amended returns and pay back taxes and penalties.
@_date: 2016-06-15 13:12:35
Pieter is fantastic, incredibly reasonable and pleasant to work with.
@_date: 2016-06-14 14:25:46
Why no ACKs then if it has been months?
@_date: 2016-06-27 18:39:36
Who is 'the community' and what percentage would have to agree?
Every single informal poll I've seen over the last two or three years shows broad support for increasing capacity by raising the 1mb block size limit.
@_date: 2016-06-20 13:18:25
Excellent comment, excellent video.
Since demand for Bitcoin transactions is much more elastic than supply (easy to use another altcoin or fiat, miners don't have many alternative uses for their mining equipment), economics says the miners will pay more of the cost of the block size quota tax.
And we all suffer the deadweight loss....
@_date: 2016-06-15 12:47:45
I reviewed the specs and gave feedback (that resulted in improvements in the BIPs).
I haven't done a code review for two reasons: because I think other technical work should be a much higher priority (malleability isn't a critical problem), and because I've decided life is too short to spend time working with unpleasant people (and in my opinion Core had several very unpleasant people contributing these days).
@_date: 2016-11-22 18:35:59
Excellent point! Deleted comment.
@_date: 2016-11-05 21:15:51
We needed more space a year ago.
If you would ever bother to listen to people who actually produce the most Bitcoin transactions instead of worrying about non-existent problems (like excessive centralization) you would know that.
Instead, you call them spammers and ignore them.
@_date: 2015-08-25 16:51:59
There can be many bitcoin client implementations and many implementations of the consensus code.
There may be implementation bugs in those many codebases that cause them to lose consensus with the rest of the network-- and we've seen several variations of that even with a SINGLE codebase. Bitcoin Core versions prior to 0.7 could self-fork, even if running on identical hardware.
Versions prior to BIP66 roll out could fork on 32-bit versus 64-bit machines.
The bugs get fixed, and blockchain consensus marches on. I call Core the "reference implementation" and not "The One True Implementation" ....
@_date: 2015-08-24 02:30:35
what CPU and memory characteristics are non-linear with block size? I tested 200mb blocks to verify linear behavior.
@_date: 2015-08-25 18:10:03
From an email I sent yesterday:
"I think that's the thing that bothers me most about this whole thing-- I said I was stepping aside as lead of Core to focus on bigger, cross-implementation issues. And I did, spending a lot of time talking to people and writing and then coding up a compromise that I thought could reach broad consensus (not just among developers-- there are lots of people who want NO block size limit at all).
And then all that hard work is dismissed and it is 'well, Gavin hasn't even really submitted much code lately....' "
That said, I'm proud of the code I've contributed lately -- for example, the RollingBloomFilter class that other people have picked up and are using for various optimizations (I implemented it to optimize memory usage).
And the accurate sigop/sighash counting code will almost certainly be used by *whatever* max-block-size-increase proposal eventually gets consensus.
And before all of that, I was working hard on testing infrastructure; I'm happy to see people extend the Python-based qa/rpc-tests (and REALLY happy about the work the Chain Code Labs folks have done to take that to another level).
@_date: 2015-08-25 12:14:02
have you talked with any of the major exchanges?
There will be only one chain.
@_date: 2015-08-14 17:09:54
As people have said in the comments, rolling out a change to the maximum block size takes months.
We're already hitting the limit sometimes when miners get unlucky and don't find many blocks; that will just get increasingly worse, and fees will rise.
That's not a disaster-- Bitcoin will survive. But it means it is less expensive for an attacker to drive up fees unpredictably (wait until the memory pool is backed up because miners have been unlucky, then send a few megabytes of higher-than-average-fee transactions).
It means fees are higher for users than they should be, but miners will get less overall revenue from fees (because there are fewer transactions than there should be), so the network will be a little less secure. In economic terms, artificially limiting the block size results in deadweight losses for both miners and users.
It means lower-value transactions are not economical to do on the blockchain, so they're driven to more centralized off-chain solutions (like trusted wallets or, eventually, Lightning hubs).
All of the above make me think that scaling up by increasing the maximum block size is urgent, and why it has been at the top of my priority list since January.
@_date: 2015-08-25 17:21:42
... but BIP100 isn't in Core...
@_date: 2015-08-27 11:31:46
Yes, dial down your -maxconnections to 16 or 20 and see how you go...
@_date: 2015-08-23 17:58:06
No, I don't see how large miners can "cheaply" fill excess space in order to get an edge over small miners, NONE of the simulations that have been done show that. Please point me to one that does (Pieter's doesn't count, he was simulating a miner that was bandwidth-constrained. My simulation showed a very small effect with 20MB blocks IF the small miners are stupid and are somehow convinced to also produce 20MB blocks, but that effect goes away if the small miners produce small, quick-to-propagate blocks).
I DO see how a more-than-30%-hashrate miner could try the selfish mining attack... but as Mike points out in another comment here, miners aren't generally malicious AND these "how many miners can dance on the head of a pin" theoretical attacks backfire if more than one miner attempts them at the same time.
@_date: 2015-08-25 12:38:50
Fun fact: the average web page in 2015 is two megabytes big.
So if you browse more than one page a minute, your computer is downloading more data than the initial 8MB max limit of BIP101.
Reference: 
@_date: 2015-08-04 14:36:23
He demonstrated no such thing.
He demonstrated it is bad to have a low-bandwidth connection to the rest of the network (and that will be worse when transaction fees become significant).
That's all.
@_date: 2015-08-14 17:11:09
To be clear: I think the Lightning network is a great idea, and I fully support the changes to Bitcoin needed to support it.
It is not either/or, we should do "all of the above."
@_date: 2015-08-07 14:32:53
The conclusion is that poorly-connected miners suffer at the expense of well-connected miners? What happens if 'F' is directly connected to 'A'?  Are you assuming that 'A' can somehow control network topology?
It is interesting that it is 'B' and 'C' in your simulation (the big-block-miner's peers) who profit the most. If your simulation is correct, it seems like it is a disincentive for 'A' to try this attack.
And when you say 'A' stops doing work after relaying a big block... that's odd. Does the simulation change if it keeps doing work while its huge block is propagating?
@_date: 2015-08-27 15:43:39
The simulation is for the "attack" scenario of a rogue miner intentionally creating slow-to-propagate blocks to try to attack other miners.
Propagation to the edges of the network doesn't matter (miners are very well connected).
And common case is instant propagation, because almost all transactions have already been propagated and the bulk of validation (e.g. signature checking) has already been done.
@_date: 2015-08-06 14:30:13
I'm curious to hear Luke's answer, too.
@_date: 2015-08-27 11:33:24
Can your computer handle downloading four web pages every ten minutes?
If it can, then it should be able to handle 8MB clocks.
@_date: 2015-08-25 17:10:33
You're wrong in two different ways:
1) Current p2p message size limit is 2MB (it was changed a couple releases ago).
2) My implementation of BIP101 keeps the limit for all messages except for 'block' -- those messages are limited to whatever the current maximum block size possible is (given current time and network rules about how accurate clocks must be).
Please, this debate is noisy enough, don't spread misinformation.
@_date: 2015-08-04 11:22:47
is anybody still claiming that miners can get an advantage creating slow-to-propagate blocks? Simuations show that ain't true....
@_date: 2015-08-24 02:26:51
New block propagation must be fast or miners will choose to create smaller blocks.
As for attacks: straight-up selfish mining with &gt;30% hash rate using small blocks should be a much more viable and profitable attack than some convoluted combination of huge blocks full of never-before-seen transactions and  control over network topology and assumptions about peer behavior (e.g. If your attack requires sending to 30% of hashpower AND that peers do full block validation before relaying or building on a block, then that attack is trivially thwarted by immediately relaying all blocks with valid POW or mining an empty block on top of new blocks before they are fully validated).  (.... Although just penalizing blocks that take a long time to validate by refusing to build on them for a few minutes after validation is done might work nicely, too)
@_date: 2015-08-27 03:05:38
you mean like this:
And this:
And this:
@_date: 2015-08-08 12:19:09
Tell your ISP it is a false positive-- the DNS query is to find peers on the bitcoin peer-to-peer network, it is not the fast flux not.
I got the same report from Chunkhost, and they were understanding.
@_date: 2015-08-25 20:14:02
Mike Hearn has dictatorial control over one implementation of the Bitcoin consensus protocol.
If you don't like it, use another implementation. I've just started playing with  (because I want to run some more large block tests, and they've already implemented most of what I want to do:  ).
@_date: 2015-07-17 17:32:48
And if trends don't continue it is easy for miners to create smaller blocks, either individually or collectively as a soft fork consensus change.
@_date: 2015-08-25 16:56:13
Pieter and Greg are both brilliant, and this is exactly the type of thing that is perfect for deploying and testing on a sidechain before rolling into Bitcoin-proper.
@_date: 2015-07-01 16:59:46
If you assume that larger pools have better bandwidth and connectivity, then yes, eventually, in several years, when the block subsidy is smaller and fees are important, there will be an incentive to join a pool capable of producing bigger blocks.
However, "bigger pools can afford better connectivity/bandwidth" is just an economies-of-scale argument. With the block sizes we're talking about the cost of sufficient bandwidth is small enough even a small pool should be able to afford it.
There are much larger economies-of-scale factors that might tend to make bigger pools more attractive; a big pool might be able to give better customer support or be able to afford a fancier website or have enough staff to keep 24/7 uptime or a gazillion other things that will cost a LOT more than the $50/month hosting costs to keep two or three full nodes up and running...
@_date: 2015-07-25 00:38:40
You have to break DOUBLE-sha256, which is much harder.
If Bitcoin used MD5, a very broken hash function, in all the places it uses SHA256...  it would be secure (because the attacks against MD5 are prevented if you hash everything through MD5 twice).
@_date: 2015-07-30 19:48:54
@_date: 2015-08-23 15:32:33
why would they take so much longer to propagate?
Transaction validation can and is performed prior to new block announcement; compared to ecdsa verification (which is the work that can be performed in advance), validating against a warm UTXO CACHE (that is the work required when seeing a new block) is a trivial amount of CPU time.
@_date: 2015-07-07 22:10:16
@_date: 2015-07-31 13:50:12
Try running with a smaller -maxconnections 
@_date: 2015-07-30 20:10:47
But I did address those concerns:
@_date: 2015-07-01 02:07:18
Miners won't create blocks that propagate slowly-- they lose if they do.
And a hard fork should be scheduled long in advance... So the right order of events is:
1. Schedule hard fork
2. While hard fork is rolling out, continue working on optimizing block propagation (and everything else).
If (2) takes longer than we think, no worries-- miners will not create big blocks unless they propagate efficiently.
All of the 'miners will attack by creating big block that propagate slowly' is just not true-- latency/bandwidth matter, but block size just doesn't.
@_date: 2015-07-02 16:25:47
Let's see...  8GB every 10 minutes is:
...13Mbps. It's got gigabit ethernet (1,000Mbps) so that should work just fine.
...about 30,000 transactions per second. I'd have to benchmark the Quad-Core ARM, and I don't know how fast the Quad-Core GPU would be at signature verification (somebody would need to write GPU signature verification code). My guess is together they could validate a few thousand transactions per second.
You'll definitely need a faster CPU (or one with more cores) to fully validate 8GB blocks.
You'll also need more fast-ish memory for storing the unspent transaction outputs set-- 64GB almost certainly won't be enough. But in 20 years we'll be measuring memory chip capacity in terabytes.
 And, of course, if you want to store the entire block chain you'd need to hook up a hard drive (you could run pruned, of course, with a few hundred GB of storage).
@_date: 2015-08-01 14:45:06
so if I say 'we are governed by the physical laws of the universe' that is code for 'God manipuates...'?
You have a narrow view of what the word 'govern' means...
@_date: 2015-07-30 21:31:42
Run in a datacenter where a government crackdown is unlikely. Are you worried about a global ban from every government? If that happens, Bitcoin will doomed to be a niche thing anyway, and the max block size won't matter much.
Go lookup how much datacenter bandwidth costs have dropped in the last 20 years and report back; I'd do it but I'm feeling a little lazy.
@_date: 2015-07-27 13:01:57
Try running with -maxconnections=16 for a month.
@_date: 2015-07-30 17:25:25
I don't know how to implement "no limit" without opening up trivial memory exhaustion attacks, or maybe I would have...
@_date: 2015-07-30 19:08:03
... so an attacking peer tells me "here's the last chunk of a valid 4 terabyte block" (along with a merkle proof for that chunk and valid proof-of-work).
And then it gives you the previous chunk, and so on. You can't count tx chaining until you're given the first chunks.  You'd have to require chunks get served in order (limiting parallelism opportunities) and assume that the attacker didn't have enough entries in the UTXO set they could chain to run you out of memory.
It is certainly much simpler to have a reasonable-but-large maximum block size, so we CAN, for example, parallelize download of parts of blocks from multiple peers.
@_date: 2015-08-04 13:24:34
Yes, bandwidth and latency ("well connected") matter at any block size, and when transaction fees matter miners that can broadcast bigger blocks faster will have an advantage over miners that can't.
I don't see the connection between that and "miners can get an advantage by purposely creating slow-to-propagate blocks."
Or, to be more precise: if miners want to try to gain an unfair advantage, their best strategy is selfish mining with fast-to-propagate blocks.  Nobody has shown an effective strategy where creating a purposely-slow-to-propagate block (as opposed to just "bigger block == more fees") gives any advantage.
@_date: 2015-07-01 02:11:27
All a miner accomplishes publishing a bloated block is increasing their orphan rate and losing money.
@_date: 2015-07-21 12:23:23
The BIP101 patch changes it. It is harder to change with a voting scheme like BIP100, but not crazy-hard.
@_date: 2015-07-16 18:25:26
Up to twelve now...  (it's been running since yesterday, but it's on an IP address that is new to the network so I expect it will take a while to get close to the bitnodes -maxconnections setting).
And yes, I was excited: Elevenses!
@_date: 2015-07-27 12:57:45
what advantage? If you create a slow-to-validate block all you accomplish is increasing your orphan rate.
If you are thinking of any sort of selfish mining strategy, then the best thing to do is create fast-to-validate blocks but don't announce them right away.
I think Peter Todd still claims some advantage if you can manage to connect to some particular percentage of hash rate, but simulations (mine or Pieter Wuille's) show that isn't true. Latency and bandwidth matter (if you are mining you definitely want a low-latency high-bandwidth connection), but they matter at any block size.
@_date: 2015-07-30 20:05:34
See 
My objection to those since-2008-projections is we've been in a lull with bandwidth-to-the-home growth; the global financial crisis meant governments and big telecom companies stopped investing in rolling out fiber-to-the-home.
I think all signs point to a very rapid acceleration of bandwidth to the home; Google is working on it a few different ways (fiber, high-altitude wifi), Elon Musk is working on it (satellites is it?), and there is certainly demand for it (e.g.  for a hyper-local example from where I live in rural Massachusetts).
If I'm wrong... then either you might eventually have to run your full nodes in a data center (but my existing, admittedly-above-average home internet connection and home computer could easily support fifty megabyte blocks), or maybe there will be consensus to soft-fork to a smaller max size.
@_date: 2016-02-29 21:47:28
Don't know why talks about performance of BLS being a problem: 8,000 signatures per second is more than three orders of magnitude greater than current transaction volume.
With a couple more orders of magnitude with signature validation caching (because transactions trickle in constantly but blocks are found on average only every 600 seconds).
Immaturity of pairing crypto implementations is a pretty good reason not to go for BLS, but it looks like there are implementations in C and Java that have been around for years (doesn't mean they've been well reviewed, of course).
The ability for MINERS to combine all the signatures is really appealing.  One 20-byte signature that works to validate all the transactions in entire block would be a really awesome bandwidth savings.
PS: funny to see the communication chain on this one: Joe Bonneau mentioned BLS to me at Financial Crypto last week, I mentioned it to Adam Back (because Adam has been talking about Shnorr as a way of saving space per-individual-multisig-transaction), and then Adam talked to Greg...
@_date: 2016-02-25 17:38:33
Uhh... that depends.
If the deposit key(s) are uniquely assigned to you, and the holder isn't doing a CoinJoin-like thing and combining withdrawals from multiple users into a single transaction...
... then withdrawal transactions will be valid on both branches (assuming one of the branches of the fork doesn't do something like change the transaction format).  The withdrawal would be valid on both branches, so you'd have access on both.
HOWEVER:  the incentives are insanely strong for there to be Just One Surviving Branch. Miners aren't idiots, when picking which branch to extend they WILL (and do) pick the most-work branch.
@_date: 2015-07-07 22:08:44
limiting max transaction size keeps it O(n)-- which is why my increase block size pull request limits txn size to 100k (current IsStandard size).
I need to remember to write another BIP for that when I'm back from vacation...
@_date: 2016-02-24 11:46:54
It is exactly wrong to claim that the survival of Coinbase might depend on not getting stuck at the current block size limit. Coinbase is big enough to code around the issues.
The little startups trying to use the bitcoin blockchain are the ones that won't survive -- or won't get started at all.
@_date: 2016-02-05 22:11:05
A 20mb limit would be safe.
A 20mb block would be dangerous for the miner producing it, but safe for everybody else... which means we wouldn't see 20mb blocks produced until the network protocol was upgraded to make block propagation quick.
So, again, safe.
I think everybody besides maybe Peter Todd has given up on the idea that miners are either evil or stupid and would try to attack the network somehow with big or expensive-to-validate blocks, which seemed to be the worry a few months ago.
@_date: 2015-08-04 14:34:54
What do you mean by "smaller miner" ?  Less hashpower?
Again, YES, CONNECTIVITY MATTERS. What does that have to do with "smaller miner" ???
Unless you're making the "big miners have economies of scale that let them afford better network connections" argument.  In which case I'll point out the economies of scale for other parts of the mining operation (e.g. buying electricity at bulk rates) VASTLY overwhelm any possible network connection costs.  Yes, even through the Chinese firewall...
@_date: 2016-02-23 09:45:52
No, that is exactly wrong.
Smaller blocks can be a competitive advantage for larger companies, because they can:
a) Contract with big miners or pools for guaranteed block space or "preferred" inclusion of some number of their transactions.
b) Arrange once-a-day (or -hour/-week/-month) settlement with other large companies with whom their customers do a lot of transactions. E.g. Coinbase and BitPay might agree that any payment from a Coinbase customer to a BitPay merchant settles immediately off-chain, and is combined with all the other such payments into a single on-chain settling transaction.
That work (which is engineering and business development and lawyers) only makes sense if the savings and improved customer experience outweigh the costs.
Big, well-funded companies like Coinbase can afford it. Little companies may get squeezed out, and individuals will have an increasingly strong incentive to use a service like Coinbase's centrally managed wallet-- it will be more reliable and cost less.
If Brian was running a really big, old company then maybe he would be all for keeping the block size low to drive more business his way. But he's running a startup company in Silicon Valley, and he (and his investors) understand that growing the entire Bitcoin infrastructure and ecosystem is the only rational strategy. Spending any time even thinking about putting smaller competitors out of business is a waste of time when you're a startup in a brand-new market (or a startup disrupting a huge potential market).
@_date: 2016-02-06 16:20:20
Why do you think waiting a year will make any significant difference for software that is unmaintained garbage with nodes stored in a close or who don't have a dev to deal with upgrades any more?
And isn't it better if such poorly maintained software breaks sooner rather than later?
@_date: 2015-07-31 02:29:56
i am being cautious and allowing slack; 20MB was about half last years's typical fast broadband connection, 8MB is even more cautious, and the double-every-two years is below long term cpu/storage/bandwidth price/performance trends.
I wonder what block size the Facebook 3G internet drones that were announced today would support....
@_date: 2016-02-25 14:35:13
If you live in a country with a good legal system and property rights...
... then they're mostly yours.
I think the original poster's point is good, and 100% ownership and control is best, but I also think it is not the end of the world if lots of people decide that they'll sacrifice some ownership and control for convenience.
@_date: 2016-02-10 20:16:46
Vote For Me!  I promise Free Bitcoin For Everybody!
(wait... no... I'm not doing that any more....)
@_date: 2015-07-30 04:01:43
Greg Maxwell is doing fantastic work on the privacy/stealth address front, deployed to the Elements side chain first.
@_date: 2015-07-01 21:04:00
I don't understand the question. "Larger pools not faster" &lt;-- not faster at what?  Announcing bigger blocks?
No, there is no reason why a larger pool would be able to announce new blocks faster than a smaller pool.
@_date: 2016-02-10 20:14:42
Unsafe how?
People keep saying this, and "slightly less profitable for miners with crappy connections" I might agree with, but unsafe how?
@_date: 2015-07-17 13:14:18
Because implementation is simpler if block size is a pure function of the 80-byte block header, which contains a timestamp but not a block height (calculating height requires you have the full chain of previous blocks or the coinbase transaction).
Simpler is better in consensus-critical code.
@_date: 2012-03-17 03:24:13
No lead pipes this time (lucky me!).
@_date: 2016-02-07 13:37:18
I absolutely do not condone running 'fake' nodes of any kind. Don't do that.
Running real full nodes to make sure there are enough connection slots for SPV wallets is a good idea.
@_date: 2016-02-05 22:02:38
I like BIP143, we should do that, too.
@_date: 2017-04-18 12:54:11
Good advice, but too 'Buck Rodgers Secret Decoder Ring!' for my taste.
I'd say: buy a bitcoin wherever is most convenient (e.g. Coinbase).
Print a paper wallet on a computer that isn't acting wonky. Send the Bitcoin to the paper wallet.
Put it in a zip-loc bag in your safe deposit box (best, but you do have to remember to pay for it every year) or safe.
If you were storing 100+ BTC then unplugging and then destroying computers in case of malware and being paranoid about your future government confiscating your paper wallet for some future wealth tax starts to make sense.
@_date: 2016-02-24 11:48:17
How many transactions per second can the computer you're using right now verify?
@_date: 2012-04-25 15:55:38
I haven't talked with Zhou or Amir, which is good because I can speculate without betraying any secrets: I'd guess this is a reaction to the Linode breach. Bitcoinica needs to get serious about wallet security, and it makes sense to outsource that.
@_date: 2016-02-06 14:09:08
I asked them if 4 weeks was long enough.
"Plenty of time" was the response from ALL of them.
If anybody has a situation where a download/restart or patch/restart would take more than a couple of days, let me know. I can't think of one.
@_date: 2015-04-07 11:17:37
This slide deck is not a post-mortem of pat mistakes, but a 'this is where we are, and here is a proposal for how to move forward.' 
Why would you expect a lot of detail on past expenses?
@_date: 2015-04-07 18:01:32
I love hoi polloi!  (that's the yummy Indonesian dish, right?)
@_date: 2015-04-12 14:06:14
I think the technology is still too young to market to 'the mainstream.' What would you tell a mainstream potential bitcoiner to do to get bitcoin? Open a Coinbase account? Use localbitcoins.com (after setting up which wallet?)
A generic, vague 'got BTC' marketing campaign is a huge waste of money until consumers can buy BTC in the grocery store next to the milk.
At this point I think market research and marketing should be done by companies trying to solve real problems and find the niche markets where bitcoin will succeed first. 
@_date: 2015-04-01 11:54:22
... So drone operator 2 establish good reputation and then just starts stealing packages instead of delivering them. Once their reputation is ruined they create a new one and do it again.
Or drone operator 3 is the DEA and adds a GPS tracking gizmo to every package.
@_date: 2015-04-13 17:20:41
Marginal-fee transactions ALREADY compete to get into a block, even at the current average block size of about 300kbytes. See  for a nice graph (uses JavaScript to draw the graph).
Why do you think that would stop being true with a larger maximum block size?
@_date: 2015-04-07 18:03:12
You can't get rid of me that easily, I'm not going anywhere.
@_date: 2015-04-22 14:19:55
The block.version=2 to block.version=3 soft fork for BIP66  . Pieter is graphing adoption: 
RE: people could lose money: how would people lose money if the maximum block size was increased? Are you talking about miners that refuse to upgrade?
@_date: 2015-04-07 12:32:41
What would y'all like me to talk about?
@_date: 2015-04-27 12:56:05
Because the p2p networking code isn't robust enough to call it "1.0" (it is an accreted set of hacks to mitigate DoS/Sybil attacks; it should keep track of resources used by each peer, and prioritize work so no peer can overwhelm CPU or bandwidth or disk usage).
And because the wallet code needs work to match best practices (use keys derived from a master seed, do NOT rely on BerkeleyDB).
@_date: 2015-04-07 14:30:40
You're confusing talk about a certification program for software (maybe something like "this wallet's source code has been examined by somebody trustworthy, as has the process for producing the binary from source, and has been certified not to email your private keys to the company") with "certified developers."
Olivier, it is OK not to know what is going on, but it is not OK to assume that you DO know and then spread false information.
@_date: 2015-04-08 15:50:08
Okey dokey, but this smells very much like a conspiracy theory.
Especially the action item at the end: "spread the word."
If you had ended with "These are the concrete things we all should be doing" then I'd be more sympathetic... assuming those things weren't just "BE EXTRA PARANOID! THEY ARE OUT TO GET US!"
@_date: 2015-04-07 04:23:36
yes, just like the most efficient computers are the biggest, which is why we all use mainframes instead of carrying computers in our pockets....
@_date: 2015-09-05 15:17:29
No, BIP100 votes are in the blockchain, so it won't have any more difficulty coming to a consensus.  If you have an up-to-date blockchain, you have all the information you need to calculate the maximum block size for the next block.
I agree with you on the rest of your points.
@_date: 2015-04-10 15:52:40
I have no idea how mobbr calculates contributions (and am not going to take the time to find out).
I hope it is not lines of code, because if it is I suppose I'd owe mobbr.  My all-time stats for lines of code added / removed from Bitcoin Core is:
   62,915++ / 76,186--
@_date: 2015-09-29 01:18:14
Nope, I'm giving Greg/Pieter/wlad a couple weeks to do what they said they'd do in Montreal-- work on a solution everybody is unhappy with but everybody can live with.
@_date: 2015-09-02 21:31:04
If by "desired levels of usage" you mean "infinite usage", then okey dokey, there is some level of transaction volume that even Google and it's datacenters would not be able to handle.
But if you mean "capable of handling the current level of worldwide electronic payment volume" then I must strongly disagree. We certainly CAN scale up that far and remain at least as decentralized as we are today.
If you don't think it can be done, again, okey dokey. I'd just ask: do you always assume problems are impossible if you, personally, can't figure out how to solve them?
@_date: 2015-09-05 19:07:58
How fast is your network connection?
@_date: 2015-09-02 19:55:41
No, new blocks with timestamps too far in the future (or too far in the past) are invalid.
@_date: 2015-09-03 18:05:49
Re-orging with larger than 1MB blocks DOES work, I ran those tests last December:
  
Re-org test results, short re-org, ~20MB blocks
/usr/bin/time -l memory usage:
674,136,064  maximum resident set size  (node that performs the re-org)
297,807,872  maximum resident set size  (other node)
Total time for re-org: 17 seconds
Re-org test results, 100-block-long re-org, ~20MB blocks
1,436,880,896  maximum resident set size (node that performs re-org)
  377,962,496  maximum resident set size (other node)
Total time for re-org: 4,516 seconds
@_date: 2015-04-19 15:55:11
it might be time to fork into two projects: one that targets the needs of companies, and another that targets geeky I-wanna-run-a-full-node users.
Their needs are different (eg companies will want easy deployment and management of multiple nodes, integration with enterprise databases and monitoring systems, etc. Users want friendly ui's and for it to run on their six-year-old 32-bit laptop).
@_date: 2015-09-05 19:50:38
i'm mostly libertarian. Not a punk, but I do enjoy listening to the Ramones and Sex Pistols pretty regularly.
@_date: 2015-04-07 11:21:52
Please don't keep talking like it is any of your business how I (or Wladimir or Cory) decide to make a living. If I want your advice or money I will ask for it.
And it is insulting for you to say I would agree to be 'beholden' to anybody. I am certainly not beholden to you....
@_date: 2015-09-05 15:39:15
SWAG ? Silly Worthless Average Guess ?
8MB is no problem for anybody running a node in a data center-- e.g. ChunkHost will rent you a 2GB RAM  / 1 CPU cores / 6TB Transfer/month for just $9 per month / 30GB SSD storage (you'll have to run with a pruned chain). 
Note: if you're operating a node that stores or handles other people's bitcoins you should NOT be running a VPS-- you need a secure dedicated server. Any secure dedicated server can easily handle 8MB blocks (and will have hundreds or thousands of gigabytes of disk space, so can easily store the entire chain, too).
If you're operating a node from home, CPU and disk won't be a problem even if you want to store the entire blockchain. Bandwidth might. The easy fix is to limit the number of connections you make (run with -maxconnections=16, for example); there's also work happening to Bitcoin Core to let you limit how much bandwidth is used.
@_date: 2015-09-07 13:33:08
I think it's because I unsubscribed from @_date: 2015-09-11 01:08:59
yep. Patches welcome.
@_date: 2015-09-06 11:24:28
yep, your goal 3 is not attainable unless the hard fork includes a change to the way miners are paid tx feed. But that goes against KISS....
@_date: 2015-09-04 01:42:57
Sure: 
That's out of date (several things on the roadmap are already implemented, and work on UTXO commitments has stopped), but it's a clear plan.
@_date: 2015-09-04 03:53:13
i have pointed out the efficiency of gossip networks to you many times before. Please stop spreading misinformation about how the p2p network works.
@_date: 2015-04-07 11:13:03
Foundation paid me $150k per year until November, when it was cut to $125. Cost to the Foundation was higher than that because of employer-paid payroll taxes and group health insurance, of course. And yes, the tech dev number is... uhhh... 4.5 people's salary plus miscellaneous tech expenses for running the Foundation (server, CloudFlare protection, etc).
@_date: 2015-09-24 12:35:15
The code I wrote for BIP101 includes limits on validation, so it is impossible to create a block that takes a lot of time to validate.
Please stop spreading FUD, it doesn't help the debate.
@_date: 2015-04-09 13:08:59
No, it is not possible.
Well, it is not possible unless you are the issuer of both the token and the national currency.
Here is why it is not possible:
Start with a token that is supposed to be pegged to $1 per token.
Lets say there's a huge demand for the token, so a trillion tokens are issued at $1 a piece.
Then, for whatever reason ("animal spirits" maybe), there is a huge demand for dollars instead of tokens. Who pays $1trillion to maintain the peg?
There are all sorts of complicated financial schemes involving futures contracts or derivatives that, in my humble opinion, just confuse the issue. The fundamental questions are: who is making money by issuing the tokens? And what are the risks of buying or holding the tokens?
@_date: 2015-09-10 12:33:30
why so negative all the time?
We will accomplish more if we all stay positive and help and encourage each other.
@_date: 2015-09-07 14:37:06
If they want a head start (and they have enough hash power) they can try to selfish mine; you don't need bigger blocks.
If/when fees become more important, miners who are able to propagate bigger blocks faster would get a small advantage.... except that isn't true even today because miners connect to Matt's fast relay network so even huge blocks propagate very quickly.
@_date: 2015-09-05 16:25:41
I think it is fine if home users switch from running a full node to an SPV solution.
They are already doing that, because SPV wallets are more convenient, have more features, and don't have to be connected 24 hours a day. And because home PCs are quickly being replaced by smartphones and tablets.
I also think it should be POSSIBLE to run a full node from most homes. Worldwide average connection speed is 3.9 Mbit/s (see  ). 8 megabytes every ten minutes is 0.1 Mbit/s ... which even somebody in Venezuela or Paraguay or Bolivia with an average connection should be able to keep up with.
@_date: 2015-09-07 22:07:19
Yes, GetBlockTemplate / CreateNewBlock hasn't been optimized... But that's easy to fix.
@_date: 2015-09-03 18:01:40
Unclear where average transaction size will be in a year or three-- Gregory and Pieter at Blockstream are doing fantastic work on more efficient, more powerful signature schemes -- see  ... but they're also working on more confidential transactions :  whose proofs are bigger than normal signatures.
My guess is average transaction size will grow for a couple years, then start shrinking when that work gets deployed. Then maybe grow again if confidential transactions take off....
@_date: 2015-04-13 18:11:25
I'm about to get on an airplane to London...
... but two points:
1) Apply the same reasoning to a 1MB block-- is there any reason to think that a 1MB block is the perfect size that will result in a secure network?
2) Their model is too simplistic, assuming that the people most interested in having a secure network (bitcoin-accepting merchants) are not mining themselves to make sure they have a secure network.
@_date: 2015-09-07 14:05:23
 Because discussion of some important matters is being repressed, over the clear wishes of a majority of participants.
@_date: 2015-04-10 19:32:10
Kinda mostly reasonable... but why is  in the list? That's a handle I don't recognize.
The tricky bit is somebody like gmaxwell, who doesn't write a lot of code but does a LOT of code review and contributes in lots of other ways.
The other tricky bit is we've had other "pay for github activity" tools, and
a) they are not a significant source of funds for developers
b) it is really easy to incentivize bad behavior (like making lots of trivial changes)
I'm all for experiments for how to reward people doing good work, but I'm not optimistic that ANY radically decentralized model can replace the tried-and-true models for funding open source developers.
@_date: 2015-09-05 15:42:56
... I should have mentioned: mining pool operators fit into the "handles other people's money" category, so they should be on a secure dedicated server. If you've invested a lot of money in mining hardware and are solo mining, you should also invest in a secure dedicated server.
@_date: 2015-04-07 10:58:17
'Leaked' is the wrong word to use when the executive director publishes a document on the member forums: 
@_date: 2015-09-13 10:48:30
so if I write code that makes 8MB blocks propagate as fast as 0.2MB blocks you'll support increasing the cap?
@_date: 2015-04-07 00:57:24
Somebody did. No, the local authorities have bigger things to worry about.
And no.
PS: somebody could also store illegal content in web cookies, so every time you merely visit a website illegal content is stored on your disk. I'm sure if you use your imagination you can figure out other ways illegal content might get onto your disk... (store illegal data in the EXIF part of JPG files posted on Facebook).....
@_date: 2016-10-26 12:10:07
Yes, completely unwilling to compromise. I should know, I tried compromising for well over a year.
@_date: 2015-11-03 12:39:59
I still contribute to Core. My latest was a framework for benchmarking, because I'll be doing a lot of optimization work soon.
@_date: 2015-11-25 18:53:38
What about them?  Why don't we see restaurants selling food at a loss to drive their competitors out of business so they have a monopoly?
Or is the end goal for these monopolizing miners a 51% attack?  If it is, how would that work exactly?
@_date: 2015-11-25 18:50:13
There is no requirement to store each block for perpetuity. Why do you think there is?
@_date: 2015-11-20 12:31:54
You can absolutely run BIP101 with the data caps. Until the networking code is optimized you will just have to set some options to minimize bandwidth use. Current Bitcoin Core defaults assume first-world bandwidth, not Comcast-monopoly-we-don't-care-about-our-customers bandwidth.
@_date: 2015-11-15 15:03:20
Imagine we live in Voluntopia. There are no governments, everybody is free to interact, to put whatever they like into their bodies, etc.
There would still be law enforcement, of course, it would just be private law enforcement instead of government law enforcement.
If we lived in such a world, would it be OK to talk with law enforcement about how criminals use Bitcoin or the block chain to defraud or extort people?
I'm pragmatic; I want to see Bitcoin succeed, and I want to help law enforcement catch people who are harming other people. Yes, I know government law enforcement does a lot of OTHER things that have nothing to do with bad people hurting good people; I hope that changes one day.
Reasonable people can disagree about whether it is better long-term strategy to engage with people you disagree with or disengage. Maybe if I'd studied history or politics instead of computer science I'd believe disengagement was the best strategy; maybe somebody who HAS studied history or politics can chime in....
@_date: 2015-11-12 15:31:58
... so to solve the problem we should decrease the block size?  Or do you have some other solution in mind?
I propose that "we" make propagating big blocks as fast as small blocks. See my latest blog post for why I believe we should "design ahead" in the protocol: 
If you'd like to help, there is plenty to do. Participate in the -testnet tests that jtoomim is running, or help test Mike's "thin blocks" patch.
If you're not willing to help, the phrase "lead, follow, or get out of the way" comes to mind....
@_date: 2015-11-10 17:09:12
IF you have 30% or more hashpower...
... and IF you are willing to sustain the attack and tolerate a much higher orphan rate and lower profits until the difficult changes...
... and IF you can control network connectivity so you're sure just 30% of hashpower gets your bigger block...
... and IF you assume that no other miner is or will try to pull off the same attack...
... then maybe this would work.
That is a lot of ifs.
And if the first two conditions are true, then you could just try to selfish mine with ordinary size blocks. The fact that we don't see selfish mining means either miners can't, or they won't because they take into account meta-incentives of wanting Bitcoin to succeed so they're mining hardware doesn't become worthless.
@_date: 2015-03-19 17:57:11
Nice, that's what I ride, too!
@_date: 2015-11-10 20:58:06
Do you agree that there would be ~no problem if everybody outside the GFW started doing what the miners inside the GFW are doing (SPV mining on top of block headers)?
If you agree with that, then the problem is SPV mining, NOT the size of blocks.
And the solution is to get blocks propagating as quickly as block headers (which Matt's relay network accomplishes, and which can be improved with even smarter block propagation techniques).
@_date: 2015-03-23 11:47:02
yes. See  for example, for a non-vapor ware implementation.
@_date: 2015-11-25 00:24:35
Really good discussion.
I wish Christian had mentioned that a miner would be stupid to create a block that took a long time to propagate to other miners-- he made the mistake of confounding the max block size with the size miners are actually willing to produce.
(and yes, yes, non-economically-rational attacker with lots of hashpower could mine huge blocks to try to disrupt the network... except a non-economically-rational miners with lots of hashpower has lots of ways of disrupting the network, and in general a non-economically-rational attacker would be better off spending their money on other attacks... the kinds of non-economically-rational nuisance attacks we DO see, like DDoS attacks against nodes)
@_date: 2015-11-10 21:06:15
So if you're worried about selfish mining, then we need techniques to mitigate selfish mining attacks.
I've been having an email conversation with Ittay and Gun about that... I think a protocol that pre-publishes 'weak' blocks might accomplish that, because when you're finding POW you're committing to real-time timestamps in the block headers.
If you start selfish mining, then you have to stop pre-announcing intermediate work (weak blocks) that are more than one block ahead of the network's idea of the chain tip. And when it comes time to announce your 'surprise, I'm ahead' secret chain to the rest of the network, your peers have evidence (in the form of timestamps on the blocks you announce and the LACK of any intermediate weak blocks) that you're cheating.
What your peers can safely DO with that evidence is something that needs a lot more careful thought....
@_date: 2015-11-25 03:48:01
See  for why I think 'designing ahead' is the right thing to do.
@_date: 2015-03-16 17:04:34
Maybe related: it is economically rational to NOT use all of your hashing power to mine for yourself, but to use some of it to attack big pools:
 or 
That would fit the rise in "other" hashing power (keep your hashing private so you can't be attacked) and the decline of the biggest pools-- have the biggest pools had an unexplained increase of 'bad luck' since they've been declining?
@_date: 2015-03-09 03:40:31
Because I don't let the perfect be the enemy of the good.
@_date: 2016-05-27 20:10:57
One could design a hardfork that was incompatible with old transactions.
But that would be a really bad idea, not just because of timelocked transactions, but because you would obsolete hardware wallets and every single transaction-generating piece of software.
I will argue really loudly against any hardfork that made 'non-attack' old transactions invalid.  (I think it would be fine to obsolete old transactions designed only to cause problems or that use edge-case behavior like SIGHASH_SINGLE weirdness).
@_date: 2016-05-28 01:30:28
My Gox account was under 'Andresen' and I made no claim.
@_date: 2015-03-09 11:28:57
.... And start at zero on the Y axis.
@_date: 2015-03-09 03:29:15
No, you are just wrong. If exchanges or merchants or full-node-running users reject their chain, then the bitcoins they produce are worthless, no matter how much hash power they have.
@_date: 2015-03-20 12:29:37
Use importwallet, which had a simple ascii format and let's you specify a time/date
@_date: 2017-05-26 19:03:13
We all know what the OP means:
market cap == total units created * current price per unit
Wait, I said that wrong:
WE ALL KNOW WHAT THE OP MEANS....
@_date: 2015-10-05 15:16:05
A full node knows consensus is broken when they see a lot of hashpower continuing to work on a chain that they consider invalid.
The reference implementation already has checks for that, and will warn you something bad is happening (even if it is on the most-work, winning chain).
30 different branches is silly, the incentives to be consensus-compatible with the most work chain are incredibly strong.
libconsensus is a fine idea, although the software engineering towards getting a libconsensus has been really bad, in my humble opinion. In hindsight, I think starting a separate libconsensus project that could evolve and mature on its own separate from Bitcoin Core would have been the better approach. I don't think Jorge is the right person to lead a libconsensus effort-- I don't think he's ever shipped a C or C++ API/library before (at least, I don't see anything on [his resume]( that would lead me to hire him to tackle that task).
The libconsensus changes have been extremely disruptive to day-to-day pull requests, and have made working on Core a lot more painful than it should be.
@_date: 2015-10-28 12:26:40
No, it usually doesn't work. Sometimes because if the child doesn't have enough fee to get relayed to miners, broadcasting the parent won't help (it won't be relayed or mined because the child is forgotten).
And the rest of the time because almost no miners implement cpfp code.
@_date: 2015-03-06 22:34:18
Wait... What?
The biggest scale technological systems we have are the least centralized-- our road systems, the Internet, the international container shipping system....
@_date: 2015-11-15 19:41:29
What about gay marriage or marijuana decriminalization?
Can you point to some issue where 'not playing nice' worked? The non-violent resistance of the civil rights movement in the US kinda-sorta fits, but ultimately it was playing nice with the federal government that brought change.
@_date: 2015-03-10 14:51:05
This is the process of reaching consensus-- public and private discussion and debate. I don't want to wait until there is a crisis or disaster to have the debate.
@_date: 2015-03-21 21:30:47
It seems to me the pattern is:
Places with either weak or over-controlling governments have lots of grey/black market activity for ordinary, everyday business.
But in rich countries the only big businesses that operate in the grey/black market are illegal businesses. And there seems to be a limit to how large an illegal business can get (or how long it can stay in business) before it falls apart because it gets infiltrated by law enforcement.
Small businesses might also operate with two sets of books (one to show the taxman), but once they get big enough that becomes impossible (too much risk if your bookkeeper is not you or your spouse).
I don't think Bitcoin or ZeroCash will change that pattern.
@_date: 2015-03-25 12:14:53
If the blockchain became so big no single entity could store it all (that won't happen, storage is cheap, but let's pretend).... That would be wonderful for transaction privacy, fungibility, etc.
Besides block explorers, why would anybody need to store the whole chain?
@_date: 2015-03-11 13:40:23
i block and ignore anybody who uses foul language in tweets that reference me.
There are plenty of smart people who know how to engage in a rational, reasonable conversation; life is too short to pay attention to the people who don't .
@_date: 2015-03-10 15:23:09
They forgot to change the P2SH prefix byte for litecoin?
That's not good, it means a copy/paste error could lock up BTC or LTC in a way that is very difficult or impossible to spend.
@_date: 2015-03-25 12:05:30
Bah... 'Fit the legal definition....'
@_date: 2015-10-02 14:15:22
+1.  Diversity is good.
@_date: 2015-03-10 14:09:55
Wanna bet?
There is not yet consensus on exactly how and when to increase max block size, but there is strong consensus it must rise.
@_date: 2015-11-26 15:39:49
I said BIP101 was the only proposal with well-tested code people can download and run.
Pseudo-code or untested patches that ignore the O(n^2) SIGHASH_ALL vulnerability don't fit my 'ready to go' criteria.
Anyway, I really and truly hope consensus emerges from Hong Kong; that would be the least disruptive path forward.
@_date: 2015-03-09 11:59:28
Mining with a 'compromised' hash function is not a problem, for all reasonable definitions of 'compromised'-- so there would be no reason to change.
Bitcoin uses double-SHA256 for other things, and we would probably change those to use another hash function. I say 'probably' because if we replaced all SHA256 in Bitcoin with the old, weak, compromised MD5 algorithm, Bitcoin would still be secure today against all known MD5 attacks (because Bitcoin tends to hash everything twice).
@_date: 2015-10-19 01:52:01
This has been tried before: 
Maybe it'll work this time, but I bet buyers putting up double the coin is really hard for people to accept.
@_date: 2015-03-20 12:21:56
But character matters. 'This person is (blah)' can be useful information, ESPECIALLY when they are hiring themselves out as a consultant.
Replace (blah) with character traits to see what I mean (honest, dishonest, easy/hard to work with, productive, lazy, etc).
@_date: 2015-10-24 11:23:05
I was with you until the end of your comment.
Where have I ever said the protocol should evolve so ANYBODY needs ANY sort of permission to submit transactions?
@_date: 2015-10-23 12:39:52
Really? No influence?
You don't think congress-people, state legislators, or governors listen to the concerns of law enforcement and consider them when deciding to allocate budgets or prioritizing a legislative agenda?
I don't think law enforcement has control, but I think they have a lot of influence (not to mention quite a lot of .... Uhhhh... 'discretion'....  on which laws they choose to enforce vigorously).
@_date: 2015-03-25 12:04:45
 Because we are not lawyers, and most of us try not to talk about things we know little about (like whether or not eleven miners in six different legal jurisdictions who have never all met or communicated for the legal definition of 'custodian').
@_date: 2015-03-09 17:58:30
My 2-year old Mac can verify about 1,100 transactions per second, and my home Internet connection could handle about 400 megabytes of transactions per ten minutes.
I have started benchmarking international latency and bandwidth, and it looks like THAT will be the limiting factor, not bandwidth or cpu to your home.
@_date: 2015-03-06 13:15:55
The absentee miners getting forked off problem wouldn't have been solved if P2SH used a NOP; both DUP HASH160 EQUAL  and   NOP1 DUP HASH160 EQUAL  were non-standard.
The problem was triggered by an IsStandard() script that SPENT an invalid-under-new-rules P2SH input.
My life would've been easier if P2SH outputs were one byte bigger, too.
As for more complex handling: I think if we had used a NOP we'd have another source of malleability -- which opcode used to push the 20-byte hash. Making it "must be exactly 23 bytes" was the right decision; adding a NOP that meant "kinda sorta do the OP_EVAL thing" would open up all sorts of cans of worms.
@_date: 2015-02-14 14:19:50
Hmmm.... Looks like  paid to keep his fake follower percentage private...
@_date: 2015-03-02 11:55:15
you need to fix the process-- the real experts are too busy being productive to "go through the process" when that process involved an unnecessary edit war.
@_date: 2016-05-23 11:28:10
Core's problem is bad priorities-- 'rearranging the deck chairs on the Titanic' syndrome.
@_date: 2015-03-23 11:38:53
See  for a good discussion.
@_date: 2015-03-20 12:15:31
You are right-- We're OK as long as SPV clients and new full nodes can easily find eight peers with open incoming connection slots.
The only time that was a worry was during a period of rapid growth before the code had UPNP support to open up a firewall port for incoming connections.
More is theoretically better to fight Sybil attacks.... But Sybil attacks against Bitcoin clients are 'meh' -- attacker can keep you disconnected from the network and/or can keep your transactions from being broadcast but that's it (they are a much bigger potential problem for miners, but miners have stable long-lived connections).
@_date: 2015-03-25 12:24:01
Mining is a zero-sum game-- you are competing for a fixed reward.
Don't play zero-sum games unless you have some advantage over the other players.
I see absolutely no way cloud hashing could give you any such advantage.
@_date: 2015-03-10 14:47:43
@_date: 2015-10-05 13:42:13
I was sick on Thursday.
@_date: 2015-03-29 14:32:41
I shouldn't have tweeted that, I was having a very bad day yesterday.
@_date: 2015-03-19 16:44:14
Wonder if he's gonna use one of these:
  
 (they accept bitcoin....)
@_date: 2015-02-05 14:22:55
yes, what kind of tweaking?
@_date: 2015-02-05 15:25:55
what huge problems?
@_date: 2015-02-05 23:50:28
The data I'm working from:  testing with my 3-year-old Mac, assuming a "pretty good" broadband Internet connection (300GB per month data cap), and assuming only half the bandwidth is used for Bitcoin.
And going with historical estimates of growth in bandwidth, storage and CPU (bandwidth is the bottleneck, growing 50% per year).
I'm happy to implement another method of implementing the fork-- picking a block number or date N months out would be just fine. I think it still makes sense to ask miners to produce block.version=4 blocks, because that will trigger automatic warnings asking people to upgrade.
@_date: 2015-02-27 13:05:52
Headers-first is Pieter Wuille's work. It speeds up initial chain download times, but doesn't make block propagation faster.
@_date: 2015-03-20 11:57:33
i haven't ridden my unicycle since last fall, and rode just a few times last year. Maybe when the ice and snow go away I'll start riding to and from my office.
@_date: 2015-02-10 01:26:18
Why not? There is no reason besides history and prior technological limitations why we don't just directly use fed payment systems. See 
@_date: 2015-03-06 13:00:48
Good idea. You should code it up and measure CPU/bandwidth usage to see if it is practical.
@_date: 2015-02-10 12:57:41
yes, with IBLT's transaction data only needs to be broadcast once, when the transactions are announced, so new block announcements are tiny.
@_date: 2015-02-14 14:10:37
 says 17% of my followers are 'fake' -- it would be interesting to know if those are bots who follow me or just users who got tired of Twitter after following one or two people...
(I have never done anything to try to increase the number of people following me besides publishing my  handle).
@_date: 2015-02-10 01:31:37
mmmmmmm........ Bacon......
@_date: 2015-02-09 17:30:09
Don't be ridiculous, what was done initially CERTAINLY matters. The 21million coin limit is the most obvious example of an arbitrary decision that must not be changed.
Scaling up to handle very large transaction volumes is a promise that needs to be kept, and there is no reason (beside what I think is irrational FUD) not to.
@_date: 2015-02-27 13:01:30
No, one (important) little bit is faster and uses a lot less memory.
It is important because it is the code that gets run when a computer gets a new block full of transactions, and it has to be run before the computer will tell other computers about the new block.
@_date: 2015-02-09 19:43:43
No, that is not possible. I won't suggest a hard fork unless I am convinced all major miners and merchants and exchanges will agree.
@_date: 2015-02-28 23:28:49
But do the positives of his contributions make up for the negatives of the drama he causes?
I used to think the negatives outweighed the positives, changed my mind, but he seems to be needlessly stirring up trouble again.
@_date: 2015-02-10 13:16:35
you are confusing the size of blocks with the maximum possible size they are allowed to be.
I am proposing raising the max allowed size. The actual size will, indeed, rise or fall incrementally as transaction volume changes and miners decide what block size they think makes sense.
@_date: 2015-02-14 14:21:27
... But OUR petertodd  is legit. Who is that other Peter Todd?
@_date: 2015-02-13 05:43:16
no; a Merkle branch to a root hash in a block header is just as secure for proof of existence.
@_date: 2015-02-10 01:40:47
Off-chain transactions will take some pressure off while network bandwidth scales up to 'everybody can make a dozen transactions per day,' at the cost of inconvenience and increased centralization.
Right now, the problem is convincing people Bitcoin is a good way to pay for things-- and convince them to earn bitcoin directly. I believe having a clear plan for the future will help reassure people, and increasing the max block size is the simplest, easiest to test and least disruptive way of accomplishing that.
@_date: 2015-02-19 15:49:22
I think keeping track of some statistics about the behavior of your peers and dropping peers that are acting "weird" (and banning them if they insist on reconnecting) is a great idea.
"patches welcome"
@_date: 2015-02-10 17:17:42
The Tor project has stats. E.g. see:
  
... which implies it would take anywhere from 40 seconds to two minutes to download 20MB.
So WITHOUT ANY OPTIMIZATIONS AT ALL, that is too slow to mine-- you'll have a high orphan rate if it takes 1 or 2 minutes for you to find out about a new 20MB block. Although if the entire network was forced to work through Tor that wouldn't matter, since everybody would have higher orphan rates.
Once block propagation is optimized not to retransmit all transaction data, the orphan problem goes away, and yes, should be perfectly reasonable to run a full node in the Tor network.
@_date: 2015-02-15 14:38:41
Whoever does the best job convincing merchants, consumers, exchanges and miners to run their software.
I expect sooner or later that will NOT be the Bitcoin Core reference implementation, and that is OK.
@_date: 2015-02-14 14:03:13
i have never been on the TWIT podcast so I don't think I can claim to be a twit.. But I was on TWIST and Triangulation.....
@_date: 2015-02-17 03:02:49
The security card gives almost the same security as a screen, at much lower cost. I think that is the right trade off, especially since the ledger is more likely to survive a trip through the laundry.
@_date: 2016-07-05 13:33:33
No, they will certainly tell me what they think.
Most successful Bitcoin companies are headed by former geeks who are very technically competent and very smart.
@_date: 2015-02-24 14:24:24
"They've convinced the developers and the owners of the largest exchanges to go along with this."
I can't speak for the big exchanges, but the current developers would be absolutely, positively against this.
See  for some other things we would be absolutely, positively, 100% against changing.
@_date: 2015-02-15 14:15:29
Pathetic? No, this is how open source, decentralized development works.
I think Bitcoin-XT is great, and running a couple of XT nodes myself is on my TODO list.
@_date: 2015-02-09 19:42:00
really? I don't remember that; it is just as much a problem at 1mb as 11mb max block size....
@_date: 2015-10-29 00:05:34
We have a few more block halvings to see if this is actually a problem.
I think big companies that rely on the network will mine themselves, at zero or temporarily-negative profit so their business doesn't fail.
And who knows, maybe by then there will be a rock-solid distributed consensus algorithm that can replace mining proof-of-work. Once POW has distributed essentially all the coins, IF there is a technically sound alternative, why not switch?
But that's a debate to have 20 years from now.
@_date: 2015-02-17 17:36:54
Nice! I like the design, seems simple enough to work.
You should experiment with using unspent bitcoins to  "buy" space in the DHT (e.g. you get to store 11 megabytes in the DHT as long as you keep an 11 BTC output unspent... where it really isn't eleven but some clever market-based mechanism where people bid unspent coins for the right to take up space in the DHT).
@_date: 2016-07-21 12:53:06
Would a post about 'why the ethereum hard fork demonstrates controversial hardforks are not an existential threat' be OK?
I won't waste my time posting such a thing here (I'll post it other places) if that particular discussion about Bitcoin is not allowed here.
@_date: 2016-07-10 13:10:25
I don't recommend core as a wallet for most users because its disadvantages (no multisig, no foolproof backup via HD seed, extra bandwidth and disk space usage) far outweigh the benefits you list.
@_date: 2015-11-04 18:04:24
Please be more specific-- what attack are you worried about?
And can you respond to my emails where I have been asking you or anybody else at block stream to engage on a validation cost metric exactly so that the worst case validation costs are not an issue?
@_date: 2015-02-10 01:27:28
Why not?
@_date: 2015-02-05 23:45:57
Miners, merchants and exchanges will be warned well in advance of the possibility of a bigger block. The reference implementation automatically warns you if it sees a majority of up-version blocks being produced.
And a few months before the fork was scheduled alert messages would go out to old versions.
Most wallet users won't have to do anything, because most wallet users are not downloading the whole blockchain.
@_date: 2015-02-24 14:02:33
There was a similar paper at the IFCA'15 Bitcoin workshop: 
During the Q&amp;A for that paper a clear consensus emerged that it would make more sense to use the Bitcoin p2p network for botnet command and control, but it really doesn't make sense to store either commands or payloads in the blockchain. Mostly because it is a really bad idea to leave a permanent public record of your criminal activity, but also because it is expensive.
@_date: 2016-07-10 12:22:14
There are much better wallets than Bitcoin Core these days that give you complete control over your bitcoin (so let you 'be your own bank').
The only people who should be running fully synchronizing nodes are miners and businesses and uber-geeks like theymos who enjoy fiddling with technology.
@_date: 2013-10-27 07:57:16
First: don't use a brainwallet, we are all MUCH worse at being unique than we think.
Second, if you do create a brainwallet, create two, a short one you think nobody will guess in a million years and a long one that is the short one plus your mobile phone number.
Put a small amount of Bitcoin in the short one, and the bulk in the long one.
When the short one is hacked, you know it is time to move the coins you stored in the long one.
@_date: 2013-05-06 19:03:39
Nope. I wrote a gist estimating the marginal cost to a miner of including another transaction in their block, and it is well above 1 Satoshi.
@_date: 2013-02-01 02:08:50
Did I really say "governments will be the same?"  I meant to say "governments will adapt."
Again, I could very easily be wrong, but I think the Internet and Bitcoin won't change government as much as we might hope.
@_date: 2013-12-12 11:27:30
I dunno, but if Bitcoin starts to get broken we'll fix it.
@_date: 2013-10-28 05:00:06
Yes, and I started it by buying 10,000 BTC for $50.
@_date: 2016-07-04 12:34:49
Small blocks are hurting adoption; every single successful Bitcoin-transacting company I talk to agrees with that.
@_date: 2013-10-26 05:07:10
More workable with OP_RETURN allowing 80 bytes to be attached to transactions-- you'd probably want to put Alice's public key (or a hash of it) and... oh, I dunno, maybe a short URL from which Alice can fetch the full, encrypted message. Or maybe a Freenet content hash key.
Practical: I haven't seen any really successful systems that require users to pre-exchange keys, that seems to be "just too hard" for non-geeks to do.
@_date: 2015-02-10 18:19:11
It'd be fine with me if there was an additional rule of something like "blocks may not be any larger than X% larger than the average size of the last Y blocks" to avoid "shocks."
I just like things to be as simple as possible, because complexity is the enemy of security. And I don't see any way the sudden announcement of a much larger block would "shock" the network --
peeking at the front page of blockchain.info right now, I see largest block of 730K and smallest nearly three times smaller at 268K.
@_date: 2015-02-09 17:26:58
O(1) block propagation doesn't require a fork of any kind, it is just communication optimization.
It will almost certainly be implemented first as a separate peer-to-peer communication mechanism, with "gateway" nodes relaying blocks and transactions between the O(1) propagation network and the inefficient p2p network we have now.
At least, that is how I'm planning on implementing it, as soon as I'm sure there is a consensus on how to raise the block size (implementing O(1) block propagation will be a waste of time if we never have larger blocks).
(I don't think sidechains just for more transaction volume will work; I don't think they have a big enough advantage over completely centralized solutions like "Lets just clear all transactions through Coinbase and BitPay" to get adopted).
RE: your alt hardfork being painful: is there a post-mortem somewhere I can read to learn from what you done did?
@_date: 2013-08-19 07:29:04
I'm not panicking. Just doing the thought experiment: "What if there ARE lots of widely accepted *coins in the future?"  Because that is the world that the alt-coin promoters seem to be promoting.
I don't think we'll get there-- I think the Network Effect for money is huge, and I think the first viable, truly worldwide currency (aka Bitcoin) has the best chance of replacing all of our current one-physical-location-in-the-world currencies.
@_date: 2013-11-29 23:55:02
Peter Todd doesn't even believe that any more; see his "merkle mountain range" ideas for how to scale up.
Maybe I don't communicate clearly enough, but much of the fear, uncertainty, and doubt about the block size issue comes from people like Peter who want to know exactly how technical problems that MIGHT come up three years from now will be solved TODAY.
I find that strange, because much of the FUD about Bitcoin itself comes from people who want to know exactly how problems Bitcoin MIGHT create three years from now will be solved TODAY.
I claim we can't know, but as long as the incentives are correct smart people will figure it out.
@_date: 2015-02-10 13:07:05
We already validate using multiple threads. And CPU power is scaling up even faster than bandwidth; we will have dozens of processors with built-in support for SHA256 to make validation much faster.
Database access won't be an issue, unless there is a non-linear increase in the size of the UTXO (unspent transaction output) database. We don't need the whole chain to validate, just the UTXO. And I don't see any reason why the UTXO would explode super-linearly with more growth (if it does, same issues would happen with 1mb blocks).
@_date: 2013-03-24 16:56:00
It is a wiki... it is not The Word of Bitcoin.
I removed the Pyramid scheme section (why didn't you if you disagreed with it?), but I expect that edit to be reverted as soon as Luke-Jr notices...
@_date: 2013-05-06 01:04:05
"told by the foundation lawyers?"
RE: anonymity: I'd love to see a completely anonymous transaction system adopted for Bitcoin, but it has to be practical. An "eleven transactions per minute on consumer-level hardware" academic proof of concept isn't good enough (I dunno how fast ZeroCoin is right now, but I understand it isn't fast enough)
@_date: 2013-05-29 20:19:57
I estimate there is a 99.811% chance proof-of-work will last through the year.
Plus or minus 0.003%.
@_date: 2013-11-09 21:09:22
Sigh; Milly is confused because mtgox incorporated a legal entity with the same name in an aborted attempt to start a Foundation.
The current Foundation is a bit over 1 year old.
@_date: 2013-09-15 00:37:18
Woulda been better if it was 111 million.
@_date: 2013-10-26 22:04:57
No, there is only one transaction, broadcast twice (maybe-- Coinbase's code might notice it was already broadcast and not bother sending it again).
@_date: 2013-05-17 13:46:09
Are we really going to decide technical issues like the maximum block size using cutsey videos full of misinformation like "big blocks means miners cannot be anonymous" or "big blocks means mining pools go out of business?"
@_date: 2013-11-29 23:49:42
The block size must rise, but there are a couple of technical things that have to get done first:
1) Fees have to float. That's what I've been working on.
2) Broadcasting large blocks has to get more efficient. We know how, "we" just have to write the code ("use full bloom filters").
Then "we" will have to convince people that increasing the block size won't Doom Bitcoin To A Downward Spiral of Transaction Fees (just ain't true) or result in 100% Of Hashing Power in a Single Server Closet!
@_date: 2013-05-31 11:29:40
If you have a Bitcoin-related business (or are starting one), talk to your politicians, with the message "I am creating jobs and generating tax revenue using an awesome new technology called Bitcoin."
(Unless you are the 'all government is evil' type; you should stay home, because telling a politician they are devil-spawn is rarely productive).
@_date: 2013-11-17 21:23:32
Yes, way ahead of you.
@_date: 2013-11-03 02:01:54
I'd advise:
1) Decide how many you want to keep and how many you want to cash out.
2) Create a paper wallet, and send the "I want to keep" coins there.
3) Create an account at an exchange. Get verified/etc.
4) Transfer 50 bitcoins to the exchange, sell them, have the money transferred to your bank account.
Repeat step 4 as money flows into your bank account. If you have more than 50 XBT then I wouldn't advise transferring it all at once, there is too much risk the exchange might get hacked, have its bank account closed, etc.
5) Be sure to report the money going into your bank account to the Appropriate Taxing Authorities (e.g. in the US, file a quarterly estimated tax payment with the coin sales counted as capital gains).
Yes, you WANT to pay taxes on that income. Keep the USB stick, it might be very handy if your Taxing Authority or bank wants proof that you got the coins via a lucky investment that paid off big-time and not some illegal activity that you're trying to money-launder.
(most people think money-laundering is about hiding income; it is not, it is all about finding ways to pay taxes on illegal income so it looks legitimate).
If we're talking about a lot of money, talk to a tax attorney about the laws in your country. You want to avoid "Daddy left me a million dollars paper wallet that I can't spend because if I do I'll be thrown in jail as a tax evader."
@_date: 2013-06-29 00:55:02
The bank is following 'know your customer' laws. You don't have to answer, but you also do not have a right to a bank account, and if you refuse to tell them your business you may find yourself 'unbanked.'
I don't think the EFF tackles financial privacy issues; financialprivacy.org does.
@_date: 2013-05-06 15:32:07
Okey dokey. So how would you fix the problem of newbies who get a couple satoshis from advertising-supported Faucet sites and then find out they wasted hours of their lives solving CAPTCHAs only to get a wallet full of bit-dust they can't spend?
PS: If you're going to talk about what the 'original client' did, you should get your facts straight. Otherwise you just look misinformed.
@_date: 2013-11-27 09:47:12
Fees are likely to go up as transaction volume rises, until there are some technical innovations to make transmitting blocks more efficient.
@_date: 2013-09-23 02:59:25
It solve _a_ random number generation issue.
You still need good random numbers when creating your wallet key(s).
@_date: 2013-12-26 19:08:48
Sounds like you are risk-averse and should NOT get on the Bitcoin roller-coaster train right now.
@_date: 2013-11-04 02:49:21
Initial comments after a quick read: they seems to be ignoring network propagation delays and the "orphan cost" of not publishing a block as soon as it is found.
edit:  ah, they propose combining selfish mining with a Sybil attack to get around the orphan cost problem. Anybody done the math on how much of the network they'd have to control to significantly slow down block propagation?
There is also an intuitive flaw in their reasoning:  Imagine there are 4 sets of miners, each with 25% of hashing power, all pursuing the "Selfish Mining" strategy.
What happens?
@_date: 2013-05-06 15:37:29
Your service is a disservice to whoever you are sending dust, because they cannot spend it without spending more in fees than it is worth.
Or, in other words: there is a hidden cost on your users and the rest of the network.
You should change your service to send out larger amounts of bitcoins to each user.
@_date: 2013-05-21 04:46:42
I bet a lot of people would love to get paid partly in Bitcoin, and since employers will need to convert BTC to (local currency) to pay employment taxes anyway, that should be pretty easy.
And I think more people than you think would choose to get paid ALL in BTC...  (maybe I'm biased, since I'm paid 100% in BTC).
@_date: 2013-04-06 15:08:34
Talk to an accountant and maybe a lawyer.
@_date: 2013-09-07 23:08:52
"We" are writing code that sets fees based on miner's transaction acceptance policies.
So: miners will decide (as they do today) which transactions they will include in blocks. And fees will rise or fall based on the number of transactions and the network's ability to transmit and process blocks and transactions. "We" are also working on scaling up the network to handle more transactions, which will decrease fees.
@_date: 2013-11-15 10:20:02
To get "pretty good bitcoin privacy" you need three things:
1) A private Internet connection. You need to run over Tor or i2p.
2) CoinJoin or something similar, so it is harder to track your transactions.
3) Paranoia and knowledge, so you don't screw up and just tell somebody (an exchange, a merchant) who you are.
None of those things require any changes to the core protocol.
@_date: 2013-03-01 18:22:26
I think hazek can't see the forest for the trees; he has to deal with all the drama because he's a bitcointalk moderator. Drama on the interwebs does not equal an oh-my-god-the-world-will-end problem (it mostly correlates to "this is simple enough for me to understand and have an opinion about").
The block size limit will be raised sooner or later, there is solid consensus on that. It is just a question of when and how.
@_date: 2013-11-17 12:28:56
3) you thought this would be an amusing way of disposing of a sock-puppet.
Gotta consider ALL the possibilities, right?
@_date: 2013-08-01 10:24:40
The reference (Bitcoin-Qt) implementation broadcasts the payment transaction on the p2p network before also sending a Payment message with the transaction directly to the merchant via https.
So it doesn't have to worry about "what if the merchant loses the transaction or something" (and there could be lots of "or somethings" -- maybe the network connection between the merchant and customer goes down at the wrong time, maybe the merchant's web server gets DDoS'ed at the wrong time,etc).
@_date: 2013-04-06 01:23:52
Yes, there are great, careful journalists who take the time to try to get the facts right. They tend to be the long-form reporters, not 2-paragraph print stories or 30-second tv segments.
Some of my favorites who do a great job: NPR planet money, the Economist, the Wall Street Journal, New Yorker magazine.
@_date: 2013-09-12 03:45:33
Put $200 towards the mortgage payment, pay off $200 in principle, and hold $200 until Jan 1 2014.
Seems like this is a case where you can make everybody happy.
@_date: 2013-04-21 11:57:44
If the company gets some of its revenue in BTC then paying some salaries or bonuses in BTC works well-- but until BTC exchange rates get less volatile pegging the salary or bonus to the local currency is the only sane thing to do.
I have been very happy getting paid my salary in bitcoins. Since I don't spend it all right away, receiving a deflating currency is better for me-- it is worth more by the time I do need to spend it.
@_date: 2013-12-11 01:44:01
Payments of less than 5-thousand-something satoshis are still considered dust, so this does NOT open up the market for micro-transactions.
Plain-old transactions might never be affordable for transactions worth less than a cup of coffee, and in the next year or two you should expect low-value transactions to get forced off the blockchain because transaction fees are likely to rise.
I have no idea what will happen in the long run; there might be micro-transaction systems that use Bitcoin as the "settlement currency", or technology and innovation might make transmitted-all-across-the-world Bitcoin transactions inexpensive enough for micro-transactions.  We'll see!
@_date: 2013-12-05 20:55:08
The 0.9 code should better estimate what priority is needed to get transactions confirmed quickly exactly to fix the 'free transaction in limbo' problem.
@_date: 2013-09-15 19:28:31
@_date: 2013-05-31 12:02:40
Sure. In the bitcoin-qt send page, just poke the plus icon an you can create one transaction that pays to multiple amounts/addresses.
@_date: 2013-05-15 23:55:55
Stunning amount of equipment here....
@_date: 2013-04-07 02:04:29
We haven't even begun to make 0-confirmation transactions safer. They will never be 100% safe, but I am confident we can make them as safe as accepting cash (which is not 100% safe because it might be counterfeit).
PS: I don't think bitcoin has to replace fiat to be successful. 
@_date: 2013-11-06 06:19:54
But... but... I was busy actually writing some code, then the price started exploding so reporters started bugging me, then the Cornell research paper thing happened and they bugged me some more...
And I'm trying not to be the Bitcoin Celebrity Spokesmodel. I don't look good enough in a two-piece.
@_date: 2013-10-22 00:45:54
Mmm. If you want my respect, write some code.
@_date: 2013-05-30 12:06:05
It will change dynamically when we teach the code to adjust fees dynamically (next release).
@_date: 2013-11-21 06:30:36
The first one was in Malta.
@_date: 2013-12-07 22:14:15
Most ordinary folks should NOT be running a full node. We need full nodes that are always on, have more than 8 connections (if you have only 8 then you are part of the problem, not part of the solution), and have a high-bandwidth connection to the Internet.
So: if you've got an extra virtual machine with enough memory in a data center, then yes, please, run a full node.
@_date: 2013-09-13 07:04:18
So... what is stopping you from doing a total scan of the source? It is not hard to find...
@_date: 2013-10-22 00:45:00
Don't be lazy-- read that thread. Mike Hearn and etotheipi (lead developers for bitcoinj and Armory, respectively) give the rational side of why the payment protocol relying on the existing certificate authority infrastructure makes sense right now, and why we're not trying to "boil the ocean" and create our own.
@_date: 2013-08-20 02:31:19
The purpose of money laundering is to take income made illegally (e.g. by selling drugs) and turn it into income made legally (e.g. selling hot dogs).
In other words, drug dealers launder their money so they can pay taxes on it, so when they buy a Ferrari with cash the IRS doesn't come knocking on their door asking "where'd you get the money?"
I've never understood how moving money through Bitcoin and other cryptocurrencies helps accomplish that goal.
Maybe you're using a different definition of "money laundering" ?
@_date: 2013-11-22 22:54:38
What is the benefit to consumers: depends on what you are purchasing. Bitcoin is a darn convenient way to split the check if you're at a restaurant with friends.
Will merchant or customer adoption grow fast enough:  that is a loaded question, you've got a hidden assumption-- that merchant/consumer adoption is necessary for Bitcoin to be successful.
There are trillions of dollars worth of transactions that have nothing to do with consumers-- business to business supply chains, investment, foreign exchange, etc. It is very possible Bitcoin will never take off for consumer spending but will still be incredibly successful.
Is there a legitimate threat from competing currencies: not right now, no.  Could be in the future.
Is the lack of "real value" a threat:  doesn't seem to be so far. The longer Bitcoin survives and is proven to be a reliable way of sending value across the Internet, the more people will trust it. And trust is incredibly valuable-- hard to earn, easy to lose. The threat is that something happens that shatters people's trust in Bitcoin.
@_date: 2013-05-05 14:21:50
It was all resolved to both parties satisfaction in the end, and shows the weak point in a 51% double-spend attack-- the entity cheated by the double-spend will almost always know who did it.
@_date: 2013-05-20 07:33:58
They are pretty damn comfortable, soft, and warm.
@_date: 2013-05-04 14:38:46
Yes, I'm 46.  How old did you think I was?
@_date: 2013-08-16 23:07:57
Yes; if you signed two or more transactions involving the same key on an Android device there was a small possibility the private key would be revealed, no matter where that key was generated.
@_date: 2013-04-11 00:02:58
Can't think of a better cause.
+tip .15 BTC verify
@_date: 2013-11-24 02:26:35
My daughter paints my toenails for me.
Y'all don't seem to understand the notion of "decentralized, open source" -- if you see something wrong, then fix it!
bitcoin.org is open source, if you see something that needs fixing, then submit a pull request: 
@_date: 2013-03-31 22:07:25
Has your SecureRandom() been tested for cryptographically secure randomness?
I don't know nuthin about generating random numbers in Javascript, but the code in rng.js looks like it might produce weak random numbers in some browsers.
@_date: 2013-04-01 01:25:11
Mmm.  Looking at the code some more, I definitely don't trust it; it looks to me like it is seeding based on Javascript's Math.random() (which definitely isn't good enough for this use) and the current time (which is WAY too easy to brute-force).
Where does the Javascript library come from originally?
Seeding the rng should use  , and should refuse to run on browsers that don't support that method of getting entropy, in my humble opinion.
@_date: 2013-10-21 23:04:05
Big mining pools are already significantly more profitable than smaller ones, due to fixed costs and economies of scale. And orphan costs...
@_date: 2013-05-06 01:14:56
I'll respond to two reasonable points:
1) WHY YOU NO ANNOUNCE?
So... this change just got pulled into the reference code.  It will be part of the 0.8.2 release.
Which will go through a release process, and then pushed as the default download, etc.
It will take months before a majority of miners or users are running this code. Therefore, if your business is creating tiny outputs, you have months to change.  Instead of creating 1-satoshi outputs that cannot be economically spent, create 54uBTC outputs.
Although if I was a betting man, I'd bet that growth in transaction volume and competition for space in blocks will drive up transaction fees and make create-tiny-output businesses unprofitable first.
2) WHAT'S THE PLAN, MAN?
The code change ties the definition of "not economical to spend" to transaction fees.
And the plan for transaction fees is to let the market decide.  Miners want higher fees, users want lower fees; the pull request gives users more control over the fees they pay. Future pull requests will give users much better advice on what fee they'll need to pay if they want any given transaction to end up in the next block or two.
I would like to let the market decide the block size, too, but that is a whole 'nother debate.
@_date: 2013-10-24 20:49:36
Cross-posted to the bitcoin-development list:
So: there are multiple layers of reasons why OOB fee payments will not screw up the fee estimation code:
+ If the transactions are not broadcast, then they have no effect on the estimates.
+ If the transactions are broadcast but not relayed because their priority and fee are way below current estimates then they will have very close to zero effect on the estimates.
+ If the OOB transaction is zero-fee, zero-priority (e.g comes from a high-tx-volume service and relies on recently spent outputs) it will have zero effect on the estimates.
+ If they make up less than about 40% of broadcast transactions they will have very close to zero effect on the fee estimate (because of the distribution of fees and behavior of taking a median)
The only case where the estimation code is even slightly likely to get confused is estimating the priority needed to get into a block IF there are a significant number of zero-fee, low-but-not-zero-priority OOB transactions being broadcast.
And since priority naturally increases over time, even if that case DOES occur the failure is very mild-- it means your free transactions might have to build up more priority than the code estimates before successfully entering a block.  If that gets to be an actual problem, then implementing Pieter's idea of keeping track of memory pool transactions that are NOT getting mined would fix it. But I don't want to waste time on a theoretical problem when it is very possible miners will decide to stop accepting free transactions alltogether.
AND: the practical problem with any "replace with a higher-fee-paying" solution is the user experience sucks.
Consider the common case of running with an encrypted wallet:  You type in your password, transaction gets sent, then you go off to watch the Red Sox game.
Come back a few hours later and find out you need to type in your password again so your client can unlock your wallet, resign, and re-transmit with a higher fee?
Really? That is a really bad user experience, and really bad from a security standpoint, too ("Oh, sorry, need your password again please, transaction didn't go through" is a perfect way to train users to be vulnerable to phishing attacks).
 
@_date: 2013-12-09 22:14:06
Yes, it should still be compatible with osx 10.5.
We will almost certainly be dropping 10.5 support for the 0.9 release, and might drop support for 32-bit processors  as well.
@_date: 2013-09-21 10:20:27
Yes, you need software that stores a time-locked transaction and throws away the funding transaction's key (so the time locked txn is the only way to spend).
@_date: 2013-11-28 03:59:28
Doesn't matter at all.
@_date: 2013-12-04 07:02:17
Put on your tinfoil hats:  I'm scheduled to do a Q&amp;A session at the Council on Foreign Relations in DC in early February.
@_date: 2013-10-21 23:08:43
Points we have responded to a dozen times already.
@_date: 2013-05-16 01:36:11
Give Leo a break, he was up at 5am to go to Google I/O ....
@_date: 2013-05-06 01:05:08
@_date: 2013-04-04 17:49:27
 has good advice.
@_date: 2013-09-11 21:17:59
The Foundation does not make decisions about the protocol or currency; all protocol decisions are made using the BIP process. The Foundation does play a role in supporting that process, by doing things like giving grants for hosting or hardware.
So the only types of decisions made ate things like 'should the Foundation organize another conference next year? Should there be a suit-wearing, well-spoken bitcoin expert  in Washington DC who is always available to talk with regulators an lawmakers? And if yes, should th Foundation try to arrange for there to be a Bitcoin expert (or experts) available to governments all over the world in case they have questions about what it is and how it works?'
Joining the Foundation gives you a voice in making that type of decision. If you want to change the protocol, then you need to convince the geeks your change is a good idea.
If you want to change the currency.... Then you first need to convince the geeks, then you need to convince everybody else. 
@_date: 2013-10-21 20:43:33
A reasonable miner will weigh the increased "orphan cost" of including a transaction versus the fees paid by that transaction.
That orphan cost is what puts a floor on transaction fees, and is why I still think eliminating the block size is the correct thing to do in the long term.
@_date: 2013-08-19 08:44:35
"That's why you have to be very considerate about anything you write or say because you are today the most public face of Bitcoin."
I think you overestimate my influence-- did you see my tweets during the last big price bubble?
April 1: "Bitcoin breaks $100 on April Fool's day; I expect an avalanche of "greater fool" headlines."
... price kept rising...
April 2: "I'm thinking about this 2011 story by the  folks as I watch the  price zoom:  "
(story is about economic experiments showing that bubbles arise naturally even in tiny, simple markets)
April 3: "Four Reason You Shouldn't Buy Bitcoins -   I'd add: be sane: dollar-cost average in and out, and diversify!"
" Sure... but lots of infrastructure still needs to be built. I'd prefer steady progress over 3 steps forward, 2 back..."
... no discernible effect on price rise...
April 8: "When will  speculators start chasing another squirrel?   
The answer turned out to be 2 days later, when Mt. Gox melted down.
I suppose I could have been more explicit and said "HEY EVERYBODY, WATCH OUT, THIS REALLY SMELLS LIKE ANOTHER UNSUSTAINABLE SHORT-TERM PRICE BUBBLE!"  but I don't think it would have made any difference.  Maybe I'll try it during the next unsustainable short-term price bubble.
@_date: 2013-12-11 22:02:26
Jinyoung Englund has been doing a great job.
@_date: 2013-03-23 01:37:56
The little outputs were to miners who split their coinbase into multiple outputs.  No idea why (maybe a private three-or-four-person pool?).
The biggest outputs are to big mining pools that had multiple orphans.
@_date: 2013-05-15 22:00:38
Yup. And I'm bringing cupcakes...
@_date: 2013-11-25 03:45:48
Does your country have a significant "wealth tax" (you must pay X% of any wealth saved if you have more than $Y) ?
I don't think many do (unless you count inflation as a hidden wealth tax) because it is easier to tax when the money is either earned or spent. That is the only kind of tax that I think Bitcoin makes much harder to implement.
Income taxes and value-added taxes and property taxes and almost all of the other ways governments have of generating revenue will still exist, and I think existing mechanisms for catching tax cheats will continue to work "well enough."
I think the "Bitcoin will Topple Governments by Starving The Beast!" meme is just wrong. Governments have adapted to new payment technologies for hundreds of years, and will continue to adapt.
@_date: 2013-08-18 06:51:45
Run -testnet -printtoconsole
And see if any of the debug messages give you a clue.
@_date: 2013-05-06 01:17:16
Have you actually tried to spend any of the coins you got from faucets?
If you're getting less than a penny's worth of bitcoins at a time, it will cost more to spend them than they're worth.
(The original Bitcoin Faucet that I created and ran always gave away at least 2 cents worth of bitcoins, but often paid as much as it gave away in transaction fees).
@_date: 2013-09-15 19:23:26
Bogus transactions?
@_date: 2013-10-29 21:20:11
You should spend half of them on something you need or want being sold for Bitcoins.
And save the other half.
Then if the price doubles, do that again (spend half, keep half).
@_date: 2013-12-10 21:13:43
Are you going to keep them in your bank account?
If not, then wire them directly to the money's final destination. If you are paying off your mortgage, ask the mortgage servicing company or bank where to wire payment.
If you are investing them in a mutual fund, ask the mutual fund company where to wire.
It is not unusual for mortgage servicing companies or brokerages to receive very large bank wires...
@_date: 2013-10-24 13:04:55
Peter is wrong. I'll explain how when I have time.
@_date: 2013-03-17 00:51:03
Unpatched/outdated nodes will have an alert telling them they must upgrade.
And if they ignore the alert, when a too-large block is eventually created, they will be left behind.  And their code will tell them... that they need to upgrade (the software notices when there is a longer fork that it can't validate).
And if they REALLY REALLY REALLY don't want to upgrade...there is a workaround that we'll describe.
In other words:  Don't Panic.
@_date: 2013-05-06 15:38:59
They will see their micro coins when the transaction makes it into a block. That will get harder and harder, assuming miners accept this change.
@_date: 2013-10-15 20:40:04
I think Andrew Miller put it best: "The trouble with Proof-of-stake is that there is nothing at stake."
Consider the basic function of proof-of-work and the blockchain: together, they let the network come to a consensus  when there are two (or more) different, competing chains.
Miners must decide to dedicate their hashing power to just one chain-- they cannot "bet on" more than one. So their best strategy is to work on the chain that they think most other miners are working on, and that quickly drives the system to a consensus on a single, best chain.
The trouble with proof-of-stake is there is no natural incentive stopping a miner from assigning their stake to multiple, competing chains. If you try to create such a system, you "go meta" -- you started by trying to solve the transaction double-spend problem (which proof-of-work and the blockchain handle nicely), and end up trying to solve a proof-of-stake double-spend problem.
@_date: 2013-12-12 22:35:47
I've accepted advisory positions with a few Bitcoin startups; it gives me perspective on what challenges the companies "on the front lines" are struggling with. That helps me in my role as "Chief Scientist", where deciding what to prioritize is probably my biggest challenge.
If you think I'm not being neutral or independent, please let me know.
@_date: 2013-05-03 21:25:12
Okey Dokey.
@_date: 2013-12-09 05:43:28
According to economic theory, it will work itself out in the long run:  
Another economic theory that is relevant: 
@_date: 2013-12-12 22:20:39
It will depend on how disruptive the change is, and how big a benefit it brings.
If it is a really big, fundamental change, then it will take a very long time to happen-- or might never happen. Think of IPv4 versus IPv6.
If it is an easy-to-deploy obvious improvement that can work alongside the existing system, then no worries. Think adding SSL to http to get https.
Academic researchers tend to underestimate the risks of making big changes, the effort required to get consensus that a change is good (there is a LOT of inertia, for very good reasons) and overestimate the benefits of their spiffy new idea.
@_date: 2013-05-29 20:22:04
There are no videos of the CIA talk.
Well, none that they told me about. There were probably lots of teeny-tiny spy cameras that I couldn't detect (because they didn't let me wear my tinfoil secret-camera-detecting hat).
@_date: 2013-12-25 18:30:33
Yes, please don't bother; there are much more efficient, cheaper ways of doing distributed data storage. Don't anise the blockchain.
@_date: 2013-10-24 22:49:35
@_date: 2013-11-15 10:24:27
Gee, those Nanny Statists at the Foundation have been doing a lousy job, they didn't fire me when we pulled Tor support into the reference implementation. And didn't force gmaxwell out for suggesting that CoinJoin should be part of the reference implementation. And didn't influence the privacy-preserving features built into the payment protocol (like being able to satisfy a payment request with multiple transactions from one person or with one CoinJoin transaction, to make it more difficult for an attacker tracing transactions).
Please just stop spreading FUD.
@_date: 2013-12-10 21:04:29
Jeff is working on a cubesat project to broadcast the blockchain from space.
When that is done in a few years you can then start worrying about 'what if they cut the cables AND shoot down the satellites?'
@_date: 2013-04-10 03:57:04
Don't use brainwallets!
If you INSIST on using a brainwallet (don't do it!), then follow the advice here: 
... and set up a 'sentinel' wallet that lets you know if you chose a bad passphrase.
@_date: 2013-05-06 15:33:17
Thanks for the kind words!
If you think this is bad... just wait until we tackle the block size issue.
@_date: 2013-08-26 05:37:33
Who is "us", and what are "our" motives?
@_date: 2013-07-18 07:21:31
Umm, no. We didn't hurry any patches because of increasing transaction volume, we were as careful as we always are. The blockchain fork would have happened with or without SatoshiDice, and with or without the patch-- the old code could (and would eventually) "fork itself".
Erik has always been open to changing SatoshiDice to make it a good bitcoin network citizen (I think the only valid complain us core developers have is that SD doesn't use compressed private keys, but last I heard that is because the version of bitcoinj they use doesn't support them).
@_date: 2013-04-27 11:57:29
Several hundred people have already paid and registered. I expect there will be at least 1,000 people attending.
@_date: 2013-09-15 23:20:21
What attack are you imagining?  Just dropping hashrate, or an attacker that also has a bunch of hashing power waiting on the sidelines to "jump in"?
Just dropping the hashrate means slower confirmations until the difficulty adjusts.
If you're imagining an attacker with a bunch of hashing power... then you're talking about somebody who has invested millions of dollars. How are they going to double-spend to make back the millions of dollars they spent on hashing hardware and remain anonymous?
@_date: 2013-03-21 22:04:11
"patches welcome" : 
@_date: 2013-10-22 03:53:33
It is outside the core-- the entire implementation is at the Bitcoin-Qt (wallet/GUI) level. No changes to core code / protocol / etc at all.
...insert snarky comment about non-devs telling devs how to do their jobs here....
@_date: 2013-12-05 20:48:30
0.9 should have floating fees based on what it takes 'right now' to get into a block. But fees are likely to be higher, not lower. More transactions means more competition to get into blocks...
@_date: 2013-03-06 22:44:44
PSA  is "Bitcoin is an experiment"
@_date: 2013-09-21 10:21:50
Making low-value 0-confirmation transactions safer is high on my personal priority list.
@_date: 2013-03-26 23:13:11
Imagine you're a Russian oligarch with a couple hundred million dollars.
You just found out the government of Cyprus is going to take 10 or 20 or 30 or million of it.
You hear about this Bitcoin thing, and you don't really understand it, but technical people you trust tell you they can set up a system where you store some of your wealth, have absolute control, have the ability to spend it easily, and no government in the world can confiscate it. Oh, and it's been increasing in value, too.
That is pretty darn attractive, and even if you don't 100% trust the newfangled money, putting a few hundred thousand dollars into it might be a risk you're willing to take.
Don't underestimate the value of "store of value" -- I suspect some of the "speculation" that is happening right is wealthy people looking for another place to park some money that they don't need right away. I expect that as the solutions for doing that get easier to use and more trusted using Bitcoin as a store of value will get more and more popular.
@_date: 2013-11-07 10:15:21
So when the tax authorities ask where they got the cash, they say 'from selling bitcoins'.
And what do they say when asked where they got the bitcoins?
@_date: 2013-05-03 16:47:25
Good idea.  Regulations are the big barrier, but you could start by operating only in places with reasonable money transmitter laws to see if you can operate at a profit.
@_date: 2013-10-28 04:58:23
IRC has not been used for peer discovery in a very long time.
@_date: 2013-12-12 11:24:42
Bitcoin has, by far, the most developer support. Just look at github activity for Bitcoin versus any other-- dozens of developers contributing to Bitcoin, three or four for the best of the rest (many are single developer projects).
The network effect works when it comes to attracting developers, too-- why work in obscurity on FooCoin when you can make a name for yourself (and maybe get hired by one of the dozens of Bitcoin startups) working on Bitcoin?
@_date: 2013-07-28 07:37:31
The server doesn't need to know anything about you (unless you want it to confirm payments via phone or SMS of course), and if you communicate with it over Tor then it doesn't even know your IP address.
If you REALLY want to be private, then run your own server. But most users are not competent to do that securely.
@_date: 2013-12-27 07:19:24
Nah, old answer was Red Sox tickets from a friend.