@_author: nanite1018
@_date: 2017-08-18 07:00:07
You mean Core? Yeah... this was my understanding as well. If 2x ends up not being activated, miners could dump BTC for Bcash or you end up with three different bitcoin blockchains all running simultaneously causing market/network chaos.
@_date: 2017-08-18 09:30:54
What do you want to use it for? I want it to replace the global financial system and serve as the settlement layer for a totally new decentralized and trustless, low-cost, efficient financial system. Opening and closing a payment channel takes two transactions, and so Bitcoin needs to ultimately get to at least a couple thousand tx/s on chain so that each person on Earth could process through one payment channel every few months.
*Edited for spelling/autocorrect errors.
@_date: 2017-08-20 09:13:18
Maybe I'm missing something, but if you're committing to a UTXO set from thousands of blocks ago it's not going to effect anything happening now unless the blockchain is going to be rewritten going back thousands of blocks --- which probably won't be allowed to happen anyway in reality. If Bitcoin serves as the basis of the financial system, no one is going to listen to a new secret chain that someone spent billions on secret hash power making because real-world economic stuff has long since been spent and assigned and jumping to such a fork would result in something akin to the 2008 financial system crisis. There'd be an emergency hard fork to ignore the chain.
I'd be surprised if, as the size of the chain grows, people won't have an on-disk (or in-memory) cache of the UTXO set, and a live/smaller cache of likely-to-be-spent UTXOs that's periodically flushed. You wouldn't refer to blocks or transaction history to determine what UTXO has been spent or not, you'd just keep a separate cache of them (at least for relatively older UTXOs). Way more efficient.
@_date: 2017-08-10 01:52:32
How many people live in Australia? Relatively few. They can just piggy back off of the entire rest of the West where an 8MB block takes ~16 seconds to relay on my extremely slow home internet connection, and where millions of homes have bandwidth 5-20x faster than that. 8MB*10min/block=~35GB/month of bandwidth. I'm running a full node on a server I use for other stuff (mostly my work). I'm planning to jump to a different provider for home internet with ~20mbps upload capacity, which can upload such a block in 3 seconds.
Europe, South Korea, Japan, etc. have widespread 100mbps+ upload capabilities that can share an 8MB block in well under a second, and 1gbps is deploying quite widely at homes which gives people the ability to share 8MB blocks in under a tenth of a second.
@_date: 2017-08-18 09:19:17
So you could have high speed internet but don't want to. That's fine but I don't think people like you should be able to hold the future financial system of the world hostage because you don't want to buy internet from Comcast. If you don't feel like paying a company for internet that's your choice. 
Also, 1MB/ten minutes is ~5GB/mo. You could surely handle 2MB/ten minutes too, no? May not be able to mine but you'll be able to download the blocks and transactions. 3G is 144kbps minimum, so you could download a 2MB in no more than two minutes.
@_date: 2017-08-18 07:32:35
For me the answer's yep! Two, actually. One at home (where I have ~8mbps upload speed) and also one on a server I rent for work as I need way higher data throughput speeds and computational loads than I can reasonably have on a personal/free-standing device (85/mo for 12 cores, 72GB of RAM, a 128GB OS SSD and 3TB of HDD space, and symmetric data speeds of &gt;300mbps).
I'd be able to pretty easily handle 32MB blocks, so Bcash's 8MB or segwit2x's 2 (or 4-8 depending on how you define it) is perfectly fine by me. And since storage, bandwidth, and CPU costs drop by at least a factor of two every ~3 years, I see no good reason not to increase blocksizes by about a factor of 2-4 with every 4 year halving of the reward rate for the foreseeable future. At least not from a centralization perspective (2 every four years would mean decreasing costs to run a node over time, after all, so you should get decreased centralization). There are arguments against forks of course.
@_date: 2017-08-18 07:14:59
Let's just all be a bit clear re: 5. A 1mbps connection can download or upload a 2MB block in 16 seconds, with average block times being 10 minutes. So your internet doesn't really have to be "good" in order to handle it. That's about a simple DSL connection. ~80% of people in the US and virtually everyone else in the developed world have connections with upload speeds of more than 4mbps.
@_date: 2017-08-20 09:26:25
But why isn't it sustainable? As long as you soft fork to UTXO and minimum POW commitments in the blocks for some (possibly large) period into the past (a year?) then the total storage size needed is constant. As for bandwidth, most homes in America can get an 8mbps connection, and with compact blocks you can handle probably easily handle 64-128MB blocks (since &gt;90% of the transactions will already be in the other nodes mempools). And typical bandwidth is about to explode as most cable companies are planning to fully upgrade their networks to enable 1gbps symmetric or 1gbps/300mbps down/upload speeds (depending on fiber vs cable) within five years, enabling potentially 1GB blocks.
@_date: 2017-08-23 08:41:47
How much do those cost though? I rent one to do work stuff and run a full node on it, $85/mo. It only took me three days to sync to the present because it's so fast. You might say that's a lot, and it is a good amount, but it's not like you have to dedicate the server full time to the block chain.
1MB blocks only generate 50GB a year. If you're worried about syncing then institute checkpoints of the utxo set for every 50k blocks, (ie blocks 500k-550k have a checkpoint of the state at 450k, and 550-600k have one for 500k, etc.). That means the checkpoint is always for the state of the network one to two years ago. Then storage and sync time are capped and you compromise basically nothing with regard to security. 
Its not really any less secure, practically, then trusting you got the genesis block, and if someone can overpower 50-100k blocks of hashing power than they could pretty much do anything they want to the block chain anyway via 51% attacks, etc. If someone could fool people as to the checkpoint, by say corrupting the software, and all records/nodes that people can access, etc., then they've already gained effective control of network regardless. And it could be implemented as a soft fork.
@_date: 2017-08-20 09:51:12
Re: storage size: That's what UTXO and POW-level commitments in the blocks give you. You download all the block headers, validate them, then download the last [insert large period of time here] full blocks (say a year=~50k blocks) and a checkpoint UTXO set and verify a) that the hash of the checkpoint UTXO set matches the commitment from 50k blocks ago, then validate all the blocks from here up to the present. Then the storage space required is constant.
Even huge 1GB blocks would only require 1TB to store 1000 blocks (about a week's worth). 50TB is a year's worth of 1GB blocks. That sounds like a lot, but storage capacity is doubling every ~two years, and a 128TB SSD is coming out next year from Samsung iirc, so 5-10 years from now that won't be a ridiculous amount of storage and may well be readily available to individuals at home. And a checkpoint once  a year is pretty damn conservative. More realistically the world won't ever accept a rewrite of block history more than 30 days into the past (too much real world stuff will have happened for the world to countenance such a rewrite) and if someone tried to spring a new longest chain rewriting the last 30+ days of history there'd probably a hard fork to reject it. And 30 days of 1GB blocks is only ~4TB, which is achievable for people even today, and will be dirt cheap 5-10 years from now.
I'm not against more efficient code, but I don't see why a few relatively minor modifications don't allow you to scale quite a bit through simple increases in block size.
@_date: 2017-08-18 09:03:28
Some can't afford it, but then they don't have computers enough to run full nodes in the first place even at 1MB. The only people who can't buy an internet connection with more than 4+ Mbps upload speed are people in rural areas in the US, basically. Virtually the entire rest of the developed world has access to them (except maybe Australia and New Zealand). 1gbps connections are common in Japan and South Korea, tens of Mbps standards for under fifty bucks a month in Western Europe, etc. You're pretty much talking about people with HughesNet or people who are very poor who can't handle a 1mbps connection.
The banking system is bloated, costly, slow, and fragile. Bitcoin isn't. That's why we need Bitcoin. It's not meant to run on a 300 baud modem.
If someone does only have a 300 baud modem, they can just use an SPV wallet.
Edit: also, if you only want to be able to verify transactions but don't care about being about to relay all the transactions/ blocks, then you can safely have a dialup connection and still be able to keep up with the new transactions and blocks pinging around the network at least up to 2MB blocks.
@_date: 2017-08-10 07:34:39
Payment channels can't scale to global financial systems. 15 tx/s (the limit at 1MB with segwit) is only 450 million tx/yr, ie each person with internet gets to open and close three payment channels in their lifetime. You need at least ~4000tx/sec on-chain to run a global financial system with payment channels as a second layer solution taking on the overwhelming majority of the actual transactions.
You don't need to run a full node, pruned nodes with a smallish block history on hand are fine so long as there are a large number of full nodes/archival nodes with no faction controlling a large fraction of them, and such nodes only need bandwidth, not significant storage. Even at 4000tx/s, you'd only need around 60TB/yr of storage for a full node. Assuming we target that rate for 5-10 years from now, that'd only be a few thousand dollars a year per node to provide a component of a backbone for the entire global economy. Samsung just announced 128TB SSDs will be produced within a two years, so within a decade full nodes will be able to slap a new one of those in every couple years to meet storage demand at 4000tx/s at not that high of a cost.
Australia will eventually catch up with bandwidth, and most of the rest of the rich world has ready access to enough bandwidth. 4000tx/s needs ~10mbps symmetric. Most of the US has access to that now and within five years most of the US will have at least 100mbps upload capacity at home, with millions of homes at 1gbps. 100mbps is widely available in other countries already. (My father works in cable on expanding their gbps network, so I'm well-versed in these timelines and am being somewhat conservative.)
@_date: 2017-08-22 22:16:46
Bcash was more profitable, so miners switched to mine the Bitcoin Cash blockchain. 
@_date: 2017-08-18 09:56:21
But what future do you see for a chain that can only do 10tx/s, tops? How does it help anyone long term if each American only gets to one transaction a year?
@_date: 2017-08-22 08:12:12
The checkpoint hash is just a number you'd be able to find from a bunch of places too, particularly if checkpoints are rare. If the checkpoint is  once a week, that may be too short. But if the checkpoint is once a year? It's still pretty much infeasible to outrun the chain for an entire year, and if you can you're certainly a threat anyway due to 51% attacks and the like. There's a trade-off between ease/space and security, but a year is definitely too long to reasonably be worried about. And I don't really understand how it's not Bitcoin, it could be easily implemented as a soft fork.
@_date: 2017-08-20 17:57:46
Put the latest commitment in the code you download. If they happen rarely, like every three months to a year, then it's fine. If there's a problem then people will figure it out via social consensus. This is no different than trusting you got the right genesis block. The hash would end up being printed in newspapers, known to your friends, on Google, Twitter, in the code, and in everyone's block headers, so it will be very easy to know what it is and no one is going to be tricked.
@_date: 2017-08-21 05:02:33
Do you verify the genesis block? No. So there is some trust, namely that the software isn't lying to you. If all I need is a hash and I can verify that the utxos are all correct (match the hash), the work is at the right level for it (the blockheaders), and verify, say, a year of work to get from that has to the present, what more do I need? No one can fool anyone whose node was active in the previous year (or since the last check point, however long that's been), and that's basically everyone. The only people who might be fooled are noobies who get the wrong hash and a fake set of block headers and blocks, but if you can forge a year of work you can probably forge ten years or twenty, and the only thing that you ultimately have to trust is that the hash in the code and/or the one everywhere in the real world is correct. It's the same as believing you have the right genesis block. So I don't know how it compromised security or significantly greater trust than Bitcoin today.
I'm not saying only remember ten blocks or something. I'm talking at least a thousand, and maybe as much as 50k. That's a lot of POW.
@_date: 2017-08-20 18:00:24
Within five years 300mbps upload connections will be commonplace in the US, so I don't think we really have much to worry about there, especially with compact blocks implemented.
@_date: 2017-08-18 09:36:57
I'm not sure what you mean by that. Could you explain?
@_date: 2017-08-20 08:54:54
Why do we need everyone to run a full node rather than, for instance, do a soft fork to including a UTXO set hash in each block? i.e. block 500k-501k all include the hash of the utxo set at block 499k, and then 501k-502k commit to 500k, etc. Then if I get all the headers, I can check a utxo set I download from someone trustlessly, and the UTXO checkpoints can be put into the software that someone downloads when they want to start their node. This would allow most people to verify any recent transaction, and not have to keep around more than a few thousand blocks.
Even miners don't need to store the entire chain in order to validate and secure the chain, they just need to know that their node is up and running and valid with a good chunk of the history. If 1000 blocks isn't long enough, do a checkpoint every 5k, 10k, 25k, or 50k blocks (approximately every a month, two months, six months, a year, respectively).
Am I missing something?
@_date: 2016-10-26 21:57:19
Obviously not, because the problem is the network can't process more than a fixed number of transactions a second.
Increase the price of confirming a transaction and you reduce the number of transactions people want to make (as some aren't worth it anymore), and eventually you reach equilibrium.
@_date: 2017-07-06 15:57:32
It seems like if that's the case then Bitcoin needs to adopt PoS instead of PoW to massively cut the investment of running a full node. Like they're working on in Ethereum. If transaction fees are very high, you're going to have nasty effects on your resulting economy (financial transaction taxes are widely considered one of the worst/most destructive forms of tax from a deadweight loss perspective, and transaction fees are a version of them, in a sense).
@_date: 2017-07-06 20:42:26
A transaction is, what, few hundred bytes, tops? So that's about 10MB of data storage and net transfer. 1GB of enterprise SSD storage costs about a buck, so that's a penny worth, and the price of 1GB costs no more than 20 cents, so you're looking at a reasonable storage/network fee of 1.2 cents per transaction. Pretty darn low fee, so long as the amount of compute those 30k machines have to do to add that transaction is small.