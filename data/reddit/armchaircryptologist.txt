@_author: armchaircryptologist
@_date: 2018-01-01 23:18:59
I suspect Electrum does something non-standard for the SegWit private key format since there is a warning specifically for SegWit wallets in the private key popup that "the format of private keys associated to segwit addresses may not be compatible with other wallets". This warning is not there for P2PKH, and the Electrum private keys do match the iancoleman generator for that wallet type.
@_date: 2018-01-07 19:58:32
Those are consolidation transactions. You can spot these by that they have a lot of inputs and one or two outputs. Some are present in [this block]( for example [this transaction](
5 MB is only enough to consolidate about 28000 "standard" inputs. There are plenty of companies that receive that many or more transactions per week, like BitPay and Coinbase, but as these are P2SH, they are most likely not from either of those.
@_date: 2018-01-04 16:00:43
Schnorr signatures won't have a huge impact on transaction capacity, since while it would reduce the average transaction size by ~25% with today's transaction pattern if everyone started using it, the space savings are all in the discounted witness space. It is still quite helpful for consolidation transactions with many inputs, but even if everyone switched to SegWit w/ Schnorr signatures, that would only add a bit less than 10% capacity compared to SegWit w/ ECDSA signatures.
The big savings transaction capacity wise are in SegWit itself, and that's available *now*.
@_date: 2018-01-20 22:10:36


Indeed it is.
With nested SegWit inputs (P2SH-P2WPKH), this 200-input 1-output transaction has a weight of 111561, or 27890 vbytes.
If it were using legacy inputs (P2PKH), it would be around 61000 vbytes and be **118% more expensive**.
On the other hand, if it were using native SegWit inputs (P2WPKH), it would be around 23000 vbytes, saving another 18% or so. Granted, most wallets cannot send to these addresses yet, so it is still a bit early to push for that.
@_date: 2018-01-05 01:25:39
There are some requirements that must be met to make Lightning successful. One is those is that channels are, on average, long-lived. Another is sufficiently widespread acceptance that it makes sense to open a channel rather than send a transaction directly. Call it the network effect, if you will. Both of those are achievable, though not guaranteed.
You do have to close a channel if you need to free up funds for another channel or on-chain transaction, but the goal with Lightning is that you shouldn't need nor want to do this. You should be able to route a payment to your target through your existing channel without directly opening a new channel, and that on-chain transaction would generally be much faster and cheaper through Lightning anyway. In theory, the only reasons you should have to close a channel is if the counterparty becomes unresponsive or if your channels have become very unbalanced - meaning you on average send more BTC than you receive, or vice versa.
For this to work well, the network effect does need to materialize. But keep in mind that experienced HODLers will generally have an interest in providing cheap or even free liquidity for Lightning, as giving Lightning (and therefore Bitcoin) more utility and thus more value will likely have a direct positive effect on the value of their HODLings.
@_date: 2018-01-05 00:00:13
ECDSA is what Bitcoin uses today to sign transactions. If you are looking for how it is currently used, the [Bitcoin Developer Guide]( is a good starting point. If you are looking for details on the signature algorithm itself, I honestly haven't seen one that wasn't mostly math, but it basically relies on an assumption that reversing some operations for elliptic curves requires solving a math problem that is thought to be intractable - specifically, the elliptic curve discrete logarithm problem.
EC-Schnorr works in a similar way, but has some properties that makes it desirable over ECDSA. Signatures are slightly smaller (7-9 bytes) and slightly faster to validate, but more importantly, it allows signatures to be safely "merged". This means you can get native multisig without relying on large P2SH/P2WSH redeemscripts, and you can combine signatures for multiple inputs into a single signature.
@_date: 2018-01-06 15:37:31
P2WPKH inputs are 24 bytes smaller than P2SH-P2WPKH nested format inputs, and those savings are all in non-witness data. The witness (signature and pubkey) is the same. See [BIP141](
@_date: 2018-01-21 00:27:32


I believe it would reduce the total signature size from 70-72 WU per input, to 64 WU for the first input and 1 WU for every following input. So the total signature weight for 200 inputs should be reduced from ~14200 to 263.
Compared to P2WPKH with DER-encoded ECDSA, assuming no changes to the size of the script templates, it should reduce it from ~23000 vbytes (92000 weight) to ~19500 vbytes (78000 weight).
@_date: 2018-01-30 21:44:00


While it doesn't inherit the label of the P2PKH address, in my experience it has been working perfectly fine by calling setaccount on the converted P2SH-P2WPKH address you get from addwitnessaddress. Out of curiosity, are you aware of any potential pitfalls from doing this?
@_date: 2018-01-20 20:05:17
Looks like a bunch of P2SH-P2WPKH reconsolidation transactions, possibly Bitstamp. This is about as large as blocks can get until native SegWit is widely in use.
@_date: 2018-01-20 23:55:11


It's rather negligible compared to the savings in the scriptSig. With P2WPKH, you save 1 byte per output compared to P2SH-P2WPKH, and 3 bytes per output compared to P2PKH.
@_date: 2017-12-25 00:09:11
Electrum already supports P2WPKH with bech32. But yeah, native SegWit is still some ways off for general use.
@_date: 2018-01-05 10:34:19
Certainly. You could think of Lightning channels as your spending account and your hardware wallet as your savings account. The salient point is, even if your sends and receives are unbalanced, it allows you to batch transactions so the effective per-tx cost becomes very low, and enables Bitcoin to be economically used also for very small transactions, even microtransactions in the sub-satoshi range.
@_date: 2017-12-30 16:10:34
I have read it previously, but channel factories are not coming with Lightning 1.0, and I'm not aware of any practical work towards implementing it. The conclusion is also somewhat skeptical towards it being a practical solution on a larger scale.


@_date: 2017-12-30 17:26:10


The lowest cost transactions that have cleared naturally in the last couple of weeks is about 65 sat/byte back on December 17th, so if you "lower the sat/byte" too much, your transaction just won't get mined at all. 6-block fees right now are in the 200-300 sat/byte range, but we're coming off a long holiday, and you usually don't want to wait too long for a channel to open or close, especially if it involves a penalty transaction - otherwise you risk losing the funds.
While opening a channel *right now* could be done for less than $20, the argument here is that high fees will cause centralization pressure on Lightning and might prevent uptake altogether.
@_date: 2017-12-22 00:24:13
P2SH-nested SegWit addresses, that is, standard Base58-encoded addresses that start with a 3, are backwards-compatible and can therefore be sent to by any wallet, even those that predated SegWit.
Native SegWit addresses, that is, bech32-encoded addresses that start with bc1, cannot.


There is no risk, it would just reject the address and refuse to send it.
@_date: 2017-12-30 10:15:47


This is incorrect. Certainly, if you use BitPay or a similar third party processor, they will offload the cost on the customer with their "network fee", but if you actually accept Bitcoin directly, there is a per-tx cost of 140-170 bytes depending on address type that the merchant has to eat to consolidate UTXOs.
Support is also a major cost with receiving Bitcoin, fees (and backlogs) being what they are.
@_date: 2017-12-30 10:30:18


On this, at least, we agree. But the consolidation costs remain, even if you run your own wallet, and even if you use SegWit - albeit with 50% less per-tx cost than plain old P2PKH.
@_date: 2017-12-30 18:45:59


They probably will, for opening channels, but "cheap" is relative and those times have been few and far between lately. It might settle down a bit now that the wild speculation has cooled off somewhat, but that's pure conjecture.
Closing channels is however the wildcard, and if the channel's counterparty posts an outdated commitment transaction at a time where fees are high, you still have to get the penalty transaction mined before the locktime expires, and thus are forced into paying the higher fees.
@_date: 2017-12-21 15:24:19
Electrum only supports wallets with the transitional P2SH-P2WPKH format if you use it with a hardware wallet or use an external program to generate a BIP39 seed, which is somewhat non-intuitive and rather user-unfriendly. Since native bech32-style Segwit addresses aren't supported by most (any?) other wallets yet, moving any coins to such a wallet in the first place requires jumping through some hoops.
@_date: 2017-12-29 17:43:23
It happened like five days ago, on the 24th, so I assume most people just haven't been making payments over the holidays. A server host I usually pay with Bitcoin just dumped BitPay for Mollie because of this.
The only decent desktop wallet I'm aware of that supports this is Electrum, which can also be used with your Trezor or Ledger, but it is still a major inconvenience if you happen to use something else.
@_date: 2017-12-30 14:52:55


Huh? No, it's not. To open a channel, you have to make an on-chain funding transaction, which incurs transaction fees. To close the channel, you have to make another on-chain closing transaction, which also incurs on-chain transaction fees. If these transactions are $20, Lightning is DOA.
@_date: 2017-12-21 18:40:31


To send funds to a native SegWit address, the sending wallet needs to understand the new address format. Apparently, outside of Electrum, there is also the Samourai wallet, but any other wallet would reject bech32 addresses as invalid.
The easiest way to fund a native Segwit wallet today is probably to import an existing wallet into Electrum itself, and then move the funds from the old wallet to the new one. Otherwise, you would have to wait for your current wallet to be updated to support the new address format.


The short answer is "yes".
The long answer is, if the receiving wallet doesn't have SegWit support, the transaction might not be visible until after it is confirmed, and even then, it could potentially glitch out somehow since it wouldn't know how to interpret the sending address format. A well-coded wallet should handle it gracefully, but I couldn't make any guarantees for any particular wallet. The funds should not be lost in any case.
There is a reason the P2SH-nested formats exist, and personally, while I do have a native SegWit Electrum wallet that I've played around with for a bit, I wouldn't use it to send payments to someone else quite yet, unless they actually provided a bech32 address.
@_date: 2017-12-30 10:22:16
Ironically, the biggest threat to *Lightning decentralization* is also the Bitcoin fees. If the cost of opening or closing a channel is $20-50, people might forego it altogether, and they are certainly not going to be maintaining the 10 or so channels necessary to keep Lightning decentralized. Rather, they are going to limit themselves exclusively to trusted, long-lived nodes with many channels. Which means that the feared hub-and-spokes model will almost certainly materialize if the fees remain high.
@_date: 2017-08-26 20:22:22
Correct on both counts.
When it comes to Trezor encryption, how would you store or derive the encryption key? Let's assume that extracting the content of the SoC's internal RAM and/or flash memory is possible. If the encryption key were stored internally, it could be extracted along with the encrypted data. If it were derived from the PIN, the restricted length and entropy means it could easily be brute-forced. Given these limitations, using passphrases is a far better solution than encryption would be, as the passphrase is not stored on the Trezor.
Physical security for the seed is not its primary advantage; you may be able to physically extract the seed from the SoC somehow, and evil maid attacks are still a concern; it is however vastly more secure than a soft or paper wallet if you need to regularly send coins. 
It should be noted that the Ledger may be better hardened for physical security, as the key itself is stored in a secure element rather than a standard chip like the Trezor's Cortex M3. There could however still be attacks found on this secure element, and for most people, I don't think the difference is all that significant.
@_date: 2017-08-30 15:48:00
I'm not sure if they are aware that it is possible. My understanding is that covert ASICBoost is not possible if the mined block includes a witness commitment, and blocks from AntPool and ViaBTC both seem to consistently include one. 
@_date: 2017-08-30 12:36:18
This is most likely what is going on. If you can mine BCH at a ~25-30% advantage with ASICBoost, it is still more profitable than mining BTC without the advantage as long as the "expected" profitability of BTC is less than 20-25% above BCH. This is true even if 100% of the miners are using it.
@_date: 2017-08-27 08:54:44
You have to type it in. It is a passphrase, after all.
@_date: 2017-08-07 13:22:19
Transactions below 10-20 sat/byte are completely ineffective as a way to build up a backlog that prevents "real" transactions from being included. Even if this is "spam" rather than low-priority transactions, it's not anything to worry about.
@_date: 2017-08-26 13:26:31
There is a reason for this. The Trezor and Ledger both allow you to not just store the coins safely, but also spend them safely. Sure, you can have an airgapped setup with an offline signing computer using something like Armory, and this is more secure if done properly, but it is a major hassle compared to plugging in the hardware wallet and entering the PIN.
@_date: 2017-08-31 12:06:46


This depends on the chip design. For a "pure" ASICBoost chip, while an individual pipeline/processing unit on the ASIC may not be faster, the reduction in gate count and electricity consumption per processing unit means you could fit more processing units on each chip. I.e., you get a higher hashrate with the same die space and power consumption. Arguing whether this is a "speed increase", or an increase in performance/watt or performance/die space, is somewhat pointless. [Refer to the whitepaper.](
If the miners are also capable of "normal" mining, they may only be using a lower-power mode when the pre-crunching for ASICBoost is done. However, it is also possible that the reduced power consumption means that the chips can then be overclocked to operate faster than they would otherwise be capable of.
You really cannot tell which technique they are using unless you examine the chip design in question.


Of course, but using the alternative technique would be immediately obvious.
@_date: 2016-11-22 12:52:28
Most people would have bought one or the other, and wouldn't be able to give you a first-person comparison. FWIW, I bought a Trezor, and liked it well enough that I bought a second spare. It's open-source, which makes it as future-proof as you can hope for, and supports two-factor auth via U2F out of the box.
The official browser wallet is somewhat simplistic, and I particularly miss coin control, but it still does everything you strictly need, and you can use it with other wallets if you need additional functionality.
@_date: 2017-08-25 21:15:13
Though the old Berkeley DB lock limit was not restricted by block/transaction size but by the (not easily predictable) number of locks acquired. Are you saying it was impossible to create a 1 MB block without running into the lock limit? (It might have been, for all I know.)
@_date: 2017-08-26 18:35:04
I did. It was caused by a soft reset bypass on the SoC, and was patched. It is one of the reasons why an airgapped signing computer is still more secure, but it is a huge step up in security compared to a soft wallet, and much easier to use than offline signing.
Even the exploitable version of Trezor has comparable physical security to an offline-generated paper wallet, seeing as both would be compromised by physical access unless you use a passphrase.
@_date: 2017-08-27 08:58:01
Hm? It has a virtual scrambled pinpad, where the actual digits are only shown on the Trezor. The only information anyone would be able to discern even if you entered it on a fully compromised system is if you reused any digits in the PIN, which they specifically warn against. There are more expensive products that have this on the device itself, like the [Ledger Blue](
@_date: 2017-08-22 15:07:50
The problem *right now* is that ~40% of the hashrate has switched to BCH, since it the difficulty reductions means it is currently far more profitable to mine than BTC. The drop in hashrate means that the rate blocks are generated has slowed substantially.
About six hours from now, BCH will have a difficulty retargeting that increases the difficulty by approximately 300%. I expect this will normalize the hashrate, as it makes BTC more profitable again.
@_date: 2017-08-25 20:56:41


Please elaborate on the exact mechanism that prevented a block larger than 1 MB from being created.
@_date: 2017-08-28 13:45:14
I was playing with Segwit transactions yesterday, and the dozen or so transactions I made over a couple of hours were consistently included in the next block. Miners also seemed to be prioritizing by Sat/WU rather than Sat/Byte, as they should.
@_date: 2017-08-20 08:58:35
There is no value to a list of companies that haven't explicitly supported the capacity increase. If you had a list of companies that have explicitly come out against it, with a link to the statement, that would at least have *some* value.
@_date: 2017-08-20 11:15:58
The problem was that you had not already opened a channel, which is why you had to wait for an on-chain transaction. In day-to-day use, you will usually have a usable channel open already, which will make the payments near-instant.
@_date: 2016-12-04 12:21:16
Any online software wallet is only suitable to keep small amounts for day-to-day use, much like the wallet you have in your pocket. Hardware wallets can be used for somewhat larger amounts, but even if you examine and build the code yourself, you still have to trust that the developers didn't sneak anything else into the hardware.
The only method I'm aware of that makes you virtually immune from the wallet itself being compromised is using something like Armory to do offline signing, and examine the signed transaction you got from your offline signing computer before broadcasting it. This would still be susceptible to weak seed generation attacks if you let the wallet auto-generate one, but you can make your own seed to mitigate that.
(No one ever said that being your own bank was *easy*.)
@_date: 2017-11-01 19:26:26
You have some fair points, but I would like to address these:
1: Compared to the Trezor 1, the touchscreen makes it easier and more secure to enter the PIN and recovery seed, and the fact that it will not do USB communication before it is unlocked further improves security. Compared to the Ledger, while PIN input is already fully on-device, it is super fiddly, as is entering recovery seeds.
3/5: I haven't seen any announcements indicating that the Trezor 1 will not gain new coin support as well. Presumably, they will be selling both, similar to the Ledger's Nano S and Blue. It seems to be positioned somewhere between the old Trezor and the Ledger Blue feature- and price-wise - you get the touch screen, but no battery or bluetooth/NFC. The Blue still costs twice as much, however.
@_date: 2017-11-21 15:51:56
Sane wallets that support both BTC and BCH, like Trezor and Ledger, do so by using different derivation paths and being very verbose that BTC addresses are different from BCH addresses. They do this for a very good reason, and the issue I'm pointing out for this app is that muddling the waters by using the same derivation path *will* cause people to lose money.
@_date: 2017-11-21 15:38:05
That really doesn't change the fact that it is a horrible idea to present BTC and BCH addresses as interchangeable. Even if you make the choice for what to send extremely obvious, doing this means that some people are going to assume that they are interchangeable in general, rather than just for this app.
@_date: 2017-11-12 13:52:25
It's tongue-in-cheek.




@_date: 2017-11-12 10:19:46


If BCH has shown anything, it is that the majority of miners will blindly mine the most profitable (as opposed to valuable) chain, at least if the difference is more than 10-20%. However, because the new difficulty algorithm uses a 144-block moving average with re-adjustment every block, it *should* respond quickly enough to valuation changes that the hashrate split between the chains remains more or less stable, relative to the value of the chains.
As for what miners and large holders plan with regards to trading coins on one chain for the other, it can generally be assumed that they don't want to take substantial risk with their own holdings, but talk is cheap and there are certainly people who have the clout to influence others to risk theirs.
@_date: 2017-11-12 00:38:40
Nov 13th at 19:06 UTC, to be exact. Which means BCH difficulty should re-adjust to ~0 blocks/hour about 24 hours from now, then start re-adjusting difficulty for every single block later that day, after the time of the fork. Which should put an end to the massive hashrate fluctuations on both chains.
@_date: 2017-11-21 15:30:20
You seem to have missed the point. The problem isn't when you send BTC or BCH to this wallet, but when you send them from this wallet elsewhere.
@_date: 2017-11-21 15:08:10
The main problem with this app is that it is a really bad idea to give people the impression that you can send BCH to BTC addresses and vice versa. People are inevitably going to screw this up, and while a recipient may be able to recover funds manually for standard P2PKH transactions, it *will* cause loss of funds if, for example, you send BCH to a P2SH-style Segwit address.
@_date: 2017-04-21 09:10:13
You can minimize the amount of fees you pay by adding $100 (or whatever is the maximum in your region) to your Steam wallet at a time instead of sending a separate transaction every time you buy a $3 DLC.
@_date: 2017-04-17 18:35:57


That is the dumbest possible reason I've ever heard for placing an obfuscated backdoor in client-side software, and should be sufficient proof for anyone to never run any code provided by you.
@_date: 2017-09-10 20:36:31


Not as such. The reason the exploit is feasible is that with the pre-0.15 chainstate database, UTXOs are stored on a per-transaction basis. When an UTXO is spent, the record for the full transaction needs to be updated in the chainstate database. Because the LevelDB atomic commits used for this requires the full record to be temporarily loaded in RAM, if you can make a transaction that references a lot of large transactions, this database update could potentially take several to tens of gigabytes of RAM to complete.
With BTC, the time required to build the required number of large transactions to perform this attack is much longer than with BCH, and it would be far more expensive. Which means that while it is certainly feasible in theory, it's unlikely to be real-world exploitable for BTC, at least not before a large portion of the nodes have updated to 0.15.
@_date: 2017-09-10 23:52:29
Realistically, Bitmain already reneged on 2x by making BCH.
At this point, it would probably be preferable if they just add a malleability fix to BCH and get rid of EDA. Then we can have a big blocker chain and a small blocker chain and all be a happy cross-chain atomic swapping family again.
Also there will be free unicorns for everyone.
@_date: 2017-10-22 12:39:31
This is a broken suggestion from a brand new Github account with no history. Miners who do not agree to 2x would obviously not adopt this softfork, so even if they fix the obvious ban issues, making it work would be contingent on the hashrate for a "no transactions" 1x chain being larger than a "with transactions" 1x chain. Which basically means that 2x-supporting miners would have to donate hashpower to keep the 1x chain in check. In other words, a plain old 51% attack.
@_date: 2017-09-07 10:12:40
It really is mostly that many wallets just outright suck both at fee estimation and optimizing inputs/outputs.
You know what's worse than 600 sat/byte transactions? 600 sat/byte transactions with a 4 cent change output.
@_date: 2018-02-27 20:54:29
"Added support for sending to Bech32 addresses" seems important enough to include in the blog notes, but seeing as I noticed that by accident, thanks for that too.
@_date: 2017-09-10 20:13:13
It could be exploited by any party that has a lot of large transactions with unspent outputs. If such a party exists today, they would probably not have an interest in attacking Bitcoin, but obviously that's a rather weak assumption to make.
For the miners-only attack vector with 1MB transactions, BCH is more vulnerable to this than BTC, since it is effectively free for a miner to add 7-8 large transactions with ~32000 outputs, and still get all the pending transactions into the block. Doing this on BTC would have a potentially large opportunity cost in that fee-paying transactions could not be included, and you'd only get the one transaction.
For the attack vector available to everyone, BCH is *far* more vulnerable since adding a large number of 100 KB transactions with BTC would be cost-prohibitive, while doing it with BCH is far cheaper.
@_date: 2017-10-29 10:29:40
Last I heard it was "scheduled" for Nov 13th, but I don't think there is any public build available yet.  Which, seeing as this is two weeks away, doesn't inspire a whole lot of confidence.
@_date: 2017-05-29 14:29:16
It is compatible if the 80% threshold is reached and non-BIP141 signaling blocks are considered invalid by a majority of miners *before* the BIP148 USAF kicks in on Aug 1st. Otherwise, you could still have a situation where a majority of miners are accepting non-BIP141 signaling blocks when BIP148 activates, which would cause a chain split. 
@_date: 2017-05-29 10:50:41
This is far safer than trying to strong-arm miners with the USAF, and ultimately accomplishes the same goal of having Segwit activated. Six months is somewhat tight for a hard-fork, but because the scaling debate has been stalled for years, it seems that we are out of time. With a deadline in place, I expect the vast majority to have updated to a compatible version in that time.
@_date: 2017-05-28 12:02:12
Your advice is sound, but realistically, the most likely scenario is that the USAF either wins or dies.
If it gets less than ~12% of the hashrate, it will not be able to activate Segwit in time, and it will almost certainly die. If it gets less than ~20% I also wouldn't be surprised to see active interference with orphaning to prevent transactions from being processed.
If on the other hand it gets more than ~40% of the hashrate, the chance for a reorg on the other chain is large enough that most miners will likely jump ship, and it will almost certainly win.
At over ~20% block orphaning attacks won't be effective, as it would split the majority chain hashrate and risk tipping the scale. Which means that the only situation where you will realistically have two working chains for an extended period is if you get between ~20% and ~40% of the hashrate for the USAF.
@_date: 2017-05-17 20:02:49
It is necessary in the short term. Segwit, meanwhile, is primarily important for the malleability fix, which enables LN, which is hopefully production-ready by the time the average non-witness blocksize approaches 2 MB.
@_date: 2017-05-29 11:47:33
The "Consensus 2017" agreement will in fact activate Segwit as it is currently deployed, it just goes some way towards enforcing an additional hard-fork six months after activation.
@_date: 2017-05-29 13:17:09


True, it provides no extra capacity above and beyond Segwit alone for Segwit transactions. It does however provide a capacity increase for P2PKH and P2SH transactions, both for new ones and existing UTXOs, and most people seem to underestimate how large the scriptsig is compared to the base transaction and output size.
For a median 1 input + 2 output P2PKH transaction of 226 bytes, the scriptsig makes up 105 bytes and would be discounted, while the remaining 121 bytes would not. So with the current block size limit, you could theoretically have 1000000 / 226 = 4424 median-sized transactions. With 2 MB "size" and 4 MB "weight" you could have 6791 transactions, with a total "size" of 226 * 6791 = 1534766 bytes and a "weight" of (121 * 4 + 105) * 6791 = 3999899 bytes. *(Edit: Corrected the scriptsig size)*
The more inputs you have, the better that ratio would look. An average 10-input 1-output sweep transaction of 1524 bytes would have 1050 bytes of scriptsig and 474 bytes of other data, so with the old limits you could get 656 transactions, and with the new ones you get 1312 with a "size" of 1524 * 1312 = 1999488 bytes and a "weight" of (474 * 4 + 1050) * 1312 = 3865152 bytes - thus, filling the 2 MB "size" limit.
But by far the most important part is that if this compromise gets Segwit activated, it will finally allow for realistic second-layer scaling because of the malleability fix it provides.
@_date: 2017-05-17 21:39:28
Understand that there are more than technical risks involved. Such as the very real risk that sufficient people will be fed up with a $5 transaction fee and/or a confirmation time measured in days that another blockchain usurps Bitcoin's current leading position and network effect, and Bitcoin becomes the also-ran of the cryptocurrency world.
There are no indications that a SW+2MB HF is excessive, but if it turns out to be so, it is always possible to reduce the non-witness blocksize or the witness discount with another softfork.
@_date: 2017-05-29 11:56:37
The 80% threshold is to start enforcing signaling for the current BIP141 Segwit deployment. That is, blocks that don't signal BIP141 will then be considered invalid, and the 95% threshold for BIP141 will be guaranteed.
@_date: 2017-05-18 09:30:46


Of course there is no *technological* reason for a larger max block size, no one is arguing otherwise. There are however strong *economic* reasons for a larger max block size, and no technological reason for why it should not be done that are not mitigated by systems that are already in place - specifically,  compact blocks and pruning.
@_date: 2017-05-29 10:52:25
For code complexity and testing reasons, new Segwit proposal cannot start signaling until the current Segwit deployment signaling has expired.
@_date: 2017-07-02 22:35:18
I hope you are wrong, but I guess we'll know by the end of the month.
Seeing as 2x went out of their way to change the activation to have a better chance of supporting BIP148, I feel the only way the support for BIP148 could be reduced by 2x is if it were to activate close enough to the end of the month that the BIP141 enforcement doesn't start in time. In that case, Segwit should already be guaranteed unless a massive miner conspiracy with more hashrate than Jihan controls reneges on the enforcement. And if so, Bitcoin with the current PoW would be more or less done anyway.
@_date: 2017-05-18 09:34:53
"No, we want to pay 400+ sat/byte fees and have days of transaction backlogs" said no economic majority ever.
If miners agree on 2MB+SW, the economic majority will be there. It's just up to the Bitcoin Core devs to push a patch to enable it.
@_date: 2017-05-29 16:04:33


A hardfork *will* split off nodes that have *not* updated. A softfork *can* split off nodes that *have* updated, if and only if it starts enforcing with a minority hashrate. It is certainly backwards-compatible, but can still cause a chain split with insufficient hashrate, since the nodes that have not updated to enforce the softfork will follow the majority hashrate regardless of the softfork rules.


They are both very much softforks as they add additional rules to what makes blocks and/or transactions valid (or, more precisely, what makes them not valid).
You are incorrect about BIP148 being temporary - it permanently specifies that blocks created between 1501545600 and 1510704000 must signal Segwit. (Note what would happen if it were not permanent: on November 15, if it is still a minority chain, it would be reorged into a puff of logic.)
@_date: 2017-05-29 11:43:50
Luke's proposal is effectively Segwit extended to having the same discount rules apply for legacy transactions and Segwit transactions. That is, cheaper inputs than outputs, and a close-to-but-not-entirely-2MB 1.7-1.9MB effective block size for today's average transaction load. This is a Good Thing For Bitcoin™ compared to a plain 2MB increase that doesn't distinguish the input and output costs, because it avoids having the hard-fork contribute to an acceleration in the UTXO size until methods can be fully developed to handle that better (like UTXO archiving and other proposals).
More importantly, it will allow Segwit to activate SOON™ rather than later with BIP149, and far safer than with the BIP148 USAF.
@_date: 2017-05-29 11:53:10
A softfork can cause a chain split if it doesn't have the sufficient 51%+ miner support to enforce the additional rules, which is the primary danger of the BIP148 UASF.
The main difference between a softfork and a hardfork is that a softfork is convergent, while a hardfork is not. If a softfork gets 51%+ hashrate, there will (eventually) be one chain. If not, there will likely be two.
@_date: 2017-05-14 16:28:38
But the situation is not that one side wants a change and the other side wants the status quo. The "compromise" is that if one side says "we want a hard fork that does x" and the other side says "we want a soft fork that does y", then doing them together gives both sides a reason to unify behind the proposal. No one gets everything they want, which is the nature of a compromise, and since the changes have been so heavily politicized, it also gives people an easy out without losing face.
Seeing as either side can unilaterally block the changes wanted by the other side, an actual compromise seems to be the only way to make any progress at this point. While it is certainly true that Segwit is a capacity increase that will take effect as people start using it, enough people are opposed to it that it seems unlikely to be accepted alone.
@_date: 2017-05-19 22:37:26
That's quite a handy tool. BIP141 would have exactly 107 full days to activate after BIP148 kicks in, which means that in a worst-case, to activate Segwit on a minority chain it would need an average of about 12% of the hashpower throughout the period to be safe. (The worst case would be forking either on block  or block  of the current 2016-block difficulty period, depending on whether ~100 blocks with the old difficulty takes more or less than 14 days with the new hashrate.)
@_date: 2017-05-29 11:31:11
As you say, the 4000000 blockweight would apply to legacy transactions, and scriptsigs (parts of the transaction inputs) would get the full Segwit discount. While the median transaction has 1 input and 2 outputs, sweep transactions and other transactions with large inputs would skew the average number and size of inputs, while transactions with more than 2 outputs are less common. And for the block as a whole, the average is more important than the median.
Making it so that creating UTXOs is more expensive than reducing them is, in my opinion, a property of Segwit that is strictly a good thing. The UTXO size *is* an issue that hasn't been addressed, and the distinction makes the hard-fork far safer than it would otherwise be.
@_date: 2017-05-19 17:15:07
You say 8%, but there's no way to measure that with coin.dance or any other metric based on counting nodes, as it is easily manipulated with Sybil attacks. Fully validating nodes are great, but when I say "economic" nodes I'm talking the nodes run by Bitpay, Bitstamp, Coinbase, Kraken, Bitfinex and so on that are central for the economic activity in the ecosystem. These are the nodes you need to have running the USAF if you want to force the miner's hands with a mining minority.
Though it is certainly true that if the USAF does cause a fork, and the hashpower for the segwit fork is only slightly below that of the legacy fork, it would not take much for the segwit fork to overtake the legacy fork and become the majority chain. This could happen any number of times over the 1915 to 3931 blocks it would take to lock-in Segwit, but it would take fairly significant miner support to have any chance of getting there in the first place. (Of course, if the segwit fork has higher hashpower than the legacy fork, the legacy fork will be short-lived if it even forms at all.)
@_date: 2017-07-16 09:07:38
I'll be over here when you are done arguing with your strawman.


Still the same chain up to the fork, the continuation just has a relaxation of the blocksize rule. So this is your opinion, not mine. Longest valid chain wins, as it should be.


I said "miners determine transaction ordering", which they absolutely do. SPV clients also rely on them (and the fully validating node they connect to) for determining overall validity. You want to fully validate? Run a fully validating node.


"Satoshi himself" [intended to hardfork in a blocksize limit increase with little fanfare]( and, as such, obviously would not have considered the blocksize part of that "fraud proof". Are you done appealing to authority now?
@_date: 2017-05-30 23:09:08
This was the case, but after a rather intense yet surprisingly cordial discussion on the Github pull request, it now [looks as if]( the plan has been tentatively revised to have the SegWit2x proposal activate the existing BIP141 Segwit deployment via the BIP91 mechanism. This means that all existing 0.13.1+ nodes would remain fully validating even after the Segwit part of SegWit2x activates.
@_date: 2017-05-23 23:34:25


In a perfect world, possibly, but Bitcoin doesn't live in a technical bubble. If, hypothetically, most major exchanges, online wallets, payment processors and miners decided on a multilateral hardfork of their choosing, you could argue that they were making an altcoin, but it likely wouldn't matter. At that point it seems better to make the compromise, bite the bullet, and agree to a sane hardfork in a reasonable time with reasonable limits that both prevents the increase in quadratic scaling validation time by limiting transactions to 1MB each, and prevents the UTXO set from increasing excessively by differentiating the cost of increasing and reducing it.
@_date: 2017-05-19 17:35:43


Certainly, that one does count actual support from economic entities, but it doesn't necessarily mean that the people who say "yes please" will run a BIP148-patched node. I strongly support Segwit and the miner centralization is by far the worst thing that has happened to Bitcoin, but by design you do need at least 51% miner support if you want to do anything like this reasonably safely.
@_date: 2017-05-21 21:51:09


If it needs 20 weeks to activate, it won't activate. The deadline for BIP141 is November 15th.
@_date: 2017-05-19 15:20:44
Due to the inherent risk involved, there is a very high probability that exactly zero nodes used for actual economic purposes will apply an unofficial patch that could end up rejecting a majority chain. It's a variant of the trolley problem, and not doing anything in this situation will invariably cause the least possible liability.
Which leaves getting at least 51% of miners to run it. Psychologically, that will happen if and only if over 51% of miners believe that at least 51% of miners will run it, as any hashpower put towards the "wrong chain" will inevitably be lost.
Short of a unified statement from the major economic actors, I don't have a high hopes, but I genuinely wish you good luck.
@_date: 2017-07-06 15:28:47


Without knowing the rest of your setup, I can't say where your bottleneck is. All I know is that for me, it's usually the CPU. SHA256 hashrates are not really relevant for this, I'm guessing much of it is spent in LevelDB.


Exceptionally slow storage can be alleviated with more disk cache. This doesn't mean exceptionally slow storage was not the "issue".


It would be interesting to simulate the actual access pattern, but without actually doing that, I would say this is likely true. The "hot set" is only UTXOs that have been recently used, and it seems obvious that over time, this set will not increase linearly with the full UTXO set - among other things because of the blocksize limit itself, lost coins, built-up dust that isn't worth sweeping, and coins in long-term storage. Though if you know of any simulations that show otherwise, I would like to see it.






So which is it? Does your node sync in 5 hours, 2 days, or "weeks"?
@_date: 2017-07-06 19:44:53


Considering that I only do initial sync from other (fast) nodes I control, my guess is that you are hitting inv/getdata limitations. Obviously, if the CPU is not the bottleneck, there is a different bottleneck.


I had a bit of a showerthink about this. The UTXO cache is of course significantly different from say a CPU cache in that there is not a lot of re-reading of the same data going on, and there is no data locality in the UTXO cache itself since UTXOs are cached individually.
Generally, a cached entry will only be read when a transaction is validated for mempool inclusion or a block is connected, and it will only have the existing entry in the cache if it is still hot, which in most cases means it was recently created. So, for the initial sync to keep the same cache hitrate, the UTXO cache size would need to be linear to the number of UTXOs that are created during a given time period.
In other words, except in the trivial case where the cache is large enough to hold the full UTXO set - i.e, assuming that there is actual cache churn - the UTXO cache size would need to be linear to the *peak growth* in UTXO size rather than to the UTXO size itself to maintain the same hitrate.
There are however some additional data locality effects from caches in LevelDB and the OS itself, if two or more "cold" transactions happened to be validated close in time and they were in the same LevelDB block, or the LevelDB block was previously cached by the OS. But the significance of this would greatly depend on how much spare RAM the system as a whole has.


If you think an Atom could fully sync in 5 hours and it took you two days, I can only assume you are running Bitcoin on a toaster. :p
@_date: 2017-05-18 13:03:30
Didn't even state anything about it, outside that it's been a source of contention and distrust ever since it occurred. We just need to move past it at this point.
@_date: 2017-07-11 18:56:23
There are no public block archives of segnet that I can find, but if you are certain that no blocks were actually created by the outdated nodes, I will take your word for it.
This doesn't really change anything however; outdated nodes rejected the main chain, and if any of them happened to be used for mining, a fork would have occurred. It doesn't matter - it was a testnet, and those things are expected - but that is the case for testnet5 as well. Especially considering that this was a particular adversarial condition that is impossible to perform on mainnet.
@_date: 2017-05-23 22:48:24


For one, this would also discount the input sigscript of all existing UTXOs. While the total block weight is the same, the realistic effective block limit with 1MB/4MWU BIP141 alone is still around 2 MB except for transactions with abnormally large witness, and would be 4 MB with 2MB/4MWU.


Segwit has been thoroughly tested, as we all know. I'm sure the additional change for the 2 MB blocksize can be thoroughly tested in the six+twelve months until the proposed flagday activation. And even if the blocksize is increased with a hardfork, it can be reduced again with a softfork before it activates if it genuinely turns out to be unworkable for some obscure reason. 


Absolutely! But all in good time. Those can be done in future hardforks, and it seems that the major economic actors are planning to hardfork anyway. I for one would still want the current Core team in the driver's seat.


I don't disagree on the technical consensus for Segwit. However, with the political situation being what it is and the fees and block fillrate being how it is, a compromise seems to be the safer option. That is, compared both to an uncontrolled hardfork forced by the economic actors and miners together (which would risk being countered with a PoW change), or the possibility of a chain split caused by a contentious push of BIP141/BIP148 by itself.
@_date: 2017-07-10 14:17:22
As I said, there is no point in having a cache if the access pattern is fully random. Then all you care about is having fast access to the entire key-value store. So yes, if you want to naively ignore the temporal locality, just drop the cache and put the key-value store in RAM. It would then be linear to the size of the UTXO size. Not what's implemented in Core, and not relevant since it would be impossible to get sufficient performance on low-memory systems even with today's UTXO size, but there you have it.


So I'll just leave you with this quote from Eric, which you helpfully fished out.














@_date: 2017-07-07 19:06:40
Can't speak for the Ledger, but the Trezor isn't designed for environmental protection. It would probably survive a drop test simply because it is so light, but it would be unlikely to survive crushing or any kind of environmental exposure.
Ledger and Trezor both use standard BIP39-based recovery seeds, which are also supported by several software wallets, and you absolutely need to keep a copy of it. To keep the seed safe from most things except for physical theft, you could consider something like the [Cryptosteel]( 
@_date: 2017-07-02 21:37:39
Edit: I misread your point. Considering that the activation period is 2.33 days, this would have to be done very carefully, and seeing as SegWit2x is potentially compatible with BIP148, there is no reason to not do both.
The main focus point of the article seems to be that "the hardfork will fail". But the only way Segwit doesn't activate is if SegWit2x never gets the required 80% signaling in the first place, meaning there will be no hardfork to fail.
@_date: 2017-07-16 09:15:30
Thanks for proving OP's point.
@_date: 2017-05-29 12:20:23
Going with BU/Emergent Consensus wasn't palatable for most of the community, nor for many of the industry actors. Besides, it's not particularly helpful to point fingers and talk about who has been stalling what; the important thing is finding a way to resolve the stalemate, and right now, this agreement seems to be the safest way to do so.
@_date: 2017-07-06 09:07:25


I'm not sure how this became a discussion about climate change, but as you say, anthropogenic climate change is happening orders of magnitude faster than previous natural shifts. What I'm saying is, it doesn't matter a whole lot if the delta temperature is 1C per decade or 2C per decade, if nothing is done to halt it; then it only becomes a question of which generation it will start to really hurt. And the reason it was an imperfect analogy is that while the goal would naturally be to halt global warming rather than addressing the symptoms, you cannot realistically halt UTXO growth. As such, blocksize only affects how much time you have to improve the system to handle a larger UTXO set.
@_date: 2017-05-17 19:18:20
At this point, the risks of further stalling a blocksize increase far outweighs the risks of a 2MB blocksize bump. It should have been done over a year ago and activated by now, but it wasn't, and now we have 400+ sat/byte fees and a massive backlog of unconfirmed transactions.
@_date: 2017-07-08 13:03:12


Did you even *read* the code before you wrote this? Absolutely nothing of what you say here is relevant for the "cache strategy", and only demonstrates that you do not understand the code at all. It *only* affects the trigger for a cache flush. Specifically, if FlushStateToDisk is called with FLUSH_STATE_IF_NEEDED, fDoFullFlush will be true if and only if fCacheCritical is true, and fCacheCritical is true if and only if the current size of the map used by pCoinsTip is larger than allowed. In this case, pcoinsTip-&gt;Flush() is called, which flushes and clears this map.
@_date: 2017-07-02 21:37:05
Seeing as the BIP is two years old, the first increase would have been January 2017. This is the sort of reasonable proposal that *should* have been merged two years ago.
@_date: 2017-07-07 20:53:59


Uh, no. [pcoinsdbview]( is an instance of [CCoinsViewDB]( and is backed by the LevelDB database itself, it absolutely does not "live in memory". [pCoinsTip]( is the cache I'm talking about, it is a [CCoinsViewCache]( instance which caches UTXOs in an in-memory map. The "sophisticated multi-level cache" you are referring is this UTXO cache and the LevelDB LRU cache itself.
The "validation cache" is not persistent, it only holds entries temporarily while a block is connected. It is backed by pCoinsTip, and flushed to it if ConnectBlock is successful.
There are no other caches involved for UTXOs that are not in the mempool that I can see. Point at where it is the code if you think I'm wrong.
@_date: 2017-07-15 22:05:29
I'm seeing one useless rant, one unsubstantiated ad hominem comment, and one post with a fair explanation on why they chose to not to set it.
You can read about the proper use of the nVersion field [here]( There is no point in setting it for a hardfork unless you want to go out of your way to support a minority chain, and when you take the compatibility concerns into account, the reasons why people who oppose 2x want the hardfork bit should be as clear as the reasons why the people who support it don't. Educate yourself just a little bit before you keep spewing venom.
@_date: 2017-07-06 09:33:36




My last full node sync was to platter storage, happened a couple of months ago, and took about 8 hours with the most recent version of Core. It was pegging the CPU (an E3-1270, roughly equivalent to the faster LGA 1150/1151 Core i7s) starting with blocks from around 2013, all the way through to the end. It just doesn't verify signatures, which speeds it up for sure, but it is a far cry from eliminating CPU usage.


There is no linear increase with blocksize/chainsize and the dbcache option. It is a *cache*, and is only used to reduce I/O load.


USB flash drives can be far slower than internal platter storage. I've seen 200ms+ latency and &lt;5 MB/s throughput, and they overheat easily which throttles them further - and this is likely the case here. "Someone has a setup that syncs really slowly" doesn't mean that syncing is slow in general, and all the RAM in the world would not speed up that particular setup.


The only major mempool explosion prior to May was in March, and while the price was certainly less volatile than in May, it was still up 50% from a month ago. I'm not saying that every mempool spike can be explained by looking at a price chart, but the May one seems to be perfectly attributable to that.


The 7-day averaging does obfuscate it somewhat, but if you look at the 1-day averaging charts for [mempool]( and [price]( the large mempool drop starts around June 3th-5th, which is when the price hit $2500 - which is where we are today. The correlation is certainly strong enough that you don't need a "spam attack" to explain it.
@_date: 2017-07-02 21:02:38
Worst article I've read this year. Disregarding all of the other unsubstantiated nonsense, if the argument is that the hardfork part would fail, how would that in any way stall Segwit? Particularly, how does


in any way, shape or form make sense as a stalling tactics, seeing as Segwit's activation is itself the intrinsic trigger for the hardfork part of SegWit2x?
@_date: 2017-07-15 09:37:38
SPV clients will always follow the chain with the most work. If you run a SPV client, you are already opting in to this behavior and trusting that the majority hashrate knows best. And yes, I'm in the "always run a fully validating node" camp.
A hardfork bit would not entice them to make a choice, it would only make them stick with a potentially dead chain regardless of the actual chain work.
@_date: 2017-07-08 13:42:17
Is is when the data locality of your data access pattern only has temporal locality and no spatial locality. :)
@_date: 2017-07-05 11:04:49


You don't *need* 300MB, it only affects how fast you sync. It is still measured in a few hours with a fast CPU and good bandwidth. How quickly it will become a concern is hard to estimate, but you are talking "in a few years" compared to a "in a few more years".


We'll know more come August, but when Segwit activates, there's no backing out of that.


I don't subscribe to the "spam attack" theory. There was obviously some fee estimation algorithm manipulation going on, but blocks are still mostly full, and the initial mempool backlog built up during a highly volatile period where Bitcoin rose in value by ~200%. Many people were moving coins onto and off exchanges because of that.
@_date: 2017-05-18 20:12:06
While playing chicken with the miners may ultimately prove successful, saying that there is no risk at all for only BIP148 nodes is inaccurate. The only way it will survive as a minority chain for any period of time is if it gets a sufficient amount of support from miners to trudge on until the next difficulty adjustment. If BIP148 were to get 51% miner support, everything is awesome, but it is questionable whether people would be sufficiently tenacious to maintain a long-term minority chain. And if the BIP148 chain is eventually abandoned, those block rewards will be effectively lost, just as if they were reorged away.
@_date: 2017-07-11 20:25:15
I never said the testnet5 fork was *caused* by old nodes, only that old nodes didn't have the rule in place to prevent them from following the chain that is invalid under the current 2x rules. My reference to "outdated nodes" in the previous comment was regarding segnet.
My understanding is that the 2x chain didn't progress for ~29 hours primarily because not enough transactions were available on testnet5 to form a block &gt;1 MB, and there was no fallback to fill it with dummy transactions. This would of course not have been a problem on mainnet for very long. As for why it took them 29 hours to discover this and generate the necessary volume of transactions, I have no idea.
If there were any additional issues preventing the block from being generated that weren't caused by unexpectedly jumping ahead 6000 blocks, I'm not aware of it. While the default mining policy would of course not have generated one, and should have been updated to do so, miners would generally be expected to override this anyway, and [according to jgarzik](


I don't personally agree with the addition of this particular wipeout protection, nor am I planning on running a client that implements it. I strongly believe that the longest valid chain should be determined independently of such kludges, and if the 2x chan turns out to be the minority chain, *no one is going to be using it anyway*. As such, I'm not going to defend any possible quirks caused by the &gt;1 MB requirement outside that a) it's on testnet, and b) except for not having the transactions required to generate the block, the btc1 clients seem to have been acting as intended. 
@_date: 2017-07-02 21:19:12
This, in particular. There is no "firing" involved. This is ostensibly a decentralized system, with decentralized code contributions and decentralized consensus. Any given code repository does by its very nature have to be managed by *someone*, but if control of said code repository is equated with having veto rights against changes that are desired by a vast majority of the system's participants, the system is fundamentally not decentralized.
@_date: 2017-07-05 09:43:42


Ideally, they should have used Luke's "discounted inputs" proposal to make the impact on UTXO growth smaller. However, in terms of systems design, if a (potential) 100% increase in growth is a problem, today's growth is only a slightly delayed problem. This is the main reason why I would reluctantly accept the higher blocksize, since methods to address growing UTXO size need to be developed anyway. It is also one of the main reasons I hope the existing Core developers decide to accept it, since that means we'll have the competence to do so.
@_date: 2017-07-07 13:48:49


I am not describing a "naïve" caching strategy, I am describing the *actual* caching strategy used by Bitcoin Core for the UTXO cache. Not sure if you are a troll or simply incapable of grasping the mechanics, but I am sorry I wasted my time trying to explain how the cache works. I see no point in continuing this.
@_date: 2017-05-29 12:04:23


Only if every single transaction starts using Segwit. After the hard-fork, capacity for plain old P2PKH would be increased as well.


It still changes things for sweep transactions and other transactions with many inputs, which are still common.


I see your point, but the block-weighting rules is a single paragraph worth of text, and people should be able to understand it well enough. Saying that "it's not a 2MB increase since it doesn't allow 2MB worth of outputs" isn't entirely fair, much like saying "it's not a 2MB increase since it doesn't allow for a single 2MB transaction" would be.
@_date: 2017-07-08 13:32:01


Exactly. And all else being the same, when UTXOs are created (and, incidentally, looked up) at twice the rate, it would fill twice as fast, which means it would have to be twice as large to be flushed at the same rate. /thread
@_date: 2017-07-05 08:53:15
Good article. All the talk of split coins is likely disturbing for some people, but from what I can tell, the title for "Bitcoin" will very likely boil down to 1MB Bitcoin with Segwit ("legacy") vs 2MB Bitcoin with Segwit ("SegWit2x"). "PoWCoin" is more likely to be widely deployed in response to "ABCCoin"/"Bitmaincoin"/"VerWightCoin", and all of these face the dilemma that they start off with a hardfork. They may be developed and ultimately deployed, but they are in my opinion highly unlikely to ever gain significant traction.
Fringe elements notwithstanding, I still don't think any sane developers would go to war over 1MB vs 2MB base blocksize, regardless of the connotations for Bitcoin governance. May the longest valid chain win. (Validity specifically not restricted by reasonable changes in blocksize.)
@_date: 2017-07-15 08:44:59
There is no such thing as a "subversive hard fork". Hard forks are opt-in, by definition.
@_date: 2017-07-08 15:24:43
Appealing to authority is not a valid argumentative technique when the source provides no backing for an assertion. Nor is this actually what he is saying. He claims that the temporal access pattern is not "continuous" and not "strictly segregated" when answering a question about why the cache isn't just dropped entirely, thus relying the caching behavior of the OS. He does  not say that there is any spatial locality - which, for a hashed set, there could not be.
@_date: 2017-07-21 19:10:41
I think the argument springs from the fringe theory that the NYA was just a ploy to derail BIP148, and the plausible deniability is not so much in the signaling, but in the fact that after BIP91 activation, it only takes one non-BIP141 signaling block to make the chain invalid under BIP91 rules. Thus, if less than 51% of miners "accidentally" fail to enforce it, you could have a situation where BIP91 activates but still fails because the majority hashrate doesn't follow the chain that is valid under BIP91, and instead of abandoning this chain "because reasons", BIP91 is abandoned instead.
Again, I consider this scenario unlikely, but Todd tends to have a highly adversarial view of Bitcoin as a whole, and it makes sense to be aware of the possibility.
I do however consider it highly likely that we will see several temporary splits and reorgs until Segwit locks in, as this will happen every time someone mines a non-BIP141 block, but I don't expect them to be deeper than one or two blocks.
@_date: 2017-07-10 13:45:46


Huh? There is no spatial locality inherent in address reuse. That would imply that parts of the SHA256 hash of the transaction can in any way be determined from the address alone, which is also ludicrous. Nor is there any temporal locality beyond what you already have by performing the same access pattern on different addresses.




Then you are not talking of a "cache", but an in-memory key-value store that's degraded because parts of it is paged out to disk. I previously excluded this case from consideration when I said


Of course, if your idea of the UTXO "cache" is just having the full UTXO set in RAM instead of taking advantage of data locality, you have a literal tautology - the size of the full UTXO set is *obviously* linear to the size of the full UTXO set. But this is not what is implemented in Bitcoin Core, and if you were taking Cache Theory 101 and suggested a cache that's 50% the size of the dataset and has a 50% hitrate, you would get an F.
@_date: 2017-07-08 13:24:13


Uh, no, of course it is not flushed on every block. You should probably re-read my original analysis from a few posts back. The conclusion of which was, the UTXO cache fills and is flushed at a rate that is linear to the growth of the UTXO size, not the UTXO size itself. And as such, to maintain the same hitrate, you would need to maintain the ratio  of cache size to UTXO growth rate (possibly with a constant factor), not cache size to UTXO size.
If you disagree with this, I really have nothing to add. Have a nice weekend.
@_date: 2017-07-21 09:13:30
If you cared about maintaining plausible deniability, sure, but even if the offending miner starts signalling for Segwit after the chainsplit, it will not fix the chainsplit. Furthermore, since the activation thresholds are different (80% vs 95%), and BIP91 would no longer be in effect, it is then unlikely that Segwit will activate on the BIP91-invalid chain - especially since some miners are then likely to stop signalling for Segwit, assuming it really is some kind of conspiracy.
This is a worst-case hypothetical; I don't think it is in the miners' best interest to crash Bitcoin in such a manner. But it is useful to be aware of the possible outcomes, especially since this is one of the few cases of miner conspiracies that aren't prevented by fully-validating nodes, as they aren't generally running btc1/segsignal.
@_date: 2017-05-23 22:18:31
There has been a debate since 2013, if you haven't noticed, and this not-yet-quite-a-BIP is a draft that allows for another six months to fully settle consensus. Furthermore, presuming that the technical debate has been "settled" and that this is "against better judgement" is fallacious - especially considering that Segwit alone would allow for the same total blocksize, witness data included. There has been no demonstration that a minor increase in the blocksize limit is significantly detrimental, and the combination of compact blocks and pruning means the impact is even smaller than it was just a couple of years ago.
@_date: 2017-07-02 12:30:58
Miner centralization is a problem, but changing PoW to a different ASIC-easy algorithm will only be interpreted as a massive raised middle finger to everyone who have invested in existing miners. Even assuming that you have the power to make people switch to Bitcoin 2.0 in the first place, no one would invest in developing, building or purchasing miners for it since they would have no faith that you wouldn't just change the algorithm again at a whim.
Making it ASIC-hard and run on commodity/reconfigurable hardware would be preferable to needing specialty hardware, and would work to counter the Bitmain situation where you can end up with something close to a monopoly on the supplier side. GPU mining with Ethash or similar memory-hard algorithms have its issues, but that's one option. Though I wonder if you could use a massive DAG or otherwise increase the memory requirements to make it impossible even on GPUs, instead requiring a CPU with 128-256GB RAM?
@_date: 2017-07-16 09:27:23
The "let's hardfork to avoid a hardfork" argument is not particularly persuasive. It doesn't make the minority chain "survive", all you are doing then is make a third chain.
@_date: 2017-05-19 17:25:52
I certainly don't have a "Bitcoin Expert" badge, but Luke understates the risks of running UASF-BIP148. The risk, specifically, is that you fork yourself onto a dead branch. If the mining hashpower for BIP148 is significantly less than 50%, it can take hours for each block to be mined, and it would need *at least* 2016 blocks to have the difficulty fully reset without an additional hardfork. Hypothetically, if the hashpower for BIP148 is 10% while the remaining 90% of the miners are honey badger and keep doing their thing, you would get one block every 100 minutes on average on the segwit fork until the first difficulty recalculation, and then another 2016 blocks at a reduced rate depending on exactly when the fork happens before the final difficulty recalculation and Segwit lock-in.
Ideally, you'd get 51% miner support for BIP148. Then there is little to no risk of a long-term fork. But there is a risk that you won't get anywhere close to that.
@_date: 2017-07-06 14:05:02


How is this relevant? Even if it is faster, it is still CPU-bound.


I'm not talking about the interface bus. USB flash drives, specifically the actual flash memory and storage controllers, are by themselves very much a mixed bag.


It is a cache. Its efficiency is not linear to UTXO size but depends on data locality and the size of the hot set. I'll refrain from covering Cache Theory 101 any further with you.
@_date: 2017-07-21 18:56:32


It is primarily miners who have to enforce BIP91 to avoid a chainsplit. While you could apply the btc1 or Segsignal patches on your fully validating node to enforce it on your end, it's unlikely that enough people would do so in the less than two days before activation to make much of a difference.
You could still join some of the people here in running a BIP148-enforcing client, which would indirectly help by discouraging miners from not signaling Segwit. It's questionable how much that would really matter in the end, but at this point it can't hurt.
@_date: 2017-07-05 10:12:54


It certainly matters, but only when it comes to how much time you have to address it. The rate of global warming doesn't matter if nothing happens to reduce or eliminate it; then it only affects whether or not it'll be a problem for people living today, or whether it'll only affect future generations. 
But unlike global warming, you cannot do anything to *eliminate* UTXO growth; all you can do is make better systems to deal with the fallout, and the rate of growth only affects how much time there is to make such systems come to fruition. 


They also *accepted* suggestions and contributions. Most significantly, making it fully compatible with BIP141 (it was originally a "different" Segwit for no good reason), and making changes to the activation to improve the chance for BIP148 compatibility. Considering the rushed deployment, they seem to have made a choice to take the KISS approach for the minimum-viable changes required for hardfork compatibility rather than using more complex metrics for the blocksize limit calculations, which has both pros and cons.
@_date: 2017-07-16 20:33:50


This happens with every fork, hard or otherwise.


As is your supposition that he intended to add fraud proofs to prevent a blocksize increase from being "covertly" deployed on SPV clients. Don't appeal to authority, and I won't link my token "Satoshi said" post.
@_date: 2017-07-10 22:11:57
Someone who wanted to create FUD.
It's not an "irrecoverable fork". The "problem" is that the wipeout protection of SegWit2x consists of requiring a block at a specific height (Segwit activation + 12960 blocks) to be &gt;1 MB. Someone mined a bunch of blocks that do not follow that rule, and older pre-release SegWit2x clients did not have this rule in place yet. The 2x chain on testnet5 will continue as soon as a &gt;1MB block is mined at the correct block height.
Incidentally, does anyone remember when [Segwit caused a fork on testnet]( about a year ago? Because that was also a thing that happened, and I'm seeing a lot of the people who were adamant that it was a non-issue and that problems are expected on testnet saying quite the opposite now.
Edit: It was pointed out to me that it is disputed whether there was an actual fork on segnet. In this case, it would simply have been a matter of outdated nodes rejecting the main chain, but as people in the referenced thread did believe a fork had occurred, it does not change the rest of the argument as such.
@_date: 2017-07-11 08:58:25


And here it is testnet5, not testnet3.  Not sure how this statement is significant. A long chain considered invalid by updated btc1 clients was mined on the net they were testing stuff. That is all.




How is a fork caused clients with outdated sigops counting rules not a big deal, while a fork caused by pre-release btc1 clients followed a chain that violated rules they did not yet have, in where the updated btc1 clients worked as intended and enforced the &gt;1MB on SW+12960 height rule, somehow an indicator of gross incompetence?


(Just for the record; I don't believe the segnet "fork" was indicative of a problem, in the same way I don't believe the testnet5 "fork" is indicative of a problem. I'm only pointing out the apparent hypocrisy displayed by some people.)
@_date: 2017-05-23 21:19:00












This seems quite clever to me. It implies that producing UTXOs will be more expensive than combining them, even for non-Segwit transactions. It also means that any mined block cannot produce more than the old 1 MB limit worth of outputs, as that would hit the weight-unit limit, while still allowing for the effective 2 MB block limit to be fully taken advantage of as inputs are typically larger than outputs. 
Keeping the WU limit also neatly counters the "but 2 MB blocks would really be 8 MB blocks with Segwit" argument.
Outside of the inherent forking risks, which seem manageable, I believe this is a good compromise and should be acceptable for all sane participants.
@_date: 2017-07-16 13:44:18
Meanwhile, that other sub want to hardfork to get rid of AXA and Blockstream and whatever boogiemen *they* can dredge up.
I really wish everyone would stop treating everything as a massive conspiracy. Economic actors are primarily concerned with the network being able to reliably handle the necessary transaction volume, also in the short term. Miners primarily want on-chain transactions to remain affordable, or at the very least, viable. And the developers have different and arguably more nuanced priorities. It doesn't take a conspiracy to make these priorities clash.
@_date: 2017-07-05 22:54:06


Did you actually sync up a client recently? Unless you are using a Raspberry Pi with network-mounted platter storage, that is off by an order of magnitude.


Do you have a source or actual calculation on that? It will obviously not be less than half the time for 2MB compared to 1MB, seeing as 1MB blocks are currently full, and 2MB won't be initially. Even assuming exactly double growth, it is still linear; i.e, we would reach the "problem level" twice as quickly.


The only situation where it might be I/O limited is if you use platter storage and you are starved on RAM for disk caching. Otherwise, if enough bandwidth is available, it is CPU limited even with a reasonably high-end CPU.


This is provably incorrect. Compare the [price chart]( and the [mempool chart]( both covering a 90-day period with 7-day averaging. The mempool started ballooning when the price started rising above $1200 in early May. Price has been relatively stable in June, and as such, the mempool got beaten back down again.
@_date: 2017-07-15 15:05:49


Unlike fully validating nodes, SPV clients rely on miners to determine some forms of validity as well as transaction ordering. If 51% of miners decide that something is valid, within certain restraints, SPV clients will consider it valid. That's how it works. 


I said a hardfork bit would not entice them to make a choice. Even assuming there are any SPV clients that use nVersion to determine validity, changing it would not *force them to choose*, it would leave them on a minority chain. Which you could equally well argue is "disruptive and unethical", for clients that follow miner consensus by design.
@_date: 2017-07-15 09:23:33
The price is crashing because of uncertainty, not because most people particularly care about who "wins".
The uncertainty can be resolved in one of two ways:
NYA participants could agree to postpone the 2x hardfork in favor of a better spoonnet-based hardfork. For this to have a chance to get off the ground, the Core repo maintainers would necessarily have to propose and explicitly agree to stick to such a plan, with a rough timeline for implementation. Seeing that SegWit2x has a lot of momentum at this point, it seems unlikely that NYA would otherwise yield for promises of a hardfork "eventually", and I have no idea if they would even if Core proposed an alternative fork.
Or, Core repo maintainers could agree to merge the necessary changes to let it follow the 2x chain when the block height for the fork has been determined by the activation time. The problem here is that some devs object to it for technical reasons, while others object to it due to the connotations for Bitcoin governance.
The third alternative is no reconciliation, where we blindly careen into a potential contentious fork, which may or may not succeed. At this point, this starts to look like the probable outcome.
@_date: 2017-07-10 11:35:33
I'm not going to reiterate anything I have said, so I'll just make these closing points.
First of all, if you do not understand that the UTXO cache data locality is 0% spatial and 100% temporal, this is futile.
As the UTXO cache is a key-value store with a key starting with the transaction hash, and LevelDB cache key layout is [grouped on this key]( if it were to have *any* spatial locality at all, it would imply that if an UTXO created by the transaction with SHA256 hash e056451009eb0cbc21a9b01528f3c2693f6a8f78e423793fe3e8ff425bc843a8 were to be accessed, that would somehow affect the chance that an UTXO in a *different* transaction with a SHA256 hash starting with e056451009 would be accessed soon. Such a statement would be ludicrous.
Which means that if the UTXO cache has *any* data locality at all, it is *exclusively* temporal locality. My argument, which you can revisit, is based on this simple realization: because the *cache fill rate* is effectively limited by the allowed UTXO generation rate, which is again hard-limited by the blocksize, it follows that with no spatial locality, the hitrate for the UTXO cache itself is relative to the size of the cache divided by the UTXO growth rather than the UTXO size.
Furthermore, if the cache has no to minimal temporal and no to minimal spatial locality, it would be pointless overhead and should be removed entirely. Access pattern would be more or less fully random, and the entire UTXO set would have to be fully in-RAM or at the very least on PCIe SSDs to get the necessary I/O performance.
Past a given transaction age, you would revert to such a close-to-random access pattern, which, as previously stated, means a cache is pointless.
@_date: 2017-07-21 08:56:52
To avoid a chainsplit, 51% need to be *enforcing* Segwit signaling come BIP91 activation, about two days from now. Even if 95% signal Segwit, if less than 51% enforce it, you will still get a chainsplit if only a single non-Segwit-signaling block is mined.
The concern is that, either through malice or incompetence, sufficient miners are not enforcing it to prevent this from occurring. But if it is truly accidental, it would quickly be marked invalid by the miners who weren't enforcing it even though they said they were. In this case, the chainsplit would be short-lived and the non-Segwit chain would be abandoned and eventually reorged.
It would take a significant conspiracy to have 51% of miners *keep* following the now-invalid chain in violation of NYA, so I find this unlikely. If this does end up happening, I expect that the rest of the Bitcoin ecosystem would respond with a POW change.
@_date: 2017-06-19 12:53:47
You seem confused. Bitcoin isn't a project that can be "hijacked", it is a decentralized consensus system where it takes overwhelming majority agreement to change the consensus. If a new consensus forms and the repo "gatekeepers" for the current de facto reference client aren't interested in following it, people will just change to one that does. But I hope it doesn't come to that.
"Core" is not an entity that can be "fired". Bitcoin is based on volunteer contributions, and most of the developers are independent and speak for themselves. If some of them disagree so strongly with a consensus shift that they refuse to follow it, that is perfectly fine, but if they do that over a capacity increase that has 80%+ support from miners and economic actors *and* significant (yet not measurable) community support, that seems silly to me.
@_date: 2017-06-22 09:01:19
In the current BOLTs, the channel limit is indeed set to 2^24 satoshi, or ~0.1678 BTC, while the HTLC limit is 2^32 millisatoshi, or ~0.0429 BTC. However, they are both 64-bit integers, so my assumption is that the idea is to start low and increase it with softforks as the network matures.
Edit: assume this was made in reply to @_date: 2017-06-09 21:48:28
I realize that you personally prioritize decentralization by keeping the cost of fully validating nodes to an absolute minimum above all else, but I'm curious if there are any forms of Layer 1 capacity increases beyond that already granted by BIP141 you would personally be on board with at this point in time? The details of the hardfork are still malleable, and weight calculations specifically would significantly affect the potential for future UTXO growth.
The only reason there is a SegWit2x in the first place is the perception that Core has been unwilling to take the lead for the required short-term scaling until the time Layer 2 solutions are available, and I'm sure most of the people involved would still welcome constructive involvement from more of the existing Core devs.
@_date: 2017-06-23 21:02:20
I think you replied to the wrong person, but yes. If someone is under the impression that any consensus change makes an altcoin, changing the PoW to counter SegWit2x would leave you with two altcoins and a dead chain that is "Bitcoin".
@_date: 2017-06-17 10:39:33
The only reason there is a competing implementation is that "we cannot possibly do a safe hardfork in 6-12-24 months" has been an ongoing argument for *over three years*, and despite high fees and a massively congested network, your statements are very clear that you do not consider it a problem.


It is possible that you believe these things, but meanwhile, people who actually want to *use* Bitcoin as it was intended to is struggling to do so and/or being priced out of it.
@_date: 2017-06-05 11:42:07
If the work gets done in time and signaling for SegWit2x starts prior to August 1st, and it is a foregone conclusion that it will reach 80% and trigger BIP91 - which will in turn enforce BIP141 - then BIP148 really is no longer necessary. While compatibility would be nice, ultimately, BIP91 and BIP148 have the exact same goal. They just achieve it in a slightly different way.
@_date: 2017-06-12 17:59:59


Fair point, but I still feel that at this stage, Bitcoin is more or less all-in on Segwit. The original Lightning proposal does mention [BIP62]( though it seems to be outdated by now, and I know at least some of those things were already added.
@_date: 2017-06-18 19:28:13
I have read nullc's comments; they are either uninformed, or intentional FUD.
What I can tell you for sure is that the weight not being updated for the pull request at the time was intentional; it was in the to-do, it was specifically marked "WIP", and it was actively discussed in the PR for almost two weeks.
@_date: 2017-06-12 21:50:24


I expect you mean 10 MB blocks? Wouldn't be a problem for me, and I run five public nodes (less for altruistic reasons and more to guarantee reliability and security for the private nodes that only connect through them). But 10 MB blocks is only 30 tps, and we want much, much more than that.
You *can* probably do the world's global payment volume fully on-chain, but fully validating nodes would only be able to run on multi-gigabit servers with heaps of storage, and either a ridiculous amount of RAM or at least Optane-level SSDs to store the UTXO set.
@_date: 2017-06-17 09:21:41




This is likely a misunderstanding on your part. "Segwit weight change" was in the "todo" until the changes in the [WIP 2MB pull request]( were merged. The [second comment]( in the pull request is about the weight not being updated yet.
@_date: 2017-06-18 19:16:23
It is compatible with BIP148 if SegWit2x reaches 80% and lock-in prior to around mid-day on the 29th of July.
When (or rather, if) SegWit2x activates, it will start to reject blocks that do not signal for Segwit just like BIP148 does, but they do not use the same trigger to start this rejection, so compatibility depends on how long it takes to reach the goal.
@_date: 2017-06-13 08:55:58




Like I said, you could just use a normal transaction if you didn't need or want the channel. The capability for normal transactions with a decent fee should still *be* there for the cases where there is no possible existing route for a one-off payment between two parts, and in this very specific case, if they *know* there will be no additional payments or that future payments will be sporadic and one-way, there is no gain from making the direct channel.
Just because you have a channel open with someone does not mean that you have to accept routing requests through that channel, it is perfectly valid to only accept payments destined for yourself. Of course, if neither party is willing to route, you should not open a channel in the first place unless you expect back-and-forth payments between the nodes.


Lightning isn't intended for someone who only ever makes a single payment on the network. I can pretty much guarantee that hubs will not do 1:1 funded channels for random wallet users, and the primary intention of Lightning is not to replace normal transactions entirely but to offload frequent, smaller or repeat transactions from the blockchain.
@_date: 2017-06-23 20:49:52
So what you are saying is that you blindly follow what someone tells you rather than independently looking into things on your own to gain understanding of a system? Because that explains a lot. Thanks for clarifying.
@_date: 2017-06-13 18:18:51


Most people don't buy buy their groceries and their cars using the same payment method, but they also buy groceries far more often than they buy cars. Lightning payments and direct blockchain transactions have different use cases, primarily since making large payments over Lightning requires large amount of funds to be tied down in the payment channels, but it is excellent for smaller transactions and in-store purchases.
@_date: 2017-06-19 14:45:11
And where did they say that?
The btc1 repo has just one singular purpose at this time: to add the new Segwit activation and hard-fork logic on top of the existing Core codebase. Unless it becomes clear that compatible changes will not be adopted by the Core repo, there is no real purpose in accepting contributions or giving commit access to anyone else.
Because the hard-fork activation is designed to ultimately result in a specific flagday, most of the changes do not even need merging. When the activation time has been determined, based on when Segwit is activated, it literally only needs to add a rule that starting on a specific block height, the block weight/size limits are increased to match SegWit2x. [To quote Satoshi](


    if (blocknumber &gt; 115000)
        maxblocksize = largerlimit
@_date: 2017-06-14 10:59:31
You cannot guarantee compatibility and still have BIP91 for activation mean anything. But the ~~activation~~ start of signaling for SegWit2x/BIP91 remains as June 1st in the [merged activation code]( so it is still possible it can activate in time.
For miners, the safe-ish thing is to make sure the Segwit bit is set prior to August 1st. That means they'll be on the majority chain regardless of what happens. I will not make a recommendation as to whether or not they should run BIP148, though if enough of them do, that would be great too.
@_date: 2017-06-12 23:20:18


That would be better solved with live replication of your node's state to a server at a different location, which could take over immediately in case of the master's failure, rather than occasional backups or duplicated storage backends. Could also be done on the virtual machine level if the lightning daemon doesn't have native support for it. (Still doable and cheaper than mining hardware.)
@_date: 2017-06-13 00:23:17


Imprecise wording on my part; it's used for both channel/route discovery and node discovery, so wallet users can use it to find hubs to connect to, and hubs can also use it to find other suitable hubs to establish payment channels with for better connectivity. Though you would of course be free to promote your hub with other methods as well.
@_date: 2017-06-23 19:15:36
So what you are claiming is that the constant named **MAX_BLOCK_BASE_SIZE** is not the "base block size" and does not explicitly restrict the "max size" of the "base block", sans witness data?
Because, yeah, it is, and it does. Is it redundant? Of course it is, like I said, the weight by itself is sufficient. But if you are saying that SegWit2x is 8MB blocks, then BIP141 is already 4MB blocks. Both are factually incorrect, since you actually cannot make a block with 100% witness data.
@_date: 2017-06-01 20:29:15
As long as SegWit2x activates Segwit via BIP141, as is the current stated plan, it will prevent ASICBOOST just as if it were activated naturally. *If* however the plan ever shifts back towards a different deployment of Segwit, it would absolutely be worthy of strong suspicion.
@_date: 2017-06-14 10:51:52
BIP91 for BIP141 Segwit activation was merged; see [PR So it's just the hardfork stuff remaining.
@_date: 2017-06-16 17:56:57
Good writeup. Some minor comments;


It will only produce a (notable) chain split if less than 51% of miners are rejecting Segwit-signaling blocks after August 1st. Otherwise, while it can technically split, it will reorg back to one chain as soon as the Segwit chain gets one more block height than the non-Segwit one, which will be inevitable with more hash power.


The possible exception is if the signaling reaches 80% and lock-in happens before August 1st, but the activation is not happening in time for BIP148. The activation period in the recently merged changes to improve BIP148 compatibility is 336 blocks (about 2.33 days), so if the lock-in is later than 1600 UTC on July 29th but before the end of the 31st, this would likely be the case.
Note that even in the case of false-flag SegWit2x signaling, a Segwit chain is already guaranteed by the SegWit2x activation as long as any of the miners running it are honest, and would require significant (&gt;30%) false-flagging miner collusion and a lack of action for the remaining 20% for it to not become the majority chain - again causing a reorg for the dishonest chain.
@_date: 2017-06-13 20:42:41


It should be obvious, but would this not be simply because the current fees make it uneconomical to send smaller payments with Bitcoin? Try analyzing a sample of blocks from three years ago, I bet you'll get a significantly different outcome.
Which of course means that Lightning won't be able to offload 90% of the payments that are *currently being put on the blockchain*, and as I keep reiterating, I am not saying that additional blocksize increases are not also necessary. The entire point of the original post is only that the blockchain storage does not have to scale linearly with the number of payments you get on the network, and I specifically mention how I was not going to talk about the actual level of blockchain storage scaling required, because this is really hard to estimate.


You are conflating different metrics. Lightning does not eliminate the need for blockchain transactions, but it lets the same transaction represent potentially thousands of different payments. Let's take a small hypothetical closed-circuit network with a store, its supplier, an exchange, five reliable payment hubs and 100 customers.
The 5 different hubs, knowing that a store channel will be popular, each open a 10 BTC payment channel to the store. The store closes them all at the end of the month to free the coins, then does a blockchain transaction to pay its supplier. Total number of blockchain transactions: 5 close, 5 open and 1 large payment from store to supplier.
Each customer opens a 1 BTC payment channel just once to any of these hubs, then go grocery shopping at this store 10 times for various stuff during a month. They *leave the channel open*. Total number of blockchain transactions: between 0 and 100, depending on how many customers need to open a new channel.
The store's supplier wants to sell the BTC for fiat, and makes a blockchain transfer to the exchange. The exchange uses those BTC to open 5 10 BTC payment channels to the 5 hubs, while closing the old ones. These channels are used to route payments back to the store's customers when they re-purchase their spent coins. Total number of blockchain transactions: 5 close, 5 open and 1 large payment from supplier to exchange.
(I only use fiat here to preserve a closed-circuit BTC network in this example; alternatively, you can assume that all of the customers are employed at the supplier who pay them in BTC directly, and would do so just like the exchange does.)
So what do you get? If every customer already had the channel open, all of this happens with just 22 blockchain transactions, and even if all customers have to open a new one, you can do it with a total of 122.  
The alternative? In addition to the large payments, 100 customers do 10 blockchain transactions each to buy goods, and one transaction from the exchange to refill their balance, for a total of 1102 blockchain transactions.
Realistically, of course, nodes may not be reliable, or additional open/close transactions may be necessary to rebalance channels. This is a small exaggerated example, but it still illustrates the potential for reducing the need for blockchain transactions.


And Rome was not built in a day either.
@_date: 2017-06-14 11:37:44
I can't say for sure, I read it pretty closely back in PR and I can't recall anything missing that isn't in PR The BIP91 activation is there at least, and that's the important thing.
@_date: 2017-06-23 19:04:32
It's [here]( and [here]( in the Core repo, and [here]( and [here
]( in the btc1 repo.
The reason people say there is just "one limit" is that it is mathematically impossible to construct a block &gt;1MB size but &lt;4M weight because of how weight is calculated.
@_date: 2017-06-07 10:36:35
A Trezor, or any other hardware wallet with a display, will securely display the address you are sending coins to. While it is certainly still possible to intercept and substitute the original address depending on how it is transmitted, it 100% protects against clipboard substitution and allows you to manually verify the receiving address via secondary methods like a phone call for larger amounts.
@_date: 2017-06-13 00:54:15


If the receiver doesn't have sufficient balance on the counterparty side of the channel to cover the payment, this is correct. In this case, the Lightning payment would initially fail as no route could be found, and the sender would open a new payment channel to the recipient by making a new funding transaction on the blockchain, then send the payment through that. (Or just pay with a regular transaction, if they didn't want to open a direct channel.)
But a single-funded channel isn't more or less useful than a double-funded channel - whether there are enough funds available to cover a Lightning payment doesn't depend on how it was initially funded, only what the balance of the channel is right now. And after someone has used the network for a while, chances are they'll have a channel open that can handle a given incoming payment.
@_date: 2017-06-19 18:45:34
Yup. Which means that non-mining clients that aim to be compatible only have to code in the weight/size changes at that block height, they don't have to concern themselves with the rest of it.
@_date: 2017-06-14 11:27:47


You are guaranteed a chainsplit in that case. The only way a non-BIP141-enforcing fork will remain alive is if 51%+ of miners keep accepting blocks that do not signal BIP141 after BIP91 starts enforcing it. My guess is that the remaining miners would orphan the non-BIP141 fork, as that is the fork in danger of being reorged, and Segwit would activate. And if that is not the case, we effectively have a 51% attack on Bitcoin and should immediately change the PoW algorithm.
Regardless, this is not a worse outcome than the BIP148 chainsplit or the ASICBOOSTCoin that BitMain are harping on about.
@_date: 2017-06-23 19:29:10
For me? All my nodes run Core 0.14.1 or 0.14.2 at this point in time, and I'm primarily in it for Segwit and the chance to break the current stalemate. Either way, this kind of pedantry is entirely pointless, seeing as the limit will still very much be there even after BIP141 activates, even though it is not necessary.
@_date: 2017-06-19 12:15:47
It is currently maintained at the [btc1 github]( by Jeff Garzik, one of the very earliest Bitcoin developers, but if adaptation is overwhelming, I remain hopeful that necessary changes will be merged into Core as well.
@_date: 2017-06-30 22:34:04
You can see the discussion leading to PR in [Issue  PR seems to be a fix for a seasoning discrepancy.
@_date: 2017-06-04 17:37:14
Large price movements tend to trigger more transactions as people move their coins to and/or from exchanges. The last 3-4 weeks were quite volatile, and weekends haven't been long enough to eat through the backlog, but a week of relative calm followed by this weekend means that currently the backlog is largely down to the &lt;60 sat/byte transactions that have built up over the last few weeks.
@_date: 2017-06-19 12:32:55
There is a significant difference between Ethereum and Bitcoin in that Ethereum retargets difficulty far faster than Bitcoin does. While ETC readjusted almost immediately, "old" Bitcoin at &lt;10% hash rate could be chugging along at one block every two hours for almost a year until the first difficulty retargeting.
Edit: Looking up some numbers, with the ~6% hash rate ETC has compared to ETH, you would have a block period of 166 minutes until the first difficulty readjustment. Worst-case, this would last for 2016 blocks, or 232 days.
@_date: 2017-06-13 09:21:25
Thanks for weighting in. Just a quick follow-up question to check my own understanding:


In this case, as long as the counterparty node publishes the latest commitment transaction, it would just be the pending HTLC amounts on that channel that are lost. If at this point the counterparty attempts to publish an outdated commitment transaction, it would have to be handled as a revoked transaction close in time to avoid risking more. Correct?
@_date: 2017-06-23 18:55:49
He set an arbitrary 1 MB limit to prevent potential spam attacks on the then-immature Bitcoin network, with the intention of increasing it [about 360000 blocks ago]( The reason it's still there has nothing to do with Satoshi, since he disappeared in the meantime.
@_date: 2017-07-02 22:08:11


This is of course possible, but it wouldn't be the hardfork failing, it would be the activation (and agreement) itself.
If this were to happen, it would be blindingly obvious that Jihan openly reneged on a very public agreement and is acting in bad faith, and would almost certainly cause widespread support for a PoW hardfork. Contingent on this outcome, I would certainly be supportive of that as well.
@_date: 2017-06-13 18:34:51


It really doesn't. Obviously, a "hub" with only incoming single-funded channels would not be very useful, so yes, it does need the previously mentioned seed funds to establish channels of its own to other hubs/well-connected nodes. This is by design, and well documented. But a wallet user still does not need any incoming channels or dual-funded channels to both send and receive payments, it just means they are limited to receive the amount of funds they have previously sent with the channel.
And keep in mind that dual-funded channels are not the only option - the other node can open a second single-funded channel back to you if it decides (using some metric) that it would be worth-while.


Again, no. It can just reject any routing request that it does not have funds to cover, and as we discussed elsewhere, Lightning is not intended to have very large transactions routed through it.
@_date: 2017-06-05 13:25:42


When BIP91 locks in, it provides the same guarantee as BIP148. The activation is really very similar, except it uses BIP9 signaling to trigger instead of a flagday. Compare: [SegWit2x]( and [BIP148](


It is true that if no bit 4 signaling were to start until July 21st, it would likely not activate in time to remain compatible with BIP148. However, the start of BIP9 signaling for SegWit2x in the pull request is currently set as [June 1st]( (already started, in other words) so hopefully signalling will start sooner.
@_date: 2017-06-17 17:23:47


It's FUD. "Segwit weight change" was in the todo-list for the repo, it was never merged without the change, and the [second comment]( in the pull request is about the weight not having been updated yet.
@_date: 2017-06-19 14:51:44


It doesn't have to. [They only have to make a small change to remain compatible.](
@_date: 2017-06-13 00:18:32


Lighting operates both on single-funded and dual-funded channels, but both types of channels are bi-directional. Typically, a Lightning payment hub would not want to bind up any funds in a payment channel to some random wallet user who connects to it, so these will almost always be single-funded - that is, the wallet user fully pays for the funding transaction, and will be able to spend up to that amount through that channel.
But because it is *bi-directional*, when the user then goes to an exchange to buy some more BTC, or gets paid in BTC from someone else, the funds can "flow back" through the same payment channel which was originally opened, assuming that the user has previously spent enough funds on the channel to cover the "return flow".
Because of this, a wallet user can use a payment channel both to send and receive funds even if no one has ever opened a payment channel to the user or committed any funds to him.
@_date: 2017-06-17 17:42:28
It is [tagged](
It's properly signed anyway, so I wouldn't worry.








I didn't download/test the other builds as they aren't relevant to me, but here are the [signed SHA256 hashes]( As long as they match these, they should be good:
    dd877bc247efa4c90a34ec9ce1a497a8ae1f7eac4c688aa8c8b25ffe30c20541  bitcoin-0.14.2-aarch64-linux-gnu.tar.gz
    f273eb5e56694fe5baecdd5ee8cda9ac495541ccd9df5ca1c22a1b10dc6d89e8  bitcoin-0.14.2-arm-linux-gnueabihf.tar.gz
    1a302092d9af75db93e2d87a9da6f1f2564a209fb8ee1d7f64ca1d2828f31c03  bitcoin-0.14.2-i686-pc-linux-gnu.tar.gz
    a4e98906b4fa08727cbd81371a108ed7a19ea34bb421b078e64843560490a78d  bitcoin-0.14.2-osx64.tar.gz
    463277b9139e890a713034b539583a0879bdcf0fc94c3c1fc08bb8aab81bb108  bitcoin-0.14.2-osx.dmg
    1ac4e5ce51ac03c41df0ad1e759dbb55d91e1456b9a616e43344bf2258dbe8ca  bitcoin-0.14.2.tar.gz
    522bf19ff2ac1a3f100194914071cf6d1a15076268c5c847b2f891277f427af6  bitcoin-0.14.2-win32-setup.exe
    b3b0cc67a9a602fee4a270efc154f4422e1e49e2aefd9b0d44b0c601a326b05a  bitcoin-0.14.2-win32.zip
    3a0057e4d6ca172566a93192234ef28916cbb052db9e15997569d9c08502c49a  bitcoin-0.14.2-win64-setup.exe
    8a2a5657a8b3243ab4f549bb4b16c8c34b47acfb5c6bc07eb0b875ee71a3ad5b  bitcoin-0.14.2-win64.zip
    20acc6d5d5e0c4140387bc3445b8b3244d74c1c509bd98f62b4ee63bec31a92b  bitcoin-0.14.2-x86_64-linux-gnu.tar.gz
@_date: 2017-06-13 00:01:27


I think we had this discussion already, and the response was that nodes don't generally earn you BTC, and whatever earnings you get sent won't necessarily remain the same in BTC if the value of BTC changes.


I don't agree with Luke, and I don't think the non-technical  user is a good fit for running a Lightning hub, nor is that the intention. But the non-technical user isn't going to run a fully-validating node either, they just grab a wallet from the App Store.


The costs for the VPS itself would be negligible. But similar to your argument about the cost of running nodes, someone who's been in Bitcoin for a while can fund a Lightning hub with 1 BTC in "seed channel money" without blinking.


A decentralized routing announcement protocol is a part of the Lightning spec; see [BOLT-07](


Not sure how they realistically could, unless you make a separate entry for "AML-violating Lightning Payment Hub Income" on your tax statement.


As long as it enables a non-contentious Segwit activation, I'm personally more than fine with a 2 MB hardfork. 




No argument there.
@_date: 2017-06-12 21:32:52
I'll take a crack at parts of your wall, but some of these are really questions for an actual dev rather than someone who skimmed the BOLTs, so maybe ? 




From my understanding, going offline in that case will not cause a loss of funds outside of that specific HTLC, though it would close the channel. HTLC timeouts are described [here](
Generally, any node that acts as an intermediary should be online as much as possible, and I actually don't expect normal "wallet users" to be acting in that role - most likely there will be Lightning hubs, and those will be primarily located in data centers. The main reason is the [channel failing procedures]( primarily for handling attempts to close a channel in a "wrong" way.
Lightning is however designed to have outsourcable enforcement for the former, but I'm not sure if this has been fully fleshed out yet. [This article by Rusty addresses it to some extent.](


I don't expect the fees for single payments to be significant, and if they were, people would just use a normal transaction instead. Fees and connectivity would be the main factors for competition between different Lightning hubs, and the fact that there will be competition means that you should expect the fees to be close to the operating costs and fund opportunity costs for the nodes.


Lightning uses onion routing by default, so I don't expect this to be a problem. Although I really cannot speak for whether AML would apply to something like a Lightning hub, as that would likely vary from country to country.


Which brings us to the "what blockchain storage scaling does Lightning need to scale and operate safely" point above. If several large Lightning hubs were to go offline at around the same time, the blockchain would need to be able to handle this, preferably with some sort of dynamic limit that was able to swallow those spikes of traffic.


I cannot find a quote at the time, but I believe I read somewhere that you could derive all the required data from a single key that could be backed up. Don't quote me on that, though.
[This answers some of the other stuff.](
@_date: 2017-06-14 11:15:41
It will always be true that BIP91 may not activate prior to BIP148, but as long as it locks-in prior to August 1st, I would stop running BIP148 since it's already guaranteed that BIP141 will activate.
Incidentally, SPLITPROTECTION only protects miners from mining on a soon-to-be-orphaned chain if the hash rate is only slightly in favor of BIP148, and this could be deployed separately regardless of SegWit2x.
@_date: 2017-06-13 23:44:33


For groceries, I probably do. A couple times a week, plus an occasional visit to pick up something extra, can easily add up to ten times in a month.


It is just a quite idealized illustration on how it could make a huge reduction in blockchain transactions. Of course, in a real setting it's highly unlikely that you could get a 1:50 reduction, but if used correctly, it should still be significant.


Depends if you count the Holy Roman Empire, which was in no way Holy, nor Roman, nor an Empire...
@_date: 2017-06-12 23:02:33


The problem with this is of course that people who were using Bitcoin  wouldn't be able to send a transaction to people who were using Bitcoin 
@_date: 2017-06-05 11:52:25
There are a lot of uninformed comments, but the exact details of the hardfork have still not been set in stone. The code in the current pull request is mostly about force-activating BIP141 using the mechanism in BIP91, if and only if 80% of blocks signal bit 4 for SegWit2x using the mechanism in BIP9.
@_date: 2017-06-23 20:18:05
I read the code, where the code explains it in plain code. Do you code?
@_date: 2017-06-19 13:12:08
Pretty sure no one is arguing otherwise outside of rbtc. That said, the block size increase isn't even the best part of Segwit, the functionality enabled by non-malleable transactions is.
@_date: 2017-06-23 19:33:50
It doubles all the limits. The sigops limit, the weight limit, and the (redundant) size limit. Exactly what it says on the tin. Any additional wordplay is unnecessary.
@_date: 2017-06-27 17:51:14
It cannot, and hard-forks are by definition opt-in. The question is whether or not there will be a viable hashrate for the non-2x chain when it forks, and if there is, whether there is any economic support for it. If the block period for the non-2x chain is measured in hours, I expect most people to jump ship in short order.
OTOH, if the 2x chain ends up with &lt;50% hashrate there will be no fork at all, since the 1MB chain remain the longest valid chain. And if the hashrate is only slightly higher than 50%, it will be in danger of reorging. In other words, it will only be successful if the miner support is overwhelming.
@_date: 2017-06-08 18:01:43
It does. The current plan for SegWit2x is to activate the existing BIP141 Segwit with the BIP91 mechanism after 80% signaling for SegWit2x is reached, and then do a 2 MB hardfork six months after Segwit is activated.
Of course, no one could force you to run the follow-up hardfork, but considering the widespread support for the agreement among miners and economic actors, I'm cautiously hopeful that it will ultimately be successful. And if nothing else, it seems likely that we will have Segwit active this summer.
@_date: 2017-06-30 13:45:02
I'm not saying it isn't "functioning", and like I said, I have been using it for all these years. But unless we can move past the stalemate, it is in danger of being overtaken by other blockchains. The ones you find out there certainly have both advantages and disadvantages compared to Bitcoin, but what Bitcoin has going for it is primarily the first-mover advantage and the network effect that came from that. If this is squandered by excessive infighting, that would be an incredibly sad way to go.
@_date: 2017-06-18 08:50:54
No, he didn't. From the [post you quoted](






Which is factually accurate. If there is no Segwit scaling factor, there is no difference between "weight" and "size". Neither is the comment about "extra witness space" wrong the context of removing the scaling. But he is not saying they should, just that some people have raised it as a possibility. He doesn't seem to agree that it is a good idea considering his opinion on Luke's 2MB proposal, and neither do I.




Please stop spreading FUD.
@_date: 2017-06-01 12:11:50
If the exchange has added Lightning support, you would most likely be able to have the coins sent to your wallet as a Lighting transaction, either by making a new Lightning channel or, if one is available and sufficiently funded, on a channel that is already open.
As Lightning channels require the coins involved to be committed in an on-chain transaction before they can be used in a Lighting transaction on the channel, the latter may only be possible for smaller transactions, if at all. Someone would have to have an open channel to you with enough coins committed for someone to make a Lightning transaction to your wallet, so for most people, it will initially be more useful for sending payments rather than receiving them.
(Clarification edit: Lightning channels are bi-directional and can be both single-funded and dual-funded, so the coins could be committed by either side, but a channel could not by itself receive or send more coins than were initially committed. Someone would necessarily have to have funds committed in an open channel to you for you to be able to receive more coins than you started with.)
@_date: 2017-06-14 11:01:35
It hasn't included Frankensegwit since the original uproar prior to PR it was changed back to BIP141 Segwit a couple of weeks ago. The code to force-activate Segwit was merged and is [right here](
@_date: 2017-06-01 11:14:51
It is still user-activated, but could end up with an average blocktime of infinite unless miners jump on board.
My magic 8-ball, which is not entirely reliable but has done fairly well in the past, says that the BIP148 USAF will only be successful if the SegWit2x proposal is deployed with 80% activation on time and starts enforcing BIP141 via BIP91 prior to August 1st. (It is indeed more verbose than your average magic 8-ball.)
@_date: 2017-06-23 19:22:24
As far as I am concerned, since Satoshi so casually described how you could remove or increase the spam limit, removing or increasing the spam limit does not an altcoin make.
@_date: 2017-06-01 20:21:40


The post is about additional steps to address ASICBOOST beyond what Segwit already does. Please do not overreact.
@_date: 2017-06-10 09:33:40


While the flexcap and dynamic block size stuff could certainly be technical superior solutions, it's been a year and a half since that post, and large parts of the ecosystem are primarily concerned about the congestion that is taking place right now. While the utility of a well-secured blockchain will of course mean that all the available capacity will tend to fill up over time, right now there are long periods where no economic solutions are available to use Bitcoin, and the pressure from other cryptos is definitely on.


This is good stuff. The smaller sweep transactions alone would be worth it, and as a network-level CoinJoin it would be great for fungibility as well. I sincerely hope you are continuing work on this.


I try to not assign people malicious motivations unless it is bleedingly obvious that they are not acting in good faith. I cannot speak on the people hiding behind the curtain, but jgarzik at the very least seems like a reasonable person who can be persuaded with well-founded technical arguments, even though he is bound by the specific terms in the "agreement".


While I don't think that was ultimately helpful, when "Segwit and then 2 MB" is the essence of the thing, I can at least understand why they would make that a requirement without necessarily being motivated by any malice. I respect your dedication to focus on technically superior solutions alone, but Bitcoin no longer exists by itself in a purely technical bubble with no realistic competition. My rationale for tentatively supporting SegWit2x is primarily that it seems to be the best chance get over the stalemate and activate Segwit in a short time frame, as it still seems safer than BIP148, while the BIP149-based deployment would take over a year to lock-in. 
However, personally I would greatly prefer a safer Spoonnet-style solution for the hardfork  and/or weightings and discounts based on Luke's proposal to prevent excessive UTXO growth by making scriptsig or full inputs cheaper rather than a blind increase that would also apply to outputs. Though I certainly understand if you don't wish to lend that project any credence, especially on the time scale we are talking of, it is with things like that I believe Core could still make a difference.
@_date: 2017-06-12 22:34:18


It is "better" because while scaling with transactions on the blockchain means that every single node has to download and validate every single transaction, scaling with payment channels means that only hubs that are directly involved on the path of a transaction has to do so. It's second-layer sharding, effectively. And like I said, we're not talking 2 MB blocks or 10 MB blocks for global scaling, but 500+ MB blocks.
It is also "better" because payments are near-instant, which is very relevant for things like in-store purchases (like coffee!) - assuming of course that you have a payment channel with available funds already opened. You wouldn't want to sit around and wait for the next 500 MB block for your coffee transaction to be included, as it could literally take anything from a second to 2+ hours even if blocks had plenty of space.
Lightning would arguably have centralizing factors, but it would be centralized in the same way that the Internet is centralized. There is a tendency towards large hubs, but there are always alternate routes, and any damage (misbehaving and/or poorly performing hubs) will be routed around. And unlike the Internet, anyone could fire up a Lightning hub on some VPS somewhere.


Again, onion routing.


Outsourcable enforcement for revoked transaction close handling, not the node operation itself.


Thus the "what blockchain storage scaling do we need until Lightning is ready" above. Again, not arguing against short-term blockchain increases, just better long-term scaling than "dump all payments in the world on the blockchain".


It does do those things. See [BOLT-04](
"Intermediate nodes forwarding the message can verify the integrity of the packet, and can learn about which node they should forward the packet to. They cannot learn about which other nodes, besides their predecessor or successor, are part of this route, nor can they learn the length of the route and their position within it. The packet is obfuscated at each hop, so that a network level attacker cannot associate packets belonging to the same route, i.e., packets belonging to a route do not share any identifying information."


I never asserted either of those things.
@_date: 2017-06-12 18:43:01
I don't disagree that keeping the fee reasonable with short-term scaling may be necessary. The point is that long-term, if Bitcoin is to scale up to where everyone can indeed use it as a day-to-day payment solution, just putting all the payments on the blockchain is going to create one big-ass blockchain.
@_date: 2017-06-14 12:16:21
I read BitMain's ASICBoostCoin document and it makes no sense to me. That's a completely different thing than SegWit2x, and if they want to fork off a minority chain of their own, I guess that's their prerogative?
@_date: 2017-06-01 22:03:33


While it is good to keep in mind that this is a possibility, it seems fair to not play the conspiracy theory card until the exact details for the hardfork start to get hammered down. If there is even a whiff of any attempt to keep covert ASICBOOST functional, there will be riots.
@_date: 2017-06-23 20:25:44
I'm sure he realized exactly what he was saying, considering he added the limit and created Bitcoin in the first place. Maybe you should think about it for a second, and realize it's not as simple as "any consensus change is an altcoin". Bitcoin is whatever the majority consensus considers Bitcoin to be.
@_date: 2017-06-20 21:54:34


Not just miners, there's a [list of supporters]( which includes major economic actors like BitPay and Coinbase. As for user nodes rejecting it, while the code is public and you could build your own, official binaries are not even available yet.


Some of the core developers, certainly. There is no real way to measure "user node" or community support, but from what I'm seeing here, it varies a lot. And seeing as many of the major exchanges are signatories, saying that they are "very upset" in general is incorrect.


Ownership? Of the code? This makes no sense. No one stole the Github repo, and the code is open source anyway. "Remain in control of consensus" would be a better way to phrase it.
There is of course always the possibility that the hardfork will fail. But ideally, when the flagday for the hardfork has been determined, the current "Core" maintainers will yield, merge the dozen or so lines it takes, and release an update that follows the new consensus. Some of them may want to fight it out to the bitter end, but this would be far more detrimental for Bitcoin as a whole both on the short and long term compared to accepting the demand for larger blocks from the overwhelming majority of miner and economic actors.
Assuming that the signatories are committed to the agreement, it would also be unlikely to change the final outcome. If the fork succeeds, the ultimate difference in the end would then be whether or not you can still use the "Core" client to transact what the majority now considers to be Bitcoin.
There is still a very real chance to move forward together on this, and I for one would much rather have the existing Core developers still on board. By the time the actual signaling (as opposed to the current intent-to-signal) for SegWit2x has reached the necessary level and the time of the hardfork has been determined, we should know whether this is a possibility.
@_date: 2017-06-21 16:24:36
It is theoretically workable if both coins support the necessary mechanisms. Even if this is the case, there are some disadvantages over a sidechain; mainly that it wouldn't be a two-way peg, so you'd be exposed to the additional volatility for the altcoin. More importantly, you would need a significant network effect to get any kind of liquidity in this kind of exchange - even more so for than for a normal exchange, since traders risk having funds locked for days if the counterparty reneges.
@_date: 2017-06-23 18:44:53
So instead of following a 90% consensus scaling hardfork you'd rather be doing a POW hardfork instead? Have fun with your altcoin, I guess.
@_date: 2017-06-19 10:27:41
Assuming that they manage to stick to the schedule, it would start rejecting blocks that don't signal for Segwit around July 25-28th. You then need at least 95% of a full difficulty readjustment period of 2016 blocks for Segwit to lock-in (so between ~1916 and 3931 blocks), then another period of 2016 blocks for it to actually activate. Which means - again assuming that things go as planned - that Segwit should be active 4 to 6 weeks after SegWit2x activates, or late August/early September.
@_date: 2017-06-08 18:14:10
The date August 1st is only really relevant for BIP148, which is something else entirely. Unless you are a miner or run a fully-validating node, you don't have to do anything for that. 
For Segwit itself, you only have to update to a wallet version that supports it if you want to send or receive Segwit transactions, and for Copay I'm guessing this would be more or less automatic. Doing nothing will work just fine too.
@_date: 2017-06-28 09:11:09
I don't see anything like that in the currently merged code, though there appears to be some active discussion about it. I don't consider wipeout protection necessary, because it would indicate that the support for the hardfork was not actually there in the first place, but we'll see if they release with it or not.
@_date: 2017-06-23 20:53:57
That is incorrect. As I keep telling you, yes, the block size limit is redundant because of how weight is calculated. There is however still the block sigops limit, which is also doubled. If you read the code, you would know these things, and I will waste no more time trying to explain them to you.
@_date: 2017-06-21 13:56:06
The problem with using an altcoin is that you'd need a trusted third party to exchange between the coins. In the future you may be able to do this with a sidechain, but the trustless two-way peg problem is far from solved. One of the proposed solution, the drivechain, has significant problems - enabling coin theft with 51% miner collusion, for one.
Lightning on the other hand is of course nowhere near ready for use yet, but it is the most promising long-term solution at this point.
@_date: 2017-06-12 22:47:48


Well yes, that was the entire point of the original post - if Bitcoin were to scale to global transaction levels, it would be preferable to do something more clever than dump everything on the blockchain. 


Keep in mind that people aren't actually paid to run a node, so the price of a Bitcoin doesn't factor into it as much as you might think. Of course there will be some correlation between "has lots of spare money to run a node", "is into Bitcoin" and "Bitcoin is worth a lot", but generally it's a bit of a strong assumption to make.


If it were up to me, we could do that tomorrow and have this discussion again in 2021. :)
@_date: 2017-06-23 20:21:48


Which part of "it doubles all the limits" is it that you fail to grasp?
@_date: 2017-06-30 22:16:34
You must have been sleeping in a cave for the last month, since the code repo has been public with active discussion since the end of May.
That said, most of the changes aren't necessary unless you are using it for mining around July-August. The date Segwit activates will determine a flagday, and compatible implementations really only need a small patch to bump up the max block size, block weight and block sigops at that specific height.
@_date: 2017-06-12 23:33:13


Yeah, the Ethereum blockchain is already growing at a ridiculous rate with minimal real economic activity because of all the contracts that's dumped on it. It's already larger than the Bitcoin one by quite a bit, so we'll be getting a preview of the potential pains of pure on-chain scaling there.
@_date: 2017-06-30 12:16:58
I am a long-term (since 2013) hodler, user and supporter of Bitcoin.
I support SegWit2x as a good-faith compromise to overcome and move on from the current stalemate that has lasted for over three years. At this point, the only thing it needs to guarantee success is the support from the repo maintainers of the current Core reference client, and even without it, it will still succeed if the signatories stand by their statements.
Do not presume to speak for "users" in general when you speak against it.