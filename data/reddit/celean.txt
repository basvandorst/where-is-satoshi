@_author: celean
@_date: 2016-08-27 18:22:12
The 15 minute timeout is just for the transaction to be broadcast. Almost as soon as you send it, it will normally take you to a different screen that tells you to wait for a confirmation, which doesn't have a timeout.
If it's not taking you to that screen, then either the transaction isn't reaching the network for some reason, or Bitpay is experiencing some kind of issue. Paste your transaction ID into a blockchain explorer to see if any of them have seen it.
@_date: 2015-12-30 14:44:39
Frame it as you wish, but it was still a planned hard fork, and older clients are now running a different fork.
@_date: 2015-12-30 13:34:27


Also known as a "hard fork".
@_date: 2015-12-30 09:19:48


[This is an outright lie.]( (Edit: Also on [bitcoin.org]( )


As do soft forks. The difference being that hard forks would be more noticeable.


And adding a cap on sigops as a function of block size would be orders of magnitude easier than implementing SW across all the available software, as it would be purely a node tweak that doesn't affect any wallet software.
@_date: 2015-12-19 19:20:18


So when are you leaving? Bitcoin was never intended to be limited to a 1 MB blocksize perpetually, as was firmly stated by its creator, so you and the other devs who insist on stonewalling its increase or removal should follow your own suggestion and go make SettlementLayerCoin instead of hijacking this one.
@_date: 2015-12-30 09:35:58
All good points, except.. they are either lies or misleading.
First of all, there was an intended hardfork back in 2013, on May 15th to be exact. [Here is an article]( as well as [the official warning](
Whether you do a soft fork or a hard fork, a node operator will still have to upgrade to properly see or process transactions. The difference is that it will be plainly obvious if you got left behind with a hard fork, while unsupported soft fork transactions will be much less so. A SW transaction, for example, will just appear as a "anyone-can-spend" transaction for software that doesn't support SW, and you wouldn't be able to spend it.
Finally, it would be much simpler both for the node software and for the Bitcoin network as a whole to add a (soft?) sigops cap derived from the block size as a preventive measure for the potential CPU exhaustion attacks. This change only involves node software, unlike the SW change which will require updates to every wallet software currently in use.
@_date: 2015-12-21 23:30:13
So where are the people who actually matter the most, like Gavin Andersen and Jeff Garzik? I also find it funny that Peter Todd didn't sign, but I'm not sure what to read into that.
You can post all the road plans you want, but unless you actually desire a catastrophic breakdown in the overall reliability of transactions on the Bitcoin network, a block size increase is absolutely required in the short term to tide the network over until the true scaling solutions have been fully developed and tested. Which, realistically, is at least one year away.
@_date: 2015-12-30 12:47:08
Not talking about the original accidental fork, but the intentional hard fork to address the underlying bug just two months later. At that point, older clients were indeed hardforked away from the network, and there was no drama or losses resulting from it.
@_date: 2014-06-11 18:52:02
In other words, they use more than one change address.
But if you give it a fancy name, everything can sound ground-breaking.
@_date: 2014-06-01 17:58:14
I don't use MultiBit, but this doesn't seem like the best approach. As long as they put a highly visible donation button in the client, which adds a suggested (but modifiable) amount that goes to MultiBit with any transaction, there are quite a few people who will click it.
This is what Armory does, and judging from the [Armory donation address]( it's working for them. I know I used it a couple of times.
@_date: 2015-01-25 21:37:19
... to the moon?
@_date: 2016-03-09 23:12:01
Miners aren't in general making a choice to mine empty blocks. If you look at the pattern of empty blocks, you'll notice that most of them occur when a block is discovered a short time after a previous block. This is because they are still mining only on the headers of the last block - a technique to reduce the orphan rate - and as they don't yet know which transactions were included in that block, they can't include any at all.
@_date: 2016-01-16 23:47:31
Keep in mind that you and your fellow employees caused this, by utterly refusing to compromise and effectively decreeing that the only opinions that matter are from those with recent Core codebase commits. The revolt was expected and inevitable. All you have to do to remain relevant is abandon the dreams of a "fee market" and adapt the blocksize scaling plan used for Classic, which is a more than reasonable compromise for every party. Refuse to do so, and it is by your own choice that you and Core will fade to obscurity.
Like with any other software system, you are ultimately very much replaceable if you fail to acknowledge an overwhelming desire within the userbase. And the userbase does not deserve any scorn or ill-feelings because of that.
@_date: 2016-03-06 18:40:26
It's always hard to say whether large transactions move it up more than small transactions move it down, and I haven't found an easy way of calculating the median outside of scripting one myself, but if you just want a general feel for it, you could just sit and watch [TradeBlock]( for a bit. I'd say that 5-15% of all transactions have a value of less than $10 and there are even some below $1, but most of them are far above coffee territory.
It should be noted that when looking at transactions with multiple outputs, you can't tell for sure which is the change address and which is the payment, so if you wanted a better idea of the true median transaction value, you'd probably want to limit yourself to transactions with just one output.
@_date: 2016-01-15 18:04:40
Do not worry! Someone will be along shortly to tell you how this is all your fault for not outbidding other people with transaction fees to get prioritized in blocks which are size limited to remain compatible with 1990s era internet.
@_date: 2016-03-06 13:37:27
The fee is only indicative of the problem, not the problem itself. The problem is that regardless of how high the fees go, you can only fit 1 MB worth of transactions in a 1 MB block. (This is true even with SegWit fully deployed, as it moves signature data off-chain to effectively reduce the block size requirements of transactions that use it.) Which means that as demand keeps growing, more and more people will be outright denied access to the blockchain even if they are willing to pay a fee.
And keep in mind that until Lightning and other layer-two protocols have actually been developed, fully tested and widely deployed, they are not solutions to this problem. Since it hasn't even reached the first stage, it is all but guaranteed to be at least one and quite possibly two or three years from reaching the third.
@_date: 2016-01-18 19:05:09
Classic would always be on the longest chain. So if there was a fork and the "Core fork" then got a massive boost in hashing power and proceeded to overtake the "Classic fork", Classic clients would reorg back to the "Core fork". As for confirmed transactions, while you wouldn't be guaranteed to have the confirmations in the same blocks, most of them would be present in both forks assuming that they didn't involve a coinbase from after the chains forked. (Which would by its nature require a reorg of more than 100 blocks.)
In reality, the scenario is very unlikely, since Classic will require overwhelming support for 2MB blocks before it actually starts accepting it. This hasn't been announced yet, but would likely be anything from 75% to 95%.
@_date: 2016-03-06 14:38:45
Everyone is taking it into consideration. I run about a dozen nodes myself. The cost of 2 MB is trivial. The cost of 8 MB is trivial. The cost of 100 MB would be trivial at the time it occurred, but by then we should have proper layer-two solutions up and running.
@_date: 2016-03-06 19:02:31
[Transaction Fees in USD](
I can't vouch for its accuracy, but it seems plausible.
@_date: 2016-01-17 00:10:33
Be that as it may, ultimately achieving full consensus will be the less painful way to resolve this, regardless of how it was achieved.
@_date: 2016-03-04 23:11:00
I recall downvoting a few comments from two or those simply because they were inflammatory and added nothing to the discussion. If you look at the commenting history, considering the overall tone and sheer number of posts, I don't feel it was unwarranted. You yourself deleted one of their posts just a few hours ago in this very thread for [being uncivil]( 
Does that make me an automated downvoting bot?
Or could it just be that other people downvote them for the same reason?
@_date: 2016-03-06 18:22:12
Everyone *is* taking it into consideration. Literally everyone who is advocating for larger blocks is aware of the trade-offs, but fact remains that if the "fee market" is allowed to take hold, no one from the places that would have issues keeping up with larger blocks could afford to use Bitcoin *anyway*, so the point is entirely moot.
Outside of places like the regions of Sub-Saharan Africa where only satellite internet is available, there is literally nowhere in the world that would have problems with 8 MB blocks *today*. In blocks-only mode, you would be able to keep up with the network with nothing more than about 16 kilobits/s. And yes, you wouldn't be able to mine with it, but you'd be perfectly capable of using Bitcoin for all other purposes.
As for your other point, the additional stress on HDDs going from 1 MB to 8 MB is so negligible that it's laughable. The vibration from a gnat landing on the disk would have approximately the same effect on its expected timespan. Which is to say, none at all.
@_date: 2016-01-24 17:54:23
Don't cut out of the "relatively" when you quote me. It is, in fact, relatively non-contentious, especially compared to a POW hardfork. There are a few special-interest people among the Blockstream employees who contest it, but it has wide consensus among the miners, major economic entities, and many developers. Compared to a POW hardfork, which has no support whatever.
@_date: 2016-01-24 21:20:19
Look up the definition of the word "relatively" and look at my phrasing again.
@_date: 2016-01-21 10:35:59


The problem is that if anyone ever sends a Sigwit transaction to someone who hasn't updated, their wallet will fail to handle it in one of several different ways.
@_date: 2016-01-15 22:31:31
I have a strong feeling the reason the tarball doesn't get released until all the participating miners are ready for it is to prevent stymieing adoption by having early adopters DDoS'ed off the internet, similar to what happened to XT. If everyone upgrades at at the same time, it's already too late to stop it.
@_date: 2016-01-19 16:57:58
Huh? I fully agree with the decision to close Luke's poison pull request without any further discussion. What I'm saying is, at one point in this thread they were criticized for closing a pull request without discourse, while here they were criticized for **not** closing a pull request without discourse. It's just people looking for obvious things to attack, but that doesn't make it less stupid.
@_date: 2016-01-15 22:35:39


If that happens, that just means they were at the wheel for all the wrong reasons. Lately, they have redefined "full consensus" as "the people with most commits to the Core codebase in 2015". As such, if they refuse to play unless they are granted full dictatorial powers over the decision making, then good riddance to them.
@_date: 2016-01-20 23:10:55
I believe a required update to all the nodes is preferable to a required upgrade of everything that interacts with the Bitcoin ecosystem, yes. And don't fool yourself - if sigwit is to be the great savior as far as block size goes, you will have to update everything to handle it. Which is doable as a long term approach, but not realistic as a short term capacity increase.
@_date: 2016-01-20 10:42:53
And what you're saying is that the implementation is feature-completely and actually works? So you can fire up your Lightning client and make a payment?
Because that would be complete hogwash.






So an experimental "wire protocol" for nodes to talk to each other is available. Everything else is very much a work in progress.
@_date: 2016-01-28 18:22:02
What some people don't seem to realize is that a majority of miners could force activation at any threshold, simply by not building on blocks mined by those that aren't flagging support. As such, whether it is 75% or 90% is largely political.
@_date: 2016-01-24 17:20:57
So to avoid the relatively non-contentious 2MB hardfork, you believe they would instead employ a massively contentious POW change hardfork.
Seems reasonable! You must have really thought this through, as this is definitely something a group of reasonable people would want to do except out of pure spite.
@_date: 2016-03-04 23:48:17
As in, *literally* every post? If so, I would agree that it is suspicious. Though the scale would seem to be fairly limited - looking at their comment histories I can't see a higher than expected number of negative comments, and if downvote bots were out in force, I would expect most of them to be at below -4.
I could understand the CSS tweak to avoid hiding posts at that threshold, but would that really defend the controversial sort ordering? If the goal was to make a thread immune to vote manipulation or brigadering in general, wouldn't simply sorting by post date be a more readable and, ironically, less controversial method?
@_date: 2016-03-06 13:47:21
This is a tired strawman argument that needs to die. People aren't paying for their coffee with Bitcoin, the average Bitcoin transaction is in the $600-$800 ballpark.
@_date: 2016-01-18 18:57:46
As far as diffs go, that is tiny. If you actually look at it, you'd see that roughly half of those are unit tests, build environment adaptations and simple text changes.
@_date: 2016-01-18 18:51:36
Yes. The problem is for extremely large single transactions, as was demonstrated by [this one](
@_date: 2016-03-06 23:33:13
Even if that were the trade-off, I probably would, but that's far from what you'd be looking at. First of all, even if the block size limit was increased to 8 MB today, the actual average block size would only increase marginally. Secondly, block propagation time does not increase linearly with block size if non-naive techniques are employed, and many miners pre-announce their found block headers to each other before the full blocks so they can start mining on the next one immediately (so-called "SPV mining").
As such, the immediate effect on orphan rate and wasted hash power would be tiny, on the order of a percent or less. And while it would increase somewhat as the average block size grew, at least between miners that don't pre-announce the block headers to each other, you'd still be looking at very minor losses.
But, we're not talking about 8 MB on the short term, and the miners have all confirmed that they can easily handle 2 MB even without doing anything to the propagation method.
@_date: 2016-03-06 18:50:57
Well no, it's quite accurate, but the only way you can math it directly is by basing it off *transaction value*, like the post I replied to, not actual value transferred. As a transaction will often have multiple outputs with a change address for the return, calculating the latter accurately would be much more difficult if not impossible.
Either way, just take the [daily USD transaction volume]( and divide it by [daily number of transactions]( and you should arrive at the same ballpark figure.
@_date: 2016-01-17 10:36:17


The statement is meaningless. What is a "single machine"? The article does not specify what hardware the tests were run on, and even limiting yourself to commodity consumer hardware, there are two orders of magnitude difference between the [weakest]( and [strongest]( Intel CPU you can currently buy new. One of my Bitcoin Core clients that was restarted two days ago currently has about 47 minutes of CPU time - that's on a single core of a E5-1650. Assuming a 32 MB block of simple transactions as stated in the article, it would on average be loading that single core to 50% or so.
And yes, at that point the Atom would struggle and likely start to fall behind, but upgrading to a &gt;10W CPU hardly counts as a "cluster".
So to answer your implied question - yes, all of my five public nodes would still be running, even with 32 MB blocks. They have more than enough CPU, disk space and bandwidth to handle it with ease.
However, you are neglecting the new secp256k1 library (and all glory to the Core people who wrote that), which cuts CPU time for validation by a factor of between 4 and 7. Which, as the article states, is where most of the CPU time is spent.
And we're not talking about 32 MB blocks in two years, and with 2-4 MB blocks, the data and CPU required is trivial, even without the new library.
Nice strawman, though.
@_date: 2016-01-18 11:07:08
That's funny, because elsewhere in this thread they were criticized for outright rejecting Luke's POW change pull request.
@_date: 2016-01-20 20:24:33
With significant added complexity, requiring updates to every single node and wallet software and hardware currently in use. With code that is new and untested, and will take a lot of time to get deployed widely. As well as a significant incentive to not use it for a good long while, since wallets that haven't updated yet wouldn't be able to spend it.
Compared to a clean hard fork that is simple code-wise and only requires node updates, where all the stragglers are sure to update when they realize they aren't getting blocks anymore, and which is largely non-contentious (except among the "Core" devs).
I know which one I'd rather have for the short-term capacity increase.
Sigwit is a good idea, for completely different reasons than getting an effective block size increase - which is mostly done by fudging the way block size is calculated - but it's definitely not the short term solution.
@_date: 2016-06-15 21:42:56
Are you high? If you are referring to the bitcoinfees link, I don't think it says what you think it says.
@_date: 2016-06-15 21:38:20
And what "post" would that be? According to your comment history you haven't posted anything longer than a line of text for the last week.
@_date: 2016-06-15 21:18:39
If it were a flood of "min-relay-fee" transactions, there wouldn't be 6.9 BTC in fees worth of unconfirmed transactions. You can easily chart the growth of the mempool on [TradeBlock]( If you toggle the 7-day display for Recent Mempool between "TX Count" and "Fees" you can easily see that they have increased at approximately the same rate, and if you check the TX/sec rate it has *not* significantly increased over the past week.
Rather than a "spam attack", this does in fact appear to be the cascading breakdown of transaction reliability that people have been warning you about for so long. Now that it's finally here, I hope you're happy.
@_date: 2016-06-15 22:12:05
You must have been *sorta* reading the two-hour graph. We just now had a lucky period with 22 blocks in less than two hours which did cut it down by quite a bit, but that's obviously not the block rate we'll be getting on average.
If you look at the longer-term graphs you'll see that the pending fees in the mempool has been growing worryingly large over the last couple of days, even when you look at high-fee &gt;= 50 satoshi/byte transactions.
@_date: 2016-06-15 22:34:38
There certainly was one spike caused by nearly two blockless hours, but the block luck over the last 48 hours at the time I write this has been almost exactly average at 289 compared to an expected 288. (24-hour luck is 146 compared to an expected 144.)
@_date: 2016-01-25 15:00:39
You are confusing Michael Toomim with Jonathan Toomim.
@_date: 2016-06-15 21:51:45
No one is disputing that the transactions started piling up some time yesterday, but I am disputing that there is an abnormal pattern to the transactions that are appearing. By which I mean, there is no spike in the transaction rate or average transaction fee. 
A pile-up such as this is fully expected to happen as soon as the average sustained transaction rate overwhelms the maximum transaction rate of the network, and as such is not necessarily "suspect". And it appears that we are currently at that point.
@_date: 2015-08-11 18:58:46
Just to clear things up, that thread wasn't removed by a moderator, I removed it to address some factual inaccuracies and add some additional sources. It is now [reposted](
@_date: 2015-08-15 18:39:58
Well, if the majority really is behind large blocks, they can't DDoS all of us.
@_date: 2015-08-11 19:35:00
I'm sure their original business plan didn't include Lightning specifically, it was likely "something something sidechains", but as payment channels were not devised at that point and those things are living documents, I'm fairly certain it does by now.
@_date: 2015-08-11 19:28:03
Correct, and like I posted:


It's impossible to predict the future, but it seems very plausible that they are at the very least aiming for such a position.
@_date: 2015-08-10 19:24:53
The most asinine thing is really that the developers that are opposed to larger block size and use the centralization argument tend to be proponents of the Lightning Network which, while one solution to move many transactions off-chain, will rely heavily on so-called "payment hubs". These will by their nature be far more centralized than the normal Bitcoin network nodes could ever be, seeing as people will essentially be forced to sign up with one of them (and pay the requisite fees to them, as opposed to a miner) to get access to a reasonable number of payment channels. 
Worse yet, as the signing keys need to be Internet-accessible for payment channels to work, the payment hubs will require having their full active balance in a hot wallet, which will be a huge security risk for most people, further cementing the centralization of that network to those that can manage a highly secure infrastructure.
The main problem right now is that I've never seen any easily accessible yet detailed explanation on how the Lightning Network actually works, so many treat it as this magical handwavium that makes transactions magically happen off-chain, but it requires significant infrastructure and medium-term commitments of funds to actually work.
@_date: 2015-08-11 20:20:01
I see your point, but the problem here is that some people are attempting to shut down or stonewall the discussion in its entirety, while at the same time claiming that they do not have any economic incentive to do so. That said, I don't believe the node count is dropping primarily because it is hard to run a node, but because it's not strictly necessary to run one anymore. It's much faster now to sync Bitcoin Core from scratch than back with 0.9, and with pruning it will soon require significantly less resources as well.
@_date: 2015-08-15 18:01:33
This is a troll. The tor list is used for anti-DoS resource scheduling. From  










@_date: 2015-08-11 19:57:44
No, you are reading too much into it. I'm not attacking or vilifying anyone, and I have a great deal of respect for many of the developers. However, the claim that Blockstream and its employees do not have any actual incentive to take the stance they take and that it must therefore certainly be on technical merits alone comes up often enough, and the core of my argument is that this is not the case.
@_date: 2015-08-06 13:46:47
The main problem with making physical representations of bitcoins is that there is no real way to verify that you haven't made multiple physical representations of the same bitcoins until you actually sweep the address, and when you do, the physical representation is no longer valid.
@_date: 2015-08-12 06:32:52
Like I stated elsewhere in this thread, I really do have a lot of respect for you personally as a developer and contributor to open source software, so please don't take this as a personal attack. People were questioning whether there was any connection at all between Blockstream and Lightning, and you are that obvious connection, but for what it's worth, I'm sorry for dragging your name into it.
The post only had one goal: to show that there is potential economic incentive for Blockstream and its employees to avoid discussions about block size increases. I hope I'm wrong, and that every Blockstream developer do in fact have that opinion on a technical merit alone, but there has never before been an open source project with this much potential direct economic impact, so it is extremely important that everyone is aware of potential biases when discussing the future direction of the project.
@_date: 2015-08-15 17:31:55
It's a competing implementation of Bitcoin, which is currently compatible with "Core" but may not be in the future depending on how many people switch. If that certain threshold is reached, "Core" will become a de facto altcoin unless they adopt the same large blocks rule.
@_date: 2015-08-19 18:56:07
The main problem is that a small group of core developers decided that only their opinion matters, and damn all the "non-technical" miners, wallet developers, full node operators, exchange operators, ecosystem developers, and every other kind of user of the system.
The kind of people who run full nodes would generally tend to be a) quite intelligent, b) fairly technical, and c) libertarian-leaning. That is not a group of people who are going to sit back and let someone presume authority and tell them they do not have a voice. I feel that most of the people who have switched to XT have done so not because they want a fork, but to express this particular sentiment.
Hopefully, the Core devs will wake up before this reaches the breaking point.
@_date: 2015-08-12 10:33:32


If you point out specific provably false statements I have made, I will correct them.


How do you english well?


Read the post. While I obviously don't have a copy of Blockstream's actual business plan, it provides a coherent argument for how this economical incentive could tie in with the current discussion.


I own some bitcoins and accept them for business services. Outside of that, I have no affiliation with any groups that are involved with the network. Gavin and Mike would have to speak for themselves.
@_date: 2015-08-11 19:25:09
This is correct. At this time, however, Blockstream has developers dedicated solely to Lightning.
@_date: 2015-08-12 10:25:53
I already addressed the BIP by Pieter Wuille elsewhere in the discussion. It is utterly insubstantial, and if anything it supports rather than detracts from the argument that Blockstream ultimately do not want any increase in the block size.
@_date: 2015-08-12 10:54:40
If I were to spot a potential conflict of interest and I felt it was sufficiently important to point out, I would have. Out of the ones you mention, the only one whose motives I suspect is luke-jr, and that's simply that he doesn't like the network being used for "spam" (read: gambling). See the [Gentoo blacklist debacle]( for more information.
@_date: 2015-08-13 17:04:52
Keep in mind that this is the same guy who is attempting to remove whole classes of services from the bitcoin network. Failing to do so in Core, he managed to get it into the [default tree for Gentoo]( [Twice](
@_date: 2015-08-11 20:24:14
I have in fact been reading the developer mailing list for quite some time, which is one of the reasons I feel comfortable making this post. Tellingly, I can't recall a single instance of a Blockstream employee supporting a larger block size, outside of a token BIP with minimal increase. (Please correct me if I'm wrong.)
@_date: 2015-08-11 07:47:57
That is probably the best accessible description of Lightning I've seen, and it matches my own understanding of the network, so thanks for that.
While you are correct that Lightning is trustless, there are a number of blockers I can see that might prevent competitive Lightning Network nodes from materializing. First and foremost is that, being a hub-and-spokes model, there is a significant network effect. A payment hub that is well-connected with many leaf nodes (end users) will to a much lesser extent have to commit funds to establish payment channels with other payment hubs, which is a huge advantage - especially considering the previously mentioned financial risk involved with needing to have signing keys for all committed funds on a public network.
Furthermore, hubs could potentially exploit an eventual dominating position by refusing or requiring significant fees for establishing payment channels to other hubs, which might force the end user to either establish additional payment channels to the dominating hubs, place those transactions directly onto the blockchain, or open a direct payment channel to the recipient - all of which requires a delay and blockchain tx fee for the (opening) transaction.
In the end, no matter how much the Lightning proponents want to promote their solution, the core Bitcoin network *still* needs to scale in order to make transactions relatively cheap.
@_date: 2015-08-11 19:08:46
What? It was my post. I removed it, it was only a few minutes old at the time so I didn't think it would cause any issues. :)
@_date: 2015-08-11 19:49:19
I am addressing Lightning simply because it has come up a lot, and thus I've spent a fair bit of time to fully understand the concept. Alpha Elements, on the other hand, isn't being touted as the end-all be-all solution to the blockchain scaling.
@_date: 2015-08-11 19:32:28
Their sidechain projects are mentioned in the post, but they are still somewhat fuzzy. Lightning on the other hand is starting to take shape, and as they are confirmed to be a part of Lightning development, it seems quite fair to use that as an example of how they might profit from taking the stances they do.
@_date: 2015-08-11 19:52:03
Yes, LN allows instant transactions, and like I said;


That doesn't change the fact that Blockstream developers tend to be vehemently against block size increases, nor that they have a potential economic incentive to do so.
@_date: 2015-08-12 10:39:36
I have no need for an ELI5, I'm perfectly capable of reading and understanding the whitepaper, but perhaps you should ask for one. Leaf nodes (end users) are not exposed after the opening transaction has been committed, but core/payment nodes are absolutely exposed.
@_date: 2015-08-15 20:25:29
For most Bitcoin users, at first, coins originating from before the fork will be spent in the same way on both chains - that is to say, transactions will still be broadcast across nodes following both the old and new block size rule, and be included in blocks on both sides. But this is only true as long as all the originating coinbase is from before the fork. As soon as you have transactions originating with a coinbase from the "Core chain", they will be rejected by the nodes that are following the "XT chain", and vice versa.
It does however open up variants of double-spending attacks where you can spend pre-fork coins differently on the old and new chain. For example, by intentionally including inputs that originate from post-fork coinbases in the transaction.
@_date: 2015-08-11 19:42:02
Please don't interpret my post as "Blockstream is evil!". I'm just showing how there is an actual economic incentive for them to take the stance they are on the block size, and for any company with venture capital funding, that is relevant. I'm sure many of the devs are passionate about their projects on their own merit, and I'm not even calling out Rusty (who has an awesome resume) for his Lightning work, but there is a large potential for bias and people need to be aware of it.
@_date: 2015-08-13 20:02:23
How can you possibly refute that statement considering the linked evidence? If you weren't trying to censor the services in question, what exactly were you trying to do?
@_date: 2015-08-16 18:54:50
Whenever someone claims that starting at 8 MB and scaling slowly over 20 years to 8 GB is not sufficiently conservative - remember that in 1995, [a 1.2 GB hard drive cost around ~$700]( Assuming that development proceeds at the same rate as the last 20 years, 8 GB of storage in 2036 will be cost-equivalent to less than 0.5 MB of storage today.
On the bandwidth side, [33.6 Kbps modems were standardized in 1994]( Today, 100 Mbit+ home connections are common. Using those points with the same extrapolation, transferring 8 GB in 2036 will be comparable to transferring 2.75 MB today.
And if 8 GB blocks ever actually become necessary, you should rejoice, because that means everyone reading this is likely a 2015 multi-millionaire equivalent in 2036.
@_date: 2015-08-11 20:34:03
So none of the independently operating, completely unbiased, having no motive or non-technical incentive developers would have a particular opinion that seems to be common enough outside of that particular group of people? It just doesn't sound very likely to me.
@_date: 2016-02-10 23:44:14
Well, I was about to call you out as a troll, but you are in fact right. Not "a bunch", but there is a single one of my comments that only show up to me outside of my comment history if I'm logged in, so it would appear that it was shadow removed by a moderator. (Yay, censorship.)
Still, it wasn't in this part of this particular thread, so it's not relevant for you. And since it is still visible in my comment history, you can read exactly what it says.
@_date: 2016-02-12 15:19:36


[Yeah, it has.](
@_date: 2015-08-15 18:49:20
Quick ELI5:
Running XT at this time is equivalent with running Core. It's the same network, and the same Bitcoins. At some point in the future, if 75% mining majority is reached (but not before January 2016), the network will split whenever a miner creates a block larger than 1MB. This will not be accepted by Core unless they adopt a large blocks patch, but will be accepted by XT, and at this point there will effectively be two chains.
Running XT means that you will always be on the largest (75%+) chain, regardless of whether the fork actually happens or not. Running Core means that you will be left behind if a 75% majority is reached. Regardless of which version you run, coins will be safe (on both chains) as long as you acquired them prior to the fork, and for some time the chains will largely mirror each other, but eventually they will diverge due to different coinbases (mining rewards).
@_date: 2015-08-12 06:46:24


From the Lightning whitepaper.
@_date: 2016-02-26 17:56:02
That depends on a number of factors, most important being data transfer rate, roundtrip delay and percentage of missing transactions that would have to be fetched.
Say you have an 8 Mbit/s effective transfer rate and are missing 10% of the transactions from a block. At today's 1 MB blocks, transferring the data would take 1 second for the full block compared to .1 second for the differential, so the extra request would be worth it as long as the RTT is less than .9 seconds.
A 10 MB block at 8 Mbit/s would only take 1 second to transfer for the differential compared to 10 seconds for the full block, and unless your roundtrip delay is ridiculous, that would always be preferable. 
It is correct that for fast connections, the extra roundtrip may not be worth it for 1 MB blocks - at 100 Mbit, you'd need slightly less than .1 seconds RTT to gain any time - but for significantly larger blocks, it would most likely be regardless.
(I'm glossing over the fact that there is an initial transfer for the block header/list, but this shouldn't significantly alter the numbers.)
@_date: 2016-02-12 17:44:34
There is no opinion involved, only fact. The definition of a "hard fork" is a backward-incompatible change in the block validation rules, meaning that old clients may reject blocks that new clients accept. Validation rules did change in a backward-incompatible way. Thus, it's a hard fork.
Soft forks will not have any backward-incompatible changes in the block validation rules, by definition.
(Edited for clarity, as some soft forks do eventually enforce changes to block validation rules, but in a way that is transparent to old clients.)
@_date: 2016-02-29 17:58:05
You don't need reddit gold for that. Just go [here]( and uncheck the "allow subreddits to show me custom themes" button.
@_date: 2016-02-20 15:35:22
I don't disagree that the cap should have been increased already, but ultimately, a 90-95% consensus hardfork that includes Core is far safer for the ecosystem as a whole compared to a 75% consensus hardfork that excludes Core entirely. As such, if the time frame for a hardfork to take effect is set around summer 2017, that is still, in my opinion, preferable to the alternative.
@_date: 2016-02-12 15:27:13
Bitcoin [has]( [previously]( [hardforked]( As such, the current Core developers would already have the exact same liability if this were anything but outright FUD.
^1 
^2 
^3 
@_date: 2016-02-26 10:37:15


This is rather disingenuous. The idea behind the block compression schemes based on most transactions being already known is not to reduce the **average** bandwidth utilization of a node but to significantly cut the block propagation latency by reducing the **peak** bandwidth utilization during those few seconds the blocks are being propagated.
@_date: 2016-02-10 22:26:55
I have not removed any comments, and none of my posts have been edited. Take your false accusations elsewhere.
@_date: 2016-02-10 19:24:05
Learn to read first. I asked you to link to wherever he made that kind of statement, not a link to all the statements he has made ever.
@_date: 2016-02-10 19:26:49
I see that he was listed as one on the Bitcoin Classic page, but what I asked for was if he ever described himself as a developer.
@_date: 2016-02-22 09:10:47
The question was not about counterparty trust, but whether you, by running a LN-enabled node, risk losing the coins that are held by that node for use in payment channels if the node itself is compromised. (The answer is yes.)
@_date: 2016-02-03 15:48:06
0.12 hasn't even been released yet, it's still at RC. If they want a stable client any time soon, building on the master branch would be folly.
@_date: 2016-02-20 13:34:22
To be fair to the Core team: assuming that they are really coming on board with a 2 MB hardfork, they are joining the consensus, not setting it.
@_date: 2016-02-10 19:31:24
Why are you replying to my post multiple times? Please learn to edit your posts if you wish to include additional information.
I'm not going to pursue this further, except to say this: Great minds discuss ideas; average minds discuss events; small minds discuss people. People aren't supporting Classic because of Jonathan Toomim. He is not the purported Messiah. He is, like I said originally, some guy who did the legwork necessary to figure out what the actual consensus among the miners were, rather than using them as an excuse for what can and cannot be done without even asking them.
@_date: 2016-02-07 10:37:34


Except it doesn't work like that. The "voting" is done by miners, not nodes.
@_date: 2016-02-12 22:39:30
I am inclined to believe you, but are you sure? According to BIP50, unpatched nodes were forked off the network on August 16th 2013. So if that it's true, it should probably be updated to reflect that. (Not that it would matter much at this point, according to Bitnodes there are no public pre-0.8 nodes and just three 0.8.0 nodes still running).


@_date: 2016-02-03 15:44:58


This falls more in the realm of "raving" than "fact". Most people don't believe in delaying anything in favor of a blocksize increase. The work necessary has already been done, and would require minimal resources to merge. And it should have been merged, at latest back around 0.10 when the concerns were long past becoming evident. If it were, we'd be at 90% organically already.
In fact, far more resources have been spent outright fighting a blocksize increase than would ever be necessary to actually put it safely into place, even though it's absolutely necessary to scale even if Segwit and Lighting were both ready for deployment. And unfortunately, because of this stonewalling it's still not merged in 0.12 for a 2017-ish HF.
@_date: 2016-02-10 18:59:43
If it's as easy as doing some searches, why don't you simply link to wherever he made that kind of statement? "You are wrong because of reasons you can go find yourself" is a poor style of argument in general.
@_date: 2016-02-10 16:31:29
I don't think Jonathan Toomim ever put himself out there as a "developer", and from what I've heard, he describes himself as merely a mediocre programmer. His primary involvement with Bitcoin seems to have been running a mining operation with his brother, and the whole Classic thing got started because he decided to do some legwork and poll other miners (including the Chinese) about what block sizes they would be comfortable with handling.
@_date: 2016-09-05 18:10:18


This is incorrect as of 0.94, the Armory data folder fully synced is about 170MB for me. It does however require a full non-pruned Core installation to function.
@_date: 2015-09-03 08:43:44
Not entirely correct. Transaction size is quadratic. Block size (transaction count) is linear. As long as transaction size is capped, there is no problem. See [this comment from Gavin Andersen](
(Edited for parent edit and clarity.)
@_date: 2015-09-03 07:55:44
Misleading at best. You are basing your math on the total size of the blockchain from 2009-2015, not the current rate of growth. [The vast majority of the current blockchain size is from the last couple of years.]( Two years ago, it was ~7GB.
@_date: 2015-09-03 15:14:36


As is Bittorrent.


No it's not. Some wallets (notably Armory) already default to a torrent-based initial seeding.


Wut? You seem to be really confused with the whole asymptotic bounds thing. If you claim that the current growth is O( n^2 ) then you probably mean O( n * 2^n ) and not O( 2^2^n ), but you can't have an n that denotes different things feature multiple times in the formula. The growth in this case would be O( n * 2^t ) where n is time since the genesis block (more specifically, 1 MB storage per 10 minutes since the genesis block) and t is a value between 3 and 13. You can trivially reduce this to O( n * 8192 ) which [equals]( O( n ). 
@_date: 2015-09-03 20:29:22










It seems like this has resolved the largest issue with how I understood BIP 100 to handle the voting, which was that it was easier to lower the limit than to increase it. Not sure if this was always the case and the initial coverage was flawed, but this is acceptable, and should hopefully be less controversial than the other options.
@_date: 2015-09-03 10:33:08
Oh, you actually intended that to mean that a linear block size increase would lead to a quadratic *full block chain* validation time, not validation time of an individual block? Because that's even more misleading, and I was giving you the benefit of the doubt. Full chain validation time is already increasing by O(n) just by virtue of time passing. Adding a linear increase in block size will not increase it by an *additional* O(n^2).
And if you want to get technical, since all proposals have a cap ranging from 32-8192 times today's limit, those are in fact all still O(n). (Asymptotic notation 101: O(8192*n) = O(n))
@_date: 2015-09-03 22:32:45
This is *technically* incorrect, which makes it one of the more questionable choices of the BIP. As only valid blocks in the longest chain count towards the vote, a majority of miners could at any point collude to override the vote in either direction by refusing to build on blocks that vote "wrong". It would probably be better to at least shift the thresholds closer to the median in order to discourage such attacks. Whether it should be 25/75, 33/67, 40/60 or simply use the pure median would be up for debate.
@_date: 2015-09-03 07:42:45
Any block size will make the total blockchain size "grow infinitely" in a pure mathematical sense, so your argument is complete nonsense.
In the real world, I deployed a new full node yesterday, and it started syncing at 07:03 and finished at 16:59. If we go flat out at a rate of 1 MB/10 minutes for ten years and **keep the exact same hardware and network, assume that there are no improvements to the blockchain distribution and don't use a pre-seeded blockchain**, that would only increase that to about 4.5 days.
Factor in increases in network and hardware speed, and the fact that if Core devs are unable to improve the block distribution speed it can be pre-seeded with more efficient third party methods like by using torrents, new node deployments are a complete non-issue even if the block size is significantly increased.
@_date: 2015-09-03 11:48:21
While it may have been growing at a quadratic rate so far, you can't use it to deduce the bound, and it's not going to continue if the max block size increase doesn't actually happen, it will then settle at O(n) where n denotes time. If the max block size is increased, the additional increase is a bounded variable that (arguably, and depending on the actual BIP chosen) doesn't alter the asymptotic notation.
Regardless, you cannot realistically use this in any way to defend a 180kB block size as you are only basing that calculation on the equivalent of a single year at the current max block size. If we had been pushing out a constant stream of 1 MB blocks since 2009, your calculation would be based on a ~350 GB blockchain and be 350 * 20% = 70 GB per year ~= 1400 kB blocks. It's meaningless as far as arguments go.
@_date: 2015-09-02 21:00:36
I believe ~~pretty much everyone~~ more people would be fine with BIP 100 over BIP 101 if either a) the block size was strictly increasing, or b) the voting threshold was the 50% median instead of 21% - and preferably both. As it is, it is still stacked towards decreasing or remaining static.
Notably, a major flaw of the 21% threshold is that if a majority of the miners insisted on increasing the block size, they could force the matter by simply refusing to build on the blocks that vote below their desired threshold, and that would cause slow/unstable confirmations and frequent chain reorgs. I believe it would be better to acknowledge this fact by designing the algorithm around it.
@_date: 2015-10-18 12:50:38
That doesn't at all incentivise running a *public* full node, only maintaining a private one for yourself.
Making a node public makes you a target. Any IP address with a public node better be running on a server that's locked down tight, as by my count, it will get hit with about two orders of magnitude more port scans and brute force attempts than something that's just running a web service. And don't get me started on the witless idiots who will actively attack some of the few remaining public nodes just because they don't like the version string it returns.
The sad truth is, unless you run a business that accepts bitcoin directly and therefore would want multiple entry points into the network, there are few non-altruistic reasons to run a public node. And, tragedy of the commons notwithstanding, many good reasons to avoid doing so.