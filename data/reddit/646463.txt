@_author: 646463
@_date: 2015-06-16 22:54:20
I imagine they'd qualify it more so it became a truism:
'Yous' is correct enough to use for communicating when used in rural Australia or with rural Australians. It in no way makes it canonical, or right for the other billion English speakers.
@_date: 2015-06-25 00:20:56
Burning coins is the only conceivable way to pay users, not miners. By burning coins they are increasing the value of all coins held by others, effectively paying them for the use of the blockchain. 
Spam is not vandalism, and it does not constitute the destruction or damage of a resource. The use of such terms does not help your argument.


Yes. I would very much prefer that vandals were required to destroy some of their own assets before committing an act of vandalism. Since we're not talking about spam anymore this is essentially a choice between (V damages a thing) and (V damages a thing and their own economic power), of course I'd choose the latter.
@_date: 2015-06-02 03:09:32
Yeah, certainly there are some economic conditions that mean a fake-header attack makes sense, but I don't know what they are besides being very uncommon.
What is more worrying is the code around this sort of event and if we could compensate if things went wrong. For example, if everyone mined off a broken block we couldn't get more than a few deep before realising something was very wrong, but the code needs to deal with this case before it happens.
@_date: 2015-06-24 23:10:14
Would you find spam-ish transactions be more acceptable if additional coins were burnt? Say by sending 1/0.1 mbtc per op_return output?
@_date: 2015-06-16 03:55:31
Eh, it's common to say 'yous' in Australia, doesn't make it any better. (I am Australian for context).
Who cares if a PM did or didn't know; random history/linguistics/w/e isn't a criterion for PM-ing, neither is an iota of morality or intelligence, or anything else (besides age, citizenship, etc). Away with such, [justificationism](
@_date: 2015-06-13 05:11:38
If votes are done on a secret ballot miners will not be able to choose their pool or switch based on how the pool votes (as the pool controls the coinbase). At the very least voters (pools) require the option to vote publicly if the miners are to find their equilibrium.
I am, however, personally not fond of delegates voting in secret in any case.
@_date: 2015-06-02 02:40:44
I'm sick of preoccupied core devs attacking the weakest argument. This is a very 'worst case' type scenario that Todd presents.
@_date: 2015-06-02 02:39:17
There have been some good articles on this in the past. I think late 2013, 2014 era, around when the GHOST paper was released.
Some practical considerations:
* Miners have highly connected nodes on fast pipes
* There are miner-fast-relay circuits already
The point is it doesn't matter the proportion of the network it reaches, it is the proportion of miners that counts. Harder to measure.
@_date: 2015-06-16 02:44:23
This user is correct. It is referred to as "Magna Carta", not "The Magna Carta". Though I don't think the down-voters realised.
@_date: 2015-06-13 07:48:54


I'd say it's a different type of trust.
Additionally, if there was such a threat of political pressure to justify secret ballot pools would have to lie to actually make proper use of the privacy offered.


It's a stretch -- at least in these parameters.
@_date: 2015-06-15 22:08:11
You make no argument besides control of the code base which is a very weak argument anyway. You do nothing to explain your position, and link sparingly to evidence with no context or conclusions.
Have a downvote.
@_date: 2015-06-02 02:35:51
This is an ~~unlikely,~~ expensive, and short lived attack, making it unlikely. Look into the game theory around mining for detail.
@_date: 2015-05-11 05:11:14


You are forgetting 99%+ of the white male population *also* not in bitcoinland. Don't know why you put a random bit of prejudice in there when otherwise your words were reasonable.
Just 'asplainin' my downvote.
@_date: 2015-05-26 02:29:53


Of course it's not. It just needs to be available if the need arises. Rarely should more than 1% of the population need to actually cast a vote. (Most interests can be expressed through delegate chains and so full participation is not required or useful)


Have a look at delegative democracy / liquid democracy.


Actually the NVB will have hundreds of thousands of (weighted) votes whereas non-member voters get 1. 


NVB is a good way to do it! Most importantly it's designed to be superseded when we have better ways of doing things.
@_date: 2015-05-25 13:15:56
~~Dude, the blocks are clearly reaching a soft ceiling. That's the only reasonable way to explain such uniform sizes. I see now that any sort of humor around the issue might have been a mistake.~~
~~Your question isn't even coherent.~~
@_date: 2015-05-26 01:22:42
Well, when you want to write the p2p stack to do it, sure. The blockchain is accessible and immutable, I'm sticking with that.
Also, your suggestion has no memory, which isn't useful in case of an audit.
Also, how do you **know** you've heard all the votes? You don't, and **can't**. That's why a blockchain is necessary.
@_date: 2015-05-11 02:29:54


These concerns are addressed pretty well by Gavin's O(1) block propagation proposal.
@_date: 2015-05-11 02:27:54
It's set by the miner when they create the block template
@_date: 2015-05-31 03:48:15


I think this is actually one of the best arguments against a 1MB block size. There is no good explanation as to *why* 1MB is so perfect. There are great explanations as to why we need to increase the block size.
@_date: 2015-05-25 13:22:33
Ahh, okay, I misunderstood.
Also, when you say DDOS, are you talking about a rogue party simply spending bitcoin to bloat the blockchain, or censor other transactions, or neither?
@_date: 2015-05-26 02:48:32


While I actually support elections via lot and think they are a great idea [see accidental politicians (2011) for why  - that's not what I mean here.
Those 1% of people who actually vote are voting in a weighted representative pattern. All people 'vote' its just most votes are on autopilot and just doing what the delegate does, or what the delegate's delegate does, etc
In this way you have an efficient system that can reorganise quickly but allows for most of us not to do the work we shouldn't need to anyway
@_date: 2015-05-24 22:59:33
A protocol closely based on liquid democracy is being implemented. There are more flair-y features like transferring identities and anonymous voting through coinjoin in the works.
Main difference is basically that the Pirate Party is a traditional party -- a group of like minded individuals working towards a directed policy change -- whereas the NVB is not restricted in this fashion, but also doesn't get to choose any policy it helps introduce. The NVB is like a politico-economic machine that offers democratically themed incentives to participants.
LiquidFeedback is a policy platform with a voting system. It's designed around workflow and community needs. The NVB protocol is designed for untrusting parties to come to an auditable decision. The security models are thus very dissimilar. The main reason a blockchain is used is because it is immutable.
@_date: 2015-05-26 00:17:30
I'm building a political party on blockchain. It will not work if votes cost 20c (per tx); 2c is okay. Want bitcoin to succeed? Let it go, man!
@_date: 2015-05-28 00:01:14
There are other DoS protections in place too, like priority.
And yes, 100k a day is a lot to spend, especially if you need to buy it.
Also, 432 BTC/day doesn't stop the network working, it just forces a fee market. To DoS the network entirely would be rather more expensive as you have to out compete EVERY OTHER TX, and a miss bloggs might be okay putting a 20c fee on to send $200. Will our attacker try to stop her by spending 4320 BTC/day instead?
The logical conclusions of this scenario do not play out well for the attacker's wallet.
@_date: 2015-05-25 13:11:07
Yeah, choice of words might be a little dramatic. I do think it's very much in the spirit of hacking though; it's certainly not an intended consequence.
I've had the idea for the SPH for a while (which is really the secret sauce) since ~~Feb~~ July 2014 at least - I emailed myself something at that point, which I posted recently. (Due to date formats I have accidentally misdated this on my blog). From there I've sort of talked to people at times but not really actively worked on it. I've been actively working on it since march/april of this year, I guess.
I have had spurts of inspiration at times between, but that was a more conceptual process.
@_date: 2015-05-10 23:51:02
The time recorded in blocks is very accurate, to within 15 minutes usually, which is fine when you're planning a year out. Estimating block height at time t is very difficult and can be off by a large margin if the average block time is not close to 10 minutes (at some points of growth it has been as low as 6 minutes IIRC)
Furthermore, the block time is registered in the block, and so the rules are based on that block and that block-time which is consistent across the network. There will not be chaos, even if timestamps jump around.
Height | Timestamp | max block size | ahead or behind
N | TWENTY_MEG_FORK_TIME - 60 | 1 MB | behind
N+1 | TWENTY_MEG_FORK_TIME + 10 | 20 MB | ahead
N+2 | TWENTY_MEG_FORK_TIME - 50 | 1 MB | behind
N+3 | TWENTY_MEG_FORK_TIME + 60 | 20 MB | ahead
The particular line of code responsible is this: `(nBlockTimestamp &lt; TWENTY_MEG_FORK_TIME ? 1000*1000 : 20*1000*1000)` which is consistent with the above table.
Because miners specify the TS (timestamp), they implicitly choose a maximum block size for the block they're creating depending on whether the TS is ahead or behind the cutover point.
TL;DR It's safe.
@_date: 2015-05-26 02:12:20
Great success! Gold for you!
@_date: 2015-05-26 05:31:10
Ahh I see what you mean.
I don't think the issues that remain (because some of those concerns are solved or can be ignored) will stop the NVB.
Some of these I just disagree with entirely: "every voter must be able to understand the process of vote collection and counting" for example
@_date: 2015-05-26 07:35:35
It's not that useful to reason in the manner you have. There is no 'time bank' that blocks draw or deposit time to/from. periods of 20 minute blocks aren't followed by periods of 5 minute blocks. It's not a mechanism you can rely on.
The example here isn't meant to represent the average. It's meant to represent the 1% of the time we notice things starting to break before they go catastrophic.
What occurs in my screenshot should never happen in a healthy network because that is, in and of itself, not health behaviour. (I am defining health in a way that implies it is functional for sending transactions quickly and cheaply, but that's one of the reasons we all got into bitcoin anyway)
@_date: 2015-05-31 04:39:45
Yes, presuming this growth you mention happened instantly and no increase occurred in consumer hardware you are absolutely correct. However, those assumptions are barmy.
@_date: 2015-05-11 02:49:25
I would technically define that operation as two which is why I see it as minting:
* Destruction of old coins
* Creation of new coins
Maybe that's just how I define it, and we can get around it another way, however, the 'demurrage' clause is a clincher for me in any case. (+ the altering proportions)
When it comes to economic theory IMO proportion always trumps numerical values. Proportion has meaning without context whereas exact values requires consideration of more variables, most notably the proportion. When I say proportion I mean things like "I own 0.000034% of the Bitcoin economy" or "I will send you 0.0000045% of the Bitcoin supply" (supply is a better word than economy)
Also if they're spendable by anyone while a miner has the last say and can always take them it doesn't guarentee that. I'd be more inclined towards actually destroying them and re-minting them as part of the reward.
@_date: 2015-05-11 10:11:27
It's not a precisely calculated value, but roughly speaking if we use that date we shouldn't experience much in the way of service interruptions, as it were. Whereas if we choose a later date some predictions say we'll start hitting the ceiling. It's both convenient and far enough in the future to plan for.
@_date: 2015-05-11 02:10:15
Also, don't drag global warming into this, these two issues not ethically isomorphic in the least.
@_date: 2015-05-11 02:22:51
Your block will not be accepted by the network. Blocks &gt; 15 minutes in the future or earlier than the median of the last 11 blocktimes will be rejected.
Code is here: (some of it) 
@_date: 2015-05-10 23:52:57
This is correct.
@_date: 2015-05-26 00:14:46
Gavin has tested up to 200MB, he's been public about it, too. Some bugs have been fixed IIRC.
@_date: 2015-05-26 00:19:53
It's a valid data point and a realistic view of how the network performs. Nothing to dismiss. 20 min / block on average did produce the phenomena, but we will never be free of 20 min / block periods, so we should still be conscious of this.
@_date: 2015-05-27 22:46:06


Really? 20MB block size will make it 20x as expensive?
Lets say blocks are 90% full today and 1 MB. Tomorrow they become 20 MB. Logically, we'll still only have 900 KB of txs in a 20 MB block. The remaining 19 MB are needed to be filled by the DOS-er, which is about 2 * 1024 * 19 transactions, or 30k of them. That's like 3 BTC per block. Not cheap.
If you disagree with me, then I invite you to show me the math supporting your claim and you should find you cannot replicate your results.


There must be a simply *mind blowingly amazing* algorithm for that, because IFAIK it's impossible to do securely. Please share it.
@_date: 2015-05-11 03:21:59
Thank you!
@_date: 2015-05-24 21:38:00
The NVB doesn't have any policies, it's just a politico-economic structure designed to interact with and alter the surrounding environment.
And I guess it's sort of like a micro-democracy inside a party that can safely grow to consume the entire parliament.
@_date: 2015-05-26 02:31:45
Jeez, thanks Luke!
The blockchain is used because it's the only immutable, distributed data structure on the bloody planet. If the NVB is meant to bring unfriendly parties together we need some neutral ground.
@_date: 2015-05-11 04:09:08
I'm building a voting platform that runs on-blockchain and intending to launch a party for the 2016 Australian Federal Elections.
*  -- vote explorer demo
*  -- wiki
@_date: 2015-05-11 02:00:52
This violates the socio-economic contract that Bitcoin presents. Furthermore as points out it makes the other coins worth more.
It also weakens proof of burn and other things that may have economic significance.
Don't get hung up about a maximum number of coins. The problems of 'finite-ness' go away when you treat Bitcoin like a finite section of a line with infinite points, instead of discreet little lines that sum up to 21 million that are in themselves indivisible. The explanation for why this is okay is basically that we can increase the divisibility of units because that is defined purely in software.
Additionally, you'd actually violate the 21 mil limit by reclaiming lost coins as the 'effective' new coins that were minted/reclaimed now exceed 21 mil. See [prohibited changes]( for more.
Particularly: "Increasing the total number of issued bitcoins beyond 21 million. Precision may be increased, but proportions must be unchanged." -- your suggestion would alter proportions.
Looks like it violates this one too (and perhaps more accurately) "Demurrage (deletion or reassignment of coins judged to be "lost" or "unused"). This is highly controversial in the context of currency units; on the other hand it is absolutely essential for namespace entries like [Namecoin] (which implements Demurrage for namespace entries but not for currency units)."
@_date: 2016-01-13 02:55:04
Not on their own AFAIK. I designed an atomic protocol at least as good as the protocol using bitcoin scripts, however, it requires an SPV version of chain A on chain B for that particular market. I suspect SPV versions of foreign chains will always be required for this sort of thing (even if not directly encoded in a blockchain, ie all full nodes run both chains and validate against an implied, but never recorded, state). For those curious it evolved into [this]( but never went much further.
@_date: 2015-05-14 23:44:41
I have no experience with unlimited time off but what you said reminded me of the opening lines of this BigThink with Slavoj Žižek: [Political Correctness is a More Dangerous Form of Totalitarianism](
@_date: 2015-05-11 05:08:04
From Wikipedia:


You would have to explain how Bitcoin exhibits intelligent behaviour, which IMO it does not.
@_date: 2015-05-24 13:04:37
Hey, this is Max, if anyone has any questions I'm happy to answer them here.
Will be off to sleep shortly but back in 8 hours.
@_date: 2015-05-26 07:00:25
Spam. One naughty miner could make 100MB blocks or something cray. Technically you could call this a DoS attack of sorts, but basically it just makes life unnecessarily difficult.
@_date: 2015-05-25 13:33:34
You're right, and it's easy to see when you reduce it to trivial cases.
* If each block contained just one transaction it would be easier to flood a community
* if each block was infinite the attack would be impossible.
Assume a continuous line and bam: 20 MB blocks make this attack harder for sure.
@_date: 2017-03-05 09:35:55
We don't give it to them; they choose it.
@_date: 2015-05-11 02:51:54
Agreed; additionally, overabundant supply is a property granted by **divisibility**, whereas **scarcity** is provided by a finite total.
@_date: 2017-03-05 09:49:13
No, but we don't need extra data. We use a 6 byte tag, and then a 34 byte multihash.
Additionally the limit for standard nulldata transactions has fluctuated between 40 and 80 bytes before, so choosing 40 is safer. The only additional thing that would be useful is an ed25519 signature, but they take 64 bytes so don't fit. No need to add extra redundant data.
@_date: 2015-05-26 07:32:31
Sure, and I can come up with any number of schemes, but they don't address the issue now, and they need to be debated.
It's not that a fixed limit doesn't make sense (I think you mean fixed, not hard, hard limits apply for a sliding limit too; hard -&gt; limit in code, soft -&gt; limit set by miners)
It's not that a fixed limit doesn't make sense, it's that there isn't a good explanation that favours fixed over sliding, however, there are pragmatic and immediate reasons for kicking the can down the road, then figuring out and debating **in a civil and measured fashion** about the proper solution.
I don't know of any intensive testing of sliding limits, I do know gavin has tested the "swap to 20 MB" plan well. I'm going with the tested option first.
(I'm not actually positive about that hard/soft distinction, but that's what I've interpreted from context)
@_date: 2015-05-26 02:40:14
Depends what you mean. If you're talking about May's theorem or something like that then I don't think it excludes electronic voting at all.


I understand why you're dismissive, but knowledge isn't created by intimately understanding every flaw and crag.
PS. I'm the main guy working on the NVB, and yes it will use bitcoin first because it's the only secure chain atm.
@_date: 2017-03-05 03:43:25
We have been doing on the testnet, and no doubt we'll do future tests there too. But a testnet isn't a mainnet, and if we want to gather data on fee pressure, network reaction, etc, we do need to do some production tests. After all, we are planning to sell this as a service so we need to know how it behaves.
@_date: 2015-05-24 23:06:01
Our efforts will be very directed to begin with. [GVTs]( provide a fertile ground for manipulation. The lucky thing is that [my attack]( is benevolent, and can only be used to introduce superior forms of democracy, as decided by a market.
We only need 1% of the primary vote to be effective, so the criteria for success is:
* 1% of the primary vote goes to NVB in the senate
* Other parties participate by preferencing the NVB on their GVT
As long as 1% of people are convinced, game theory and economics should dictate the rest, and the NVB provides objective incentives for other parties (which is one of the reasons neutrality is so important).
@_date: 2015-05-06 05:20:00
The fundamental issue with steganography is that one of the following things is true:
* You are using it with strong crypto
* You are not using it with strong crypto
Strong crypto is the only thing that can protect private keys (sidechannels excluded) from an adversary with read access. Steganography is neat but only *hides* data, and does not protect it.
I guess another way to look at things is:
* Crypto helps keep things private
* Stegano only helps keep things secret (which is great but temporary)
@_date: 2017-03-05 14:31:24
We could easily anchor trillions of votes via 1 transaction, but what we're doing has a few properties:
1. We assemble the votes live; they're not signed till the stress test starts. We originally intended to include the hash of the latest bitcoin block in with the votes to prove this, but didn't have enough time.
2. We're attempting to simulate a live network - where voters would be contributing more votes in a decentralised manner over a long period of time, so we definitely need more than 1tx per block
3. We need to gather data about how the network and community react to these sorts of events - especially as we're intending this to be a commercial service.
4. Some pragmatic reasons: like PR value, real world accomplishments, and community involvement.
I've gone through some of these in more detail throught this thread.
@_date: 2016-01-06 23:30:50
So I think it began mostly by what was being used to begin with. P2PK and P2PKH were the first two standard transactions (from memory). I think the motivation for isStandard was to stop people getting too adventurous with scripts and help standardise things for wallet developers and the like.
When multisig was introduced (with an opcode) that was made standard too, as the intent was to not just facilitate but to encourage its use. (It's technically possible to do m-of-n with lots of if statements and whatever but that is bloat-a-rific)
P2SH had a long and fruitful debate with plenty of other options discussed (op_eval and code separators and such), and eventually made standard to allow the burden of large scripts to be placed on the receiver (as they are the party to specify the script), and provide some extra privacy.
Nulldata txs were made standard to allow protocols like mastercoin/counterparty to operate without polluting the utxo set, and basically the core devs' hands were forced (not saying it wasn't the Right thing to do, though).
So a lot has been in response to adding a feature, pointing out things that should be supported (by wallets, etc), or ensuring the network operates more smoothly. Each of those issues probably has a wealth of history on bitcointalk if you look for the key terms (like P2SH or Nulldata or whatever).
Note: `policy.cpp` mentioned elsewhere is a newish file, I think isStandard was in `main.cpp` for a long time.
Also, for anyone interested, if you want to know how to broadcast a nonstandard tx quickly I wrote a thing here: 
@_date: 2017-03-05 09:38:35


We would spam the UTXO set if we hid the data in addresses. Nulldata txs were standardised to encourage ppl not to bloat the UTXO set.
This is an economic use, it's just not a _financial_ use. It does create value, it does transfer value, it just doesn't transfer _financial_ value.


Because we're going to operate a trustless 2nd layer blockchain that requires being added to continuously. This isn't about auditing voting, this is about providing a voting service that has never been possible before, and crucially, voting systems that have never been possible before (IE IBDD)
@_date: 2017-03-05 12:12:37
Our system doesn't require physical presence, but election providers might.
@_date: 2017-03-05 12:43:37
Litecoin isn't nearly as secure as Bitcoin. Our main reason for using Bitcoin is the cost of attacking the chain itself; no other chain comes close to Bitcoin.
@_date: 2017-03-05 05:01:28
Fully recorded, but not revocable. Dealing with revocation is difficult because you need to be onymous to revoke, which breaks secret ballot. There are ways we can do this at the expense of other people (IE a group of ballots are revoked) and there are other possible ways to do it using future commitments (e.g. using the hash of a public key) but that's at the risk of de-anonymising other votes.
I have a rundown of how our anonymisation engine works in the white-napkin.
@_date: 2017-03-05 12:19:54
No - because scalability is a core component of our system Ethereum is no easier or harder than Bitcoin (because we need to use a layer 2 solution). So since we're hosting data off chain we may as well use the more secure chain.
@_date: 2017-03-05 09:59:23
Trusting the list of eligible voters and the list of votes are two very different problems. We only solve the latter one.
Futhermore, once you accept that solving one of these problems is a good thing to do, you rapidly approach the need for a blockchain. The main reason is that voters (in a decentralised voting system) need to agree on the set of valid votes, IE they need a consensus protocol, and doing proof-of-stake (via votes), in a 2nd layer blockchain, over Bitcoin (proof-of-work), gives a fantastic security profile.
ZKPs are very computationally intensive. I would like to see anyone construct and validate 1.5 billion ZKPs in 24 hrs. We only require 2 ed25519 signature validations for fully anonymised votes.
Thank you for the link. Some red flags appear, though: "Vote privacy: no one can learn the vote of a voter. Vote privacy relies on the encryption of the votes", and "Moreover, ballots are signed by the voter credential (only eligible voters are able vote)."
Our votes are in plaintext, not encrypted, and privacy relies on being unassociable with a voter by construction.
@_date: 2017-03-05 07:38:54
Well we've done 50 test txs (basically a static fire of the producer nodes), but it's an achievement, it's challenging, and it sets a high bar.
We've put about 250 million votes anchored to the testnet chain, so we have started with a smaller set. This gives a good baseline in an unpredictable public environment though.
@_date: 2017-03-05 14:23:16
@_date: 2017-03-05 14:11:26
Hashrate and a BOTE calculation as to how much money it would take (from scratch) to 51% attack the chain.
@_date: 2017-03-05 09:33:31
Yeah, we'll essentially be spending long chains from maybe 20 outputs or so.
We were always going to do 5-20k transactions. 11k is the number we picked due to local fee pressure (IIRC we're using about 225 satoshis / byte).
The conditions we want to prove we can work in are:
* Multiple producers of votes, working asynchronously
* Many requests for pallets (basically our versions of blocks; the things that contain votes) - instead of one request for a yuge pallet.
* Competitive fee environment
And to satisfy those we need some serious volume.
So yes, we could easily anchor 150 GB of votes via 1 tx, but our architecture is fully decentralised, which means voters need to be able to submit votes directly, which necessitates multiple transactions. 11k txs is high though, it's likely that even a busy production network wouldn't have that many (especially with RBF and other improvements to the protocol).
@_date: 2017-03-05 03:47:31
We're not putting 150 GB of data into Bitcoin's blockchain. That would be tremendously silly. We'll actually only add about 3 MB. The rest is held off-chain.
@_date: 2017-03-05 05:04:42
Thanks for the vote! It's really encouraging finding voters in the wild. :D (From Max wearing his Flux hat)
(Now with XO.1 hat)
We'll be watching fee pressure closely too. That's one of the main reasons for running a stress test; Bitcoin doesn't have applications like this yet where there'll be near constant increased fee pressure for a period of time (besides spam attacks, but they have a different profile) so I'm hoping we get some good data from this. With any luck it will be minimal; 4% is pretty gentle in the scheme of things, but it is 3+ blocks worth of transactions.
@_date: 2017-03-05 12:34:00
I definitely think that is a problem, but also that it's part of the wider problem we propose. If it is indeed immoral to force the will of a majority _indiscriminantly_ upon a minority, then it is necessarily a subset of the problem that democracy does not bias the best knowledge. If democracy did bias the best knowledge it would self correct to not indiscriminately inflict the majority view on a minority.
@_date: 2017-03-05 09:34:38
We'll eventually be operating a second layer blockchain, so it has to be continuous; 1 tx doesn't cut it. 144x2 might, though, but that isn't really a stress test, just a tech demo.
@_date: 2017-03-05 03:46:09
This isn't spam, Luke. I know you have your views on which transactions constitute spam and which do not, but the reality of Bitcoin is that whether it's spam or not is decided by the sender. Spam does not come with a high transaction fee.
You can read about Australian anti-spam laws [here]( however, I think you'll find that, like most Bitcoin transactions (even the ones you consider spam), they don't fall into these guidelines.
@_date: 2017-03-05 12:18:55
Collection can still be decentralised (like making bitcoin blocks is decentralised). Being included in a _particular_ pallet is still up to the pallet assembler, but being included in _any_ pallet is a different problem (pallets are our version of blocks). We also have redundancy mechanisms (a vote can be included in multiple pallets without overhead).
Um, not sure we're on the same wavelength with voter receipts.
@_date: 2017-03-05 05:19:08


Ahh! A problem we've thought a lot about.
The first thing to note is that you do get a vote receipt when you vote using our system. However! That receipt is not cryptographic.
When you vote through SecureVote your phone will choose a random number to accompany the vote. This allows _you_ personally to ensure your vote was included (because you know which random number was chosen), but once that vote is public it's just another random number.
(The voting receipt looks like a txid + cryptographic reference to the box your vote is in + the vote nonce)
That means that theoretically you could also use _anyone else's vote receipt_ - all you have to do is find someone else's random number and claim that vote was yours.
So the key here is that the receipt is only useful to you; it's not useful to anyone else (because all receipts are technically public).


This is a more difficult problem, and really comes down to 'how do you prevent devices being compromised'. There is a lot of non-protocol stuff going on here, so this is naturally a fuzzier answer.
First off we have to consider the economic incentives of the attacker; she has to show her hand to act, so this sort of attack is only good once (ideally). Obviously having many unpatchable devices out there is not good for this, but being able to detect un-patched-ness means our software can route around it, or at least revoke identities before they become a problem.
There are also additional protocol level methods for revoking identities, such as the one used in SQRL ( No method is perfect here (if we presume an infinitely sophisticated attacker) but there's a lot of research going on and we're getting better every day.
There are also questions of long-term vs short-term identities, and they obviously have different properties and so different solutions work for them. (e.g. registering for 1 election vs registering for referenda every week).
At the end of the day though I think this is more inline with the question of keeping anything secure long term, and so is far broader than an individual voting system. We'll no doubt benefit from the advancement in other related fields as well.


Yes! (Especially because the solution to the problem of devices being compromised is already implicit in the question :P) We're planning on integrating heavily with these sorts of systems (and non-crypto based stakeholder ballots) early on as a way to prove viability for national elections.
@_date: 2017-03-05 07:42:15
Maybe; that's their choice. We'll need to test in a production environment, though, so we can get an accurate measurement. Probably justifies more tests in the future actually, so we can monitor how miners react...
Your point is taken, and these are risks of using the Bitcoin blockchain. The problem with that sort of censorship is that we can just hide these transactions in addresses like used to happen with Mastercoin in the old days - that bloat in the UTXO set was the main reason for nulldata being introduced in the first place.
@_date: 2017-03-05 12:23:13
I'd never thought of this, but technically it is. All trees are subsets of DAGs and all blockchains are subsets of trees.
@_date: 2017-03-05 07:46:06
Absolutely agree.
@_date: 2017-03-05 12:00:48
That's not us, but this is: 
You can identify us via the 6 byte prefix tag.
@_date: 2017-03-05 09:53:48


There are some compromises but ultimately I think it enables a similar result for less human-work-hours, so is a good thing. No reason to repeat work needlessly. That said, being bound to a particular delegate regardless of their actions is bad, so definitely needs to be well constructed.


After one casts the vote it's irrevocable, and attempting to do so has a similar reaction to double-spending in Bitcoinland. That said we can let users choose their vote and delay casting it for some time (like choosing a week before the election but casting on the day).
@_date: 2017-03-05 12:21:54
Thanks kind stranger :)
Being Aussie is definitely a benefit, you've got to develop a bit of a thick skin ;)
@_date: 2017-03-05 12:47:49
Let's presume there is objectively true knowledge (there is - see The Beginning of Infinity, but presume that for now). Note: objective truth is nigh impossible to find, but we can improve on existing knowledge and get closer.
That means that some knowledge will be closer to the objective truth than other knowledge. Moreover, some ideas that are created will be closer to objective truth than existing knowledge.
Solving problems requires good knowledge, and the more objectively true the better. So if we have a democracy with the point of solving problems we need to use the best knowledge possible (at the time) to come up with solutions. If we get better knowledge later that's great, but we need to use the best knowledge (IE that knowledge which is closer to objective truth).
We can't really know for certain which knowledge is more true or less true than other knowledge, but there are many indicators. Experiments in science are an example of these, but there are more indicators too.
The Beginning of Infinity goes into a lot more detail, but that's a very high-level gist.
@_date: 2017-03-05 11:42:22
Thanks! :)
@_date: 2017-03-05 12:05:04
We use a 2nd layer solution which is where the 3mb figure comes from. Because it's 2nd layer it's blockchain agnostic and we could run it for the same size on Ethereum, but then we have reduced security for the same data burden. Rather, the point was that Ethereum can't and shouldn't handle this load. Once you accept that and move to a 2nd layer solution there's no reason to use Ethereum at all.
@_date: 2017-03-05 07:44:02
See duress mode in the white napkin.
@_date: 2017-03-05 12:10:06
We need multiple transactions regardless because we operate a 2nd layer consensus network. That said collaborative merkle roots look interesting and I'll look into it.
@_date: 2017-03-05 09:49:50
Ethereum is less secure by an order of magnitude. Our product is called "SecureVote" - we'd be reneging on that immediately if we went with Ethereum at this stage.
@_date: 2017-03-05 07:05:16
Hey thanks for your encouragement :) please do yet in touch if you have some time in your plate.
The slack invite bot is offline atm but if you pm me your email I can invite you directly. 
@_date: 2017-03-05 04:06:43
Yes, so we'll publish archive copies of the ~150GB of data for later verification (or study if it's useful to someone). (We'll probably keep 1 node online for a few weeks, then just make archive copies available)
We link to it through IPFS so it's all cryptographically verifiable.
I'm also very curious personally to see how this affects fees and network congestion. I don't think congestion is a _bad_ thing per se, but we do need to study it to make all 2nd layer protocols as good as possible. Plus it will help push for Lightning and other scaling solutions.
@_date: 2017-03-05 14:22:39
1. this is a somewhat open problem - IE if we have an open protocol then there's no reason someone can't make JacksEvilClient that interacts with the voting system but doesn't include the security features we do. There are certainly physical security measures one can use to help prevent this, and there are app-provider level features (like apple/google removing clients designed to interact in a negative way), but from the point of view of an attacker with infinite time and money to make new apps I'm not sure there's a lot that can be done. It does take a very sophisticated attacker to pull off properly though, and definitely isn't possible by default (as the app chooses the number, not the person).
2. No votes are encrypted. The data structure is essentially `[([vote], [signature])]` which means lists of pairs of lists of votes and signatures. If you choose the receipt of someone else in the `([vote], [signature])` (we call this a box) that you're in the receipts are cryptographically indistinguishable.
 * Note: votes are encrypted during shuffling, but not after.
3. Ahh, I'm happy to publish that provided our patent lawyer has no objections (obviously don't want to shoot ourselves in the foot), but I'm pretty sure it's not a legal risk to us. In terms of security they provide absolute security guarantees. I came up with the mixnet algorithm and haven't proved any security properties about that, but the oblivious shuffle is used in CoinShuffle and other protocols so I'd point you to those existing cryptanalysies for some rigor.
Regarding the shuffles: they're (broadly speaking) pretty solid. They both work on the principal that everyone receives every vote at the end of the process and they lose the association between node-&gt;vote. Provided we have that property the full voting shuffle (Copperfield) falls out pretty simply, since that's about removing associations between permanent identities and votes in the presence of an active attacker (and not about removing the association in a particular step, which the oblivious shuffle and mixnet deal with)
@_date: 2017-03-05 07:02:50
Heh, yeah have done some consulting work with Factom. Using it convinced me never to trust it; even things like consistency in their API was way off. 
Also as mentioned they're still not at milestone 2 so 100% centralised 
@_date: 2017-03-05 14:09:03
We use nulldata as a notification as well as notarization, so not sure if OpenTimestamps fit our need, looking into it though. Anything that makes our service more reliable and secure is on the table, though. Thank you for the link.
@_date: 2017-03-05 12:06:47
The vote submission, counting, and auditing process is fully decentralised.
A voter receipt only proves (in the best case) to one voter (person A) how they voted. But if that can be used to prove to someone else how person A voted that compromises secret ballot.
@_date: 2016-01-12 22:13:29
* they're p2sh addresses with the same version byte as identified
* that means the redeem script is the same
* The redeem script is a 2 of 3 multisig - 
* This means the *public keys* not *addresses* of the 3 identities are included
* Public key format is consistent across btc/ltc
* Thus the keys require the same private keys to spend, even though they're on different networks
You can see in the image under 'signatures required from' they include addresses. If this had been rendered for LTC instead of BTC then the addresses would change, but everything else would stay the same.
Bonus: When I was playing with the idea of distributed exchange I realised you could have the same public key behind addresses on both ltc and btc networks. This lead to a cool idea where you'd send coins to a 'swapper' address, which would send a corresponding amount to the same public key on the other network, so one private key would give you access to both no matter which direction you were coming from. Neat. (Mostly useless though)
PS: I found redeem script in this tx:  (last part of the input script, first three parts are to solve a multisig tx) -- if the address had never been spent from we wouldn't know it was a multisig account
@_date: 2017-03-05 14:10:37
Possibly. From memory it went 80 -&gt; 40 -&gt; 80 (but I could be wrong).
I did a confirmation test last year though and found no major difference between 40 and 80 byte nulldatas, though, so I think it's probably negligible risk going into the future.
@_date: 2017-03-05 04:24:49
Ahh yes, forgot a decimal place.
`/me goes back and makes sure I didn't forget it in the code, too`
@_date: 2017-03-05 09:43:37
Unfortunately the Namecoin blockchain is open to a 51% attack from Bitcoin miners so doesn't have the security properties we're looking for. 
Trivia time: I ran one of the first Namecoin mining pools to mine a Bitcoin block when AuxPOW came in (bitchomp.info was the domain if you're interested).
@_date: 2017-03-05 07:45:41
Only 3 MB will be stored in the main Bitcoin chain.
We have used the testnet, but the testnet is not the same environment as the mainnet. Eventually you have to test somewhere with real risk and real value.
@_date: 2017-03-05 05:24:23
Our main goal is recording hashes that people can use to look up content later on - and at this stage I think that needs to be on the main chain. The advantage of Lightning is that each _transaction_ doesn't need to be on the main chain, just settlement of multiple transactions. In that case they can be entirely implicit, but for us (at this stage) it still needs to be explicit.
So _we_ can't use lightning, but because the community would benefit broadly from Lightning we would too.
It's also worth adding here that standard transactions limit nulldata entries to 1 per tx, so we have a 203 byte overhead for 40 bytes of data. If we could increase the number of nulldatas in each transaction that overhead goes down, and allows us to use coinjoin to allow multiple producers of nulldata to cooperate.
@_date: 2017-03-05 10:45:49
Voters are ticked off the electoral roll, then they take the ballot to a booth and vote. People who weren't ticked off are called up later. It's basically the same system that stops people voting twice - it just requires analysis afterwards.
@_date: 2017-03-05 09:50:23
I doubt it. Only 4% of the volume for 1 day.
@_date: 2017-03-05 03:50:29
We're expecting to spend about ~~0.005~~ 0.0005 BTC per transaction, so 6 BTC total, give or take.
@_date: 2017-03-05 09:35:00
It _is_ self promotion, but it's not an ad.
@_date: 2017-03-05 03:37:15
Thanks :)
There's a lot of different ideas about blockchain voting going around, some ethereum private chains for example, or where all votes are stored directly on a chain. I think that's less sensible because you lose a lot of flexibility, but if you do it right you wouldn't need more than a few hundred transactions to run an election for millions of people. When it's that cheap: why not?
@_date: 2017-03-05 12:42:52
Yeah absolutely. SecureVote is essentially voting system independent, so we can just as easily use Liquid Democracy as IBDD or Single Transferable Vote, or plurality (FPTP).
Flux (then the Neutral Voting Bloc) actually intended to use Liquid Democracy many years ago (well 2 years ago): 
See 19.00 minutes in for LD, and then see 36:40 in for my arguments about why LD + trade is better than LD without trade. Flux's system of IBDD (issue based direct democracy) is the evolution and refinement of those ideas.
@_date: 2017-03-05 12:13:00
Thanks! We'll be sure to publish some results later this week :)
@_date: 2017-08-01 05:38:12
Fun fact: Miners can't lie too much. From memory a block's timestamp must be greater than the MTP of the past 11 blocks and less than 15 minutes in the future relative to _the clock of the device the bitcoin node is running on_. If either of those conditions fail then the block is rejected. So an hour in the past is probably feasible, but not more than that. It also means that if you have trouble syncing the chain (esp near the head) make sure your system clock is accurate.
@_date: 2017-01-03 06:46:47
Ahh, bugger. Fair point.
Maybe if there's enough demand the hipster nixie startup will scale out? Feels like the less good solution :(
@_date: 2017-08-01 13:52:37
Citing blocks occurring 30 and 40 min before the succession of blocks is essentially the gambler's fallacy. Block production are independent events so the chance of a block occurring 1 min after the previous one is a constant probability. The same for 30 min after.
It'd help your case if you provided literally any maths behind the "much less than 1% probability claim"
@_date: 2017-08-01 13:16:18
This person is right. Any potential fork has not occurred yet. Also, it was pointed out somewhere that it takes a block &gt;= 1MB to actually trigger the fork, though not sure if replay protection invalidates that.
Edit: MTP is now past 12:20 UTC
@_date: 2017-01-03 06:22:28
If there's more demand they should get easier to buy...
@_date: 2016-11-14 06:12:20
Voting is totally abstracted via an (unwritten as yet) app. Can handle any voting protocol.
The blockchain is just an immutable anchor and then we build a protocol on top of that; so basically secures the vote the same way it prevents double spending =&gt; single history + consensus algorithm. Aiming for an announcement/public stress test later this year, more info then.


Yup, the whole package!
@_date: 2016-11-08 00:13:16
@_date: 2016-11-16 21:07:46
Thanks :)
Yeah, that's a really good point. Will look into writing a little C/C++ thing to use the rest interface.
@_date: 2016-12-26 22:41:47
The other answers are a bit crap because they're obviously not what OP is looking for.
Usually super computers are measured in FLOPS (floating point operations per second).
Bitcoin does hashing which is just integer operations so it's not directly comparable but we can estimate. Using [this post]( for data, and as is rightly pointed out the Bitcoin network does a massive 0.0 FLOPS because of the integer thing mentioned before.
Using an AMD 5970 it does 700MH/s and is capable of 2700 GLOPS (single precision).
This gives us 27/7 GFLOPS / (MH/s) ~= 3.85 GFLOPS / (MH/s)
(this works out to be 3.85 * 10^3 FLOPS / (H/s) though another estimate on the above forum post is 1.27 * 10^4 FLOPS / H/s)
Current network hashrate is about 2.3 million TH/s so 2.3 million million MH/s.
Multiply by 3.85 and we get 8.86 million million GFLOPS, or 8.86 thousand exaflops. Thats 8.86 * 10^3 * 10^18 FLOPS (or 8.86 * 10^21 FLOPS)
**Sum Bitcoin Hashrate estimated 8.86 * 10^21 FLOPS**
TOP500 list for Nov 2016 has a sum power of
627 thousand TFLOPS or 6.27 * 10^2 * 10^3 * 10^12 FLOPS = 6.27 * 10^17 FLOPS
**Sum TOP500 = 6.27 * 10^17 FLOPS**
 give or take the Bitcoin network is 10^4 ish times larger, or 10,000x. Or maybe more like 100,000x if you go with a larger estimate of flops to hash/s ratio.
All that said, all you could really say is something like "give the task of mining bitcoin, the worlds super computers would be approximately 0.01 to 0.001% of the network."
@_date: 2016-11-14 13:00:39
I knew createrawtx could make null data txs but I was looking for something easier that created change automatically. Had not heard of fundrawtx but will look into it. 
In regards to the latter filtering, it's so much faster it Bitcoind. I wrote overturn.ninja a year ago and it's reallllllly slow, and introduces more bugs. Bash and grep might be faster but I'd rather have type safety, etc. This way, though, Bitcoind does a lot of the heavy lifting, which is nice. 
Also, for reasonable speed you need to use non verbose get block, using json for the lot was a lot slower than parsing a raw block through pycoin. 
I agree there's no _need_ for it in an absolute sense, but it's worth it from my POV. Also thinking about adding more features, like indexing null data by prefix, so a fork better accommodates that.
There are more than 1m nulldatas in the blockchain now, so if you're looking for metrics the efficiency gains count for a lot. 
Ps, on phone, please excuse typos
@_date: 2016-11-14 04:46:54
Yeah, I guessed that it's not relevant to most people. Will expand in the readme but for those who don't know what nulldata is it isn't relevant.
Fair; I'm the co-founder of the [Flux]( movement and we're in the process of building a highly scalable voting system on the back of Bitcoin. For that we need to interact with nulldata quite a bit, and I realised that doing a lot of the processing we were doing in python was silly, and it's better to let bitcoind do the heavy lifting. There's at best okay support for nulldata, and since what I'm adding probably isn't welcome in Bitcoin Core (not that I've asked) I forked it instead.
@_date: 2016-11-14 20:41:22
You're probably right, but my C++ is not great. I could probably do it (eventually), but it won't be tasteful or quick, and it's much faster for me to alter the Bitcoin source, since everything is already set up (like parsing blocks).
Also, just had a look at the REST interface, it's really nice. Haven't kept up as well as I'd like with features in Core the past few years, but there's some great stuff in here now.
@_date: 2016-11-14 02:46:56
So far I've only added some RPC commands:
* getnulldatas - returns nulldatas in a block via hash
* getmanynulldatas - returns nulldatas in many blocks given two heights (inclusive)
* sendnulldata - given a hex string will encoded this as nulldata and publish a tx
@_date: 2016-11-14 07:15:40
Our end goal is to eliminate the idea of going to a poll entirely (and voting for reps, too, but that's another story). 
Can easily be done from any smartphone, but the option is there to restrict to polling places if we, for example, licence the software to a state which wants to do that. (Basically you treat it like multi Sig where one party is the government or electoral  body; no requirement for this though)
@_date: 2016-12-23 06:26:58
If you're interested in having anyone from [Flux]( come to speak please drop us a line (media The Flux Movement is essentially a big blockchain/politics/democracy startup/movement thing.
Some reading if you want: 
@_date: 2016-12-26 22:45:30
It's actually a really crap analogy (and it is only an analogy).
Quantum computers do 2^N operations (max) where N is the number of qubits they have.
So an inefficient quantum computer with just a few thousand qubits could easily perform more operations than a classical computer made from all the atoms in the observable universe multiplied over the entire life of the universe.
But, of course, they're only good at doing certain things.
If you want to know _how_ they're capable of this look into Everett's many worlds theory, which unlike the standard model, does explain this precisely.
@_date: 2016-11-14 06:50:05
Whitelist, basically an electoral roll lite stored on-chain that indicates which pubkeys are allowed to participate in elections. Because we anonymise votes we can actually link the electoral roll id to the whitelisted pubkey without compromising secret ballot.
@_date: 2015-08-13 13:39:14


~~yeah, okay, let's also presume they don't give a shit about their uplink. come on.~~
~~Oh, they're in a disadvantaged sitch, you say. Well, why should the Bitcoin community, in general, be accommodating to their lack of willingness to coordinate from a digitalocean node, where their miner can communicate via the blocktemplate model instead of communicating the entire block, each time, like a *fool*. This reduces your 1 second of 600 to (80 / 1,000,000) / 600 =  0.00008 / 600. Which is small enough to consider network conditions which you just called 'negligible'.~~
~~SPV mining isn't cheating. You only cheat yourself if you mine on SPV evidence without validation, but no-one besides the loser is to blame for that.~~
~~We've already come up with technical solutions for some of the problems you mention. Srsly, dude.~~
@_date: 2015-08-12 07:27:41


That's a matter of opinion, unless you mean something other than 'just fine' such as 'as they have been implemented through Bitcoin's recent history'.
If it really was 'just fine' I doubt we'd be having this conversation.
@_date: 2015-08-12 23:35:03


Okay, but we were never going to have a clear road ahead. Bitcoin has never been done before so a not-clear-road ahead shouldn't be unexpected.
I don't see larger blocks as the end solution either, but I also don't see why we would stop here. There's nothing wrong with kicking a can down the road provided we're looking for other solutions along the way, in fact, that's really the only point of kicking the can down the road.
What's wrong with increasing the size limit with the understanding that lighting infrastructure will be stable and around in 3 years (or w/e) and that we will only need to do this once?


What about in 2 years? The fact we're using 50%+ now is alarming precisely because we've never used this much before.
@_date: 2015-08-12 23:36:09
That would be an equally good explanation of 10 KB blocks or 100 MB blocks. Satoshi isn't here to defend himself, so it's a bit unfair to pin this on him.
@_date: 2015-08-13 04:09:50
1 KiB = 1024 bytes; 1 kB = 1000 bytes
But also who cares -- we understand each other
@_date: 2015-08-12 23:30:13


So if we were having this conversation a year ago would you advocate a 250-300k optimal size?


Is the primary reason for keeping the limit low to keep Bitcoin 'decentralized'? (in quotes because a small block size does nothing to guarantee decentralization, just prevent one method of centralization)


Why would we ever need to increase it in the future if we softforked it down? If we'd need to increase it in the future how do we know that moment isn't now? (I've read your comments on how many non-spammy txs are present in the blockchain, but at some point these things become indistinguishable or meaningless (IE sufficient obfuscation would mean anything could *appear* as a nonspammy tx) so I sort of see that point as moot and an evil we'll have to live with to some degree.)


Sounds a bit like micromanaging block size forevermore (or until we have another option like lighting). Is that what you're proposing? (I'll count a market mechanism that sets a hard limit as yes, so the micromanaging doesn't actually have to be done by a person or group)
@_date: 2015-08-02 05:08:10
I think I finally see where you're coming from. I think the problem stems from the lack of *conversion*. When mining in Bitcoin land there is a great deal of risk, and so although it is a near-perfect commodity market it is largely tackled by experts (which in turn helps keep it near perfect). However, the burn chain is a conversion that has no future prospect of value besides influencing Bitcoin's development future (and is in itself non-convertible).
Another option is using coindays destroyed, but that's *more* plutocratic because they're a renewable resource (though you do have to hold the coins, so coinbase would be a massive player). At least a bitcoin destroyed is a bitcoin lost.
Anyway, my real point in all of this is that prior to OP nothing relevant came up on google when searching for "decentralize bitcoin development". This is the *first* proposal for fully decentralized software development (albeit with a bit of trust), and the digital consensus mechanism works, even if it doesn't work relative to idealized weighting.
@_date: 2015-08-16 01:02:57
We will not agree, since our definitions of all the words we're using are different.
If you'd like to understand my reasoning you should read what David Deutsch has to say on static and dynamic societies in *The Beginning of Infinity*. Basically the argument is made that only a dynamic society can continue indefinitely and that requires the rejection of bad philosophy of which censorship is a product. Thus the only resilient and permanent societies can be ones in which censorship is not practiced. We're on the way there.
Karl Popper talks of open and closed societies which are a very similar idea (Deutsch has tweaked it to unify with a bunch of other things, such as meme replication)
@_date: 2015-08-01 06:23:39


That holds for mining, PoS voting, elections IRL, in fact, it holds for pretty much everything. The argument 'rich people can do more' isn't convincing to me because it can be used to justify almost anything (and thus really explains nothing, and gives no insight).
@_date: 2015-08-15 22:31:25


Censorship is not in the spirit of *CIVILIZATION*. Holding oneself immune from criticism is a great big huge fuck-off sized red flag of nope -- it's a hallmark of bad philosophy and faulty reasoning.
@_date: 2015-08-20 12:53:51
"A notable feature of this proposal is that it does not preclude soft forks in the future that adjust the limit downward or that simply delay the next doubling of the limit."
Love it!
@_date: 2015-08-13 13:49:06
~~Your proposition seems to be that we should *create* problems in order to spur innovation. Somehow that seems.... counterproductive.~~
~~To paraphrase "when Bitcoin's consensus model failed to improve the protocol the community didn't declare the experiment a failure. Instead they [[invented solutions not yet known to anyone]]."~~
~~Then you get redonk. Like srsly, are you actually advocating reducing our capacity to spur innovation? Like let's shoot ourself in the foot to develop better prosthetics? Come on dude, it doesn't count if **we're the shooter**.~~
~~Anything more than 1MB is a crutch? NO SIR! ANYTHING MORE THAN 100KB is a crutch, and I challenge you to produce and argument that works for one and not the other.~~
~~Would NASA have behaved differently if 70% of the core dev's didn't want to fix the antenna? It's totally perspective and you're trying to lay some sort of adversity over the top, **or at least that's how it seems and if you want anyone else to believe differently you should probably do something towards that end, instead of spouting analogies**.~~
@_date: 2015-08-13 00:09:16


Okay, so what's wrong with saying 1 MB is in the ballpark, but we don't want to hit that wall yet, so let's up it to 2 MB and give ourselves a known date by which we have other infrastructure in place, such as 2017-01-01?
That way, we a) acknowledge we *don't* have a solution today, and b) acknowledge that we *need* a solution in the future and that we're working on them.


Sure, but there will be other sticking points like this in Bitcoin's future, even if it's impossible to know what they are now, so there's no panacea, even if we did have mature lighting infrastructure today.
Problems are inevitable, *and* problems are solvable.
@_date: 2015-08-12 23:40:42
has actually posted about that before: 
I understand there are specific criticisms around the selection criteria for which blocks to measure and how, though. In particular, luke-jr says at the top:


@_date: 2015-08-12 22:36:39
You're *starting* to convince me. I just posted [this]( if you're interested in commenting. 
@_date: 2015-08-02 21:30:11
Thanks for that. I was linked to the same article yesterday but haven't had time to read it fully yet.
I do like the idea of market based solutions far more than voting based solutions; intuition seems to go a lot further.
@_date: 2015-08-11 08:29:18
If Bitcoin XT is a forkcoin then Bitcoin Core is also a forkcoin: Bitcoin XT and Bitcoin Core will remain fungible (in the *physical* sense -- they are literally indistinguishable) until the moment of 'decohesion' in which a &gt; 10^6 byte block is created. That is to say that they are, for all intents and purposes, currently *identical*. Raising the block size is not a [prohibited change]( and does not invalidate any economic contract inherent to Bitcoin. (Then there's the supermajority thing too.)
Bitcoin XT is just as much Bitcoin as Bitcoin Core is, and 'forkcoin' doesn't have much of a ring to it. One of the more important facts is that Bitcoin XT and Bitcoin Core will remain indistinguishable until the first incompatible block is created, and 'forkcoin' doesn't really capture it at all.
We could call them for what they really are: **policy options**. As we move forward, reality will judge us if we are unable to remove the bad policies, and so the question becomes which is the worse option.
Vote 1 XT, etc (unless something better comes along)
@_date: 2015-08-20 12:58:00


What urgency?
This is born of a wider general truth: static societies and communities eventually come unto a problem they cannot solve **exactly because they are determined to continue their own stasis**. Recent actions by some community members reflect this, in some ways, IMO. We are required to solve problems to survive (see *The Beginning of Infinity* by David Deutsch for more).
@_date: 2015-08-13 05:23:20
There logically can't be demand unless Bitcoin is unable to fill capacity or becomes unsuitable.
@_date: 2015-07-30 23:00:18


So does mining.
Richer peeps also have more at stake, so logic dictates they'll be more inclined to help the network. (Well, those who hold Bitcoin, at least).
This was never about being 'democratic', btw. Voting is just a way to not have a central authority.
@_date: 2015-07-30 22:12:29
Nope. If the code existed we could do it tomorrow.
@_date: 2015-07-30 22:21:16
If that were true would you expect to see as crazy shifts in the biggest mining pools as we do? Seems to me that maintaining a mining advantage is much harder than a monetary advantage.
@_date: 2015-08-13 00:05:44


I don't think I do either.


As in he believed it would *not* be changed today (or in a few months or w/e)? My impression was that it was originally arbitrary precisely *because* he understood it would be changed or removed later on.
@_date: 2015-07-30 21:47:25
The idea embodies fallibilism, so if it fundamentally broken it's designed to fail.
Also, just replace 'delegative democracy' with 'weighted preference graph'. 


@_date: 2015-08-13 05:22:26
I don't know of any reason to keep 1 MB. There seem to be some reasons not to change, though.
Who's not there? A bunch of core devs, apparently.
@_date: 2015-07-30 23:16:16
I think we half-agree.
For me, the most important thing is the *creation of new options* and the cycle of *conjecture and criticism*. On the whole this community is really good at doing that, which is one of the main reasons it will continue to survive.
However, what we're not good at doing is planning and discussing things reasonably. There's a lot of 'because I said so' type stuff from some core devs which is discouraging, and what I refer to in the intro. There's also support for a lot of bad philosophy, which in this case I particularly define as things that are easy to vary. The 1 MB limit is one of these things: if such a limit was optimal our explanation of why should indicate the optimum size. Since there is no such explanation because 1 MB was (and is) arbitrary, the fact it has so much support is what's worrying.
For more on the construction of good explanations see *The Beginning of Infinity* by David Deutsch.
@_date: 2015-07-30 21:45:25
If I wasn't worried about malicious actors I might have suggested we conduct the whole thing on survey monkey.
Long term manipulation requires constantly burning Bitcoin, which is beyond the capacity of anyone indefinitely. Also, if we tried this, and it didn't work, then we can say "*shrug* guess we go back to github". It's not ideal, but it is also certainly not a foregone conclusion.
With this in mind, why would those actors bother at all? Since *we* have very little to lose if an attacker comes along.
@_date: 2015-07-30 23:39:42
Hmm, okay. Let me rephrase:
Mining power is dependent on a number of environmental (economic) factors that change regularly. This presents chances for new mining powers to emerge, as we regularly observe on a timescale of months. Despite their best efforts, these miners are unable to predict the future and so must risk money to maintain a mining share, changes in economic conditions mean it is easy to lose out. Thus, even if a centralized state exists it is ephemeral.
On the other hand, bitcoins do not degrade and are not subject to upkeep in the same way. While economic factors on a holder will sometimes convince them to sell or buy, they do not need to risk anything to maintain their share. This means there are fewer opportunities for disruption to the distribution, which means that there is no tendency towards decentralization without considering external factors.
Do you have a link to graphs of pool share over time? I can't find any with my brief googling
@_date: 2015-07-31 01:20:38
We're still stuck on the whole definition thing. IE: Democracy doesn't require voting, IMO.
I'm not talking about things like the P2SH vote.




Again, let me point out the problem with using the word democracy:
For me, these are not mutually exclusive: Bitcoin is not democratic, Bitcoin embodies democracy, Bitcoin requires preference allocation, Bitcoin does not require voting.
I can't help but feel that you're just saying things and not really responding to me.
As an aside: I recommend you read *The Beginning of Infinity* by David Deutsch.
@_date: 2015-07-30 22:04:25
Which is precisely why I'm not advocating democracy. I'm advocating an opt-in weighted preference graph where involvement is decided by how much you want to pay.
Also, that's not a problem with democracy, that's a problem with career politicians, and two party systems, and ultra proportional systems, and pretty much every *implementation* of *representative* democracy.
@_date: 2015-08-12 21:52:17
I don't think many of us were talking about source code management when we were talking about a 'fork'. I was trying to make the point that Core and XT are both Bitcoin in a real sense.
If we are talking about version control then of course XT is a fork, but it's also meaningless to ascribe 'more Bitcoinness' to Core simple because it is 'continuing on the same trunk'. 'Bitcoinness' doesn't have anything to do with where or how the source code is written. (It seems like it does have something to do with 'why' and 'what' though)
@_date: 2015-07-19 02:32:19
Just withdrew 4 BTC from there and the tx was processed instantly.
@_date: 2015-07-01 22:47:11
The top comment in the linked post said this:


I think this is on point and the most pointy criticism of the blockstream guys. IMO Gavin has done a lot to *try* to have that discussion, and the other devs have been disrespecting that.
@_date: 2015-07-30 21:49:52
Heh, it's funny that half the criticisms here are "democracy isn't appropriate" and the other half are "buying votes isn't democratic".
Compare it to mining bitcoin. That isn't democratic either. I don't care whether it's democratic, because the results are good. The same principle is taken here. It's about results, not about 'being democratic'.
@_date: 2015-07-30 23:26:02


I don't imagine it would be that much.
We're not sheep dude. It'd be immediately obvious if ideas nobody supported were being voted for, since they would come out of the blue. Furthermore it's not like the code doesn't go through a peer review process, or that this would be instituted overnight, or that everyone would just abandon github.
How much does it cost to do the same thing to Bitcoin now without the burn-graph? How do you know it isn't happening now? There's nothing that would be possible under my idea that is impossible now, and it feels like most of the cost would be in disguising the attack anyway, which is exactly what someone would be doing now.
Anyway, point is I don't see any existential threat that doesn't already exist.
@_date: 2015-07-30 22:56:45
Thanks :). I wouldn't say it should be permanent, since I hope there are better methods out there, but it certainly *could* be.
@_date: 2015-07-30 22:11:07
Literally a worse idea than ALL of hitler.
@_date: 2015-07-31 03:52:26


So I'm trying to [start a political party]( to change parliamentary dynamics here in Australia to basically [force]( better explanations in policy development.
If I reframed your question in terms of *that* instead of *bitcoin* it would be: "why not start a new country with this approach?" to which I'd say "I'd love to, but resources are an issue".
The same is true here. I've tried [writing altcoins]( before, and if I had unlimited time and money I'm sure I could produce some sweet-ass dapps. (or some [sweet ass-dapps]( but I digress.)
I'd rather put the idea out there and do nothing with it, and if someone else wants to pick it up (or be inspired one way or another) or if the community is like "shit yeah" then maybe go forward, but realistically it isn't the best use of my time, and even if it were, I don't have the resources. There wasn't enough "shit yeah" type responses for me to consider actually working on this in the near future.
@_date: 2015-07-30 22:06:56
I considered PoS but rejected it because it gives a permanent privilege to the rich. With proof of burn a rich guy would need to out-burn everyone else for as long as they wanted to maintain an attack. Just like attacking the blockchain in the same way, it is inefficient and wasteful.
@_date: 2015-07-06 01:47:32
I've met some of the guys down in Melbourne behind the Bitcoin Group. I can't comment on their business but they're pretty heavily involved in the Melbourne Bitcoin scene, and helped set up / admin the [Melbourne Bitcoin Technology Centre](
@_date: 2015-07-30 23:03:02
So what is "one dollar destroyed, one vote"? Seems to be neither Capitalism, nor Plutocracy.
@_date: 2015-07-31 00:35:29
I think that might be a consequence of our definitions, then. For me, Bitcoin embodies 'democracy' and offers a number of serious lessons that are not in any way incorporated into mainstream 'democracy', both of which are different things. e.g. The only parts of the democratic system that matter to me are 1) its ability to remove bad leaders and bad policy, and 2) the inclusion of as many options as possible. Representativeness or proportionality have nothing to do with a good democracy IMO.
Voting systems themselves don't have much to do with democracy in a canonical sense, IMO, and canonical democracy has very little to do with my proposal. Sooooo much butthurt about that word in this thread (not referring to your original comment, further up there is some mad vitriol).
I've come to the conclusion referring to democracy at all was a mistake in the OP, and should have used 'weighted preference graph' or something instead.
"opt-in weighted preference graph /= decentralization" sounds far more like a nonsequitur than "democracy /= decentralization"
@_date: 2015-07-30 22:54:12
Correct. However, the development of Bitcoin is not decentralized.
@_date: 2015-08-13 05:19:54
Gavin has tested blocks up to 200 MB large and Bitcoin Core handles them well.
You could do something like that where the max block size cycles like 1,2,4,8,16,1,2,.... but you'd need to change some code. An altcoin could do it. But I don't see why you'd really want to (as there are 'better' options)
@_date: 2015-07-30 22:01:09
Yes and no. This is certainly the worst I've observed (so no) but we haven't destroyed anything yet (so yes).
@_date: 2015-08-28 05:04:37
By 10s of millions of years.
Also, would you say that Penguins are closely related to Emus? I mean, logically, if their ancestors are close to them, they must be close to each other, right?
@_date: 2015-07-30 22:00:09


Yup. It's the times when we're *not* at our best that I feel are the most crucial, and they are particularly the times that meritocracy does not work.
@_date: 2015-07-30 22:10:28


Gloves /= baby powder.
Two can play at that game!
Seriously though, did you even read it? The decentralization is neither equal to nor derived from the voting. Voting is just a way to not have a central authority. Decentralization is the 'anyone can participate' and 'hosted on the blockchain' bits.
@_date: 2015-07-30 22:14:35
It doesn't require a hard fork. Also, it's not useless without being enforced. Canonical source code isn't enforced now, it's just the default.
Emphasising the word 'useless' doesn't make it any more true.
@_date: 2015-07-30 21:57:32
Dude, voting already exists. Have you seen a pull request? ACKs and NACKs everywhere. Except atm only core devs vote.
Also, by definition this would be a decentralizing influence, since anyone could participate. In fact, most criticisms in this thread seem to be that it is *too* decentralized and that this compromises the integrity of the software by allowing bad actors in.
@_date: 2016-02-17 06:52:00
Max, Leader of Flux here.
Shitstorm is a pretty accurate term. Two parties with polar opposite views (conservatives and greens) are teaming up to 'reform' one of our voting systems which also happens to screw most minor parties over. Thanks to a quirk of our system we have the most diverse senate in the developed world (which pisses off the major parties). Legislation to push through their reforms hasn't been finalized, but might drop in the near future. If it does, *and* if our Electoral Commission can retrofit their software in time (probably not) then the next election will be much harder for us to get a high enough % of the vote to win a seat, though we've got some cards up our sleeve yet. However, we might see an odd situation where the legislation passes but not early enough to actually change the voting system in time and then we have a much better shot.
That doesn't even include leadership troubles (for major parties) and the potential for a double dissolution (DD) (basically if the houses are locked and can't pass a particular bill (three times) then both houses can be dissolved and we go straight to an election). This is a curious case because our senate is elected in two stages (1/2 every 3 years). However with a DD the entire senate is re-elected which makes seats half as difficult to win, in which case we're in with a better shot.
So there are like 4 distinct ways this could play out for us and only 1 is terrible, but the odds are definitely not even between those possibilities.
@_date: 2015-07-01 22:44:02
Two responses:
1, None of those things actually help the block size *now* though. The debate is about the next 8 months and how to not hit a wall, not about how good blockstream's tech might be in the future.
2, That explanation seems to ride on the idea of their work being open source, thus 'anyone could do it', thus they might not make money / anyone else can / commodity market, thus criticizing them for blocking a size increase is wrong.
I think (2) is beside the point. Until recently 'they' had been contributing in a non-constructive manner. Not just that, they acted like the problem couldn't or shouldn't be solved, despite being fully aware that a) Gavin's interim solution (8 or 20 MB blocks) would fix the problem for the moment and has been well tested and b) we will have more hard forks in the future so taking the time after fixing this problem to design a good long term solution is totally acceptable. These are both mistakes from a point of leadership and epistemology. That has changed slightly since BIP100 but even now I don't feel they're acting responsibly. Defamation is an easy way for people to express their feelings of betrayal or their thoughts on Blockstream's inadequate engagement with the issue. The defamation may be wrong, but the *source* of those emotions does not need to be.
@_date: 2015-09-08 11:42:37
That would at least settle the debate.
@_date: 2015-09-04 00:42:14
Agree'd, though Luke jr actually offers a full-ish explanation. It so happens that many of us disagree with that explanation since it has assumptions like `*just* enough space in blocks is optimum`, and the figures he uses to calculate that optimal level do include some bad philosophy (`I don't like these txs so I'm excluding them`) but on the whole he's actually put his explanation out there enough to be criticised, unlike the TERRIBLE philosophy we've got from some of the other core devs who are acting like children, or worse, the type of economist that gets on tv and says "things are too complex for plebs to understand so don't try, just trust me."
When I say 'bad philosophy' I'm talking about qualities like 'easy to vary' and 'suppressing criticism'.
@_date: 2015-09-21 12:12:21
I was also directed from Slashdot at the same time. I think my first tx was around the 10th of December 2010.
@_date: 2015-09-08 05:23:07


The context here is this is an address from  which attempts to pay miners for producing certain types of blocks.
@_date: 2015-09-08 01:06:51
The all-voters-have-equal-say is not implicit or required in this sort of vote. The point is to vote a whole lotta times.
@_date: 2016-10-12 01:25:57
Sadly not in Australia; $21 on pimpd v $18 on etsy. Not sure how shipping or volume would change that though.
@_date: 2015-03-01 02:57:29
The bottle necks for including arbitrary data in a transaction are:
* SIGOP count (this can be thought of as a numerical estimation of the CPU time required to process the transaction; there is an upper limit per block to prevent DoS attacks)
* Standard Transaction specifications prevent most clients relaying and mining 'nonstandard' transactions, but a few pools do mine nonstandard transactions (such as eligius, I think). In a standard transaction you can include 80 bytes, which mentioned.
* In a nonstandard tx you can go bigger, such as this tx from 2013 which both triggered an electrum bug **and** contained the patch, very clever:  -- In the discussion somewhere I remember reading about a maximum number of bytes allowed with an OP_PUSHDATA but can't find it. Either way, you can put huge amounts of data in as encoded addresses or (even better) as public keys in transactions that just *look* legitimate, but secretly will never be spent because the pubkey in question was not generated from a private key.
Let's stay standard, though, because we can do a lot and it'll confirm quickly. You can see over at  that people can include a lot. The largest sample I could quickly find was ~950 bytes, so quite a bit of text.
Theoretically you could mine a 1 MB block full of standard transactions and probably get about 80% efficiency (though I haven't done the numbers).
Also, [this tx]( contains the original satoshi whitepaper PDF, so the tx is nearly 200kb, but apparently someone still mined it.
@_date: 2015-09-08 01:04:14
Here is a stronger approach using proof-of-burn: 
@_date: 2016-05-02 10:14:48
Sure, and I wouldn't have disagreed with your original statement if you'd said:


I smell bullshit.
You would have made the same point and been more resistant to criticism.
@_date: 2016-05-02 09:52:43
both happened.
@_date: 2015-03-02 03:15:53
... not really. I mean maybe, but I can't think of a real case that would benefit.
Side-chains are **specifically** to do with pegging Bitcoin's value and maintaining scarcity. Side-chains are no more suited than tree-chains or any AuxPOW enabled chain, or a separate chain altogether.
Best argument I can come up with (pro side-chains in this case) is `it's cheaper`which is not a strong argument given the competition.
@_date: 2016-05-02 09:46:53
It's a hosted blog afaik...
Please confirm it's not an error on svbtle's part.
Edit: `chasedittmer.com` is also hosted via svbtle, so the best explanation is that this has nothing to do with Gavin. IMO this reflects more poorly on `
[screenshot of tweet incase  gets cold feet](
@_date: 2015-03-01 03:33:57
Yes, but it would be expensive and need to be split over multiple blocks (due to the 1 MB limit).
@_date: 2017-07-29 09:08:52
Glad to help :). Yeah, I'm probably in a vast minority so hopefully it's not too significant an effect. (will become more common though, esp if you were to interview the Ethereum community)
@_date: 2015-03-09 04:40:07
These things are borderline in terms of needing a citation. The above is pretty much general knowledge for educated people. General knowledge does not require a citation.
* Mass extinction event every 50,000,000 years (give or take) = 5*10^7
* ~30,000,000 seconds per year = 3x10^7
* thus Chance of a mass extinction even in the next second ~= 1 in 10^15
* Number of addresses ~ 10^45
* Roughly half the index for number required before a single collision (ROUGHLY) ~ 10^20 addresses
* Time taken to reach 10^20 addresses if 100 billion (10^11) new addresses were used per day ~ 10^9 days
* Days in 1000 years ~ 3.5 x 10^5
* thus Chance of collision with above network load over the next thousand years ~ 1 in 10,000
I'm ignoring a bunch of stats in the above, but for a BOTE calculation it's okay. 
@_date: 2015-03-01 06:25:45
Depends, $0.04 / KB is probably an okay estimate, so maybe a few hundred dollars for a song.
That said, you could just go with no fee and hope for the best, but it's unlikely to be mined.
Given that we're close to the 1 MB block limit you'd probably need to be competitive with the fee.
@_date: 2016-05-02 10:01:02
Satoshi avoided everyone like the plague after that...
@_date: 2017-07-29 05:47:31
Hey, some comments: (obviously too late to alter your survey, but might be useful when discussing results)
* You don't give an option for 'highly skilled but unqualified' under the job question - I dropped out of uni to pursue Bitcoin stuff in 2012 and currently implement cryptographic protocols over blockchains, so picking 'unskilled' didn't feel right (ended up putting self employed)
* related to the above: you don't have an 'infrastructure' use under 'how do you use Bitcoin' - my main use is building stuff on top, not using it as cash. Maybe that's not what the question meant, though. Picked 'all of the above'.
@_date: 2015-02-27 09:12:23
Please take your badly justified hate elsewhere.
@_date: 2017-06-27 01:26:35
Yeah, should say 'appox' like in Q1
@_date: 2017-06-27 01:28:24
    10
    4
    21mil
    Deflationary
    Blockchain
    Double spend
    2008 satoshi Nakamoto (live 2009)
    2009
    Hashes
    Byzantine fault tolerance
    Cold
    Full
    Simple payment validation
    Qr
    1 3
    True
    False
    Unconfirmed
    Satoshi
    False
    Signature and pub key corresponding to the hash in the scriptPubKey
    False
    true
    Transactions and previous blocks
    True
    Sah256
    App specific integrated circuit
    Pool
    Genesis
    Gavin andreson
    FPGA
Of note, I said 'hashes' instead of 'proof of work' and 'byzantine fault tolerance' instead of 'byzantine generals' - otherwise all g.
Was fun, and would have been very worried if I'd got any q particularly wrong.