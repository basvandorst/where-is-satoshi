@_author: prawf
@_date: 2016-03-06 12:11:39
Obviously, there is a significant portion of the r/btc community here now that is quite outspoken. I assume that they will present their own views as well.
Without ad hominem attacks on the Core Developers, Blockstream, Brian Armstrong, Gavin Andresen or ANY other person/institution, let's talk about the technical problem in language that the Bitcoin "lay folk" (if you will) can understand. Please, in this thread, spare us the conspiracy theory-ing (from both sides!) and discuss technical limitations.
Let's outline where the mathematical, technical issue is with scaling.
Obviously, raising the block size makes Bitcoin more difficult to run for a LOT of people in developing countries where bandwidth capacity just isn't there for running nodes. As well, the hard drive concern exists- though honestly I see this as a minor, quickly disappearing issue.
If we want to truly bury the Classic insurgency by means of popular vote, why not just take away their only real advance and just implement 2MB blocks AND SegWit ASAP. It seems to me that it is feasible for the network (as seen by the Classic AWS model that, at the end of the day does seem to be working) can support 2Mb blocks.
I'm a semi professional gambler, and frankly it would be great to see GOOD poker sites using BTC. It would be a massive bastion to the whole ecosystem, and it would be adopted universally. But I'll be honest: I don't want to wait another year and change for this to maybe happen.
So I suppose through all this rambling, what I'm asking is, in simplest terms, what are the technological roadblocks from growing the blocksize and allowing more transactions?