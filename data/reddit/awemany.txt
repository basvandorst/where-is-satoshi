@_author: awemany
@_date: 2018-09-22 09:44:23
That is the time stamp on the chain. Which is only every 24h .. due to fees. But read carefully have a look at the timestamp of Originstamp itself please.
And I dare you to go to the Core devs and ask them to falsify my information. They won't.
@_date: 2018-09-22 08:33:32
Yes you can. Thank you very much! Pecunia non olet. Even when it is BTC :-)
@_date: 2018-09-22 01:55:02
Thank you!
@_date: 2018-09-22 10:37:46
My perspective on the proportions is different. Also, there's a lot of anger still due to the history of how it all went down. This is not just the trolling from yesterday.
I think that unfortunately no "unifying carthartic moment" is possible in this space, as this would have had many, many chances to happen earlier.
In that sense, I would be happy if we can have it on a level like Pepsi and Coke are competing.
Not at all completely nice to each other, but also not underhanded and most importantly, not violent.
@_date: 2018-09-23 09:07:04
Thank you.
@_date: 2018-09-22 10:09:53
Thank you. Yes, it was a bit salty. I have endured about 5 years of bullshit though. I needed to vent. But absolutely noticed.
That said, just because I was salty and vented does not mean that I am not pointing at real problems and despicable behavior on "Core's side". And yes, I am also aware of the problem of lumping folks together.  But that goes for the "btrash, btrash" chants as well.
@_date: 2018-09-22 09:25:09
Doubts about my identity seem to crop up, so I like to address all those once more. Hopefully in a comprehensive way.
First of all, to explain the situation from my article again, originstamp.org is my go-to service. Usually, 24h is *plenty* and suffices to timestamp everything.
But in this case, Core went quickly ahead with release information, which made the 24h window (due to fees) too small to conclusively prove ownership *on the BTC chain*.
But let's have a look in detail. This is the text that I wrote:
    BitcoinABC does not check for duplicate inputs when processing a block,
    only when inserting a transaction into the mempool.
    
    This is dangerous as blocks can be generated with duplicate transactions
    and then sent through e.g. compact block missing transactions and avoid
    hitting the mempool, creating money out of thin air.
    
    If you SHA256 this, it calculates to: 5c45a1ba957362a2ba97c9f8c48d4d59d4fa990945b7094a8d2a98c3a91ed9b6
Exhibit A:
I timestamped that here: 
Note that there is a timestamp when it *entered* their system, which is before anything else became public and which is:
17.9.2018, 14:54:19 CEST
It shows it in your local time zone in your browser, a fact that Peter Todd apparently tripped over as well: 
Scroll down to "Submission to OriginStamp" at the end.
This timestamp is, however, just from their service and thus centralized. But if you think I faked that, that would mean that I must have hacked their service in time to do so. In the last few days. Furthermore, the window for this hack would be quite small, as there is also a later submission into the blockchain.
So if you doubt this information alone, it would mean I'd had to hack the service in time (within a few hours window) just to claim this identity, leave no trace of all of this, face the risk of being called out by the true finder of the bug (who'd be different then) and write this long article ...
But there's more:
Exhibit B: For anyone who is a member of the BU slack, I posted a message that was the above hash (as I said in my medium article) and which is still sitting unedited on the slack as well, in the  channel. There are likely several hundred members of this slack, and all of them who read it should have seen this message in time. I believe there are also (well-behaved) Core supporters in there. I would need to have hacked that service in an undetected way as well and fool or collude with all active members therein as well. That now creates a pretty big collusion, don't you think?
Exhibit C:
Finally, let me close with this PGP signed message. I created a PGP key just to keep my identity separate, at least for a while, from my main pseudonym awemany. And in the email I send out to the developers, I have added myself as a recipient. Even though the message has not been signed (I didn't see any reason to do so at the time of release), my full key id is still in this message. And that is, as far as I know, a 128-bit hash for which it is practically impossible to find a preimage for. This explicit 'encrypt-to-self' is because I fucked up with PGP encryption in the past (because, as I say in my article, mistakes just happen) and I wanted to at least be able to read my own encrypted message later. I have created sitations for myself where I wasn't able to read my own encrypted emails. Yes, call me a crypto noob, say PEBKAC or whatever, it is exactly an example of why I am saying that *I am not perfect* but so is no one else!
Here is this message, which I am sure anyone owning the original disclosure email is happy for you to confirm that it is the same key id:
    -----BEGIN PGP SIGNED MESSAGE-----
    Hash: SHA512
    
    This messsage is signed by the
    beardnboobies GPG key that I created
    just in time for the vulnerability disclosure.
    
    In reality, I am on reddit and elsewhere.
    -----BEGIN PGP SIGNATURE-----
    
    iQGzBAEBCgAdFiEERGszUXtt2s3Wfkt1yydp8d93NcQFAlumBkAACgkQyydp8d93
    NcQvegwAmcfqKSp/RZVE6HIyN9gbxa5oz2YFaaoeVCoQTsDZPX08zjBjp7jzMUGW
    izraVk+yOz8Yxdv7re8G+CBqnpgfpNvMoHPe75bgoyKzavTtukVSScDUHZ9Tu9D7
    xQcfWnwZhsUjsTsxFD7B6PLAWzeh7cA3d0xUwrFJoa//hlOylnlC/76cbBspqSll
    ispvQgBcEM6NfKvmCTb9LItts2/QrXX891LK9I4vPC1WpOrXPA9lNnuuP8/S/ey9
    O7iqwW+oCwGKLELQJE58hgwt7keQukrPEfwUtBXACW77gtk1dXaxRL5RqCkmMsMn
    rBMkTGmjDit+AVE/5oW+flds8/Hq+kQDXUZfaLbnOrleW50LTTi+etA/PPhHxe45
    CUD7Jm8d2LbTIjFWsZT/Rq2Djsy3gBcHeKqFMRXEBI7WoFe431q38gVSyfvbCrPR
    R4AJsg2eGgysu0E/SZecHHULc4CU6RdLmCRrORRSv1T9tOyJcRpfwRlE4FnT9LTC
    /+5v9mXI
    =k2oE
    -----END PGP SIGNATURE-----
And here is the public key which matches that key Id and which has likewise not been made public yet:
    -----BEGIN PGP PUBLIC KEY BLOCK-----
    
    mQGNBFufufgBDADJ3N5xocCOSyRrF42nvrujUZXRPnaq+X3E0GjNlCwuCFZELNE9
    l950cR4l+sNFbjcvWtlCgAdHPAggED3ZeutTO3fAIClN+LOgnyEF4txjdG72j9L4
    NnCVMfKhT2yc7JZQh3lS+GHFSBS8joLq09GxllTORvdawuW34yzV4rzFZZ3NfK+/
    8BtNAf+nXvtafugw4Nlln5LPvGna9bmh/74RlZTAJeV52a/WsucBQ7kVuWTAERMy
    N+DuvUIxh7gG9KbSQXsPQ+1ZleO9+nWJs4pgX3ro6ZRMYvN9jeJsDjx2uQoL77zM
    RwMKNis5ifxnkHmExOG01SQxz3j9tw1anC8dFi2zs9jlr+qjUofSUT0RctKNJlga
    BgDV1dsu8dg11xxo4slH93D5LqJJs3lg+RjxHeWE6Oxvpz4SQpU+sLT4T73xOh/d
    GDw4UmLMUgKjjlYexVhlNk6FUamAkpYzuTgN35AeUt1iGj9D9XAbbi0G3MjKYSX6
    tPkBC5h7XIGDzGcAEQEAAbQuQmVhcmQnbidib29iaWVzIDxiZWFyZG5ib29iaWVz
    QHByb3Rvbm1haWwuY29tPokB1AQTAQoAPhYhBERrM1F7bdrN1n5LdcsnafHfdzXE
    BQJbn7n4AhsDBQkDwmcABQsJCAcCBhUKCQgLAgQWAgMBAh4BAheAAAoJEMsnafHf
    dzXEi0gMAL0StgXSH4mbHPeyj0pJOmzOpEsfm7S05EKoGnMzmB/ZfCxag9YvDSSQ
    Jz28jOmPIrnLLkuOFcf0BnSKmys2WbEpGm5SgRU0anSTiiaTy2RjPa8eC34F6X/q
    LjgJ6J4hvOoDkQAjOzfspayRjRmFewNzssMHn6JC2NWvP+8+nClsJA959E9rxJ5F
    xaPmPZ9g4AJFah/vpRXbv44JQGbjr42CdB2JUTYW3rd7WjYFdcGcPU0UQhRQSflL
    2ZOCw8bJCdPRRXpy2xTewTPE4eVcrclvmbKDhDbDNkY9cqDSPqag2JG8GoPsl3Ym
    33uwzN1Y5qkocfGoVxr3eEEFQgkPnqX27OyGAL1+MoEOYuLuhUaNX2E/WmPZwtU3
    E5JdjdIRfVfzI+oWs6Mfn1mbxeePBikjHgNgr4vs2+DkujeenS8UsD5Y6qrk9Ypt
    Erh5GRT0BauSSV52U3mEboMyxRHriObFT+BQAK0cJ4ZZ9aAUVLZcC4TXps2PKcjZ
    ozJYgvFm1rkBjQRbn7n4AQwAx7JiWJSuwAidK0AcPS2kt5gpzsESgxq1qyoeELYg
    tNb6G2SihbFj4hVMjc8Ol+a0wtcd+3D7Wcyu5EDbfnIydfmytIvF6CABWCkKtulG
    lxKSydMg16QGMwWixqTLRo1FoCdAzvKJktTshIlARoRt1cII/5n0C+Ny33kdm809
    c+5EPFW22Hu5cNZR6xjYkONoM+Gw9JVIo5O9DY1l2s7qaQhnnTQDMBJLZjtOVFZF
    l/QQjnM5SJZr7lkzNMOgdA3saCbjk7NVMnV8ledLHYZguR3lDfsfdwWvw9Q3tEp9
    Ii5P3AHzzV7eu0g6T7xpjV4LNssP1abvrBBd/RFfA6A3ec9wXEWTk2ewXpZLkicm
    9VBy3nsz5bedoAvcyTVB0HF80yHbo99eSwEUenlrs0K0Yv97hxJ2ioPrhx4y7M9Q
    XRWRXFRaLBgLT5GxvIs9jRWJq7jwtKknA7GSun06UFKnOmiT81dmVf4Dne1F9y/R
    U7ld9Doo7IARUYP11/twEh5HABEBAAGJAbwEGAEKACYWIQREazNRe23azdZ+S3XL
    J2nx33c1xAUCW5+5+AIbDAUJA8JnAAAKCRDLJ2nx33c1xMiGDACbqHLuXMZ2937O
    aDfuchIYJ7BoqLiY+Po0V78jenYcx4pXXnau2rL44f02B6nV5RK21b+PwFDX+SMh
    usQfAYdBBRxIb0uDePKx2/Vb0UC5yb456eprYBXOIN7odl0J68PpjUQik5kqizig
    n/vyrIMMQehnFFee88xdSUYK495I6URJtIp6YLCYoalFs49l3szLJZK57OcCmfsR
    gzQbBIsPqQ7uqKZlGYZY9a/PYEZd3Lb6qLF693jZyNjDZ8IIfBjvJa3ZwJiTtNXi
    NknfmW2KcokFljOa5Fvs6Gu11Q9KpbVRpkKeHF79TSN5lPSwvBjsBbx9j4KoFBum
    yNNQTclRMe+AWHfcnoIXooFemiv27n6HEwoFEyoKm3ita1V+RiDuZ1e3FEA4zUPO
    XlZv6e7p+Cd0coP4FDWR5mq1ck+SOFoFuqNrqpEIumrHEC4wKcIA7iy/jJ5frgab
    UjEcFa/MBAaZ7If9+3kHh2kpfPwLOT+7Mm7i9kD1Yu3UBvwoYOE=
    =DyTh
    -----END PGP PUBLIC KEY BLOCK-----
I am not going to disclose the original email just yet, because there is exploit code in there. Even though I think that exploit code is quite simple and will likely not do harm, there is no reason to add more risk and this could also still be used against me by trolls by being called irresponsible. So I hope folks understand why I refrain from that for now.
@_date: 2015-12-03 22:54:15
Yes, it is introducing useless entropy at best.
By the way, as you are outspoken and opinionated about RBF, if you have some good ideas about this, we're discussing what to do with Bitcoin Unlimited with regards to RBF.
So you're more than welcome to join at bitco.in/forum !
@_date: 2018-09-22 01:15:09
Fair question. Here you go: 
    BitcoinABC does not check for duplicate inputs when processing a block,
    only when inserting a transaction into the mempool.
    
    This is dangerous as blocks can be generated with duplicate transactions
    and then sent through e.g. compact block missing transactions and avoid
    hitting the mempool, creating money out of thin air.
    
    Apply SHA256. Then go here: 
@_date: 2015-06-16 11:02:12
What is your opinion on the 32MB cap in it?
It is arguably making the proposal self-contradictory. Because
if he says he's for a market based solution, why is he putting in a 32MB hardcap, another cap that will eventually bite us badly?
If the market decides block size won't ever growth above 32MB, that cap is not necessary. If block size does grow above 32MB, the hard cap would be a centrally planned limit, counter to the whole intention of his proposal.
@_date: 2015-06-29 08:12:52
Very true. I do not want to have this debate again. 
@_date: 2015-06-12 09:28:10
Yes, they can, they can in principle build a completely parallel chain if they want, where you don't have any amount of money in it.
Stop clinging to illusions.
@_date: 2015-06-16 08:30:27
^ This!! How can people not see this.
@_date: 2015-06-01 22:47:11
Indeed. Especially, I like to hear good reasons why Greg Maxwell is avoiding a more constructive discussion on a block size increase schedule.
@_date: 2015-06-16 13:16:10
Exactly. 1MB is central planning.
Jeff Garzik's proposal (without the additional self-contradictory 32MB hard cap!) would be a good compromise in so far as it alleviates the fear that Miner's could suddenly produce GB-sized blocks (a fear that I think is unfounded, but that is besides the point, it would help by just addressing the *fear*) yet at the same time allows Bitcoin to basically grow as much as the participants want it to grow.
@_date: 2015-06-12 11:27:38
A 3tps system can easily be blacklisted as a whole - without any public opposition or economic risk. 'Ah, that system that only drugdealers use'.
A successful, widely deployed Bitcoin, with full nodes in many different, competing jurisdictions? Not so much. Even if those nodes are bigger than what you can put below your desk at home.
@_date: 2015-06-03 12:41:34
We are depending on 50% of the miners being sane anyways - anything else is just unnecessary stuff on top.
@_date: 2015-06-09 10:00:50
He was patiently arguing for this since *years*.
He made several proposals, and tried to came closer with his blocksize-increase proposal to what the other devs might accept.
But all he got was deafening silence and arguments which basically amount to concern trolling. 
@_date: 2015-06-13 10:06:30
With some people stuck on seemingly opposing any progress on the blocksize front at all costs (our costs, btw.), the game of chicken might become unavoidable, though.
@_date: 2015-06-11 20:06:26


Ok, sure, with that you obviously get O(n^2 ) (total network).


This can (and should) be solved with UTXO commitments. I concede that there is currently an O(n ^2 ).
@_date: 2015-06-18 19:26:59
It is a de facto, code hard limit (network-protocol restricted block size). It is not, *at all* meant as another hard cap on the blocksize. It can also be lifted. First of all, it would be ridiculous to put *two* caps in, and, secondly, Satoshi envisioned Bitcoin to scale, clearly beyond 32MB blocks eventually.
Using 1MB or 32MB as the *ought* limit is going the wrong way - from technical protocol to meta protocol (original vision). It should be the other way around, Gavin knows that, and is acting accordingly.
@_date: 2015-06-29 08:21:35
I see it this way: Bitcoin core can scale to gigatransactions/day, and should be *able* to, it should not be artificially crippled.
Lightning networks might be a cheaper way to transact and allow things like micropayments on top of Bitcoin. It might also cause Bitcoin to *naturally* not reach gigatransactions/day.
It would be best if devs would take a hands-off approach and let the market decide what works best. Especially not centrally planning a hard cap.
has some excellent insight on that.
@_date: 2015-06-16 08:28:37
Also: Validated UTXO sets, pruning, and having full node run with just a year of data. 
Maybe the data for a network that just keeps 1 year (or some other reasonable figure) could be something for to add as columns to the google docs?
@_date: 2015-06-30 19:19:08
'Can't be evil'.
Hysterical LOL.
@_date: 2015-06-16 23:05:10
That's true, but that's *network code*. AFAIK, the core consensus code can be stuffed with bigger blocks and will validate them just fine.
Also, by putting 32MB into the proposal, it is arguing that 32MB is the next hard cap (or other similar silliness) that was *intended* all the way to be in the Bitcoin protocol. 
Satoshi talked about VISA levels, 32MB is not VISA levels, and so his networking code was/is temporary in that sense.
We should not, in any way, derive meaning for Bitcoin's purpose from that number. That is the danger that I am seeing. As the 1MB limit was just an anti-spam measure, too. It WILL be redeclared to be 'meant so'.
Jeff should actually explicitely put into the code that the current 32MB limit is due to the networking code and meant to be replaced with no hard cap as soon as the network code can be upgraded.
The way he put it is as if it is the next 'meant to be' limit. That, I do not like at all.
EDIT: I am basically repeating what you already said, sorry for this wall of text, I just think that this 32MB issue CAN NOT be neglected as just minor at all.
@_date: 2015-06-11 09:45:17
Huh? SMTP servers are very much in a P2P mode, connecting directly to each other over the internet, which *is* P2P?
@_date: 2015-06-15 21:49:04
1MB blocks are not default, as per Satoshi's vision.
@_date: 2015-06-27 17:46:13


That would be very short sighted of him. Look at the contention that we have now. Why should there be less contention in an *emergency situation*?
Something needs to be done! 20MB! No, I like actually 100kB, the flashing modem lights are annoying me now! No 8MB! 
@_date: 2015-06-02 13:05:00
Yep. I was worried about this since a long time, too.
I also believe that Miners have an incentive to keep the blocksize reasonable, to not kill their own ecosystem. So I'd argue for a very reasonable and adaptive by-default soft-limit (for the lazy-ass miners who do not bother to configure anything), like moving average over last couple days/weeks, and indeed, best would IMO also no limit on the hard size, too. But Gavin's original growth formula would be good enough for me.
When 51% of them become crazy, we have a whole set of problems anyways. So we are already relying on that.
@_date: 2015-06-11 14:13:02


There is still going to be the chain with 51% of the hashing power. That means that there is still a level-0 consensus.
Don't get me wrong, actual consensus between people is a good thing to strive for.
@_date: 2015-06-16 10:25:29
My intent is indeed for each side to make as good a pitch as possible. You should trust your fellow human's thinking skills more IMO: No one who's not interested in BS (blocksize :) issues will ever read these pages.
I think this will still elevate the level of discussion.
@_date: 2015-06-11 19:22:53
The problem is that Bitcoin is bigger than you and me. To stay in your
analogy, the son grew up and has to *some extend* its own will now.    
You had and still have a lot of trust in the community, due to your high
levels of skill, expertise  and in general sound ways of arguing. This makes you
*more* of a 'mother' than many others such as 'just very interested fellows' like me.
And I think most people agree *with that. That's 'consensus'.
However,  you are not the only one - and I think the point in the blocksize
discussion is that there simply *is* a breaking point after which consensus
between the devs will not work anymore. Which has been reached. Not your   
fault, not Gavin's fault - but you can't deny it simply exists.
I must also say that the reference is a bit too emotional for my liking. It is an *experiment*,
not a *baby*. I understand that you might have very personal feelings
involved in this, but I can imagine that expressing them overtly might actually weaken your
position. Because a large fraction (the majority?) of the ecosystem is now
consisting of people who are in for the cold hard cash that Bitcoin
represents. If people start to interpret this as whining (which I am not at
all saying it is!), you *will* be taken less seriously.
I followed the block size discussions closely. I don't know whether you
noticed this, but: 
Gavin has been actually arguing about this for years. He
has made several concrete proposals, downsizing his intended increase from
20MB + 50%/y, to 20MB+40% to 20MB constant.
This was - and perhaps still is - an invitation for you to *haggle* with Gavin on
the exact modalities, dates and so forth of an increase. Because, in the
end, 1MB is arbitrary. And as consensus goes, this number is only temporary.
So unless 1MB is replaced with something completely alien and different,
there must be a new number eventually that goes into its place. A concrete
string of figures, or maybe a simple formula!
Furthermore, Gavin wants to put in this number well in advance, to prevent
cutting up babies. Because, as you can see now, this issue *is* indeed    
contentious and thus needs planning in advance. There is no reason to assume
a short-term hardfork will be less contentious.
@_date: 2015-06-09 13:26:29
It should be noted, though, that ***Gavin's proposals are in line with Satoshi's original goal*** (he envisioned big full nodes in data centers and not under everyone's desk on some remote ill-connected islands).
[Also, this issue being contentious has been predicted as far back as 2010.](
Lets get this stupid cap lifted.
@_date: 2015-06-04 09:08:01




How the heck is OP_RETURN spamming the blockchain?! It is specifically meant to be prunable data!
If you would remove OP_RETURN, people would encode their data into UTXOs that cannot be spend - much worse right now, without a way to coalesce the UTXO set yet.
@_date: 2015-06-12 14:27:01
I am actually getting more suspicious. The whole submission appears to be part of a concerted effort to get into our heads, that no, changing a single constant will suddenly make the world fall apart, Bitcoin needs to stay at 3txn. With all kinds of derailing tactics at every single reply along the way.
@_date: 2015-06-29 08:02:44
You do in the sense that if they go down, you have to wait for your BTC to become unstuck again. Not a problem if I can make a transaction every 3 days or so on Bitcoin. A serious problem if it means my coins are stuck for half a year. Read what I wrote elsewhere in this thread.
The whole decentralization argument goes both ways.
Look at how often and long-winded arguments you hear about the virtues of running a full node for so called 'trustlessness'.
All I am asking for is a reasonable limit - just like Gavin is proposing - that would also dovetail nicely with LN.
@_date: 2015-06-16 21:45:27
BIP100 w/o 32MB cap would do that and settle this issue once and for all.
Sadly, that is not on the table. So I'll go with XT.
@_date: 2015-06-05 18:32:58
A hardfork of Bitcoin with regards to the block time is a whole other can of worms. And I don't think there is any clear consensus on any change yet.
Lets get the blocksize cap lifted (or replaced with a scaling limit) first, before we discuss this.
@_date: 2015-06-28 23:48:24
With about the chance to set up a payment channel only about twice per year, though. Meaning that your Bitcoins are locked in for half a year and you are going to depend on the availability of a centralized hub for that time.
8GB would eventually allow a payment channel per person about twice a week. A very reasonable number, IMO. Mind you, with *max.* 8GB blocks in 20 years.
I think Gavin's proposal looks more and more like just the right fit for this problem. See also my other post.
@_date: 2015-06-14 09:39:24
Open for what?
@_date: 2015-06-12 09:40:36




I have to agree. Greg, I skimmed the thread and read most of what wrote in there, and although certainly contentious and with some name-calling on all sides, I do not see threatening to kill you. Someone else might have done that, and I might have missed it, but not whoever is behind the nick(s) cypherdoc*.
EDIT: Regarding some 'hashfash' debacle - I am very much an outsider on all this - can someone put up a link to what this is all about? I am curious.
@_date: 2015-06-16 21:33:20
Later? This is one of the things at the core of the whole frickin' debate, what people deem sufficient decentralization, isn't it?
@_date: 2015-06-20 09:48:13
Exactly. Satoshi had a long hard look at the blocksize and this was also one of the first questions he answered - that about scalability of Bitcoin. Bitcoin is scalable. No FUD from any of the Blockstream guys or who else yet is convincing new data that fundamentally changes this picture.
Maybe not with Bitcoin on everyone's Rasperry Pi, but rather with bigger (though not huge) nodes in data centers. Exactly as he envisioned.
Meanwhile, bigger adoption rate creates decentralization, not crippling Bitcoin to a toy 3txn/s system that is oh-so-decentralized.
@_date: 2015-06-30 14:46:07
When getting tunnel vision, many bright people can indeed behave in very *stupid* ways.
@_date: 2015-06-17 14:30:46
Yes ... and I think this is exactly the way forward!
Plus validating the UTXO set per block and coalescing old unspent outputs together after a while, to also put the burden and cost of *storage* back to the user - or to someone who wants to do this for him, for a fee.
And this would also allow to bring full nodes online without having to have a full view of the whole chain first.
@_date: 2015-06-15 21:48:14
Yes, they have. And yes, they are very intelligent. But no, they are not infallible. And you now have many intelligent people disagreeing.
@_date: 2015-06-29 08:07:46
Money for which I have to *wait* for it to de-ice first is worth a lot less than money that is liquid.
Again, I think Gavin's limit is just the best match here.
@_date: 2015-06-30 22:17:52
That's what Adam tells us why the whole network scales as O(n ^ 2). [Which wouldn't be any problem from a per node anyways - and the whole internet can easily support a fully scaled gigatransactions/day Bitcoin]
And let me also link to [another argument I made earlier...](
@_date: 2015-06-11 20:44:46
Having a scaling system that everyone can directly interact with?
I am not opposed to things like LN and similar. If it works and gives instantaneous confirmations, works on p2p nodes etc, great. It will move things off-chain, great, let's do it. But that won't help now.
@_date: 2015-06-18 10:07:12


What are those two polynomials describing? Please be more specific.
@_date: 2015-06-28 11:53:20
And NAT is a kludge by the way...
Sometimes I get the impression the Blockcoin guys LIKE complexity.
There seems to be certain character in IT and CS who just likes to build huge, complex Rube Goldberg machines.
@_date: 2015-06-14 21:06:12
Good! So, then, lets keep this from Satoshi in mind:


would be about 400 bytes (ECC is nicely compact). Each transaction has to be
broadcast twice, so lets say 1KB per transaction. Visa processed 37 billion
transactions in FY2008, or an average of 100 million transactions per day.
That many transactions would take 100GB of bandwidth, or the size of 12 DVD or
2 HD quality movies, or about $18 worth of bandwidth at current prices.


sending 2 HD movies over the Internet would probably not seem like a big deal.
And get to some agreement on Jeff's or Gavin's proposal, please :-)
@_date: 2015-06-11 14:06:31
You are right, but the amount of *stalling* that is visible since years does sometimes make me a bit concerned about the Good Faith assumption, too.
And I am not saying 'Look Gavin, 20MB is ridiculous, 2MB would be ok' is stalling. Or 'lets keep 1MB until xyz% of blocks are full'. But simply not working constructively on a plan for that limit at all? Putting it off to the last minute as 'hey, hardforks are easy' (yet a softfork down of the cap would even be easier, and given all the contention about this issue, assuming the hardfork will go smooth in the last minute is just ... insane, sorry).
There is a value, currently 1MB in software, and this value has to go at some point. That always was consensus. So the actual planning for that should be up to discussion very much!
Again, assuming good faith is probably still ok, but I think the point stands that the 1MB-blockistas do have a less productive discussion style than Gavin. And he has been very forthcoming with lower and lower increase proposals.
@_date: 2015-06-12 14:11:00
So tell us... 
What would be your conditions for eventually coalescing these two comparatively fragile and complex sidechains of 1MB and 20MB back together into one single chain with simple rules?
Could you come to an agreement with - that can be put into a specific algorithm - of maybe having a *temporary period* for this softforked chain pair that eventually gets replaced with a full single hardforked 20MB chain?
@_date: 2015-06-09 17:47:59


...yet. Coalescing old UTXOs might in principle constrain the UTXO set long term to basically O(1) (albeit for high values of '1'). Yes, new development, yes, far future, but in principle I do not see any obstacles. And I have seen discussions along these lines in several places.
@_date: 2015-06-28 22:24:26
Crippling to 1MB forever is a hard fork of meta protocol. Meta protocol decides what software users and miners run.
Contention simply is, you cannot legislate it away.
@_date: 2015-06-03 09:32:41
Quote from the [original whitepaper](




Those that got addicted to keeping the blockchain available in full at all times are obviously NOT doing this. This is part of the ORIGINAL paper. So there you go.
@_date: 2015-06-11 20:33:04
Calling it GavinCoin is social engineering. 
Satoshi himself saw Bitcoin as eventually supporting *many, many* transactions and mostly having *full nodes in data centers*. That is the meta protocol rule that I follow and I'll certainly go XT. I also expect the resistance of the 1MB-blockistas to crumble soon.
@_date: 2015-06-28 11:57:58


Made to clear to me by the overlord IQ900 gods that rule Bitcoin, or what are you implying?
Please. It has actually been fucking well explained to me, by Satoshi:


would be about 400 bytes (ECC is nicely compact). Each transaction has to be
broadcast twice, so lets say 1KB per transaction. Visa processed 37 billion
transactions in FY2008, or an average of 100 million transactions per day.
That many transactions would take 100GB of bandwidth, or the size of 12 DVD or
2 HD quality movies, or about $18 worth of bandwidth at current prices.


sending 2 HD movies over the Internet would probably not seem like a big deal. 


Bullshit. Yes they are. Big blocks are very workable. Again, no hard data showing it to be impossible. As someone infamous once said: Repeat a lie often enough and it becomes the truth. That is what you appear to be trying.
@_date: 2015-06-17 00:45:29
But the dust needs to be merged suddenly with *multiple* other inputs. Why should that happen, and which wallets are doing this?
@_date: 2015-06-28 14:13:15
This is about the original point.
@_date: 2015-06-05 18:35:55
Only if you keep the current model of an forever-increasing, always available blockchain, though.
@_date: 2015-06-27 11:29:25
The earliest dev is Satoshi [who clearly showed and intended that Bitcoin can and should scale, to Gigatransactions/day]( And dare I say, except for some *opinions*, no hard data yet arrived that shows that Satoshi's vision of a scaling Bitcoin is impossible.
And the way how Bitcoin works is still to &gt;90% Satoshi's way, and not of some of the core devs. They are at most stewards implementing the *Bitcoin the code* for *Bitcoin the money system*.
I thank them for that, however I do not thank some of them at all for trying to socially re-engineer Bitcoin into 1MB cripplecoin, something which it was never intended to be.
@_date: 2015-06-30 08:27:16
You should think about the fact that nobody is trying.
Would you be trying to double spend if you go to your favorite coffee shop down the street and buy a coffee with Bitcoin?
Of course not. Because people know you there. They expect you to be honest, you have *already* a trust relationship with your barista. They'll accept zero conf.
It is interesting how some people in the Bitcoin space always argue from the POV of just a network of psychopathic scammers... 
@_date: 2015-06-17 14:08:52
It is not a compromise, it is rather compromising the core of your proposal: A market based solution.
Do you believe in your proposal, or not?
@_date: 2015-06-12 15:41:14
So it is 75%? 
I like it. Please talk to Gavin whether you two at least can come to some kind of agreement.
@_date: 2015-06-21 09:37:58
Yes, maybe a Bitcoin/GIS (Gavin-increase-schedule), with a corresponding version string (to voice support) and increase schedule patch would be in order.
@_date: 2015-06-24 18:19:50
E.g. here: 
See that he initially proposed *unlimited* blocks - he came down a lot from this!
@_date: 2015-06-30 11:36:54
Ah, interesting :)
Maybe you should point to that next time Greg or Adam argue from CS authority on the blocksize debate. I think that could be quite effective in maybe at least stopping the worst of the bullshitting.
@_date: 2015-06-29 09:09:03
Which part of:


do you fail to understand?
And the burden of proof is on the people who want to change Bitcoin's whole model from an open-end blocksize cap to an ill-defined 'decentralization', and it should be noted with a *centrally* enforced hard cap. Because a scaling Bitcoin (as clearly shown to be possible by Satoshi) is the status quo.
@_date: 2015-06-28 21:00:42


I can see that a 1MB-crippled Bitcoin could allow a company to create something on top, sell it as Bitcoin, take over the user base and then profit from transaction fees.
This might be an incentive.
@_date: 2015-06-11 09:43:15
SMTP isn't centralized - it is a hub and spokes model, but it is not like there is a *single* entity that controls all SMTP servers or that there is a *root* SMTP server.
And hub and spokes is fine - we have that already with full nodes and SPV nodes.
This is the ridiculous part of the discussion: People use completely weird and inappropriate ideas about 'centralization' and apply it to Bitcoin.
Saying SMTP is centralized because of hub-and-spokes is the same as saying the road network is centralized because there are highways and footpaths.
@_date: 2015-06-30 22:57:17
As far as I understood it, they would be ready for 8MB in early 2016 already, though?
@_date: 2015-06-30 09:09:56
Except for his neverending stalling tactics.
But yeah, rational and balanced exists on a scale, I agree.
@_date: 2015-06-25 08:35:17
Especially since the code actives only when a majority of &gt;75% of hashpower agrees.
Calling a successfully XT an Altcoin in that case is ridiculous.
@_date: 2015-06-25 08:49:10


That exists on a scale. I would say it is generally more risky.
As some other poster said in this thread: If we would have 10 implementations running on 10% of the implementations each, we could easily see who's off. Hard forks would happen (relatively) often (I guess thogh mostly in the beginnings when the kinks of those ten implementations are worked out), but they wouldn't be disastrous.
Of course, this is a complex dynamic equilibrium - btcd is not as trusted as bitcoind yet, and if it would fork, people would have a strong incentive to switch to bitcoind instead of calling that one the faulty implementation.
All I am trying to say is that I think that many different implementations reaching the same conclusions about the validity of a block / the chain from a common spec gives more confidence that the spec is implemented as intended.
@_date: 2015-06-03 09:34:21
Look up the original paper - he very clearly intended to do pruning. so, no, size of blk files != size of blockchain.
@_date: 2015-06-12 09:55:24


First of all, it very much matters because he specified the protocol and meta protocol rules. And as far as 'consensus' goes, had a [convincing post.](
Face it, there is *no consensus on the meta level right now*. Greg vetoing to keep the status quo doesn't suddenly make a consensus.


A payment system that goes through their hubs, with some fees going to blockstream attached, and that 'settles' on the blockchain might eventually be preferable to Bitcoin itself.


He specifically said he was going to step back to do planning on capacity and scaling. Which part of him saying did you miss?


Risking the whole ecosystem by blocking and stalling any discussion on block size increases (is it not just that Gavin's blocksize proposal specifics are disliked - there is *no frigging constructive input* whatsoever), to keep Bitcoin a toy 3tps system is even more reckless. Saturating blocks is changing Bitcoin's mode of operation and is far from a 'consensus status quo'.
Doing the less reckless thing in a non-consensus situation is actually responsible.
@_date: 2015-06-24 16:12:28


Fair point, how about making the coalescing period a decade? That'd mean we still have some time before the first coalescing happens, and eventually people need to get a clue anyways - should Bitcoin become a dominant payment system.
After all, office software proficiency is expected for most jobs these days. And it was once 'special knowledge'.
There would be a need for software, of course, that takes some Bitcoin addresses and produces a block of data to be written to safe storage, with GUI.




True. It is a trade-off. It trades some additional network bandwidth for (potentially lots of) freed storage space. The trick would be to set the age limit on UTXOs so that expanding a coalesced hash 'almost never' happens (negligible fraction of transactions, lets say 10^-3 or so). That would save storage of all the old coins (or dust), but doesn't really change the overall bandwidth required of Bitcoin. I understand that there is additional complexity, but I could see the trade-off as worthwhile.
 


That's why I think the process should be staged so that essentially the whole network would need to be dishonest for a time long enough to only have a negligible impact on trust. Like 10 years.
People argue that a node which starts from a coalesced UTXO set is never as good as a full node having validated everything since the genesis block. Technically true, but I think the discussion becomes very academic at some point.
A.k.a.: 'The whole internet could have lied to me, and there is this big conspiracy going on, Bitcoin was always meant to be scrypt-hash based, and Litecoin is the true real Bitcoin.' 
EDIT: I'd also argue for making UTXO commitments part of the enforced protocol.
@_date: 2015-06-16 22:54:34
The miners supposedly have 51% of hash power. If they are not able to deal with bigger blocks, they effectively *constrain* block size. Because they orphan chains with blocks that they cannot handle.
The major economic argument of the 1MB-blockistas is that blocksize needs to be constrained or miners would let it grow unbounded.
Which has been hereby debunked *another time*.
The Chinese miners not able to handle bigger blocks will mean blocksize with XT or bitcoin-core without the saboteurs would come to a *natural equilibrium*, not an *artificial saturation*.
@_date: 2015-06-12 13:02:43
Which is a ridiculous look at the system. Because what counts is the O(...) per individual node!
@_date: 2015-06-12 14:39:07
We can only hope that holding some BTC creates a large-enough incentive to stay rational for most users...
@_date: 2015-06-30 12:36:16
The point I was trying to make is that it is a lot harder to invent paper Bitcoins and use those to affect the price of real Bitcoins.
@_date: 2015-06-10 10:34:52
You mean deterministic in the sense of deterministic key creation like in a deterministic wallet? 
@_date: 2015-06-15 22:02:03
Exactly. We can create as many forks of Bitcoin as we want. Economic pressure makes us agree (mostly). The blocksize debate seems to be a very deep schism. I tend to think now this is due to interest (intellectual and maybe monetary) of putting layers on top of Bitcoin.
I am not against layers on top - but I want to scale layer 0 (Bitcoin itself) up as much as possible. And 20MB is a reasonable compromise in any case, not the sky falling.
@_date: 2015-06-28 23:58:34
Consider that it is a change only every other year. There is *plenty* of time to act should things go awry - and soft fork stop the increase. 
And with LN on top it would allow Bitcoin to nicely scale.
@_date: 2015-06-12 15:15:49
NACK why? Because it completely removes a hard limit? I am still trying to understand how this works in detail.
In any case, it is interesting that is even less conservative than on that front...
@_date: 2015-06-09 15:49:40


I'd rather ask why there was no movement from the other side while Gavin was very generous to downsize his increase proposal. For a while I believed this is mainly due to valid concerns from the other devs, but the lack of *counter proposals* (with numbers, like Mike said in the other linked post) and *constructive input* to Gavin makes it reek more and more like sabotage.
I heard Gavin is already down to 8MB.
@_date: 2015-06-02 13:30:18
Since *years*, to be exact. 
Without ever getting constructive input from the naysayers!
He's one of the few people who still have the clear, original and *totally doable* vision in mind and are not stuck on 'do not change my precious 1MB bitcoin ever!1!!'
I think it is time for him to do the fork now.
@_date: 2015-06-17 12:57:30
But 20GB might. In maybe 20 and some years.
And you know what? If Lightning Networks or similar system become so usable and nice, I might actually use them instead to pay for my coffee. I am not opposed to putting layers on top of Bitcoin, I am opposed to crippling Bitcoin to *force* layers on top.
1MB is a centrally planned limit.
As a quick reminder, Satoshi's vision:




@_date: 2015-06-12 15:40:36
If you think you can steer Bitcoin by centrally consensus/committee-planning the necessary 1MB-lifting hardfork at any short notice, you must surely also think that you can appropriately steer Bitcoin if things go awry?
@_date: 2015-06-28 21:01:41
[There is indeed some arrogance...](
@_date: 2015-06-30 22:47:26
True. But this is ignoring the cost of possibly making 0confs impossible. They work well enough for many cases now.
It might work out with RBF, too. I think being able to specify the output that can be bumped would be even better.
And any contention about these changes could be removed by introducing a flag on which transactions can be RBFed.
What about that?
@_date: 2015-06-09 10:09:24


They did, but to please the other devs - they tried to came down with their proposal, to have a smaller, hopefully more agreeable increase.
But all they got is vetoing, arguments which amount to concern trolling, and no *constructive* input at all.
You can see this if you google for 'Gavin blocksize increase' (and similar) and look for different times that he brought this issue up. It is nothing new at all. Gavin has been very reasonable and patient arguing for this since *years*, but it now looks like Greg and some of the others became outright obstructionist.
 
@_date: 2015-06-30 09:13:31
My purpose isn't to always disagree with you here... :D
I was just trying to extend the point of view a bit.
@_date: 2015-06-11 20:21:24
Well - go one year back to an UTXO commitment that is buried under enough hash power and start from there. 1 year of blockchain data is asymptotically going to be a zero fraction of the whole blockchain.
Now you can say you still do not have 'true full node security'. Because you didn't start at exactly zero. But if you go into that direction, you can as well as whether there might be a hidden goverment chain of Bitcoin with lots of hashpower behind it or people lied to you all the time and the real Bitcoin is actually using scrypt and not SHA256 and so forth.
So I think this model will work.
@_date: 2015-06-08 18:07:30
Quoting on this:




Also, by the way, your user account seems to have been created recently for the specific purpose of just arguing against blocksize increases?
@_date: 2015-06-24 17:55:25
OTOH, maybe you want core devs who are able to see that the vision of Bitcoin (including that it scales, as Satoshi himself very early on described it and showed it to be possible) drives the implementation, the code. *And not the other way around*.
That can be more dangerous than the other core devs withdrawing back to some academic ivory tower. There, they can make valuable contributions to cryptocurrency that might even be picked up by Bitcoin, but they won't destroy Bitcoin itself by letting their intellectual and code-centered focus destroy the vision.
@_date: 2015-06-24 18:24:44
Even if they had unlimited block size at their disposal, they'll think twice before they are going to cripple the Bitcoin network by pushing insanely large blocks.
It is not like Bitcoin is a toy and some miner might just *experiment*. There's money on the line and you can and should expect miners to behave in their self interest. Which nicely aligns with the interest of other people involved in Bitcoin.
This trust in 51% of miners is actually the foundation of Bitcoin.
@_date: 2015-06-09 18:55:54
Yes. Also, I think the whole set of counterarguments to an increase are very diffuse. That doesn't make them invalid *per se*, but it does put the burden of 'work this out to more detail' on Greg Maxwell and the other small-blockistas. It especially means that they shouldn't be as stuck as they are if it is such a vague concept.
For example, 'decentralization' needs a much more rigorous definition to really make sense in this discussion. Same with 'working fee market'. I could go on and on.
@_date: 2015-06-14 13:45:20
The argument could be made that a sidechain from (some of) the official devs has more weight and is more likely to succeed.
I am not saying this is what is going to happen or intended, though.
@_date: 2015-06-21 19:55:51


You can thank the influence and lobbying from the cryptopoliticians for what we are going to have now (in the optimistic scenario)... /s
@_date: 2015-06-11 20:56:27
With the way the 20MB opponents behave, it indeed does appear possible.
I think thought along those lines initially: 
Maybe this is subconscious in the way of 'Bitcoin can keep at 1MB because I am inventing this great thing to make it scale and then I'll actually be paid for it'. Integrating 'good for Bitcoin' with 'good for my company' for maximum funny cute total zen valley happiness. 
And potential minor issues that are actually not so good for Bitcoin might easily disappear in such a mindset, or major issues with a 1MB limited Bitcoin might become minor ones.
There is a potential conflict of interest and I don't think it is wrong to point it out.
@_date: 2015-06-21 09:17:47
Gavin left day-to-day development for big picture questions.
I rather have the big picture guy 'leading' us.
@_date: 2015-06-28 10:46:58
There is arguably a hub and spokes structure with the full nodes being the hub and the connections to the SPVs being the spokes. The full node network is still 100% p2p. Also, the SPVs can connect to any full node, so even there the hub-n-spokes is hardly centralized.
But yes, I agree, Satoshi intended this, it is the way forward, this is Bitcoin.
Also, no one has proven Satoshi's vision of Bitcoin's scalability wrong yet.
@_date: 2015-06-11 10:58:13
Interestingly, the miners putting artificial soft limits on the blocksize means that the 1MB-limiters have even less of an argument.
They always suppose that miners will all set their blocksize to infinity because of some runaway tragedy-of-the-commons.
@_date: 2015-06-24 08:13:54
You might be interested in my submission [here](
@_date: 2015-06-01 23:56:37
This might interest you, some good foresight:
@_date: 2015-06-12 11:10:58


Decentralization, decentralization, decentralization. This word is thrown around all the time but then left in the unclear: What exactly is your worry?
@_date: 2015-06-09 17:26:20


It is actually a good question whether it isn't the crowd that wants to let Bitcoin run into the 1MB limitation has to provide evidence that it is ok to do so. Because running without (any real) limitation is so far how Bitcoin operates successfully.
Just framing the debate such that people who want to increase the blocksize (like Satoshi originally intended for Bitcoin to grow anyways!) have to prove something might be a case of social engineering here.
In the video of the other post today on reddit, the interview with Mike Hearn, this view was also articulated.
@_date: 2015-06-30 19:10:55
It is mostly them, though. And their common opinion could well be group think, special interest or a mix of those. And it is isn't just that they opine against bigger blocks, they stall and block(stream), giving a lot of support for the idea that some conflict of interest is driving this.
And the number of core devs opposing or proposing a change shouldn't be the only metric to personally decide whether it is a good idea or not...
@_date: 2015-06-11 13:05:13
Um, yes, thank you :D
@_date: 2015-06-01 23:05:12
Very good argument. I think should also say this more: Many of the naysayers give detailed arguments in a very narrow subfield of Bitcoin - but the whole question that the blocksize issue needs to be solved in is *much* larger.
@_date: 2015-06-12 13:30:18
The divergence risk is there all the time. It is just now that enough people actually *want* a divergence.
If I go and create my own little 100kB fork, I am diverging from 'the consensus'. That risk is always there. Yet no one is going to take my 100kB coins.
'The consensus' is a result of mostly economic forces, not the other way around.
Hashpower will eventually sort out who won.
@_date: 2015-06-30 12:13:33
Excellent comment. (And this holds in the blocksize debate, too...)
For some reason, many people in Bitcoin see their miners as the *enemy*. It appears to be a way to project the insecurity[1] about reasonable (as in *long term rational*) behavior of the miner majority into an outright hostility towards them. This usually comes flavored as some 'game theory argument' with a lot of tunnel vision and ignoring a lot of variables for the sake of a singular argument.
[1] - And I do feel that insecurity from time to time, too. But one should be able to actually separate between knowing the inherent risks in Bitcoin and trying to project them away in some weird way.
As Gavin says: Bitcoin is still an experiment. Ironically, trying to get this core risk out of Bitcoin seems to push the whole ecosystem closer towards failure more than anything else. 
@_date: 2015-06-06 08:34:24


You must have missed that Gavin is on the blocksize issue since years, and has been trying to get the discussion going since years.
From the naysayers came some (partly) valid concerns, but no constructive input.
So it is really hard to argue that he's doing a rash move.
@_date: 2015-06-24 17:37:09
... oh and I hope they do lose some of their creds with regards to Bitcoin if they continue to derail the blocksize discussion. [What Adam did, for example, with his O(n ^ 2) scare, is IMO just ridiculous. And shouldn't come from a CS PhD.](
And it is also [interesting how Greg argues, see also my reply...](
@_date: 2015-06-27 16:49:45


My point here was rather in regards to the 133MB.


I don't think so:
- Paypal is still *a lot* more complexity than even a 8GB full node would be.
- Paypal is a single company and not a loose group of people/companies working on top of a common protocol. Internationally!
@_date: 2015-06-16 20:57:33
Yes, because it limits usefulness of the blockchain to 3txn/s. Which is not enough for *any* payment system.
@_date: 2015-06-16 13:34:19
Which increases fee pressure by the way, because it means a certain fraction of hash power is idling by choice because fees are deemed too low.
Only when fees increase enough so it is worthwhile to include the block will that policy change.
But no, we don't have a working fee market and everything will go crazy when we go to 20MB blocks. /s
@_date: 2018-09-22 09:45:27
I have hopefully addressed this comprehensively here: 
@_date: 2015-06-27 21:49:11
And ops gonna op. 
And obs gonna ob(struct).
@_date: 2015-06-28 14:25:52
     What exactly is a wall here? If you believe users ~ full nodes, and thus O(n ^ 2) in the whole network, how is that a wall?


I have a problem calling this a wall. Running into this limit would be running into physical/technological limits, and one should not confuse an artificial 1MB limit with the physical limits here. It should also be noted that it is far from clear that we'll ever run into those, either. As I said in my other, older post, economics will kick in way before then. In contrast to an artificial 1MB hard cap, the economical supply and demand laws will be much more benign and continuous instead of abrupt.
And who says one has to choose between block size increase and algorithmic improvements? I am with you, it makes sense to make Bitcoin more efficient, and it probably also makes sense to also put some layers on top of Bitcoin. 
But the argument in the blocksize debate is for planning scalability of level 0, very soon. Both Gavin's and Jeffs's proposal would do that for me, and personally actually removing the cap would do it, too. That's where we are back in the political realm - those proposals are already compromising a lot with 'your' side. 
Consider that both Gavin's and Jeff's proposal (w/o a self-contradicting 32MB hard cap) are somewhat older in spirit - I remember Jeff's ideas coming up a lot earlier already. Gavin has simplicity in his favor and more testing of his solution, Jeff mainly that it is fully open-ended and more market based, the disadvantage being the additional complexity.
I should add here that I believe even no cap would be fine  - because I fail to see the miners risking their investment in Bitcoin's as well as their special-purpose-only-useful-for-bitcoin hashing hardware just by bloating blocks to insane sizes. In a way, taking the cap off is Jeff Garzik's solution, but without the 75% rule.


Ok, I see. Well, the question is whether you create them with a 1MB limit or just let the need of your merchant create that market for full nodes. I fail to see why you need to centrally plan any limit for that.
@_date: 2015-06-11 14:25:35
It is indeed weird that RT-Russia doesn't promote Bitcoin, though.
You'd assume that if someone sees Bitcoin as a viable contender to other currencies (like USD/EUR), that RT would spread Bitcoin as much as possible in Russia, to get it into the hands of the local people. And not the other way around. Maybe the though behind this is that Bitcoin can be helpful in attacking western currencies but will and can eventually be shielded against with Internet censorship?
@_date: 2015-06-03 13:02:52
Sure, with 3txn/s and stubborn devs it is totally going to replace cash. /s
@_date: 2015-06-17 13:45:49
Thank you! :)
@_date: 2015-06-14 10:04:22
 Jeff's proposal might amount to essentially slow, stepped removal of all caps (because that's what Miner's are going to vote for in their own economic interest).
In that case, the cup of coffee on the chain is possible - and I like to see that, too. The original idea of Bitcoin.
The question remains whether 20% of miners (due to taking the minimum of the top 80% of votes rule) can hold Bitcoin hostage - not really if the other 51% start ignoring their blocks. That must be a quite contentious situation, though.
@_date: 2015-06-02 13:15:24
Here you go:
@_date: 2015-06-28 09:27:48
This is a lot of fear-mongering. This can as well be turned around, too: People might not like someone destroying Bitcoin with 1MB limits either... blocking consensus is an action in itself!
You seem to feel responsible for Bitcoin the ecosystem, yet you are at most partly responsible for a certain variant of the Bitcoin network client. No one gave you responsibility over the ecosystem, you are trying to take the burden yourself. 
The problem with taking this burden seems to be twofold: It creates pressure on you that shouldn't exist, and it will make people see your actions coming from an non existing responsibility as a power grab or a power game.
If you are honestly worried about all this, how about neutrally representing different forks and factions on bitcoin.org and github.com/bitcoin?
That way, you could shift all your responsibility burden back to the user: When they decide to select clearly labeling, forking incompatible clients for Bitcoin, it is *their fault*.
@_date: 2015-06-17 21:23:39
And it should be clearly seen as a technological, not an ideological 'meeant to be' limit.
[That's also the complaint I have about the otherwise very reasonable BIP100 - Jeff Garzik is contradicting himself by putting in a 32MB limit explicitely. Because he is arguing all over the paper for market based solutions. Yet he put in a centrally planned 32MB limit again.]
@_date: 2015-06-29 08:23:30


If I do not see numbers from Adam soon, and a detailed critique of Gavin's proposal, I'd have to go back to the state of assuming these are power plays and politics. Lets hope he's earnest this time.
@_date: 2015-06-28 20:58:33
@_date: 2015-06-15 22:45:28
He tried hard to keep consensus. Be he has all the data now, visible to everyone, that some people are simply sabotaging the discussion.
@_date: 2015-06-12 14:36:40
There it is again: 'Decentralization'. The FUD word.
@_date: 2015-06-28 17:09:10
And funny is this expectation that it will 'create incentives'.  Apart from the inherent central planning of 'making those incentives', they don't really look at the supply side either: Why should there suddenly be, with a failing and 1MB-crippled Bitcoin, any code magically appearing that will make Bitcoin scale in other layers?
'It will cause incentives' appears to be more and more a convoluted way saying of 'Please use Blockstream technology to do your transactions.'.
Incentives indeed.
@_date: 2015-06-18 19:31:16
Exactly. Gavin's proposal does that, it ensures the possibility for growth.
The problem is that some people, like Greg and Peter, go from the narrow, technical view of the code base and infer major goals for Bitcoin from that.
But it should be the other way around: The major goals for Bitcoins are implemented using technical means, and if the technical details of the protocol collide with the major vision, the technical details eventually get changed.
Of course, technical *limits* need to be considered, but Gavin has clearly shown that those limits are currently not limiting growth of Bitcoin.
@_date: 2015-06-27 17:55:35
Letting them use the brand 'Bitcoin core' is a mistake even, and not warranted. That brand doesn't belong to them.
@_date: 2015-06-17 00:13:42
Awesome! Thank you, Gavin.
@_date: 2015-06-30 19:25:29
Ok I see what you are saying. Agreed. And I think he will.
@_date: 2015-06-22 21:13:20
Then those technological limits should be reached with a successful Bitcoin, no?
Maybe Gavin's growth proposal allows the technological limit to be reached, maybe not.
But 1MB *surely* won't reach the limits of technology. 
@_date: 2015-06-14 10:42:19
As you say, a 75% limit still changes the dynamics, though: 
If you are a minority miner and you want to have larger blocks but see a minority of nodes blocking, you have to weight potentially orphaning your blocks against censoring parties who are voting a small limit.
Because other miners are potentially not going to join you in silencing the block-limiting minority.
Only when a 51% single miner or cartel emerges (the cartel might well be a 'benevolent cartel' in this case) does the situation change.
And thinking about this further, this might actually create incentives to form cartels between miners - because a blocking minority might create a 'common cause' on the other side. I wonder what thinks of this aspect?
@_date: 2015-06-28 23:41:37
As a CS guy and the inventor of hash cash, I bet he also understands the general concepts of scaling very well. I think he doesn't like it, though. 
But maybe there is reason for optimism as I sense a bit of a shift in position from Adam. That would be great.
@_date: 2015-06-01 23:49:51
Nope. The issue *itself* is divisive. 
Gavin was friggin' trying, *politely and openly*,  to get input and get this discussion going for ***years***.
Also, look at [this](
) for context.
Eerie, how in 2010 someone already said that the a friggin' blocksize limit will bite us badly, isn't it?!
@_date: 2015-06-19 13:09:39


The funny thing to also note is that GB sized blocks for Bitcoins would be a *wide* success. I am *absolutely* sure Miner's will not pump it up to GB-sized blocks right now, just for the heck of it. They have an economic interest in this succeeding, too.
Blocksize limiting seems to be due to being fearful of Bitcoin's success.
Big pools in China apparently even don't want big blocks now (at least anything bigger than 8MB), with supposedly &gt;51% of hash power... and if they don't want them, they're not going to happen, regardless of the actual hard limit.
@_date: 2015-06-29 08:05:36
No gun. The way consensus works in the meta protocol.
EDIT: And also, if you think there's a gun to your head, you must feel threatened by something.
The only thing you could feel threatened about is that the hardfork might actually be successful.
If it is successful, it is what users, miners and the rest of the ecosystem want.
So are you saying you are indeed trying to block progress in Bitcoin?
@_date: 2015-06-24 16:00:41
... and I haven't come across any real new data which prevents Satoshi's vision from becoming true. Except for the special economic interest in a limited blocksize...
@_date: 2015-06-25 17:33:07
That is true - but consider the long term perspective: Bitcoin is widespread and every so often, UTXO set hashes are distributed and hashes are in the header by default. Full nodes are *mostly* replaced with 'SPV+' (or however you want to call it) nodes that start from an UTXO from ten years back. Long term, I see this lessening the burden of storage. I believe what Gavin has called the 'UTXO uh-oh' might be *the* very long term data size issue eventually hitting Bitcoin.
The amount of Bitcoins in the set is hashed, too at every level, and can be trivially checked against the inflation schedule, to see that no new coins appeared.
Everyone can check whether s/he sees all her coins in the set. If not, she can pay an archival/full node for proof of shenanigans - or for the branches she might have forgotten to save that lead to her coins.
@_date: 2015-06-11 17:41:14
Thanks! 
In a way, I guess, the effect of the default soft limit depends on the 'miner laziness variable'.
@_date: 2015-06-24 17:33:50
@_date: 2015-06-12 14:19:37


LOL. Yes, very fucking true. It all appears as stalling tactics. And is damn patient here. 
The other one is: Gavin went public with this on his own!1!
As if there is some kind of NDA that he signed that he can't express and discuss his thoughts on a blog. And he's interacting here on reddit.
This is getting pretty ridiculous.
@_date: 2015-06-25 08:12:18
Exactly. The whole O(n ^ 2) scare was about that. See also my other post.
@_date: 2015-06-07 18:35:27
Yes, but 20MB gives us at least a couple years of solid growth - and by then, technology probably improved enough to handle even bigger blocks - the reason for the original growth formula; which could be softforked down if it doesn't fit the growth in bandwidth.
I agree that other things in the Bitcoin system should happen, too. And Bitcoin itself will probably get more efficient in terms of data transmission and storage.
@_date: 2015-06-15 22:25:45
Files should work now. 
@_date: 2015-06-16 10:34:53
It is my impression that we won't be entering any bull market anymore when the system is artificially constrained to 3txn/s with a centrally planned limit. Because people who might be interested in Bitcoin are turned off by this.
@_date: 2015-06-30 09:59:01
... there is also the issue of hiding a personal opinion behind 'I understand the technology, believe me'.
@_date: 2015-06-16 21:05:24
Yes, and yes. Not evading anything.
@_date: 2015-06-11 21:18:53
And consensus for an emergency hardfork will just magically appear. While there is *this* heated debate for a hardfork planned to happen well in advance. /s
You must be kidding.
@_date: 2015-06-24 18:40:13
LOL. I vote for blocks being *safe spaces* for transactions. And being *inclusive* :D
@_date: 2015-06-30 08:29:45
Not only the odds - you forget that Bitcoin isn't necessarily the only link to the customer. 
No one (well almost) is going to scam their favorite coffee shop down the street. Because a) most people are actually honest and b) those that are not are under a lot of social and also legal pressure to behave - regardless of whether you *could* double spend Bitcoins there or not.
You can also shoplift a candy bar very easily. The majority of people still doesn't do that.
People are making ridiculous assumptions in here.
Zero confirmation is good for when there's good faith between the parties transacting. For quite a bit more security, just wait an hour or two. Luckily, those latter transactions are also usually not the time-critical ones.
@_date: 2015-06-30 22:49:15
Does block-the-stream want wide adoption of Bitcoin or of BScoin?
@_date: 2015-06-12 15:50:10
Who else? Do you rather want a central planning committee for that?
@_date: 2015-06-17 12:33:17
Jeff Garzik wants a market-based solution to block size (which I can fully agree with). 
Putting in that 32MB hard cap is contradicting his own stated proposal goals.
His original proposal without the 32MB cap had the chance of killing the blocksize debate once and for all. With the 32MB hard cap, we'll just get to the same insane contention as we are in now at that future date when 32MB will not be enough.
@_date: 2015-06-27 11:15:34


I'd rather say the minority is way too loud here...


Yet you are referring to a specific set of people again, below. Who decides who is core dev?


@_date: 2015-06-09 09:30:49
Is blocking consensus at all costs itself consensus?
I don't think so...
@_date: 2015-06-11 13:38:51
My reading comprehension is fine, thank you. 
As I said, *Evidence invalidates theory*. The block size are not full now. The only way to integrate that with the 'results' of that paper is if there is a lack of goods that could be bought with Bitcoin that are sufficiently cheap. Yet, at the same time, Greg Maxwell talk about having *advertisements* on the Blockchain. 
So there is no reason for blocks not to be full according to their simplistic calculation. Ergo, it is wrong. End of story.
Also of note: 
Which kind of invalidates their model anyways from another angle.
@_date: 2015-06-27 21:47:04
Theoretical scenario: I go and compile myself a bitcoind with 100kB of blocksize now. It will soon fork. This is a controversial unilateral hard fork.
Explain why this is different from what Gavin is doing, and *maybe* you get a clue to what forces that you consistently seem to ignore are in effect here.
@_date: 2015-06-24 17:29:45
Exactly. It is interesting that there is this whole distrust of the miners by the small blockists. They somehow forgot that they are at the whim of the miner majority anyways. And incentives for miners are so that attacking the money that they create is very counterproductive! That's just how Bitcoin is designed. There seems to be certain undercurrent of opinion in the small-blockista camp who wants to decree/prescribe what Bitcoin is supposed to be from a 'professional, expert consensus committee'. No thank you.
Miners large scale attacking Bitcoin would be akin to gold miners *convincing* everyone that gold is completely worthless. 
@_date: 2015-06-27 17:40:42






a) Anecdotes don't make data. b) Why is outsourcing necessarily a bad thing? Paging to infuse some market based sanity here. It is a risk trade-off for that particular company and its customers.


Billions of people are not going to run 1MB Bitcoin. I doubt that (many) millions of people ever run a full node. And I still fail to see the problem with that.
@_date: 2015-06-12 13:59:35


With the major difference that you intend to keep the confusion ongoing with this proposed sidechain....
@_date: 2015-06-01 20:37:39
So then how about 40%/year?
That's what Gavin is proposing, and I think I remember the argument proceeded very simiiar on bitcointalk.org to what just put up here on reddit.
So independent people arrive at the same scaling, maybe that's a hint that 40% isn't too far off?
Yet the naysayers want to cripple Bitcoin until 'a fee market starts to arrive'. As Gavin pointed out, we have that already.
@_date: 2015-06-18 11:20:34
No. If you read BIP100, you'll see that the whole foundation of Jeff's argument is a market based solution.
By elevating 32MB to anything else than a technical limit, he's reintroducing a hard cap. That is unacceptable.
@_date: 2015-06-13 10:11:59
So do I get this right that you sort by having someone mention 20MB or 1MB in the comments of the users?
Isn't that very simplistic - I mean, there could be somone arguing against 20MB all the time, just mentioning 20MB?
@_date: 2015-06-19 14:19:04
But with Mike's idea of assurance contracts to pay for hash power eventually, the POW could settle at the equilibrium where it is deemed sufficient to thwart attacks.
@_date: 2015-06-09 10:13:07
Noone can force you to do anything. You can go and fork Bitcoin right now and run a bunch of nodes with a 876kB block size limit - because you might like it.
@_date: 2015-06-11 09:41:23
It should be noted that the protocol of Bitcoin will not allow invalid transactions (which might be compared to spam in the SMTP scenario), so it is not necessary to restrict Bitcoin node connections to certain, static IPs.
If you are a node and push out valid transactions and blocks, it is in my best interest and the best interest of the rest of the network to connect to you.
The SMTP analogy only goes so far.
@_date: 2015-06-19 22:45:02
I have not seen any convincing argument yet that Satoshi was wrong on the blocksize issue, and I also have honestly not seen any fundamental new data on it, since Satoshi invented Bitcoin.
He had a long, hard look at it and figured that it would be ok to scale Bitcoin up. And that is what he intended to do. And I still think it is wise to implement that vision.
@_date: 2015-06-14 10:16:59


I think that's why the metric should be bytes in transactions in the end (what miners also mostly use for transaction cost), not the transactions themselves. Validating a transaction will also be *roughly* proportional to its size.
@_date: 2015-06-01 22:57:43


Having the full block chain available is a rather dangerous addiction. Satoshi talked about pruning in the very initial paper about Bitcoin - before even the code was written.
Bitcoin can be pruned now, and it needs to change eventually to have UTXO commitments or ZK-proofs of wellformed-ness (though these are right now moon math) or similar so that you can run a node it in eventually (almost) constant amount of space.
Or maybe not, because disk space gets so cheap it doesn't matter. 
But if it matters, please do not cripple Bitcoin the transaction network because you are all clinging to 'I have all transactions available'.
Like with 1MB, people got used to 'we all need all transactions all the time'. If people are going to become stuck on those issues, Bitcoin *will* die.
@_date: 2015-06-28 18:46:26
As you are opposing apparently even 1MB blocks, there is no consensus on Bitcoin, and so it is itself an altcoin?
There's never 100% consensus. It is ill-defined. Get over it.
@_date: 2015-06-21 09:07:37
Very. He has shown that scaling is not a big deal and no data so far has really changed this picture.
@_date: 2015-06-11 20:10:12
Yes, but you really need the vast majority for this, meaning people who have money will lose some of it.
The interesting part is that Bitcoin is (with an open Internet) in open competition to altcoins and to government issued currencies, and Gold and whatever.
As long as it stays that way, I expect Bitcoin to behave mostly 'sane', for varying definitions of 'sane'.
@_date: 2015-06-19 14:21:50
How about: It is all three factors?
@_date: 2015-06-03 19:37:14


I am slowly starting to wonder about that, too.
@_date: 2015-06-14 09:22:45
I am an individual that still think the initial vision of Bitcoin is possible and, yes, I am arguing strongly for that. Name-calling doesn't change that.
@_date: 2015-06-12 15:37:55
Yes, but some majority of hashing power can vote in a higher limit, correct?
How much of the hashing power is this going to be? 80%? 
@_date: 2015-06-30 14:41:55
We're waiting for Gavin to come out with his hard fork.
I think he will, given the never-ending stalling tactics from the Blockstream guys.
@_date: 2015-06-12 16:07:47
On IRC, Both Gavin and Mike seem to be receptive to his proposal. 
Lets hope for the best. Bigger camps that have agreement would be a sign of at least progress IMO.
@_date: 2015-12-05 21:49:05
Do you think customers won't care when their electricity bills go up?
I have repeatedly seen advertisements for 'green ethernet' in magazines of the networking industry. ARM and Intel both make a big deal out of the power consumption of their processors. 
Sure, you can try to sell unsuspecting consumers hardware with mining embedded, but it is going to be a competitive market, as others want to have a share of that, too.
TL;DR OP, there is no problem.
@_date: 2015-06-30 14:00:59
... and Gavin's proposal is a reasonable plan that if things go reasonably well, won't need to be revisited in the future. Not enough capacity to overwhelm the system, but probably enough taking into equation technological and adoption growth.  While LN has time to develop and be the fast quick layer on top.
And if there appears an emergency with too big blocks, do a soft fork back down. Easier than a hard fork.
@_date: 2015-06-16 21:59:36
We'll see. That would be interesting, but certainly to be avoided.
@_date: 2015-06-12 13:57:19
Exactly. And this is the number that is of interest to a *full node operator*. 
That famous entity that Greg and Adam all say that they are worried about. *The Burden on each single full node operator*.
Slowly, I am beginning to think this blockade of any change to the blocksize might actually be planned.
@_date: 2015-06-16 09:13:59
Exactly. I think this is the ridiculous part in proposal: 
He seems to be arguing very much for market based solutions (which I can agree with), but he's putting another major pain in by calling the 32MiB limit part of the protocol.
This just ensures that the current 1MB problem will appear as the 32MB problem again. He should make it clear that his proposal is indeed open-ended.
And if he doesn't believe that 32MB is ever going to be exceeded, he should tell us whether he trusts more in his very own market-based solution or central decree again (the fixed 32MB cap). Because if market decides that equilibrium is below 32MB, there is nothing to worry about.
@_date: 2015-06-19 14:31:54
Also relevant to comment, a very recent conservation I had with Adam [here](
@_date: 2015-06-12 14:12:19
No. Gavin intended the fork to only happen with the majority of the hash power. As in, that is a trigger condition for the fork.
@_date: 2015-06-16 21:03:38
And I answered your question in the economic sense: Yes. I disagree. You didn't ask for a technical answer. My focus is a bit wider than that, sorry.
Because it will not allow massively decentralised nodes in people's homes.
Because no one will eventually run such a thing. Because it will be worthless.
In the technical sense, sure. You can build a 1kB, 1MB, 10MB block network across the globe with most homes participating.
@_date: 2015-06-30 20:47:28
I submitted a link to your ML post [here]( (and further commented on it). Given reddits design favoring fast scrolling news, there might be more exposure to the discussion there. 
@_date: 2015-06-11 20:17:23
No one can be a true despot in Bitcoin. You don't like bitcoin-xt? Run core. Don't like core? Run your own. Write your own. No one is stopping you.
@_date: 2015-06-03 19:48:46
The problem, though, is that the supply is fixed with a fixed blocksize and transactions (real ones) are pretty inelastic demand. People want their money system to *work*.
I do see that there might be those spammy txn that will go away when a transaction costs 10ct, but maybe not at 5ct. But after that, the cost per transaction will rise steeply.
@_date: 2015-06-11 17:38:49


But why are blocks not full yet with 1MB? The same argument applies.
@_date: 2015-06-12 13:50:01
Then why do we already have a working fee market, as pointed out?
Transactions are *cheap*, but they are not *free*.
@_date: 2015-06-18 21:12:46
Yep... they supposedly have &gt;50% of the hash power, so they are going to decide on the blocksize of Bitcoin. And funnily, they are going to *limit* it, not grow it without bounds.
Maybe we could start some effort somewhere to make Jeff clarify that 32MB should be stated just as 'with intent to remove'?
I argued with him personally, he seemed somewhat stubborn on that. I have to admit though, that I directly pointed him to the blatant contradiction. That was probably not the most *effective* way to argue here :D
Maybe and hopefully this part of the proposal can still be changed. Maybe a submission to on this issue?
I fear if I do that, it will just create another schism, though... :D
@_date: 2015-06-30 09:35:22
So let's just do Gavin's 40%/y proposal that is below 50% and be done. If things go awry, we can easily soft fork back down.
@_date: 2015-06-24 17:32:08
Also compare it to this question: "Is it ok to centrally limit and plan blocksize?"
@_date: 2015-06-18 19:56:45
I understand all that. All I am arguing for is not elevating the 32MB limit to anything silly again. Like it apparently happened with the meant-to-be-temporary 1MB limit.
@_date: 2015-06-28 09:43:04
BS. Scalability has been tackled by Satoshi. Nothing fundamentally new came after that.
@_date: 2015-06-24 17:21:02
I have no intent to flatter here...
But if you look him up, google him, he has written *a ton of well informed opinion and analysis* on Bitcoin. In terms of the ***overall picture*** with regards to Bitcoin, he's probably even better informed than your average, dictatorial core dev.
Now, I think his arguments are valid by themselves, and this is the way it should be for the core devs, too. Arguments from authority are as meaningless as from a random guy down the street - if they are wrong. Look up, for example, how Adam from Blockstream, certainly a very bright fellow, [uses bogus and emotional and IMO manipulative O(n ^ 2) arguments]( They do not become correct just because he's the guy who invented the idea of proof-of-work.
@_date: 2015-06-22 19:01:18
Arguably, if Bitcoin's price is high, it is so because people value it that much.... :D
@_date: 2015-06-18 20:39:04




I am not that worn down yet. I understand the sentiment, though.










Yes.. I proposed a couple days ago taking Jeff's proposal to please the Blockstream guys but adding in Gavin's open-ended 8MB (or 20MB) + doubling every two years (or 40%) as the hard cap, again to please the Blockstream guys. That would also create a clear path forward in *intention* with regards to the 32MB worry. As in, Gavin's open-ended hard cap would be the way forward.


Yes. Agreed.




Fully agreed. I argued several times around here for building a merkle tree for UTXOs and putting the root hash into the blockheader, and enforce that eventually by protocol. This way, you could start a node from a while back, and when you further go and take very old UTXOs and just store a hash of several UTXOs hashed together, you could put the burden of storage of the blockchain itself back to the users to a large degree. A user would need to prove that he owns the relevant UTXO by supplying merkle branches leading down to his coins.
@_date: 2015-06-29 22:12:44
Which has to be believed, too. Yes, I know, gold is a 6000 year old bubble...
@_date: 2015-06-19 10:24:27
Good! Then let's all bury this O(n ^ 2) scare and see that Bitcoin can indeed scale the way Satoshi originally intended.
Everyone in Bitcoin would be happy if you'd get together with the others and make some plan. Such as BIP100, with clear intent to formulate the 32MB limit as being due to technical reasons and not an ought-to-be. Because otherwise, that limit would eventually create the same mess that we are in now.
We *all* profit from a settlement of the blocksize issue.
@_date: 2015-06-30 18:55:41
Agreed. I think it therefore helps to write up these kinds of opinions and ideas now. So when hopefully Bitcoin survives the necessary hard fork, it can be seen that this issue has been discussed before, to maybe inform future debate.
A lot of what comes up now has been thought about in the early days on bitcointalk.org.
Depending on the dynamics, I could expect Bitcoin's protocol settle so much that this kind of change is out of question for the next decade or so. But again, I think it would be helpful for people to look back then and say: Ok, people have thought about and discussed this before.
Because I am sure the older Bitcoin gets, the more suspicious any proposed changes to its more core behavior become. There will be special interest, and so people will assume special interest in any case.
If some guys from 10 years back had the same thoughts, it might be easier to argue for it.
Because coalescing UTXOs by automatic merkle tree hashing old ones and saving just the root hash of them would mean that people somewhat regularly have to check and save the merkle trees that lead down to their unspent outputs.
Nothing extremely controversial right now, but I can see this becoming contentious when such a change becomes necessary.
Other than this UTXO uh-oh, as described it, I really can't see too much that prevents Bitcoin from scaling and becoming an *easily accessible* worldwide layer 0 with eventually multiple-GB-sized blocks.
@_date: 2015-06-12 16:57:41
Jeff makes a good case that Miner intent is visible long before the changes happen to the users.
@_date: 2015-06-11 09:20:34
This is very interesting in the blocksize debated. Thanks for digging this out. Makes the 'nodes are all disappearing!!1!' argument pretty weak.
EDIT: It also puts the table that floated around yesterday by GMax into perspective.
@_date: 2015-06-29 00:31:23


As someone outside the developer community but being very interested in all things Bitcoin and following it quite closely, the picture is quite different to me.
He repeatedly brought the topic up to evoke discussion. But he repeatedly hit a wall with you guys. Him talking to companies  cannot be held against him, as he was and is open about that and I think it is also *good* to get a feeling for what the positions are in the blocksize debate.
Threatening an XT hard fork has been seemingly and hopefully effective in getting the ball rolling. It wasn't the first 'measure', it wasn't brought up randomly. And it is still on the table if it turns out the debate is still stalled.


I think we all understand lightning truly only when it exists. And I don't think he misunderstood lightning either, he's trying to keep the top level view of the whole ecosystem and lighting from that perspective is just one of the ways Bitcoin could be made to scale.
Which brings me to the last point - isn't Gavins 8M..8G ramp proposal very much in line with a succesful Bitcoin and LN handling most transactions on top, just like I described in the other post? It would allow to open an LN channel about twice a week. A reasonable number, IMO.
@_date: 2015-06-12 14:15:18
That's fine with me, too. 
I believe there is lots of social engineering going on here by equation 20MB with infinity.
The actual proposal of Gavin is 20MB. Nothing more, nothing less. Not unlimited block size. This should be the point of discussion.
Going into O(...) notation and then suggesting that that is the real issue at hand appears to me to be an underhanded way of creating distraction.
@_date: 2015-06-12 14:22:40
Oh, hello Mr. Pirate Party. What's your opinion idea on the blocksize debate?
@_date: 2015-06-08 17:54:48
^ this
Also, I think the network is fine with 20MB blocks, in data centers probably even 100MB right now.
I suspect something might need to be done to the blockchain-on-disk growth eventually (besides pruning) - validated UTXO sets and cutting the chain after 5 or 10 years or so (older UTXOs than that might be spent by supplying proof that they are part of a TBD UTXO summary hash).
This would also nicely put the burden of long-term storing the necessary data that goes along with one's Bitcoins from the network back to the users eventually (doing a simple update of some 100kByte of data for each unspent TX every couple years shouldn't IMO be too much to be asked for) .
@_date: 2015-06-27 13:51:37
A year ago, I would have brushed off fears that the Blockstream guys are up to no good.
But there are *interesting* lines of IMO usually quite effective psychological confusion tactics, [double binds](  coming from the 1MB limiters. For example:
A) User count is proportional to full node count -&gt; Bitcoin scales in O(n ^ 2) -&gt; cannot scale -&gt; doesn't work -&gt; need 1MB cap
and 
B) User count is less than proportional to full node count -&gt; full node proportion drops with more users -&gt; centralization -&gt; Bitcoin cannot scale -&gt; need 1MB cap
Of course, both are wrong *along* the chain of reasoning. A) is wrong because *per full node* behavior is still O(n) even if network would be O( n ^2 ).
B) is wrong because the word decentralization is defined as however it fits the limiters, excluding the decentralization by widespread Bitcoin usage, technological progress and by also along the way changing the definition of 'scaling' to suit their needs - it supposedly now only scales when it runs on a raspberry pi under everyones desk.
Yet, it appears that whatever route you take, solving Bitcoin scalability is supposedly impossible. Ridiculous, wouldn't it be so dangerous.
All that should have been said about the whole blocksize debate (the BS debate...) is this: 
Satoshi clearly thought hard about the scalability of Bitcoin, found that it *can indeed scale up* - and there is now new data yet that shows this initial vision to be impossible. Just at best opinions and otherwise FUD.
@_date: 2015-06-08 20:49:05
I think Gavin even repeated this '1 implementation is ok, 2 can fail horribly (two competing chains), 3 or more would be better (the faulty one would be voted against)' in some talk somewhere.
In a way, it would also make Bitcoin more responsive to user input, as the protocol would indeed become a protocol and not a protocol implicitly defined through a particular implementation.
But honestly, I think that this is still a couple of years off. First, this damn blocksize issue needs to go.
EDIT: And, also, this whole debate is ridiculous. It is scary how much of a problem the naysayers have already socially engineered into the debate. One really wonders who the interested parties are. I remember Bitcoin from 2010 and even a fully open no-cap huge blockchain was not seen as a principal problem back then.
Also, people were a lot less conservative with regards to changes how the protocol can operate: I remember reading the 'ultimate blockchain compression' thread, and I still think that ideas from that particular thread and similar ones need to be eventually implemented in Bitcoin.
Interestingly, ideas like validated UTXO trees (which could make nodes very small and lean in terms of storage space at least) are argued away as 'not giving full node security' (which is BS: The argument basically goes that some rogue 51% miners can supplant the validated UTXO set with whatever they intend - but then they can also supplant the whole blockchain), whereas talk is concentrated on 'moon math' concepts like ZK proofs of blockchain validity. I am curious about the latter, too (in principle - I think this is years into the future at least) but building a hashed UTXO tree is something that can be done with the cryptographic primitives that are available right now, are being used in Bitcoin and can be understood by everyone. Yet it appears like discussion is effectively derailed by replacing down-to-earth goals with pie-in-the-sky discussions or idiotically clinging to the notion of 'my full node needs to have the full block chain available at all times!!1!'
@_date: 2015-06-12 13:28:16
As he's clearly saying, the metric of interest to a full node is the O(...) PER NODE, which is (mostly) O(n).
@_date: 2015-06-28 09:13:01
How about Bitcoin/BS and Bitcoin/XT? 
@_date: 2015-06-24 17:52:23
And as far as I know, Gavin was also one of the first guys to be involved in Bitcoin - AFAIK earlier than Greg Maxwell and the other 'core devs'.
 I believe Mike Hearn was also an early participant.
And ... as I said elsewhere: No hard data appeared yet why Satoshi's initial estimation *that Bitcoin is indeed scalable* is wrong.
Just opinions and social engineering to make Bitcoin the bitch of some special interest....
@_date: 2015-06-12 09:31:13
What is a valid block? One which follows the Bitcoin protocol. Meaning valid transactions with the largest amount of hash power.
What is 51%? The largest amount of hash power. They can go and start mining from zero (the genesis block) if they want and invalidate the whole blockchain.
Now I concede that there are 'checkpoints' and similar. But that is actually centrally planned and not trust-less.
@_date: 2015-06-11 09:36:50
I am not saying 5 nodes is a good situation to have - but the situation would still be different if those 5 nodes are in vastly different jurisdictions: Say the US, Russia, China, North Korea and Iran each run a node - do you think they'll go and form a cartel and cheat easily? 
@_date: 2015-06-11 19:53:34
Huh, why so?
Also, O(mn) is O(m) per node.
@_date: 2015-06-15 22:27:05
Yup. Let's see when Greg and the others finally agree to 20MB. I guess it will only happen in Feb 2016 or something.
@_date: 2015-06-01 23:53:58
And a soft fork to reduce the block size in an emergency could be done even quicker!
@_date: 2015-06-14 19:11:19
I think you are probably right with regards to blocksize dangers  - but we both have to admit that no one knows for certain.
Jeff Garzik's proposal seemingly has the support from Gavin (and I'd like to think would accept it, too?) AND it has the ability to kill the neverending blocksize worry for good.
If BIP100 has chance of succeeding and actually convincing devs, Gavin should make sure now that the confusion whether it is going to be 20MB or something like Garzik's BIP100 settles in time.
Actually putting more options out there can potentially stall discussion again: I see the danger of people who have a desire to stall to hide behind this now: "Yeah, I like BIP100, but we need to come to a decision on ... *details where I am disagreeing for the heck of it* ... so that's why I am still NACKing."
And if that goes on for too long, time will run out to do a 20MB fork as well as doing BIP100 in time. Like how is describing it [here.](
@_date: 2015-06-15 21:47:32
Blockstream more and more evokes the picture of a hydro dam in me :D
@_date: 2015-06-12 14:04:17
Exactly.... people seem to have forgotten about this in the last couple of years.
@_date: 2015-06-11 19:50:18
.. take the longest chain in terms of hash power. Bitcoin rule   Done.
@_date: 2015-06-03 20:00:44
That's the same argument Gavin was making - I agree with it probably being the largest factor, however to be honest I like no one else really knows what caused the drop in full nodes.
@_date: 2015-06-02 09:45:30
Soft forks are technically a lot easier than hard forks. 
We could soft fork a new cap in should stuff get crazy - but we apparently can't agree on a hardfork, as you can witness here.
@_date: 2015-06-11 13:40:13
I am not a native speaker, so that I guess would count as somewhat of a defense :D
In any case, I probably was thinking of 'irrespective'.
@_date: 2015-06-02 09:47:41
Exactly. 51% of the hashpower agreeing to not cripple the network and to not build on top of bloatblocks.
The 51% sane majority btw. is needed anyway in Bitcoin already. By design. From day 0.
So lets get the damn 20MB + 40%/y and if stuff becomes crazy, it should be easier to do a soft fork to a new cap than a hard fork.
@_date: 2015-06-28 09:53:43
Not at all. I am talking about the possibility of an SPV client fooled by the wrong chain of block headers. He'd need to be fooled by a lot of hash power.
Of course, you can get into chicken and egg problems of network splits etc.
Reality proves that the SPV model works well, though.
@_date: 2015-06-28 11:26:19
First of all, no one should start framing the discussion as if a blocksize limited Bitcoin is the ought to be.
EDIT: And for continuing the discussion, yes. Maybe it is more productive to talk about the actual issues, so lets go [here.](
@_date: 2015-06-16 11:40:38
Do what you must... But I still have a lot of hope that economic pressure and the option to switch to XT will flush away the current blockade.  I hope it will reduce the influence of the Blockstream-affiliated guys to a sane amount: Have say in protocol details, but not in the scaling of Bitcoin, as in this area there is conflict of interest.
I think there is meta consensus in the sense that economic pressures cause for mostly just one single Bitcoin implementation to exist, and not the other way around. That there will be two soon is relief of this pressure. It is obviously far from perfect and a new equilibrium is still in the works. Actually, this process just started.
@_date: 2015-06-27 11:08:33


The vast majority is arguing for at least greatly increasing this hard cap


Who are the core devs and who says that they ought to be the core devs?
@_date: 2015-06-24 15:58:16
The price of Bitcoin is long-term correlated to its (perceived, future) utility ...
@_date: 2015-06-12 16:54:58


The status quo is essentially open-ended blocksize. Conservative is to keep the status quo. 1MB will eventually end up with Bitcoin running into a wall. So arguably, the 1MB-limiters need to make their case why it needs to stay in place.


Interesting to say that a 1MB constant in software is 'the technology'.
@_date: 2015-06-28 10:33:35
    This is a lot of fear-mongering


Talking about legal attacks or physical attacks is fear mongering. 
It is not my fear, but if it is yours, maybe remove yourself from the responsibility that you claim is yours?


You do not have authority on what people will run as Bitcoin, and you'll soon see that effect. You only have the value of merits, and it appears that you are squandering those.
@_date: 2015-06-20 09:42:39
And I do not see a problem with 130MB blocks *eventually*. I also do not see a problem with 8MB blocks *now*.
Satoshi's initial look at scaling is still valid. Bitcoin can scale. Just because some people want to steer Bitcoin away from the initial vision (for profit or otherwise) doesn't mean that the original vision became impossible. That is just what those people try to make you believe.
@_date: 2015-06-24 17:11:44
Not to mention that anyone owning Bitcoin would be stupid to support a change that makes him-/her lose the corresponding value.
Assuming that the majority wanting to have a higher blocksize correlates strongly with people who own BTC, it can also be assumed that they do indeed know what is better for Bitcoin.
@_date: 2015-06-13 12:08:16
Metcalfe's law is about the *value* of a network, not the number of transactions happening in it.
As a user, I am not going to suddenly interact with n people in the network just because there are n people around. I will interact with a certain number of people, do a certain number of transactions per day, and that's it.
Certainly increased value of the network will scale the fraction of transaction that I am going to do over Bitcoin. Also, there might be some slow economic growth that makes the total number of transactions per user per day go up somewhat.
But I am not interacting with all n users in the network. That would be insane.
@_date: 2015-06-11 19:36:51
I don't think this will happen but: 
If full nodes become that rare that they are overloaded by SPVs - isn't that solvable by *paying* the full nodes then? After all, this is a discussion about a form of money.... :D 
If full nodes eventually require super-extra-duper computers (which is IMO still far away and unlikely), I might pay a guy somewhere to provide me the chain.
@_date: 2015-06-03 19:41:35
@_date: 2015-06-12 13:18:57


^ This
@_date: 2015-06-25 09:18:38
But as we all know, Satoshi also said that Bitcoin can and should scale to gigatransactions.  And he had a hard look at the data and concluded that *yes, it is indeed possible to scale Bitcoin up*.
And I have yet to see new data that shows that this initial scaling plan for Bitcoin is not possible.  Just all kinds of random *opinions* and *social engineering* on what Bitcoin should be now.
@_date: 2015-06-28 12:06:06


Exactly. There's data on computing and network capabilities, though. And Satoshi's vision is possible with the Internet hardware of 2002, even.


Then how about you giving Bitcoin the chance to kill itself? You can be sure that Bitcoin won't succeed with 1MB. If it grows big and fails, an Altcoin might have the chance of replacing it. Less so if it fails early.


I am suggesting you get off your high horse first. Thank you :-)
@_date: 2015-06-09 16:13:30
Yep. I see the anti-increase side more and more like either runaway academic discussion and, yes, actual nefarious social engineering tactics and attempted divide-and-conquer. Because the style of discussion *clearly* points to Gavin and Mike being the sane top-level guys in all this.
I have seen no one so far *opposing* things Lightning Networks on top of Bitcoin out of principle - because it doesn't make sense to oppose added functionality on top. Yet, multiple people here try to frame it as if it is an either or thing.
@_date: 2015-06-16 20:19:00
Unfortunately, you parent got deleted. Do you mean the blocksize debate?
If so, arguably there is [this](  post that I like to point people to, with a lot of foresight even back in 2010. And I remember I had worries right when I started to know about this limit. Which must have been in 2011 latest. The current stalling is insanity.
@_date: 2015-06-19 14:28:58
Look up idea of (Bitcoin based) assurance contracts to pay for mining/hash power. This could be a very adequate solution to diminishing coin issuance.
@_date: 2015-06-12 12:56:38
Much harder for the police to step in if it is a *widely used, worldwide* system. Especially hard to blacklist in that case.
@_date: 2015-06-27 22:38:02
And I have to admit, I LOLed. made the memes fit nicely together in those three lines.
@_date: 2015-06-02 01:08:21
And Gavin didn't intend to remove the blocksize cap...
@_date: 2015-06-23 06:35:15


This is what I also expect - that there will be no jump in transaction rate just because the cap is lifted.
I also hope that this could convince they naysayers that maybe, just maybe, we can actually trust the miners to not undermine (no pun) Bitcoin. Because we have to trust 51% of them anyways...
It is my fear, though, that the blocksize will be economically so entrenched by centralized 3rd party providers in the future, that it will be basically impossible to lift. Given that Satoshi basically answered all scaling questions for Bitcoin and nothing really new has been brought to the discussion, I see the current debate already being shaped by economic interests wanting to profit from the layers on top of Bitcoin.
@_date: 2015-06-12 16:13:19
He has earned it. Really. He has shown a lot of patience and ability to take input and heat and wasn't appointed as quasi leader by Satoshi for no reason.
He only has this much power because we give it to him. And I'll continue to, especially if he gets some agreement with Jeff Garzik.
@_date: 2015-06-02 13:08:31
I am kind of divided between median and average, median is more robust statistically, OTOH Gavin has a point saying that a median always excludes the vote of the hash power minority. So average would be more inclusive. Either one would be fine with me.
Your idea looks very interesting &amp; reasonable and like it should be/should have been discussed more! I like the short memory behavior of your approach (reset at difficulty), but I think there needs to be some averaging there so you don't have any jumping behavior (I might understand it wrong, though).
But given the state of affairs, I am currently more worried that the blockcripplers get their way, though. So if the main push forward is Gavin's original growth formula, lets rather get united behind it before we lose to the cripplers.
@_date: 2015-06-27 13:27:10
The problem is that decentralization is a *very* wishy-washy word here.
Having tens of millions of people own Bitcoins is a lot more decentralized than having 100k nodes serving a banking system on top that only settles with 3txn/s through the blockchain.
And the question would remain in that scenario why people would actually run 100k nodes for just the wealthy and big players...
@_date: 2015-06-09 15:17:20
Mike, as you are discussing SPV here - what do you think about validating the UTXO set, by the way?
@_date: 2015-06-24 19:27:30
Having more than two implementations would solve that to some extend...
@_date: 2015-06-09 10:33:02


Which is rule 0 in Bitcoin land, on what counts as valid. 51% of the SHA256 hashing power. Arguing around that means that you actually try to put one of Bitcoin's most *core principles* in question. Be fucking honest about it.
@_date: 2015-06-12 09:24:59


The IMO very valid case that many people are making is that changing from a 'blocks have some space in them' to 'blocks are constantly full' operating mode for Bitcoin is changing the systems behavior much more than upping the limit from 1MB to 20MB. Satoshi intended Bitcoin to grow, and not to be artificially hard-capped.
And 'centralization' is a very diffuse concept. As thinks, rightly IMO, spreading Bitcoin widely is a much better way to ensure stability against outside influence. If you have comparatively few nodes, but with high traffic and usage in vastly different jurisdictions that might even be competing on a state-level (say Russia, China &amp; U.S.), it can well be argued that they can be shut down much harder than a toy 3tps system 'used mostly by drug criminals'.
And if Bitcoin succeeds but eventually gets close to a centralized payment processor and tool for oppression, its history will show that it is indeed possible to set such a thing up and get it running. Bitcoin will be in competition with Altcoins to the very end if it is successful, probably even keeping it honest and usable and away from that kind of state. But if it fails, it will kill the whole crypto-currency space.


The problem is that  suspicions of conflict of interest are due to the *de facto* situation of blockstream eventually wanting to make a buck - and that there *might* be reasons this influences your *decisions*.
You can *say* whatever you want, even go and say things that are contrary to your company's position once in a while, if what you *do* - blocking a blocksize increase, is in line with your investors.
I am not saying that is the reason you do this - but no talk can ever get you out of that position, as long as you are paid by blockstream. Actually being more constructive on a blocksize increase schedule might.
@_date: 2015-06-25 08:52:50
I know, I know. It is a lot easier to reason and argue about 'the spec' than 'the code' though - especially since the latter is in constant flux, too.
The spec is written in English. The code in C++. The former is a lot more expressive as a language and fits our minds better.
@_date: 2015-06-30 09:06:08
I think the argument between you and here is a little bit off track because it simplifies too much:
It seems to be very easy to go from 'I am an expert, I can explain to you how this works' to 'Listen to me, I am the *authority* on this, shut up and take what I give to you'. There is a fine line between educating about something technical and *pushing through some personal goals redeclared as technical issues*!
The former is very valid interaction between an expert and a layman, the latter unnecessarily centralizes opinion in authority figures and increases centralization risk for Bitcoin.
For example, we can see in the blocksize debate that devs tend to make technical arguments but forget in various ways about the economic and whole system ones. Points where outside folks can easily add to a better, more wholesome perspective.
For better or worse, I think experts do have to some extend convince the user base on technical matters. Authoritarian attitude gladly drives Bitcoin types away. 
Regular, non technical people who own Bitcoin have a strong financial incentive that the system doesn't fall apart. Reasonably, they might select the person that appears to understand the technical matters in question but also aligns with the goals of the outsider person and behavior on their personal level - all which is often subsumed in a gut feeling.
I don't think that is in any wrong or inefficient. I rather think it is indeed healthy - and maybe part of what is saying.
But buzzwords like 'Wisdom of the crowds' or 'swarm intelligence' don't really cut it here, IMO.
@_date: 2015-06-27 17:03:56


First of all, straw man again, no one's talking about 8gigs tomorrow. Don't be ridiculous. Second, yes, current bandwidth would theoretically allow full nodes worldwide even with 8GB.


And why should that happen?
@_date: 2015-06-09 17:44:32
Exactly. I feel the people who want Bitcoin to run into the 1MB wall have much more responsibility to state their reasons than the people who want to tear down this artificial wall - and basically keep it growing as it is already!
@_date: 2015-06-30 14:54:15
@_date: 2015-06-30 13:58:50
[1MB is a hard wall that will not be just 3ct more per txn.](
@_date: 2015-06-28 21:02:36
So you have two dissenting opinions @ MIT, this is evidence that a) the group think and b) the monetary incentives to cripple Bitcoin can't be too strong at MIT.
@_date: 2015-06-14 09:21:31
Listen, if you are afraid of hardforks as attacks on Bitcoin, you should be afraid of them all the time. Because they can happen all the time.
What keeps them from being successful is economic forces. So if you are worried about Gavin's proposed fork, you might be worried that Gavin is right in the economic sense...
@_date: 2015-06-15 22:05:07
Damn! Can you suggest me a hosting service that works without too much hassle (login creation)?
I'll look what I can do.
@_date: 2015-06-12 13:37:56
Yes, it appears that way. The problem is that it is much easier to hide biases and true goals behind inaction (Greg) than it is to hide it behind action (Gavin). That's the sad part of this discussion.
@_date: 2015-06-11 19:48:45
It should also be noted that constant veto to keep the status quo is not 'a consensus'.
@_date: 2015-06-27 14:09:00


Changing operation mode from *no effective hard cap* to *very effective hard cap* is not a small change.
@_date: 2015-06-28 21:07:51
That means that you can voice support by running an XT node (it will show up in crawls done by statistics sites, because your node sends an ID upon request), but it doesn't have the code in it yet that eventually will allow bigger blocks.
I think to save yourself some hassle, just wait until XT is released with the bigger block code, to go active sometime in 2016 (and when the majority agrees).
If you dislike the other features in XT (though I don't think it matters much), I am certain that there will be a release of what is on github.com/bitcoin (so far the main bitcoin) with just Gavin's block size increase code added.
@_date: 2015-06-12 10:31:04


First, arguing that 1MB blocks are the only valid ones is akin to arguing the true Bitcoin is the one still using berkeleydb.
Second, yes they can: 51% of the hashpower can also create a new 1MB-limited chain overtaking 'the original one', if that is what they want.
Technically this happens all the time with orphaned blocks.


Again, 51% of the miners can also go and start a completely new chain right from the beginning, block 0, genesis. By the protocol rules, this is the valid chain.  They can also do this on a 1MB-limited chain.


A fully validating node in a network with &lt;50% of the hash power is worthless. It can always be attacked by existing hash power.
As I said in another comment: Anyone can go and start their own little Bitcoin, lets say a 100kB fork off the main chain. But most people won't follow that fork. With XT, it is going to be different.  The only rule which will then effectively say which fork won is going to be hashpower.
@_date: 2015-06-01 22:39:07
As Gavin pointed out, the fee market is working already.
And blocks aren't full yet, but are growing to be soon. The very issue he was talking about since *years*.
@_date: 2015-06-09 19:32:28
There is always the centralized counter party - the bank - in this scheme, though.
Still, it is a very interesting development - to have euro-colored Bitcoins on the blockchain now.
@_date: 2015-06-27 17:24:32


I must have a new fan here :)
Ignoring for a moment that you are continuing your argument on top of the straw man you just built, modern CPUs can validate about 4000 txn/s.
4000txn/s amounts to about 720MB of blockchain data (with 300bytes txn) per 10min. Parallelize that and you are easily within the realm of 8GB blocks. Use an ASIC and you get it a lot cheaper, power-wise.
I am not saying you can do it on a RasPi. But CPU power is clearly not the limit.


UTXO commitments. And again, you are straw-manning like hell. There won't be 8GB blocks soon.
@_date: 2015-06-24 22:20:41
Longest chain, 51% of HP decides what is Bitcoin.
@_date: 2015-06-30 18:43:00
Group think could explain the singular opinion at Blockstream.
Here on reddit, there are at least two factions.
@_date: 2015-06-24 18:00:49
Interesting, I didn't hear that one yet. Do you have a link?
@_date: 2015-06-18 10:11:55
That's just BS. If transactions are constant per user, for example, it scales as O(n). If transactions are log n with n users, it scales as O(n log n). It is a big assumption that they are even polynomial. Which Greg assumes without further explanation.
You could as well go and introduce the 'time per transaction validation', call that b  and then argue that Bitcoin grows as O(n * u * t * b) with n nodes, u users, t transactions and b time per transaction. Then Bitcoin suddenly would scale O(n ^ 4).
There are different dimensions to the problem, and as Mike and I pointed out, there is no reason to assume that they all go linear with the user base.
And when they go linear with something else, it still doesn't make sense to lump them together, as I described above.
EDIT: I also have to say this: With Greg's poly(n) x poly(n) notation, he's adding a lot of bright-sounding, academic fluff, but nothing to the discussion. Both Mike and I addressed in detail exactly the question of whether we have two at-least-linear polynomials multiplying here.
EDIT: Fixed misattribution. isn't talking about poly(n) fluff, Greg is.
@_date: 2015-06-18 19:58:07
Gavin has been asking for input for frickin' *years* and was *very* flexible with his proposal. The other side is simply *stalling*. And they don't stall accidentally, as it looks now, they knowingly stall, I might add.
@_date: 2015-06-24 18:26:39
You are counting on the miners wanting to cripple Bitcoin.
Yet they have the highest stake in Bitcoin, with purpose-build hardware that is going to be absolutely worthless should Bitcoin fail.
Miners will behave in their self interest. Which includes a healthy Bitcoin ecosystem.
@_date: 2015-06-16 20:07:22
Safe? No, we all don't know shit if you take in external market conditions. It can well be argued that a 1MB limit will cripple Bitcoin and kill it in the mid-term. I don't even think that scenario is unlikely.
In any case, a open-ended variant (w/o 32MB cap) of BIP100 would be good for everyone and market based. And has some very bright things to say on how to pay full nodes - in essence there is no reason that they provide their service for free in a Bitcoin-success scenario.
@_date: 2015-06-27 14:32:22


Then why is he so stubborn about *planning* to raise the cap?
You need a *hard fork* to raise the cap, but only a *soft fork* to bring it back down, should the need arise. Surely, if stuff goes wrong, he should be able to soft fork it back down again? 
Soft forks are easier than hard forks...
@_date: 2015-06-28 09:42:00
If the whole network is 51% lying to you for a while Bitcoin has BIG problems anyways.
@_date: 2015-06-30 22:23:58
This is not a helpful comment...
@_date: 2015-06-30 13:46:58
Ah, thanks.
@_date: 2015-06-24 18:29:56
'No limit on blocksize' is pretty concrete and needs no further explanation. But I guess you need a BIP with TPS cover sheet...
@_date: 2015-06-27 18:56:45
Agreed, actually.
To clarify: I say this in the context of the social engineering that is going on right now that is trying to make a crippled Bitcoin the ought-to-be. 
EDIT: And it should be pointed out that 'preferred solutions' is a wide space to choose from. A preferred solution might be Bitcoin failing for some...
@_date: 2015-06-27 19:18:50
Centrally planning a hard cap is not a very subtle thing to do, though...
has some excellent thoughts on that.
@_date: 2015-06-02 13:35:09
That's why I think it eventually needs to be coalesced together (after a couple years or so, meaning buried in a couple TB of blockchain or so in a Bitcoin-success scenario) into hashes saying
'UTXO merkle-root for N coins'.
And then, to prove that you own the coins in that merkle root, you have to open it up again, supplying all the necessary merkle tree branches for the transaction you want to make.
That would stop the UTXO set from ever growing, would put the burden of storing the data necessary to transact to quite an extend back to the users and make Bitcoin itself almost O(1) lean (except for the *very slowly* growing blockheaders).
I think is describing what needs to happen next anyways. 
@_date: 2015-06-02 13:41:08
That's a very good point.
@_date: 2015-06-27 12:13:18
Thank you!
This whole debate is so weird. [Here we have another one]( who seemingly thinks that Bitcoin's price is irrelevant and it is all about some ideals, yet for some reason is keen on making *Bitcoin* the subject of the 1MB cripplecoin experiment. Instead of creating his own leet uber-cypherpunk altcoin that he can implement all those ideals in. And he shouldn't worry that it is an altcoin - because the price/market cap apparently doesn't matter...
@_date: 2015-06-21 09:21:56
Point is that those connections *exist today*, and people with that much storage, even though this is still far out in 2035 for Bitcoin.
@_date: 2015-06-12 10:17:25


That's a bit simplistic, you forgot about and I think it is indeed important to point out the blockstream affiliations of Greg &amp; Co. here.
That said, the majority of the dev team is not the only party that matters in the ecosystem. Most of the [users]( most of the [companies]( also have an effective say. Just pointing to the 'majority of the devs' is arguing from authority. 


This can as well be said of letting Bitcoin run into 3tps saturation.


I remember the 250kB debacle. It was awful. But it had been fixed by changing the softlimits. A change to the 1MB hardlimit will not have any agreements on specifics when SHTF. So in a way, Gavin pushing the issue now and risking hardfork in 2016 is sane comparing to risking total havoc last minute. There is still the chance that either bitcoin-core or bitcoin-xt will emerge as the clear winner of the current 'war' until March 2016.
@_date: 2015-06-28 20:55:25
The question still remains is who controls the brand, i.e. bitcoin.org and github.com/bitcoin.
I do think now that Gavin is *waaay* too nice to the other devs here.
Bitcoin/XT (Mike) and Bitcoin/BS (Blockstream) should get equal representation. Give the responsibility whether it comes to a hardfork or not back to the users, where it actually belongs.
@_date: 2015-06-17 15:04:17
That would be great, yes. However, it seems that usable ZKPs are currently 'moon math' and some schemes have been broken AFAIR?
UTXO commitments would be doable with the crypto primitives that are in Bitcoin right now and they are easy to understand.
@_date: 2015-06-30 14:33:45
A problem of tunnel vision. Same problem with the blocksize.
*Good enough* zero conf in most cases is totally workable.
Attacking the non-zero failure rate and certainly some problems with it as 'fundamentally unworkable' is putting up a straw-man (the never intended goal of perfect zero confirmation) and attacking it. That is dishonest.
Apparently just to get some attention? Or is there a profit motive behind this?
@_date: 2015-06-18 11:24:44


Transactions scaling linearly with users per user is a completely ridiculous assumption. There is data on these kinds of networks. See my other posts.
@_date: 2015-06-29 08:29:13


Subpoena and then what? Get all the data that is publicly available anyways? Also, a lot less to sift through if we stay stuck with 1MB...
Don't be ridiculous.
@_date: 2015-06-16 10:04:13


Exactly. The constant is where an centrally-planned artificial limit on capacity comes from.
@_date: 2015-06-16 13:13:27


I wouldn't favor 100MB blocks on the network *now*, but consider that 'decentralization' is a *very* ill-defined metric and Gavin and other people have a good point IMO in saying: Widely spread Bitcoins are the best measure to ensure decentralization.
Fear of centralization seem always to be coupled with fear of government interference. But in the very unlikely scenario that only a couple tens of full nodes remain in different countries, you'd still have those countries competing in allowing uncensored access to the blockchain. And with the corresponding high transaction volume in such a success scenario, you would have *many* people keeping an eye on Bitcoin, as well as governments competing and ensuring fairness.
Reasonable anti-spam blocksize, yes. 20MB has been shown to be reasonable by Gavin's work.
Don't kill Bitcoin prematurely with a 1MB limit because you fear its success.
@_date: 2015-06-27 13:54:23
There is a lot of variables in that, though. You'd need to roll out the necessary change (which could be very large if the SC is completely different to Bitcoin, for example) and you need the Bitcoiners to be *not stupid* about doing this.
 
So yeah, in the end it is probably all some kind of selection process, with some (more or less) intelligent design though :D
@_date: 2015-06-09 18:03:18
Yep. And there is indeed centralization when 4 people, led by GMax, block the whole ecosystem.
And there is indeed a crisis already, just in the visible deadlocked blocksize discussion.
Now, they want to tell us: "Just wait until the last minute, no planning necessary, we will *hardfork* in the last minute.". As if there will be no contention then. People will argue: 2MB! No, 8MB! No, 20MB! No, fees are great, lets go down now to 100kB!!
A *softfork* to decrease the blocksize back down (should there be a misssed problem) can be done even easier than a *hardfork* that needs planning (just as Gavin is doing), certainly so if it is an emergency issue. Schedule the hardfork now, and softfork back down if unexpected problems (sudden immense node centralization or similar) appear.
This whole discussion is ridiculous. Gavin should fork this.
@_date: 2015-06-27 14:10:47
And which is still possible. No data has arrived yet that Satoshi's vision is physically/technically impossible. Just FUD and opinions.
Heck, the Internet from *2002* could have easily supported a global Bitcoin payment system used by *everyone, worldwide* for their cups of coffee.
You don't even need Moore's or Nielsen's law. But Bitcoin *does* need a large user base to be successful.
@_date: 2015-06-29 23:31:38
I don't think it can be defined to have a binary test either - but people who use it a lot should also put up their exact definition. That includes all the blocklimiters.
Because it is *very* ill-defined and diffuse. Leading to this word being abused all the time as a scare tactic against bigger blocks.
@_date: 2015-06-11 09:24:08
dug [this]( out. Makes 100k -&gt; 10k a lot less worrysome and the arguments that nodes are disappearing at an alarming rate pretty weak.
@_date: 2015-06-17 14:55:11


Yes, that is technically true. So in a way, you could argue that it is a tiny bit smaller security than a 'validate from block 0' full node.
But if you'd take data from a year back, you'd need to outrun that year with your own hidden nefarious fork.
At some point, the security discussion IMO becomes as academic as the question whether someone could in secret produce a full chain replacement with more hash power at the tip than the current chain.
You'd need to fool the whole world for a full year. If a year is not enough for you, take a decade (more hashpower than the current chain).
EDIT: If you think about it, the level of security to choose would actually be up to the full nodes anyway. With UTXO commitments, no one can stop anyone from pretending to have processed the whole chain and starting to participate in the whole network.
And without, as it is right now, I could also get an image of a pruned full node from somewhere and run it, and it could be malicious. And I wouldn't know.
So arguably, UTXO commitments increase the full node security and give people a slider to choose their security level. If someone wants to have archive grade security from block zero, fine, he can pay for that.
@_date: 2015-06-14 19:39:02
So you are back to O(n), which isn't disputed by anyone. No one disputes that more users mean more transactions mean more costs. The whole fucking O(n ^2 ) doesn't need to enter into the discussion, this is misleading chaff.
Here's a much simpler argument, straightforward: A user makes some approximately constant amount of transactions per day (yes, maybe another log^x n because of Metcalfe's usability here, but so what, definitely no (n) ), and those need to run by each full node in a gossip network.
So a full node's bandwidth needs to scale with about the number of users. Just as is saying. This way, it also sounds a lot less scary and O(n ^ 2)-y because this is obviously what each full node sees.
No one is disputing that more users == more bandwidth required. What is disputed is that it scales with significantly more than O( n ).
@_date: 2015-06-28 14:39:56
Until XT starts a revolution... 
@_date: 2015-06-12 13:22:59


It is not even that: apparently expects it to be not controversial in the last minute, as far as I understand him. Ludicrous. 
And if it *is controversial*, you are damn right on doing what you do and planning now. The alternative is 'ok, it's controversial, so lets say lalala and drive Bitcoin into a wall'. XT at least gives people the option to vote and decide on the fork well in advance.
@_date: 2015-06-30 23:11:10
Can you explain why that is so?
I've heard that several times, this irreversible direction of centralization, but have not yet seen a convincing argument for it.
@_date: 2015-06-09 15:53:12
I think you really want to validate the UTXO set (make some kind of tree out of it, and hash it at each level) and put that hash by protocol into the blockchain, and have all full nodes validate that hash.
This way, you can eventually go and start full nodes by going back to the UTXO set one year in the past (or similar), which is buried under lots of hashing power and thus obviously a valid part of the blockchain. 
Further along this line, I can imagine that very old UTXOs might at some point be coalesced together so that you could do a trade-off of storage needed for the blockchain/UTXO set vs. a small increase in bandwidth (send proof that you own part of a hashed-together UTXO).
@_date: 2015-06-16 10:11:11
The thing I don't understand about Garzik's proposal is the reference to the 32MB hardcap.
He says he's for a market based solution, so why is he putting a 32MB hardcap, another cap that will bite us badly, in?
If the market decides block size won't ever growth above 32MB, that cap is not necessary. If block size does grow above 32MB, the hard cap would be a centrally planned limit, counter to the whole intention of his proposal.
And, yes, I know that the 32MB cap is coming from network message size limits. But don't tell me it is the consensus code. Because it can't be right now if 1MB is also consensus. So it would be another cap socially engineered as to be seen as 'ought'. 
@_date: 2015-06-02 09:52:38
Well, he did sell half his coins when ghash.io was close to 51%:
So maybe he has a love/hate relationship with Bitcoin now (like probably some of us do)?
@_date: 2015-06-28 21:11:48
And it should be added that if XT becomes successful and the expected form of Bitcoin at exchanges and your trading partners etc., the 1MB chain could as well be the wrong one to be on.
If it comes to a hard fork (which I think is almost inevitable now), it is either going to be a clear majority either way (probable), and if it isn't, it might be a good idea to wait around the fork date with any transactions, until the whole ecosystem goes out of the metastable state.
@_date: 2015-06-30 19:58:06
Are you the centrally appointed spam/ham arbiter?
I've seen a couple of those 'downscaled' transaction rates (excluding the spam). They are not very convincing...
@_date: 2015-06-09 17:56:37


Exactly. And as it was designed and proposed by Satoshi himself. 
[And this guy predicted the current messy situation as far back as 2010.](
@_date: 2015-06-30 22:59:00
Gavin's proposal is just 40%/year, Nielsen's law, not Moore's.
And it can be *soft* forked down if it turns out it is too much.
@_date: 2015-06-11 14:48:01
Yes, thank you for doing this, by the way! Are you related to the Bitnodes guys? 
I remember you are mostly doing stats for a single node and no active crawling? Do you think Bitnodes new data is matching what you see?
@_date: 2015-06-27 17:17:39


[See here. 'Nuff said.](


Not at all. Bitcoin is designed to scale, Satoshi presented all the relevant info, after than only opinions came. And the goal for Bitcoin is to scale, and not run into a hard limit, do not try to social engineer your new desired status quo.


LOL. You are accusing me of a straw man and a sentence later, you make one up. No one expects billion of people to join the blockchain tomorrow. In 10..20 years, maybe!


As you like to throw rhetorics around: False dichotomy. I answered that also in my other comment.


You have tunnel vision. It would certainly be *nice* to have more full nodes, but it isn't the only variable.
And please do not enforce it with a centrally planned limit.
@_date: 2015-06-30 19:45:46
Long term, total miner hashing power might need to be regulated to not eat the planet's energy output.
But I think that is really a far future thing.
@_date: 2015-06-11 10:13:18


 Especially the lack of such tests by the people who dislike his 20MB increase...
@_date: 2015-06-25 13:56:20
Maybe it would be best to give users the choice and make it a market-based solution: Let users decide whether to pay for a full node that stores all UTXOs or just a coalesced hash and it is up to the users themselves to supply sufficient merkle tree branch proof.
@_date: 2015-06-30 11:21:38
That's the level of discussion now, seriously?
Painting Gavin and Mike as pedophile child abductors?!
@_date: 2015-06-24 18:46:25
I think I might have heard about someone doing just that? :D
@_date: 2015-06-14 20:23:42
Hear hear. I can as well argue that we need to stay away from the central 1MB planning committee.
Lets please get back to the actual issues.
@_date: 2015-06-12 11:40:56


Fair point. But this is only about usage of words, right? Blacklisting every single Bitcoin transaction is about the same as banning, right?


Yes. But where is the constructive, forward-looking planning from and others? Where is it?
All he's saying is: Last minute is going to be fine, we've done it before. Which is completely ignoring that there is very visible contention already, right now.


Yet TPB operated (operates?) in Sweden for a long time, IP rights are very relaxed seemingly in Russia, too, in China you can buy bootlegged CDs on the street. As long as you have a mostly uncensored internet, a full node in Russia (e.g.) has all reasons to keep the node in the U.S. honest, and vice versa.


My imagination is creating a deadlock in scaling Bitcoin and shifting people away from the original vision of a scaling Bitcoin through social engineering. Exactly what is happening now.
@_date: 2015-06-30 14:44:37
Why not?
No they are not guarantees. I very much agree. But this is part of the Bitcoin system. To trust the miners sanity to some extend (&gt;50%). Also see what I wrote [here](
@_date: 2015-06-30 19:43:05
Hehe. Yeah. I don't get Peter. He's approaching a genius schizophrenic on seeing all sides in the (too) narrow games that he looks at, but consistently fails to see the bigger picture and also that his behavior might actually be unnecessarily rude and chaos-inducing.
Or maybe he has something to sell, too, I don't know.
@_date: 2015-06-30 09:57:35
Interesting idea! Though I think it will be very hard to get through now, as it changes a lot of assumptions.
@_date: 2015-06-11 17:56:16
And their scheme - which they have knowingly and willingly implemented given market conditions - creates a real pressure on the fee market. Because it will be a certain fraction of blocks that are empty due to their policy and they will only be filled with transactions if fees are becoming sufficiently high so that the AntPool people reverse their decision.
But yeah, we are going to runaway zero-cost transactions, blahblahblah...
Altogether another reason that 20MB (or probably any higher block limit) will be fine.
@_date: 2015-06-12 09:43:58
The protocol allows for the miners with 51% to create the longest chain of Bitcoin transactions, and with 51%, they can invent a new chain that will eventually contain whatever transaction history they wish.
@_date: 2015-06-02 09:55:39
Planned for March 2016.
@_date: 2015-06-07 09:13:15
10GB-20GB blocks is not scaling to worldwide usage?
If we ever get there, Bitcoin is widely successful and has scaled.
I am not disagreeing that there will still be other layers on top then, though.
But 20GB blocks would mean about a transaction per person on the planet per day. Enough to fuss around with many, many LN channels and the like. Probably enough for all kinds of hardware to have their own Bitcoin wallets, too.
@_date: 2015-06-17 01:19:20
Yes, that's a good point, thank you. Indeed, old, empty addresses can be affected, too.
@_date: 2015-06-28 09:54:53


Very workable. Just not with RasPis under your desk. Too bad.


Nothing at all, and you *know* it.
@_date: 2015-06-28 20:51:40
No one is hijacking anything. It is free association. You could actually only argue that bitcoin.org and github.com/bitcoin are hijacked by not giving the XT variant of Bitcoin equal space.
That said, I think there might actually be value in a hard fork: People seem to be very scared about it, and it creates a lot of distortion in thinking. I cannot see *any* situation anymore that a hard fork won't happen with Bitcoin, unless the 1MB resistance significantly crumbles in the next couple days. I think it is too late. It *will* happen. With at least 5% of people on either side. So more than just a random dude in his basement hard forking...
And look on the bright side: Bitcoin is *always* at risk of a hard fork. Seeing how well Bitcoin can survive this risk will certainly shake a lot of assumptions loose, but if it survives, it would show whether it is indeed 'antifragile' (what an overused buzzword..)
Until now, people are operating under the assumption that Bitcoin is highly fragile. The whole block size debate is only existing because people will fear that it breaks apart without a limit. I think this fear might be holding Bitcoin up more than anything else.
@_date: 2015-06-12 11:52:21
Even the extreme case of a couple tens of nodes doing Bitcoin worldwide is far from the traditional banking system.
And it wouldn't be centralized.
@_date: 2015-06-27 17:07:39




No one can exactly predict the future. That's a given.




Lol. You go in circles and then come back to where you started. Moon landing won't happen with 1MB.
@_date: 2015-06-12 11:47:36
Hard fork obviously needs scheduling now or soon. Look at the fucking contention around here. What the heck makes you expect there to be no contention last minute?
So. If you accept that this is contentious, and that planning is necessary, is simply right in his urgency and *he is being stalled*.
Constructive means something like saying: Ok, maybe 5MB end of 2016. Or similar. 
It is about a concrete number, constant in bitcoind. That number needs to go, and planning for that needs to be done now. But no input is coming to Gavin.
These are not lies at all.
@_date: 2015-06-16 09:06:53
From what in the posting history of a redditor do you infer this?
@_date: 2015-06-11 09:32:54
I though rt.com is the foreign news service of Russia anyways?
@_date: 2015-06-16 10:23:29
Thanks for that effort. What might be missing: People argue about scalability in more detail. Look at [this]( thread, for example. 
@_date: 2015-06-03 19:14:02


Which is false in general. Because you can do pruning. Which is exactly my point.
@_date: 2015-06-17 14:47:42
By putting the 32MB cap in as he does now in the proposal, he is making the implicit political statement that it is somehow important.
*Instead*, he should go and say that there is a 32MB currently in place, but in the context of this BIP, this is to be lifted at some point to allow his market-based approach to flourish beyond 32MB.
That he is not doing that is very telling.
The 32MB cap would only be the new 1MB cap, with similar amounts of crazy contention.
@_date: 2015-06-30 08:20:20
Good points.
@_date: 2015-06-12 09:59:32
And as Gavin points out, we have a working fee market already...
@_date: 2015-06-19 14:47:59
Not if the change is that you can divide those units up by another factor. The sum counts. And that should ***always*** stay at 21M.
@_date: 2015-06-14 09:39:03
The 'maximum of bottom 20% of votes'-rule is in a way a centrally planned economic rule though, but this probably cannot be avoided...
@_date: 2015-06-27 11:58:55
Yes. But I honestly only want side chains when there is either a very generous or open-ended hard cap on the main chain. Because else you might actually potentially get that hostile takeover situation that seems to fear.
@_date: 2015-06-16 21:37:13
No, the issues are a lot deeper, really. 
Do you think people will be that stubborn just because some formal process (that, as asserts, can't even really be followed properly) has not been done yet?
put that 'he didn't make a BIP1!1!' silliness in [some]( nice [words](
EDIT: And what points out is indeed enough to show that the BIP process is broken, I agree with Justus Ranvier.
@_date: 2015-06-11 20:13:03
Maybe it is a hidden test whether economic incentives are enough to force some consensus ;-)
@_date: 2015-06-28 20:36:32
@_date: 2015-06-29 18:33:11
Yeah, I followed all that, look at my history, the blocksize is on my mind as soon as I knew about that limit.
But you are right, it is indeed the biggest, longest one. What I meant is that there is just this huge disparity between people who are involved in it (basically less than 50) and people who own some. There must be millions who own some by now. But maybe that's just normal.
@_date: 2015-06-28 21:39:18


Told you so :-)
@_date: 2015-06-11 09:30:17
We need validated UTXO sets and we need to hash together old UTXOs (put the burden of proof that you own part of them by supplying all the necessary information when doing a TX).
That could make a full node almost O(1) (Except for the block headers)
@_date: 2015-06-25 08:32:38
That's why there is a 75% barrier.
@_date: 2015-06-28 11:55:44
And a lot of hashpower?
@_date: 2015-06-19 23:17:49
We actually do have ideas for a proper mechanism. Look up Mike Hearn's assurance contract idea.
@_date: 2015-06-27 02:13:45
Who actually decides what official bitcoin-core is?
Unless you can precisely answer that in a philosophically satisfactory way, don't call XT an altcoin either.
@_date: 2015-06-11 10:53:54
Maybe true - but this could be *very* short-sighted... 
@_date: 2015-06-17 00:47:11
That explanation makes sense and would explain the process. Which wallets are doing this?
@_date: 2015-06-12 15:47:59


If 20MB + 40%/y of Gavin's original proposal would always be the upper limit for the increase - would you agree to that? Could you work this out with Gavin?
Lots of people are looking forward to some movement in the debate, and maybe you can get stuff going.
@_date: 2015-06-28 14:47:22
Gavin is pretty much the first guy after Satoshi to be involved in this and still being a core dev.
Both Satoshi and Gavin understand Bitcoin very well. And so do a lot of other people, non devs, who are taking part in this debate. Stop the arguments from authority.
I think it is rather a problem of tunnel vision for some of the core devs - seeing just the code, not the larger picture.
Oh and I think the whole layering is a) possible profits for 3rd party (has a point there) and also b) the crazy addiction quite a few CS folks have to create layers upon layers of complexity. There is of course also c) some need or additional layers on top, but it appears it really is not the driving force anymore on the side of the blocklimiters.
@_date: 2015-06-03 19:57:31


They might be hesitant to use Bitcoin because they see the 1MB limit.
@_date: 2015-06-11 09:22:29
rt.com does seem to promote Bitcoin to some extend, though.
@_date: 2015-06-12 23:11:37
Also, contrary to banks, mining is a very level playing field. Wanna mine? Buy some mining hardware, connect it, boom, done.
No stupid barriers to entry or similar.
@_date: 2015-06-27 22:25:06
8MB doesn't go far enough, I agree. Yet, even this is seen by too much. See the relevant discussion.
BIP100 has a 32MB hard cap. There are technical reasons for having a 32MB limit right now - but the way it is written in the paper makes it sound like an *ought to be*. As you say you don't want to see this drama again, try to convince other to get the language in BIP100 cleared up in this regard.
@_date: 2015-06-03 20:29:14
I think Gavin does that because he is more worried about a stalled, deadlocked discussion (as we have right now) and Bitcoin being unable to change in the future. The 40% make sure that there is always headroom, while compromising with the crowd that thinks that computers might not be able to run it, except in big data centers. 40% is the expected average increase in bandwidth over the next two decades. 
Because he, as I am (as far as I know) in the end pretty much not worried about blocksize. He's rightfully arguing incentives for miners are aligned with reasonable blocksizes.
@_date: 2015-06-12 15:31:09
Above, I suggested combining Gavin's original growth formula of 20MB + 40%/year with this one, taking the lower limit of the two.
@_date: 2015-06-11 20:12:27
Indeed he seems weirdly aggressive. I wonder why.
@_date: 2015-06-12 15:46:05
The problem is that these people need to communicate. Hopefully, Mike, Gavin and Jeff can at least get together to get a consensus on their side.
... as Jeff's proposal is completely open-ended and thus in a way even further from the goals of the 1MB-blockistas.
@_date: 2015-06-12 11:43:01
An arbitrary 1MB constant in the software is an *intrinsic limitation*?!
Are you kidding me?!
Intrinsic limitation is the hardware maxed out, processing whatever it can.
@_date: 2015-06-30 19:20:10
You're welcome :P
@_date: 2015-06-22 16:48:36
There are people now, regardless whether you like it or not, who want to put some additional profit layers on top of Bitcoin. They need a limited Bitcoin for their layers to be profitable and they are powerful enough to centrally plan and cap Bitcoin's growth. Gavin is pushing hard but this seems to be the maximum to get through.
It really looks like Gavin's proposal is -unfortunately- the best we can hope for regarding blocksize.
@_date: 2015-06-09 10:15:08
It should be noted that Gavin repeatedly came down with his proposal, to try to reach consensus...
@_date: 2015-06-09 19:43:00
Very true. On bitcointalk.org, I have seen someone argue along very similar lines recently regarding the Gavin/Greg dynamic. I am not saying he's necessarily right, but it is an interesting perspective. From






@_date: 2015-06-15 22:00:39


This hyperbole is IMO a good indicator for weakness of argument.
@_date: 2015-06-28 20:30:00
Too narrow vision. Code is not everything. There are meta rules in place, too.
The problem is that it is somewhat ill-defined - else we wouldn't have this debate here.
@_date: 2015-06-28 12:00:13
Enough to make 6 blocks?
@_date: 2015-06-17 00:18:39
By the way, Gavin gave some updates on his progress. He's actually intending a BIP. See here on what he's doing:
Given those parameters, I'd be ok with either proposal, with a slight favoring of Gavin's, as it is technically simpler and has been discussed for longer.
I also think that Gavin's approach will amount to something close to Jeff's in the end anyways, as I am pretty sure Miners are not going to produce megabloatblocks and decide -market-based- for themselves what to do. This is not to say that Gavin's formula is producing megabloatblocks, just that if they favor slightly smaller growth than Gavin's formula, they'll make it so.
As they apparently do that already.
@_date: 2015-06-12 11:50:38


True right now but can be changed with UTXO commitments, to bring it down to O(n) per node. So intrinsic is a bit strong.


Only if you believe in keeping very diffuse notations of decentralization. And somehow, magically, 20MB will make it all centralized.
Also, technological progress.
EDIT: And it is ridiculous to believe that Gavin cannot understand the limitations of the hardware and the network.
@_date: 2015-06-27 12:08:12


 
Bitcoin has worked since years without an effective hard cap and should continue to do so. Do not socially engineer what you like Bitcoin to be.
 


As you say, those cypherpunk idealists apparently do not care about the value of money, so they are free to create a (mostly useless, dare I say) super-duper decentralized Altcoin. Go for it!
@_date: 2015-06-07 09:13:56
Tree chains are vaporware that apparently noone except Peter Todd understands.
@_date: 2015-06-16 09:36:34
I am snarky whereas you go ad-hominem... :D
@_date: 2015-06-30 15:58:56
Exactly. I can also fork Bitcoin really hard in my basement. And stick those forks into my Rasberry Pi(e)s.
That doesn't mean that it will have any effect on the large scale behavior of the network.
Adoption does. Which will be due to economic incentives of the involved players.
And apparently Gavin has finally found a sweet spot there. Because else the people who would profit so much from a crippled Bitcoin wouldn't worry so much about a hard fork.
Because it would be very successful. Hopefully &gt;75%.
A safeguard that he built into his hard fork by the way.
75% of hashpower. Else nothing happens and Bitcoin/XT behaves just like Bitcoin/BS.
So even less reason to worry about it.
@_date: 2015-06-29 22:33:01
... no answer. Telling...
@_date: 2015-06-23 06:22:25
@_date: 2015-06-16 20:43:19
How do you know that? Because Bitcoin is processing transactions right now?
@_date: 2015-06-01 22:40:03
Look up what happened when the miner's didn't care to even increase their soft limits - it was a world of hurt.
So we have that data already. You do not want to go there.
@_date: 2015-06-30 21:28:23
At least there is [*some* movement](  from the blockstream guys now.
 
@_date: 2015-06-28 23:01:11


Exactly. A 1MB limited blocksize does.
@_date: 2015-06-17 10:24:27
I think you misunderstood me. My point is to state the *intent* that 32MB is only *technical* not *political* in BIP100, to help avoid the deadlock, BS and contention that we have right now at the point in time when 32MB is not enough. 
@_date: 2015-06-12 14:41:59
[Not FUD, clearly seen years ago as inevitable]( thanks to 3txn/s is not enough. This is consensus. Productive discussion on a hard fork in the last minute is not going to happen with this contention. Thus the planning needs to be done now.
@_date: 2015-06-11 14:02:13
In another light, people call 'staying at 1MB the conservative engineering decision'.
But it is far from conservative to change the operation mode from Bitcoin with essentially unlimited blocksize right now, to operating with a hard cap and in saturation (like the 1MB people who plan on 'a fee market establishing' want to do).
Consider an electric company owning a power line from a power plant to a city. Lets say the city growths and the initial capacity of the power line of 1kA isn't enough to supply the city anymore, so eventually with 1kA, blackouts will happen.
Is it really wise and 'conservative' to stick with 1kA in that situation - or should the power company rather go and increase the line limit?
@_date: 2015-06-05 18:15:06
Exactly. However, Mike is a little bit more under scrutiny by me because he seemingly wasn't too opposed to 'CoInvalidation' and 'redlisting' ideas and the like.
I don't know his exact stance on this now, and it might have changed, it is just that I'll be listening carefully if there are changes that would make coinjoin impossible, for example.
 
@_date: 2015-06-17 12:49:02
No 1MB is surely not enough for the world's coffee. We all agree on that.
@_date: 2015-06-12 10:03:21
No, the blocks they can create would all follow the rules of the very Bitcoin that you are using now.


Care to point me to the specific thing that I am not understanding?
@_date: 2015-06-14 10:25:43
I think his proposal is a way to compromise with the blocksize hard-capping crowd in the sense that their 'Miners are going to collude and break Bitcoin really quick' is alleviated with everything happening in slow ~3 month intervals. 
I bet that if his proposal gets implemented, hard blocksize limits will essentially turn out to be a complete non-issue and miners - being interested in a healthy ecosystem - will eventually select quite high caps. Because I think Bitcoin can be scaled quite a lot - Satoshi was right there.
But if the miners don't do that and a healthy compromise of Bitcoin with somewhat higher TXN cost but a good 2nd level of payment protocols, 'off chain' stuff emerges and becomes useful, I'd be happy, too.
EDIT: Spelling.
@_date: 2015-06-01 23:34:53


Exactly. I wonder why there is all this evasion. Get a compromise, guys, Gavin is compromising *a lot* already. (Because after all the arguments, I still think *unlimited blocks* will even be fine)
@_date: 2015-06-10 08:04:31
My comment is in respect to a bigger picture in mind: Initial discussions were about lifting the cap completely, then 20MB +50%/y, then 20MB + 40%/y, then 20MB and no yearly increase. Now it is haggling about the constant (well, except that the other side evades the haggling).
That's what I meant with generous. This discussion is going on since *years*.
@_date: 2015-06-29 08:44:30
Agreed. Unless the block-the-stream people move soon - and remember, Gavin came down a lot from an initial completely uncapped blocksize! - there are reasons to assume stalling behavior on their part.
Yesterday, it looked a bit like there might be some movement from Adam, but this could as well be psycho tactics still.
We'll see. Meanwhile, Gavin needs to keep the hardfork on the table, methinks.
has some good insight to the political/psychological part of the debate here.
@_date: 2015-06-16 21:18:46
Point to what the numbers are for Greg to accept a hard fork.
@_date: 2015-06-16 21:16:48




No I think this is actually enough of a reason, in general. In a broad sense.
 


Because I *like* having a functioning global payment network? 
I *need* shelter and food.
@_date: 2015-06-14 19:54:13
Exactly. When including confirmation time in the equation, there was and there will always be a healthy fee market, because any single miner can on average delay confirmation times of TXNs he doesn't like (too low fees).
This was once common knowledge, but has been lost when people started to argue for 1MB being some true fundamental law. It is sickening.
@_date: 2015-06-11 14:40:36
... for some stupid reason, people equate max size == actual block size all the time, and the simplistic economic arguments for fee markets would only work out if blocks would be full all the time already - which they aren't.
I saw your video interview that was linked a couple days ago, where you expressed support for default soft limit == hard limit. Is this something that is set in stone, or could you concede a lower default in XT to ease the pain for the 1MB-blockistas?
@_date: 2015-06-24 18:31:36
Hear, hear!
Funny that Adam, one of those intellectuals you seem to admire, uses a [direct, proportional link between number of users and number of full nodes for his (entirely bogus) O(n ^ 2) scare tactic!](
The arguments of the blocklimiters:
Full users proportionally related to full nodes - &gt; O(n ^ 2) wall (completely ignoring the difference between scaling per network and per node, btw) -&gt; Bitcoin scaling impossible
Full nodes drop with more users -&gt; CENTRALIZATION -&gt; Bitcoin scaling impossible
Which one is it, now?
@_date: 2015-06-02 13:39:47
^ This. 
Gavin proposed reasonable things since years. The naysayers just avoid a *concrete* discussion at all costs.
@_date: 2015-06-12 23:10:12


Please show me the math on that.
@_date: 2015-06-23 06:56:50
It should also be noted that in an open Internet, the country with the bad connectivity still profits from the dispersed full nodes in the country with good connectivity.
So the guy on some pacific island can still use SPV to connect to nodes in the U.S., for example. And he still enjoys the political benefits of 'decentralized' (with whatever definition you want to apply... :D) nodes in the U.S.
@_date: 2015-06-11 14:17:54
I do look forward towards the day that I can replace eBay with it.
@_date: 2015-06-16 22:41:00
I don't expect the friction and contention to be less when the 32MB limit is going to be raised.
Because there will be some interests entrenched around that limit. 3rd party providers (Blockstream?), whatever.
JG's proposal with no hard cap, which would also actually make sense, would take away the possibility of that contention. Why even put a 32MB limit in?
... and that we figured out the fee market is in itself contentious.
EDIT: Note also that 32MB would mean a pretty big ecosystem. It will be *extremely* hard to change anything in core then.
@_date: 2015-06-02 00:07:53
Very relevant, someone from *frickin' 2010* having the exact right worries and eerie foresight on where a blocksize limit will lead us:
Now, can we please get Gavin's 20MB+40%/y proposal through?
@_date: 2015-06-30 09:55:15
Saved for future reference. Sums the insanity up. Or as [said](


old desktop Linux/free software debates.
@_date: 2015-06-01 23:30:25
The issue is mostly bandwidth. Blockchain storage can be pruned already (look it up).
And, in my humble opinion, we should eventually move to things like doing UTXO commitments and hash trees and putting the burden of storing the information that a particular output can be spent as much as possible back onto the user.
That should - in principle - make Bitcoins space usage almost O(1).
Unless the blocksize cripplers become the majority.
@_date: 2015-06-28 09:47:27
The problem is abuse of the brands: bitcoin.org and github.com/bitcoin do not belong into the hands of a central planning committee.
I think it should be changed like this: If a developer is wanting a hard fork, he should get equal spacing as one of the alternatives on bitcoin.org and github. It then shifts the burden back to the users (where it belongs), to build and run a network that can form consensus.
Of course, 'who's a developer?' is another thorny question in itself - but I think we could have a vast supermajority of users agreeing that those with commit to github.com/bitcoin are to be considered developers.
Devs should not have to take this responsibility as a burden, and also should not able to take the power that accompanies it.
@_date: 2015-06-23 06:26:29
... and same here!
@_date: 2015-06-11 10:12:27
True, but there's also a lot of ideas and valid input from people *with* knowledge. 
You always have to ignore the chaff, in any discussion. So arguing for 'less noise' is to some extend pointless. One simply doesn't listen to noise. It is there, it will always be there, tryo to focus on the good bits.
@_date: 2015-06-24 18:09:25
Ah, thanks. Yes, implicitely he did agree to 7MB indeed.
@_date: 2015-06-17 12:28:39


I don't think so. [See what I wrote here.](
Otherwise I 100% agree, Bitcoin is meant to be scaled up. On the layer-0 network. 
And wanting fee pressure to develop is just a sane-sounding way of crazily, centrally planning artificial scarcity. Bitcoin blocks should be as big as the hardware and miners allow. 
@_date: 2015-06-30 19:23:52
And safe RBF would make this safe.
@_date: 2015-06-20 10:04:34
Yes. And arguing from the technical details of Bitcoin to what Bitcoin ought to be is just stupid. That applies to the temporary, long obsolete 1MB spam protection limit.
Bitcoin's social contract overiddes Bitcoin's technical implementation. 21Mio coins is part of the social contract of Bitcoin. So is a scalable Bitcoin.
Else, if technical trumps *ought-to-be*, every bug would need to change Bitcoin's social contract, which would be ridiculous. And you guys should not, ever, touch Bitcoin, as it would be blasphemy, to maybe get the point across to you, akin to rewriting the holy bible. 
I consider the fact that we still have a 1MB limit a bug.
@_date: 2015-06-20 09:40:55


... and I don't think there's any problem with that, except the FUD from the blocksize limiters. As I said elsewhere, since Satoshi's initial look at Bitcoin's scalability, I have yet to see fundamental hard data that shows that Bitcoin can not scale the way he intended.
Let's raise the blocksize.
@_date: 2015-06-18 11:23:37
Yes. Probably. Same with the number of transactions per user - they'd probably saturate when Bitcoin reaches maximum usability.
@_date: 2015-06-05 18:41:14
Why should I acknowledge something that isn't clear at all?
Honestly, I do not know how the Bitcoin world will look like in 30+ years (if it still exists). I don't think anyone *knows*.
However, thinks like [this]( make me optimistic that this can be worked out.
@_date: 2015-06-15 21:53:27
I expect that there is an expectation for the money put into blockstream to generate some returns though.
Although this might not create a conscious conflict of interest for the involved parties, I now tend to think that indeed it might nudge people involved however so slightly to favoring what might make money eventually...
@_date: 2015-06-17 00:24:47


So lets make them lucky and happy :D
@_date: 2015-06-18 00:05:53
Yeah. My point is that this should be clarified (changed from how it is stated now, actually): Because signing off on the BIP will be seen as intent, and if the 32MB stuff remains in there as it is, it will mean another round of hardfork pains and suffering when we reach that limit. With people arguing that '32MB ought to be for philosophical reasons'.
BIP100 is meant to avoid any further hardforks regarding blocksize in the future, and so it should reflect that.
@_date: 2015-06-30 14:28:13
And I think that makes it reasonable to pay for your cup of coffee as a regular at your coffee shop with zero conf.
I think this is the most important point in this debate: Do not dream of unbreakable, game-theoretic totally sound (under a lot of assumptions one might add!) 0-confirmations all the time, about crypto and code, and acquire tunnel vision that prevents you from seeing the larger ecosystem. Trust exists outside the Bitcoin ecosystem, for example, but is part of the equation on how Bitcoin is used.
And in that view, 0confs indeed work well enough. If that fundamentally changes, things like full RBF and so on might have a point. 
@_date: 2015-06-30 23:21:16
Or maybe they are just being lazy or conservative. Remember, those are stress tests, and as a miner, I wouldn't lift my limits just for a short stress test. It would also give the testers a more accurate picture.
But the miners limiting blocksize - the chinese ones even talking abut how they can only support so and so big blocks - that all points towards just removing that damn hard cap altogether.
Because apparently, the miners like restricting block size.
@_date: 2015-06-13 10:03:36
@_date: 2015-06-21 09:32:09
The fast relay network exists now and does essentially the same thing - but maybe not as efficient yet.
I read Gavin's implementation and test code for IBLTs and I cannot see a reason why it won't work.
Please be more specific.
@_date: 2015-06-01 23:26:50
Are you also caveden on bitcointalk?
Is this you: 
If so, what do you think about the current 'debate'?
@_date: 2015-06-07 12:41:14
Where does the [butterfly effect]( fit into that definition of intelligence?
@_date: 2015-06-20 10:06:32
Maybe so! If not, we can still have Bitcoin, even if full nodes have to reside in data centers.
But a 3txn/s cripplecoin joke will be abandoned.
@_date: 2015-06-27 18:02:30


This is what you asked: 


And this is what I answered. And now please stop trolling. Thank you.
@_date: 2015-06-12 11:08:41
Ok, thanks in advance.
On another note, I now weirdly get downvoted in seconds for many of my postings - even if they are replies in older submissions. That didn't happen before. The desparation to stop Gavin must be big now.
@_date: 2015-06-16 21:58:29
His BIP is self-contradictory with the 32MB hard cap in it.
Without it, it would be very supportable, indeed.
But that cap is in because I think you are misled a bit about the willingness of some devs to engage in productive debate. There are some who are indeed stalling now.
@_date: 2015-06-29 18:43:50
And it is ignoring all the ways that Altcoins will have an edge by being able to process more transactions.
All this game theory talk from ptodd and the like usually correlates with pretty narrow tunnel vision.
@_date: 2015-06-12 11:31:26
And seems to be one of the few ones still having the whole, bigger picture in mind, it seems.
@_date: 2015-06-27 21:05:31
Yes we agree on the issue itself. We do not agree on the presentation.
Look at the other post. There is no reason to call anything an O(n ^ 2) wall. 
If you are indeed scared about *saturating the internet*, you are scared about Bitcoin's success. Economic factors will kick in to stop blocksize growth way before that.
@_date: 2015-06-09 19:51:25
I think this is one of the things that Satoshi had a very good grasp of these kinds of 'economic meta dynamics' and the way that people behave. I bet that's also why he's keeping quiet here, as he's probably trusting the incentive system to be right.
I think that on a meta level, we should continue as we do already: The medium term planning is pragmatic (i.e. *some* agreeable blocksize increase in this context), and the long term planning is open-minded, with trying to keep the ecosystem open for innovation, so that sweet things like Lightning Networks will come along and into being eventually.
In the end, one should also notice that apart from trying to make a point here on reddit or bitcointalk, or maybe writing a patch or an email to the devs, or, at most, becoming one oneself, not much can be done by any individual. Bitcoin in that sense is quite the independent beast now.
@_date: 2015-06-12 13:54:18


As pointed out, Bitcoin scales (roughly) with O(n), with n blocksize *per node*. The O(n ^ 2) is only relevant when looking at the *whole network*, but the whole network doesn't decide on whether to run a full node or not, to decentralize or not, a single node does.
@_date: 2015-06-16 10:26:56
These are reasons why I think a page for each side can put references in, call out the lies from the other side etc. We are already having a deep schism, and now it would be best to expose everything. 
I think such a 'battle' approach could work best.
@_date: 2015-06-12 11:22:44
Just wait and see.
@_date: 2015-06-13 11:59:10
Sounds plausible. One thing that should be noted though is that Gavin is far from the only one who's seeing a problem with 1MB and a stalled discussion. This eventually being an issue was stated back in 2010 by many.
With regards to 1MB being a problem or not: The problem is rather whether a consensus solution can be found in time, and I think Gavin's behavior is very conducive to that - pulling the contention to NOW, instead of when 1MB blocks are actually limiting the system. 
@_date: 2015-06-30 14:03:58
Initial goal is scaling to worldwise usage. Satoshi's vision.
How about *you* stop moving the goal posts?
@_date: 2015-06-16 10:14:30
Some say 'educating', some say 'social engineering'...
@_date: 2015-06-14 20:01:03
Lead dev, lead scientist - whomever you want to elect yourself, for yourself :-)
Power games here obviously only work insofar as you can try to make the other side look bad and thus losing influence. Gavin failing at his hard fork or blocksize increase would make Gavin look bad. 
Greg trying and failing at making Gavin look bad would make Greg look bad.
@_date: 2015-06-11 20:37:15


The problem is that this ideal/goal isn't alone and has to be weighted against other goals.
@_date: 2015-06-17 10:07:38
Gavin's going to make a BIP anyways, so this point is moot in respect to the blocksize debate.
In any other case, what justusranvier brought up are still reasons to seek for an improvement.
@_date: 2015-06-28 14:37:06
     First of all, no one should start framing the discussion as if a blocksize limited Bitcoin is the ought to be.


Fully agreed, also with your list of issues to work on. This is not the point of the discussion though. The point is whether to keep an artificial 1MB limit in place and/or how and when to lift it.
Naturally, and of course, the block size will never exceed physical limits. I am, like is, arguing to let the market decide on what the right block size is.
Also consider that if a fraction of the faster, more well-equipped network creates blocks so big that a significant fraction of the nodes on the edge are not able to keep up, there *will* be back pressure to reduce block size, because Miners will see that it decreases functionality and thus value of the network.
Having some additional safe guards against this might be prudent to some extend. In this regard, a dynamic limit might, for example, help in preventing a hacked rogue miner from producing a couple of big bloat blocks.
Those safeguards should not hamper the functionality of the network, though. I think it is very clear that a centrally planned, static limit is not going to cut it as a safe guard and is rather very close to hampering now - and will be soon.
@_date: 2015-06-11 09:50:54
At some point, it isn't about rising fees anymore - when the elastic part of the transaction demand has been squeezed out (such as SatoshiDice-like usage, which as far as I can see has already pretty much stopped), there ***simply are no more transaction slots available, regardless of what the demand is***.
If you travel by train and suddenly everyone wants to get to some destination (lets say a soccer match here in Europe), the railway company can supply extra cars and thus address the extra demand, make some good money but prices will probably still be reasonable - *and everyone who wants to gets to see the soccer match*.
The analogy for Bitcoin would be: No extra cars available, ever. Centrally planned 1MB cap on cars, and if you want to transact during a peak, well bad luck for you.
You'll hit a wall. TXN prices will spike, people will want to get out, TXN price will spike even more, and Bitcoin is quite possibly destroyed after that.
EDIT: Fixed irregardless -&gt; regardless.
@_date: 2015-06-09 09:39:30
Bullshit! Gavin repeatedly changed his proposal to try to please the naysayers that like to have Bitcoin hit the blocksize wall. He never, ever went 'my way or the highway'. Not even now, he's basically saying, 'ok, I am going to fork and let the market decide'. Nothing totalitarian about this at all.
Gavin worked on this and argued for an increase since *years*. To have *reliable planning* of the hardfork, well in advance.
Without any *constructive counter proposal* by Greg and the others.
@_date: 2015-06-01 22:45:02
Yes, but he always evaded any input to a concrete plan of action with regards to block size, even though a hard fork needs planning beforehand.
Saying that 'a hardfork can be done in time' can just as well be turned around: 'a softfork, going back to a smaller block size, can be done in time, ***even easier***'. Because *decreasing* would just be a *softfork*.
@_date: 2015-06-12 11:12:26
Sometimes, the simple explanation is right, notwithstanding quoting anything sounding intelligent or wise.
@_date: 2015-06-28 10:29:17
In depth? I think it is rather trolling tactics and psyops.
I made a study awhile ago on sock puppets - didn't see anything back then. Maybe I should repeat this - I think even talking about sock puppets gave some people an idea now...?
@_date: 2015-06-15 21:57:42
I hope we can get eventual consensus (after getting consensus on blocksize) to implement some kind of UTXO set validation, so that one can run a full node without validating the full chain back to the beginning.
@_date: 2015-06-12 17:04:03
No one is forcing you to do anything, so that state is exactly as you want - right now :-)
Note that voluntary association and rule by force a very different things. Preventing voluntary association is a form of rule by force, by the way.
@_date: 2015-06-30 22:54:29
I don't really get why you want to forcefully cap blocksize more than a safe limit, though. Gavin's proposal would do that, so would Jeff's.
If lightning is turning out to be awesome, people will use it. If Blockstream will be in the business of profiting from a hub network on top of Bitcoin, so be it.
But why constrain Bitcoin to force this?
@_date: 2015-06-14 19:18:50
... and one might eventually pay a fraction to transact through a full node... if it would ever get to the point that only really high end data center ones are able to process the chain.
@_date: 2015-06-12 12:59:11


And I actually have to revise this: It isn't O(n^2 ), rather, with bootstrapping a node like it is now, there is another factor that goes with the number of transactions since the beginning, but it definitely isn't O(n ^2) like you say, either.
This can be avoided with working ZK proofs or UTXO commitments.
And is completely right: A running full node is just in O(n), with n the number of transactions. Maybe multiply a small log n for database lookups, which doesn't really change the picture.
@_date: 2015-06-16 20:51:01
And how do you know this is decentralized enough?
With a 10kB blockchain, you could put Bitcoin into a typical smartphone app one the side, ensuring that it is *everywhere* and thus a lot more decentralized.
@_date: 2015-06-01 23:37:22
Consider that Gavin was trying to evoke exactly this since *years*. But all the reaction was 'meh, no, well I actually like 1MB forever, well this, well that, ...'.
Gavin is also talking about the big picture, whereas arguably, many of the other devs are stuck in a small world. Many of their concerns are very valid, but in the end it is a trade-off - which I think Gavin actually has right with 20MB+40%/year.
@_date: 2015-06-25 09:15:12
LOL. Nicely said. :D
@_date: 2015-06-29 00:12:45
All that fits Gavin's proposal.
@_date: 2015-06-11 14:42:01
Unless resistance in bitcoin-core falls (mostly) apart, a thing that could (and I hope will) still happen when Gavin goes public with the actual hardfork code. 
@_date: 2015-06-16 20:20:16
There is no magic constant that is safe 'from the point of view of decentralisation in a flooding based consensus network.'.
And that, by the way, is a very ill-defined specification. What exactly is decentralization? What is a flooding based consensus network? Bitcoin, you say? Ok, Bitcoin is the biggest and the first - and an *experiment*. So we are gathering data as we go along.
@_date: 2015-06-16 10:59:00
Yeah. It makes no sense, though. It basically rips the very core idea out of proposal.
@_date: 2015-06-12 14:06:16


Are you fucking kidding?
Code complexity is actually the *worst*. A change from 1MB to 20MB is changing a constant - plus very few  lines of code for triggering.
@_date: 2015-06-11 20:34:57
Exactly. And Satoshi intended bigger blocks to not be capped at 1MB eventually. Gavin wants to implement those, and which software will be run will decide who's fork is going to win.
@_date: 2015-06-15 22:43:01
You are IMO clinging to an illusion.
The so-called consensus is a product of the economics of Bitcoin and not the other way around. As you know, anyone can do anything on the internet, fork the software, tweak it, run it, run whole networks of a tweaked client (a fork), etc...
We have a mostly stable and coherent/consistent/'consensual' progression in the protocol *because* there is a strong economic incentive to keep it that way.
The 20MB fork is also driven by economic forces. It is the need for larger blocks causing a schisms in the ecosystem. 
Certainly, devs have a say (due to their trust, which is economic value) in the direction of Bitcoin.
But to see that the fork is even happening should tell you how high the pressure is.
It is futile, no, rather destructive to oppose it.
@_date: 2015-06-13 13:53:38
Whether Gavin is pro shorter block times or not (I am actually opposing him there and think 10min is mostly fine) doesn't make the blocksize issue and his input on that any less valid.
Block time changes are a whole other can of worms.
Don't throw two mostly separate issues together. Gavin is able to separate them, and so should you.
@_date: 2015-06-09 17:43:20
One incentive is reachability of a full node. 
I bet if it ever would get so bad (which I doubt it will) that there only very few nodes, overloaded with SPV-wallet connections and the like, there *will* be nodes emerging that you can pay for access to the blockchain. Because it is essentially only public data, that market will also be competitive.
@_date: 2015-06-29 22:37:17
Exactly. See also what I wrote [here]( about the 'gun to head' hyperbole from makuu. This all starts to smell very fishy from block-the-stream now.
@_date: 2015-06-09 18:13:03
Nice way and interesting way of putting this. I am probably more in spouse A's camp after reading all the issues and checking how much they apply - I am not even worried anymore that a no-cap-at-all scenerio would be harmful. Satoshi spent time on thinking through the scaling, and I still think his few assumptions are correct.
That said, I have nothing against trying to keep stuff off-chain - if it is due to better functionality, such as instantaneous confirmation etc. But I do not want transactions be forced off chain, due to some attempt at central economic planning and especially pointless ego-centered obstructionism.
@_date: 2015-06-25 08:10:23


Not true. [Adam from Blockstream is seemingly worried about saturating the internet.](
I find that notion ridiculous, to be honest.
@_date: 2015-06-12 13:16:52
With the fork that implements the change that triggers on the majority!
I have not seen otherwise, but if you can enlighten me, go a head.
@_date: 2015-06-12 15:13:09




This is the same as doing a hard fork to 2MB, right? Basically the same as proposed, but 1 to 2 instead of 1 to 20?


upgrade with a 12,000 block (3 month) threshold. 
 
Can you explain this more? How would further increases happen.
@_date: 2015-06-14 19:43:12
I wondered whether he revised his position - he now arrives at O(n), too, but writes it so that it sound likes Bitcoin is not scalable. See my reply [here.](
Having a visual, intuitive understanding of the network would be my expectation of such luminaries like It does make me suspicious that he's just trying to argue 'is not scalable' for the heck of it.
@_date: 2015-06-14 09:45:10
Note the comments from maaku and Gavin in that thread. 'Nuff said.
@_date: 2015-06-19 14:25:54
Exactly what was saying: 1MB was meant as a temporary measure, and the vision of Satoshi is clearly that Bitcoin itself, without layers on top, can scale up to Gigatransactions/day.
And if the economics play out that it won't (but could) but layers on top make it better to have a medium size blockchain, so be it.
But do *not* constrain the blocksize because you are fearing Bitcoin's success.
@_date: 2015-06-02 13:26:00
The ongoing deadlock is proof that we aren't, unfortunately.
@_date: 2015-06-27 12:00:01


.. and do this, they think that system should be limited in transaction rate. Fine. Create an Altcoin for that. No one is stopping you. But do not cripple Bitcoin.
@_date: 2015-06-28 12:07:29
I let my SPV client wait for 6 blocks. How are you going to make those?
@_date: 2015-06-27 13:23:01


No. The network protocol currently cannot deliver more than 32MB messages. And again, you are elevating a technical issue with an *ought-to-be*.
It is quite funny actually: So there are two hard caps in now, one at 1MB and one at 32MB, just to be sure? Fucking ridiculous.
There is not intended long term hard cap in Bitcoin.


Oh yes! I'll go along with the hard fork it if the 1MB camp doesn't move, and I'll still call it ... drumroll... ***Bitcoin*** :-)
@_date: 2015-06-20 09:58:31
Conceivable, in principle, yes. 
But consider that the meta protocol, that is the social contract that people who own Bitcoin agree to, is to keep the 21mio Limit in place. And ***very*** strongly so. No one wants to dilute their own money.
The social contract also contains Satoshi's initial scaling vision of Bitcoin being able to handle lots of transactions on-chain - which is still very possible despite all the FUD.
And when the hard fork to lift the 1MB limit happens, it is because the economic forces due to people believing in the *social contract* of Bitcoin are stronger than the *technical implementation* of the current Bitcoin protocol implementation. Where some people now play stewards of a centrally planned 1MB blocksize and want 1MB cripplecoin.
The social contract overriding the technical implementation is a *good* thing. Because else, you could argue that any *bug* in the technical implementation could change the overall social contract of Bitcoin. And that would be ridiculous.
The sane thing is to consider the 1MB limit a bug (I think it is actually pretty close to that, as also said, Satoshi could have simply spend 10min more to time-limit it) to be fixed.
Insanity is to declare the bug a feature and try to change the social contract of Bitcoin to make 1MB part of Bitcoin's constitution.
@_date: 2015-06-16 13:23:38
Sabotaging change and hiding behind 'we need consensus for change' is action in itself. It just isn't very visible action. put it nicely [here]( and [here](
It is easy for Greg &amp; Adam to hide behind 'we just want consensus', while at the same time blocking all productive discussion. Gavin's repeated unsuccessful attempts at having them interact with him should be proof enough to see what is going on. Either egos or anti-Bitcoin incentives must be involved here, I fail to see any other explanation. 
@_date: 2015-06-29 16:06:32
I have read it. So what?
@_date: 2015-06-03 20:07:34
If it is easy to do a hard fork to increase blocksize in an emergency situation, it is even easier to do a soft fork to cap blocksize again in an emergency situation.
So arguably, arguing and planning for the hard fork now - just like Gavin did since *years*, is the safer option.
Unless you actually believe 1MB will be enough forever (or little enough to kill Bitcoin).
@_date: 2015-06-09 19:30:42
That's an interesting perspective. If I understand you right, what could happen is that basically the bank invented 'highly flexible, digitally tradeable euros' (backed by the bank) and thus they might sell for a premium?
If that is the case, it would mean that there is a market for converting euros into colored coins, which could be a very weird fiat leak :D
@_date: 2015-06-17 14:18:40


If that doesn't distract you from your other work (thank you btw.), that would be great.
Gavin's set of blog posts addressing block size worries is great, but I think he's giving mostly a clear high level picture (which should actually be enough, but I digress)
I think also has a set of nice and insightful posts on how full nodes etc. can actually be paid with BTC in micropayments or similar. I think his arguments are important in the debate because he is able to point out how acting on decentralization worries would amount to being anti-free-market if all resources can be properly paid and allocated. And he's showing a path on how this can be achieved.
There is probably lots of other stuff, insightful posts here on reddit, bitcointalk, ML, etc. It would be nice if someone had a comprehensive list of all the debunked blocksize worries.
@_date: 2015-06-17 19:30:46
[Coincidentally, this is also what Gavin is proposing as the next block size.](
Lets do the XT fork. 
@_date: 2015-06-20 12:26:08
We should be able to separate Adam's valuable contribution in the form of the idea of hashcash to his otherwise IMO not very constructive behavior (to say it lightly) in the blocksize debate...
@_date: 2015-06-19 09:12:39


Is it, really? It's an immediately replicated, distributed database. From a 'keep transactions that happened safe' POV, I am pretty sure this is close to what Banks and the CC processors do. Just with a network that they fully own themselves (centralized control)
@_date: 2015-06-13 19:19:58


The problem with full nodes in terms of power is this, I think:
Who decides how much power a full node has, and how do you prove that a node is a full node?
@_date: 2015-06-07 09:16:03
With Gavin's growth proposal, Bitcoin does scale. The question is whether the other devs - and the people who like to see Bitcoin fail - are successful in keeping the 1MB cap on.
Lets see what happens when BitcoinXT contains the patch for larger blocks and how many nodes will switch over.
I expect the vast majority of nodes to switch.
@_date: 2015-06-24 08:05:44
Thank you! 
 Yeah, I know. I could go and filter it some more, but then it is less data...  or I could go and do some time window in the past, or add this, add that, or ...
But then I figured, as I wrote in the introduction, that I didn't expect for sock puppets to change their behavior on short notice just from announcing a bounty for analysis - without any details on how it could be done.
So, either way, I think any sock - puppet - style manipulation is at most small scale. 
Let's see when announces his bounty distribution/conclusions.
@_date: 2015-06-28 10:12:19
I could copy and paste, but I think it is better to place a [link...](
@_date: 2015-06-28 20:35:10
Satoshi showed that Bitcoin can scale. He didn't see it scaling with a RasPi under everyone's desk, but full nodes in data centers.
There is no new data on this that shows Satoshi was wrong, just FUD.
And I sincerely doubt the Block-the-stream-team successfully socially engineered the whole Bitcoin world into their direction yet.
@_date: 2015-06-25 12:45:25
Surely the UTXO set is (mostly) someone else's data?
@_date: 2015-06-12 14:35:02
If they are central entities that you must transact through, it will be much easier to blacklist. 
If I directly send you a transaction on the blockchain, and all the transaction is N coins from [address abc] to [address xyz] (essentially), it is much harder to block.
@_date: 2015-06-15 21:51:43


Is it really? All transactions do run by every node, and it is certainly less efficient than some sharded key value store, but that whole O(n ^2) scaling ghost has hopefully been killed by for good. And financial technology is often somewhat inefficient and redundant on purpose...
@_date: 2015-06-28 15:21:08
Good point! What's your opinion here, @_date: 2015-06-11 17:49:37
Set the soft limit to *zero* to encourage more communication between the users and miners in Bitcoin. /s
@_date: 2015-06-09 18:34:23




I know damn well what I am talking about:
UTXO means ***unspent*** transaction output. 
@_date: 2015-06-09 17:50:54


Since *years*, one might add. And, simply put, he wants to keep Bitcoin true to its original vision, that was also proposed by Satoshi.
A Bitcoin that is able to scale to very high transaction rates!
@_date: 2015-06-08 17:51:23
And noone can prevent a hardfork, either...
@_date: 2015-06-24 17:49:01
And they have (almost) successfully engineered the debate to make 1MB the *ought-to-be*. Scary.
@_date: 2015-06-02 13:35:59
This is the ***maximum***, by the way...
@_date: 2015-06-16 21:26:41


Well, I don't think we even have an agreement about what 'decentralization' really means.
@_date: 2015-06-17 00:56:52
I am not saying that &gt;32MB can be done right now or that the BIP should already contain a plan on how to change the code to support &gt;32MB.
My point is that the proposal on a *political* level should clearly state that any further hard cap limits are to be lifted eventually.
Because the proposal sounds right now as if 32MB is *intended* to be another limit. And that is contradicting the goal of a market-based solution.
@_date: 2015-06-26 14:39:18
And he did...


would be about 400 bytes (ECC is nicely compact). Each transaction has to be
broadcast twice, so lets say 1KB per transaction. Visa processed 37 billion
transactions in FY2008, or an average of 100 million transactions per day.
That many transactions would take 100GB of bandwidth, or the size of 12 DVD or
2 HD quality movies, or about $18 worth of bandwidth at current prices.


sending 2 HD movies over the Internet would probably not seem like a big deal. 
@_date: 2015-06-12 13:35:39
Is 60% consensus? 70%? 90%? 99.5%?
The truth is: There is never *true consensus* and there has never been. That's why I wrote 'consensus', in quotes.
I can go and just out of spite say 'I agree with blocksize reducers and go to 100kB Bitcoin' and create my own little fork.
Boom, no consensus anymore.
But in the end, 51% of hashpower is what really matters as the arbiter on which fork is going to be right.
@_date: 2015-06-25 08:25:02
Bitcoin's byzantine fault tolerance would always give you the consistent long term view. The set of different implementations would give you a fault tolerant voting system to ascertain which implementation is buggy.
I am arguing that you could combine the two to reduce the influence of bugs that creep in when going from spec to implementation.
With regards to losing money: Consider the accidental BDB hard fork. There was an eventually wrong chain that you could have transacted on persisting for considerably longer than the usual expected orphans. The chance of this occuring could be reduced somewhat with fault tolerant voting schemes.
@_date: 2015-06-26 14:46:16
I wonder why the automatic build failed (bottom of the pull request). Surely Gavin must have submitted something that works, or at least compiles cleanly?
Is there something else that got merged in before he made the PR?
If so, is there politics involved in switching HEAD around on github.com/bitcoin?
Though I guess I am probably too conspiracy-minded now...
@_date: 2015-06-27 17:29:15
@_date: 2015-06-09 09:24:27


I understoond the argument for different implementations insofar as they'd all follow *one* common protocol *specification*.
@_date: 2015-06-22 16:58:02
If people want centralization, why are you to decide that they can't have it?
Some argue Bitcoin with centralization is worthless - if so, Miners will have an incentive *not* to make it worthless....!
@_date: 2015-06-16 22:17:18
He's probably weighting his time and efforts - and I think he has shown here on reddit, on the bitcoin-dev mailing list and elsewhere that some people are simply too stubborn to be reasoned with.
But I agree, in an even better world, he would have had the time to go through the formal channels to show the silliness once more.
EDIT: Regarding your edit: No I don't think so. One of the deflection tactics with regards to the blocksize has been that 'follow the BIP' one. The other side is conflating.
@_date: 2015-06-02 21:13:33
It was also only 'just' a bug fix, basically. LN is a whole infrastructure.
@_date: 2015-06-16 20:23:06
BS. Why do you think things like physical gold delivery exists?
And don't fool yourself to think that Bitcoin with gold's properties (being very hard to move) would be worth *anything at all*. 
@_date: 2015-06-25 08:38:55
Your view is code centered. In general, the spec drives the code, and not the other way around. (Of course, there is some back flow if something is deemed too difficult to change in code or similar)
@_date: 2015-06-11 21:45:15
Sure. But how does that make Gavin's 20MB increase different from Satoshi's initial vision?
The status quo is effective unlimited blocksize, not 1MB hardcap.
@_date: 2015-06-18 10:14:56


How about solving this problem once and for all?
@_date: 2015-06-12 10:00:12
No? They have all the hash power, so they can make it valid?
Again: They can start at zero. Genesis block.
@_date: 2015-06-09 13:18:35




Exactly. There were *lots* of problems and bitching. And it was only a *soft*limit.
This alone is enough of a case to raise the possible softlimit blocksize with a hardfork scheduled now. If things do not work out, a softfork (which is easier) can always be done to constrain the blocksize cap again.
I really fail to see how one can argue 'sure, we do the hardfork in an emergency, there will be no contention', when now, where the hardfork can still be *well planned*, there is already lots of contention. The supposed alternative is to wait until blocks are full, nothing really works anymore and suddenly, * poof * there is consensus on the exact blocksize schedule?!?! I am sorry, but that mindset is indeed insane.
@_date: 2015-06-17 00:44:44


That combination of inputs happens independently of whether you add some tiny dust or not, or doesn't it?
@_date: 2015-06-13 14:30:28
But there is still a lot of 'centralization'.
Look at the topology of your ISP's network: It is to large extend a hierarchy. Yet you (mostly) have *options* in choosing your ISP.
That kind of centralization is just unavoidable. 
has a fair point.
I just think that the 1MB-limiters might be able to kill the (inertia in the) Bitcoin ecosystem for good, if their opinions prevail for too long.
@_date: 2015-06-28 20:43:56
I pointed out Gavin, the other side as a reaction to this BS.
@_date: 2015-06-27 17:53:56


Very fucking true. I even think assigning the devs some kind of responsibility for *Bitcoin the money system* instead of *Bitcoin the code base* is wrong. They are at most stewards of Bitcoin the money system.
And consider that Sathoshi wrote the vast majority of rules that are still in effect and governing Bitcoin.
That is not to say I dislike what the devs do with Bitcoin. Mostly I like it. I do not like the attempt of a fraction of them of socially engineering a new course for Bitcoin, away from Satoshi's clearly intended goal of large scale full nodes.
@_date: 2015-06-25 08:25:43
See my [reply to BitFast](
@_date: 2015-06-02 17:25:07
Consider the worst case, but not assume it everywhere - else we wouldn't have the internet, for example.
@_date: 2015-06-09 09:25:38
Also, it isn't exactly consensus or decentralized if the other core devs block what a majority of Bitcoiners out there want - or is it?
@_date: 2015-06-30 21:09:16
I see [a reply]( from pointing out again how ill-defined decentralization is. 
He's answered by someone posting [a link to a research paper]( that tried to define those terms somewhat in a different context. Skimming it, that paper has an interesting quote:
Centralization is now a word constantly repeated but is one that, generally speaking, 
 no one tries to define accurately. 
- Alexis de Tocqueville
LOL. People debated blocksize some 100+ years ago? :D
@_date: 2015-06-01 23:11:05
***Very relevant***, I have found this link somewhere in the comments today:
caveden had some very accurate foresight! Is he a user on reddit? @_date: 2015-06-03 19:32:54


And that phrase is invalid in a system with expected dynamic growth like Bitcoin and a *hard limit* in place.
Also: Soft fork is easier than hard fork. So plan the hard fork now, well in advance, and do the softfork if an emergency arises.
@_date: 2015-06-16 21:41:23
And, who does the ranking...?
@_date: 2015-06-19 10:00:14
Also somewhat relevant: 
@_date: 2015-06-30 20:29:36
I think this is 
- way too low
- not really definite yet
- and not open-ended like (w/o a 32MB cap) or at least eventually high rate like But maybe it is a start to sanity on this matter!
@_date: 2015-06-02 00:15:24
The original whitepaper, *of course*.
There's even a web conversion of it now:
The code to prune is a lot more recent and by Peter Wuille, I think.
@_date: 2015-06-16 11:59:49
Lol, yeah :D I am still in nerd-mode with Bitcoin mostly, though. Seeing it as a very neat toy. So I continue using it and will do so with XT.
But this could get interesting: When/if XT manages to get a clear majority, there could be rush of transactions on the network because people suddenly feel this issue has been solved.
@_date: 2015-06-11 14:05:02
... and as Gavin pointed out, there *is a working fee market already*. 
@_date: 2015-06-01 23:06:46
See this from Gavin:
@_date: 2015-06-19 22:33:55
Can you please point out the magic new information that you or the Blockstream people have that invalidate what Satoshi said?
I *really* fail to see what dynamics Satoshi didn't see with regards to the blocksize.
Really. He did have quite a hard look, and he addressed the scalability question quite early.
Quote Satoshi, ***2008***:


for users to use Simplified Payment Verification (section 8) to check for 
double spending, which only requires having the chain of block headers, or 
about 12KB per day.  Only people trying to create new coins would need to run 
network nodes.  At first, most users would run network nodes, but as the 
network grows beyond a certain point, it would be left more and more to 
specialists with server farms of specialized hardware.  A server farm would 
only need to have one node on the network and the rest of the LAN connects with 
that one node.


would be about 400 bytes (ECC is nicely compact).  Each transaction has to be 
broadcast twice, so lets say 1KB per transaction.  Visa processed 37 billion 
transactions in FY2008, or an average of 100 million transactions per day.  
That many transactions would take 100GB of bandwidth, or the size of 12 DVD or 
2 HD quality movies, or about $18 worth of bandwidth at current prices.


sending 2 HD movies over the Internet would probably not seem like a big deal. 
@_date: 2015-06-17 14:57:20
Ok, I can agree that somehow mentioning the 32MB explicitly is sane.
But consider that agreeing to the BIP will be considered a statement of intent. So if there is language in there that suggests 32MB *ought to be*, there will be new contention forming.
This can be avoided by clearly stating: 32MB is in there currently, due to historical network code reasons, to be replaced with no further limit at some point.
@_date: 2015-06-12 11:54:20


RasPIs are very low-level hardware. A good point that 20MB is not causing massive centralization (in whatever metric).


Well, but do you need to in that widely successful scenario? If there is one full node in your city (lets say) - isn't that enough in that case?
@_date: 2015-06-30 10:04:41
Yet we see lots of people point to much better known pools. Your game theory is not as easy as you like it to be. Explain: Why is zero conf working so well so far?
@_date: 2015-06-16 13:06:35
Let's hope so. I tend to agree with you now, Greg and Adam seem to be employing stalling tactics. [ Look at where I pointed Adam's O(n ^ 2) scare tactics out.]( 
Very hard to explain from such a bright fellow without involving misaligned incentives or ego BS.
@_date: 2015-06-27 11:32:24


Emphasis mine. 'Nuff said.
@_date: 2015-06-15 23:34:15
No. It is hyperbole. Arguing that 20MB is 'centralization and destroy all the value bitcoin had' as if it is a sure thing is hyperbole.  Neither me, you nor have a magic crystal ball. 
He's saying outright that this is going to be the result of 20MB blocks.
@_date: 2015-06-28 23:09:02
So... If I remember correctly, the calculation in the presentation was something like 133MB blocks for 7e9 people creating a payment channel twice a year.
Considering that one cannot really be sure about the reliability of such channels, isn't locking in at least part of your Bitcoin value and trusting some centralized hub for half a year a bit much to ask from users?
With 8GB blocks, you'd give people the opportunity to change/ switch/open lightning channels about twice a week, correct?
I think that would be a much more reasonable figure for allowing on-chain transactions.
That would be Gavin's proposal then, given that his proposal is a slow increase over 20 years, shouldn't you actually support it then? 
Gut feeling then seems to make number sound about right, not only from staying below projected bandwidth growth, but also what is to be expected from Bitcoin with a successful LN on top. What do you think?
EDIT: And I realized that there is something *beautiful* to Gavin's proposal: It starts with a Chinese lucky number (8MB) and it ends with a Chinese lucky number (8GB).
And it should also be noted that it will eventually give about one byte transaction space per block per person on planet Earth. In about the time for a child to become an adult (20 years) :-)
@_date: 2015-06-17 01:11:08
- The specifics on how users will vote are not specified, though. This will lead to the same contention that we have now. 'It is historic, it is intended!!1!'
- If you believe that a market based solution is right, which seems to be the gist of most of your document, there is no need for a 32MB cap. It is self contradictory. Making an explicit 32MB cap is again central planning. 
- Arguably, the sane thing to do about the 32MB limit would to cleary state that it is there but meant to be subject to the same process that you laid out in your BIP100 anyways.
- As you say yourself, the check and balance in your proposed system is the *time* and slowness of the process of block size cap changes. 32MB would only be reached after a full year of (basically) all miners consistently voting for maximum block size increases. So again, by putting in an explicit 32MB, you are essentially not believing your own proposal.
@_date: 2015-06-17 15:16:42
Yes, this might actually be a very good idea! I'll do it if I find the time.
BIP100 is not yet on github, though.
@_date: 2015-06-15 22:29:57
Thanks for doing this!
I asked you [here]( about some things I didn't understand. What do you think about them?
@_date: 2015-06-27 14:01:21


orders of magnitude less than the number of full nodes,
My point is that Bitcoin is much bigger than electrum, and so the number of
full nodes is at a completely different scale. You compared 6000 *Bitcoin* full nodes
with just 30 nodes from *Electrum*. Apples and oranges.


easier to control. Therefore any plan for block size increases should err on
the side of encouraging people to run full nodes rather than SPV servers.
No that is too short sighted. Even if you are constantly worried about
government shutdown of Bitcoin, you completely exclude decentralization due to
a widespread user base from your picture. You also exclude having different
jurisdictions in the picture. Just as Nick Szabo recently tweeted...


doubling time of 2 years would push people firmly into the SPV camp whereas
a more relaxed increase (doubling every 3 or 4 years) would IMO be
sufficient to keep people running full nodes.
And I think Gavin's plan is already compromsing *a lot* on a scalable
@_date: 2015-06-28 21:20:04
Gavin, I don't remember how bitcoin.org, github.com/bitcoin etc. came into being.
But it should be clear that in the name of honesty, if there is unresolvable contention, all different hard forks of a core developer that are sufficiently popular(*) should get equal representation on those sites.
There should be Bitcoin/XT, Bitcoin/BS, maybe Bitcoin/JG (Jeff Garzik) for download, with a clear warning and instruction that it is up to the users to decide and work out consensus for building a working Bitcoin network.
This would remove to a large extend the burden from all developers to centrally steer decisions on what Bitcoin is correct - and it would also greatly reduce the possibility of an abuse of developer power.
bitcoin.org, github.com/bitcoin make up quite a part of the brand name of Bitcoin, and maybe we could at least all agree that it would be unfair to give them to either side alone.
(*) - Hard to define, I know. But I think a vast supermajority of users can agree though that XT and 1MB are the popular two choices.
@_date: 2015-06-06 08:42:11
I wonder whether someone might pay these people to try to cripple Bitcoin with the 1MB limit...
@_date: 2015-06-28 11:40:52


What exactly is a wall here? If you believe users ~ full nodes, and thus O(n ^ 2) in the whole network, how is that a wall?


What do you consider as an economic dependent node in this regard?


That might be true, but not necessarily so. I could see a certain fraction of free transactions per full node, but if you want reliable service, you open a micropayment channel / pot on the blockchain.
And to clarify: I am not opposed to layers on top of Bitcoin at all, just forcing users by centrally planning a (1MB) limit.


Is it due to perverse incentives that some apparently try to stray from '51% of hashpower decides what is valid Bitcoin'?
@_date: 2015-06-29 20:57:03
Another way would be to use the hash of your text as an input to the key generator on brainwallet.org and transfer a tiny bit of money to it.
Not a clean method, but workable.
I don't know whether you would count brainwallet.org as a 3rd-party online tool, though. All the code is running in the browser.
@_date: 2015-06-16 10:16:55
IMO very [relevant observation]( by on this.
@_date: 2015-06-27 11:56:53
But Bitcoin is *the cryptocurrency*. 
@_date: 2015-06-29 09:03:55
I think the seemingly never-ending patience is running out a bit, on the Gavin side of the debate. So we might soon see the hard fork.
We had discussions like this one before here on reddit. It was about the same picture: Adam, Greg painting rosy pictures of non-existing but supposedly awesome widely scaling, non-3rd-party LN networks to try to point away from the fact that the whole discussion is about lifting a crippling 1MB limit and a very reasonable, well-researched proposal from Gavin. And that a Bitcoin without direct access to layer 0 at *least every so often* (more than twice a year!) isn't Bitcoin anymore.
Adams behavior in this thread made me hopeful that maybe the BS guys shifted a bit in their position, but there is a limit to being hopeful and patient about a side that has only been stalling so far.
This could be seen as another psycho tactic from BS: Stall, stall, stall, evoke some hopes, and stall, stall, stall again, and repeat this to eventually wear down the other side.
It fits the picture well. I wonder what thinks about this.
@_date: 2015-06-28 09:38:46






Your n^2 wall doesn't become true just because you repeat it.
    There is a difference between saying and doing. All the actions from you and Greg so far have been an effective stalling of the blocksize debate.


Yes, and thank you for that! You are missing the point though: I am talking about the *hard cap blocksize debate*. And there, all actions of you guys so far have amounted to stalling. The problem is that no one can prove a negative. But the data starts to be overwhelmingly point into one direction.


He's talking about how full nodes could be paid. VERY relevant in this discussion. People dislike central planning of economic variables in the Bitcoin system, and you seem to agree on this - so he has some very interesting things to say how a market for full nodes could be established.


Understood. But Bitcoin the code != Bitcoin the network, the ecosystem. The problem of a blocksize limit is much wider than the code, is political, as it might dawn on you. And in this context, again, has some interesting things to say.
And as I said in my other post, if you are seriously worried about too much responsibility - take yourself out of the equation on which client to run. 
@_date: 2015-06-02 13:22:37
I more and more get the impression that there is a lot of social engineering going on now by trying to cripple Bitcoin with a 1MB blocksize limit, such as:
- Calling it 'Gavincoin'. 
- Saying that 'hard fork is easy last minute (but for some stupid reason softfork isn't?!)'
- not giving *constructive input* to Gavin who is working on this since *years*.
was exactly right and spot on in 2010.
***Gavin's original growth formula (TM)*** and be done.
@_date: 2015-06-28 21:52:45
I see, so the connection graph looking like a geodesic tesselation? 
I think you actually have this problem when fundamentally describing the architecture of the Bitcoin network in context of any decentralization debate - because there is the logical structure of the P2P network on top of the (assumed to be scale free) structure of the Internet.
The logical structure I could imagine as geodesic. But I fail to see how Bitcoin would ever get a physical geodesic structure in any scenario. I think some hub and spokes is unavoidable. And if it is only your DSL router distributing traffic to your full nodes behind it.
Is a physical geodesic structure something you think should be the goal for Bitcoin?
@_date: 2015-06-28 21:59:28
Very true. However, I do feel there's some truth to what is saying, though: 
Blockstream does potentially create misaligned incentives.
As said earlier, it is not even necessary for those involved to be aware of acting on their incentives, it could be the VCs in the background knowing how to play with the minds of some nerds. Brilliance in one field doesn't cause one to be immune to these tactics.
And then there is also the whole issue of potential group think. And Egos.
I have seen the limiter guys to be way too stubborn on the ML, in the discussions with Gavin, here on reddit etc. to not be at least a bit concerned about this.
@_date: 2015-06-24 18:36:03
You can also buy a lot of explosives for $5k and blow some stuff up and cause one heck of a disruption.
But people do not generally do that, *because, fortunately, they are actually sane*. 
@_date: 2015-06-27 10:38:16
I think you are seeing decentralization and censorship resistance *way too narrow* and thus get to the wrong conclusions. Things like adoption rate, number of different jurisdictions Bitcoin is successful in are more important than full node count. See also Nick Szabo's comment on AWS instances...
Do you really think if TPTB want to outlaw Bitcoin, they couldn't come after some 6k of rabid 'anarchists' who run their own little 1MB toycoin servers form home? *Especially* when Bitcoin, in the public eye, is *the niche currency usually used only for illegal products and services*. Which it certainly would stay with a 1MB block limit.
But imagine Russia starting to push Bitcoin adoption and allowing many large full nodes to exist. It will be much harder to take that node down, especially when a lot of regular joes are transacting through these nodes. 
*This* is what you want - friendly government competition for an honest, widespread money system that is arguably a lot easier to understand and less prone to shenanigans than the CB-style scheme we have now. (Note also that having a large on-chain layer 0 is less complexity than a complex system of intermediaries like LN. I think the latter have their place, but consider all the hierarchy and complexity inherent in such schemes!)
There are countries in the world in which just using the wrong words on a blog posts could get you into jail - or worse. There are many countries that have implemented country wide Internet censorship.
Bitcoin relies on at least some countries in the world being somewhat free. It can well be made *impossible* to use if your government really wants that to happen. Do you think you can freely transact Bitcoin in North Korea?
For better or worse, western governments are still to some extend a product of the masses. Successfully sell Bitcoin to the masses and have governments that actually are (mostly) not standing in the way. 
That said, Bitcoin is the only system having the chance of maybe, possibly achieving wide adoption and success. If for whatever reason, it becomes 'evil' and a tool for oppression, you can still start or use an Altcoin.
But if Bitcoin fails, you can be sure that no Altcoin will take its place because the whole cryptocurrency experiment will have failed.
And this is, as it has been pointed out many times already, also the initial vision of Satoshi that most of us still see as the way forward.
If you do not like that vision, create your 10kB block, censorship resistant cripplecoin. But do not stop others from scaling up Bitcoin. 
@_date: 2015-06-25 09:04:08






I consider it a trade-off from the beginning of which the current behavior is just one extreme.








I see a problem with the current approach of *other people paying for storage of your data*. Whether it will grow to have enough inertia to cause a hard fork, we'll see.
But I am absolutely certain that unless Bitcoin dies, a hard fork away from 1MB will grow enough momentum.
What I proposed is a solution to that, and, yes, a hard fork.








Sure. But what is the problem here?








Who says this is the actual genesis block and not 
12a765e31ffd4059bada1e25190f6e98c99d9714d334efa41a195a7e7e04bfe2 and the whole Internet lied to me all the time?
Ridiculous, you say? The Bitcoin network lying to me for a decade would be similarly ridiculous.
@_date: 2015-06-30 19:52:58
You assume elastic demand with basic infrastructure like a payment/money system?!
Please. Bitcoin will break if the wall is hit. Mike's perfectly correct about that.
@_date: 2015-06-12 09:08:14
EXACTLY. They can do all kinds of shenanigans now with 51% of the hash power.
This is Satoshi's assumption. That the majority of hashing power will stay constructive and honest.
For some reason, many of the people here (even if they call themselves 'ancap') don't seem to be able to trust this, at all - and want all kinds of weird governance.
@_date: 2015-06-28 10:23:43
Thank you. 
@_date: 2015-06-03 19:13:03
Not to care is a little strong - but yes, that was exactly what I was trying to say here: Size of blockchain != size of blk*.dat files.
@_date: 2015-06-14 09:34:20


Citation needed.
~~Also, please show me how 20% gives you 3MB - I end up at 2MB.~~ That part checks out.
@_date: 2015-06-16 09:45:23
Thank you very much! I sure can. I just put that as a link into the selftext above, so it all stays in one place.
If you want, I might also be able to dig the list of all comments I used for the cross-reddit 'study'. As far as I can see, this would take a bit more effort, though.
@_date: 2015-06-12 13:32:17
How do you see it, originator of this post?
@_date: 2015-06-12 13:26:07
Blocksize is a centrally-planned, hard limit. Inelastic demand meets finite supply is a recipe for disaster.
@_date: 2015-06-09 15:44:57


Exactly. I might want to lock a bit into a Lightning Network channel to transact quickly, microtransactions etc., but I also want to want to move my coins *on chain*.
If LN is a great option for instant payments, most people will use it over the 'raw network' because it adds usability!
***Fucking crippling Bitcoin to make a case that it is 'only a settlement system' is insane and very much against the original plan of Satoshi***.
@_date: 2015-06-12 13:58:40
BS. The miners have a veto also with 's proposal. That's the way he intended it to happen. Majority of *miners*.
@_date: 2015-06-30 08:17:46
I see. You sound like you work in academia in a STEM field? :)
@_date: 2015-06-27 11:24:04
That's why I think the hard fork possibility needs to stay on the table. Walk away from bitcoin-core to XT - or even better yet, see this as what it really is: Getting bitcoin-core in line again.
Maybe, just maybe, the value of Bitcoin as money will eventually actually override the individual anarchist's craziness. Hopefully. Maybe!
@_date: 2015-06-02 18:21:05
There is actually another problem with worst case - it is very often ill-defined. That said:
Bitcoin assumes mostly a Poisson distribution in the incoming transactions, for example, to work properly.
TCP makes similar assumptions about the time distribution of Internet packets and behavior under congestion.
Both will fail when those assumptions are wrong.
@_date: 2015-06-29 00:04:08
It would fit the numbers and everything else pretty well. That's why I am it calling that way.
@_date: 2015-06-30 08:43:40
It looks very doable though; According to [this]( the 90th percentile of transaction propagation is reached after some 10s.
I think RBF changes the dynamics for zero conf to something different. I don't think it is necessarily bad, but why even touch a working system here.
@_date: 2015-06-30 16:02:56
Agreed, except I am not sure about a maybe problematic long term UTXO growth. I am worried about really long term disk space, not RAM. UTXO access can be cached to more recent set.
We might need coalescing transactions into common hashes after a decade or century or so, when Bitcoin becomes *really* successful. Maybe not if people don't lose Bitcoin dust too much.
But I think this is completely OT in this discussion. The blocksize limit was clearly intended to be removed, and Gavin's 8MB...8GB lucky-number-to-lucky-number proposal is already quite a compromise with the Block-the-stream guys.
please finish the spectacle soon by releasing your fork on XT. Thank you!
@_date: 2015-06-09 17:28:37


I think you just described the extreme examples - the buying and selling panic mode. But there's lots of gray area in between - and the result of all the actions is the market price of Bitcoin that we have right now.
@_date: 2015-06-18 21:03:35


Thank you very much, much appreciated :)




The hubs part would still be organized in an expander graph, though. Arguably, we have a hub and spokes network already, with so many SPV clients around. So the more exact question would be whether the expander graph size of the underlying hub network is big enough for one's goals.
And here we do indeed get into muddy waters. Censorship resistance is such a damn complex issue, that I don't know where to start. I could argue now, straightforward, that my gut feeling tells me that Gavin's approach, with Bitcoins widely spread but a maybe smaller full node network would help the most in censorship resistance. And I strongly believe other people are equally using their gut feeling to argue otherwise in the end. As you say below, these issues are also human. In short, I believe that successful, widely spread Bitcoins will make governments actually compete to ensure fair access, as they have much more to lose when they oppose a large userbase than a tiny set of Bitcoin businesses who are just settling on the blockchain.
I also think that different jurisdiction and a borderless internet are doing more to Bitcoins decentralization than node count. And being able to use encryption in at least some jurisdictions.


Yes. I think this can happen regardless of the blocksize, though.


Agreed. I think, and although I dislike their existence and competition to Bitcoin in some ways, Altcoins might play a role in censorship resistance. It see it this way: If Bitcoin stays very small (1MB forever, due to lack of compromise), it is likely to die. And, being the leader of the pack, is also probably going to take the whole ecosystem with it. But if Bitcoin becomes successful, even if it is going to be come centralized and evil, Altcoins would still have the ability to be a worthwhile alternative. That makes me favor less limiting of blocksize: If Bitcoin turns evil, it will be just what we have already, in a different form + the idea out there that it is indeed possible to bootstrap money. But if it fails, the setback will be much larger.


Success as money comes first to me - and I think censorship resistance follows from that, and not the other way around.
I hope this somehow makes sense. I am getting really tired today, this blocksize issue is exhausting.
@_date: 2015-06-30 23:13:13
With pooled mining, the hash power itself is still decentralized, though. That's what is important to keep things honest.
And I fail to see how you could ever prevent someone from building a big 'centralized' farm.
@_date: 2015-06-07 12:35:22
Care to explain the downvote? What exactly is wrong about this statement? :D
@_date: 2015-06-06 08:39:55
In this context: [Interesting perspective on Mircea Popescu...](
@_date: 2015-06-27 10:46:07
Exactly. The very possibility of Bitcoin becoming worthless due to the 1MB cripplers is priced in right now.
Which brings me to another thing seen here on from time to time in the blocksize debates:
The argument that people who are arguing for a block size increase are doing it to raise the price, so that they are able to sell their coins as 'bagholders'.
That is actually quite funny. Because why the heck should the price increase if increasing the 1MB limit is so worthless, dangerous and wrong?
Exactly. Because this would actually make Bitcoin worth more, because it can then realistically be used for more transactions more people, better network effects and so on. Because it is the *economically sound* thing to do!
And whether it increases the chance of the 'bagholders' to sell their stash at a gain (in $/EUR) is framing the debate as if there is something wrong with an increase in Bitcoin's price.
The price is still the primary reflection of Bitcoin's sucess as a widespread storage of value and money system.
These kinds of arguments are good at unmasking the intent of their authors - to artificially keep Bitcoin small and useless.
@_date: 2015-06-18 11:17:29








the transaction rate polynomials in the user count-- the constants don't
matter for asymptotic analysis. No amount[1] of simple bloom filters or what
have you change that.
As I said elsewhere, your poly(N)-fluff is a fancy way of saying nothing. If you would
actually take the time to look what and I wrote, we address the very
question of whether we have a poly(N) x poly(N), or , in simpler and more
appropriate terms, a N*N situation... or not!


Nice way of evading the argument. First you go and make a scare of 'O ( poly(N)
x poly(N) ) is a superset of O(n ^ 2) for any n at least linear', and then you
avoid all further discussion of the specifics. You are not particularly pedantic here,
by the way :-)


change your position in the slightest, so why bother debating it?
There is no reason to agree to O(N2 ). You appear to participate in this discussion,
yet you are evading the core issues. Deflection tactics.


does-- especially when you spend your time arguing that you are "Right"
without making an effort to understand why what you're saying is different
from other experienced and educated people-- is produce a bunch of sciency
noise, which obscures the real disagreement on principles. Doubly so because
in practice it may not matter if something is even exponential complexity,
so long as the constants are friendly.
Sciency noise. You are projecting.  *YOU* started to talk about poly(n) x
poly(n) right away to generate sciency noise. You start talking about Bloom filters.
If you look at my other post, I was actually making the effort to break the
issue down to be understandable - and if you think there is an error in
that, I'd be glad if you can point it out. Yes, I am not but
part of the other side here. And I see him as being (mostly) trying to explain issues, too.


CS PHD)
Appeal to authority. You don't need to do that. We respect him for his
*arguments*. If they are actually correct, and not scare tactics. Same with
you, whatever academic credentials you have. 


related to the number of users, because you believe that the system can
operate with a few large scale central validators
I guess he's reasoning along these lines: Not centralized, but hub and spoke. ***Hub and spoke != centralized.*** As Satoshi intended. You apparently want to change this model, but you are not honest and forthright in telling us that this a change in course from what Bitcoin was *and is* intended to be.
















You are starting to conflate issues here. I am very much opposed to coin
red/black/white/ violet-with-green-stripes listings. I *will* oppose Mike on
that, if he'd ever bring it up again. Doesn't change a bit in the O(N2 )
discussion, though.... And with regards to the checkpointing: Point taken,
that's just going to be unnecessary anyways, though.
And that you group yourself to 'the pesky cypherpunks', the underdog, sounds a
little ridiculous, to be honest...


fundamental trade-off, where Bitcoin -- as a system of self-determined
trustless validation, where its rules are upheld cryptographically "no
matter how good the excuse"-- must compromise ability to verify and personal
control to achieve maximum scale, or throughput and ability to transact to
achieve maximum autonomy. Those who understand this want to navigate a
middle path that maximizes total delivered value, and build additional, more
scalable tools that require less compromise.
Now you go about O(N2 ) again... angels dancing on heads or pins, or not?
The problem a lot of people, myself included, have with your approach is
that you want to *enforce* your very own version the middle path by stalling any 
progress on the blocksize issue. 


fundamental trade-off, where Bitcoin -- as a system of self-determined
trustless validation, where its rules are upheld cryptographically "no
matter how good the excuse"-- must compromise ability to verify and personal
control to achieve maximum scale, or throughput and ability to transact to
achieve maximum autonomy. Those who understand this want to navigate a
middle path that maximizes total delivered value, and build additional, more
scalable tools that require less compromise. The O(N2 ) a bit of intuition
that expresses this understanding, but without constants it is useless as a
concrete explanation for things; but the point being made by mentioning one
isn't intended to be concrete. It's setting up the framework for
understanding the rest.
So O(N2 ) seems to be your *intuition*. Ok, how about breaking down those poly-terms
and actually looking at whether O(N2 ) is any way relevant behavior for
@_date: 2015-06-11 10:09:59
What brings up is a very valid point, though. 
What he described is absolutely possible today - it is just that we align into different fractions and let certain spokespersons decide for us (Gavin/Greg). Both fractions only agree on blocksize until March 2016. The question is whether the economic incentive is big enough to not split Bitcoin apart and possibly hamper/kill it.
In the end, 51% of the hashing power is what really matters - who has the longest chain.
@_date: 2015-06-30 20:04:55
Numbers! Good. Not a definite proposal though.
As far as I see, 8MB is your proposed maximum, Gavin's proposed minimum?
@_date: 2015-06-30 19:23:08
A risk we all (have to) accept since we read the original paper.
That is just a core part of this experiment.
1MB blocks are the issue now. Not a potential 51% miner attack. That will stay with Bitcoin and isn't up for debate.
@_date: 2015-06-16 22:37:46


I'd love you to be right. 
[But that argument was made already in 2010.](
And look at where we are now.
@_date: 2015-06-21 09:09:26
Doesn't change the big picture. Changing the protocol as it is implemented is a certain amount of friction, of course, that is taken into account by any decision in the meta protocol..
@_date: 2015-06-02 10:02:48
20x is not at all silly, look at Bitcoin's history, we had 20x the blocksize limit compared to *actual* blocksize in the past and the sky didn't fall and the blocks were not full of spam all the time. Also note that you can always soft fork it down, whereas a hardfork to increase it is much more difficult. If you are interested in a sane and working Bitcoin. So the whole argument that you need to keep the hardfork from happening stinks more and more like divide and conquer and manipulation from interested parties. 
***There were no counterproposals and compromises*** from the other devs, just empty naysaying. That is EXACTLY the problem. Gavin was at this for YEARS, very reasonable, very calm, requesting constructive input etc.. And if this changed recently, please enlighten me.
Blocksize limits were initially not intended at all for Bitcoin - no extremism here, quite the contrary. You try to rewrite history.
Also [relevant.](
@_date: 2015-06-17 00:05:43
Right. But the point is that if it is just the technical limit in the network parts of bitcoind (like it is right now), it should clearly be stated that it is just that and not intended to interfere with the core idea of JG's BIP100 proposal, which is a market based solution.
It should be made exceptionally clear in that BIP that it is not meant in any way as a meaningful limit in the bigger picture of Bitcoin's progress, and that the market based solution is also intended to work above 32MB as soon as the involved code has been reworked as necessary. 
But what I see in the proposal is along the lines of 'the 32MB limit is meant to be'. People will cling to that, there will be contention, many problems, etc., just like there is now for the 1MB limit.
That needs to be fixed with his proposal, IMO.
@_date: 2015-06-24 08:16:49
You are suggesting to steal old people's money! :-)
How about my other comment [here]( 
@_date: 2015-06-27 11:01:31














Regarding Path of decentralization: Ill-defined concept. And social engineering to make Bitcoin something what it is not intended to be. See [this.](
Do not hide an intent to change the status quo of a growing Bitcoin to a crippled Bitcoin behind some technical 1MB limit that always was intended to be lifted.
Regarding Inefficiency: I think as a distributed, instantly replicated database, there isn't too much inefficiency in the system. VISA/Mastercard will use similar schemes too to protect data integrity.
Regarding blacklisting: FUD and scare tactics. In regards to the layer 0 scaling discussion, this argument is completely out of place. Because there is no indication that LN will be in any way less prone to blacklisting attempts.
Larger user adoption creates pressure against overreaching governments to back off of Bitcoin. Fancy Rube Goldberg - mechanisms create additional, often unneeded complexity and potentially ruin the user experience of Bitcoin.
@_date: 2015-06-09 16:20:44


To go further: The elephant in the room is *why the fuck* do people like GregMaxwell not give their own counter proposals and schedules, with their own *tuning variables*?!
It actually starts to make me suspicious that this does not happen. Is it trying to push people into whatever blockstream is developing? Is Greg the biting underdog, trying to topple Gavin because of some ego BS? What the heck is this?
If the blocksize-crippling side would be sane, they'd have stated their plans for acceptable increases *a long time ago*.
And Gavin has been very giving in the actual details and trying very hard to ensure consensus here.
@_date: 2015-06-25 17:36:55
I like BIP100. The only thing I dislike is the possibility of the technical 32MB being elevated to an *ought-to-be*. Just like it has happened for some with the 1MB limit. I think the language in BIP100 should be cleared up in this regard.
@_date: 2015-06-02 09:40:20
This, first and foremost:
@_date: 2015-06-06 17:47:09
I hope you are right. But if someone is pulling Bitcoin into that direction, I still might not like that particular pull, and will not support it / speak out against it.
@_date: 2015-06-09 09:36:58


That's the point. Consensus isn't reached when one party (the blocksize limiters) vetoes everything.
And they don't even really veto it. They just stay stubborn and let Gavin argue for an increase - and they basically sit there without reacting.
That, IMO, is not constructive behavior at all. That alone makes me think that it is indeed a wise choice of Satoshi to hand Gavin the keys and not Greg. Greg certainly has some very deep, technical knowledge and great ideas - but lacks the overall high-level perspective and foresight that Gavin has. Because he was on this blocksize increase issue for *years*, and given the contention here, he was obviously right in planning it and pushing it forward *now*.
@_date: 2015-06-12 23:15:15
Straw man. 20MB is not at any cost.
@_date: 2015-06-28 09:50:29


No. I don't think so. 
Imagine someone, Mr. X, comes along and invents a card game.
You start playing that game with your buddies. Suddenly, someone wants to change the rules of the game, while it is going on.
Is it in any way an argument from authority to say 'Mr. X intended it like this?'
Nope. It is just a convenient way of pointing to the original rules.
@_date: 2015-06-17 14:06:21
In this context, block size limiting is even more insanity.
Block size limiting means that someone is scared of Bitcoin actually being *used* thus *scared of it being successful*.
Assume the unlikely scenario of Bitcoin scaling to billions of transactions per day and also assume that no Moore's/Nielsen's law holds at all from now on and just a couple tens of nodes remain worldwide, because they would be too expensive to run for the typical home guy. 
There are gigabit links in today's internet, and computers which have enough power today to process tens of thousands of transactions per second. So *technically*, this scenario is *very possible*.
[And again to note: This is a *success scenario* for Bitcoin that is possible. No one of the limiters presented a plausible, successful 1MB scenario yet.]
And this would also be the 'centralization horror scenario' the limiters want to avoid.
Now, first of all, having tens of nodes (in probably dozens of different jurisdictions, some of them at odds!) is still not centralized - there is no common center. 
And if you now say hub and spokes networks are centralized, you could as well call the Internet centralized, because your ISP might just have one BGP router connected 'at the top' to the rest of the internet. So clearly, decentralization is a fuzzy word to be defined further by the limiters. It seems to be used as a diffuse FUD word.
  
But even if what happens and subsequently Bitcoin gets co-opted by banks and governments and turns evil, it still showed the world that 
a) the bottom up approach to inventing money *works* and 
b) it *does not prevent an altcoin from taking its place* as alternative 'non evil money'. With blocksize limits or whatever.
I personally think it is unlikely for that evil scenario to happen, but even if it does, altcoins would save freedom.
Limiting blocksize means being afraid of Bitcoin's success.
@_date: 2015-06-18 10:20:12
Thank you :-)
@_date: 2015-06-11 21:43:51
If you deploy the code that will hardfork in March2016 now, there is still a very realistic chance that the network and userbase will flock to a single implementation (core or XT).
With a last-minute hardfork like this, you won't ever have the chance. And the same contention.
And the way that the 1MB-blockistas behave, I'd say Gavin is actually very sane going forward with 20MB on BitcoinXT. It is, in a way, the most responsible thing to do.
@_date: 2015-06-14 19:25:25
It is weird. certainly a highly intelligent person, too, believes that number of transactions will scale in the network with O(n ^2 ) with n being users - which doesn't really check out and has been debunked by too.
These are relatively simple questions - I'd expect those to be settled as *knowledge* between the devs, and not wild opinions.
@_date: 2015-06-02 09:44:02
But the others can deny to build on that block, keeping any single party with &lt;51% from wreaking havoc onto the network.
So this is a good point - blocksize is already controlled by 51% of the hashing power.
That said, a couple filled blocks 'for the lulz' will not threaten anything even.
@_date: 2015-06-03 19:51:22
Yeah. I have said elsewhere: Have several full nodes in different jurisdictions, and keep a bigger club (e.g. hacker-/makerspace) able to run a full node. That, IMO, would be enough decentralization.
As and others also point out, decentralization of the full nodes is just one thing, decentralization of the user base is another important factor to consider. If 100mio people are using Bitcoin, the bar to mess with it will be much higher.
@_date: 2015-06-27 19:09:39


Good. Means that if you are right, people are willing to pay for that. One of the big arguments of the blocksize limiters falls apart.


So if we have that growth anyways, what's the problem?


True. And I want to rather see Bitcoin become widely successful and an Altcoin to fill that niche. If things get bad with Bitcoin, that Altcoin might become successful.
There are many altcoins to choose from already.  So that niche is already filled.
@_date: 2015-06-29 22:46:34
Didn't see this post before. Very well written. Thank you. Are you in the business of traffic or capacity planning?
@_date: 2015-06-01 22:50:19
Me too. Most funny are the redditors who say 'why don't the devs avoid reddit with all the lowly redditors'. Kind of ironic.
There are quite intelligent and knowledgeable people around here, including some of the core devs.
@_date: 2015-06-25 09:23:42
I agree. I think he's trusting the economic incentives built into the system to eventually find the optimal configuration for maximum Bitcoin value.
And I also think he's expecting the internal economic pressure of Bitcoin to scale to overwhelm the outside economic pressure on Bitcoin to be molded into shape as part as someone else's payment system.  
Maybe he put in the 1MB hard cap as a test to see whether he's right on the economics/psychology of Bitcoin and money.
@_date: 2015-06-19 22:42:13
Exactly. Greg has not been too constructive in the discussions with Gavin. And Gavin has trying to get the ball rolling, consensus forming if you will, since *years*....
@_date: 2015-06-25 09:12:53




If they'd be 'disastrous', Bitcoin would fall apart when I start running my own little hard fork. If it isn't 10%, what is it that would be acceptable as 'non disastrous' for you? 1%?


To also answer your other question, yes, 'the spec' is incomplete, and yes, many of the buggy behaviors from the code flow back into the spec. Not arguing with that.
But consider this: There was once a bug that allowed more than 21mio coins. And *obviously* the code got overridden by the spec there. 
Same with 1MB btw. Spec &gt; Code.
@_date: 2015-06-02 10:04:28


We'll actually see. As you know, it would be a growth of the *limit* of 40%/year, not the actual size.
We are depending on the miners not being 51% attacking us anyways. And as long as the majority of miners want to keep it sane, they even have a *very clear economic interest* to keep the blocksize sane. And they can prevent rogue miners from bloating it, by *not building on their bloated chain*.


You want to enforce centrally planned artificial scarcity. There is natural scarcity already in Bitcoin blocks, and there will always be. [Look at this.](
@_date: 2015-06-09 17:53:04
Consensus is 50% of the mining power. That's rule zero.
@_date: 2015-06-08 18:09:51
This is an angle that needs to be discussed further, IMO.
I am obviously fine with delegating responsibility to the devs. But not if some devs think it is responsible to postpone an already *very contentious* decision up to the very point when things start breaking. And wishy-washy argue 'we have done hardforks before, it will all go smoothly'. 
... I don't think so ...
Gavin is at least *planning*, and has been, since *years*.
@_date: 2015-06-12 10:58:20


20MB is far from a 'gmail only' situation. Also, even now, there are many smaller companies still running their own mail servers.
Furthermore, even in a situation where there is gmail level full node with Bitcoin, it will a) be widely successful, so the decentralization metric of 'how many people have Bitcoin' it is a lot better, and with that wide success, there will still be many full nodes outside U.S. jurisdiction, too. Even if Bitcoin the network will be almost as centralized as master card, it is an open system still and there will  be equally big nodes in Moscow in Bejing (for example). A 3tps system used by nobody can be pushed aside and outlawed by every state as 'used by drugdealers only'. A widely used system covering competing states is much harder to manage for TPTB.
Also, with just a couple tens of huge nodes worldwide, I'll still consider Bitcoin decentralized.


Fair point, but I fail to see how 20MB could be Gavin serving his masters. If he went and made coinjoin impossible (for example), that would be a different matter.


That the point you blocksize limiters don't get: Bitcoin the technical protocol is just a *small part* of Bitcoin the ecosystem. Bitcoin the ecosystem depends on outside investments, the price (yes!), and many other factors. Think in a bigger picture.


I think this got mixed up, that was a reply to what you deem possible destruction of Bitcoin.




Well, what can be relied on then when transactions saturate with 3tps? Gavin's hardfork will be widely spread by then and will be the easy and available option - given the contention around the 1MB limit, there's no reason to expect the remaining core people will get their shit together and agree on an increase schedule in the last minute. And they'll work against Gavin's already existing and widely deployed solution by then.
In a way, all Gavin is doing is giving options: The option to have the system running with up to 20MB blocks in 2016.
@_date: 2015-06-14 21:04:09
Non-miners can do proof-of-stake (purported to be broken and complex by some), but what is easily doable is proof-of-work. Bitcoin's foundations.
Especially, it is almost impossible to let full nodes vote: Because you can easily create many of them for the vote, and you can also easily create fake full nodes in a [Sybil attack](
@_date: 2015-06-12 13:43:32


How much Bitcoin can scale could still be determined by what the hardware can do then (to the *true, intrinsic limitations*!), and not by the central 1MB blocksize planning committee...
@_date: 2015-06-16 13:37:48
I am not going to spend all my time collecting the myriad of of mailing list posts, reddit posts etc. where Gavin argued in favor of bigger blocks.
You have been here all the time, you know these discussions. 
There is no backdooring going on. Independent market participants are deciding what is a good way to go forward. Coinbase &amp; Co. wouldn't support Gavin if it would hurt their business.
@_date: 2015-06-19 12:12:15
You edited your post to be more detailed now, so maybe a more detailed answer is in order, too:
If you really believe that 0.1% of users are running a full node and this causing *full-network* O(n ^ 2), you should fully support plan of growing the *userbase* of Bitcoin as much as possible. (I also think the very real opposite effect exists of potentially driving users away with a limited Bitcoin)
Because, according to your formula, this would due to nodes = 0.1% x users, create many new full nodes, too.
There is this often-heard argument that the full node count will decrease (centralization!1!) with a higher blocksize and that would also incidentally mean that the scaling behavior of full nodes with users is sublinear and thus the *full network* bandwidth also growing in something *smaller* than O(n ^ 2)! So either one of those arguments can not be true.
Regarding the fear that we are going to saturate the internet: This is a fear of Bitcoin's *wide success*! I am not saying that we shouldn't consider the physical limits of this whole shebang, but consider that economic factors will kick in well before the Internet is going to be overloaded with Bitcoin transactions. And it is my very strong conviction that we shouldn't cripple Bitcoin because we are afraid of its success. If Bitcoin is so successful that it is going to break the Internet because of its overwhelming bandwidth, I say, as a tongue-in-cheek battle cry: Let's *try* to break the Internet :D
Furthermore, I am pretty sure that the *current, existing Internet infrastructure*, in terms of global bandwidth, would be able to support a couple hundred full nodes worldwide. To make my point with numbers:
[Quoting Wikipedia](


~ 256GByte/s. In the worldwide network. In 2002. This 13 year old technology alone, would support allow blocks with a size on the of the order 150 ***Petabytes*** going around.  Take in a *very generous* factor 1000x of inefficiency and the fan out of this global network to the full nodes in each connected country, and you are still at 153GB blocksize. With some generous 500 bytes of transaction data, this would allow 7 billion people over 6 transactions per day. With yesterdays bandwidth!
Of course, this is just bandwidth. Transactions would need to be verified in a massively parallel scheme. But that would mostly be software ... I am not saying we are there yet. I am just talking about the technological limits.
Again, this is for a scenario where Bitcoin is *widely* successful and in everybody's hands. And we haven't even factored in potential growth in technology. The above is assuming Moore's and Nielsen's predictions flatlined *yesterday*. I'd expect that in 2040, *at least* every city in the modern world with &gt;100k people can elect to run a validating full node. In many different jurisdictions.
That is compatible with Satoshi's vision.
And it should also be noted that the last word has not been spoken in terms of market efficiency with regards to full nodes. has written some profound blog and mailing list posts detailing, among other things, how full nodes could be paid.
And if the *natural* market equilibrium, limited by technology will create a Bitcoin with many successful layers on top, Sidechains, Lightning Networks etc. ... I'd be happy about it. But please, again, lets not cripple Bitcoin in fear of its success.
Last but not least, let me address the Miner worry. I am always somewhat worried about what the miners do, too. Just relying on 51% of the hashpower not being destructive evokes a certain funny feeling in my stomach from time to time. That's just how Bitcoin is, in the end, though. If you look at my submission history, I have actually warned about the ghash pool getting 51%, and them being involved in some apparently shady stuff.
However, I am not so sure that a *direct* link between miner centralization and full node centralization exists, so we need to be careful to not conflate two different issues here.
@_date: 2015-06-17 10:24:50
Where does that address my points?
@_date: 2015-06-14 09:53:28
Which is in the end the only viable option - no one can take that choice away from users; well maybe except in very oppressive regimes.
@_date: 2015-06-27 19:06:16
I am not mocking their contributions, where did I say that?
They should simply not have power over what Bitcoin ought to be. They might be contributors to the code, but they are 'just' stewards of Bitcoin the money system.
@_date: 2015-06-29 20:53:08
And I think he's going to do that.
I am actually not that worried anymore. I think scare of hard forks is FUD in itself. That is how Bitcoin corrects itself on the meta level.
If Bitcoin survives the onslaught of Block-the-stream special interest, the next level of onslaught will probably be a lot tougher and more insidious.
@_date: 2015-06-30 10:34:10


And reputation to some extend, apparently. Wasn't it f2pool who was backpedaling?
And if it is going to be lots and lots small miners (the wished decentralization scenario by so many), I fail to see how they are going to successfully try to subvert the whole network to snatch a 0-conf double spend here and there. They'd also only make a block every so often.
EDIT: And you have the large scale games too: If a miner is rogue and encourages 0-conf double spends, he is threatened by the rest of the miners (majority) to be cut off and orphaned.
Incentives in Bitcoin work out.  Except for the FUD trying to destroy it maybe.
@_date: 2015-06-28 14:55:40




Rather, from the fear of that happening. Are you distrusting the miners?
@_date: 2015-06-12 09:57:21
No. Miners with 51% of the hashing power can eventually create the longest chain with *whatever content they wish for*. They can simply start at zero.
@_date: 2015-06-11 14:36:14
I am actually waiting for some kind of landslide on the devs who haven't been too outspoken yet to support Gavin, maybe with the  odd grudging acceptance and a  'I feel forced to, to keep Bitcoin healthy and non-forked' comment, but the vibe seems to be that as soon as Gavin puts the fork online, there will be a new consensus/equilibrium happening.
We'll see.
@_date: 2015-06-24 17:45:02
Yeah, sure, it is the lack of a BIP stalling it all. Though I am still waiting for Gavin to produce correct TPS cover sheets for his BIP. Because the BIP is absolutely needed to be produced with TPS cover sheets... /s
@_date: 2015-06-12 13:01:39
Can you explain this further? I don't get it, and especially not the comparison to a worldwide, successful Bitcoin.
@_date: 2015-06-30 22:01:50
Mmhm. But Bitcoin always operated and is intended to operate with an effectively uncapped blocksize (except for what the miners decide), and by letting it run into saturation, you are changing the operation mode.
@_date: 2015-06-27 13:15:27
Thank you!


I thought about that too. Very interesting dynamics. Basically, the 1MB limiters might be driven to temporarily, 'almost' destroy Bitcoin to buy cheap coins.
I think this is a *very dangerous* strategy, though. And I think Bitcoin profits more from, honest open and *directly stated* motives. In the short as well as the long run.
@_date: 2015-06-12 14:03:50
will put in a majority-of-the-hashing-power-check as a trigger into his fork. 
If that would not be the case, then you'd have a fair point.
@_date: 2015-06-17 14:38:47
Fully agreed, but people are worried and this could address that worry.
Another factor to consider is cost to bootstrap a node. With UTXO commitments, you could go and get a full node running from lets say all blockchain data of just a year back.
@_date: 2015-06-18 10:19:17
As far as I see Greg is just using fancy ways to distract and say nothing (new), and Mike's and my point still stands. No crushing going on here, at all.
Greg is using smokescreens. 
@_date: 2015-06-27 21:02:21
Hey Adam,
for example, repeatedly talking about an ***O(n ^ 2) wall***, although apart
from me (as just another random redditor), and have clearly shown that this is not the case, makes people think that way.
And of course, one won't change technical or physical limits to scaling with
a change of a constant in software. But one can obstruct possible scaling with
keeping the 1MB limit in place. Or seemingly sneaking in 32MB
as another *ought-to-be* limit into BIP100, instead of clearly stating this
as a technical limit eventually to be lifted not meant to impede blocksize growth.
There is a difference between saying and doing. All the actions from you and
Greg so far have been an effective stalling of the blocksize debate.
Doing nothing is doing something still. Lack of consensus is the nice way to
say it, vetoing a change more appropriate, but outright obstructionism
eventually becomes the way people see this behavior if it continues.   
              
Do not abuse the power that is given to you as core devs to try to mold
Bitcoin into whatever you like it to be. The risk of a hard fork appeared   
because some of you guys seem to ignore market and user demand. The danger
of a hard fork is pressure relief from a very concerned Bitcoin user base.
This issue is much deeper and wider than just a constant in software.
Consult with more. He's active on the mailing list. And he
seems to have a good picture of what Bitcoin is and can be, broader than just a
view of the code.
@_date: 2015-06-24 08:09:55
How about coalescing the old UTXOs all together into a merkle tree of which only the hash tip is stored, and then having anyone who wants to spend them provide proof that they are part of the old UTXO set by providing the corresponding merkle tree branches?
By storing the cumulative sums of UTXOs at every node in the merkle tree, it can also be ensured by anyone that the total amount of Bitcoins is constant.
This scheme could be automated so that every 5 years or so, one who has some old outputs on the chain needs to update some merkle branches to be able to access his/her coins. As this is only a couple 100kB of data per output, this should be doable?
It would put the burden of storage back to the users of Bitcoin, where this burden belongs.
@_date: 2015-06-25 15:37:05
Well, we are going in circles here. If one has seen the blockchain successfully and diligently operating and validating UTXO sets for a decade, there is no reason to assume an output subsumed under a UTXO hash is invalid.
@_date: 2015-06-20 12:22:43
^ This. Also applies to the whole blocksize debate...
@_date: 2015-06-12 13:24:12


He's not increasing the risk: He's making people able to vote with XT for a clear alternative well in advance before the blocks are full.
Increasing the risk is saying 'bigger blocks are controversial, so lets just be happy and drive into that wall'.
@_date: 2015-06-09 16:29:01
You know this: There won't be a business plan, ever.
This has pros and cons:
A con is that from time to time, contention emerges around intricate issues (like now, with the blocksize)
A pro is that due to its open nature - it isn't a company after all, rather a whole ecosystem - new solutions and ideas have it much easier to bubble up.
@_date: 2015-06-09 19:02:39


Maybe this can be framed in other words: Are the economic incentives to 'get our shit together' strong enough to stop obstructionists from blocking Bitcoin's fair progress?
I hope so, but I honestly do not know.
@_date: 2015-06-27 16:54:06


I haven't seen real data on Bitcoin not being able to scale very big either. Just FUD.




Blocks are always capped by what miners intent to actually mine.


A single car will not serve the world's transportation needs. A couple hundred million of them do, though.
@_date: 2015-06-17 00:58:53
I asked about this and other things directly, but didn't get any response. I think I also asked him to further explain the 32MB limit.  He should state clearly that no hard blocksize caps are intended for Bitcoin in his proposal. To avoid another point of contention.
EDIT: See [here]( and [here](
@_date: 2015-06-16 08:46:42


[Why the heck do you still call it a O( n ^ 2) wall if we had this discussion here?!](
has the same complaint with you, I think. Rightly so.
@_date: 2015-06-12 14:37:44
Exactly. Talking about 20MB==infinity or some far out goals while there is actually technological progress the last couple years and the proposal at hand is a very concrete 20MB, looks more and more like social engineering the whole debate.
@_date: 2015-06-12 13:55:41
Or they go the more easy engineering route and change the constant bitcoind and keep their wallets. Rather than supporting essentially two chains at the same time...
Especially since Gavin already tested and planned this...
@_date: 2015-06-12 23:14:38
Exactly. Gavin's the most reasonable guy around. Satoshi made a wise decision there.
He shouldn't become too reasonable though and take away the pressure on this issue. This stuff needs to be done soon, at least soonish.
@_date: 2015-06-13 21:29:23
Hey, I think I am starting to understand the details better now.
From the proposal:


/BV8000000/ to vote for 8M. Votes are evaluated by dropping bottom 20% and top 
20%, and then the most common floor (minimum) is chosen. 
I have the following comments here, if you accept input from some random redditor :) 
a) It might be me not being a native English speaker, but I feel 'the most common floor (minimum)' is unclear wording. So, I understand it as follows currently: You have N votes with different amounts, you sort them, and you throw away the lowest 20% (of number of votes).  What remains as the then lowest vote from that subset is the new blocksize? Why do you write 'dropping the top 20%', too?
b) Can the BlockSizeRequestValue be compressed with a floating point representation? What are the specifics to parse it out of the scriptSig?
c) 12k blocks amounts to 83.3 days with nominal 10min rate. That's not quite three months. 13k blocks would be closer, but a dozen is a nicer number. Given the state of the blocksize debate and any minor thing causing contention, I'd say 'about three months', or 'just under 3 months' to be safe from arguments along the lines of 'jgarzik is fooling us'.
@_date: 2015-06-16 21:10:40


What is not enough to run a global payments network?


Other reason that what?


Nope. Need is too strong for me. Let's say: It would be very nice.
@_date: 2015-06-11 13:07:03
And yet, it works right now. Evidence invalidates theory.
@_date: 2015-06-02 17:43:15
Right. I still think the next issue about scaling - after we get Gavin's proposal through, hopefully - is thinking about blockchain size and UTXO set on disk. At some point, initial blockchain downloads might make it difficult to distribute the blockchain wider, for example.
Because if you search for 'ultimate blockchain compression' and the like, there is a lot what can be done while still keeping full node security. (Some people argue full node means having all transactions since day 0 - but they kind of argue a circular argument, because there could still be two block chains, a fake one, and a real one)
@_date: 2015-06-14 09:31:31


Wait a bit! Please explain this further: As far as I understand, this is a message level limit completely out of the consensus code and the rest of Bitcoin core is very able to process blocks &gt;32MB. 
Are you saying you are introducing another cap at 32MB?
Please clarify.
@_date: 2015-06-16 20:28:24
A bit is too nice. For way too long I too have been patient with what came out of people like Greg &amp; Adam. Have you seen [how Adam uses O( n ^ 2 ) scare tactics](  to make it all sound like there is more growth than Satoshi has foreseen?
And debate is socially engineered as if 1MB limiting - or even artificial limits at all - are the intent of Bitcoin, even though Satoshi very clearly said how he envisions a scaled up Bitcoin.
Is the evasion of Greg whever Gavin asks for concrete numbers, is that just him being stupid? That fellow is highly intelligent! (And, yes, I still value his contributions)
Is this egos? Is this Blockstream? What the heck is this?
Well, pointless to rant to you, you know what I am talking about. I have to say I lost a bit of patience, because I have run out of options to explain the behavior of the people stopping any size increase proposal in a benevolent way.
Oh, and nice post by the way. You put into nice words what I am also thinking about Bitcoin since long. We have a money system. It must be possible to correctly price all interactions in that money system. It is work, sure, but no reason to bury one's head in the sand.
@_date: 2015-06-12 13:41:01
Much easier to sort out the 100k TXIDs than the 100M TXIDs.
@_date: 2015-06-28 14:51:24
Exactly. Thank you. People get scared about that fact, and they start to get addicted to 'solutions' to this (although it is the very core of Bitcoin):
- making a temporary spam hack  a centrally planned 1MB limit
- always having a full node for all transactions since genesis block available 
@_date: 2015-06-07 12:27:53
Yeah, sure, because that just happens automatically. No planning needed, Gavin totally wasted years thinking and arguing about this. /s
@_date: 2015-06-01 23:47:08
Nail on head. But that they don't do it looks more and more to me like they *want* to be involved in a power game vs. Gavin.
Which I hoped wouldn't come this far, but now that it is out in the open, hope that Gavin wins. Because he's far more reasonable - he has since years, very carefully, very politely but insistently brought this issue up and requested input.
But other than (and, yes, I have to admit I am leaning towards how /y/cypherdoc2 described it) action which amounted to unproductive concern trolling and stalling, *nothing* productive came out of from the other devs.
@_date: 2015-06-28 10:43:15
The problem is that stalling the debate is very easy - you can't prove a negative. And they started to (sucessfully, unfortunately) socially engineer the whole debate so that Gavin now has to fight an uphill battle for what is the only sound way for Bitcoin to go forward...
Very frustrating.
@_date: 2015-06-09 10:37:49
And reading reddit ;)
@_date: 2015-06-12 15:56:10
It would be meant only as a failsafe on Jeff's proposal.
@_date: 2015-06-15 22:49:01
[You might like my submission from just a while ago. ](
@_date: 2015-06-16 21:43:12
Jeff Garzik's proposal is great, except that he now contradicts himself by on one hand wanting to avoid central planning in favor of market forces, yet putting in *a hard 32MB cap.*
If not for that, I'd fully support it. I'll go with XT, as Gavin's proposal is the one best planned and argued for, with the least amount of surprises and I am sure that Gavin will eventually (some time after the 20MB fork was sucessful) accept a revised proposal from Jeff that doesn't add another hard barrier.
@_date: 2015-06-30 23:03:05
They would profit from the overlay networks, though. That changes the whole picture, because there *are* incentives now. Usually, people are expected to remove themselves from a situation when there is a conflict of interest.
I don't even think that overlay networks (for profit) are bad if they are not forced onto Bitcoin by crippling the underlying layers.
I also (again) asked Adam this very question [here.](
In any case, people need to be forthright and open about wanting to change the course of Bitcoin, and not by socially engineering it. 
@_date: 2015-06-12 15:57:51
Ah, ok, cool, thanks!
I know, I repeat myself, but: Please talk to Gavin, too. Phone him, whatever. People want to see this solved and you might be able to do the Bitcoin world a great service - on top of what you already do for us.
@_date: 2015-06-27 14:19:59
a) LN networks are not here yet, and their exact behaviour in practice is not known. 
b) Who are you to prescribe people how to use Bitcoin? Let the market between Miners and users sort it out, please.
c) Supposedly Greg and the other can quickly *hard fork* Bitcoin to up the blocksize, if necessary. *Surely*, he can also *softfork* it down at least as quickly, if the need arises?
@_date: 2015-06-12 23:19:58
Problem is, what is a user? If I run three nodes, do I have 3x the vote of others? Do I prove my stake in Bitcoin? Is it per Person? How do you ensure no sybil attack of any voting scheme?
In the end, what counts is hashpower. So Jeff's proposal makes the most sense.
@_date: 2015-06-22 16:54:51
@_date: 2015-06-27 16:55:51
Erm, no, Mr. Redditor for 10h? 
Sidechains are not in Bitcoin yet, non federated side chains need another fork and the bigger blocksize is actually researched by Gavin - since years.
@_date: 2015-06-05 18:07:04
Masters? We are following voluntarily. Free market! No government involved, no violence and threats involved! If he suddenly would want to increase the 21mio cap, you can be damn sure that (almost) no one would follow him anymore. Gladly, he stated *clearly* that messing with the 21 mio. coin limit is never going to happen with him. So there is actually a sanity check you can do on Gavin.
Only because he actually has a *plan forward* with regards to blocksize doesn't mean that he's a dictator.
We actually like progress in Bitcoin and not deadlock because of diffuse developer worries. Those worries are good to consider, but in the end we believe that an increase in block size cap is the most reasonable thing to do about this particular problem of Bitcoin.
Disclaimer, disclaimer: Now if I say 'we', I *think* I only speak for a large fraction of the Bitcoin community and not necessarily everyone.
Look at it this way: Gavin has popular support for his 21 mio coin limit. You can go and start your own 'naynaynay1mbforever' fork and start running it today. Heck you could even decrease the blocksize and fork the chain today and start selling it on your own exchange.
Simply do a git clone of bitcoin-core, adjust the blocksize limit to your liking and after that never change it, just keep it as it is, and gather popular support around your fork.
If you succeed, we might be stuck at 1MB and Bitcoin will die. But that's actually why I am hopeful: 90% of the users see the need for a consensus, which means voluntarily submitting to 'master Gavin' (as a knowledgeable guy) and going with his fork. 
And if his fork is successful (which I think it will be), most will follow it and market forces will probably make your 1MB-forever fork completely worthless.
No force involved or necessary at all.
@_date: 2015-06-27 20:33:24
This must be a new phenomenon. Today, I got a couple of interactions with a completely new account, though.
sponsored a bounty for an analysis a while ago, and [here is my analysis.](
Maybe it would be a good moment to present the summarized results as a submission, @_date: 2015-06-28 11:27:45


Decentralization. A diffuse fear word the blocklimiters are unable to really explain.


Nothing that is workable right now or soon. See also what Mike wrote on the ML about this.
EDIT: Also, this is not data that Satosi's initial scaling vision is not workable. Which I meant in my post.
@_date: 2015-06-03 20:18:41


How so?
@_date: 2015-06-28 15:28:20
@_date: 2015-06-27 11:02:57
Can you please explain that so called consensus to me?
Can you also explain who is ramming through what?
@_date: 2015-06-20 10:23:38
Exactly. I really wonder where all this social engineering and FUD is coming from to cripple Bitcoin with 1MB.
The limiters seem to be either profit motivated (Blockstream) or simply stuck in a 'we need everything to be small' mindset. With their very stubborn ideas on what 'small' means, and very stubborn ideas on whether this is actually good or bad.
As I said elsewhere: I am not against considering technological, physical limits. 
I am against inferring the Bitcoin's contract from implementation details, or even bugs in the implementation. It can be argued that the 1MB limit is obsolete and thus a bug. Bugs are not elevated to Bitcoin's constitution, bugs are fixed. Bitcoin's constitution is a (somewhat ill-defined) social contract that mainly flows from Satoshi's whitepaper. But not the other way around!
Adam, for example, [has been arguing]( that the userbase of Bitcoin translates *linearly* into full nodes, such as 0.1% of users are going to run a full node. He was arguing to show that the Bitcoin network is unscaleable and running into an O(n ^ 2) wall. I don't want to repeat myself over and over again, but a) that is n^2 for the whole network, which is actually scalable and isn't the metric for a single user to run a full node or not, and that metric divides out on n again, and so the metric is rather O(n), and b) if so, then *great*! Because that means growing the user base will directly increase the full node count, the ever important decentralization metric for the blocksize limiters.
He eventually even goes to the fully ridiculous notion that Bitcoin might break/saturate the Internet. Really, read it, it is quite funny.
@_date: 2015-06-12 13:07:15
The reason seems to be: Egos.
[Some guy predicted the ugly mess we are in due to a fixed blocksize back in 2010.](
@_date: 2015-06-28 18:55:53
Well I think one thing that should happen and which I pointed out in other threads is to broaden what the brand Bitcoin actually is.
Bitcoin/XT (Mike Hearn) is as much Bitcoin as Bitcoin/BS (Blockstream) is.
'Official channels' should reflect this, IMO.
@_date: 2015-06-30 11:52:58
On Blockstream... agreed. For a (too) long while I assumed benevolence on the Blockstream side, as Greg was involved in Bitcoin before and did have some good ideas. The way those people stall the discussion and resort to manipulative tactics and trying to put themselves into authority that they don't have when pressed on actual plans - makes me believe that 'can't be evil' is marketing along the lines of 'repeat a lie often enough...'. And quite the opposite from not evil.
What Mike wrote, you mean in regards to scaling order of magnitude calculations? The whole O(n ^ 2) talk?
EDIT: Oh and with regards to stopping the bullshitting - sometimes, I think the perspective of arguments should be to convince *others* not the opponent in the discussion. As for well known reasons, and as you also pointed out, there is no point anymore to convince someone who knows what the truth is but still distorts it for his personal goals. 
@_date: 2015-06-27 11:21:10


Not for the money but for the Bitcoin? The cognitive dissonance is strong here...
@_date: 2015-06-15 21:45:50
Gavin initially planned 20MB + 40%/year. Striking the latter part was trying to appease the subset of other devs who are completely stubborn.
@_date: 2015-06-30 15:01:07


Who does that, how? I think that's just impossible.
Otherwise, agreed with the gist of your post. It is complicated, one might want to add.
@_date: 2015-06-27 19:03:45
That's a good point, maybe I should have said released instead of written.
@_date: 2015-06-21 09:36:14
You might want to pull the XT version string commit so you are letting your node express support for Gavin's schedule, though.
@_date: 2015-06-12 13:03:48
Exactly. Increased, bullshit complexity because Adam, Greg &amp; Co. dug their heels in.
Mike gave some excellent comments on the mailing list with regards to the usability of such an approach.
@_date: 2015-06-17 14:27:36
I actually touched that in my above post. As I said, maybe usage per user for n users goes like O(log n) or similar. That would make scaling O(n log n), still in line with some of the best scaling algorithms.
@_date: 2015-06-03 19:40:01


This might be valid argument for a more delayed blocksize increase, but the side opposed to Gavin hasn't yet really shown that this is actually the case.
@_date: 2015-06-25 08:31:04
It would also shift the focus of attention away from the implementation back to the spec.
Which I think would indeed be a good thing.
@_date: 2015-06-05 18:23:46


We had full blocks in the past (only ran into a softlimit!), and it was awful. Also, I do not see any reason that in an emergency '1MB is full' scenario, the increase will be less contentious: "Great, the fee market starts to really work!1!, Hey, lets increase to 8MB! No, 20MB!!1! No, 2MB!1! No, lets make Bitcoin generate even more fees with 100kB!!' With much less time to act.
I am not even considering the other 'outside' effects, like media attention ("Bitcoin dead, unable to serve more than 1000 people"), investor fear in the ecosystem etc.
Also note that the proposed hardfork of Bitcoin by Gavin &amp; Co would be well in advance, as the actual stuff will happen only in Mar 2016. Which means it is planned, well phased in and with a large support of the majority of full nodes - which can actually be tested in software.
*Should* there be an emergency situation where reducing the blocksize again, down from 20MB or whatever, arises, this will actually be *simpler*, as decreasing it back down would only be a *softfork*, not a *hardfork*!
@_date: 2015-06-16 08:35:35
There is social engineering hiding behind the big, ill-defined word consensus. Because it allows to paint sabotage (stopping any progress on the blocksize debate) as fuzzy-warm 'consensus'...
@_date: 2015-06-27 11:31:59
Then please enlighten me: How are they *in it* then, and what are they *in it* for?
@_date: 2015-06-24 17:05:56
Not to mention that 'Bitcoin is too complex for you to understand - just invest in/use it and trust us', is a *very dangerous* route to take...
That said, there are *a lot* of knowledgeable people around who haven't memorized merkleblock.cpp line-by-line, but still know the overall architecture, performance and behavior of Bitcoin. You don't need to know every single line of code to understand the overall architecture - seeing the way that some of the core devs argue, this might actually be a hindrance (people who are living in the code seem to get a warped perspective - lacking the overall picture)
Finally, Satoshi himself, whose brainchild this whole shebang is, took a hard look at the scalability of Bitcoin and concluded, that, *yes, it is possible to scale it up*. And we should also see that he wrote most of the core code himself, *before* the other devs took over. Bitcoin's core behavior is to &gt;80% Satoshi's work.
And as I said elsewhere, except from many *opinions*, I haven't seen any new hard data yet invalidating this view. What happened is that suddenly, people cling to 'I need my full node' and 'decentralization means that I am able to run a full node' and made this retroactively a part of what Bitcoin is supposed to be. But instead of arguing the necessary uphill battle for this change in status quo, they socially engineered it.
@_date: 2015-06-12 11:34:05
The problem is that the side who wants to keep the 1MB cap on isn't really *constructive* in the discussion with Gavin.
Whereas he came down repeatedly with his increase schedule. He proposed 20MB+50%/y, then 20MB+40%/y, then only 20MB now.
No constructive input from the other side.
And as you say, hardfork planning is *now*.
@_date: 2015-06-15 07:53:29
Lol. No, I am a very separate human being. Don't be paranoid :D
@_date: 2015-06-30 15:46:41


True, but one can still transact given that risk. 


That doesn't follow for me and apparently also (from the general mindset on the ML) many others. I cannot really prevent your miner or node implementing RBF, though. As I said, incentives...




Indeed, mostly sensible. As it is not about blocksize...


What is SE here?
@_date: 2015-06-18 11:19:14
[I just replied to Greg.](
@_date: 2015-06-14 09:50:49


Big words, nothing else. We could start flinging this and each other and it would be pointless and you know it, too. So I'll stop here.
@_date: 2015-06-27 17:09:15


Who else is going to decide this? Some non-elected, ill-defined consensus comittee?
@_date: 2015-06-09 09:49:37
We had a confirmation time mess when we ran into the too-low default softlimit (was it 250kB) a while ago.
It was horrible. So I very much think their point is *valid*.
@_date: 2015-06-16 10:31:29
Storage can be made more efficient by validating the UTXO set each block (put a hash of a hash tree of all valid outputs into the blockchain) and enforce this as protocol.
You could then go and start a full node with blocks just from a reasonable time in the past (lets say 1 year).
@_date: 2015-06-18 10:29:10
That 'coincidentally' was just me being a bit snarky/sarcastic/whatever. Sorry, all these crazy arguments lately :)
@_date: 2015-06-16 08:55:13
Cool, thank you! I hope this will still be picked up by too. Haven't heard from him yet. He's an opponent in the blocksize debate, but I hope he'll like it too.
@_date: 2015-06-27 19:30:13
Kind of exciting, the whole debate, in that light :D
@_date: 2015-06-12 13:47:08


Fundamentally true, but Bitcoin can be hurt quite badly (maybe until the point of being destroyed) when figuring it out the hard way and staying with the 1MB hardlimit central planning comittee..
And Gavin's fork can't change anything about that consensus either - fundamentally, that is.


There is no infinite supply, it is inherently limited to what the miners want to process.
Which BTW, isn't even the question here. This is already the debate derailing. The question would be whether 20MB is infinite supply. Which it isn't.
@_date: 2015-06-16 09:10:03
It actually shows that miners are not going to extend blocksize to infinity. Isn't that the main 'economic' argument of the 1MB-limiters like you? :-)
@_date: 2015-06-21 09:18:56
Short term congestion != long term congestion.
@_date: 2015-06-24 19:24:21
Says guy with 1month old account...
@_date: 2015-06-12 13:18:09


Or, you'll drive people away. Same way of arguing with centrally planning 'the fee market'.
@_date: 2015-06-30 14:19:13
This might well be me being a non-native English speaker here. 
What I am trying to say is: You are moving the goal posts for the game of Bitcoin, in the sense of what the goal for Bitcoin is - Satoshi's initial and still valid view of mass adoption.
As far as I understand Wikipedia [on this issue]( it does address the situation.
But again, I am not a native speaker, so go ahead and enlighten me if I get it wrong.
@_date: 2015-06-21 10:36:53
Yet they managed to socially engineer the whole discussion as if the people who want to remove the 1MB limit have to prove something.
It is exactly the other way around: Vision for Bitcoin is unlimited blocks, and the scalability of this has been addressed sufficiently by Satoshi already in 2008... everything which came after that was mostly FUD and BS to try to limit Bitcoin.
Greg and the others want to force us into their own centrally-managed economy.
@_date: 2015-06-16 22:12:06
And then risk those giant blocks being orphaned by the rest of the miners.
I read multiple times that China is now &gt;50% of hashpower. If that's true and those are the three big ones making up that &gt;50%, then they'll all *constrain* the block size, because they'll orphan all bigger blocks with their massive power and crappy internet connection.
 There will simply be sane equilibrium if people would actually dare to take their hands off. 
But, nooooo, we need central planning.... /s
@_date: 2015-06-29 18:15:28
In what sense?
@_date: 2015-06-11 10:01:45
Local you mean broadcast? I only visit rt.com the website every so often.
@_date: 2015-06-11 10:15:01
Are you moses, the official interpreter of the word of god (Szabo)?
Get a grip.
@_date: 2015-06-29 00:10:52
I bet the fork is off the table as soon as you actually come forward with something detailed, with numbers. That is what he wanted and still wants, to haggle about the details, and I don't know what exactly it is that prevented this from entering your thick skull, but was and is pretty clear about that.
@_date: 2015-06-05 17:54:49
Quotation, Gavin: 'If you don't like it, find another project.'
So what's the problem here? Its not like he's going to force anyone to run a particular node. The market will decide. It is actually impossible for *any* of the devs to push *anything* through, it all happens voluntarily.
As he said further: 'So much *incentive* to go with the majority.'
There is no one forcing anyone to anything.
@_date: 2015-06-11 11:38:54
As I wrote in another comment, of course the analogy of Bitcoin vs. SMTP only goes so far.
However, the point still stands: SMTP is as much a hub-and-spokes as Bitcoin is with full- and SPV-nodes.
From a very local perspective, you can say that 'All my friends to connect to this full node so it is centralized'. Sure.
But that's not the kind of centralization that is of interest with regards to Bitcoin. The question is whether is a single worldwide authority (or maybe gremium or a few) like there is for the central banks. Or the DNS system.
An in that sense, both Bitcoin and the email system are decentralized.
@_date: 2015-06-28 18:47:54
I hope not. Given the amount of people probably involved and owning *some* Bitcoin, it is amazing how quiet the blocksize debate is, overall.
Just some couple tens of people interacting, basically. With some random dude showing up from time to time.
@_date: 2015-06-12 23:45:15
Or the US. Or China. Or Europe. Same everywhere...
@_date: 2015-06-16 20:45:25
Is he here on reddit? Maybe we can ping him? Is that @_date: 2015-06-28 19:35:38
I think that is the best you can get, though.
@_date: 2015-06-27 21:09:16
What Satoshi has initially said about scalability has in no way been disproven yet, though...
@_date: 2015-06-14 19:18:09
Sometimes, it appears to me that he's doing it as a power game to try to topple Gavin and become new leader.
But with too much stubbornness, he also risks influence in the Bitcoin space.
@_date: 2015-06-12 14:32:08
The same the other way around. 
In any case, you'd still have one chain eventually with 51% of mining power.
It might make the analysis interesting whether you actually need a 51% voting, though.
@_date: 2015-06-10 08:09:09
There is the old MTUT proposal: 
And I made a related self post here on reddit a couple weeks back: 
@_date: 2015-06-02 21:15:14
... and storage can be pruned.
@_date: 2015-06-03 19:30:42
The ones who are against Gavin's proposal are not giving a reasonable plan/roadmap on their own, though.
Which they should be able to, given that they are also for lifting the 1MB limit.
And, as I said, the argument that one can wait until the last minute with the hard fork - can be turned around and said: How about planning the hard fork now and doing a soft fork to cap the blocksize again (less risk) only when an emergency arises?
@_date: 2015-06-01 23:14:24
I saw this link somewhere here on reddit in the comment section, ***very relevant*** in the larger picture:
caveden on bitcointalk.org had some amazing foresight in 2010. Predicting right a way that any block size limit will badly bite us.
@_date: 2015-06-30 22:12:15
The fast relay network. Exactly. And I am sure IBLT or similar will come soon, too. When Gavin has the time to work on it instead of showing goodwill by jumping through the hoops (BIP formalities for something that won't get consensus with block-the-stream anyways etc.) of the blockcripplers.
Worries about orphans are especially FUD, as they just make Miners to make a fee market, something the blockcripplers want to ensure all the time.
And as points out - eventually full nodes will (or could/should) be paid for transaction relay.
@_date: 2015-06-27 19:27:35
So lets have a set of downloads for the different hardfork factions on bitcoin.org then. All neutrally named 'Bitcoin/&lt;hardfork-marker&gt;'!
This is not what is happening, though. People with commit access or website ownership (bitcoin.org) abuse their power!
@_date: 2015-06-17 10:08:39
Yeah, I forgot about old addresses, my fault. With old addresses and a wallet doing merging, the attack indeed makes sense.
Someone else clued me in in this thread.
@_date: 2015-06-08 14:34:23
The ones against it argue that it all will go well if we do it in the last minute. The hard fork for larger size, that is.
HOWEVER, the contention we have now - just because Gavin wants to plan it well in advance - shows clearly that it is insane to expect things to go smoothly in the last minute.
And it should be considered that if Gavin's hardfork plan goes through now, any emergency situation to clamp the blocksize down again can be handled with a *soft*fork - which is easier than a hardfork and thus less risky.
So going with Gavin's growth formula is in a way the most prudent thing to do. He also spent years thinking about and arguing for just this (in a very reasonable manner, one might add), and this is something that other proposals still need to do to be considered for implementation.
I am still not at all convinced that we need a cap at all anymore, but I think we'll see what happens if the blocksize gets lifted.
@_date: 2015-06-15 07:51:38
Yeah, I wasn't trying to say that there are only two. Make as many pages as there are proposals, basically.
@_date: 2015-06-17 01:17:04
I think the correct approach is to discuss the issues down to the parts of data and opinion, separate those two and state each part as such.
I am not opposed to Greg having a different opinion - but I am wary of him hiding his opinion behind stalling tactics. For an otherwise very intelligent fellow, that just reeks of manipulation. If he had the best in mind for Bitcoin, he would lay his data and opinions bare. But he doesn't do that.
@_date: 2015-06-16 20:05:07
Ask I think he's probably planning to collect all the data, do some of his own cross checks (see the other post he made in here) and then make a comprehensive post citing all sources and distributing the bounty. That's my guess. Maybe you can chime in, @_date: 2015-06-28 18:56:06
Proof of stake.
@_date: 2015-06-28 22:31:32
A potential (maybe arguably even ongoing) power abuse that I like to really see addressed are the common points of entry: bitcoin.org and github.com/bitcoin.
For an honest hands-off approach to the hard fork debate, they should give all the popular hard forks equal space. 
And it should be made clear that it is up to the users (including the miners) to build a working, successful consensus network.
@_date: 2015-06-02 00:04:56
Relevant: 
This is an issue known since years. As you can see, people in *2010* argued that a blocksize limit is dangerous.
Kind of hard to accuse Gavin of having the wrong incentives when he simply wants to follow the initial goals of Bitcoin, isn't it?
@_date: 2015-06-11 13:03:55


Yet you do it all the time. It's ok, though, because semantics are important.
Satoshi is certainly valid here. However, whether 6000 nodes is currently not enough in the sense of his paper is *very* disputable.
In any case, as others have pointed out, overpowering an SPV in the end might just mean controlling the internet connection of the SPV node - a very different attack scenario than centralization.
@_date: 2015-06-16 10:17:52
Care to explain the 32MB hard limit? I thought this is going for market based...
@_date: 2015-06-12 13:19:33


Bitcoin has no bank accounts.
@_date: 2015-06-01 22:41:43
Gavin talked about that:
I agree, having a live plot of that figure in clients could be helpful - or just a couple summary figures giving a birds' eye view.
@_date: 2015-06-01 23:53:00


No. This argument gets stronger if you turn it around. Prepare to increase the blocksize now, with a *hard fork* effective in 2016.
If SHTF, you can as well go back down with a *soft fork*.
A *soft fork is easier than a hard fork*, so this makes it all the more reasonable to do it the way Gavin intends.
@_date: 2015-06-28 21:38:02
What is a geodesic structure in this context?
@_date: 2015-06-12 14:25:07
Huh? So you want to exclude the price per transaction from the discussion of supply and demands of transactions, although these concepts are intrinsically linked!?
That doesn't make sense.
@_date: 2015-06-10 10:32:24


There is a fee market already. The effective counterproposal is putting in a hard wall with regards to the blocksize and enforcing artificial scarcity. Which won't go well for Bitcoin.
@_date: 2015-06-24 22:14:47
OTOH two implementations would vote the faulty one out.
@_date: 2015-06-02 00:35:10
Well, maybe because caveden was pretty much spot-on with his analysis?!
@_date: 2015-06-14 09:40:25
The same can be said for the people putting a 1MB hard cap on.
In that sense, you must like proposal - market based. I like it, too, but notice that it is open-ended...
@_date: 2015-06-12 11:21:33
As I said elsewhere: XT will be the pressure-relief valve in time (around or after March 2016) when blocks get full and transactions slow.
By doing the fork now - but which will only be effective starting in March2016 - he actually gives people a choice to place their bets and votes before SHTF.
There is no reason to believe that the devs who are stalling now will come to any kind of consensus or agreement in the short-term, when Bitcoin is saturating. Gavin's fork will be ready and widely deployed by then, and will take over.
Consensus is created by economic forces.
@_date: 2015-06-28 14:43:12
Maybe the Bitcoin/XT people should ask the Bitcoin/BS people to do at least the following, in the name of honesty:
Equally represent the different nodes on bitcoin.org and github.com/bitcoin (e.g. in brand neutral branches).
Make it clear that running different forks on the network will be two different coins existing.
Let it be fully up to the users to decide which bitcoind/btcd whatever to run. State that being on the wrong fork might involve losing money. State that no one will know what the wrong fork is.
Of course, users have that chance already. But in the name of honesty and non-decentralized openness, at least the XT and BS branches should be represented neutrally and fairly on all brand related sites.
@_date: 2015-06-04 08:51:33
[That's existing and working already.](
@_date: 2015-06-30 12:00:03
Ok. First of all, my point isn't that there will ever be a perfect landscape of miners. What I am arguing for is a *good enough*. And I think we do indeed have that. 
That said, a 100% anonymous miner will also be a small one. A big one will have a warehouse (at least) full of mining hardware and a corresponding electricity bill - hard to hide.
A large miner - or pool operator - will be visible. And for the most part behave in a way that will work in Bitcoin's favor, due to incentives.
And a small one cannot really do much damage.
@_date: 2015-06-16 22:04:19
Ok. If you go and promote something and the discussion is clearly going nowhere because the other party stalls, are you then taken the silly burden of trying to fulfill technicalities when *everyone* knows the BIP is going to end nowhere?
Requesting to fill out a BIP is a deflection tactic.
@_date: 2015-06-28 23:36:10
Understood. And I think Gavin's proposal is very reasonable in that regard.  It would also be about right for the end game numbers of a successful Bitcoin with LN on top. See the other post I made in here.
@_date: 2015-06-16 20:39:42
Well, you can, if you assume conflict of interest (Blockstream) or egos (something like: Greg envies Gavin as the de-facto leader of Bitcoin and wants to topple him by his stalling tactics, waiting for him to come down to his knees and beg for a blocksize increase).
I am still undecided between the two. They certainly do not sound nice. But as I said, lack of alternative explanations. Follow the money, some say. OTOH, bright or driven nerds often have weird egos. Been there, done that. When I was 15, though...
That said, I think your posts are very valuable, because you make calm and sound arguments why 1MB limiting is ridiculous. We need that in here to point those on the 1MB side that can be swayed to ideas and lines of argument.
I don't even really know why I lately spent a lot of time here on reddit trying to counter arguments of the 1MB-blockistas. I guess I really fear that social engineering and blocking might be too successful for Bitcoin to survive.
@_date: 2015-06-19 14:36:16
.. and the comment that O( n ^ 2) applying to the whole network doesn't mean anything for a single user to run or not run a full node (which is *O(n )* even if the whole network scales O(n ^ 2) ) applies, too.
It is insane how often this stuff needs to be repeated.
@_date: 2015-06-22 21:00:28
Especially since their special-purpose double-SHA256 mining hardware will be pretty much worthless if Bitcoin fails.
I really don't get the whole 'those evil miners gonna screw us' argument... if we can't trust (the majority of) the miners, we have much bigger problems anyways.... this is a Bitcoin core assumption.
@_date: 2015-06-11 21:11:26
Only if you reject going the UTXO commitment route. Because then you take out one n from the nodes and end up with O(n), which is a lot more benign. Very doable. I also don't think having all transactions available at all times is something that is a fixed meta-goal of Bitcoin. Satoshi himselved wanted to do pruning. I am fine if I have a validated, coalesced UTXO set and can be sure it is the right one.
Also, there is still a lot of technological progress.
But I am not disagreeing - try to keep it small and transactions off-chain is certainly worthwhile.
@_date: 2015-06-30 23:18:43
You say decentralization is too low. The market doesn't say yet that it is too low. I understand that it might be helpful to put in some safeguards that prevent sudden unplanned changes. Like a miner being hacked and suddenly producing GB-sized blocks. But again, why centrally set a hard cap?
@_date: 2015-06-30 11:45:46
Less easy to be rigged as delivery of 'physical' is *much* easier.
@_date: 2015-06-27 22:30:26
The reason is that there is no clear, plannable way forward. And it looks like there is obstruction going on, some apparently trying to prevent any change to the 1MB limit.
People are worried, rightfully so IMO, that this will stop or kill Bitcoin.
@_date: 2015-06-11 21:04:05
Meta protocol: Longest chain (highest sum of work) in terms of SHA256 hash power.
Because if you do not follow that chain, you are risking 51% attack exposure. 
[And saying 'oh, actually Bitcoin has always been meant to be scrypt mined' is hopefully so far into insanity that basically everyone is going reject it.]
@_date: 2015-06-14 09:24:20
1MB x 1.4^ (2016-2010) = 7.5 MB
@_date: 2015-06-14 10:12:54
Per node, cost is O(n) with n transactions. With maybe a small log n in there for db lookups and similar.
[See this discussion I had with where he seems to stick to a really silly interpretation of Metcalfe's law: That in a n-user-sized network, all nodes will interact with all n-peers and thus the traffic goes like O(n ^ 2). 
Silly, because I am neither having bank transfers to and from everyone with a bank account, nor am I going to phone everyone in the phone book.
I think for a less-noisy blocksize discussion, someone should maybe start reddit posts about specific subtopics of the blocksize debate?
The O(n ^2 ) contention is a good point to start.
@_date: 2015-06-14 21:16:34
That's a single page - I was thinking more along the lines of  giving each side their own space to present the best argument.
For that,  I think separating into different pages would help. 
@_date: 2015-06-28 22:22:57
If you don't have the majority of the hash power behind your chain fork, you seriously risk an 51% attack. This will probably keep people mostly on a single chain. That is the power of the miners.
A hard fork that is just used by 51% of users and 51% of miners will be successful, if intended to be so.
Gavin inserting a 75% supermajority check into his hard fork code is playing it the extra nice way (as he should).
@_date: 2015-06-30 10:12:58


is not attacking *your age*, he's attacking *the age of your reddit* account. Get a grip. He's pointing out that you are more likely to be a sock puppet. No need to bring your religion into the discussion or anything else, either.
He would also be right in pointing out that those behaviors (misunderstanding 
the simple difference between your age and the age of the account, and adding unnecessary chaff like you being Islamic and then trying to derail the debate to a weird link to terrorism) indeed point to you being a troll.
@_date: 2015-06-16 09:17:34
They might exist but in the very heated debates here on reddit, they do seem to be in the minority. Also, have a look at [this.](
@_date: 2015-06-10 08:01:15
Yup, it's certainly a couple of trade-offs to consider... but nothing impossible.
@_date: 2015-06-11 10:59:25
Ah, ok, I see! Didn't even know about russian.rt.com. Are you Russian?
EDIT: coinspotru -&gt; coinspot.ru -&gt; Probably? :D
@_date: 2015-06-11 22:58:58
That's why I said 'in terms of hash power'.
 I admit, I stated it a bit confusing... I meant longest in the sense of 'longest in hash power metric', not in 'number of bytes in all blocks'.


True, but there is very strong incentive to stay on the stronger chain that can't be 51% attacked easily.
@_date: 2015-06-30 09:17:35


This is the real issue: Personal goals are disguised by some folks with technical knowledge as technical limitations, technical necessities or just protocol functionality.
Very dangerous centralization of opinion. And I hate that behavior to the core (no pun). And honestly, Gavin has the best track record of not doing this so far.
@_date: 2015-06-09 19:22:07
Your post got me thinking... :D
If this is indeed a power game of Greg vs. Gavin, I'd expect a couple of the other developers to switch sides to Gavin soon (if he wins, which I also expect), causing a landslide and eventually Gavin will have 'forced' consensus to 8MB on this issue.
We'll see, but this is what I am predicting. That the other devs are so quiet right now suggests to me that they are pondering whether it might be time to actually come out in supporting Gavin.
This would also be the most positive outcome of this whole fight, as it could change the dynamics to make the XT fork irrelevant as 8MB would be included in core.
@_date: 2015-06-30 19:56:41
@_date: 2015-06-21 10:29:45
In the end, I'd rather go with no one and form my own opinion, thank you very much. But with regards to blocksize, this opinion has a very large overlap with Gavin.
That said, Mike Hearn did scalability engineering before @ Google and has probably the strongest credentials if you are looking for 'formal qualification' regarding this issue. 
@_date: 2015-06-09 16:24:44
I think his point is that Mike's not ideologically stuck on having as much as possible on-chain the same way that some others seem to be stuck to have as much as possible off-chain.
@_date: 2015-06-18 11:32:28
The definition of decentralization in the article is still so diffuse in the context of the very technical discussion of Bitcoin that I fail to see how this article adds anything to the discussion.
I fail to see how even a scaled up, more concentrated Bitcoin is still not decentralized. Because few full, big nodes that many small SPVs connect to is a hubs and spokes topology, and this is *decentralized* for any meaningful, technical and social definitions of decentralized.
P2P filesharing networks are called decentralized, even if they have a subset of distributed (and decentralized!) supernodes having more responsibilities in the network.
@_date: 2015-06-18 21:28:20


Yes, of course. But an informed conjecture. It is impossible to point out stalling explicitely, because the tactic to choose for stalling is do things like evasion, ignorance, and derailing.
I have seen Greg arguing, and I have been involved with arguments with him. I have a really hard time now not seeing what he does as *stalling*.


I am not saying Blockstream is the only motivation. I think there are many different arguments against touching the 1MB blocksize. But I think they are clearly outweighted by Gavin's arguments, and, yes, also a hard fork.


Yes, I heard that. But again, Greg's behavior makes me now think otherwise. It might be an Ego fight with Gavin, too.  I just don't know.


No rushing when this issue has been known and ignored for years. And when it needs to be addressed in time.
@_date: 2015-06-16 08:51:55
It is up to the people who want to redeclare a temporary stopgap measure as a core economic tool to prove their case.
@_date: 2015-06-17 10:10:05
Yes, it was, actually just 'think of putting money on old addresses' would have probably been enough to pull me out of my pool of brainfarts :D
@_date: 2015-06-08 14:26:32
@_date: 2015-06-10 08:32:45
Look at how past 1MB blocks filled and how the blocksize grew.
Also ask yourself if you want to constrain this growth for no real reason.
@_date: 2015-06-30 23:57:55


Productive discussion about lifting the limit - that was only meant as a
temporary anti-spam measure and *always* meant to be removed - is being stalled. 
That is indeed different from *proposing* to not lift the limit, because it
allows the block-the-stream guys to hide behind a wall of passivity. 












Mhhmm. Unproven, pie in the sky consensus algorithms that magically all make
us agree? Are you believing in unicorns, too?
As long as there are humans involved, there will be disagreements. No
algorithm can change that. 
Oh, except one. A singular agreement on longest chain of SHA256 proof of
work wins. Which brings me to the hard fork: It still works under that
agreement! And it is something which is completely inert in code until a supermajority 
of the participants agree to go along with it!
And *anyone* can make a hard fork. I can fork the code really hard and run
lots of RasPis with 10kB blockchains. It is only that Gavin's actually has a
*pretty* good chance of success! If he didn't have a chance for success,
there wouldn't be anything to worry about for the blocklimiters. In
essence, they are indirectly confirming that Gavin's proposal is the right way to go
forward. 
[Or as someone else is saying it...](










A soft fork means that only the miners need to be involved. Something you
cannot do when currently all full nodes reject blocks above 1MB. You can try
to work around that with Rube Goldbergesque contraptions of extension blocks
or similar, derailing BS.








Who wins a hard forks are the meta level for reaching consensus when the
code level doesn't allow it anymore. Any block size increase would need a
hard fork. You cannot avoid a hard fork at all when you want to do *any*
change to the 1MB limit.








No guns an no force involved, just people voluntarily associating with Gavin's
perspective instead of Adam's and Greg's in how Bitcoin should go forward.
You are just arguing against it because you fear its success. Success which
takes a lot of potential revenue away from settlement providers (banks) who want to
provide 'Bitcoins' to you. 


















Interesting rhetorics. So Gavin has no idea of what Satoshi intended
(although he made scaling pretty clear and the topic of many mailing list
posts from early on and clearly showed that it is a) intended and b) very
possible with Bitcoin).
But somehow Satoshi *might* have something different in mind? And thus gives
Greg's arguments support? Give me a fucking break.


















Again, talk about force, even though that is pretty much impossible with a free
association network like Bitcoin. And talking about scaling and limits and
laws: The internet from 2002 could have supported the bandwidth for a Bitcoin with
gigatransactions/day. Yes, not under your desk on a RasPi - but in larger
datacenters. Exactly as Sathoshi intended it.














Using the undefined word 'decentralization' as a scare tactic...






The best approach is to remove the block cap and maybe put in some safety
circuit breakers against a single rough mining node (sudden GB blocks by a
hacked miner or so). 
There is a free market between miners and users. No one should centrally
plan a limit for that.




















A hard fork is the free market in action, you fool.










Not at all. Just a hard fork.














Free market, free market. But centrally planned hard cap, hard cap! How the
heck can one keep this cognitive dissonance for so long?






Funny logic all that.
Ok. This is all so tiring. Pointing out the BS again, and again, and again. 
Is this the tactic now? To wear the blocksize increase supporters down with
BS and propaganda? I gotta sleep now.
@_date: 2015-06-28 11:25:16
How do you make that invalid block, then?
@_date: 2015-06-24 21:35:50
Yep, exactly this!
@_date: 2015-06-24 18:04:34
You should think that 20ct transaction through: The 1MB limit is a *hard wall* right now. But the demand for transactions is inelastic.  With any significant, sudden usage, that will lead to a *strong* price spike (could be very well above 20ct), and then people, as they see that Bitcoin has been crippled and cannot scaled, want to get out, increasing transaction demand temporarily even further.
More likely is that Bitcoin will wither away as people know that it is just a toy system with 3txn/s.
@_date: 2015-06-09 18:50:00


Even if it needs a complete recalculation of all hashes every so often, I don't see why it would make this strategy impossible. Especially since this would be a set of eventually quasi-constant size (coalescing hashes of old UTXOs would be put into the tree as is).
That said, there is no reason to implement it in a way that you couldn't reuse most parts of the hash tree (or trie or whatever is going to be used) when it changes. 
@_date: 2015-06-30 23:07:41
The chinese miners can limit the effective blocksize however much they wish though.
Hypothetically, if we would go and all have awesome consensus on removing the block cap altogether tomorrow, blocks would still not grow above 8MB.
Because they have all the power (&gt;50%) of not letting them grow.
@_date: 2015-06-12 23:08:53
Problem is that it is an informed guess about a solution space that is so big that you'll never, ever find THE right solution. It will be a trade-off in any case.
@_date: 2015-06-12 14:17:12
Mining is the core principle by which Bitcoin operates right now.
Whatever 51% of miners fake is 'consensus'.
Our two biggest diffuse words in this whole discussion everywhere... 'consensus' and 'decentralization'.
@_date: 2015-06-24 19:06:59
The 101 of Bitcoin :-)
@_date: 2015-06-11 20:15:24
See. You can go, take the code from github/bitcoin, change the blocksize to 100kB, run a full node, and with one of the next blocks, you'll be running your own Bitcoin fork - on a single island node.
If you go and talk to friends and persuade them to also run with blocksize 100kB, you made your own little working true Bitcoin fork, congratulations.
But economic forces and basically being *reasonable* prevents people from doing that too much. That's the way Bitcoin consensus establishes. And in the end, having 50% of the raw SHA256 hash power counts. Because you don't want to be on the fork that can be 51% attacked.
@_date: 2015-06-27 21:26:26
Of course!
See how the very, VERY reasonable request for compromising on just 8MB is handled (and rejected) on the ML. By Adam and Peter.
Stuff is ridiculous. The only thing that really bugs me is that Gavin is so nice that apparently the devs can keep bitcoin.org and the github.com/bitcoin brand names.
All of those, but especially the Bitcoin brand clearly belongs to Gavins fork. 
If there would be any honesty left, it would be time now to create different branches on github and have bitcoin.org neutrally state all possible choices (two so far, XT and 1MB).
@_date: 2015-06-12 13:15:31
Regardless of what happens to Bitcoin, 3txn crippling or not, you'll get law enforcement and policymakers interested in the system if they have a reason to, worldwide. They already do. That's *unavoidable*. That's them doing their job. The question then becomes how far their reach can become technically.
With 3txn/s, it is much easier though to black- and whitelist, than it is with 20GB blocks. Stopping a transaction going through on Coinbase is much easier than on the blockchain.
Also, a small 3txn/s settlement system will be out of most of the public's view and will be much easier to regulate. If everyone directly interacts with it, many more people will notice.
@_date: 2015-06-30 22:41:58
Apology accepted.
I was talking about first-seen-safe RBF, not child-pays-for-parent.
A maybe even better way would be specify in script which output is meant to be able to be bumped.
It would need to be optional, because it somewhat impacts privacy of a user.
@_date: 2015-06-22 16:56:34
If a centralized Bitcoin is oh-so-worthless like the blocklimiters assert, Miners will have a strong incentive to *not make it worthless*.
Also read what posted on the blocksize issue.
Gavin's proposal is a compromise, but I fear it is a compromise with the cryptopoliticians...
 
@_date: 2015-06-28 15:26:58
    Are you distrusting the miners?




Agreed so far...


It maybe quite rational for miners to try to discover the switching cost of 
users of the network, whether that's testing tolerance for centralisation or
testing tolerance for small blocks and high fees.
The latter they don't test, we have data on that, so that point is moot. The former, I'd rather
reformulate as the tolerance of the users for a centrally planned limit. 


miner's honest. If you cut those links the system will probably function 
worse, and maybe fail. Dont forget that very little about the system is 
arbitrary - it is serving a function to keep the system incentives in  
balance, to keep the system secure, decentralised and policy neutral.
Yes, but this alignment is not built around the 1MB limit. That is, indeed, 
@_date: 2015-06-28 22:03:35
I see. Thank you.
@_date: 2015-06-27 14:14:27


*At the moment*, they clearly are.
That is why expect we won't see them, even if we will have no hard cap at all.
Miners have an interest in this succeeding, too!
@_date: 2015-06-25 17:39:10
And a SPV++ or 'full node (-)' could be done with UTXO commitments...
@_date: 2015-06-12 23:32:56
Ok, that sounds about right. My well-trained order-of-magnitude gut-estimator was telling me this must be off :D
It is obviously still a lot, though. I agree, I agree it would be good to have less mining or make it as useful as possible (use the heat productively).
@_date: 2015-06-27 21:36:52
Calling it core is a problem in itself. Core is a brand.
And not (eventually) having representation for Gavin's hard fork proposal on github or bitcoin.org is untenable.
At least, to be honest, those sites should have different branches for the different coins (github) or equal space for representing the different choices (bitcoin.org).
@_date: 2015-06-29 18:41:58


FUD. And a strange thing that some of the devs have for taking more responsibility than they should bear (they should care about the code, not the ecosystem - economical incentives of everyone involved in Bitcoin will properly take care of the latter). But on the other hand, responsibility usually comes with power, so maybe that is what they are actually after.
@_date: 2015-06-15 08:59:40
Great post. Now you just need to get it into the ears of Greg and Adam, and more importantly into their minds (the hard part...).
@_date: 2015-06-18 00:00:02


TotalWork/day != Work/(fullnode * day)


Why are there two n's now? I thought they are running an SPV client *instead*?
(And in that case it would be O(n log n ) ... )
@_date: 2015-06-12 15:58:45
2x all three months?
If so, sounds good!
I am actually looking into IRC right now:


Looks positive. Keep the discussion going!
@_date: 2015-06-12 15:29:07
Interesting is, though, that is for an protocol-wise open-ended  blocksize, but all we hear is how all core devs except for Gavin and Mike are all for staying with 1MB supposedly....
IMO, his proposal is technically even better than Gavin's, but Gavin has the chance of winning 'consensus'. 
Would be glad if and could start talking, unless I missed something, they both don't seem to be too ego-driven.
The smaller initial limit of 2MB could be ok as he's also proposing an earlier date. Maybe they can haggle to get more time to switch (March 2016) but also a higher limit?
How about combining 20MB + 40%/y as a hard limit + this proposal? I.E. always taking the lower limit of the two?
@_date: 2015-06-15 22:12:57
Hmm, scribd wants some credentials, too... so I first need to make an account there.
For github, I want to create an account on github first, because I'd like to keep my reddit ID and real life separate as much as I can. Will do if there is some resonance.
 
@_date: 2015-06-27 13:12:19
He thinks that a sidechain that has value over Bitcoin - *for example a high transaction rate*, could be taking over all transactions from Bitcoin and eventually collapse the original network. And this sidechain could well be centralized - or, said in a fancy, less obvious way: 'federated'.
But maybe might chime in, too.
@_date: 2015-06-30 21:38:56


Only trivial if you do not see the completely different landscape a permanently limited blocksize would cause in the Bitcoin ecosystem. The status quo is an effectively unlimited blocksize - except by the existing fee market between users and miners.
A damaged landscape I might add.
For example, I can easily see Miner's running on lots and lots (billions) of tiny transaction fees in the end game. I can't see that so much with 1MB blocks. I also cannot see any more full nodes in such a scenario than with an uncapped or at least practically unlimited blocksize.
Who the heck would run a full node if it is just about 'settling' transactions between a few major blockstream banks?
That's right. No one. That network probably wouldn't even exist, as Bitcoin would fail.
@_date: 2014-12-20 11:28:59
I think the block size limit actually depresses the market somewhat - because there is no clear direction in sight yet. I think if Gavin would go and implement his 50%/y increase starting sometime in the near future (couple months), a lot more people would see that Bitcoin has no inherent limits anymore to eventually reach VISA scale.
@_date: 2014-07-26 17:01:58
How do they deal with double-spends, then?
@_date: 2015-06-17 12:02:54


I rather think that 1MB will limit Bitcoin enough to kill it early, rather than even creating any kind of centralized system, because for that 1MB would need to be somewhat successful in the long term, which I cannot see happening. At most, an off-chain settlement system by some company will become successful by abusing the Bitcoin brand name (we are Bitcoin!!1!), get the majority of the potential user base, abandon Bitcoin and let it die and then, as the new Paypal, will tell everyone 'This is just how it was meant to be, Bitcoin could never have scaled to higher levels.'.
 
Bitcoin with unlimited or at least higher blocksize might have the *danger* of becoming 'a centralized system' (for odd definitions of centralized: It would still at least some large nodes distributed over the globe - there is no center), but it would definitely allow Bitcoin to grow and continue to thrive.
1MB limiting is at best trying to kill Bitcoin early in fear of its success.
@_date: 2015-06-02 13:02:18
Well, how about doing as Gavin proposed then, having the growing blocksize limit, BUT also making a very reasonable default adaptive soft limit, like 1.5x average since last 2016 blocks?
The *majority* of miners would then need to *maliciously* increase the blocksize to insane values.
That should solve the problem and make everyone happy.
@_date: 2015-06-14 09:20:02
I think the argument goes as follows:
Create a sidechain that has massive improvement over a limited Bitcoin. Such as in transaction rate and instant settlement. Make it to some extend centrally controlled, let the payout go to company XYZ.
Get mass adoption on your sidechain. Advertise it as 'decentralized Bitcoin'.
Shut down the Bitcoin link. Keep the user base. Kill Bitcoin.
That's also I believe about the argument why people who have certain company affiliations and seem to want to limit Bitcoin more than necessary in certain regards are being suspected of misaligned incentives...
@_date: 2015-06-03 19:42:06
After pruning.
@_date: 2015-06-12 10:21:25


That chain is at constant risk of 51% attack, though. With 51%, even so by single miners. That will kill the smaller chain (in terms of hash power) be it XT or core.
@_date: 2015-06-14 10:28:15
Then Jeff's proposal must be good compromise in your eyes, too, or not?
Because if 51% of the miners start to do shenanigans, we have much bigger problems already. And the single spamming entity is prevented with the 3 months view. If a single miner gets hacked (or becomes nefarious) and produced bloated blocks,  he can only do so much damage.
@_date: 2015-06-12 15:36:01


Hmm, but why do you want the extension blocks if the main chain can extend itself?
I'd be fine with either Gavin's or Jeff's proposal, or as I said, the subset of Gavin's 20MB+40%/year and this one.
Jeff's has the advantage of hopefully killing the hardfork contention for good.
@_date: 2014-05-08 08:37:50
Thats an interesting point.
One argument one could make here would be that a heat pumps need connections to a heat reservoir and are thusmore complex/cumbersome than a space heater. But that argument also admittedly has some weakness, as miners are complex in themselves. However, they do not need a transfer medium and they do not need plumbing etc. It would be just solid state electronics, something we know more and more how to mass produce very cheaply.
The question then would be whether there is a niche for simple space heaters but not heat pumps. So far, there is a huge number of space heaters installed that are not heat pumps. But you're right it might and will change, at least to an extent.
The devices in the remaining niche would then compete against a facility (centralized mining) that wouldn't use the heat at all. The central miners then have the economy of scale, but the niche of space heaters the slight competitive advantage... 
Is that enough? Honestly, I don't know. An interesting question, though :D
@_date: 2015-06-16 13:40:40
[Wikipedia maybe?](
In very few words and in the context of CS, it means that a certain dependent variable (CPU time, bandwidth, memory) does not grow with a rate faster than what could be described with what's in the O(...) brackets.
So O(n ^ 2) per node for bandwidth would mean that it scales quadratically with the number of users. Meaning for every new user, the bandwidth would be described with something like x * (n*n) for some constant x (and the constant would include all the specifics of the hardware etc.).
O(n) - the (mostly, there might be a log n there or two) correct formula says that per node bandwidth only growths with some constant x times n users.
@_date: 2015-06-01 23:57:31
Relevant: 
From *frigging 2010*, nonetheless.
@_date: 2015-06-19 09:10:02
That would be O(n ^ 2 ) for the *whole network* then, though, and the interesting metric for a user to run a full node or not is what the scaling behavior *per full node* is.
@_date: 2015-06-12 13:04:49
The hardfork will only happen with 'consensus'. I am pretty sure is putting an if-majority-of-miners-agrees into the code.
@_date: 2014-12-13 14:16:59
I think Moore's law is unlikely to stay true for the next 12 years, as he assumed in his blog post. If there is sub-Moore-growth, there will  be  a bit more centralization of nodes. I guess it could still turn out to be acceptable, though.
@_date: 2015-06-18 11:34:31
[I replied to this.](
@_date: 2015-06-12 14:16:06
Where is the pivot?
@_date: 2015-06-11 09:57:47


No, he didn't, and he also didn't expect everyone on Rarotonga to run a full node under their desks at home. He also intended for Bitcoin to *scale* - and was very confident that there are no meaningful inherent limits (which I actually agree with) to very high transaction rate.
Now a bunch of people come along and say that '1MB is conservative'. No, it isn't. *Sticking to the original vision for Bitcoin* is conservative. Meaning dynamic growth of blocks, without hitting an artificial wall.
@_date: 2015-06-09 17:58:08
The 250kB softlimit can aptly be described as a crisis...
@_date: 2015-06-06 14:55:01


Have you seen a workable proposal that isn't consisting of 'lets just rise it in time (i.e. last minute)?'
@_date: 2015-06-19 14:00:29
So you see the inefficiency in PoW? But that part creates the consensus/agreement. 
I see Bitcoin's endgame as many different companies and other entities in different parts of the planet owning many full nodes, and the PoW is still meaningful because it allows agreement for all those parties. The distributed database is different in the sense that it would span *across* what is VISA and Mastercard now.
@_date: 2015-06-02 00:12:24
Centralization is VERY diffuse. I hear this word thrown around all the time.
Consider that it might work against you: Do you want to centrally plan blocksize?
@_date: 2015-06-27 19:35:14
There is no non-decision on this, though. Changing the limit is a decision, but keeping it in place is a decision, too.
That drives this wedge.
@_date: 2015-06-05 18:28:19
Exactly. There is economically driven meta-consensus yet people behave as if any change can only happen with full consent of all developers. I think it is the other way around: Any change could happen at any time, but economic forces keep people (mostly) reasonable and trying to work together. With the blocksize debate, which is an old story btw. that people seemingly try to ignore, some other people/devs got stubborn and so someone has to and will give eventually. The pain could be smaller if the non-Gavin side had been more constructive in the discussion.
Gavin has been *very reasonable* so far, looking for constructive input to his blocksize planning for years, without anything good coming back to him.
I think it is indeed time to do the fork on BitcoinXT and let the market work it out.
@_date: 2015-06-30 18:43:26
@_date: 2015-06-10 10:48:21
Ok, but that's not really helping the UTXO set size growth very much, or is it?
@_date: 2015-06-24 17:22:39
Greg, arguably. Pressed by Gavin on concrete counter proposals *with numbers*, he simply stalled the whole thing. And Gavin has been at this problem for *years* now.
@_date: 2015-06-11 19:43:14
Can you explain the O(n^2 )?
I see O(mn), with n nodes and m block size. But no n^2 .
@_date: 2015-06-13 13:55:23


You say this as if it is some proven truth.


Large expenses and losses?


And as Gavin pointed out, you have to wait for those. Reddit is 'free' to you and me, too. What is your point there?
EDIT: And it should be pointed out, again, again, and again: Blocks are not full yet! The whole 'blocks are always naturally going to be full' argument is BS.
@_date: 2015-06-16 08:29:36
... and with 1MB, it will plateau very soon. And then decay...
@_date: 2015-06-16 08:39:51


Also, look at [this]( where Adam in a discussion with me eventually agrees that *per node, which is the important metric, it is mostly O(n)* scaling and [this]( where he repeats the O(n ^ 2) scare tactics again, at a later time!
I am now having a hard time not seeing this as nefarious.
@_date: 2014-05-07 09:25:15
Read this, and it gets scary:
I don't know why so many people still trust Ghash.IO.
@_date: 2014-12-20 17:35:48
Since when is Scalability orthogonal to centralization?
There is all this talk here exactly because people think they are closely related.
@_date: 2015-06-17 23:54:38
Ok. Lets say you are going to a party on new years eve. At 0:00, you go and
greet and wish the best for next year to everyone who is around you. Because
it is customs, everyone else does the same. Lets further assume that you are
in a single room with everyone else, and for the greetings to happen
orderly, only one person can greet another person at any single time.
Now, doing a greeting takes time, lets say d seconds.
For a party with 10 people, you'd greet 9 others. Assuming each greeting
takes a second, and everyone else would do
the same, so there would be 10 people each greeting 9 people, 10 x 9 == 90
greetings, taking just 1.5 minutes.
Now assume you are going to a bigger party instead, with 100 guests.
Naively, you might assume something along the lines of: 'Ok, ten greeting
each takes 1.5min, so 100 must take 15min.'.
But at the party with 100 guests, one hundred people are greeting 99 other
guests each, so a total of 9900 greetings. And that takes 2h 45min.
The reason being, of course, that for n guests, it takes 
d * n * (n-1)
seconds to do the greetings. And the term n * (n-1) becomes big rather
 
So this process of doing new years greetings is something that clearly
doesn't really work for large crowds. If you'd train everyone who goes to
the party to talk really, really quickly (reducing d), you might get the greetings
done within the assumed 15min.
But then, this process would still break if you'd go to a crowd of a
Computer science people would say it is an algorithm that *scales* rather
badly. They say it *is in O ( n ^ 2 ).*
Now, clearly, d * n * (n-1) is not the same as n * n for almost all choices
of t and n.
But for very large n, n * (n-1) gets very close to just n ^ 2. And the
factor t, the time it takes to shout the greeting, isn't really affecting so
much the problem that at some large crowd size, it becomes infeasible to use
the above greeting algorithm.
And this is captured in O( n ^ 2 ). It only gives you the overall, rough
behaviour of a scheme or algorithm, without the unnecessary details.
Consider that algorithms on a computer can be *very complex*, and there can
be many such factors and weird numbers in front of the n^2 part.
This so called *big O notation* requires some more (but still relatively
simple) math of which the specifics are not relevant to you as a 5 year old.
[But you can read more on Wikipedia, for
Ok. So now lets turn to the topic of Bitcoin, or rather the topic of its 
The Bitcoin network consists of a set of nodes that send messages to each
other to inform each other about new transactions to process. For Bitcoin to
work as proper money, the whole network has to somehow agree on what
transactions have been done, and which have not.
There is now of course a lot of complexity and thought into
the specifics on how this all works, but what is interesting in the context
of this discussion is that somehow, *all full nodes need to learn about all
transactions from all other full nodes*. 
And here we can go back to the above picture of the party on new year's eve.
Imagine a Bitcoin full node is a party guest, and greeting means passing a
transaction on to others.
If Bitcoin would work like the party, all n nodes would announce a new
transaction (the greeting), to everyone else.
As the CS guy says: This is O(n ^2 ), this doesn't scale.
But the problem is, that this is the wrong picture of the Bitcoin network.
In the equivalent picture from above, a Bitcoin full node would make a
greeting (forward a transaction) to every single other node in the network.
And I think you can see the problem here: There is no party going on among
the full nodes (they are not sentient yet), and they do not care whether
they receive the SAME transaction from every other node personally.
Rather, they just care about somehow receiving it at all.
And so, the Bitcoin network has been designed in an more efficient manner.
Basically, a transaction is *gossiped* across the network. If a node hears
a valid transaction, it just forwards it to whatever other neighboring,
connected node has not seen it yet.
And if you now think about it, a network of full nodes that are all somehow
connected just needs 'n gossipings' to get a message to everyone in the
That is O(n) behavior, and that is an algorithm that scales a lot better.
That's *linear* behavior.
Now, and this might not be exactly ELI5 anymore, the situation is of course
more complex. Because full nodes are not the same as users in Bitcoin, one actually
needs to introduce another variable describing the number of users in the
network (let that be u) and the number of transactions a typical user makes
in a given period of time, let that be t.
Could it be that somehow,
the load on the network, the number of transactions to push around, is
something that grows too fast with more users joining, or with the number of
transactions going up?
It can be assumed that every one of the u users wants to make a certain
average number of transactions per day, t, and so the number of transactions
going into the Bitcoin network scales like O(u * t).
The *total number of transaction transmissions* in the network then are in O(n * u * t),
because, as we discussed above, every full node sees every transaction.
If you assume that every user just has to have a node under his desk, it is
n == u, and so, the scaling of the whole Bitcoin network would be O(u ^ 2 *
t), which is quadratic, CS guy doesn't like it, doesn't scale, forget it!
But wait a bit! First of all, the metric that we used here was for the
*whole* network. In the context of the blocksize debate, some people worry
about decentralization, and have some (unstated, vague and ill-defined, one might add) idea on
how many full nodes there ought to be in the network. They argue that a full
node might become too expensive to run in a non-scaling Bitcoin network.
The metric, though, that every user would use to decide whether to run a
node or not, is not in O(u ^ 2 * t), the whole network, it is rather the
load *per node*, which is in O(u * t).
And secondly, but equally important, the idea that every user is going to run a
full node is not the state of and has never been the vision for Bitcoin.
Satoshi himself said and proposed that most users are going to use SPV (simplified
payment verification) wallets, which are very thin clients interacting with
the network, and not full nodes.
Finally, a thing should be said about the number of transactions t. There is
this thing called [Metcalfe's
law]( that states that the
*value* of a network with u users goes like O(u ^ 2), as each user *can*
connect to each other in the network.
If this would accurately describe the total number of transactions in the network,
Bitcoin would be not scalable, as full network transactions would go like
O(n * u ^ 2) and per node as O(u ^ 2). That would not be scalable.
Now, it is certainly to be expected that with a larger network, a user might
do more Bitcoin transactions, as the network has more utility to him/her. He
might shift more of the usual cash or banking transactions to Bitcoin, as
soon as the Bitcoin network allows him to do so. In any case, he's not going to interact with all u users, just because they are there!
But it can reasonably be expected that there is (on average) a certain number of
banking and cash transactions a user wants to make per day,  overall, and if
Bitcoin would take over *all* those transactions, it would still level out
at some point. Technically, this would be O(1) in the final state.
However we are not there yet and, certainly, *some* growth is indeed to be expected with a growing Bitcoin user
base. In
Metcalfe's law is discussed in the context of the first dotcom boom. In
there, an argument is made that the value of a network with u users rather
grows like O(u log u) than O(u ^ 2). Assuming that this growth in value
reflects fully in the *actual* transaction rate, the growth in transactions per node on the
Bitcoin network would then also be O(u log u), which is benign, and
Interestingly, it should be noted that in that paper, they argue against
Metcalfe's law as being *too optimistic* of the growth in value of a network.
Ironically, here we have people now trying to stop Bitcoin because it might
grow too much, it might be too successful!
Does this EILYA5 to you sufficiently?
Phew. I think this is actually pretty comprehensive. 
feel free to make/mix this into a blog post, if you think it helps.
@_date: 2014-12-16 09:53:47
But ***we can choose*** not to run religiously motivated 'special case' code from luke-jr.
@_date: 2014-06-14 23:42:17
I like to add an important point - if someone buys SHA256 hardware, it is currently only really usable for bitcoin. So if it loses value completely, because Bitcoin falls apart because too many sheeple direct to ghash.io - that hardware is pretty much worthless!
This should create somewhat of a long-term or at least mid-term incentive to keep things stable.
@_date: 2015-06-30 11:28:07
So he's probably an independent guy against the block size increase.
No one is arguing that those do not exist. A lot more of those seem to exist on the other side, though.
In any case, it can well be argued that Blockstream has a conflict of interest.
It could also stuff like the Egos of Adam/Greg clashing with Gavin, CS people tending to like to build and stack complex Rube Goldberg layers everywhere, a general try at a power grab in the dev community, and a case of group think at Blockstream. (The latter seems to be not unlikely, consider also that they are all highly technical people which might lead to a bit of tunnel vision)
@_date: 2015-06-16 21:47:22
If I run a single node with any hard fork of Bitcoin, which I can easily do, the software vote is not unanimous.
So, I'd rather say 'contentious', 'consensus' and so forth are ill-defined in this context, rather than someone not understanding them.
@_date: 2015-06-18 10:53:18
This is not anyone crushing anyone. This is Greg talking academic and intelligent sounding fluff but adding nothing and actually evading the discussion.
@_date: 2015-06-30 21:10:38
If the step is Adam's worry, I think Gavin is very likely to change it to a (relatively short-term) linear increase from 1 to 8MB.
AFAIK, ~~BIP100~~ BIP101 also means linear increases and no jumps between the doublings. So it is a smooth curve (except for the initial jump)
EDIT: Meant BIP101, sorry.
@_date: 2014-05-08 06:41:00
Similar problem with democracy. That's why education is so damn important. I would argue a good basic education is as important as other basic infrastructure.
@_date: 2015-06-27 18:58:16
I hope you are right. Some might see the 'solution' to an emergency in having Bitcoin fail, though. And will try to achieve this outcome in the last minute. The number of watchful eyes will certainly be less, and it will only be the core devs.
Also, the 1MB limit might easily cause Bitcoin to fade slowly, never creating said emergency.
@_date: 2015-06-28 14:49:34
Wasn't bitcoin.org controlled by someone (theymos?) who is very much opposing Gavin?
@_date: 2015-06-30 19:15:32


He's not secretly plotting it, he's publicly doing it. Doesn't make it any better. And his own personal opinion is one thing, the investor behind his back might have a different opinion on whether those $20M invested should be profitable or not!
People do not start companies for charity. I am sure that if Greg isn't good with psychological manipulation but just your random nice stubborn nerd with an ego, his investors are good at this (of him.. and the public).
The whole company motto of 'can't be evil' stinks very much in this context. 
He has stalled the debate way too much and FUDded way too much to not at least cause a very strong, reasoned suspicion into this direction.
@_date: 2014-11-27 14:07:39
Scanning the wallet imports the key into whatever wallet app you have - which is connected to the internet, thus can be compromised. It might be in the wallet only for a fraction of a second but there is a non-zero risk of the private key leaking out.
If you sign all your transactions on an air-gapped device and only send finished transactions to anything that is connected to the internet, there is no chance of your private key leaking/being stolen.
For those of us who are single-digit BTC-poor like me, it is waay too much hassle. A password protected wallet on my own HW does the job, and I also would argue that is good enough for 99% of all people. But some might like to take some extra precautions.
@_date: 2015-06-18 10:25:22
Satoshi himself expected SPV nods to arrive and full nodes only in data centers.
And even if all users have their own node, the cost is O(n ^ 2) in the network, but just O(n) *per user*. You have to divide out one n again.
And the latter is the obviously the important one for deciding whether to run a node or not. 
I also have to say Greg is using quite annoying deflection tactics. Poly(n) x Poly(n) for any polynomials which have a linear term being at least in O(n ^ 2) is correct, but it is just fancy, academic and bright-sounding fluff to obscure the fact that  both Mike and I *did* adress this question of whether we actually *have* Poly(n) in the other variables.
@_date: 2015-06-30 20:58:48
I think this points a bit towards proposal.
I also think that Jeff's proposal is just an expression of being afraid of the miners. Because with or without a hard block size cap, the miners will decide anyways. Same with a change in blocksize at all.
Gavin said he's fine with something like Jeff's proposal. It would be a market based solution. A little bit complex maybe. But I think most would accept either one.
One thing should be made sure: That the 32MB limit in there is clearly explained as a technical reason (network protocol issues to be eventually lifted respectively fixed) and in no way a political statement for an *ought-to-be* on the blocksize. 
Because Jeff's proposal would be self-contradicting if he a) wants the free market to decide on blocksize cap but b) puts in another hard limit - with the same issue of probable contention when that one is reached.
@_date: 2014-12-20 12:24:17
You are dangerously ignorant. Please take out your calculator and calculate what 7 billion people could do with 7txn/s.
You'll find the answer is decades between transactions for each person on the planet.
Also note that the original promise of Bitcoin is at least several ktxn/s.
@_date: 2014-12-20 11:35:31
Also, even though all side chains will use the same unit of account, they'll fragment the market - because you'd still have to make an explicit transaction between side chains if you need your BTC in a different one. I like to see both: Reasonable growth of the block size (dynamic limit or x%/year) ***and*** side chains.
@_date: 2015-06-18 21:22:00
Ok. I feel I do have a hard time explaining this, but here's another try:
My beef with BIP100 is not with saying that technically, 32MB will remain as a hard cap for now, because it will break parts of the code. That's totally fine.
My beef is solely with the fact that BIP100 makes it sound like the 32MB could be an *ought to be*, like a vision for Bitcoin.
 
It should be stated clearly that 32MB is a technical limit and not meant to be a distraction from the otherwise very agreeable market-based approach that is selecting.
Because signing off on BIP100 will be considered a somewhat binding state of intent, anything that could be construed as an intent to keep 32MB forever *will* be another focus of insane contention in the future, when we will approach the 32MB barrier.
And at that point, it might be indeed to late to change the workings of Bitcoin. Look at how hard it is now.
@_date: 2015-06-28 22:43:41
Except for wishing for Bitcoins failure, I agree. There simply *is* a potential conflict of interest.
@_date: 2014-12-22 11:23:43
Are you asking to 
- avoid that bank where he works
- or to move over to that bank because it is so Bitcoin-friendly? 
@_date: 2014-07-17 20:46:23
I said this somewhere else already:
People have high expectations now that bitcoin will rise, by looking at 'past performance' and seeing all the positive news that bitcoin acceptance (but not so much its volume) is increasing. Now they expect since a while to ride the next bubble but are slowly getting disappointed. I think this will accelerate and crash the price way below $400 soon.
@_date: 2014-12-20 11:33:53
Gavin made an IMO reasonable proposal [here.](
I even think 50%/y is a little bit on the conservative side, I'd make it 100%/y. However, side chains might solve this issue.
@_date: 2015-06-03 19:27:44
Exactly. People will move to the off-chain solution that is the USD if we don't get that limit raised soon.
@_date: 2015-06-11 21:25:36
I looked at Greg's description. Does indeed sound very interesting.
@_date: 2015-06-30 09:12:45
And temporarily convincing one miner to try it out with a lot of talk?
I think Miners do indeed have an incentive to behave correctly here - that of losing reputation by encouraging misbehavior. 
And I think we can gladly see that those incentives work sufficiently well.
They are not perfect, they don't have to be, there will always be theft with Bitcoin.
But zero conf works well enough.
@_date: 2015-06-09 18:16:06


I think an interesting perspective here is that maybe the 'conservative' approach would be to let Bitcoin scale as it is and *not* introduce the artificial 1MB barrier? That was the initial way Bitcoin behaved, and arguably, until the cap is hit repeatedly, also the way Bitcoin is functioning now - without an effective limit.
If you rephrase the whole argument as 'should Bitcoin have an effective artificial cap on the number of transactions or not', then Gavin's suddenly pro keeping the status quo.
I also think this is the more accurate view - Bitcoin so far worked without forcing fees. It should continue working without forcing fees. As Gavin described in one of his blog posts, there is a fee market already, even though blocks are not full yet. Meaning that the fee market works without an artificial cap!
@_date: 2015-06-27 17:33:39


No ad hominem, but I know that you like big sounding words. And as if I ever said everything has a singular cause...
I must be doing something right... having a fan here :D
was it you who said recently on BCT: You get flak if you are on target? :D
@_date: 2014-12-21 23:26:41
Bitcoin is being sold by everyone as being able to cheaply pay for your cup of coffee - but maybe not 'microtransactions'.
So I think Gavin is on the right track with his plan to increase slowly but eventually reach VISA scale. I just like to know when the change is planned for this.
@_date: 2015-06-12 11:16:23
XT will survive. Because it will be the only viable option in or after March 2016, when blocks are full and demand is big.
And there's no reason to believe that the other devs will get their shit together just in time *and* getting large support for overridding Gavin's fork with whatever they come up with  as Gavin's fork is already out there and widely deployed by then.
Gavin is behaving very responsibly in so far as to make people able to place a bet and vote now (by running XT) on how to proceed *before* blocks are full. 
Because when SHTF, there will be hardfork contention anyways. XT and 20MB blocks will be the pressure-relief valve in time.
I also hope and expect a successful XT fork will shut the naysayers up and reduce their influence on Bitcoin in this regard to the appropriate levels.
@_date: 2015-06-13 12:02:07
Gavin moves the contention about the hardfork to *now*, instead of the last minute. He's making the already existing contention *visible now*, he's not creating it.
And as you can see, Gavin is far from the only one wanting this problem address before SHTF.
@_date: 2015-06-17 13:42:59
No deification. Meta protocol.
@_date: 2015-06-11 17:43:29
Indeed. That's why Bitcoin needs a functioning fee market. [Just as it does.](
@_date: 2015-06-01 23:01:04
You are misunderstanding Satoshi: He was clearly talking about Bitcoin being able to eventually scale to VISA-scales and beyond. And he described it. He was fine with not every damn 90s dumb phone being able to run the block chain. He foresaw that eventually, full nodes will be run by bigger entities. NO PROBLEM, in my opinion, as long as those are in different jurisdictions, and lets say, a bigger hackerclub/-space (for example) can elect to run a full node. We are FAR from that being impossible.
And this is the Bitcoin that I bought into, as well as Gavin and most other reasonable people.
The 1MB crowd looks like it might be able to actually kill Bitcoin, if their stubbornness and counterproductive 'nay nay nay, but no I don't have any counterproposals/constructive criticism' continues.
@_date: 2014-12-20 12:11:01
The counterparty in the main:america:usa:los-angeles sidechain might not want my main:europe:france:paris coins, or only for an additional handling fee - introducing additional friction into the flow of BTC.
That's why I think both sidechains and blocksize limit increase should be implemented.
@_date: 2014-12-20 17:45:33
That's not orthogonal, that is anti-correlated or a trade-off or whatever.
Orthogonal is independent of each other.
@_date: 2014-05-07 10:01:57
Yes, I sometimes do worry a bit. But just a bit. I think if BTC or the like become really really successful, something like
Geolibertarianism, 
would IMO become almost inevitable policies in the long-term. Governments would need to tax land/physical goods/physical transactions/processes to be able to raise taxes and for keeping things balanced.
But I think such a scenario wouldn't necessarily be a bad thing, just a different way of organization. I don't think there is a problem with some people owning more (even if it is massively more) than other people. I think problem starts to appear only when people convert money to scarce resources (land, energy, etc...) or to use up scarce resources (luxuries, like the gold-plated 747s some oil sheikhs seem to possess..), or to acquire more power. 
And except for the power issue those things happen in the physical world. And are therefore _easily_ taxable. I think it would also mean that people realize that money is nothing else than distributed trust. 
And this problem of non-taxable capital is already existing: For example, assume I would know a lot of famous/influential people, that would be of value to me and probably give me an advance to the average joe. So, my 'social capital' my (unfortunately non-existing :/) relationships with 'the elite' would be worth a lot. Just seeing these relationships under the angle of money/economic wealth, they could potentially be worth a lot of money, gold, whatever. And it is pretty much impossible to tax that. Even though it might be worth something, just like the stocks or gold or money you own.
Also, consider that if the rich people start to all use Bitcoin to keep their wealth - and a new '99% situation' arises, it would be A LOT easier for simple people like you and me to band together and switch to a different currency.
This, I think, also might alleviate/solve the issue with buying more power, through money. [Obviously, I think people need to be always watchful for the exact policies. That's IMO a pretty big/much bigger problem in itself :)]
Because in a 'cryptocurrencies rule the world scenario', if things seem to become unbearable with BTC, I could get together with my neighbors and mint my own coins. I could even agree on premining all of them, giving everyone the same fair one coin, etc...
I imagine just the threat of being able to very easily switch currencies and most people being aware of that might actually stabilize society a bit. If people would keep being aware that money in the end is just a collective, but very helpful illusion, I think and hope that there might be a feedback effect caused by the possibility of decentralized issuing of currency that might keep things honest.
To further speculate on this point: I can imagine that at some point, 'security like' meta currencies will be formed by having personal trading bots that would make a deal with a merchant's bot with priorities that can be set as the user. Such as, 'In general prefer to pay with BTC, but have some LTC, and because I am citizen in Seattle, I really prefer to pay my local businesses in Seattlecoins...'.
Thinking the even more speculative/utopian thoughts: I think we might collectively acquire a new, more aware understanding of money, power, justice, currency, etc. when the bare facts of currency and currency creation become a very visibile fact of society.
But who knows, so far this is all just a very interesting experiment.
@_date: 2014-05-13 03:13:17
Does this also apply to the stores in the US?
@_date: 2014-05-14 10:19:55
Huh? The cap is obviously important for economic reasons and is thus not going to change?
@_date: 2015-06-17 12:43:34
Per node bandwidth scales with ***O(n)*** with the number of users, if you assume about constant demand for transactions per user.  Every transaction needs to go by every full node once. That's ***O(n * t)***, with n users and t transactions per user.
 
Bitcoin's increasing usability might increase the per-user demand of transactions to increase somewhat, meaning t might for example be O(log n). But it is absolutely ridiculous to assume that every user will actually interact with every other user on the network, which is the assumption behind O(n ^ 2). That would be akin to everyone phoning everyone else in the phone book.
A *successful* Bitcoin might make people be *able* to *do that*, and this might increase the *value* of the network greatly (as per Metcalfe's ideas). But just as with the phone network or current banking network, the ability does not translate into this actually happening.
  
To summarize:
***The O(n ^ 2) is just scare tactics.*** 
[Have a look at my comment here, where I point out how IMO Adam repeatedly uses O(n ^ 2) as scare tactics. He also admits it is just O(n) per node pretty much.](
Limiters sound more and more like: We should limit Bitcoin because else we could have more transactions and Bitcoin might actually be successful and might not scale (in a way some people like it to scale). So lets cripple so it doesn't scale and so we don't get new users, because we are actually afraid of them. Insanity.
@_date: 2015-06-27 17:11:41
Yes he did? By the architecture he made and which is still defining Bitcoin to the largest extend....
@_date: 2015-06-28 21:09:07
This is a very interesting idea! Maybe the whole core should be pluggable modules that *clearly* give responsibility back to the users of building a consensus network.
@_date: 2015-06-09 18:04:17
Yeah... but different people have different preferences - and wasn't intending to say that the price is going to stay constant forever.
@_date: 2015-06-09 10:24:19
It is indeed interesting that there is a flurry of posts about how awesome sidechains are now on top of *Right when the blocksize debate is boiling*.
@_date: 2015-06-03 09:37:48
In a crisis, *soft*forking the blockchain down after increasing it through the hardforking should be even easier, no?
@_date: 2015-06-30 14:38:05
Because they
- might have an incentive to increase the value of their Bitcoins
- might have an incentive to create and/or not destroy a good reputation
- have an incentive to not be excluded from mining by a 51% cartel that cares about the above
- and maybe *because their current, empirically demonstrated behavior mostly shows that something must make them care about more than 2s of profitability?!*
@_date: 2014-12-11 12:53:46
This all does not, however, take away my long grown dislike for M$ and their long history of 'embrace, extend, extinguish' and other unfair tactics.
Time will tell what happens next.
@_date: 2014-05-14 09:52:53
Which should be entirely doable, even 1GiB/block - but it probably would stop quite a few hobbyists from running a full node.
@_date: 2014-12-10 11:51:03
You have a point, but comparing MySpace to energy resources is ridiculous. 
Oil price *will* go up again.
@_date: 2014-07-26 03:23:00
I like it!
Here's some hopefully helpful criticism: bitcoin.de is not on it. I'd also maybe add the largest companies accepting bitcoins, and link them on the map to which company is the processor for them?
@_date: 2014-06-14 05:05:37
But the post wouldn't have been written without the &gt;50% incident...
@_date: 2014-12-22 19:16:05
If there is one thing that benefits the big and powerful, it is currency devaluation. The value of each $/EUR/Rub/... bill slowly (or not so slowly) leaking away into the hands of a few.
For a while, I bought into the idea that money supply inflation is good because 'economy, stabilizing, no bubbles, etc...'
But with the bank bailouts/bailins etc., it is harder and harder to stay  convinced that forcing the economy to run on something that has an adjustable 'leak rate' is actually a good idea.
@_date: 2014-05-24 23:28:15
What about the orphan cost? You write 'It doesn't create an additional cost for the miners to include more transactions.', but I cannot see how that can be true.
More txn -&gt; bigger blocks -&gt; bigger block propagation time -&gt; bigger chance of orphans -&gt; there is additional cost with including more transactions
Isn't that line of reasoning still true?
@_date: 2014-12-22 16:07:57


No, it would only give you the information that the blockchain is formed according to the rules you state - you can't compress all the blockchain data into a SNARK.
But the same is true for block headers since genesis if block headers include the UTXO hash.
@_date: 2014-12-11 13:24:50
The funny thing is, people who volunteer to do development but at the same time hold bitcoin are in a way doing it for selfish reasons, too. Bitcoin is kind-of a global corporation without looking like one.
@_date: 2014-12-16 10:47:58
There are still thousands of nodes. And I fail to see how your proposed changes would do anything about the blockchain size issue.
Apart from pruning (which will already give us lots of room to breathe), I think the solution to potential data storage problems is going to be sth along the lines of people who make valid transactions having to keep their inputs to prove that they own the coins. As I just described [here](
Many people have this idea or similar ones, see e.g. [here]( [here](
@_date: 2014-12-22 18:12:52
No I think you misunderstood me. I am arguing for validating transactions from one block to the next by having
valid-utxo-hash, block-headers
for all blocks &lt;n, and transaction data to go from blocks before n additionally containing the merkle-tree path down to each input of the transaction. Such that the transaction basically becomes: 
1.) An explanation how the valid utxo-hash can be split to prove that the transaction refers to valid inputs in the UTXO set
2.) the usual, signed transaction data
The validating node would then rework its utxo transaction hash from this information to the UTXO hash with the transaction included, and if that fails, reject the transaction.
@_date: 2014-12-16 11:13:52
Well.. I think we got used to owning the full blockchain, just in case.
But pruning first and some form of UTXO hash-tree later will allow Bitcoin to continue without storing a lot of data, even in all full nodes.
I think there might be a point in time where the full blockchain is lost to history, and only the blockheaders remain. I do not see any problem with that.
@_date: 2014-12-13 01:25:02
I hope a financial crash will not send the real bombs flying. And I am serious. I rather hope US 2015 is like USSR 1991, rather than the bad way out.
@_date: 2014-05-11 04:57:42
Thanks for the link. Interesting. I still consider it just an intriguing and fun experiment, but the CC companies now take it serious... wow! 1000 bits @_date: 2014-12-20 12:55:27
I meant conservative more in 'keeping the current state of things, don't overdo the increase'. So, yes, agreed, 50% is a *slow* increase in the transaction rate. However, I am also somewhat sceptical as to whether a 50%/y in bandwidth increase 'law' will hold. We still have the big thing of FTTH etc. coming, so maybe 50% is totally realistic in the next decade or two. 
That said, I still favor a purely SPAM-oriented limit of n times median since last difficulty (n somewhere between 2..10 or so). 
But I'd be ok with a decreed 50%/y and sidechains.
@_date: 2014-12-15 12:19:42
Nice one! 1000 bits @_date: 2014-07-31 02:46:20
I am not so sure. That would be a protocol-breaking change. Even if a consensus is found, rolling it out properly would take many months.
@_date: 2014-05-21 17:10:10
Care to have a link or two?
@_date: 2014-12-13 12:53:09
And for that we need a growth in the block size. I don't particularly like Gavin's proposal of 50% block limit fixed growth per year, but it is an easy formula and given that everything else is even more contentious, I hope it will be implemented soon.
@_date: 2014-12-09 11:58:25


I think this is very much a matter of perspective. We all got used to having the full blockchain available, permanently. But we'd only need the last couple hundred blocks + UTXO set + full chain of block headers (and the latter is AFAIR only ~420MB in 100 years) for verification.
So in a way, the 'able to keep full transaction history feature' comes with a tradeoff - that some people now call blockchain bloat.
@_date: 2014-12-15 12:09:18
I don't do drugs and I don't do darknet. .. But I like Bitcoin, and use it in some cases, and find it extremely interesting. 
So we should now self-police because some people can't separate issues?
Anticipatory obedience is chic now?
Erowid is an information-only site, they don't sell drugs or similar. They make as much a case against drugs (just read some of the crazy user stories for some of the drugs) as they make a case for drugs.
@_date: 2014-05-08 05:53:36
Sure. But nobody wants to pay 'eventually'.
@_date: 2014-07-31 02:25:56


No it isn't. There is still a 1MiB blocksize limit and no signs of that being lifted anytime soon.
@_date: 2014-07-27 06:52:36
Makes sense :D I thought that maybe the office might not be next to the campground or one books in advance and then just drives off with some ticket or something.
@_date: 2014-12-13 20:55:42
We urgently need a web-of-trust for reputation. I imagine a browser plugin that adds a cryptographic signature line to all posts on reddit and other forums, as well as an app to weight these signatures individually or by proxy voting.
@_date: 2014-12-11 13:30:40


That's a highly irrational way to look at it. What does it matter *now* for how much someone bought bitcoins?
@_date: 2014-12-20 15:43:38
Do you have a link for a more detailed description of this idea? I haven't seen that yet.
@_date: 2014-12-16 23:42:57


Complicated tax laws might have a purpose: You want to follow the law, but it gets so complicated that they'll always find something to arrest/make trouble for you if they need to. And it makes you more obedient, because you'll always be scared of not having something in order.
Add to that that lawyers, judges, policemen etc. are better fed when there are complicated tax laws.
I am totally fine with paying taxes - but my goverment absolutely needs to be transparent, and tax laws easy to understand.
@_date: 2014-12-20 13:41:08
How so?
The node would still validate on the integrated difficulty of the chain *before* the input to be used - not on the buried depth like a SPV client.
@_date: 2014-05-08 05:33:52
That's the problem I was trying to talk about about. Obviously, I like to transact in at most a couple hours. And I as well as people around me perceive the fees as too high.
@_date: 2014-12-18 09:29:53
Yes. Get out. Meanwhile, people with a more long term view can buy some cheaper coins :)
@_date: 2014-12-16 10:24:07
I think the real long term answer needs to be that users themselves have to store the parts of the merkle tree that leads to their inputs (certifying that a user owns coins), and regularly, lets say at least once a decade or so, update that information from the network.
The main blockchain could then throw away all transaction data except the block headers after, lets say, 10 years, and only hash old utxos together into a single merkle tree every so often. 
That hash would also be part of the chain, and for making transactions, users could prove their possession of coins by giving a partial view of the merkle tree, proving that the inputs are in the chain.
This also means that long term storage of BTC would cost a little bit more than zero(update coin data every couple years), but it would avoid the problem of an endlessly growing block chain or UTXO set.
@_date: 2014-05-08 02:30:57
The linked data in the thread is pretty clear, though. Can you explain how that happened? I presume you work for ghash.io?
@_date: 2014-07-26 16:43:33
&lt;$600. That doesn't look like a pressure cooker :-) Now what?
@_date: 2014-12-11 21:19:58
I don't want to rain on the parade, but we can all be pretty sure that there will be an initial high number of transactions to Microsoft followed by an exponential decay. As far as I know, this has been the case for most companies starting to accept bitcoin.
One could argue, however, that this is a sign of Bitcoin being better money than $/EUR/.., and that Gresham's law applies.
@_date: 2014-06-05 07:32:01
There could be a scenario where people collectively decide to 'restart currency', each member with a certain, equal amount. I am not saying that this is going to happen - but it is possible.
@_date: 2014-12-21 22:19:17


Pheww.. Bitcoin is magic internet money as we all know, but I don't have a magic crystal ball yet... :-) 
If you want my guess: 50% is long-term enough but there will be at least one squeeze where demand increase from adoption exceeds the 50% growth. But with offchain stuff etc., no one is really going to know what is going to happen wrt. to txn volume.


 
If the UTXO hash would be in the protocol and part of the header's hash it would be as much a proof as that it is the right set as the longest chain is proof that you own a certain number of coins. If you read the proposal for MTUT, you'll see that the guy who wrote it also argues for a soft-fork approach which will eventually result in protocol enforcement. This is what I have in mind.
EDIT: Another note wrt. to txn volume. 7txn/s is clearly ***NOT*** viable long-term, so hitting the block size limit on the way is different than keeping the BS at 1MiB all the time. Also, any fix with another constant block size won't do it. Gavin's approach is arguably the simplest way to do it open-ended.
@_date: 2014-12-17 12:51:43
I think world news/events are partially responsible for this.
Engineered economical collapse of the world's biggest nuclear power is potentially very dangerous. Even if you think this is not a big deal, there doesn't seem to be a lot of good news coming from 'the mainstream markets'.
I think this is a flight to safety in general. And Bitcoin is still too young to be believed safe. It is still considered an experiment!
@_date: 2014-12-17 10:36:45
Replacing the block size limit with Gavin's 50% growth formula!
I honestly think there is a lot of psychological reservations wrt. Bitcoin due to the 7txn/s limit.
@_date: 2014-12-05 17:14:43
And you could keylog by default and 'prepare' /dev/random, and ...
@_date: 2014-12-25 21:09:56
Says the guy who thinks orthogonal is anti-parallel :D :D :D 
@_date: 2014-12-23 09:34:58
We're running in circles. See [here.](
EDIT, to quote:
Block Height vs. Depth
It is important to distinguish between block height verification and block depth verification.
A client verifies the height H of a block by checking that there are H block before it, all of which are well-formed and obey the maximum-difficulty-adjustment-rate rule. Currently only the Satoshi client, libbitcoin, and btcd do block height verification. Block height is the fundamental anchor of trustless security in the Bitcoin system.
A client verifies the depth D of a block by checking that there are D blocks after it (also called "confirmations"), all of which are well-formed. ***SPV clients substitute block depth for block height as a transaction validity check.*** All clients use block depth as a measure of the liklihood of a block chain reorganization producing a new longer fork which excludes the transaction (i.e. orphaning its block).
See also some comments on probabilistic verification of block height. 
Emphasis mine. Are we done now?
@_date: 2014-05-26 10:23:09
Insignificant to you making the transaction, not to the miner processing it, or leaving it alone because he wants a faster propagating block!
@_date: 2014-12-17 12:17:01
The crazy thing is that I feel that most (though not all) of what RT does as propaganda is simply .... stating facts that get let out in the western media. Not so much of twisting yet.
It is scary to see that western media detoriated so much that fair and balanced is completely dead.
@_date: 2014-05-17 10:01:18
Yes. Now the question is, if it actually is happening, whether the people around have enough of a cool head to ignore it. And whether whoever is trying to do this will be successfull in subverting the web of trust that slowly forms within the community.
@_date: 2014-05-15 02:05:53
Well, but with a very high volume, things like transaction cost and volume such stabilize, which should make it predictable.
And I would argue that, if possible, the block size limit should not be a factor in the transaction cost - that should rather be the orphan cost, which would be natural limit, based on what the HW/SW can do.
@_date: 2014-11-29 00:30:18
You seem to believe the market is an omniscient being. Omniscient beings seem to be part of religions - as well as large parts of economics... go figure.
kingofthejaffacakes describes quite clearly how lack of information whether BTC are stored or lost is indistinguable to the market. How did you miss that?
@_date: 2014-12-14 20:57:20
Matter of perspective. [Long term looks spiky, with some people rushing in and then quickly losing interest whenever there was a price bubble, but there is still growth.](
@_date: 2014-03-07 08:26:05
Technically that is not true. Here is another scheme to prove that the two persons are distinct:
Dorian could go to some event where he goes through a metal detector, gets new clothes that are proven to do not contain any hidden transmission devices or similar. He is then staying in a room without any communication possibilities for an hour or so. Then, Satoshi on the internet could receive a bunch of random numbers that are public, fresh and well known to be good (such as from the lottery) and sign those fresh numbers, all while Dorian stays in the room. If he successfully signs the numbers that are fresh while Dorian is staying in the room, the two are distinct persons. If Satoshi fails to show up on the net in time, that is evidence, but not proof that the two persons are identical.
EDIT: Clarified.
@_date: 2014-12-25 14:38:51
As I said earlier ... replace UTXO set with (validated) UTXO hash, and transactions with transactions + proof.
Now, please tell me how you reconcile your statement
"SPV security is exactly what you're talking about. You aren't validating the blocks, you are taking them as truth because they have the most height. That is the definition of SPV security."
with this from the Wiki
"SPV clients substitute block depth for block height as a transaction validity check. "
Your definition contradicts the Bitcoin wiki. How can we possible talk about the same things when you apparently have your own set of very personal definitions? Given this, discussion doesn't really make sense.
@_date: 2014-07-31 03:08:24
What makes me a little bit uneasy is how there seems to be no consensus between the core devs yet on this. I'd like to see the initial promise as it was written in satoshis paper implemented - i.e. bitcoin able to support a high (VISA level) transaction rate.
At least two devs seem to be either very uneasy with dynamic or no limits or are opposed to them.
Edit: Now that I think about it, I think I brought this up before and Greg Maxwell mentioned to me that he sees a 'dynamic limit' as having 'potentially unforeseen effects'. That might somehow be true, but to me it very much feels like it is just evoking a diffuse threat (ghosts).
I have not seen any simulations or any good further arguments that would indicate, for example, that an 'n-times the median blocksize since last difficulty change as the new blocksize-limit' would cause any weird feedback to happen.
AFAIR, artificially limiting the block size was only done for spam protection reasons. Now people say there are somehow further economic reasons for it -even though the orphan cost will set a lower bound on the transaction cost.
Also, setting this limit fixed feels like setting arbitrary boundaries to a system that would be much better served by allowing it to grow dynamically. And if in the end it turns out that people will want Bitcoin to become a few 'so called centralized' high-transaction-rate servers around the world by voting with their feet/money - isn't it futile to prevent this by a blocksize limit dictate?
@_date: 2014-12-14 22:12:40
The idea of Fnord planning a dashboard-integrated bitcoin wallet for drive-throughs makes more sense to me :D
@_date: 2014-05-08 08:39:11
I actually stumbled across bctip before. Thanks for reminding me. I'll check them out. 1000 bits for you. @_date: 2014-12-18 20:59:38
I don't know why PotatoBadger got downvoted, but he's right. For any symmetric distribution of price swing from buy to sell, it will cancel out.
Now, you can argue at length whether the distribution is symmetric - but if you integrate over all of Bitcoins lifetime so far, it is even a bit skewed to the upside.
*EDIT: It should be noted that symmetry for the distribution is a sufficient criterion, but probably not necessary / the only one where your long term loss/gain cancels to zero.
*EDIT2: Even more detailed, this would be random walk and variance will become big - but under the given assumption the expected value would still stay at zero - and std-dev will only grow sublinear in O(square-root(number-of-transactions)).
@_date: 2014-12-13 12:50:17
Debt unwinding is much scarier for the US than for Russia... they actually have their state finances still somewhat in order...
@_date: 2014-05-13 08:38:39
Is the user name related? :D
@_date: 2014-12-20 11:30:57
7txn/s is about 30 years between each transaction if the whole planet would be using Bitcoin. But if we keep 1MB/10min, we obviously won't ever reach that stage.
@_date: 2014-05-26 09:31:31
A 20% drop in profits is not significant to your business? That must be some awesome profit margin that you have :-)
@_date: 2014-12-08 13:19:58


I'd argue it is rather the guy with the bigger gun and who is willing to use it who treats the guy without it like shit.
Now, money can buy guns, but those two aren't really the same...
@_date: 2014-12-20 17:32:22
If you mean the addressing you gave to me - I fail to see how you showed that it is nonsense.
@_date: 2014-12-18 09:48:34
Go ahead and get out. Just do it. We simply have different views on long-term viability of Bitcoin :)
@_date: 2014-12-15 20:08:16
Russia has little debt. I am not so sure that it is the east that is going to implode this time.
And lets all hope this stays purely economical.
@_date: 2014-12-20 12:52:01
I'd take the median instead of the average (more statistically robust) and make the absolute maximum more like 3..5x the median (for transaction-wise spiky things like xmas shopping etc.) But otherwise, agreed.
@_date: 2014-12-19 20:04:42
So it would be colored coins then, because side chains are not merged yet, correct?
@_date: 2014-11-19 22:05:04
Good point. Maybe a better way to express this would be that powerful technology stimulates emotion and creativity so much that concepts of good vs. evil are projected onto its implications, evoking emotions like attachment and/or disgust?
I'd then further argue that the amount of emotion involved in the discussion about a particular new technology is a reflection on its power.
I think that fits many parts of the history of the relation of technology and society pretty well, doesn't it?
@_date: 2014-12-21 14:09:58
Gavin's 50% formula is simpler, not my preference, but I would totally alternatively accept it, too. If you read my post history, you'll see that I promoted basically the same idea as you do around here.
@_date: 2014-05-14 18:39:39
I really fail to see how people can still trust GHash.IO, considering this:
Are bitcoin miner owners sheeple like the rest?
@_date: 2014-12-10 11:29:52
... but the mobile app could be compromised, too, no?
I like your concept and it should be better than a cellphone-only wallet, but I think a wallet with built-in screen/keyboard and engineered for security and single purpose like the trezor is arguably even more secure.
You are doing a different security/usability/cost tradeoff, which is fine, but IMO you should acknowledge the potential problems with relying on complex 3rd party ecosystems (mobile os).
 
I neither own a ledger nor a trezor wallet.
@_date: 2014-12-09 20:14:06
Or simply [Gresham's law] (
@_date: 2014-12-22 11:17:08
Then the longest blockchain is not proof that you own a certain amount of coins either. And a SNARK would only confirm that a so-and-so blockchain has this and that UTXO set - not giving you any proof either!
@_date: 2014-05-08 02:52:29
What about the mining space heater idea? I think that could be a good way to keep it decentralized/make it decentralized again. And at some point, there should be so many ASICs around that the individual chips cost next to nothing - but not using the waste heat would be a competitive disadvantage.
Of course, it might be that there are large scale industrial processes that would need electrically generated heat. But I do not know anything that could fit here - ideas?
And there is a need for electric heating and there are several countries doing it on larger scales. And because we'll at some point run out of oil and will use fusion/fission/regenerative for our energy needs, it actually seems to me like the Bitcoin mining process might be viable long term.
@_date: 2014-11-25 21:09:48


And running the mining hardware! The number of joules to undo the blockchain is clearly cumulative.
@_date: 2014-12-10 18:55:52
Yes, however you'd need physical access to the trezor in that case. If they are doing it right, there should be no way to get to the data through wiggling the USB lines. That is, as far as I can see, the difference. By relying on the 'accept' button and amount/receiver display being properly wired through software to the user, there is an additional attack surface.
*Edit: To clarify, of course you won't get to the data with a smartcard, either. But you could fool the user into signing malicious transactions.
@_date: 2014-12-03 16:45:58
I would actually rather see these 'outrageous' passages from the proposed G20 resolution highlighted directly than reading about it on the web sites of all kinds of biased people. 
Is there a nice cross-referenced summary what is in these new rules, hightlighted to easily grasp what is going on?
@_date: 2014-07-26 16:58:37
How do you deal with double-spends at the food truck?
@_date: 2014-05-14 09:46:04
I understand the denial-of-service reason for the blocksize limit. What I do not understand is why the limit is not replaced by something like:
block size is limited to 10 x (or similar) the median of the block size during the last difficulty time span. That would allow for dynamic growth, spikes during christmas shopping etc. but shield against DOS attacks.
Does anyone have an idea why it is still a fixed number, other than inertia?
@_date: 2014-11-19 19:33:53
I would argue that the mark of actually **powerful** technology is that it can always be used for 'good and evil'. If it can't, it isn't powerful.
Many examples come to mind: Mass media &amp; the Internet, nuclear energy, heck even cars ...
@_date: 2014-12-09 12:04:02


Wasn't there a single address with the equivalent of ~65M$ in it? That's quite the incentive for finding flaws in the crypto...
@_date: 2014-12-20 12:19:09
@_date: 2014-02-19 03:46:15
I think your the poster of the comment you replied to - and several others - confused 'value' and 'price'. Something might be very valuable (such as your love of your wife) but not have a price.
@_date: 2014-07-31 06:46:44
I read these discussions. I am still not convinced. 


That arguably means that miners control the blocksize artificially and how it suits them already. No need for a global limit then?


Same argument applies. Miners can choose to only process transactions based on the fee/txn-size quotient. (which would arguably be the most natural thing for them to do)
@_date: 2014-12-12 10:03:02
But you eventually arrived at the same state 'X dollars, Y bitcoins, z AAPL stock', or whatever your exact portfolio is. *How* you arrived there is completely irrelevant for future planing. The past is just a personal bias.
@_date: 2014-12-20 12:18:06


^ This. We need something that is scalable. Either constant growth, or a dynamic/no hard limit (just a configuration soft limit).
@_date: 2014-12-20 12:47:03
A couple ideas here:
- Hash the UTXO set tree, put the hash in each block
- Forget transaction history after a couple hundred/thousand blocks
And, in the not-too-far future, maybe change the Bitcoin validation protocol such that people would need to not only provide a transaction, but also a 'certificate chain' of how and where their input transactions are part of past blocks. So that anyone holding a bitcoin address would need to regularly (say every couple years or so) update the certificate set for their addresses from the network so that they are able to prove that they own coins in the future.
 
AFAIR, this would make nodes be in O(1) in storage and something like O(n log n) in bandwidth (n txn/s). And it would shift the burden of keeping all the data from the network to the individual coin holders. 
Reading all the various topics on bitcointalk.org etc., makes this look at least technically very feasible to me.
EDIT: I think the main hold-up comes from the acquired taste to keep the whole damn block chain on HDD forever. We don't need that in the future. With little change, just keeping the UTXO set + block headers in memory would be enough, and with somewhat more change, the requirement of keeping the UTXO could be shifted quite a bit to address holders.
@_date: 2014-05-13 03:14:07
Here's 5000 bits for tipping. I trust you to use it all for tipping :-)
@_date: 2014-05-14 10:28:06
Ah, ok, I see. Well, I guess 1MiB blocks simply were a comfortable upper limit when the limit was put into place.
The 21Mio limit quite likely has a reason: 'double' precision floating point numbers are accurate to about 21e6*1e8 units. And using 21million then comes from the halving schedule.
See also here: 
@_date: 2014-05-29 09:53:10
You are right. Reasonable fees would indeed work with a GB-sized block. Thanks for the explanation!
Now the question remains how long it will take until GB-sized blocks are allowed and seen on the network.
@_date: 2014-05-08 06:37:49
I don't think so. This would be similar to the current pooled mining. Sure, the pools are the visible, 'centralized' frontend, but the owners of the actual miners can very quickly vote with their feet.
I would argue that the only thing affecting centralization of mining is who owns the SHA256 HW (and runs it).
@_date: 2014-05-13 02:39:15
Did that even before you wrote this post. I was curious how many would accept bitcoins. I have about half of the people accepting the tips so far.
It is definitely fun and feels good :D
EDIT: I also tried to keep it a decent amount, so if someone is interested in how bitcoin works but doesn't have the patience or ability to buy them, they can play a little bit. So far, I always tipped 1000 bits or above.
@_date: 2014-12-20 13:14:53


True, but I don't see how we can't scale that up by a large factor right now even. I have ~10MBit/s at home. Others even have 100MBit/s. Home users.
 &gt;&gt;   Forget transaction history after a couple hundred/thousand blocks


How so? The longest chain wins. The longest chain of block headers would at the tip have a hash of the UTXO set and you could validate transactions by having the transactions explain (through a partial UTXO merkle tree view) that they are valid.
Just keep the headers. A very modest amount of data, ~500MiB in a hundred years.
@_date: 2014-12-16 09:50:46
Reducing block generation time only increases centralization due to network lag, but does little else. The only thing bitcoin needs to change is to eventually have a facility to slowly increase the transaction rate - and maybe allow sidechains.
$5 'instant' online transactions, tipping etc. can go through 3rd parties, as it does now. Everything else (online shopping etc.) can wait for confirmations.
You are inventing problems.
@_date: 2014-05-08 05:43:45
Well, it's about 1% of a coffee. So people see: 1% of a coffee as a transaction fee, that's maybe better than credit cards, but not _that_ great.
@_date: 2014-12-07 10:07:02
Do you have a link to an issue on github to back this up?
@_date: 2014-12-20 12:16:08


That is not easy. You'd have to convince the majority of the network and the majority of the miners to switch over. You don't want to rush that out in a hot fix, you want this tested and then implemented to happen at a certain block # in the future. So that the new Bitcoin code that allows the block size increase can penetrate the network first. 
I think calling the block size issue (the BS issue :D) somewhat urgent makes sense.
@_date: 2014-05-17 09:42:15
I wonder whether all the fuss and supposed problems (or lack thereof), the discussions about the foundation, etc. are actually results of careful social engineering to try to destroy Bitcoins community? 
@_date: 2014-12-26 21:14:18
If germany 'pivots' and turns away from the U.S. towards the east - it would be the end and probable collapse of the U.S. empire. There is a reason propaganda war is so huge in germany right now.
@_date: 2014-06-04 07:40:21
I think you are touching on a point that is much more profound: People worry about the 'new 99% of Bitcoin'. But because Bitcoin is just _one_ implementation of the protocol, there is a chance to switch away. I cannot see a future in which there will be laws outlawing altcoins.
In the (IMO still very unlikely scenario) that the Bitcoin rulers become unruly, the rest of the people could coordinate to switch to an altcoin. That is a very interesting feedback mechanism.
@_date: 2014-12-20 15:47:59
ROFL. And now we have SJWs trolling everywhere and feminists deciding what is right and wrong in every regard of life - pretty interesting how times change.
@_date: 2014-07-18 07:11:31
But that doesn't really seem to happen now, does it?
@_date: 2014-07-31 08:04:03
let a satoshi disappear @_date: 2014-12-17 12:13:39
At least you didn't report my comment as SPAM. :D
@_date: 2014-12-23 15:33:26
Isn't the 'non proven' true for any of the crypto primitives?  I mean, isn't the elliptic curve stuff etc. all just 'known not to have known weaknesses'?
How do you feel about deterministic wallets? They use SHA256 to generate the key chains, right?
@_date: 2014-05-14 09:50:44
I believe the calculation is about like this:
~250 byte per transaction
1MByte block size limit
A block every 10min (difficulty goal)
1MByte/(600s *250 (byte/txn)) ~= 7 txn/s
@_date: 2014-12-21 23:53:13
Thanks for answering. But, hmm ... from his blog post on scalability, he's outlining it more along the lines of 'this is what is currently my idea' instead of a 'we're going to do this'. Am I misreading this?
@_date: 2014-12-16 10:53:31
He's pretty right wing and I certainly wouldn't agree with him on lots of political issues - but I'd still argue that the 'breeding cattle' is ad-hominem and hyperbole.
@_date: 2014-12-20 17:34:27
What I outlined above would be possible with the crypto-primitives that are in Bitcoin already, though.
@_date: 2014-12-09 18:33:52
OTOH, there are people who do use electric heaters.
@_date: 2015-05-07 22:03:03
On a meta level you could say that whether we have a limit or not happens in a market place of ideas and competing alt coins.
It is, after all, not a government enforced limit. So a fully free market that creates such conditions/limits/systems.
Looking at the blocksize chart, I agree with Gavin that an imminent increase is urgent, though.
@_date: 2015-05-21 20:17:08
You have development costs?
@_date: 2014-12-20 13:58:47
But if you put the UTXO set hash into each validated block header, you'd know from that hash and the full height of the block header chain as much as a 'regular' full node what the valid UTXO set is, without needing to go from the beginning of time and rebuilding all transactions.
[In here]( SPV clients are explained as looking at depth instead of full height. That's what I remember, too.
Granted, you could call anything not knowing the eternal history of Bitcoin 'a thin client'. But that's why I am arguing for this extra hash above - because it would allow a node solid, non-probabilistic (well, except for the properties of the hashing itself) transaction confirmation/rejection by just having the block headers.
@_date: 2014-12-16 10:16:30
That's quite the accusation - can you prove this at all?
@_date: 2014-12-01 10:49:38
What I do not get, and maybe you can enlighten me here: The gold forward offered rates seem to be estimated by a small number of market participants in a way that looks artificial compared to price discovery on an open market/exchange (where the Gold spot price comes in). Why do you think there couldn't be similar things going on as happened with LIBOR? I understand that 'gold paper' might be worthless compared to real gold in a dire situation where trust is evaporating - but if you think the negative rate is as reflection on the amount of distrust 'in the system', how can one be sure that the forward offered rate isn't also just fiction?
I am really curious as I am not knowledgeable in these matters.
@_date: 2014-12-20 12:40:26
I think it would be very helpful for the core devs to post such calls for tests here on reddit.
Not everyone is following the github.com issue list etc.
@_date: 2014-12-14 17:56:26
That's why, in a properly designed system, I can personally weight the trust  ratings of others.
@_date: 2015-05-09 13:22:50
Bitcoin is still an ongoing, but so far successful experiment. 
Its developers, its user base, etc. are part of the system, too. It is able to adapt, even though the process can be painful (such as now with the blocksize issue)
@_date: 2014-12-21 16:21:11


?! What do you mean by 'only works in the case that Bitcoin grows 50% year?'
Gavin's formula works if the transaction rate growth is &lt;=50% and bandwidth gets cheaper with about that rate. Noone ever denied that? 


You'd need the soft-forking change of requiring the hash of the UTXO set in the block header. I didn't say that this is the case right now.
EDIT: And with SNARKs, requiring a soft-fork wouldn't be different, but you'd introduce a lot more complexity. Tree-like hash data structures are all over Bitcoin.
@_date: 2015-05-31 16:57:19
Nice set of reasonable answers to the blocksize issue. Lets get it growing.  In particular, the duck in the millpond is a great analogy:
Greg thinks a hard fork will be easy in the last minute. Why the heck does he think a softfork won't?
I am slowly starting to think might be onto something...
@_date: 2015-05-31 16:18:36
The definitions of what is ads, spam, ham etc. will be contentious - but I am also interested in this. Especially since argued that the blockchain is full of advertisements.
As a first order approximation, you can see that there is a weekly blocksize modulation of about 30%.
I would say those are *definitely* part of the 'valid' transaction set.
@_date: 2015-05-31 10:46:36
The issue is bandwidth, not storage.
And bandwidth with 20MB is at about 35kByte/s per connection.
Very doable.
@_date: 2015-05-21 20:14:19
Compared to all the other stuff, this is IMO actually *indeed big news*.
Because porn.
@_date: 2014-12-14 22:01:28
So if there is...
* Inflation, the spending power of my euro bill  constantly trickles away whenever I hold it. As if it is melting - constantly losing value. 'The hidden tax'. Same is true if it sits in my bank account.
* If there is deflation, banks now charge fees, if not negative interest
What exactly is the purpose of banks nowadays?
@_date: 2014-12-14 17:58:22
I am thinking more about a system that works across a lot of platforms.
@_date: 2015-05-31 11:28:37
In that case, the correct answer should be: Someone internal gave me an invalid, forged contract, we have problems, I'll be back with a clarification.
And sort that stuff out internally, *before* making any weird public, incomprehensible and wrong statements.
@_date: 2015-05-08 21:05:39
That 250k input block would only reach the direct neighbor nodes, though, and wouldn't be forwarded, correct?
So this wouldn't really impact the whole network.
Also, bloom filters could be used to do a quick look up 'might this UTXO exist'. If you worry about predictability of the bloom filter and the attacker decidedly querying the gaps, the bloom filter hash could be randomized on each node.
@_date: 2014-12-25 11:41:02
We are talking past each other.


That's the point. That is not what I was proposing.
@_date: 2015-05-19 08:05:21
Companies tend to group think. Also ones lead by luminaries from Silicon Valley.
This happens when
'This sounds very interesting and like a good idea'
is replaced by
'This must be great because you are so smart'.
@_date: 2015-05-11 20:53:31
I think everyone around here feels the contention on the block size issue, and everyone's eyes are on you. 
Your short comment on decreasing the block time looked like it was the second half of your 20MB proposal that you just mentioned as in 'by the way, let's also do 20MB together with a block rate change'.
That's why people freaked out - there was no way to tell from your comment whether the blocktime proposal was as serious business as the blocksize proposal - or, as it is clear now, just thinking out loud. And everyone is expecting serious blocksize arguments from you at this time.
EDIT: spelling
@_date: 2015-05-31 16:13:32
Maybe he meant the attack on just the 0.1% of nerds being able to use Bitcoin... :D
@_date: 2015-05-09 16:51:32
I think that is a lot harder to get consensus on because it meddles so directly with the economics/social contract part of the Bitcoin system.
@_date: 2015-05-10 02:20:47
I totally see your point but consider that non-action might as well be the influence of a corrupting force.
So the best is, as another poster said, to stick to the issue at hand. Which clearly shows that we need more blocksize urgently, and think hard about what can be done to have something like multi-party payment channels up and running soon.
@_date: 2015-05-31 14:29:46


I am not convinced of the 'wisdom of the crowds', as millions of flies eat shit. That said, there is not reason the crowd needs to be wrong either.
Gavin was pushing for this since *years* and with very sound arguments, and he *was open to criticism and* ***counter proposals***.
The latter which did not happen in any meaningful way. So he's pushing forward now. I think that is completely reasonable.
@_date: 2014-12-23 12:48:28
Do you think the bias is more than lets say a factor of 100? If not, you could just run the 100x the nominal output of the RNG through sha256 to 'compress' it down to your needed amount of entropy for address generation?
Like what you'd enter into the text field for the generator on e.g. brainwallet.org
EDIT: To be really sure about randomness, I'd just a lot of different sources of entropy, /dev/random, that one, dice etc. and then hash them all together using SHA256. Thoughts?
@_date: 2014-07-31 02:48:28
Also, if they fix it, I'd like to see something dynamic (either an n-times-median limit or no limit at all) - because arguably, changes to the protocol become harder and harder to implement the more important bitcoin becomes.
@_date: 2015-05-31 10:10:47


Ok. So how about this: There is a growth in transaction rate, correct? And a fraction of those are advertisement, lets generously say 50%. Lets say that 50% of the actual transaction is the inelastic demand. Do you think that people will keep using Bitcoin at all transaction costs? What level do you think people are willing to pay?
So take the total growth of transactions, divide it by two and extrapolate it exponentially until say 2018. That gives you a rate that will be reached by then according to this very simple model, probably around 4MB blocks. Technically feasible - or not?
So would you be fine in planning now for 4MB blocks in 2016? Would you be fine with a predetermined growth from 1MB to 4MB from 2016 to 2018?
@_date: 2015-05-29 17:25:07
The smaller coin can then be 51% attacked at any coin with the existing mining power from the bigger one. That creates a strong incentive to keep your money on the bigger coin.
In a way, Satoshi's ideas work on even on that 'meta protocol' level.
@_date: 2015-05-07 18:02:19
Opposing the 20MB will very soon look ridiculous. If the 'governing structure' stays the way it is now, I think we'll see similar debates happen every couple years, with eventually a modest and accepted proposal to raise the size.
Not really a beautiful solution, but probably workable.
And if in the meantime, a fully decentralized solution to this scaling problem (Lightning networks or whatever) crops up, even better.
@_date: 2015-05-30 20:45:49
I actually think that is an acceptable state - IMO still decentralized enough. As long as it is not just one place one the planet.
@_date: 2014-12-18 21:20:35
Not in Mexico, no plans on travelling there anytime soon - but you should maybe fix the post's title if possible - it is sendbitcoi***n***.mx
@_date: 2015-05-31 09:34:11
I'd like to see those calculations, too.
@_date: 2014-12-21 14:12:40
But 100MB/s in ten years. When there is presumably a lot more FTTH.
And as I discussed with you in the other thread - having full nodes store all transaction data forever is not necessary with things like MTUT and pruning. What would be left is block headers and UTXO set, and even the UTXO set storage burden could be shifted to the people actually making transactions...
EDIT: Also, with *Gavin's 50% growth formula*, 100MByte/s would only be reached in about 27 years, not ten..
@_date: 2015-05-31 14:22:48
What are your commits?
@_date: 2015-05-07 18:20:54
Maybe - we are still away from block size pressure, though!
On a whole other level, there seems to be a meta chain of reddit and bitcointalk posts, validated with our mental processing power proof of work that probably gives the incentive structure to the actual block chain... 
So lets try to keep the sybil nodes (== paid trolls trying to divide us) here on reddit down ;)
@_date: 2015-05-07 18:59:27
Yeah maybe. I have followed all these discussions (look at my history) and I am still optimistic that now, with a very concrete patch set, things will get done.
But I have also seen the effect that people apparently need to get burned (txn delays etc.) before there is any real change. AFAIR this was the case in the &lt;$100/BTC range when most miners didn't switch off the 250kiB soft limit.
In the (IMO unlikely) case that the 1MB-forever crowd wins, I think we'll see txn rate going to that limit, delays and problems and Bitcoin's publicity and price SEVERELY tanking and *then* Gavin and the other devs wonderously agreeing on no block size limits anymore.
Whether this would be enough to outright kill Bitcoin or just delay it a while, I do not know.
@_date: 2015-05-07 23:03:00
That and I also fail to see how fees will suddenly flow to support LN-related development. If anything, a higher exchange rate might support LN dev. With LN there is at least a tragedy-of-the-commons type of situation, so it will probably happen because a couple of people/companies have foresight, a workable product or just because some devs have an itch to scratch.
@_date: 2015-05-30 20:37:59
That's quite a random guess though, isn't it? :)
I think the point is rather that with O(1) transmission, the burstiness of Bitcoins network use will be reduced to the inherent Poisson-burstiness with which transactions arrive. No big block clogs anymore. And you save up to about half the amount of bytes to transfer.
@_date: 2014-12-15 20:06:42


I sincerely hope this war stays economical/a proxy war. Like USSR 1991, but maybe the other way around this time. A hot war, basically no one would survive.
@_date: 2015-05-07 21:59:43
Agreed. But imagine running a node requires more than a home internet connection. Regulation, raids etc. could easily target anyone who wants to run a full node.
The main point of the limiters seems to be:  If it is just something you can fire up (for almost free) on your home PC, it is much harder to shut down. I can see that.
But I think it is much more important right now to grow the ecosystem and support for Bitcoin to prevent that from happening, though.
And, after all, we are discussing an IMO very modest increase here. 20MB. And blocks won't be full, just like they aren't now.
@_date: 2015-05-09 11:23:34
As I am thinking about it: Another way would be to have a coalescing rule on the block chain: All outputs that are older than a certain age are going to be compressed into a hash tree root in a deterministic way.
If someone then wants to spent from that old stuff, she has to provide the branch leading to her coins.
@_date: 2015-05-07 18:50:39
Agreed insofar as I think it would be better if it is a non-issue.
I still bet that the 20MB increase will go through, though.
I think we're just clearly seeing the workings of the 'Bitcoin money cooperative' and everyone who is interested or a stakeholder is doing the best s/he can to provide input to this problem. Which looks like noisy chaos and uncertainty, but OTOH will mean that the problem is addressed in a broad way. Add in a couple of paid distractors trying to destroy BTC, of course.
Compare this to a company where people will not speak up because they are afraid of their supervisor or similar and where there is a PR department providing a cleaned up picture of every internal, murky decision making process. Bitcoin's CEO committed suicide and all that.
@_date: 2015-05-31 16:02:03
The point is that they *can* - and *don't*, already.
@_date: 2015-05-31 16:23:02


Calling it like that is IMO already the result of social engineering. Its still Bitcoin, as it follows the original vision.
@_date: 2014-12-12 10:28:57
This is what I meant. In a way, I more and more see Bitcoin as some kind of 'insurance money', in the unlikely(?) case that there is a bigger financial desaster, it would work and still allow easy international trade.
@_date: 2014-12-21 22:11:01
Gavin, do you have a time frame in mind when you plan on rolling out 20MiB blocks and the increase-schedule? Or do you avoid this because you are still pondering whether there could be different/other solutions?
Thanks in advance for answering!
@_date: 2015-05-20 06:34:06
Why not by increasing blocksize?
@_date: 2015-05-07 19:16:49
I hope you are not saying that 'PT likes to be controversial for the sake of it' equals 'PT has neurological issues', or are you?!
@_date: 2014-06-20 07:05:38
It would rebase the currency.
@_date: 2015-05-07 22:48:59


I meant it in the way the he put the pieces together. Hash cash, P2P, and globally longest chain. That is something that is indeed really new. I bet you won't find anything mentioning such a system before his paper in 2008.
@_date: 2015-05-31 10:21:45
One party who is part of a contract doesn't have that as an important document in-house but gets it from some external third party on demand?
@_date: 2015-05-07 22:23:29
The argument against something like maxblocksize=n x median(blocksizes since last difficulty) or similar is that a) they could be gamed and b) they are much more complex to analyze than a fixed cap.
I am not sure either one is a problem, but that's why they have been thrown out.
Gavin's original solution was 20MiB ASAP and then 40%/year increase until arriving at 16GiB blocks (and then stopping). That would be a long term fix.
@_date: 2015-05-22 08:17:23
Do you have experience in building browser plugins? I have zero experience in that area. I looked into it once (for firefox) and it seemed that even the smaller plugins were a vast mess of arcane JS and XML configuration files.
I think from the UI POV something that supports the three main uses cases today in a basic way would suffice for the moment and IMO in a way kickstart more development/investigation:
- pay for online orders with BTC (e.g. your Pizza delivery). I am thinking here of automatically parsing 'bitcoin:'-style URLs and popping up a window with a payment request. To avoid 'BTC begging spam' that will certainly eventually pop up, make it like firefox asks for permission to open other pop up windows and have black/white lists.
- pay small amounts, maybe 'configurable implicitely', like newspaper access. You say 'WSJ is allowed to take x uBTC/article from my wallet' and it will all just happen in the background.
- pay repeated small amounts through payment channels (video streaming) (open up a 'video cost meter' that counts up and shows you the current remaining balance while the video is streaming). Again, have per-site configuration of websites allowed to stream video for , with a configuration on how much is allowed.
It would be nice if it would actually support links to external wallets, too, via JSON-RPC or similar. At least so that the firefox wallet could ask the main wallet 'I like to replenish myself with $5 in BTC, do you agree?'
Before kickstarting/lighthousing it, I think one should have a small prototype. If you know how to set up the general framework, I'd be willing to help and put some of my time into it.
@_date: 2015-05-04 20:13:36
Thanks for all this, Gavin. I like your calm way of leading this project.
@_date: 2015-05-21 09:13:29
Yes, I've watched Balaji's video. I am not even disagreeing that there is a certain allure to a more federalist governmental structure.
I am not so certain, though, that a 'no voice, free exit' scenario, as put forward by a lot of the DE people, is exactly the described utopia.
I am worried that - although I see these developments as somewhat inevitable - that there is an instability forming with Bitcoin and silicon-valley-style feudalism on one side and the misandry bubble and our increasingly bizarrely baroque, nepotistic and oppressive western governments on the other side.
The reshuffling of the cards that seems to be inevitable now will not necessarily bring out the best in humanity. I certainly hope it will.
It could happen peacefully by brining sanity back into our institutions while flushing away a few of them.
Or it could happen in a way that the supply chains are broken, mass starvation, bank runs and so on end in one of the dystopias depicted in many movies.
And, on another angle, I do sometimes wonder how much of lets say the misandry bubble - is actually engineered by TPTB to cause this collapse.
@_date: 2015-05-07 20:20:24


The BS issue discussion is going on since years. It was going one since forever on bitcointalk. If he discusses in private in addition, what should I blame him for that?
Are you arguing along the line that he is trying to do politics here and rally the mob for a blocksize increase? 
I'd rather say he did a good job convincing many people. Nothing wrong with that.
@_date: 2015-05-07 18:52:36
The US dollar is your off-chain solution.
@_date: 2015-05-19 07:50:51
This is what makes more sense IMO. Have a 'hardware wallet on a chip' that you can talk to via I2C/SPI and that you could buy either precharged or load yourself with satoshis/BTC.
If they'd produce a hardware wallet chip with comparable security of lets say the trezor, but without all the UI attached - just the bare chip, and lets say you sell that chip for a couple dollars each - I could see how this could be a viable product. If it would include a switchable miner, too, fine.
But I wouldn't make the miner the focus of the product.
@_date: 2014-12-22 22:23:09
Which is not what I was talking about....
@_date: 2015-05-11 13:58:44
There are proposals to do very strong reduction of the data set needed for a fully validating node, such as [this](
They can compress almost everything in Bitcoin down to a couple of hashes, but one thing that is not going to be compressed are the block headers themselves. So this would be a linearly growing data set that would be 10x larger.
It is, compared to the rest, a tiny amount of data, but if it is hard to compress this in any way further (I don't know, didn't investigate), it might be wise to keep the 10min interval?
@_date: 2015-05-29 18:59:02
It is in a way a power play and Gavin bets on the users supporting him. I think he's going to succeed, though.
@_date: 2015-05-07 18:34:18
Also, it should be noted that 'buying a coffee' is not really a micro-transaction.
If Bitcoin fails as a workable solution in the cent/sub-cent range, so be it.
But damn sure I see its original promise as *in principle being able* for all 7e9 people on this planet to buy a cup of coffee twice a day.
That's what was said in 2009/2010, that's whay Satoshi intended, AND that's what is IMO still very doable.
If this happens through a series of arguments on block size and eventual insight and agreement or through a good idea on a general formula, I don't know. But I do know that a 7tps Bitcoin has no chance to become 'the world's reserve currency'. I am not very convinced that a 70000tps Bitcoin will do it, either. But it is certainly a lot more capable as a payment system.
If we somehow come up with a workable decentralized clearing system, fine. But even then we clearly need &gt;&gt;7tps:
If every person on the planet is able to make a clearing transaction *once a month* (which I think is still pretty rare), with a txn size of 1KB, we would still need 1.6GiB blocks eventually.
@_date: 2015-05-08 20:30:15
Good luck with that 40%/y. I'd like to see that, too, but we are out of luck that. Gavin apparently had to scrap that immediately because it is even harder to push for than 20MiB.
20MiB seems to be extremely hard to get through already. And I think 20MiB is absolutely harmless and benign, and really hard to argue against.
@_date: 2015-05-09 09:07:02
*A lot* more?
It is two inline functions to detect when the timestamp is new enough to support &gt;1MiB blocks. Certainly more, but not alot IMO.
@_date: 2015-05-11 09:37:10
I am slowly starting to think he's just pulling our legs here :D
@_date: 2015-05-09 13:38:21
Sorry. English isn't my native language - and I was writing too quickly and didn't proofread :D
I meant 'who spent them'. People who spend coins that are old enough to be coalesced into a single UTXO hash would need to provide all info to break down the root hash into eventually the UTXOs that they want to spend. They would need to send that data alongside with their transaction. And they also would need to store that data.
@_date: 2015-05-07 20:00:05
He's kinda well-know for being that way though, isn't he?
@_date: 2015-05-10 17:05:54
With validated UTXO merkle root hashes, the longest chain will be proof enough in *all* cases that a certain output is valid.
@_date: 2015-05-31 09:54:35


As you can see *right here*, even a well planned fork is contentious as heck. What makes anyone believe an emergency fork will work smoothly?
"Ok, so nothing works anymore, Bitcoin is clogged up. Yeah, ok, lets increase to 1.5MB blocks. No lets remove the limit altogether! No, 2MB! No 20MB! Bitcoin is just fine clogged up, people need to pay more!"
I already see that on the horizon.
*EDIT: spelling
@_date: 2015-05-09 11:45:48
What about this idea I wrote here: 
Would still be pretty fair. Noone who keeps the necessary data every decade or so would lose anything. Makes running a pruned but full Bitcoin node truly O(1) in terms of storage space. Services might (probably will) spring up providing views of old UTXO hash trees for a fee to you, so that you can spend all old coins.
@_date: 2015-05-07 11:47:37
But a higher allowed blocksize would not make those spam rules ineffective, or would it?
@_date: 2015-05-31 10:18:22


Yep. A default, not a hard limit on the network. Something any single user had the chance to change already since day 1.
But you do see the contention that is developing around the hard fork, right?
What makes you believe that a hot fix solution will be less contentious than long term planning?
And what makes you believe that communication - which from what I gather here on reddit for example - apparently is already broken down between you guys in the context of long term scalability planning, will work better when it is a matter of days or weeks at most?
@_date: 2014-12-21 14:19:46
skilliard4, what I think would be nice to also extrapolate transaction rate growth trends from history for different time spans (last 3 months, year, all time etc.) and see where we could end up, soon.
@_date: 2015-05-30 12:42:37






It might be a failure to communicate between you guys and I do not know what
you are all up to, and how much and whether you communicated privately by
email. So this is just what I see from the *outside* (I do
occasionally browse bitcoin-dev, bitcointalk and hang around on freenode):
There is apparently consensus on a block size increase. There is also
consensus that hard forks need to be be planned well in advance for them to
work smoothly. There is also consensus that Bitcoin someday, somehow needs
to scale to *many, many* transactions. In former times, people were thinking
this to happen as per Satoshi's original paper (high throughput on-chain
payment processors), but I guess the opinion shifted a bit into the
direction of also trying, if at all possible, to get off chain alternatives up and running to keep the chain smaller. So far, so good.
In *any case*, there is still consensus that 1MB will not be enough for all
So here come and and make tests and very
concrete proposals on how and when to up the blocksize. At least on reddit, on
bitcointalk, and you have been aware of them and took part in the
But at this point, the discussion went from 'yes, in principle, we want this' to:
'how about this?'
And the answer from you wasn't: Well, no, this is too early, but lets say
when blocks are full 90% for a couple months, I'd agree to a modest
increase of x% (for example).
Instead, the answer was many variants of: No, blocksize increase is problematic,
because of reasons (which are certainly valid). Full stop.
And at this point, Gavin was, as I see it, expecting more: A constructive
counter proposal, he also very much repeatedly tried to evoke that from you
and Like he does in this very thread.
Constructive in the sense of 'Given all my doubts, this is what I could
agree to'.
Because, if the consensus that 1MB is not enough still holds, and if you
agree in principle on this, there MUST necessarily be a parameter set that will
work for you with &gt;1MB blocksize. It might be a lot more conservative than what Gavin
proposed, it might be 5 years out. But that parameter set never appeared.
And Gavin certainly tried to get the discussion going on that one.
@_date: 2015-05-31 14:40:51
I do not see, however, how blockstream will profit from a destroyed Bitcoin. Honestly, I think Greg Maxwell is just honestly worried. He's extremely intelligent and far-sighted, but I think it makes him run into deep traps of worry, *with no way out*.
Gavin seems to be more optimistic (dare I say realistic) and says about this: "Well, we can't solve all problems and eventualities right away, but a) we have to do something about this (block size issue) now (foresight on Gavin's side!) and if there are worries, please tell me. Also tell me what could be done to go forward, what needs to be changed with my proposal to address that worry'.
But if you are seeing doom and gloom everywhere, you don't have a constructive, direct way to address that worry.
As Gavin probably does to some extend, I see the immediate doom: 1MB blockchain and developer deadlock, loss of interest, destruction of Bitcoin.
Greg and the others see doom further down the road: Centralization and government regulation. With a very tenuous definition of what centralization is, I might add.
Both are valid completely valid, but one solves more pressing problems first.
That is at least my opinion. Yes, gut feeling, I know.
@_date: 2015-05-07 20:06:45
Yeah. I still think dreaming of fairy tales is a good motivation and inspires necessary creativity, though. Because Bitcoin was long a fairy tale, too :-)
 
@_date: 2014-12-20 13:32:10
Ok. I meant putting the UTXO set into a sorted merkle tree or similar structure, and putting the top hash into the block header.
By looking at the longest chain, I then know which UTXO set is right.
If I'd have just a partial view of the tree, down to a particular input for a transaction, I could still prove 'see, these coins are going into this transaction, and I have the key to sign, and from this partial view of the tree, you can see that they are part of the current set'.
EDIT: With MTUT, I was talking about [this.](
@_date: 2015-05-09 13:09:20
Thank you for that motivation, Gavin, helps alot!
Hopefully I can find the time to do that soon.
@_date: 2015-05-07 23:17:14
They'd all be orphaned.
@_date: 2015-05-31 09:42:00


I think I read the word 'Moonmath' to describe this on Bitcointalk. I found that apt wording :D
@_date: 2015-05-07 22:09:18
Full ack!
@_date: 2015-05-29 18:55:06


Interestingly, all transactions on either chain would be valid ones. So if you see one on one chain, you can replicate it on the other (regardless of who you are - the originator cannot stop you).
So people who WANT to stay on the current chain can be made to comply by repeating their transactions on the big one - except those originating from new coins, of course.
This IMO will be another strong incentive for just one chain (Bitcoin XT, and I bet Bitcoin core will by then have the same rules) to exist.
@_date: 2015-05-31 17:05:37


That argument can be turned around: Just implement bigger blocks (the hard fork) with a plan well in advance, like Gavin intends, so it goes smoothly.
If stuff stops working, it should be easy, according to yourself, to do an emergency soft fork. And the emergency soft fork should be easier than the emergency hard fork.
@_date: 2015-05-31 16:22:11
Go forward with BitcoinXT, start running BitcoinXT nodes, convince others to do the same and hope that before March2016, a very clear consensus emerges.
@_date: 2015-05-21 20:15:51
Well, but now you are trusting some JS code coming from some website running in your browser with the safekeeping of your BTC.
I'd rather trust a plugin from a reputable vendor than any website.
The trick is to just use _small_ amounts. Not 1000s of dollars worth in your firefox wallet. Just a couple bucks maybe.
@_date: 2015-05-08 20:21:25
I fear some of this crippling and resistance to 20MiB blocks is arrogance and delusions of people who think that Bitcoin will grow just fine with 7tps, because it did in the past.
If they succeed in stopping the 20MB bump, they will burn themselves (and all others in the ecosystem) badly.
@_date: 2017-02-14 12:56:08
Which is a full node incentive! Yay :D
It might also be impossible to run it multi-hop in a trustless way without SegWit, so this might be good news for miner fees and chain security.
@_date: 2015-05-29 18:45:46
Bitcoin going its own way :D
@_date: 2015-05-09 15:16:47
Yeah, that's exactly what I was trying to say. Glad to see others understanding my gibberish :D
@_date: 2015-05-07 19:36:43


Those experts only chime in and only need to if they see it to be worthwhile. You can safely bet that there is a large amount of read-only followers of Bitcoin ***with*** a high level of technical skill and a solid background.  They might not know every current line in the code, but they do understand the network and the issues to a deep level.
Those people do not want to be left out of the loop. If you go and make all discussions happen in a closed-doors consortium that publishes some 'this is the spec how stuff is going to be done', people will get *very suspicious* and *rightly so*.
EDIT: I am not saying that there are no clueless folks around. But there is certainly a gradient of involvement and an only partly overlapping gradient of knowledge wrt. Bitcoin. And if you look at the discussions here on reddit, I'd say they are still for the most part productive and civil. Because it is not like you paint it: 'A cathedral of enlightened beings - and the stupid common man'.
@_date: 2015-05-10 01:54:37
Ok, I haven't flushed this all out yet, it is more of a high level idea. 
Assume that per person on the planet, there will be on average 100 unspent outputs (wild guess). That is 7 x 10^11 UTXOs. A balanced hash tree would be about log2( 7 x 10^11 ) deep, so about 40 levels.
Assume you have to store the branch leading to your UTXO per level and the other branches to be put back into the chain for the others. Further assume full 256 bits SHA256 for each hash. That would be 2* 40 * 256 bits  = 20480 bits, or 2048000 bits for all of *your* 100 UTXOs.
That is 256KB of data for you, to be updated as often as a new coalesced hash will be made (every couple years, hopefully only every decade or so).
Third party providers might pop up and store the full block chain forever in their centralized databases and might provide you the merkle branch information leading to your UTXOs for a small fee. In case you forget.
Edit: The hash might need to include partial sum amounts of unspent coins and so would be slightly bigger (by a factor of &lt;2). But you get the idea. Probably a couple 100kB.
@_date: 2015-05-31 10:38:40


I have never seen a reasonable explanation of how they are supposed to work, much less any actual *code*.
@_date: 2015-05-09 11:42:00
But isn't this alleviating the concern over the needed RAM size for the UTXO?
@_date: 2015-05-09 15:19:06
Coalescing the UTXO set like described would alleviate that problem - the attacker would need to send valid data explaining the root hash.
EDIT: And I believe (though maybe wrongly so?) that Bitcoin core already looks for misbehaving nodes that do that and disconnect.
@_date: 2015-05-09 12:09:37
Right but Gavin showed the full UTXO set in his blog post. Isn't rather the active UTXO set what is interesting?
I mean, all those time-stamping hashes etc. will certainly never be of interest again...
I am not saying it won't ever be a problem. But so far the discussion seems to be like all UTXOs have to be accessible instantly - but it appears only a small part is actually used often?
@_date: 2015-05-09 09:11:00
You could even combine the two.
1. Make Gavin's formula of 20MiB + 40%/year growth the absolute maximum.
2. Make '3 x median-blocksize-since-last-difficulty' the actual limit, capped by 1.
This way, you have capacity planning worst case capability from 1.) and you have a reasonable max block size from 2.).
@_date: 2015-05-29 15:40:36
It would be great if you could extend linphone or csipsimple on android with support to automatically pay through a Bitcoin wallet for a SIP account with you.
@_date: 2015-05-20 20:48:33
In the whole document they sound like they are afraid. I actually generally do not put too much value (no pun) into the idea that 'the banksters control the world'. Or that they couldn't adapt even to a 'Bitcoin is wildly successful' type of world.
But if they sound like this, it does actually make me wonder.
@_date: 2015-05-10 13:30:21
Right... in the case of Banks, though, they could take care of your private keys as well as the extra data needed for expanding the coalesced UTXOs.
@_date: 2015-05-31 09:37:27


Changing a constant in the software meant to be lifted anyways since a long time is 'quick and dirty'?
It might be kicking the can down the road - but for a good reason, because all other viable solutions so far are ***even dirtier***.
@_date: 2015-05-31 14:20:44


Please be more specific?
@_date: 2015-05-31 09:29:36


Wait a bit! If I am not mistaken, I read here on reddit (was this authoritative?) that OKCoin backed the idea that v8 is valid for quite a while. And now you say they turned around and say 'yeah, that was crap'?!
If that's the case, this still needs to be kept aware in the debate, because it is saying a lot about the reputation of the OKCoin party.
If what was posted on reddit earlier is indeed true, that's like saying: Hey, yeah I know, I tried to scam/fool/betray you. But not anymore, lets just continue our normal deals. So we need to forget that old stuff. Ridiculous.
@_date: 2015-05-27 21:03:30
a) Bitcoin wasn't intended to be run as fully verifying nodes on very low end laptops. Even Satoshi said so.
b) Need is just that, a need. Better try and fail by adopting to a need, than to not change but fail because a need is unmet.
Wish, of course, is entirely different.
@_date: 2015-05-31 14:56:22
I'd rather say he comes across as one. Actual troll would involve malice, I'd say.
@_date: 2015-05-20 20:51:15
Or just raise taxes, like they do already. I don't get all the fuss. It's not like any government wouldn't have any knobs left with successful Bitcoin.
@_date: 2015-05-08 08:59:23
True. Excellently argued, well written post rehashing (no pun) the main points.
Lets go to 20MB asap and when everything is fine, have a stronger position for the next leap.
@_date: 2015-05-09 11:29:28
UTXO set growth could be addressed by tree-hashing old UTXOs according to a predetermined schedule and then making it the responsibility of the coin owner to provide the branch of the tree leading to his/her UTXOs in case they want to spent.
On the blockchain, spending that old UTXO would then result in the single UTXO tree hash being replaced with the hashes of all necessary subtrees of the other coin holders so that their ability to spend their coins is not affected.
Lets say coins older than 10years are not stored anymore as UTXO but only their combined hash, details TBD.
@_date: 2015-05-31 14:46:05
Yes. But he has a gut feeling of what decentralization is (which I might not even fully disagree with, but that definition in itself is going to be an area of contention).
He answers Gavins *very clear*, *thought out* plan ***forward*** with 'I have centralization worries, and we need a metric for decentralization'.
That metric will a) be as contentious as the blocksize, so you are just shifting the point of discussion *without getting anything done*.
And b) Gavin, confronted with a *very real problem of blocksize and the necessary mid-term planning involved (next year)*, asked for ***concrete*** changes to his proposal, or ***counter proposals***.
And those, I have not seen yet. And Gavin is at it for several years now.
Lets go forward with BitcoinXT (and I hope and think, eventually Bitcoin core will join again)
@_date: 2015-05-21 19:09:47


This means we need (a) Bitcoin (wallet) browser plugin(s). Something which pops up and says 'do you want to spend X BTC to website Y now?'.
Funded, of course, still with minimal amount you might not care about losing when reinstalling firefox or similar.
@_date: 2015-05-09 13:56:06
Yes. I would argue to simply make the 'merkle branch proof' part of the transaction and thus part of the cost of the transaction in uBTC/kB.
@_date: 2015-05-08 21:14:05
True, but there's going to be a mean number of UTXOs per person that is probably going to saturate. Especially as many small UTXOs increase fees.
I really do not understand the fuss about this. UTXO set can be on disk and efficiently cached. So even if it is 1TB someday, it can still be looked up.
@_date: 2015-05-07 23:16:44
LN uptake? As in more people read the paper and wonder how to implement that thingy?
Certainly good, no doubt about that, but a short term *solution*?
@_date: 2015-05-10 09:34:17
OTOH, the recent exponential growth (albeit with a long time constant) of the transaction rate will become a line you can extrapolate and intersect with the 1MB limit to make a rough guess when problems will start to appear.
@_date: 2015-05-07 22:39:04
Not exactly help, but maybe it shows clearly that the biggest distractors are so rare that they can be clearly pointed to.
Also, it might mean that the most crazy of the BSLs (block size limiters) are so crazy to be safely ignored.
@_date: 2014-12-08 11:50:47
Two things are infinite: the universe and human stupidity; and I'm not sure about the universe.
 Albert Einstein
@_date: 2015-05-11 09:31:17
I am aboard with blocksize. I am NOT aboard with changing the 10min interval. That IMO changes too much, last but not least the coin schedule.
@_date: 2015-05-10 02:30:42
Yes, they'll adjust by going to the offchain transaction system named USD. Where they came from.
@_date: 2014-12-16 15:19:19
Maybe so! But I sense that a lot of people are of the opinion full node == full archive node. I think there only need to be very few archive nodes with the full blockchain (as I said, in theory, none at all), and, other than that, just 'validating full nodes', i.e. nodes which see all transactions and blocks, but consolidate the data after a short amount of time (couple GB at the moment, maybe couple TB in the not-too-near future)
@_date: 2015-05-07 15:50:18
This is actually a damn good argument that I thought about today myself. If consensus is to keep BTC running, making insane blocks as a miner will make others (effectively) eject you from the blockchain.
Which brings me to an interesting point: What would happen if the max. block size would be a configurable variable that the full node operator has to select? Would the network fragment or would eventually everyone who runs a full node select a high but still reasonable (against such attacks) limit?
.. it would also be absolute freedom / anarchist/'market force'-based, so maybe it would appeal to that crowd?
EDIT: I mean, essentially, the whole block size hard limit and hard fork is a social thing. We all accept the code that is on github.com/bitcoin. But there is nothing preventing anyone from fiddling with this already - so if it would be unlimited with the clear option to fiddle - would it stabilize or would Bitcoin seperate into fighting fractions?
For some reason, I believe people would still come to some kind of agreement, but would like to hear input from others. 
EDIT  After a sufficient while and in case some people/miners start rejecting blocks with an arbitrary size cutoff,  there should still be a chain that is the longest (has 51% of hashing power) and is therefore the gold standard. So isn't putting in a block size limit effectively saying: 'well, 51% is gold, but not quite, because it must be &lt;1MiB, too'.
EDIT  Creating uncertainty in 'what the right Bitcoin is' would be IMO the most effective way to attack Bitcoin right now. Maybe something to ponder about. And I hereby out myself as a supporter of Gavin's 20MiB proposal. (I actually liked his original growth formula even better)
@_date: 2015-05-27 08:42:48
Without a clear alternative forward, doing nothing while there is an, although maybe hated, alternative (bump to 20MB) amounts to 'watching things break', though.
@_date: 2015-05-08 11:44:52
It actually seemed to trigger an interesting bug in my FF (it happened after FF was unresponsive for a couple 10s of seconds while rendering): Keyboard entry on all tabs was starting to stutter and losing characters. Weird!
@_date: 2015-05-07 21:06:30
Exactly my point. He's even waiting with the PR. You'd have a valid point if he's somehow going to circumvent the process of *eventually submitting a PR*.
@_date: 2015-05-13 18:51:49
you should read [this.](
EDIT: I also like to have devs comment on the status of commitment to UTXO hashes.
@_date: 2015-05-30 20:27:09
Why 600x? 
Isn't it rather a difference of (number-of-connections x blocksize)/(10*min)?
Without optimization, bandwidth is very approximately 2x(number-of-connections x blocksize)/(10*min), with the problem of a big burst for every new block. OR isn't it?
@_date: 2015-05-10 01:42:53
Interesting objection, I hope you are not right...!
Though I think if Bitcoin became successful and is a main payment system, such a user-responsibility change might actually happen because Bitcoin would then be irreplaceable as a payment system. Or not, because there would be the risk of revolt. Who knows, hard to tell :D
So I rather like to stick to the more technical stuff.
@_date: 2015-05-30 20:42:10




Why not? Present the validated root of the UTXO set in the block header before and after UTXO shenanigans + the involved transactions for that block and you prove someone is up to no good.
As there will still be validating nodes, this is an extremely unlikely failure mode anyways, basically amounting to a *long* conspiracy of the vast majority of full nodes.
@_date: 2015-05-26 17:58:59
A little bit of bickering and 'paralysis' might be good though - it shows that things are handled conservatively.
But I agree, 20MB and be done.
@_date: 2014-11-18 10:43:25
The block would need to be propagated throughout the network, and the likelihood of it being an orphan increases with its size. So there is an additional (stochastic) cost involved with making larger blocks.
And if the issue is spam, this could be solved with a running median (lets say updated when difficulty gets adjusted), and make it the network rule to only accept blocks n times (lets say 5..10 for things like xmas shopping) the median block size.
@_date: 2015-05-09 13:07:12
I can understand that. But consider that this is essentially your local, personal view.
In addition to the private key, you need the unspent output on the blockchain to transact. But right now, those unspent outputs are stored by *others* at their cost and with something like coalesced UTXOs, they would need to be stored by *yourself*.
@_date: 2015-05-29 15:55:28
Ok, now I am confused, maybe can clear this up for me.
Will BitcoinXT now contain 20MB @ March2016 + 40%/y or just 20MB @ March2016?
@_date: 2015-05-07 23:25:03
So far, LN is a paper tiger. But blocks are soon at their hard limit. I am not arguing against LN and whatever, btw.
@_date: 2015-05-08 20:16:29


With a functioning lightning network. Which we DO not have. And a single channel per year is crippling, too.
EDIT: And that STILL means that in the success-case, 7tps ***will simply not be enough***. Sheesh!
@_date: 2017-02-14 12:58:16


Bwahahaha. That from the guys crippling Bitcoin at 1MB is truly hilarous. I mean: Good luck. We'll do tens of k per second of nice on-chain transactions, vending machines with subchains and so forth: And you can do your fancy LN Rube-Goldberg machine. Like the banks do Ripple.
@_date: 2015-05-09 13:18:39
That would make it possible to DOS Bitcoin quickly by exhausting the UTXO set. And 1MB is already waay below the current size.
The more I think about it, the more I feel something like a coalesce after a decade or so (whatever HDDs could carry) is inevitable someday if Bitcoin stays successful.
@_date: 2015-05-07 17:48:51
You shouldn't evaluate an argument only depending on what some doctor had to say about that person's mental health. This is a variant of arguing based on authority. 
And, given that health records are usually private, I wonder whether saying 'PT has neurological issues' is actually an attempt at character assassination from somewhere.
The thing that really matters is whether the argument is sound on its own.
That said, I think even infinite block size will be fine and Gavin's original path (+40%/y) is IMO the way to go eventually. But lets get the 20MB done, first.
@_date: 2015-05-31 10:36:48
We have lots of contention already about a *planned increase*.
What makes you believe that there will be magical consensus for a hot fix?
@_date: 2015-05-11 09:33:15
Exactly. I am starting to get a little bit worried about Gavin. And I am with him wrt. the block size.
@_date: 2015-05-30 12:02:45


Ok, fair enough. There *could* be other results than turning people away from Bitcoin, if those solutions would be anywhere near to be ready for prime time until March 2016, which I honestly do not see.
Also, I have seen this argument of 'the right folks will be incentivized' multiple times, yet I fail to how exactly that incentive will work. Who is exactly going to be incentivized to integrate LN/OT with Bitcoin, and why?
And with regards to kicking the can down the road: Yes, Gavin's 20MB proposal was a compromise of kicking the can in the short term while keeping the network sane and still working.
@_date: 2015-05-10 09:42:09
a) There is a competition of governments, b) Bitcoins are a many-to-many mapping as another poster so nicely said and c) it is a push system, so it is easy to poison anyone's Bitcoins with small amounts of bad coins then.
So I agree, hostile governments might make this unworkable if they all coordinate, but I don't see that easy control scenario you are painting.
@_date: 2015-05-31 09:16:56


[Not true.](
Note that Gavin puts a hard limit in place with the exponential growth, so it is only going to be a *temporary* exponential growth.
AND you can always soft-fork new limits in.
@_date: 2015-05-09 13:17:02
I think this is another whole point in the ongoing scaling discussion:
How much is reasonable to be expected with 'normal' usage patterns, how much could it be worse due to attack, and what is the most crazy worst case scenario. My impression is that not all people are starting from the same scenario when arguing.
Reasonable, IMO, is to argue for normal usage patterns, try to deal with attacks by making them uneconomic and add a certain buffer to deal with the risk that your usage patterns change. Always assuming the worst (full blocks) would e.g. mean that the blockchain would be at ~350GiB right now, which it isn't.
I particularly don't see the UTXO set fragmenting so much that it becomes unbearable anytime soon.
@_date: 2015-05-01 10:57:04


@_date: 2015-05-09 09:23:55
Huh? The UTXO set would necessarily be part of that megablock and thus that megablock would grow at least like the size of the UTXO set.
However, I do not really see that UTXO set size is any worry soon, because of efficient caching.
@_date: 2015-05-31 14:58:11
Gavin was waiting for that input since years. Dare I say that it is too late now?
@_date: 2015-05-08 20:56:19
I wonder if UTXO is really that much of a problem, fee estimation could be changed to make UTXO creation more costly. Which, of course, would have a couple drawbacks such as making privacy more costly.
But in case it really *is* that much of a problem, it could be included in the 20MiB block size discussion.
@_date: 2015-05-29 17:40:27
Good luck using it. It looks like the 1MB coin will be smashed by attacks and then abandoned.
@_date: 2015-05-29 15:58:41
The nice thing is that as a fallback, 50% of mining power will always decide what the correct chain is.
That's rule number zero in Bitcoin space. The longest chain wins. That rule also prevents any *real* hard forks.
@_date: 2015-05-25 09:10:10
Bitcoin mining isn't really following any ascetic ideals, though... !
If it becomes long term diluted into space heaters and coffee machines, it might be acceptable. But so far it is a wasteful process.
@_date: 2015-05-09 13:51:24
I submitted [this]( a couple hours ago with some further ideas on how to avoid even that but still put the burden of storage back onto the users eventually.
I think that would be an even better solution. After 5 or 10 years (whatever is feasible), you'd have to prove that your UTXO is part of a hash tree of which only the top hash would be stored by the full nodes.
@_date: 2015-05-29 17:34:03
If Gavin succeeds (which I hope and tend to think he will), he will also get more effective authority by nature of having 'won this game'.
Whether this extra authority is a good thing or not is a whole other question, though.
@_date: 2015-05-24 15:15:32
Where's the RJ-45 jack?
@_date: 2015-05-29 17:22:04
But then, the minor fork will always be in immediate danger of a 51% attack. Only the bigger fork will be immune. That will be a strong incentive to abandon the minor fork.
@_date: 2015-05-08 20:32:46
He does, because most of the user base sees him as the level-headed lead of Bitcoin core dev.
And that is just spontaneous, force-less self-organisation. So no need to bitch about that.
If he'd screw up all the time, he'd be replaced. But he built a reputation of being pretty reasonable, exactly what people want for leading such a project.
@_date: 2015-05-08 08:47:18
Yes, but they'd need to properly propagate through the network.
@_date: 2015-05-31 10:33:29


Oh yes, there is. What Satoshi described in his original 2008 paper is certainly at least a very strong suggestion on what is meant to be. That's the Bitcoin I support.
@_date: 2014-12-17 00:00:50
I was wondering about this, too. _If_ the anticipated SHTF  is merely, hopefully going to be a financial breakdown without (civil) war, and infrastructure stays intact but financial trust evaporated - Gold is going to be very, very unwieldly to use in the aftermath. But Bitcoin would be easy and actually almost perfect to start using right away for trading necessities.
And the war scenario is so dark (we live in the nuclear age) - that it doesn't really make sense to buy gold or to do other things to plan for that.
@_date: 2014-12-25 17:56:57
The longer will always be considered the valid one - as it should be. That's why the 51% attack would work. And having any kind of 'valid blocks' wouldn't help you against that. I am sorry, you just don't know what you are talking about.
@_date: 2014-03-07 08:58:02
That is true! But then he's not really acting alone anymore. Would someone like him trust a conspirator? Maybe so...
@_date: 2015-05-06 18:55:51
Yeah, but he would still need to explain why we don't see constant 1MiB blocks. Even if he successfully argues that Bitcoin txn are 99% spam, he'd still need to explain why there is only 1/3 as much spam as blocks could carry.
 
@_date: 2015-05-31 21:50:57
That's a fair point. However, it also means that it would be a very gradual, almost hidden process that nobody notices. Because if you notice, you can still declare it an emergency, right?
When you talk about a measure of (de-)centralisation, do you have any metrics in mind that can be applied to this debate?
 
@_date: 2015-05-08 20:43:30
Shouldn't the whole discussion then more about the *active UTXO* set rather than all unspent outputs?
EDIT: Removed redundant redundancy...
@_date: 2015-05-07 22:42:35
... and spammers apparently do not even use all the available block space below 1MB yet.
So I'd argue the spamming problem can't be that bad. The mining-spammer conspiracy even less so.
@_date: 2015-05-07 20:25:30
He did a lot of testing, and public discussion of that testing. He made sure months ago that 20MB is fine on common HW. 
[And here is the pull request.](
@_date: 2015-05-09 12:13:00
What if you'd make it a 10 year grace period (or similar) before your UTXO gets coalesced into a single hash along with others?
That'd mean you'd have to backup some data referring to your transactions every couple years. Is that still asked too much?
I mean, keeping the full UTXO set is putting the burden of essentially storing one's own data on the full nodes and they do not get paid in any way for that .. so I see that concern if the UTXO set growths unbounded.
EDIT: And that extra data you store is a lot less sensitive than your private keys - it will not allow any spending of anything, as it was all public data at some point anyways.
@_date: 2015-05-31 14:22:27
Stuff is heating up now. In the end, this is I think because there is a lot of 'gut feeling' involved, too.
And my gut feeling is 100% with Gavin on this issue. And I follow Bitcoin since years, and it is all consistent with a clear vision that IMO most people bought into.
@_date: 2014-05-08 09:04:38
I agree. I don't understand it as no fees either and never did. 
But I think a cup of coffee should have optimally a fee &lt;&lt;1%. I still hope and think that the economics of the Bitcoin protocol can - in principle - allow that. But obviously, there is a lot of old code, fudge factors, block size limits etc. that still are in the way of a completely dynamic fee estimation.
I was trying to address those.
@_date: 2015-05-31 08:58:07
Unlikely. But to easen even that worry, that's why I said keep transactions for a year (or so).
@_date: 2015-05-02 19:33:28
@_date: 2015-05-07 20:43:27
Altruistic ideas, such as 'Bitcoin is better for society' might make some miners allow 0-fee txn.
But certainly not a behavior to base the success of Bitcoin on. 
... but Bitcoin apparently has a function fee market, as Gavin shows, even when the block limit is not exhausted.
@_date: 2015-05-07 22:32:56
I don't worship Satoshi but I'd still argue he's pretty creative &amp; wise:
- he came up with the whole scheme of bitcoin in addition to implementing it, and many people before tried unsuccessfully
- he knows the multiple ways that there is value in hiding his identity as the creator (personal safety, selling the mystery, increasing the decentralized nature of his project by letting the head/lead disappear)
 
- he's thinking very long term (didn't sell his stash)
The idea of cryptographic decentralized money was certainly out there, without the exact ideas on how to implement it.
While thinking about crypto money before BTC myself, I actually came close to his ideas of using chains of hash cash to prove and generate value. I think quite a few must have had similar ideas. But he followed through. And I certainly didn't see the full picture and full value of this approach, though!
Most importantly, I felt decentralized cryptocurrency could be a grave way to impact the world with unforeseeable consequences and found the concept of inventing such a thing and putting it out there of taking on an almost unbearable amount of responsibility. [Edit: For some reason it feels to relate to Nietzsche's ideas...]
That also made me VERY interested as soon as I heard about such a thing (Bitcoin) actually existing, and well, now that it exists and everything isn't as dramatic as envisaged, lets improve and build on this :-)
@_date: 2015-05-01 10:29:09
copyright != patent.
@_date: 2015-05-29 19:04:28
Will there be a moving median limit on size, too?
Like n times since last difficulty, to prevent people from mining 'obnoxoblocks'?
You said at several points that you like such an automatically adjusting scheme.
@_date: 2015-05-31 15:14:13


It is 20MB + 40% per year of growth *of the hard blocksize limit*.
@_date: 2014-06-14 23:34:42
What I don't really understand is how people can miss this, creating at least a decent amount of doubt in ghash/cex:
I am glad, though, that the *&lt;50% regulation actually seems to be working* - as ghash is down somewhat now in hashing share.
But there is way too much lag. It should not spike above 50% for a day.
I fear that the education that 50% should never be reached by a single party will have to be a very hard crash because someone actually does double spending.
Also, I think this argument I saw somewhere around here that even if the &gt;50% party is not intended to abuse power but might be hacked by a 3rd party - that argument still stands.
@_date: 2015-05-07 21:30:25
I see your points - that's why I wrote 'in the midterm'. 
The horror scenario of a single, perfectly efficient, centralized,oppressive,world-governing Bitcoin processing entity might be prevented by nation states or similar entities having an interest in keeping the playing field level and the process honest.
But that's certainly a weak argument; At these timescales, I think everyone's predictions seem to become murky and uncertain.
@_date: 2014-05-08 04:01:21
I think what is also a good idea is physical world bitcoin tips. 
Does anyone have a good link that mass generates wallets on a single sheet of paper? 
The usual bitcoinpaperwallet generators are meant for high value/high security, but use a full page for each and every wallet. I am looking for something where I could print a batch of paper wallets for easy tipping and then scan and load each QR code with my cellphone. And it doesn't need to be with holographic stickers or anything, just double sided, simple fold and some scotch tape and scissors would do it for me. Any ideas?
@_date: 2015-05-29 17:31:19
Exactly. As I wrote above, Satoshi's idea creates incentives also on the meta level involving protocol design. That makes Bitcoin so intriguing.
For some reason, we tend to forget that when we argue endlessly about what are essentially just [Schelling points](
Those arguments are not worthless, however: They will make sure that the next Schelling point will be closer to the optimum.
@_date: 2015-05-31 14:58:43
Exactly. BTW, are you 'Solex' from bitcointalk?
@_date: 2014-07-23 07:01:18
Wait a bit,  is syncing with the block chain? They have miners in there?
@_date: 2015-05-24 15:22:03


You could also see it the other way around: 'Blockchain' currently sells Bitcoin better than 'Bitcoin' itself does.
@_date: 2015-05-07 18:40:51
The algorithm to do the automatic knob turning will have some new knobs, though.
That's why I like Gavin's original 40%/year formula.
We should do 20MB now and add that back in later - when it is shown that 20MB won't be full, either.
@_date: 2014-12-22 00:16:32
I remember that, yes. But even the 20MiB now, 100%/(2y) has an unknown time/phase component:
- is every other increase going to be coinciding with the reward halving? 
- is 20MiB going to start at a fixed block height, or rather when x% of mining power shows that the miners support the new block size?
@_date: 2015-05-07 19:24:44


I rather like to have no logs in the first place...
@_date: 2015-05-07 23:06:17


Do you have a link to code for a P2P payment channel network that can settle off-chain and really supports most BTC use-cases without bloating the blockchain?
@_date: 2014-12-20 13:22:30


Yes, that's why I argued for hashing the UTXO set and validating it in the block header, too. What was once referred to as 'MTUT' (Is this still being discussed, btw.?)
@_date: 2015-05-31 14:32:22
Exactly. He's trying very hard and well to be a good, listening, inclusive and reasonable leader but a leader.
Analysis paralysis indeed.
Lets get the 20MB+ fork done.
@_date: 2015-05-02 20:28:43
I feel sorry for the people in Kalispell. It is such an utterly ugly town.
The surroundings make up for it, though!
@_date: 2015-05-20 07:34:59
Payment channel scaling is an independent development that puts the focus of solving the scaling issue outside Bitcoin's core development, though.
I could agree that a working implementation of Lightning networks (or similar) would show the way forward, but even then their full deployment would depend on a blocksize increase in Bitcoin core.
If you are convinced that blocksize scaling is strictly not necessary (but it is eventually, but that's besides the point now), you'd only have a reason to doubt Bitcoin's scalability if you are convinced that what is laid out in the Lightning network paper can NOT be cast into working code.
So do you believe the Lightning Network proposal might have a flaw? Or do you think people are sceptical until they see that thing running?
@_date: 2015-05-07 17:42:32
Exactly. The best way to attack Bitcoin is to try and divide its userbase.
20MB blocks and be done (for a while).
And hopefully/(probably IMO) this will show that we can go to Gavin's original formula (with a 40%/2y growth) eventually.
@_date: 2015-05-06 20:41:49
Ok, got it. FWIW, I was actually assuming some way of calculating gave a figure of -5.7% interest. I guess I expect too much craziness :D
@_date: 2015-05-31 14:27:25


The problem is whether you think transactions are elastic or inelastic demand. Of course, this is not a binary thing, there is always *some* elasticity.
was talking about advertisement transactions. I'd be fine if fees are going so high that *most* of them disappear. That'd be probably almost no free transactions and costs of, wild guess here, 5-10ct per average transaction.
Other than that, I am convinced that people mostly use Bitcoin for things that they would otherwise use banking systems for. And those transactions are completely inelastic from a Bitcoin point of view: If they cannot happen on the Blockchain, they will happen off-chain, meaning in USD/EUR. And all that happens on chain is that former Bitcoin users will put a last transaction online to exit the system.
So if that happens, you just starved Bitcoin to death because you were fearful of success. Congratulations.
@_date: 2015-05-31 09:22:10


Where the heck does he say that? You pulled this out of nowhere, didn't you?
From [here](


Are you spreading FUD intentionally?!?!
EDIT: Also, more recently, [here.](
Are you a paid troll?
@_date: 2015-05-08 20:48:57
I don't think so. I think that attack scenario is very limited.
Either the attacker only has a few UTXOs under her control and then the load is insignificant.
Or the attacker has many paged-out UTXOs, but then the fees are significant.
And whether an UTXO might exist or not could be solved with bloom filters.
@_date: 2015-05-09 08:48:44
@_date: 2015-05-30 12:12:49


No offense, but 'blocks are full of advertisement transactions' sounds like hyperbole. They might be a large fraction, but 'full of them' suggests that they dwarf all other transactions. 


But that is what Moore's law went into all the time already, isn't it? Transistor count goes up and all those costs fall.


The burstiness is due to the full blocks still being transmitted, correct?
But with efficient block transmission, 20MB blocks amounts to an average data rate of ~35kByte/s... so isn't that the number that is/should be at the center of the 20MB discussion?
And honestly, 35kByte/s don't look bad to me.
@_date: 2015-05-20 17:46:22
Awesome how the least well-dressed is the most knowledgeable about the whole thing. Nerd power :D
(To stop the flames: No I am not saying dressing well is bad or the others are stupid.)
@_date: 2015-01-14 16:23:03
Still waiting for psyops like this: 
But maybe many of the people 'who want to keep the blocksize small' are part of that scheme already...
@_date: 2015-05-31 15:12:04
This can't be emphasized enough. For three years now, all that happened from the other side was *effectively* stalling the discussion.
@_date: 2015-01-03 23:08:36
The very important difference between gold and Bitcoin though, is, that Bitcoin is easily transferable.
@_date: 2015-01-27 17:17:21
Do you have a comprehensive write-up somewhere why 1MiB is good enough? Like Gavin does for the increase?
I only find a few random posts, compared to Gavin's comprehensive set of blog posts. It would help many people to weight Gavin's vs. your arguments.
@_date: 2015-05-10 02:09:10
Why 80s?
@_date: 2015-05-07 20:14:21
I know that worry, but am not sure whether it will happen that way. 
Really hard to say. One argument on the optimistic side could be that many unfilled 20MB blocks show the limiter crowd that there really isn't much to worry about.
We have unfilled 1MB blocks. I really like to see the argument for why that is the case.
@_date: 2015-01-22 10:41:50
Understood @ trying to keep the balance.
But in light of this, Gavin's growth formula does address this in a reasonable way, or do you think otherwise?
@_date: 2015-01-10 21:35:30
Weekdays/Weekends. I think this is one of the good, healthy signs in the Bitcoin network.
@_date: 2015-05-11 21:36:13
Not if it gets upped to constant 20MB. 
There will be a time when 20MB is not enough, and that time will bring up the need for another hardfork. 
@_date: 2014-12-09 14:03:54
Re blocktime issue: I think this can and will be solved with appropriate reputation networks, and things like local, implicit reputation when buying a coffee: If you bought coffee once and didn't scam the cafe, it is unlikely that you are going to scam in the future...
Bitcoin payments could be extended like this: When you pay for your coffee, you give the coffee owner a signed token, signed with a pseudonymous key/identity just made for the cafe. The key/identity could be autogenerated from something like concatenate(secret-owner-id, domain name of cafe) or similar..
The second time you buy something at the cafe, the owner remembers you and will trust you with 0 confirmations.
And a 'long con' isn't really meaningful with stuff that you'd buy at a cafe.
Also you have to add to that that usually (I'd guess &gt;90% of cases), you go to your cafe in your local neighborhood, and if the cafe owner could tarnish your reputation by rightfully calling you a scammer, the usual social pressures will work to keep people honest.
Really, one should honestly contrast the 'but 0 confirmation problem!!' fear with the reality when using cash: If you don't notice that you received a counterfeit bill right away, you *might* realize it when counting the cash for the day. 
At least, with bitcoin, you *will* notice in an average timespan of  ~10min.
I honestly fail to see this as very problematic. For high value transactions, waiting ~10min or even an hour (selling cars, houses) is not a big deal anyways.
@_date: 2015-05-31 10:31:30
Yes. But instead of saying 'this would be ok to me', the only arguments I repeatedly say are concerns on why it could not work.
Don't get me wrong: Those concerns are often very valid and very informative.
But at some point, you *do* have to risk something because you are a point where you *have to select between alternatives*.
And what Gavin was missing from the others, IMO, are viable, different alternatives. Saying 'no' is not an alternative.
@_date: 2015-01-11 15:07:14
True. I was just trying to point out that expecting essentially zero value for a Bitcoin with a working Bitcoin network behind is impossible.
@_date: 2015-01-21 19:12:58
Actually, if it doesn't run properly on a low end VPS that would be fine with me. His performance testing is for a home computer, centralization concerns are and have always been over not being able to run bitcoind on home computers. The interested amateur case.
But it actually DOES run, and (relatively-speaking) FASTER for larger blocks. The issue here is an unknown change in performance, for the better. With a reasonable suspicion why that is, but no further investigation. As you know as well. And I explained why I like the maybe part.
Now, if you think there should be more VPS testing and are worried about a performance-blocksize relationship that actually gets better with larger blocks: Contact him, I am pretty sure he's glad that you are willing to help...
@_date: 2015-01-21 21:54:12




That O(1) figure, I have seen it in many discussions about the blocksize 'the bottom drops out' - scare - but is it actually true that there will ever be true O(1)?
As far as I can see, O(1) assumes a perfectly well-informed network wrt. to all included transactions before the creation of  the next block, as well as basically speed-of-light propagation, and constant lookup, verification etc. in all nodes.  But that's quite unrealistic, isn't it? Surely, there must be at least some O(log n) lurking in there, somewhere?
Certainly, work on reducing propagation times will reduce orphan cost. But I see this rather as 'more transactions are possible with low fees' than 'the bottom falls out'. And given that Gavin actually does put a reasonable limit in place, I can't see why there needs to be a fight at all about his increase schedule.
@_date: 2015-05-31 21:57:28
If you look at this mess, 40% per annum is very reasonable to include.
Because if getting anywhere is ***this hard*** already...
@_date: 2015-01-10 22:10:25
Of all the things/systems/protocols one can try administering oneself - I find email by far the hardest. And I have heard other people saying very similar things.
Email is certainly decentralized but besides that feels like an old system with lots of warts that simply works well enough so that inertia keeps it running and from being replaced with something else...
@_date: 2015-01-21 22:52:03
That the average is not too important is a good point. However, it would be one incentive (albeit a small one), to sell 'a bit less confirmation delay'.
Another one I see is that even after reading the O(1) propagation gist from Gavin, it appears to me that his proposal is rather something that feels like O(1) propagation in number of transactions compared to what is now, rather than being truly O(1)?
I mean, there are still limits in network speed, in processing transactions on the host, etc.
So there are at least a couple of smaller effects that create a floor in the transaction fees, or aren't there?
@_date: 2015-01-11 17:04:17
If this works well, I can see this being very successful. With the same type of content that made the internet as big as it is...
@_date: 2015-01-21 22:22:08
Sure, it is a market. But I fail to see why there is no bottom. O(1) propagation time, although called like this by Gavin himself, isn't simply strictly true.
@_date: 2015-01-10 22:16:27
That BS part about 'blockchain cloud storage' stood out so much that I stopped reading the rest of the article. That's such a completely ridiculous idea that it takes away all remaining credibility from the rest of the article.
@_date: 2015-05-20 17:53:55
True. However, arguably, the 1MB limit is approaching soon, especially when there will be another 'hype cycle'. 
Maybe it would help then - in the whole blocksize-increase-or-not-debate - if the pro-20MB folks would gentlemanly promise that they'll focus on scalability in the sense of getting payment channels off the ground as the next important goal?
I don't know what has on his mind, but as he actually did focus very much on scalability through blocksize increase, I'd guess he might focus on other solutions to scalability next?
@_date: 2015-01-04 00:23:41
Sure it isn't 69% of the population?
@_date: 2015-05-07 18:44:04
Agreed. I bet we'll see the merge of that 'controversial' patch in the next month. After that, the issue will go away temporarily, only to come up in late 2016 again...
@_date: 2015-01-10 23:51:33
For $5e6 you could have a conspiracy of the necessary people involved. 
Or you do checkpoints under some pretext - export the VM image to backup for a cluster of machines which just happen to contain the Bitstamp ones. Scan the backup.
Or you knock the machines of the network (switch 'failed') thus switch off logs, do your thing, bring it back online...
That's just a few scenarios. All the VM layers only add additional security risks and could have been avoided. You have a lot of potential new holes, more people involved, more facilities, more uncertainty that could be exploited in various ways. Also note that log audits and anomaly detection happen after the fact and are very different from frontal defenses.
A lot more dangerous compared to a seperate server containing the hotwallet that is physically secured, and only talks a very limited protocol for signing outgoing transactions, ideally with bandwidth and coin-rate limits in place.
@_date: 2015-01-07 20:37:16
I believe most $/EUR/... bills test positive for cocaine...
@_date: 2015-01-04 00:27:14
Sure it isn't 69% of the population?
@_date: 2017-02-14 13:08:32


Working on it with patience :) And it will happen.
@_date: 2014-05-14 10:18:12
1024^3 B == 1GiB != 1MiB
@_date: 2015-05-11 09:32:00
We had that discussion already...
@_date: 2015-05-07 23:16:00
I know that one. That's for repeated payments to the same party and doesn't really allow settlement and coalescing of multiple transactions into one settlement off-chain.
The latter is what we would need.
@_date: 2015-05-10 01:37:16
What upper bound? There should be an upper bound with regular old UTXO coalescing and a saturated usage of Bitcoin.
@_date: 2015-05-31 17:12:24
Validate the UTXO set. And coalesce it after a predermined time, put the burden of proof that a certain UTXO exists back to the user.
@_date: 2015-05-31 16:51:28
He believes a blocksize increase -thus hardfork- can be phased in in the short term.
Yet Gavin is more cautious and wants several months for the majority of nodes to make a decision. Thus he is pushing it. Rightly so.
Because the very fact that there is a very heated debate now shows that Gavin is right on that point. Getting a consensus is hard already, it will be impossible when SHTF.
@_date: 2015-05-09 09:08:03
Median would be better to lessen the influence of outliers.
@_date: 2015-05-07 17:54:09


But they are not truly competing if 1MB is not filled either, or are they?


What do you mean by fooling into subsidising?


a) LN does not exist yet.
b) A dwindling exchange rate due to Bitcoin turned cripplecoin will not allow money to flow into LN-related development.
c) The link of high fees on the main chain turning people into supporting LN is very remote with at least a tragedy of the commons in there. Rather, people will go (back) to the competion ($/EUR/...)..
@_date: 2015-01-21 22:31:19
Growth in blocksize is a misnomer. It is growth in blocksize *limit*.
And as you point out, a recent one was only 375kB. So why artificially cripple the network with a 1MB limit?
@_date: 2015-01-04 18:44:13
Does anyone have information on that art guy mentioned in the trailer? Sounded like a 'Vladimir Cerkoviz'?
@_date: 2015-05-31 09:51:31


Isn't the ongoing contention about it proof that this is just not the case?
What makes you believe we won't have similar contention a year down the road, blocks are full, Bitcoin is unreliable as heck and CNN titles 'Nerd money dead - only three people can use it at a time'?
*EDIT: spelling
@_date: 2015-01-13 19:50:01
Well said!
@_date: 2015-01-10 21:40:42
But any randomization could only be done at the page level? You could simply regularly scan all memory for what appears to be private keys.
@_date: 2015-05-30 21:24:20
Well, in that case, a 'true' full node is useless as well. Which one of the replicated Bitcoin histories do you want to trust?
But the point is that this is actually solvable: The longest chain, in terms of hash power, wins. Done.
@_date: 2015-05-26 17:59:28
A little bit of bickering and 'paralysis' might be good though - it shows that things are handled conservatively.
But I agree, 20MB and be done.
@_date: 2015-01-13 19:55:16
That's what Gavin proposes anyways, isn't it? 20MiB limit now (whenever 'now' exactly is) and a doubling every two years, for ten times, up to ~20GiB blocks. About ~1txn/day per person on the planet.
@_date: 2015-01-21 17:34:51
Arguably, notes are less of a hassle than moving chunks of metal around.
No such advantage exists for bitcoin notes compared to raw bitcoins.
That alone makes bitcoins very interesting.
@_date: 2015-01-11 20:31:58
True, but I think it is in everyone's interest that the transition/fork is carefully planned, smooth and painless. That takes time.
@_date: 2015-01-01 23:49:25
OTOH, there is money to be made selling the surplus 'unusable' mining hardware ...
@_date: 2015-01-21 02:14:11
a) technology improves, b) it is an upper *limit*. Blocks are not even filled today yet.
@_date: 2015-01-21 02:07:34
Well said, thank you. Lets support Gavin's proposal and be done with it.
I like to see sidechains in the same hardfork, though.
@_date: 2015-01-06 23:40:36
The block contains the block header contains a version field. I guess the rule could be x% of last blocks on latest version?
@_date: 2015-01-13 10:41:40
Is this Rick Falkvinge?
@_date: 2015-01-06 11:22:36
evade justice == want privacy?!
@_date: 2015-01-11 20:10:28
No, I think what aaronvoisine said is correct, too. Of course, when the cutover happened, old clients will be out of luck. But the idea is to stage the whole process so that a very high percentage of nodes will immediately be able to deal with the new blocks.
@_date: 2015-01-21 21:44:19


Well, is that necessary, though? I think what brought up is interesting and I also think that somewhere in the speediness of transaction completion lies at least one part of the solution to the fear of unlimited block sizes ruining it for the miners:
Let's say that I am a miner and own 5% of the total hashing power. If I deny a certain transaction, it means that this transaction can at most be confirmed with the other 95% of the hash rate. 
IOW, this means that I can shift the average confirmation time of any transaction by about half a minute.  Someone who has a certain preference for speedy transactions is thus willing to pay a certain price for my share of the hashing power - or isn't s/he?
@_date: 2015-01-06 23:17:10
Will every other block size doubling coincide with a decrease in block reward? Or will they be off-phase?
@_date: 2015-05-11 13:40:40


Sure, but I fail to see a lot of use cases where 10min isn't enough bit 1min block time is.


In that case, the whole block size increase debate should have been framed as being tied to a possible decrease in block time.
I think it is bad timing to raise 'well lets reduce block time' at the same time a 20MB increase is fought for.
@_date: 2015-05-09 16:43:59
I remember now why I really dislike bitcointalk - you have to be a hardcore-glued-to-your-screen follower of those threads. Ewww. 
But still thanks for the link!
@_date: 2015-01-22 10:42:43
Why do you think the blocks would be these sizes if the limit is lifted?
@_date: 2015-01-02 20:19:54
Indeed. Interesting times. But think further: The US is trying to get a cold war with Russia going again. If Germany 'pivots' to the east/to isolationism due to internal tensions/uprisings/general expected EU mess - it will be the fall of the US-led western empire.
EDIT: I stumbled across this today - I agree with most of this analysis: 
@_date: 2015-01-10 21:54:45
Completely agreed! 
I would add modularized 'blockchain storage', how much and where to the list (full chain/only UTXO/only last n transactions), on my own HDD, on a shared(?) cloud storage device, in a public DHT, ...
EDIT: I think this is independent of mass adoption - wallets could still come in one piece but be taken apart and connected to each other if necessary (such as a modular bitcoin-core). Right now, there is basically only the Bitcoin protocol and to some extent the storage format as the common ground.
EDIT2: blockchain storage could also be layered - e.g. I have the last couple blocks at home, but I am connected to a public DHT if I need older info and if that still fails I know a reliable archive  ...
@_date: 2015-05-30 20:54:28
Bitcoin data is (pretty much) incompressible. But you can prune, leave stuff out of the chain or push the responsibility of safekeeping some data back to the users of Bitcoin (which IMO is what eventually should be done).
@_date: 2015-01-12 08:32:19
But restricting it from the 2x yearly doubling would only be a soft-fork, compared to the hard fork which is needed now...
@_date: 2015-05-11 09:35:43
I would like to know that, too. The block size discussion so far was sane, pushing for 1min blocks the same way is neither ok nor sane.
That is another can of worms.
Maybe this is just a joke? :D
@_date: 2015-05-07 21:51:16
Only if there are enough nodes and people cross-checking each other and not a single big cartel deciding on eventually doing Bitcoin QE.
Funnily that seems to be the solution space all blocksize increases are discussed in: Create a payment system/currency/money that can compete with the entrenched ones, yet prevent it from becoming like the rest. 
The fear of some is that it will quickly become like the rest, with just few big entities controlling what is happening with Bitcoin.
I see the points of the blocksize limiters, but right now I rather fear it will fizzle and disappear.
And I think a) Gavin's original growth formula was probably fine and more importantly b) 20MB is certainly very doable and has experimentally been shown to work well.
@_date: 2015-01-21 02:16:30
I prefer wise, careful *maybe engineers* who know their limits in knowledge and intelligence to cocky know-it-all-but-not-quite brogrammers every day.
@_date: 2015-05-07 16:05:07
1MB is a hard limit, 20MB is a hard limit.
The spammers do not fill the 1MB limit. Why should they fill the 20MB hard limit?
@_date: 2015-05-09 09:01:37


Yes, but he'll drive up the price per TXN so the other miners have an incentive to collect the higher fees. Sounds very counterproductive for that miner.
@_date: 2015-05-30 21:18:21
I think we are talking past each other, I think because I am a little bit off topic here.
Sure, SPV relies on reasonable chances that there won't be a stack of 1000 blocks of high hash power fooling him/her.
But with UTXO commitments, full nodes could get more lightweight by relying on the hash power of, lets say a year of hashing.
@_date: 2015-05-31 14:18:56


This can be done as a soft-fork, correct?
So why can't we just go forward now with 20MB + 40%/y (I mean the choice; actual blocks in 2016) and do this next?


LOL. Very true :)
@_date: 2015-05-09 15:23:56
Good point. But you'd be dependent on those public servers then. The more I think about it, the more I think just having a 'UTXO-into-hash-tree coalescing schedule' built into Bitcoin would be the easier solution to this problem.
@_date: 2015-05-29 17:32:42


What are those, and how could they be implemented until 2016? How could one help?
@_date: 2015-05-11 09:51:35
Also should be noted that block generation is a Poisson process. So there is no guarantee of a timely validation even with 1min blocks.
It would still not be fast enough for 'instant checkout at the cash register'.
But the other main use case I see, online orders, is fast enough even with 10min blocks. If you are unlucky and it takes several hours to confirm, that doesn't really harm if you get your package a few days later anyways.
@_date: 2015-05-07 20:57:23
Technically correct, but he's still putting up the code for public discussion. 
And if you will, it actually goes counter to your accusation that Gavin is pushing very hard if he waits before he elevates it to a PR, or doesn't it?
@_date: 2015-05-07 19:47:31
I believe the point of was that in a far out perfect world, full nodes could be paid for their archival and transfer capacity and the like.
Certainly not really applicable to the issue at hand here (as the current state of the art is nowhere close to such a Bitcoin system), but I think you misunderstood what he meant by 'everything is paid for'.
@_date: 2015-05-07 20:40:15
I think one argument is that a more efficient block propagation mechanism might drive those costs very close to zero.
I am for an eventually highly increased blocksize (Gavin's growth formula is sound IMO), too. I do see the argument by and others that this might increase centralization though, too.
However, I'd also like to say that I am certain that the incentives of bigger mining players are aligned to keep Bitcoin alive and not destroy it through artificial anti-competitive data, at least in the midterm.
I also think that in the endgame, only larger entities might have  full nodes. I think Bitcoin would be fair and fine if there is only, lets say one transaction processor in each bigger city and if a dedicated hobbyist or smaller group (lets say a computer club) is able to set up a full node.
@_date: 2015-01-04 23:12:47
@_date: 2015-01-21 03:03:25
As far as I see, 'the maybe' could well mean 'it could be the overhead of many small writes to disk'. As in, he's aware that this can happen, but doesn't know whether it is actually the cause.
IMO a very reasonable way of handling and stating the issue.
@_date: 2015-05-09 15:58:49
What do you think about automatic coalescing of UTXOs?
@_date: 2015-01-06 22:37:30
I think the fact that we don't have blocks being hardlimited at 1MiB all the time shows that there must be another mechanism acting to keep block sizes down - either network delays / orphan cost or something else?
 
@_date: 2015-01-13 15:20:41
Very true. It also ensures that the price will be so low again that no one will be able to argue anymore 'but Bitcoin is unfair, it rewards the early people!1!!'.
Also, I think that as soon as the price is high enough that people will only buy/be able to buy small amounts per person (~$100), price movement will not be as big as not as many people (in the 1st world) care about losing or gaining $100 that much.
@_date: 2015-01-06 23:12:47
I see. Eagerly looking forward to see a rollout of maxblocksize increase and sidechains :)
@_date: 2015-01-05 12:39:31
The mark of powerful technology is that it is beyond good and evil. Think about it.
@_date: 2015-01-11 19:23:07
As far as I understand, Gavin is proposing a 20MiB blocklimit with the coming hard fork. The blocklimit will from then on automatically double in size (built as a formula into the software) every two years. So the fork would indeed remove any needs for further increases.
@_date: 2015-05-08 11:43:14
I'd argue what Satoshi said is not so much prophetic as it is part of the Bitcoin social contract.
7tps is not the vision of Bitcoin, it is Cripplecoin.
@_date: 2015-05-05 17:13:25
Solid reasoning?
Blocks are not full yet. According to the 'blocksize must always be limited'-crowd, they should be full of spam, shouldn't they?
@_date: 2015-05-29 18:16:45
Yes, but initially, 1MB was completely oversized compared to the actual usage. If you look at actual growth, 40% isn't too far off.
That said, you brought just  one of the best arguments against block size limits at all: 


Think about it.
@_date: 2015-05-27 16:10:22
Great. So lets have 20MB in the short/mid term, and lightning networks in addition in the long term!
@_date: 2015-05-08 09:10:26
Another bug report: Is very slow when used on firefox and being zoomed out.
@_date: 2015-05-07 18:36:34


100GiB. Done. Enough for several txn per day per person on this planet. Obviously not needed now, but maybe eventually.
Sounds like a lot, but is really only 1.7GBit/s network speed. Doable in 20 years on a home connection?
I'd say yes.
@_date: 2015-01-06 22:56:39
I see! Interesting stuff. Thanks alot for the detailed answer. Much appreciated.
On the zero knowledge verification, is this what you describe as [CoinWitness](
@_date: 2015-01-11 14:56:27
How can Bitcoin go 'bankrupt' except for a 51% or other successfull attack on the network?
@_date: 2015-05-31 16:08:03
Exactly. Even Satoshi was expecting full nodes to be in data centers anyways.
That said, I am not against trying to keep the traffic down. But more importantly, I think we should concentrate on ideas on how to prune while keeping quasi-full-node security - so that running full nodes can be done without the storage and initial sync.
@_date: 2015-05-07 22:43:25
And there isn't the single big bad spam-miner out there who constantly produces 1MB blocks. 
So why should there be someone like that when we have 20MB blocks?
@_date: 2015-05-21 10:43:27
@_date: 2015-05-07 22:07:21
So maybe a compromise is then the right solution? I think Gavin's initial proposal of 40%/y growth until 16GiB blocks made a lot of sense. I still support that initial proposal, but will be behind simple 20MB blocks, too. Because it really needs to be solved now, even if it is only a temporary fix.
@_date: 2015-05-30 20:31:34
What about UTXO commitments and keeping by default just - lets say a year worth of transaction data on full nodes?
From the trust perspective, that's IMO just an infinitesimal difference to 'true full nodes' (if at all, depending on your perspective). No way a screwed up UTXO set wouldn't be noticed within a year by *someone*.
EDIT: And adherence to the coin issuing schedule can always be continuously checked by including the sum of the outputs in the committed UTXO merkle tree, and making sure the root's sum matches what is expected.
@_date: 2015-05-07 21:35:49
Not all nodes are mining. Most are 'just' validating. Bitcoin needs many nodes to be outside the possibility of strong-armed regulation.
The argument is that centralization is more efficient than any decentralization and thus Bitcoin will eventually become so centralized that its core values are lost and compromised.
@_date: 2015-05-10 02:10:06


Where is the spam filling the 1MB blocks?
@_date: 2015-05-31 15:05:10
If you assume malice, like seems to do to some extend, concern trolling without partaking in a *productive* discussion is an effective stalling tactic.
I am not willing to do that as I have not seen Greg being malicious in any case. But the effect is the same.
@_date: 2015-05-29 15:49:02
*If* this is solved, Bitcoin will be ready... and then, 20MB is not enough for 7e9 people still.
@_date: 2015-05-10 13:31:30
Pessimist, Optimist, Realist - somewhere in the middle, things will probably meet :D
EDIT: Another thing: The incentive to do this mass centralized 'well-meaning' control might disappear, as there is not so much to gain anymore from Bitcoin by inflation etc. in a big central banking scheme. And smaller banking/governments might be more effective in cross-checking each other against runaway power concentration effects.
And if the 21mio limit will be successfully attacked by the EUSA, Bitcoin will be worthless and/or replaced by something 'underground'. Unless people are convinced it is a good thing. Mind control chips?
I see and share your worries but I think this whole thing is far from played out in the clear direction of 1984.
@_date: 2015-05-09 16:48:04
Exactly. That's the most important part.
@_date: 2015-01-11 20:23:37
I think so, too. I think the hard 1MiB limit it is also perceived as a major uncertainty in Bitcoin right now. 
For a while, I was also convinced that a dynamic scaling would be better - but the formula is very hard to get right. And a lot of the dynamic scaling would open up the possibility that miners force centralization by spamming the chain until the block size becomes really cumbersome for small guys.
Most people who are ***really*** into Bitcoin would be able to handle 20MiB and a TB/year total growth right now - and more in the future with cheaper HW, hopefully twice as much every two years...
***However***, noone really knows whether there will indeed be enough cheap storage in the future - so there is a bet involved. Considering that there are ~10^23 atoms per couple grams of 'stuff', and assuming there could eventually, realistically be technology that fits an addressable bit into every 1000 atoms, 12 grams of carbon (lets assume that's the storage medium) would be able to hold ~95000years of fully 20MiB filled blockchain.
***However***, I also expect Bitcoin to become a) more forgetful and think that b), there could be a systems that puts the burden of validating a transaction on the sender of the transaction instead of the network. So that maybe, maybe, in the more distant future, all that needs to be kept of the decentralized blockchain are the headers, which is less than a CD-ROM full of data for 100years.
@_date: 2015-05-09 09:02:15
If at all, use median instead of average. More robust.
@_date: 2014-07-26 16:44:44
Wanna catch it while it falls?
@_date: 2015-05-31 22:14:50
I think Gavin has the right approach, and I think he'll prevail.
@_date: 2015-01-13 20:04:21
Then explain why the blocks are not hitting the 1MiB limit constantly already.
@_date: 2015-05-11 09:49:08
Sure he is but given that the blocksize issue is really the top priority for him and apparently the most contentious issue in the Bitcoin space right now, it might not be wise to step into another potentially very contentious area now.
Personally, I don't see the need for 1min blocks. If there are really good reasons to have them, maybe. But:
- they are a Poisson process anyways, so you'd still have not guarantee that the block will arrive soon enough
- a change touches very core things such as the Bitcoin issue rate schedule!
- increases the amount of data that absolutely has to be stored by nodes over time (the block headers) by a factor 10.
My impression still is that he's joking.
@_date: 2015-05-01 10:17:29
The question is whether patents solve that system. It appears that patents change the equation from 
'deep pockets to set up a competing assembly line for a copycat product'
'deep pockets to have an army of lawyers to crush you into the ground if you are a small, but able competitor and to generally raise the barrier to entry into ANY field by basically requiring you to hire a bunch of lawyers in parallel to R&amp;D to check for potential violations'.
I actually rather like to have the former than the latter - it is blatant, but honest, and noone falsely pretends that patents somehow save the small guy.
Maybe a *vastly improved* patent system could solve the former without causing the latter problem. For one thing, it would definitely widely exclude patents for all things software - conventional copyright is simple to follow for *everyone* (without needing to hire lawyers) and absolutely works well enough to prevent copycats.
@_date: 2015-05-07 19:43:21


Citation needed.
@_date: 2015-05-09 14:39:51
Ah, ok, got it! Stupid me, can't read :D
Well, yes, maybe that would work. As in simply have a constraint like the block size on the UTXO growth, to basically prevent Gavin's worst case scenario?
It would still mean a pretty big unprunable UTXO set in the far future, though.
@_date: 2015-05-09 09:05:19
Arguably, though, if the original spam protection measure would have been such a scaling measure, it would probably have stayed the way it is until now - and if it mostly works, no one would even think of changing it.
So this whole fuss is kind of an early problematic decision biting us.
@_date: 2015-05-31 10:52:21


Why do you think so? 133MB blocks (as required by LN for us full 7e9 people) I see as definitely viable in 20 years. 133MB blocks with efficient transmission means about 220kByte/s per connection.
My home connection already has more than that. I know that there are home users who have 10x-100x more than that, so are fully able to support a thusly scaled up Bitcoin ***today***, with a high fan-out.
@_date: 2015-05-07 23:12:25
Maybe time will show that the blocksize worry really is unfounded.
Apart from the initial spam problem long ago, there hasn't really been *any good experimental* data hinting that bigger blocks might make Bitcoins fail through centralization. There are some theories that it might.
But I'd say the status that currently, *not all blocks are filled*, even though they should be full of spam, is an experimental hint that a limitless block wouldn't really be an issue.
A larger 20MiB size could gather some data on what will happen with regards to fees etc., without really compromising the chain. If spam doesn't suddenly appear en masse, no limit should be fine.
@_date: 2015-05-08 20:22:23
If you think about it, 1.5years is actually quite the improvement, if it is about accessing your savings that often. Not enough by far, but a wholly different realm than 30 years.
But in another fifteen years, do you really believe our network capacity will be limited to 20MiB blocks?
I am still optimistic that the infrastructure for at least GB sized blocks is available then. And I am not arguing against another layer on top of Bitcoin that only does settlement on the chain.
But even if you settle on the chain, that process needs to be reasonably accessible to anyone on this planet for Bitcoin to be successful.
EDIT: Spelling.
@_date: 2015-05-09 09:31:03
Or you could say: All old coins until date XYZ stay valid - and after that, each UTXO is only valid for 5 years. That would be less unfair, only unfair insofar as there is a cost of holding Bitcoin for new people that old farts that keep their addresses do not have to pay.
@_date: 2014-12-22 17:21:24
If the miner starts off a block and introduces invalid outputs into the UTXO, it would fail validation at all other nodes.
@_date: 2015-05-09 15:54:44
Yes,  someone very early on discussed this as [MTUT]( I am very much in favor of something like this as it would also put all the concerns wrt. full node vs. SPV node to rest. You could do a lot of pruning without losing integrity.
In my earlier thoughts, I was thinking it would be enough to just store the merkle tree of one's own UTXOs for a 'recent enough' block UTXO merkle tree hash. But then any full node would still need to know whether the UTXO got spend between that point in time and the top of the blockchain.
Coalescing and expanding of UTXO trees would be explicit in the sense that anyone using such a scheme would know which data to keep to be able to expand the tree again. With block-wise UTXO trees, that data would change with every block.
So.. I see value in both, definitely also having a continuous Merkle tree. But I think they do not fit nicely together, or at least I can't see it right now.
If I get the time, I think I should try to simulate UTXO growth and transaction size from the current blockchain data, if a regular coalescing scheme would be implemented.
EDIT: Do you have a link to your post?
EDIT2: Just to make it clear, I'd calculate hash(sum-of-unspent-value, concatenation of hashes of children) at every level and keep the top hash + the unspent output somewhere, preferably the block itself. That would ease all 'hidden inflation concerns'. Because the top would be sth like 'merkle-tree-root, money-in-it'.
@_date: 2015-05-06 20:33:32
-5.2% interest?
@_date: 2015-05-31 14:51:08
I have to agree. I think Greg's completely honest and good with his intentions, but going from block size is going to be an issue soon' to 'we need diffuse, undefined centralization metrics' next is, in essence, going to derail the discussion by opening another can of worms and continuing the deadlock.
Gavin wanted clear objections, and I think his original plan of 20MB+40%/y is a good compromise. 
@_date: 2015-05-09 09:29:05
Which should be investigated. This would only be the *active UTXO set*, as I wrote above.
All the old cruft stored in UTXOs that will mostly not be accessed again, it is not part of that *active UTXO set*.
Lets say you look at the size of the UTXO set that all successful lookups per last day happen to - I'd argue with caching &amp; bloom filtering of the rest, the old cruft can be safely ignored for performance reasons.
That active set will also scale with blocksize but I am pretty sure it is much smaller than the graph Gavin shows.
@_date: 2015-05-10 02:23:37
As a supporter of Gavin's proposal, I agree that it is small, but I hope there is nothing odd lurking in changing from a constant at compile time to an inline function depending on time.
Just saying.There have been the weirdest of bugs.
@_date: 2015-01-06 22:29:07
Somewhere on bitcointalk.org or similar, I read about a VM for verification and having a thorougly tested/proven consensus code implementation on top of that - kind of a runnable formal specification for valid transactions.
Is that something that is being worked on?
@_date: 2015-05-21 19:15:00
Just wait and see. You'll understand.
Meanwhile, [this]( might speed up your thought process.
@_date: 2015-05-09 14:35:13
Or you make it the responsibility of the users to provide the long-term storage with something like automatic UTXO coalescing.
@_date: 2014-04-20 02:47:04
With regard to point 2., I think most of the sidechain functionality would be there if the full scripting language that Satoshi thought of for Bitcoin would actually be enabled. The side chain change will AFAIK only allow a few new script tokens.
So it wouldn't really be 'fixing a broken Bitcoin' but rather enabling a use case that was intended to be allowed right from the start - but wasn't, due to security concerns.
@_date: 2015-05-10 02:28:51
Exactly!! This is a *hard limit*.
The demand is pretty inelastic in any functioning transaction system (just look at people bitching when a CC processor goes down) and the supply at the point of the hard limit is absolutely inelastic. Sorry dude, no more txn possible!
It is not as if new factories can be build to produce bigger blocks.
@_date: 2015-05-30 20:51:42




Isn't the fast relay network an almost O(1) implementation that is running and working in the wild?
I fail to see how this is possibly more theoretical than LN/SC.
@_date: 2015-05-21 06:07:48
Make a law on top of the already baroque legal landscape, making it even more bizarre. Then, sell the path through that jungle you created. The law to the product. How symbiotic of a relation.
Yuck. And, yes, I agree, I have seen that many times already, too. I still want to be believe in NALAS - not all lawyers are sociopaths. But its getting harder every day.
@_date: 2015-05-09 11:36:54
Instead of expiring them, you could combine them into a hash tree, store only the root hash and then require people to spend them to provide all necessary info that their coins are actually part of that hash tree (the tree branch leading to their UTXO(s))
On the blockchain after a transaction, you'd then expand the single root hash to the top hashes of the subtrees of the other UTXOs that are necessary for other people to spend their coins. That data would need to be part of what an 'old coin spender' supplies to you.
With a regular, pre-determined schedule to compress the UTXO set, you can probably make it constant in size over the long term.
@_date: 2015-05-09 16:07:42
@_date: 2015-05-31 17:00:13
Fine. I'd rather have a working Bitcoin that scales and doesn't have colored coin support or stealth payments, than one that is crippled to 1MB. Because this is the original vision. A scalable Bitcoin. The other things are later additions.
And, by the way, no one stops anyone from keeping all transactions, for colored coin support or the like.
For some reason, people got used to 'full transactions are available at all times'. Rather, addicted. This is something that shouldn't be part of the network's core design. Again, if *you* want to keep all transactions, do it.
I am only interested in a validated UTXO set, and that can arguably even be shrunk, putting the burden of proof that an unspent output exists back to the user.
Which, by the way, is a good direction to go to keep Bitcoin lean and decentralized.
@_date: 2015-05-10 02:19:04
Awesome graph, thank you for this! Very convincing!
@_date: 2015-05-09 14:58:36
As in 'the corresponding addresses are old, and the most recent transaction involving that corresponding address is long ago'. I should have written that better.
@_date: 2015-05-27 16:12:07
The point is that 20MB gives us enough *time* for something like Lightning Networks to come together in time so that we have both.
And even they talk about ~133MB blocks for a full 7e9 people using Lightning Networks scenario.
@_date: 2015-05-07 20:33:40
This is really the strongest argument for unlimited block size. Blocks are not full because miners do not like to make them full.
Spam is not filling blocks because miners do not spam the blockchain.
@_date: 2015-05-01 10:35:11
Depends on who you call the government... as in 'actual' vs. 'official' government...
@_date: 2014-07-17 19:33:31
I think the negative pressure comes from something that will soon evolve into a crash:
People have high expectations now that bitcoin will rise, by looking at 'past performance' and seeing all the positive news that bitcoin acceptance (but not so much its volume) is increasing. Now they expect since a while to ride the next bubble but are slowly getting disappointed. I think this will accelerate and crash the price way below $400 soon.
@_date: 2015-05-09 16:54:42
How about coalescing them instead, as I describe in the [thread here](
Your change would help in making by scaling the set in a constant factor but it would still require wallet operators then to save some extra data.
@_date: 2015-05-06 07:10:01
I actually think Gavin intended the original formula with the 40% growth just as he said. But if the 20MiB cap part shows that everything is going to be fine with any limit (no spam problems), we'll still arrive at a good, size-unlimited Bitcoin indirectly.
@_date: 2015-05-05 20:18:26
I like your idea but I think finding the right fee schedule will be even harder than convincing people of a simple growth formula/limit. It will also be hard to get right. 
Note that Gavin initially proposed 20MiB ***+ 40%/year***.
That latter 40% per year got struck out, apparently because other devs are worried. Well, ok, so this one 20MiB is IMO very conservative but certainly helps in the short term.
I guess a similar discussion will come up again in a couple years or so and hopefully then, the limit will be changed to something like his original growth formula. I also hope data will be available on whether a block size cap affects the transaction rate below the limit at all (is spam actually happening?)
We'll see.
@_date: 2015-05-30 21:07:05
Or the originators of transactions keep all necessary branches of the transaction merkle trees,  showing the path to their unspent coins.
@_date: 2015-05-09 15:44:05
Well, I am unsure whether this might be workable but it seemed to me after some pondering, so that's why I wrote this post :D
Regarding tricking nodes: My line of thought so far was that it will appear in most parts of Bitcoin just like a single address (of maybe a new type 'hash tree') with more value in it and special requirements on how to spend it. Do you have a more specific idea what could go wrong for SPVs?
@_date: 2015-05-07 20:48:00
Do you remember the 250kiB soft limit issues?
I certainly do. It was disastrous, even though only the *soft* limit. And it clearly shows that you do not want to have saturated blocks.
@_date: 2015-05-31 14:59:34
Exactly. Satoshi's 2008 paper and his early forum posts set meta goals for Bitcoin. Including bigger blocks.
@_date: 2015-05-21 08:06:32
Your list is incomplete without at least one link to the critics:
FWIW, I neither think those critics are 100% correct nor that 'dark enlightenment' is our prescribed future.
@_date: 2015-05-09 13:34:47
So then the small active database is the cache to the bigger megablock?
We have that already! The UTXO set is in a cached database. Those UTXOs that are recent will be in RAM, the older ones on disk. Gavin's worry is that the RAM set is soon too big, and he's worst-case-assuming that he needs every UTXO in RAM. Right now, as others have noticed, only a constant fraction (100MiB) of the full UTXO is in RAM by default. And so far, nodes keep on chugging along nicely, no problems there.
A growing Bitcoin is probably going to need more than 100MiB for the active UTXO set - but isn't a first rough guess that if blocksize is scaling with user-base, and UTXO set is scaling with user-base as well, that we'd expect to see a need for maybe 20x the active cache size, so about 2GiB?
With today's hardware, 2GiB would be totally doable.
@_date: 2015-01-11 15:05:15
Sth. going on in Ohio?
'Satoshi Nakamoto created Bitcoin" --&gt; Satanic remittances took Ohio bad
Uhoh ;)
@_date: 2015-05-08 08:49:40
He suggested that connection, which was very disingenuous. 
Because he deleted his own post, it is hard to quote now what he said. But that action supports that it wasn't really appropriate to say, doesn't it?
@_date: 2015-05-24 15:35:05
How accurate are 0.15%, what are the statistical error bars here?
@_date: 2015-05-09 14:15:06
No sorry if I was unclear. The UTXO set is *all* unspent outputs. But only the more recent ones are in RAM, the rest on disk. And we do not run into problems with disk storage yet.
The blockchain size itself, that problem is (except for UTXO) mostly solved, I think. Even in his original paper, Satoshi showed how you can prune it without losing vital information.
Arguably, we just got used to enjoy all storing the full, unpruned blockchain, because we can do all kinds of interesting analysis on that dataset. But it isn't necessary for everyone to store the blockchain in full for Bitcoin to be a decentralized system.
Also, only the UTXO set alone would be enough to prove that one very vital information that no inflation of the money supply occured in Bitcoin. Simply sum up all unspent outputs, and with the inflation rate schedule, you know the exact what is to be expected.
If the outputs get coalesced into 'pay to merkle tree' hashes, they'd still be labelled at every level with the amount. Summing those would be even easier :D
@_date: 2015-05-07 20:22:46
The increase in blocksize is having consequences for *full nodes requirements, network requirements, orphan blocks, fees and so on*, as well.
Hitting the hard 1MiB limit severely so, and we ***know*** that.
@_date: 2015-05-31 15:04:00
In any case, the matter of fact is that Gavin's proposal was never taken seriously in the sense of either giving counter-proposals or *constructive* input.
That much is clear from these discussions.
@_date: 2015-05-07 23:01:36
How is paying for fees an incentive to pay for LN and not exit the Bitcoin system?
I think LN will happen because people with foresight and genuine interest in Bitcoin will make it happen. But it will take a while. And until then, 20MB maxblocksize keeps Bitcoin going.
@_date: 2015-05-07 19:56:34
Gavin's patch is introducing very little new complexity, though. Two small inline functions to make the timestamp-based switch. 
Is this really comparable to a BIP with outright *new* functionality?
@_date: 2015-05-11 14:42:01
Yeah, maybe you are right and it isn't really a problem, I remember calculating that all 10min block headers for a century fit onto a CD-ROM - so it would be a DVD instead.
I still like to see a comprehensive analysis of all the trade-offs with a higher block frequency. Especially if this is going to be the way forward for a higher throughput of the network.
@_date: 2015-05-15 15:17:28


Very true.  However, if the vast majority agrees to make an incompatible change, you'll be left behind on your branch-coin that few people will keep using. So the trade-off would then be to lose coins or go along, which is probably pushing people to keep on having similar goals.
I think Bitcoin's governance is metastable in a way. When enough pressure comes together, hard forks will happen. But until then, everyone will keep the status quo.
@_date: 2015-05-31 15:06:23
In some ~20 years. (10 doublings, 2 years per doubling gives you 20GB blocks) Fiber-to-the-home should be default by then. And if they become problematically big by then, smaller sizes can be soft-forked in.
Also note, this is THE LIMIT, not the ACTUAL SIZE.
@_date: 2015-05-31 22:06:08
Gavin isn't divisive. That is a total misattribution. ***The issue at hand is itself divisive.***
Hard fork or 1MB means to go in a particular direction. There is consensus Bitcoin eventually needs a higher than 1MB blocksize cap, i.e. hard fork.
Gavin wants to make sure that this happens smoothly and planned well in advance. Since friggin' ***years***.
He's often accused of 'kicking the can down the road' - Arguably, this is more applicable to devs who say 'lets just decide on this in the last minute'.
@_date: 2015-05-29 18:39:22


This sounds very reasonable, but it also sounds like the main concern here for you is UTXO memory set size, not bandwidth or storage cost? Wouldn't it also still allow spamming the network with big blocks (that create no UTXOs)?
Wouldn't the long term solution to the UTXO set size growth be rather a coalescing of old outputs by hashing them together and putting the burden of storage of old outputs onto the end users of Bitcoin? ([I tried to think a bit about such a scheme](
I had the impression so far that blocksize limits were focussed on the economics of mining income and bandwidth cost wrt. centralization.


Was this ever discussed to go along with an increase in block size? I am not a core dev, just a very interested user: This rather sounds like it would be integrated into a bigger blocksize discussion anyways. See also below.


This is extremely hard, as it is so political. When people are forced into offchain settlements due to very small blocksize, is this decentralisation or actual centralisation (3rd party payment processors)?


Understandable.  This might be a wrong impression on my side, but: From the outside, I have to say that it looks like Gavin repeatedly tried to get this whole discussion to a point where it is actually productive in outcome and failed. 
But it really looks like every time he talked about a block size increase, he ran into a wall of disagreement and eventual indifference of you, petertodd and the others. I would have expected your points to come up in this discussion and be addressed back to Gavin.
Because in the end, the transaction rate curve looks like it really will hit a limit just soon after 2016. And transactions are IMO not very elastic demand - only insofar as they'll drive people away from Bitcoin.
@_date: 2015-05-29 22:27:50
Yes, of course. Authority we give him.
@_date: 2015-05-29 17:35:34
Is this what is going to go into Bitcoin XT? The extra +40%/year? This is still unclear to me - and seemingly a lot of other people here.
@_date: 2015-05-09 14:17:48
I was wondering if that is possible, too. But I am doubtful. The hash encodes history, including history of others. And a deterministic way would necessarily not allow any history to happen.
@_date: 2015-05-24 15:30:18
Are you saying there is code existing that allows to automatically fire up a specified topology of bitcoind's? Where can I get that?
@_date: 2015-05-11 21:47:30
That discussion about the 2nd increase would not happen in Bitcoin nerd circles anymore, methinks... rather, it'll be _very_ political. 
@_date: 2015-05-31 14:53:48
True in a way, but what is built before the real crowd arrives can be very important. Creating the landscape, if you will :D
@_date: 2014-07-18 18:57:12
Ah I see. Well, someone downvoted me, that IMO points already to a dangerous echochamber effect, positive-feedback-loop that can be broken and the price crashed :/
@_date: 2019-08-28 17:06:28


I guess I should know better than to reply to a troll. But you do realize that Lightning sits *on top* of Bitcoin and thus not only depends on its annoying mempools and evil miners but also on the *Lightning intermediaries* as another layer on top? In other words, Lightning's centralization is `max(bitcoin's-centralization, lightning-network-centralization)` and strictly equal or more than Bitcoins ...
That said, you of course do have a point that there is still a third party involved in Bitcoin transactions - the network. However, that is decentralized and merely acting as a distributed timestamping server for your transactions.
It is the known minimum viable way to solve the doublespending problem in a dencentralized way, and that is true for both Bitcoin and Bitcoin Cash by the way.
With the mempool being rather a (selfmade) problem for your favorite coin.
The only *real* P2P money there is in this sense precious metal or exchanging IOUs in a world where everyone is perfectly well-mannered, just and honoring all those IOUs always correctly. Now why only do I get the impression that folks like your are about the farthest away from possessing such character traits as one can imagine?
@_date: 2019-08-28 17:12:44
I was expecting a move like this a long time ago. I am a bit positively surprised, actually, that user posts are not rewritten yet to fit the narrative. I guess I am slowly warming up to "blockchain based" forums and other social media (or at least those that have built-in timestamping facilities).
@_date: 2015-08-27 20:32:27
The weight that you guys put on keeping a blocksize cap shows that it is an important variable to you - but certainly not to Satoshi. He would have mentioned it.
@_date: 2015-08-13 08:54:07


Wondering about this - isn't the BIP66 debacle then basically showing that a softfork also has about the same risks as a hardfork for a chain split?
Was it the SPV mining or rather the old software producing the then-invalid block that caused it?
So with less than 100% miner support, you'd also always get the chance of a chain fork with a soft fork.
@_date: 2015-08-17 08:41:44


This. A bunch of folks self-appointed. They have no authority over me. I understand Bitcoin well enough to not be bullshitted by sneaky power grabs hidden behind 'expert opinion'. Yes, I am certainly grateful for them having the time and money to work on it productively (ironing out bugs, sane new features etc).But I am not at all accepting that some of them get drunk of their perceived power and want to change Bitcoin to something else while referring to 'being experts' and 'Authority'.
Doesn't fly. Bitcoin is as much my Bitcoin as it is Adam's or Gavin's Bitcoin. Bitcoin/Core (or better, Bitcoin/1MB) is as much a valid client as Bitcoin/XT is. But not a single bit more. And the latter follows the spirit of Bitcoin for me. Go and disagree and run Bitcoin/1MB. Core is not special.
@_date: 2015-08-16 22:06:19
Yep. A tough call. And any invite-only club of reasonable people has the danger of becoming 'the enlightened ones'.
I want a reputation system where I can whitelist certain people and trust whitelists from others. And it would be even better if that is done by cryptographic signatures. Maybe that would bring back sanity.
One can dream :-)
@_date: 2015-08-11 09:47:44


I guess it is rather a crude expression of what you'll simply arrive at when combining all the data on these two people...


There have been a lot of censored XT posts and theymos, a mod here, even defended his actions. Did you really miss that?
 
@_date: 2015-08-07 09:02:48
I fail to see how Bitcoin as *just* a settlement system is in any way policy neutral.
@_date: 2015-08-19 13:43:20


I comprehend him quite well, or can you point out specifically where I miss what he says?
@_date: 2015-08-15 22:02:03
Wait and see. You'll be surprised :-)
Positively, I might add. But that only much later. When it reflects in the price and you see it. After your anger against 'HearnCoin' is gone.
@_date: 2015-08-12 14:40:48
Oh, are you one of those worried about your beloved *decentralization*?
As you might know, that discussion had taken place here as much as you can possibly endure it. Satoshi saw bigger nodes in data centers eventually, a scenario that is still very decentralized. Gavin's growth schedule has a reasonable chance of keeping it even in the hand of determined hobbyists. So decentralization is a complete non-issue and not a question of importance with regards to the 8MB+ patch. Next one please.
@_date: 2015-08-15 19:23:20
Have a look at this: 
It is highly like those are his coins. Thus he could prove (but maybe with multiple keys) that he's indeed Satoshi.
And also, I believe he still has an alert key?
EDIT: And there is the key to the coins in the genesis block...
Satoshi still has enough ways to prove his identity.
@_date: 2015-08-11 17:09:05
It is relevant in a bigger context of censorship and positions/websites/... of authority in the Bitcoin ecosystem?
@_date: 2015-08-15 18:49:00
There is a 'only-bigblocks' variant that you can use if you do not like the other changes. I think it unfortunately got lost a bit that it exists.
@_date: 2015-08-15 20:14:13
What is non-trivial? 3?
When 75% of hashpower support XT, how is censoring XT as an Altcoin any different to ignoring hashpower consensus?
@_date: 2015-08-12 10:17:15
So people who have a (too) narrow view on the code and cannot see that Bitcoin could work very well with full nodes in larger data centers - (as intended by Satoshi, btw.!) congregate and form a company that then sells solutions to that *perceived* problem?
Now it sounds even more likely that the conflict of interest is real.
@_date: 2015-08-16 18:24:02
Yes. Somehow for me as a non-native English speaker, something in capital letters looks more important, though :D
But I even think 'Core' is a misnomer now. Rather, we should call it Bitcoin/1MB or something neutral but comparable to Bitcoin/XT.
As 'blockenlargers', lets go and consistently call them Bitcoin/1MB.
@_date: 2015-08-17 22:16:04
Ok. What does our proposal change i this picture?
@_date: 2015-08-19 14:17:19
Contrary to payment hubs, I do not see how Lighthouse would harm Bitcoin. But I am certainly open to an explanation for that.
@_date: 2015-08-13 09:21:20
Which is good - or do you want to have this very same fucked-up situation repeated every couple years?
@_date: 2015-08-15 19:29:23
Excuse my ignorance, who is 
@_date: 2015-08-11 09:12:41
He's talking about the miners.
@_date: 2015-08-13 17:44:49
Why so? The longest chain would win and assuming that e.g. XT forks off the main chain but then fails - all XT nodes would eventually switch back to the good old 1MB chain...
@_date: 2015-08-15 20:01:21
It might still be a different account on vistomail.com with a forged address for the ML post? Or vistomail.com might be involved in the shenanigans? It changed owners after Satoshi used it...
If he's trying to get a message out, he can easily sign it. And he knows that is the only way for him to be taken serious. And he *definitely* knows how to do that properly...
I'd say quite the length that 1MBers go to fool people.
@_date: 2015-08-16 14:07:42
Exactly. It is about taking one very important roadblock out of the way. I welcome Lightning Networks and so forth, but only if they are not forced on top of Bitcoin by crippling the blocksize.
Functioning LN and the increase schedule that is now in XT would give the possibility for global use of Bitcoin in some twenty years for basically all transactions - while you can still transact *on chain* relatively often and while there is a good possibility that a Bitcoin full node will be able to be run by a dedicated hobbyist. With the XT increase schedule, running a full node will extremely likely always be within reach of a small organisation.
What is not to like about that?
@_date: 2015-08-16 23:39:29
Please tell us when you are evicted! Thank you :) I think many of us are interested what goes on 'behind the scenes'.
@_date: 2015-08-17 08:37:37
@_date: 2015-08-16 22:08:12
Oh yes! I have this idea of implementing an awesome browser wallet. I want to find the time to do that. But I really need the peace of mind that Bitcoin is not destroyed by the current special interest first.
@_date: 2015-08-13 09:18:13
That sounds pretty irrational?
@_date: 2015-08-12 10:11:55
@_date: 2015-08-16 20:03:11
Yes they are. Regardless of the fork. If there is going to be ambiguity and a fork battle, it probably makes sense for you to wait until the dust settles before you are going to spend them, though.
That Bitcoin can hard fork is a feature, not a bug. The current XT proposed hard fork would be Bitcoin routing around the damage of people trying to instate centralized control without just having the good of Bitcoin in mind.
Hope that helps.
@_date: 2015-08-16 10:23:46
And WTF is consensus, please?
@_date: 2015-08-17 08:45:44
This. A guy *without tunnel vision.*
A rarity in those who are public or well-known Bitcoin speakers.
@_date: 2015-08-16 23:35:57
Yes. Actually just making moderation an audit trail would go a long way in making it more civil. An audit trail for up/downvotes would be nice, too.
@_date: 2015-08-16 09:41:11
@_date: 2015-08-10 08:55:44


With your own, very weird definition of consensus.
We do not have consensus on blocksize right now, so Bitcoin/QT is consensus breaking as well?
@_date: 2015-08-09 22:56:01
voat.co when I tried to register last time.
Update: You are right, it works now!  Thanks. I just registered and my nickname was even available still. Yay! :D
@_date: 2015-08-15 22:20:04


^ THIS.
@_date: 2015-08-16 20:51:27
Well, as far as I understand the whitepaper/presentation (I really like to see this thing working in practice - I have to admit that I still have a lot of things that are unclear to me), LN gets more efficient by coalescing settlement transactions. If Alice sends Bob money and Bob forwards that to Charlie and if they are all users of the same hub, the hub just needs to publish a 'Alice sends Charlie money' settlement transaction.
AFAIK, this should also work across hub boundaries, but I bet it will be a centralization driver because hubs settle regularly (reinventing the clearing house) or similar schemes will be in place.
I think LN is great as long as we have a capable layer-0. With eventual 8GB blocks, it would allow every person on the planet to make a transaction every couple days. Enough for flexible use of Bitcoin, but there would still be a lot of work to pick up by LN and similar schemes. 
That's why I also think Gavin's proposal hit a sweet spot. 
And with regards to all the worries about centralization and government interference: You also definitely want your average PC to be able to serve as a LN hub and that maybe even behind tor. Because else those things have a chance of being regulated even more than Bitcoin full nodes and miners.
@_date: 2015-08-18 11:08:12


In any sane universe, nodes with more advertising and that are referred to more often on the net are more popular.
@_date: 2015-08-16 19:57:00
Funny because Litecoin, as a much *smaller coin than Bitcoin*, already has an effective 4MB cap.
So you say you are leaving for a coin with bigger blocks? I can understand that. But I'll be patient until XT will push away the deadlocked Core.
@_date: 2015-08-16 14:40:17


Only when they play nicely. They could in principle form a 51% cartel to censor out the rest of the opposition. This wouldn't be very nice and good for the belief in the ecosystem, so I think they won't do it except in an emergency (Bitcoin picks up steam again and blocks are filled to the max, but some 25% are torpedoing for whatever reason).
That said, we have a long time for this to play out. XT doesn't need 50% of nodes anytime soon. Earliest fork date is Jan 2016. It would be nice if (basically) everyone gets onto the same page until then, though.
@_date: 2015-08-15 22:20:52
@_date: 2015-08-12 23:16:06


Yes, agreed. It wears you down around here. Do you have a short and neutral name for 'people who are keen on staying with 1MB-blocks'?
@_date: 2015-08-17 13:01:40
Ahh, someone who gets it! And with BIP101 + lightning networks if/when they actually work, there is a fair chance it won't ever come to that.
@_date: 2015-08-15 18:52:06
Not seeing any redlisting code piggybacked in there, though!
And I think the other stuff put in is rather harmless. I can live with or without it, I don't really care so much.
When he's going to put in redlists, he's going to endanger the hardfork because people will move away. So even if he hypothetically is interested in that (I don't think he is - if he ever was, he probably has a clue now), the nature of open source and forking would take any power he has through XT away from him.
@_date: 2015-08-11 10:51:36


Huh? That does not follow *at all...*


No. [Announcement of an XT download was censored and the responsible parties defended this action.](
@_date: 2015-08-16 20:02:00
No, but that one is.
@_date: 2015-08-15 23:30:50
Mmhm. Inventor of Bitcoin *crypto* system loses his wallet with some $250MIO in it.
I. Don't. Think. So.
@_date: 2015-08-16 21:11:41
The hard fork will show that Bitcoin is not going to be controlled by the Core central planners. And that will be a *good* thing.
@_date: 2015-08-16 23:27:42
Pretend devs. Interesting to call Gavin, the earliest dev who tried to make sure to actually give up centralized control of Bitcoin a pretend-dev...
@_date: 2015-08-16 15:59:53


This irks me as hell. Orphans are a problem for the 1MBers because big blocks as well as too few orphans because big blocks actually get through. Double binds and other psycho tricks. Disgusting.
@_date: 2015-08-28 12:05:58
Huh? It is about not crippling Bitcoin with a blocksize cap?
There are many things that Satoshi *did not mention* that would cripple Bitcoin equally - such as only letting transactions take place up to one hour before midnight UTC or similar.
@_date: 2015-08-11 17:11:28
[My other, earlier comment might be relevant... :D](
@_date: 2015-08-12 12:15:06
Prioritizing non-tor nodes which are less likely to be attacking nodes is a blacklist? Come on...
@_date: 2015-08-04 11:35:11
Miners will care in the sense that they *want* to have those transactions, as they are worth money to them. Eventually, full nodes will be paid by SPV clients and others for their services (such as archival nodes for lookups). Miners might actually pay these full nodes as well, to get their transactions.
Of course, full nodes that just forward and relay have no real purpose in here, but they don't really have a purpose in keeping the network safe, either. 
@_date: 2015-08-03 10:05:51
It is easy to soft fork back down should the need arise...
@_date: 2015-08-16 18:28:08
[This might be of interest to you, too.]( 
@_date: 2015-08-17 21:46:16


Why do you assume this? Do you want this happening?
@_date: 2015-08-16 10:37:54
Development on what is called the bitcoin-qt or bitcoin-core variant is deadlocked with regards to the blocksize issue. Gavin, after much planning, testing and deliberation convinced Mike Hearn to release his blocksize increase schedule (with a trigger that only activates when a supermajority of the Bitcoin miners get behind it) on his bitcoin-xt variant.
Most people assume the bitcoin-xt variant is closer to the spirit of Bitcoin and thus the way forward. Some of the moderators here on are disagreeing and censoring posts and disingenuously call the bitcoin-xt variant an Altcoin. Note that this variant is developed by the earliest devs on Bitcoin and follows Satoshi's original vision of a widely usable Bitcoin - with a high limit on possible transaction rate.
The fracturing of the developer base that is happening right now might eventually be healthy for the ecosystem, as it will remove any real or perceived single points of failure regarding development. It will create an incentive for any full node operator to pick his node software wisely.
@_date: 2015-08-04 11:14:36


Exactly. If you look very closely, they don't really have an incentive to cap the *blocksize*, but they have an incentive to work in lockstep with the full nodes.
That incentive limits the transaction rate (and in turn blocksize) *naturally* as well, though.
@_date: 2015-08-16 09:58:29
Maybe we should stick to coin_uncensored and coinXT? Otherwise, we risk dilution into an endless list of subreddits.
@_date: 2015-08-12 12:00:35
Are you fearing success - and thus full blocks?
@_date: 2015-08-12 13:42:55


IOW, VCs care about the value of Bitcoin. So what is wrong with growing that? Do you not want Bitcoin to succeed?


*Haste?* had the foresight that the 1MB limit will badly bite us back in 2010. 
@_date: 2015-08-15 23:15:49


Discussing the details of the purpose of up-/downvotes when the issue at hand is a lot more serious - outright deleting posts - seems... pointless?
@_date: 2015-08-18 21:29:23
Ok. Well, thank you for responding to this.
@_date: 2015-08-16 16:01:08
I am already in love with you. Ok, just kidding. But your posts are great :-)
@_date: 2015-08-03 10:34:08


Then lets do BIP101 as the best-researched of the bunch, have that compromise, and be done. 
@_date: 2015-08-01 16:03:40


If that is really your fear, you should rather wonder whether TPTB or banks or whoever is trying to starve Bitcoin to death by limiting possible transaction rate by hijacking the devs through blockstream or who else.
Something which was very clear from the beginning and part of any conceivable social contract: That maximum possible transaction rate needs to grow - a lot - for Bitcoin to be really successful. And which is also an expectation to happen and still priced in. Gladly, the earliest devs still on board (Gavin and Mike) happen to understand this well.
3txn/s Bitcoin is not long-term sustainable at even $5/BTC. Stop fooling yourself.
@_date: 2015-08-12 23:21:04
And that can be fine, depending on circumstances. But I think Satoshi chose right with MIT for Bitcoin.
@_date: 2015-08-23 20:01:51
My status quo is no blocksize limit, I run a client that simply rejected the illegal limiting of Bitcoin. I am original status quo of an unlimited Bitcoin.
Ok, I am not doing that because right now it would be pointless - though I did upgrade to BIP101 - but can you see who's more conservative?
Someone said (paraphrasing, if you can give me the quote that would be great) that *so-called* conservatives often fall into the habit of complaining about every change, until that change happens. After that, they'll complain about the next change, even if that change would have been in their interest before.
There is a large risk with staying at 1MB. By being afraid of all changes to the blocksize (which was known to be increased right from when the limit was introduced), you cannot make that risk go away, quite the contrary.
@_date: 2015-08-16 14:54:11
Bitcoin core has no authority for me, and it shouldn't for you...
I follow the longest chain (hashpower wise) of valid transactions.
@_date: 2015-08-11 17:20:49
Then please lets divide up into and ... and to make it just, close down :D
@_date: 2015-08-15 20:26:18
Do you think the mod behavior is acceptable? Or what should be gathered from your comment?
@_date: 2015-08-16 20:07:49


Good luck trying to use Bitcoin in North Korea. Bitcoin only works in at least half-free countries. That the U.S. is declining in all freedom metrics is clear to the rest of the world, and I pity you, lots of nice people over there.


Bitcoin somewhat pits freedom of speech vs. government issued money against each other. The former may lose in this fight. This is completely independent of any hard forks, though.
 &gt; Tread carefully with Bitcoin-XT. You are setting a dangerous precedent. 
Nice try. Again, hard forking Bitcoin is completely orthogonal to any regulatory issues. This is people deciding what software to run and what messages to accept.
So, again, FUD.
@_date: 2015-08-04 23:10:44
Thank you for this, by the way! Nicely written :-)
@_date: 2015-08-15 19:16:06
So then he can easily go and sign a message with one of his private Bitcoin keys. I think he knows how that stuff works :-)
@_date: 2015-08-16 19:53:32
I vote for -5kB. If you send data back to the miners, they get a coin. Or something.
Mine-by-DDOS. Instead of proof-of-work, proof-of-offensive-facilities.
POOF, and Bitcoin disappears.
Or something.
@_date: 2015-08-16 19:17:03
Cool, thank you very much!
To quote from the article:


This indeed puts quite the damper on the idea that Mike Hearn is somehow opposed to tor.
@_date: 2015-08-08 18:37:01
DMCA, 1996/-98. 
As for the effects of this, no comment... 
@_date: 2015-08-16 21:07:58
Market uncertainty that will be resolved with a big rally when the hard fork goes through. Quote me on that.
The weak hands lose to the stronger ones right now.
Contrary to what some 1MBers say, a successful hard fork will show that Bitcoin cannot be controlled, even by the highly intelligent core devs trying to do so. A very bullish thing!
And you can bet that after the hard fork is successful, XT will be only one implementation of many.
The way it should be.
@_date: 2015-08-16 19:49:25
I like that 'punishment'. They can keep it that way.
@_date: 2015-08-18 10:36:24
Closing the channel in the scenario that is alluded to by would be to let the channel *time out*, though! (Because that's the only way to get your money back when the other side is not cooperating)
So yes, there is the potential to maliciously tie up for a while.
But don't get me wrong, LN are a good idea. Just not the answer to scaling Bitcoin now. But BIP101 is.
@_date: 2015-08-15 20:54:11
Positions? The only power they have is trust. I trust Gavin. I half-trust Mike (I run XT but I'll watch him closely to not implement any red/white/green/black/whatever lists).
Those people are not a government authority. When they get the XT hardfork pulled off, it is because other people simply align with them as spokespersons.
And only with Mike and Gavin or at least a blocksize increase schedule I see any long-term in the economic fundamentals of Bitcoin.
@_date: 2015-08-15 19:00:46
Rather, egos are blocking progress in core. Thus they gave the chance for this ego to rise.
@_date: 2015-08-11 21:40:27
And that effect would undoubtedly be *good*.
I sense some urgently needed fresh air coming into the smoky back room that Bitcoin development became :-)
@_date: 2015-08-15 19:52:10
vistamail.com has no SPF record. Does lists.linuxfoundation.org even do any meaningful sender verification? Greylisting does not count here.
And then there is the issue that vistamail.com can possibly be bought. "I give you 100k blockstream money for sending this email". I assume it is Satoshi's writing only if he actually signs something with one of his keys. And I think he knows that he now has to supply that proof to be taken serious.
@_date: 2015-08-03 09:13:37


Emphasis mine. Currently, agreeing to BIP100 will have the effect of setting the 32MB limit in stone. "We all agreed on that, this is consensus &amp; purpose."
@_date: 2015-08-11 21:37:18
Yes, indeed!
It should be noted, though, that ironically a large part of the reason for this mess and self-appointed 'wizards' telling us what Bitcoin ought-to-be and how they are going to 'monetize' it (though they don't tell us that outright...) is that Gavin tried to remove himself from any (centralized) decision making. 
@_date: 2015-08-18 10:42:04
What says does not make sense. 
When comparing random dictionaries of users with popular ones, it will *always* make sense to have the a priori belief that the popular one is more likely to contain any other given random user.
That's just basic math.
@_date: 2015-08-16 10:59:47
I think you can, actually. It is just people aligning out of pragmatic considerations, not ideological. That motivation is not part of the question for the voting. It doesn't change the fact that those nodes support XT and bigblocks.
In any case, I think it is great for more options to exist - also to have a bigblocks variant that will advertise itself as Bitcoin core, for those that want that.
@_date: 2015-08-02 14:16:51


FTFY. I don't think it is going to be disastrous, because everyone except the 1MB-anarcho-but-central-block-steering camp will be on Gavin's quite sane BIP101 path.
@_date: 2015-08-13 08:36:34
is right. Look at this:
Regardless your stance on 1MB, this is unacceptable.
@_date: 2015-08-06 17:56:05
Good catch.
I've also seen these weird argumentation patterns multiple times from 1MB-blockistas...
@_date: 2015-08-19 23:14:13
Nice job! :)
@_date: 2015-08-15 21:03:22


Are you going to nitpick? Longest, cumulative hashpower wise. The shorthand 'longest' seems to be common here on reddit.
Now what is wrong about that?
@_date: 2015-08-15 23:35:42
Not at all. The core devs block any move on blocksize. I cannot _prove_ this in the logical sense, because it is very easy to hide behind a wall of inaction.
But if you have half a brain, you can see that this is EXACTLY what happened.
In any case: Whatever. XT is the way to go and they can keep bickering as long as they want.
@_date: 2015-08-17 07:40:42
If you do not *do* anything with those coins, they'll stay valid coins, regardless of whether XT or Core succeeds.
@_date: 2015-08-23 20:07:40
There's an aspect to mining which I think is a decentralization force on the ownership of mining hardware itself: It is a way to convert energy into Bitcoins. All other Bitcoins have to be actually bought on the market.
Meaning that in repressive regimes or for any other reasons, if you can pay for electricity (or gas for generators, or what ever), you still have a way to convert electricity into shiny tokens.
@_date: 2015-08-18 01:35:05
I still do not understand what the problem with a configurable limit is then?
@_date: 2015-08-19 14:15:30
Huh? There's a lot of early scammers in Bitcoin. Gavin is the earliest core dev still onboard. What exactly is surprising that he crossed path with a couple crazy or shady guys?
This also refers to the grandparent post.
@_date: 2015-08-19 14:30:24
Huh? This has been explained pretty well in many places.
Proof/Source: Stalling 1MB to force LN on top.
Use the device between your ears to figure that out.
@_date: 2015-08-16 19:58:48
They can try that. And people can ignore them. Nice try at FUDing.
@_date: 2015-08-15 20:50:02
Then why do you even comment on the issue? Look around. There are saved screen captures. The comments were pretty harmless and there is definitely no reason to delete them.
@_date: 2015-08-11 11:03:37
Well, if you want to be pedantic here: 
Then let him make and unusable circle-jerk but let him hand over bitcoin.org. Because *that* one is clearly backed by force of government. There are laws in place that involve the DNS system...
@_date: 2015-08-10 08:34:02


Which means that you are safest running a full node that just looks for wellformedness of transactions and otherwise accepts the longest (hashpower-wise) POW.
A full node *without any block size limit*.
@_date: 2015-08-16 18:01:21
Point taken, but it can serve as a guideline. At the moment the distinction is good enough to separate Bitcoin from all alts. I suggested it as a guideline in times when we need to change the hash function, for example.
@_date: 2015-08-19 10:21:11
Greg is certainly knowledgable, but far from infallible. But he's very successful in using fancy language to impress. This something I definitely do *not* like about him. Look at my posting history and interactions I had with him before you say I don't know what I am talking about.
He's not on any pedestal for me. I have seen him arguing. If you really poke him about specific points in arguments, you'll actually be underwhelmed.
@_date: 2015-08-27 20:51:44


That there is a cap on the coins was in the whitepaper. I doubt you'd see it as important that it is exactly 21mio coins now that we all agreed on.






Both these are extensions to the system that work well without taking anything away.
The blocksize limit does, though! You intend to completely change the whole economics of the system. That is more important than smart contracts or multisig. Very early questions by Satoshi on scalability of Bitcoin have been answered by him - affirmatively. Bitcoin can and *will* scale on layer-0.
@_date: 2015-08-22 14:08:12


Yes, until he hits limit, or a fixed blocksize cap.


The central planning comes in with the way of setting it? 
@_date: 2015-08-14 09:51:11
How so? MIT/BSD isn't viral?
@_date: 2015-08-16 14:39:02
I think and hope for the following scenario when the fog lifts:
XT is strong. But more importantly, there is no central Bitcoin cathedral anymore. Core is unmasked as really just a bunch of people. And they lost by trying to control the ecosystem. Rightly so.
@_date: 2015-08-19 09:34:17
Forking is natural in Bitcoin. It is a distributed system to arrive at consensus.
@_date: 2015-08-16 09:42:31
There is also an IRC channel on freenode now, named #  b i t c o i n x t
(Remove the spaces)
@_date: 2015-08-08 10:12:44
^ This.
If these attacks -which is just brute-forcing keys for people who made lame passphrases- seriously qualify as 'security research' and are hyped like it seems they are, the whole field has a problem.
Regarding pitfalls in entropy generation: Use good dice. Low tech, but it works. And no funky, 'backdoorable' HW-RNGs or similar are involved.
@_date: 2015-08-11 09:05:42
Naming the IRC channel  is a sign of hubris and arrogance in itself. 
@_date: 2015-08-04 16:36:45
Don't know why you're getting downvotes for an essentially correct observation.
@_date: 2015-08-15 23:39:17
The problem is that many people here are coders - and all that many coders see is: Just code.
That there might be a bigger ecosystem that can only grow with a larger blocksize - that is outside of some people's tunnel vision.
@_date: 2015-08-09 22:03:37
The problem then, is squatting the brands: bitcoin.org, github.com/bitcoin.
THIS we need to solve.
@_date: 2015-08-19 11:25:33
Not quite. There are multiple layers to an argument, and Greg is, for example, using technical language fluff *to impress*, instead of making a sound technical argument.
See [here]( for example.
He's suddenly saying poly(N) times poly(N) in an argument that was about O(n) vs. O(n ^2 ) and a couple simple multiplications of factors. That was just fluff added to the discussion (which you can also see when reading the context, the other posts from and I pointed it out in that thread as well).
Technobabble sounds impressive, and he knows that, and he uses it for political gain. I consider this very disingenuous. I am sorry, but that is the only reasonable conclusion from the arguments I had with him here.
@_date: 2015-08-12 23:15:20
I hope he doesn't mean only twice per year or similar.
Right now, everyone on the planet could interact with the blockchain. Every 30 years or so... :D
@_date: 2015-08-18 00:23:08
Fast than a programmed increase? I don't get that - a programmed increase can be (almost) arbitrarily fast?
@_date: 2015-08-15 22:22:35
Source: Months of interactions.
You can't prove a negative. You can't prove a natural law. You only collect data and make up a simple, falsifiable theory explaining the facts...
@_date: 2015-08-17 22:50:16
How are these attacks different from doing that with 1MB?
@_date: 2015-08-20 13:12:54
He's the dictator of his own little kingdom (his variant of the software), which is totally fine. We could have many such kingdoms.
But that apparently is lost on people who say everyone *needs to consense* on a particular set of 'core' devs.
@_date: 2015-08-16 20:00:41
It is only as draconian as users allow it to be. After XT successfully forks, I might well leave for a completely different client that implements the ruleset, such as btcd.
Aligning with XT is not forever. 
And it is not a surprise that Mike as the owner of this fork wants to set a different tone and style. Bitcoin Core deadlocked completely and failed.
I also have no reason to distrust Gavin. Quite the contrary. The only really sane, level-headed guy around.
@_date: 2015-08-04 16:55:27
OR he could simply show 'yes, I still own them' for whatever reason.
If he still *keeps* them, I don't think it is at all a sell signal.
@_date: 2015-08-15 21:06:32
He explained that as a hypothetical. I won't accept checkpointing for Mike's fork. Nothing like that is in his code. If he'll add it, I'll move away from XT. Your point being?
@_date: 2015-08-09 22:30:20
So, lets think that hypothetically, I run Bitcoin the way it is *intended*: I never accepted the 'illegal' 1MB blocksize limit patch when it came out.
My fork accepts 32MB blocks. I demand that the 1MB-block-limited Bitcoin is removed from as an Altcoin.
Explain why this is the wrong mindset here on please.
@_date: 2015-08-28 08:46:23
I don't understand the reason to be specific about 21mio, though...? has a fair point here...


With regards to variable difficulty, you are actually wrong:
"To compensate for increasing hardware speed and varying interest in running nodes over time,
the proof-of-work difficulty is determined by a moving average targeting an average number of
blocks per hour.  If they're generated too fast, the difficulty increases."
@_date: 2015-08-17 21:09:38


Only if you distrust the userbase to be able to come to a consensus IMO.


Maybe so. But remember that the only real reason for a blocksize limit is to restrict miners from centralizing the network because they want to raise it to the max. This can't be true both ways!
So isn't that all basically pointing to the existence of a dynamic situation and development of miners and full nodes communicating sane block size between each other?
I think also [explained the situation nicely over on BCT](
@_date: 2015-08-15 20:05:03
There is a reason balls exist! For hard forking!
@_date: 2015-08-11 11:25:56


Numerous 'XT has a download available' posts were deleted. Are you contesting this?


What exactly am I supposed to be lying about?
EDIT: Typo.
@_date: 2015-08-17 21:24:33
Then why don't we see that behavior right now? Why don't the miners go and patch their nodes for larger blocks and say 'eat it up'?
@_date: 2015-08-15 23:31:58
The real Satoshi believes his designed incentives work out - also in this hard fork situation.
And they will. In favor of XT.
@_date: 2015-08-17 21:54:11
That doesn't answer the question: What are the incentives due to the proposal for this to happen?
@_date: 2015-08-16 23:28:52
Well, I am glad he's not working on tor.
Don't get me wrong, I support XT very much because I see it as the way forward - but I am not married to it. If he screws up with some 'lists', people will fork again. He's closely watches and I am sure he won't try anytime soon, though.
@_date: 2015-08-12 13:53:53
Straw man! Where is the loud majority wanting to change the 21MIO cap?
Exactly. It does not exist because basically *everyone* involved knows that it would destroy Bitcoin's value.
But contrary to changing the 21M cap, changing the blocksize cap is actually in line with the original vision of Bitcoin, *and increases the value of Bitcoin*.
The way that you frame it, you are just doing social engineering here and pretend that 1MB was meant to be. This is disingenuous.
@_date: 2015-08-26 21:06:13
They should rather look at their bottom line. And that is still in fiat currency. Bitcoin price is IMO depressed by the 1MB BS.
@_date: 2015-08-15 20:43:11
Don't confuse unpopularity with censorship.
I can read all your downvoted comments. Voting at most hides comments and you have to click the small little [+] icon. As you know already.
*I cannot, however read what is in a [deleted] comment.*
@_date: 2015-08-18 10:31:00
[Stop trotting out this O(n^2 ) bullshit, please.](
Thank you.
@_date: 2015-08-16 23:44:18
That does not follow.
@_date: 2015-08-18 21:23:16
Perfect, thanks.
Do you have backups available for bitcointalk, too?
@_date: 2015-08-17 22:07:58


Why so? I can go and set my limit to 12.35MB right now and it won't cause any problems. Many people can do that, with their own limits, too.
@_date: 2015-08-23 17:49:25
How the heck are you going to get people to shutdown the fast relay network?
@_date: 2015-08-15 23:13:05


Exactly. [This sentiment has also been echoed on the mailing list.](
Bitcoin core is dead to me. I'll follow XT. That is true Bitcoin spirit. And if they screw up, I'll start my own fork or follow someone else, thank you very much. But I don't see that happening anytime soon. Mike has a fight to win and is closely watched. That watching part is actually good. 
What isn't good at all is abuse of brand names by the core people. I hope we can figure out a way to get this fixed.
@_date: 2015-08-10 11:37:46
Downvoting doesn't change the truth...
@_date: 2015-08-15 18:43:42
Right. Because he wanted to take himself out of the position of authority - the IMO noble approach.
But he only created the climate for a bunch of self-appointing 'wizards' to try to change Bitcoin off course for their liking.
Quite ironic, but in a sad way.
@_date: 2015-08-11 17:24:57
Look up Mike's idea of assurance contracts for hashpower. 
I don't think we need to screw around with the inflation schedule or other stuff.
@_date: 2015-08-16 10:32:54
Longest chain of *valid transactions*. Which is not quite the same as longest valid chain, depending on the definition of 'valid', as pointed out.
Let me further narrow it down for the future: Most effort spent (in joules) on computationally securing (through a hash function) a valid chain of transactions of tokens.
That latter definition should also include the cases where double-SHA256 is replaced with something else due to it being found insecure. And it is only tied to joules, not dollars, as the former is the currency of the universe.
@_date: 2015-08-12 12:04:16


Nope. It would destroy Bitcoin and all other cryptocurrencies. If *that* happens once - and to the leader and flagship of the cryptocurrency space - the game is over.
No one would believe it won't happen again. So then you have arbitrarily devaluing money with the bonus of a lot of nerd fights, additional instability and a large set of shady characters attached. Why the heck would anyone want to have anything to do with Bitcoin in that scenario?
@_date: 2015-08-15 23:42:05
Core is just a name, just like XT. Both are Bitcoin now. But 'core' elected to be stubborn, so soon only XT will be Bitcoin. 
We should get over it and not give the name 'core' any credibility that it does not deserve. Let core destroy itself. Some of them there seem to want that.
Bitcoin will live on. There is Bitcoin/XT now, there'll probably be just Bitcoin eventually again. With Gavin at the helm effectively, hopefully. With all of XT's features, including bigger blocksize, integrated.
 
@_date: 2015-08-18 00:03:00


find themselves on an abandoned separate network, and then search the Web to
find the "correct" value. To the extent that people find or choose different
max block size values, the network will split into several entirely
independent pieces, which is very bad for Bitcoin. 
Why do you expect this to happen? Most people have a common 1MB limit now. Why?


money, the more valuable and useful that money is. If you expect that the
economy can almost entirely agree on one particular value, then why not just
hard fork to this value to avoid user confusion? 
I do not know this value, and I assert no one else knows this *optimum
value* either. How do you propose we arrive at the knowledge of this optimum value?


consensus, the prerequisite for a legitimate hardfork is already met.
What is a *legitimate* hard fork?


for the software to measure its CPU/disk/network capacity and accept,
reject, or discourage blocks based on this, without giving the user an
explicit choice (except maybe as a command-line option that isn't supposed
to typically be used). So if the software determines that it's in a
situation where it can only comfortably handle 2 MB blocks, then it should
discourage (ie. delay relaying) blocks that approach this limit and reject
them after that limit. And if it appears that someone ends up on an
absolutely abandoned network/currency (based on invalid chain lengths), the
software should advise the user to switch to lightweight mode if he's on
hardware/Internet that he considers "weak", or continue protesting on this
separate network if his hardware should be supported but isn't.
I have put this thought into the section 'Further extensions' of our paper.
In discussions with on BCT, I suggested that a node might want
to enforce 1MB, for example, until a longest hashpower chain with valid
transactions arrives that is N deep, with N &gt; 1. Do you think I should
extend that section with examples?


more "turbulence" than agreeing on a single global limit. 
Isn't the turbulence part of the agreeing process? Like I said above: How
can we figure out this value?


and I think 'Erdogan' on BCT make a good case that Miners are
/going to dip their toes first. Why are miners not producing bigger-than-1MB
/blocks right now?


situation and development of miners and full nodes communicating sane block
size between each other?
This is referring to the toe-dipping and turbulent-agreement-process part.
@_date: 2015-08-15 22:05:12


How about growing up and seeing that 'handing over control' is just a delegation of my power as a participant in the Bitcoin ecosystem? If something goes wrong, it can be immediately revoked.
@_date: 2015-08-10 13:37:03
XT transactions are valid Bitcoin transactions as well...
@_date: 2015-08-11 09:17:54
Thank you. Indeed, a danger around here is that people assume in good faith that the arguments are still those of an honest engineering discussion.
That makes yourself more vulnerable to the psycho tricks and newspeak.
@_date: 2015-08-22 12:22:52
Because some *are* full? Here's your analogy:
You have a train that only carries 100 people. On average, 80 people take the train. But sometimes, your train is full and you lose revenue from the passenger who find no seat. You'd be in a better position having a train available that carries more people.
@_date: 2015-08-17 23:52:29
Why is that dependent on a configurable limit?
@_date: 2015-08-12 10:10:09
What else?
@_date: 2015-08-15 20:59:02
Very valid. 75% of hashpower supporting the original social contract of Bitcoin. But you can certainly go and play with your 1MB in your basement :-)
@_date: 2015-08-13 08:26:51
@_date: 2015-08-19 23:23:39
Have a look at paper [here](
@_date: 2015-08-15 22:28:04


Yup. I wonder whether there is a good way to somehow get the domain names and repos etc. out of the hands of now clearly dysfunctional core.
@_date: 2015-08-19 11:32:15


If you are that scared about Bitcoin being that easily broken, why are you here at all?
Bitcoin is a system *designed* to find decentralized consensus. Including cases of adversarial conditions, and including selecting among multiple hard forks.
@_date: 2015-08-19 23:18:26
Welcome to the real world. Ever browsed through tor and came across a captcha that you didn't see without tor? Mike's implementing similar things. Unfortunately, tor *is* abused.
I don't like that patch, either. A hard coded list of IPs is a suboptimal kludge IMO.
But I do not get how people get up in arms about this. 
@_date: 2015-08-10 08:53:24


And I think the essence of this fear of hardforks is nicely summed up in the [other very prominent post on Silencing serious hardfork debate is just for coziness and laziness.
Quite similar to some political programs/measures which eventually fail.
@_date: 2015-08-12 12:11:37


The 'governed by mathematics alone' was never true, it was always the whims of men. With the big and important caveat that all individual actors are tied together through common economic interest. THAT is the important part of Bitcoin. Coders and other people with tunnel vision are apparently unable to see that.
There are important and dangerus outside forces of course, which rather want Bitcoin to fail against other solutions.
The question is now whether you want to have it at the whims of men who want to cripple it to 1MB, seemingly to parasitically attach to the transaction revenue stream, or to have it rather governed by Satoshi's initial vision, which sees a working worldwide network with high capacity.
@_date: 2015-08-07 08:45:04
Blocks just need to be valid and have enough POW. When blocks arrive, they actually *clear* the mempool of the transactions that are also in the block. Transactions go through the network independent of the mined blocks that confirm them afterwards. This is also why the blocksize limit was completely ineffective at stopping the recent stress tests.
However, a large block with lots of new transactions (that are not in other node's mempools) has a very large orphan cost, asymptotically reaching (with its size) the value of 'block reward + transaction fees in typical mempool'. Mining a TB-sized block as a miner means basically just throwing ~10min of hash power (and thus money) away.
@_date: 2015-08-16 10:25:19
@_date: 2015-08-20 09:11:49
As I understand (but please correct me if I am wrong), the TOR list can be downloaded with the supplied python script, but the git contains a commit that includes a pre-made list with that script?
So that exposure you are talking about would only be the case when you redo this list download.
It does have some code review, if you look at the commits. 
I think the sane thing here would be to replace it with a more intelligent 'misbehaving node deprioritization scheme'.
That tor can be more easily used for DOS is grounded in truth:  Bitcoin is designed to avoid connecting to the same AS several times so that its connections are spread more widely across the net. Using tor allows an attacker to connect from many different AS to a node and makes it a lot easier to DOS its connection capability.
Again, I dislike that hard-coded list as well. But it does actually serve a purpose if you think more about it.
@_date: 2015-08-17 13:06:44
Economy of scale.
@_date: 2015-08-26 14:19:44
It is nothing about the blocks, it is the number of open connection slots on a node. And you have a command line switch to disable it.
@_date: 2015-08-26 14:20:26
It is much more important that you as an informed citizen are able to see all alternatives and are always able to switch to the right client. Ahem...
If XT actually implements something nefarious, switch away.
@_date: 2015-08-04 16:35:08


He still has orphan cost with the 51% majority, though. The Chinese are right now supposedly so bandwidth-limited that they only want to accept 8MB - yet they have &gt;51% of hashpower.
In any case, IBLT and similar measures (such as the existing fast relay network) for efficient block propagation make this a non-issue.
@_date: 2015-08-17 22:17:05
And what happens then?
@_date: 2015-08-17 21:36:03
Exactly. We're proposing to make MAX_BLOCK_SIZE configurable.
@_date: 2015-08-07 09:08:59


Why do you consider Bitcoin since the introduction of the 1MB limit not an altcoin, then?
@_date: 2015-08-13 09:20:19


But miner voting is the only thing that can be reliably measured.
@_date: 2015-08-15 21:00:07


I read this as a worry:


I assumed worry. Or do you not worry about undermining the Bitcoin system?


I think I do, actually, thank you. But if you think otherwise: Care you explain what I have missed?
@_date: 2015-08-15 19:05:14
Keep your private parts private and don't touch anything - you'll be fine :-)
Any coins that are received before the fork date can be used on both chains - should it ever come to that situation. Which I think is highly unlikely, as XT only ever activates at a 75% threshold.
So, yes, you are fine. But when you intend on actually spending your coins, you should make sure that you are using the correct software setup.
@_date: 2015-08-13 08:58:43
Thank you, spot on.
@_date: 2015-08-15 20:23:46
What is the difference between me doing this and Gavin/Mike doing it? How is that undermining the system?
@_date: 2015-08-15 19:14:34
Yup. He'd at least sign with a key. Anyone can make this up. This is BS. Quite the drastic action, playing Satoshi, from the 1MBers.
@_date: 2015-08-17 21:29:09


Why should they care, though?


So the competitive blocksize is limited without a hard cap, as in is right?


XT implements BIP101. I think it is pointless *right now* to discuss what might lie beyond that. They might be forked again if they do that. 
@_date: 2015-08-16 20:45:32
imaginary_usermeme writes:


... says newly created user that almost copies the handle of Go away, troll.
@_date: 2015-08-12 11:56:23
All that would be fine - if it would happen *without crippling the Bitcoin blocksize limit at 1MB!*
@_date: 2015-08-28 15:03:38
... to remove, yes.
@_date: 2015-08-16 14:10:18
That's not my point: Observe that this definition only fits Bitcoin, even right now. I simply see it as the future-proof way to describe 'longest chain, hashpower-wise'.
@_date: 2015-08-13 09:09:54
Telling that some people are simply afraid of Bitcoin's success.
The high transaction rate could saturate the internet!1!!
@_date: 2015-08-20 20:08:04
Seriously? :D
Come over to that thing not to be named here.
@_date: 2015-08-17 14:10:30
If anything, this should give a ton of credibility as he exactly foresaw this mess.
@_date: 2015-08-16 18:04:03
Nothing to explain so much - when one argues that orphan rate, i.e. miners losing a race to publish a block because it is too big for the network to quickly propagate, for example, will keep the fee market working, 1MBers object that orphan rate is a totally unproven concept and not going to limit transactions.
When one argues that that we need bigger blocks for making Bitcoin usable for everyone, people say that it will result in a higher orphan rate and that is bad because through some convoluted logic, it creates centralization.
Basically, Bitcoin is impossible to even exist according to some.
@_date: 2015-08-16 10:40:55


Hear hear. Anything to back that up? Or just lashing out because you lack  good arguments?
@_date: 2015-08-17 21:41:20


Why should that happen with this proposal?
@_date: 2015-08-23 18:07:32
So, how about letting the free market decide what works best? 
@_date: 2015-08-02 14:22:20
It should also be noted that Bitcoin is the only currency that actually *could* scale to become really big.
All other altcoins have a small userbase.
Why should Bitcoin be prevented from filling that spot, especially when a lot of other altcoins could easily provide settlement layers for LN an similar?
@_date: 2015-08-19 01:01:06
I especially like your part about the referendum. Spot on. Thank you.
@_date: 2015-08-13 17:20:03
[Unfortunately, arbitrary 'moderation' indeed happens around here.](
@_date: 2015-08-12 10:36:36


LOL, yes, nicely said. I wonder what can be done about the impasse and the now very icy divide here on and BCT?
It looks like XT is unfortunately the only way forward?
Without wanting to start a cult of personality around him - Gavin as the earliest core dev and with a clean record looks like he's absolutely essential to save Bitcoin from the 1MB-blockistas now.
Ironic, given that his good will to remove himself as much as possible from the decision making to a large extend caused the current situation in the first place.
And it is not good that a single person is again so important now.
@_date: 2015-08-18 21:25:37
And the public data cannot be extracted cleanly?
@_date: 2015-07-08 11:44:23
Ridiculous description of the situation.
And the blocksize cap [is not helping a single bit to make anything better.](
@_date: 2015-08-17 22:15:11
But in what way will our proposal open the gates?
@_date: 2015-08-03 10:27:41


So is this your motivation to write this:


I.e. "Keep Bitcoin small so Monero has a better chance"?
@_date: 2015-08-17 22:24:31
How am I going to stay on a different chain when nobody else would accept the 2MB block?
@_date: 2015-08-24 00:20:01
You don't/can't, as someone said? So don't marry a goat :D
@_date: 2015-08-16 18:15:00


I am curious: Do you have a link?
@_date: 2015-08-17 23:43:04
So go ahead and do it. What outcome of doing this do you expect?
@_date: 2015-08-19 10:17:29
Follow the money indeed: 
On one side we have the Bitcoin foundation, set up *to further the interests of Bitcoin* [x], paying the earliest core dev who is still onboard and who's easily the most reasonable and calm guy around in this debacle.
[x] - And, yes, I know that the Bitcoin foundation is far from spotless, has been infested by scammers (wasn't Karpeles onboard) and other shady people and so forth. But they had a) a recent and powerful reset in their internal structure and politics and b) it was always composed of a lot of people caring about *Bitcoin* and NOT some 3rd party systems.]
And on the other side we have Blockstream, a company that wants to sell solutions to an issue they are creating now, and *which has a clear conflict of interest with regards to caring for the well-being of Bitcoin*.
So in summary: From that setup it can be inferred that Gavin has a monetary interest in furthering Bitcoin. This is *not at all in conflict* though.
@_date: 2015-08-16 10:55:59
Same goes for the other side. So lets just wait and see what the miners do.
@_date: 2015-08-11 21:59:41


And third, driving people away from Bitcoin by making it unusable with 3txn/s is not healthy to the ecosystem. End of story.
@_date: 2015-08-17 22:09:45
So the miners don't have all the power?
@_date: 2015-08-16 20:23:04
And that only because he elevates the 32MB to an ought-to-be due to pressure from/bowing to the BS guys.
And I bet the 32MB limit would then be dangerously entrenched and impossible to change. No way around their special Lightning Hubs then.
@_date: 2015-08-23 20:17:52
^ this. They'd need to make using the Bitcoin&gt;21 coin *mandatory*, for example, and that worldwide. 
If a  single country does this, it would then just be in the position that  people would be converting to regular Bitcoin.
ESPECIALLY if a government would implement a system similar to Bitcoin and with similar usability. People would know Govcoin, so they would know Bitcoin, so they would know how to quickly convert their fiat into real money. 
This all relies on *Bitcoin being very useful and widely distributed.*
And that certainly means no stupid 1MB cap and also no core gridlock, so that features from other coins can be merged into Bitcoin, *to keep Bitcoin competitive*.
This 'Don't touch it!!1! It will break!1!' attitude from some 1MBers is really, really damaging. It's like flying a 747, seeing a mountain in front, and then just freezing in fear and being unable to move the controls - unable to avoid the mountain.
@_date: 2015-07-01 23:42:09
Quoting [user rocks on BCT]( as he said it so nicely:






@_date: 2015-07-29 16:42:39


If you really think the system is *that* unstable, why are you interested at all in it?
@_date: 2015-08-15 20:44:11
Exactly. It is fear driving them. Fear that their parasitical profit schemes are not going to happen.
@_date: 2015-08-15 22:26:21
Of all, of course.
@_date: 2015-08-16 19:48:02
To add to this: When they build off the Bitcoin ledger, it is impossible to use it as the typical pump'n'dump scamcoim scheme... 
@_date: 2015-08-16 14:17:53
A voice of reason. Thank you. I want to throw a possible definition in for longest-proof of work that would also cover more edge cases:
Most effort spent (in joules) on computationally securing (through a hash function) a valid chain of transactions of tokens.
This definition should also include the cases where double-SHA256 is replaced with something else due to it being found insecure. And it is only tied to joules, not dollars, as the former is the currency of the universe.
This definition is, AFAIK, completely compatible to the SHA256 one for Bitcoin so far. 
@_date: 2015-08-17 13:04:52
And maybe it is even 30-50 players instead. The world is big. Look at how people are wiling to cripple Bitcoin now, and even prevent it from ever becoming really successful for the ill-defined goal of decentralization.
Those people will still be a factor in an endgame scenario. They might band together, per country, and run full nodes 'to keep stuff in check'.
We just need to make sure that they don't destroy Bitcoin this early.
BIP101 is therefore a good compromise.
@_date: 2015-08-19 13:41:34
Erm, no. The parasitical attachment is forcing LN on top. BIP101 and room for growth onchain + LN is fine.
@_date: 2015-08-07 08:54:50


I tried to remember when I actually have seen that behavior, and I do not remember any case. 
I have seen many cases of rubegoldbergism, on the other hand.
There's also incentives for people to invent complexity: It always appear to add value, bells &amp; whistles, and it sometimes makes developers indispensable because they're needed to service their complicated contraptions.
For simplification, the only incentive I can see is care of the product itself.
Go figure.


Very true. Letting Bitcoin run into the 1MB hard limit is a change in the dynamics of the system that should be clearly avoided.
@_date: 2015-08-17 21:58:48


As I said, they can just go and open the gate themselves by patching Bitcoin core accordingly. So what is the difference?
@_date: 2015-08-01 09:09:04
If you actually read TFA, it is pretty clear that this is not a 'thinly veiled threat' but rather a description of what is going to happen if the status quo persists.
A peek into the future if you will. Pretty accurate at that.
Now, please, 1MB-blockistas, dig up something better if you can.
@_date: 2015-07-11 20:24:48


Well, but then this is at most a very weak argument for a blockcap anyways. As it was demonstrated - and when one thinks it through, also to be expected, blockcap wasn't effective in keeping the spam down. 
With 8MB, the only change is disk space occupied. Should some banks indeed conspire to spam the network long term to drive fees up, it would probably make most sense for full nodes and miners to learn some advanced spam protection schemes - maybe the miners could even inform the full nodes what stuff they'd ever think about mining and the full nodes can adjust their relay policies accordingly.
Thinking more about this, this is also probably something that ought to be done at some point: Have a protocol that let miners inform full nodes about their mining policies in an automated way, and probably also have full nodes inform the miners about relay policies.
As pointed out, the place of full nodes in the market is, apart from blockchain storage and safekeeping, providing relay of transactions to the miners.
@_date: 2015-07-24 08:36:37
@_date: 2015-07-02 09:07:24
The chinese miners are &gt;51%. If they do not like anything above 8MB now, they can just decide to not let it happen.
Remember that the miner majority can easily always adjust the blocksize down however much they want to.
Makes arguments against a blocksize increase all the more ridiculous.
@_date: 2015-08-17 22:13:47
You are missing my question: What are the incentives due to mine and proposal?
@_date: 2015-08-13 18:01:27
No. Just a mandatory free form field, 'edit box' for the GUI and a mandatory command line argument '-maxblocksize'.
These fields would default to 1MB until January '16 (or some other reasonable date, to have some time for most users to get used to this change), and after that date it becomes mandatory to pick the value as a user, without *any* default given.
And, yes, I'd also say (in a possible BIP) that the current 32MB network message limit is temporary and to be lifted as soon as someone wrote the necessary changes to the network code. As a statement of intent. To avoid a possible future 'debate' around similar BS.
@_date: 2015-08-12 13:39:21
And what does that question have to do with the metric that matters?
@_date: 2015-08-15 21:57:00
Uhhh.. for the bigger blocks patch, maybe, just maybe?
@_date: 2015-08-15 17:29:33
Great! Time to get compiling... :-)
@_date: 2015-08-19 23:08:39
Did Gavin receive funds and have significant say and protect MtGox during the time when it was found out that MtGox did all the bullshit? I don't think so. So what you wrote is a pretty worthless set of accusations.
@_date: 2015-08-12 10:20:11
And everything not fitting into 1MB is a micropayment, obviously. /s
@_date: 2015-08-09 22:02:07
They wait for a 75% majority...
@_date: 2015-08-17 13:03:18
Those fighting the battle might destroy Bitcoin by fighting it, though...
And I think you're right: That amount of centralization isn't necessarily dangerous, though. Especially not with a BIP101 constraint on blocksize. That way, running a Bitcoin node will always stay within reach of the dedicated hobbyist...
@_date: 2015-08-15 20:48:47
And anyone can do that. So why are you worried? The longest valid chain will win.
@_date: 2015-08-16 23:31:19
Cool! I am a science guy with software engineering experience due to necessity, but am very interested in starting something in Bitcoin. Yes, lets keep in touch!
@_date: 2015-08-16 09:25:30
There is no code that is doing that in XT. I'll not accept such a piece of code. Mike Hearn clarified he was talking about hypotheticals. If he puts that in, I am not going to accept it.
@_date: 2015-08-16 18:25:35
I see. Thank you. Would be even better when that is documented somewhere, though. 
@_date: 2015-08-16 16:02:34
Back it up, multiple times. Keep it on a computer that is offline most of the time - or better yet, investigate proper cold storage.
Your wallet and keys are fine just existing. When you want to spend your coins, you might want to use the wallet with bitcoin-xt. The wallets for bitcoin-core and bitcoin-xt are compatible. Note though, that in the unlikely case of a prolonged chain split, you might want to separate spending on the two chains.
When the fork happens and most are on board, using an old QT wallet will cause your transactions to not go through or be spent only on the 1MB chain. This might be problematic. If there is a fork battle, it is best to refrain from using your coins until the dust has settled. 
@_date: 2015-08-12 10:08:00
I'd be curious about this, too.
@_date: 2015-08-16 20:41:14
I guess it is all a set of trade offs. I am not particularly worried about centralization, but I think BIP101 has a nice schedule with a good chance of always keeping a full node within reach of a dedicated hobbyist or at least a smaller, dedicated company. 
I also think that a widespread level-0 and the possibility to directly settle on-chain for all people will mean that there is no reason that LN hubs will ever grow very big and centralized.
So centralization on layer-0 vs. centralization on higher layers are somewhat in a trade-off situation, maybe?
And, again, I do think BIP101 will be close to the sweet spot there.
@_date: 2015-08-11 09:38:06
No, but miners (and the worth of their coins) are closely linked to the rest of the ecosystem. But I otherwise agree to an extend, it isn't trivial to get everyone on board and there will probably be always some very few unlucky ones.
EDIT: And I like to add that it is many of the 1MB-blockistas that think you can do a hardfork easily last minute, as a weird argument against planning a blocksize increase. Gavin has been carefully planning and proposing an increase since years, and rightfully so!
@_date: 2015-08-04 10:29:58
If you think further about it:
*The fear that full nodes cannot keep up with larger blocks (centralization!!) is just a (nefarious) re-branding of the well established concept of orphan cost, a concept usually accepted by all parties in the blocksize debate. And something that works naturally against high block sizes!*
With IBLT, the balance between full nodes and miners will get even better, because Miners cannot flood the network with a big block - all they can do is confirm the data that the network and most of the full nodes (again, orphan cost!) have seen in the last ~10min.
@_date: 2015-07-24 09:11:49


What is so hard about telling people: 0-conf has a certain risk profile of you getting scammed, be aware of this but, yes, it can be used in many situations?
I also haven't seen it advertised as 'just fine' so much.
@_date: 2015-08-07 22:03:50


Except ones following the original idea of no blocksize limit...
@_date: 2015-08-13 18:13:24
There will always be the hashpower-longest-one. To be sure to be on that one, select a maxblocksize high enough so that your full node doesn't get forked off the main network... :-)
@_date: 2015-07-04 00:06:55
Listen: You don't have to worry about users and merchants trusting most nodes and miners to adhere to current 0conf behavior.
Put a flag that says RBFable into all of your transactions and replace-by-fee however much you like.
Meanwhile, merchants and users who like to use 0conf can do so.
What the heck is your problem?
@_date: 2015-07-08 00:46:00
Ok, [done.](
@_date: 2015-07-06 20:07:46


Ok, got that, I misunderstood that, my fault. Too much of this behavior recently, and I unfairly assumed the same here...


He did tests whether the validation works with 20MB. And it does. What do you expect to misbehave at 20MB?
I am not saying further tests aren't nice to have, but blocksize increased from ~0MB to ~1MB now without any issues, why do you put a burden on Gavin for testing what is just allowing further growth?
We know that there is a 32MB limit but we have time to work on that...


I have a completely different impression, which is neverending stalling by some of the devs because they made themselves believe that they need to put lots and lots of complex, vaporware layers atop Bitcoin first (and how nice that they would profit from those, too).
 
But somewhat back to the technicals: Yes, I like BIP100, too. One thing I do not like is how 32MB got sneaked in and is worded like an ought-to-be in there - instead of just being described a technical limit to be eventually remove. That actually makes me very uncomfortable, but I digress. Assuming this is not meant as an ought-to-be, I am sure the language could be changed in BIP100. The way it ended up in there only in the 2nd or third draft and the way it is worded makes me suspicious, though...
And then I noticed something which I do not know whether it is going to be a problem with BIP100, or whether it is good, just a thought: 
The 75%(80%?)  barrier might actually create an additional incentive for miners to band together into a cartel. Because in case a 25% miner is blocking an increase (or decrease), they could be tempted to band together even more to throw that one out...
@_date: 2015-07-07 14:49:54
No... I think the stealing hashes and mining immediately is not that bad,  there will be eventual consensus. Ok, it might screw with the current SPV model, but that can be fixed.
What I was talking about is the endless SPV mining. Create a block on top of an invalid one, have all the time to figure this out, but instead of resetting the chain, building further on that invalid one.
And I think this behavior will change because it costs them money.
@_date: 2015-07-30 09:01:07
Sigh. Sure it does, but seemingly not in your tunnel vision view. We had all those discussions already...
@_date: 2015-07-30 14:05:56
Yes, probably. I rather want to see a symbiotic relationship with Bitcoin and SC/LN instead of a parasitic one, though...
@_date: 2015-07-10 12:54:12


This has been addressed by Satoshi and it has been pointed out that Bitcoin can scale. There is no new fundamental data that it can't. We could run (with very big full nodes, of course), a ~1e10 txn/day Bitcoin today.
And regarding inefficiency: It is a globally replicated database. VISA and Mastercard must both have similar schemes in place to 
ensure that their transactions are stored safely.
But Bitcoin has the advantage of a) not having cruft accumulated in the system from regulatory overhead etc. and b) it is an open protocol, so different parties can connect to each other on the database-synchronization level, in contrast to VISA/MC.
Again, if you do not think it can solve that kind of problem - why not let Bitcoin at least have a try at this but use an Altcoin with restricted blocksize instead?
Finally, a case can be made for complexity: There seems to be a strong urge for a lot of CS people to put layers upon layers of complexity on top of each other, creating Rube Goldberg contraptions in the end. Again, Bitcoin can technologically scale to gigatransactions/day. Why do you want to prevent that?
@_date: 2015-07-21 21:44:42


And a rapid rise in blocksize is arguable tied to a rapid rise in the user base, which is a key aspect of Bitcoin's ***de***centralization.
@_date: 2015-08-15 18:39:12
The hashpower-wise longest with valid transactions is the right one. Regardless of whether it is below or above 1MB blocks.
XT will always keep you on that branch.
@_date: 2015-08-04 16:48:08
There was a statistical oddity to the nonce generation for blocks that are likely to have been mined by Satoshi. I think there was an analysis on BCT somewhere?
Does anyone have a pointer? Are these coins part of those mine with the odd nonce generation?
[Ok, I actually found the thread on BCT.](
@_date: 2015-07-01 11:49:45
Probably. You are asking about existence of something not too uncommon in the  Bitcoin space in a relative large set.
Who? I don't know.
But I know that some people should openly admit their conflict of interest...
@_date: 2015-08-19 14:11:56
Can you tell user or further constrain from whom do you received this email? If you do not want to publish the exact person yet: Is this blockstream? Or someone else?
Oh, and otherwise thank you for your information!
@_date: 2015-08-17 21:43:29


How does it create incentives for miners to create bigger blocks?
@_date: 2015-07-05 09:56:51
Yep... I think a change in perspective is also in order. Look at the whole thing from the perspective of a miner and unlimited blocks IMO get a lot less scary:
A miner sees a lot of transactions popping up and arriving from everywhere. He then selects the transactions that make most sense to include in a block from a profit perspective and pushes the created blocks out back to the     
network, hopefully 'infecting' enough nodes with his chain so that his blocks won't be orphaned.
It is a two-way relationship! If a miner pushes bloatblocks and thus stresses the network, apart from the direct issue of orphan cost (which also push the miner towards smaller blocks and is what this submission is about), a
lot of merchants with a lot of transactions (== money for the miners) will fall of the network, and disrupt not only the price of BTC, but also the free flow of transactions towards the miner.
Now, some argue that the game theory is that a single, powerful miner can exploit a tragedy of the commons here and produce bloatblocks to keep the transaction fees for himself but take from everyone's network bandwidth,    
leading to eventually crippling the network.
I think this is tunnel vision in action. Apart from the issue that a big miner also owns a lot more of the 'commons' himself than just a fraction of network bandwidth, in the sense that he can relies on the whole network and ecosystem to sell his generated or fee-based Bitcoins and he also owns a lot of expensive special-purpose hashing hardware that is completely worthless outside the context of Bitcoin. 
Even more importantly, Bitcoin has always been relying on the majority of the hashing power to be honest. Trying to work around it seems to often make Bitcoin worse than just accepting it.
And the majority of hashing power can always prevent bigger blocks.
What would make sense is a *safety limit* on blocksize, e.g. 10x average. It would be in the interest of the miners, too, as it would prevent a hacked miner from going rogue and temporarily disrupting the network.
When the original 1MB limit was introduced, transaction rate was *way below* 1MB blocks, so it was just exactly that - a limit preventing DOS and forceful disruption of the network. 
It didn't prevent the network growing by a factor of 10 in transaction rate per year.
Yet the 1MB limit does now. It was never intended to do that, and attempts to rewrite history are hopefully not accepted by the majority in the Bitcoin ecosystem.
@_date: 2015-07-10 09:15:34
- Fees will be collected by the payment hub and not the blockchain, enabling scenarios in which the blockchain less secure
- LN means locking in your money for quite a while (original paper suggested half a year), and I haven't seen an answer from you how you this can be avoided. That is not just a caching layer, that is a large change in usability.
- You expect 1000s of payment hubs, but are scared about ending up with just a handful of highly regulated full nodes. Why are you expecting 1000s of payment hubs and not just 5 centralized quasi-paypals?
- Most importantly: You are clearly steering Bitcoin off-course. Bitcoin was always meant to be a widely available layer-0 and I have not seen any new data yet from you guys that this is actually impossible. You are just able to refer to a diffuse centralization scare, which can well be countered with a diffuse centralization scare about payment hubs. And this is another beef that many of us have with you guys, that you do not say: "I *want* Bitcoin to be different" but you instead say "Bitcoin *must* be different, because [*diffuse reasons*]".
Be forthright about it.
Also, Bitcoin is the only blockchain that could possibly be a large, scaling layer-0. Limiting it means foregoing the possibility that it could ever become ever this.
But you can *easily* do Lighting Networks on top of an Altcoin, too. What makes you want to prevent Bitcoin from being able to fill that unique position?
@_date: 2015-07-19 14:54:46


So it is about hash power centralization? The question is how much of that can (and should) be steered through the blocksize.
And a mined block needs to originate from some node in the network - so isn't this pretty much addressed by Gavin's proposal?


Did we? I can only think of estimates that are colored by a lot of opinion and can go either way... 
@_date: 2015-07-21 13:06:15
Voting with hashpower is the only reliable way to vote in Bitcoinland.
The other participants (merchants, users) vote with their wallets already - should Bitcoin become worthless, the miners have a *large* problem at their hands.
The core incentive structure of Bitcoin assumes that the economic interest of all participants in the ecosystem will reflect in the decisions of the miners.
So it is completely fair to give miners the power to vote on blocksize.
@_date: 2015-07-09 09:06:46
So this is now suddenly an *emergency* that demands 24x7 attention?
But at the same time most of the blocklimiters and opposing core devs say 'oh well, we can wait until the last minute with a hard fork'?
Give me a break.
@_date: 2015-08-16 20:43:38
I think LN will have some centralization due to economy of scale. With a widely available layer-0 (such as XT/BIP101), there is at least the chance to circumvent LN, though. And it probably would also lower the barrier to entry for running an LN node. Because I think there is a large incentive (due to the way LN settlements work) to have as many users as possible under a single 'LN domain'.
@_date: 2015-08-09 22:01:37
So, lets think that hypothetically, I run this 'fork' which is *original* and for which I never accepted the 'illegal' 1MB blocksize limit patch when it came out.
My fork accepts 32MB blocks. I demand that the 1MB block limited Bitcoin is removed from as an Altcoin.
Do you see the ridiculousness of your argument?
@_date: 2015-08-12 14:37:12


An early adopter, pointed out in 2010 that the 1MB blocksize limit is not a good idea and will bite us eventually. I was lurking back then as well, and I have it on the radar as an issue since I know about it. Most old timers on BCT are pro bigger blocksize, as evident in the multiple polls conducted. Gavin is the earliest dev still on board with Bitcoin and, as you know, also pro bigger blocksize. Arguably, these are early cypherpunk idealists...
And you know what? Monetary value beats idealism in the Bitcoin space, and that is *good*.
 &gt; If the majority can ride rough-shod over the wishes of the minority today, a future majority can do so over what is currently a majority. Be careful what you wish for, because you may get it.
FUD. The decision on 1MB is orthogonal to the 21MIO limit. *Obviously*, if the majority of people want to double the 21MIO coin limit today, they can *theoretically* do that. Because, as I said in the [other thread]( Bitcoin is a system between people and not 'governed by impartial math'. *It has always been that way*. They don't do that, though, because social contract and economic forces work against that. In the same sense, you could freak out because the BDB issue has been solved...
If you truly believe in the 'governed by impartial math' idea, it is a delusion that you should urgently get rid of. 
In contrast to lifting the 21MIO coin limit, lifting the 1MB *blocksize limit* is in the economic interest and part of Bitcoin's social contract. People with tunnel vision and clinging to the code also seem to miss this, a seemingly common condition with small blockistas.
@_date: 2015-08-19 14:34:33


LOL indeed.
@_date: 2015-08-16 10:53:29
Nothing happens before January 11th 2016. That is the earliest possible fork date. So that things can stabilize before it happens and that everyone knows what is going on.
But the current code has that date hard-coded in. So if you run it now, it will activate in 2016.
@_date: 2015-07-09 17:41:11


Oh, I think that idea of calling Bitcoin itself layer0 is older? Or what do you mean by conceptual framework?
@_date: 2015-07-21 19:57:09
As in those Chinese having &gt;50% of hash power supposedly but only wanting 8MB?
And it is for you to decide how fat a pipe needs to be for mining?
@_date: 2015-07-10 09:20:03
I am not so sure - this appears to be a real schism with vastly different opinions on what Bitcoin ought to be.
Adam really seems to want many layers on top of each other, for example.
@_date: 2015-07-22 14:17:10


All the writings from Satoshi point into the opposite direction - that he saw Bitcoin with very big blocks - and, yes, full nodes in data centers eventually.
So 'maybe it doesn't correlate' is a pretty worthless statement.
@_date: 2015-07-01 11:17:38
BCT doesn't look very different and there are a lot of early adopters there.
Really interesting would be proof of stake signed opinions on this matter. I think the picture would be even clearer pro blocksize increase.
Despite overwhelming data, you just assume the opposite for no reason. 
@_date: 2015-07-17 09:18:37
^ THIS. A part of me wants to see the ecosystem fracturing at least in the sense that there is not 'bitcoin-core' anymore, but a large variety of clients.
@_date: 2015-08-16 10:09:27
If he does anything like that, I'll abandon XT. Until then, XT with an imperfect lead dev. is the answer to the much more broken core for me.
I agree that it would probably be best for Gavin to have his own fork under his control. Maybe he'll do that eventually.
@_date: 2015-08-15 20:17:10
If you have hashpower (mining hardware) point it to a pool that will produce XT-versioned blocks. If you are paying/funding others running full nodes, make sure they run XT. If you are using full nodes as a service provider, ask them to be on XT.
Spread the word, essentially. This is especially important now that the mods actively censor XT related stuff as 'altcoin' here on reddit. 
@_date: 2015-07-22 19:26:16
Well, at least they could make their conflict of interest clear at all times.
As far 'interested party': The best interested party is IMO a dev holding a lot of coins...
@_date: 2015-07-08 09:32:16
SPV mining continuously without a full node for checking and resetting goes directly against incentives. The Chinese miners will figure this out eventually. They already lost money due to the orphans.
Have a look at what [Tier Nolan wrote.](
@_date: 2015-07-03 10:40:33
Why break a working system?
0conf is being successfully used. If you want RBF, just introduce a flag that makes transactions RBFable.
Do not force it onto users that do not want it.
@_date: 2015-08-02 14:31:41
There is an insane belief going around that Bitcoin is digital gold and will stay valuable with just 3txn/s.
No, it won't. The expectation that the network will scale - probably to levels of bigger full nodes in data centers - is priced in.
Digital gold with just a ridiculous 3txn/s will make Bitcoin a collectors item for some nerds. Maybe above $1/BTC, but certainly below the current prices of $280/BTC ...
@_date: 2015-08-10 08:49:59


***Miner votes are the only thing that can be reliably measured!***
@_date: 2015-08-17 08:43:37
Eight *self declared* experts.
And, yes, I consider them highly knowledgeable Bitcoin people, maybe even experts, but some of them with an agenda.
And there are lots highly knowledgeable Bitcoin people, experts, in the wider community.
*But most of those don't put themselves onto a pedestal.*
@_date: 2015-07-22 08:31:10
And the only reasonable way to have a working fee market in the end game without a block reward is to have a huge user base paying tiny fees.
Centrally planning the blocksize to force artificial scarcity isn't going to do it. One only enables Altcoins to take over.
@_date: 2015-07-08 03:26:08


[Against attacks, only by at most a factor of two.](  
It does NOT keep the Bitcoin network safe from attacks. And so it is basically worthless.
We have been basically running with the training wheels off - and there is no real problem. Except the blocksize cap itself.
@_date: 2015-07-31 18:37:40
There was a post on the ML about having, as far as I understand, a mandatory variable (without any default) that needs to be set for a full node to even start. See [here.]( 
Is this about what you are proposing?
@_date: 2015-07-24 10:16:19
But the pools they point their hashpower to do.
And if they don't they'll lose more money to orphans than what it would cost them to run a full node in parallel.
f2pool recently learned that lesson.
@_date: 2015-07-24 17:40:31
Should be possible to run XT on it, yes. *Easy* probably happens only as soon as the XT with the hard fork code in it exists, because someone compiles Bitcoin/XT on ARM for that thingy.
@_date: 2015-08-19 11:35:11
It is parasitical attachment. Of course the host (Bitcoin) should survive the attachment of the parasite (payment hubs), because that is in interest of the parasite who'd otherwise die with the host as well.
But that doesn't mean that the host, Bitcoin, won't become a sick patient, then.
@_date: 2015-08-01 09:14:39
I think that is something that the market can solve - miners can simply require higher fees for transactions that require a more expensive lookup in their databases.
@_date: 2015-08-15 18:54:28
Unless you use your own full node, you don't need to do anything. SPV wallets are going to be fine if wallet provider/implementer has enough of a clue. With a full node, it is better to switch to XT in any case. If you do not like your node voting for bigger blocks, you could go and patch core to support bigger blocks - because you want to be on the larger, valid chain when the fork happens.
@_date: 2015-08-23 16:35:22
So orphan cost high == bad and orphan cost low == bad? (Fee market, IBLTs)
Which one is it?
@_date: 2015-08-16 09:51:09
I guess if you do not want to voice support, one possibility is to patch core yourself and just replace the limit with a sufficiently high figure?
@_date: 2015-07-07 19:14:30


A hard fork might happen in a cleaner way.
However, assuming you have a point, Gavin has a point *even more so* insofar as he is urgent on this issue. 
Because we won't continue for long with 1MB blocks either way. Either because we can actually upgrade - or because Bitcoin will fail. And upgrading takes time...
And see also what wrote.
@_date: 2015-07-05 10:11:58
If you look further down, Peter__R clarifies that he indeed wrote the *original* post.
is not the *O*P but forwarded it.
@_date: 2015-07-06 23:35:01


Sure it does. It means miners naturally limit the size of the blocks they create and a centrally planned, committee-chosen hard cap can only harm.


Massive turds that are costly to create and have a very high chance of being orphaned - ESPECIALLY when the network is not up to speed.
Incentives work out in Bitcoin.
@_date: 2015-07-19 13:48:25


And at that point you are lost anyways - best thing is to trust your intuition and maybe flag users accordingly. I recently learned about the Reddit Enhancement Suite, by ...
@_date: 2015-07-06 23:24:16
The idea to keep it in place might gain traction, but I am not so sure, could also be only all those trolls being very active today.
And it is only effective now - it hasn't been effective for the most part of Bitcoin's history.
And if it stays in effect, it will cripple and kill Bitcoin...
@_date: 2015-07-24 16:55:03
    No we're comparing a whole money/currency system. Include a part of what the banks and Fed/ECB does in your cost, at least.


Quote, you:


Bitcoin is more than that. Thus you cannot compare it that simply. 


I should dig up what you baselessly accuse me off? 
At least try get your damn accusations right: We were discussing full nodes with full history vs. nodes working from UTXO commitments. And my points still stand. But those are completely different beasts. But apparently *I* am misinformed. Sigh.
@_date: 2015-07-07 14:46:42


Bitcoin was never meant to be able to run on your RasPi forever...
@_date: 2015-08-15 23:29:09
Or another user on vistomail to have the correct IP showing up in the mail header, but still fake the user name? Or vistomail being bought to do this?
Many possibilities. But without a signature, this is just spam. And Satoshi certainly wouldn't say 'pretend-Bitcoin'. He'd have worded it a lot better. 
If he thinks what he says is important, he could easily prove authorship by signing it.
IMO, it shows how low the 1MBers sink in their desperation.
@_date: 2015-08-15 19:18:52
They got the two spaces after the punctuation right, though. Sophisticated! /s
@_date: 2015-08-13 17:05:15


Hmm, but with the blocksize limit for example, wouldn't following the pre-fork with more work be exactly that: self-recovery?
@_date: 2015-08-12 10:28:45
Let me quote [thezerg from BCT](








To add to that: I like the idea of LN. But first of all, *Bitcoin* needs to be able to scale. Meaning bigger blocks (and given the deadlock, meaning XT now). LN are a *nice addition* on top of Bitcoin and might add another two orders of magnitude in transaction capability *eventually*.
But in no way should Bitcoin be crippled to *force* LN on top. And this is what people rightfully get angry about.
@_date: 2015-08-03 10:24:17
brg444 writes:


The unique immutable ledger in existence. 
False: Litecoin, Peercoin, Darkcoin, Monero, Dogecoin, ...
@_date: 2015-08-23 20:11:16


Completely OT, and I am not an anarchist or other 'no government at all lover' myself, but: How about privatizing marriage completely so it purely becomes a simple contract between two people?
@_date: 2015-08-10 13:43:48
With &gt;=75% of hashing power, they are very much so.
@_date: 2015-07-25 20:03:24
The model before was SC. Arguably another layer on top to extract wealth.
The federated SC model that they have now is very close to something that you can monetize. For Blockstream.
And it certainly wouldn't help their profit if the underlying layer gets larger and takes all fees away.
You might not like the facts, but this conflict of interest *clearly* exists.
@_date: 2015-08-13 09:01:58
You are downvoted for stating the core principle of Bitcoin. Funny and sad at the same time.
@_date: 2015-08-22 12:17:36
Not at all. It just means that 1MB is *crippling* the free fee market right now and another good reason to remove it.
@_date: 2015-08-10 08:57:48


Emphasis mine. Care to actually explain that word, *consensus*?
@_date: 2015-08-15 20:39:54
I wonder who from the 'core devs' could behave like an upset teen. I have this suspicion... :-)
@_date: 2015-08-17 08:24:46
Lets elevate the discussion a bit:
@_date: 2015-08-16 09:40:24
[Not only that.](
@_date: 2015-07-25 19:58:35
Anyone can state anything. Conflict of interest *simply exists* in this case.
Whether it is a large factor is another question - but the stalling done in the blocksize debate does make me quite suspicious that this is for money extraction by forcing layers on top of a crippled Bitcoin, honestly.
@_date: 2015-07-01 20:38:44
And you need &gt;50% of hashing power to be honest anyways for the whole network to work.
@_date: 2015-07-01 20:16:25
LOL. Warped perspective, conflict of interest, egos and arrogance, with some decent amount of group think added.
And this ends in supposedly great minds ignoring others, sticking fingers in their ears and saying 'lalala, I can't hear you?'
Gavin should publish the hard fork now.
@_date: 2015-07-08 15:11:37


Ok, got it. 
But then you are still wrong:  


If I understand you correctly, what you are saying is that you'll guess your fee policy and mold it into the shape of expected transactions to confirm. Because you have a limited information horizon, you'll make mistakes. Those mistakes end up as a rate above 0 of number of transactions validated and forwarded by you but not included eventually. 
For an attack, you multiply this number with with the attack rate to get your relay rate, and for arbitrarily high attack rates, the network load on your node from unconfirmed transactions will dwarf the network load from capped blocks.
I wonder why we are even discussing this, it should be clear - and it is also pretty off topic...


I don't think so. Assertion without proof...
EDIT: fixed missing word
@_date: 2015-08-02 14:23:35
1MB blocks forever or in general too-small blocks for mass adoption *are a risk to Bitcoin*. 
I am averse of that risk.
@_date: 2015-07-22 08:37:02
As someone else said, Gavin still seems to have the power to revoke commit access on github.com/bitcoin. That he didn't do that yet just shows how much he's still interested in getting everyone onboard - he's playing it extra nice.
Then there is bitcoin.org, which is administered by 1MB-blockistas (theymos?) as far as I can see.
@_date: 2015-07-09 20:28:44
That means that even without any blockcap at all, they can constrain actual blocksize to whatever value they want. And they say they currently don't like more than 8MB ... so it won't be more than 8MB for the near future.
Which means there is even less reason for a hard protocol-wise limit of blocksize!
@_date: 2015-07-02 14:24:47
Install Bitcoin/XT as soon as Gavin merged his lucky-number to lucky-number change in and released the fork.
Then, wait until it activates. Jan 2016, I believe is the first date for switchover.
It will only happen if enough nodes agree that this is the way forward. Otherwise, this change is inert and harmless.
@_date: 2015-07-24 17:00:05
Yes. Completely correct. We are basically arguing just about defaults. But as someone on the mailing list recently quipped: Defaults are powerful.
I simply do not like that the *default* software got taken over by a certain set of people.
[There was also talk recently on the ML to simply make blocksize a configurable setting.](
It should be mandatory IMO, I want to add. So users can see that they are responsible and have the power to make this decision.
@_date: 2015-07-14 13:11:22
Why so? This could be done over whatever channel miners + users whish to communicate.
It isn't really a whitelist - it is an expression of preference for a certain miner by a user.
@_date: 2015-07-08 02:54:52
Oh, I am actually in the same TZ. Very true. However, look at how even Gavin is affected.
@_date: 2015-07-11 10:48:30


In general I quite agree with your post - but what is bad about increasing the price of Bitcoin?
The price or market cap of Bitcoin is the best long-term indicator of its sucess.
I am not so sure that the group you are mentioning even wants that... 
@_date: 2015-07-10 10:07:57
It looks like LTC as well as DOGE both are not too scared of bigger blocks. That is a competitive advantage they both have vs. Bitcoin.
Doge might take over if LTC becomes successful, blocks almost full but it won't go forward on a blocksize increase.
But right now, stronger network effects for LTC overwhelm any reason for dogecoin.
But isn't that all obvious?
@_date: 2015-07-07 19:45:45


As originally intended by Satoshi....
@_date: 2015-07-19 11:24:15
The problem is that you always introduce bias. The model could predict that all posts here on reddit are from a single schizophrenic entity.
That would be the 'sockpuppet(user)=true' model.
Or it could predict that most users are not sockpuppets. The extreme, 'no sockpuppets' would be the 'sockpuppet(user)=false' model.
Both models would be fine statistically, but just uselessly mirroring their a priori assumptions. And there is no way to say for sure which one is wrong - other than common sense, which says that the first model is very far off and the second is probably off a bit - as in there will likely be some sockpuppets.
All that you should extract from the machine learning here is is a similarity score, and this could be used to produce a ranked list of candidates. He did do that, but he should have presented it honestly.
Given that there are quite a few candidates on the list that anyone can check to have clearly opposing views on the blocksize debate, one should IMO conclude that the sockpuppetry isn't actually that bad. 
The result of this analysis should be extended and rather phrased like this:
Manual analysis of the machine-learned similarity-scored list does not give many accounts that withstand manual checks of sockpuppetry. It is just reasonable to assume that sockpuppetry (so far) is a minor problem here on @_date: 2015-07-02 10:22:53
And they have a say in *increases*. Because they can *decrease* or *stop increasing*, thus preventing an *increase*.
@_date: 2015-07-24 13:36:34
I understand your worries. But here's the point: If we get UTXO commitments and a reasonable (e.g. Gavin's) blocksize increase schedule into core, we shouldn't be worried about this anymore.
Because what you store on your node or node is solely your discretion. Contrary to blocksize and UTXO commitments, you can decide for yourself how much old data you want to store and so forth, without ever being kicked off the network.
The discussion about blocksize is so important because it concerns the rules what the valid longest chain is. P2P network and full node behavior can otherwise be heavily and *independently* worked on/changed, without disrupting this consensus. And I think it will. With awesome things to come. I still see a worldwide payment system on level-0 on the horizon, if only we get the stupid 1MB-limiters out of the way.
Oh and about the forcing of full RBF: Your analogy is very appropriate. It sometimes feels like arguing with little, psychopathic children around here now. 
@_date: 2015-07-10 14:44:03


Satoshi made the rules of Bitcoin. He created Bitcoin. What comes after this is mostly housekeeping.


btcd tested 32MB succesfully, and in the comments someone mentioned that he got 100MB verification rate - so there is even a separate implementation that can do that.  And with regards to holy god: No, Satoshi laid out the rules for Bitcoin, simple as that. And you have case of serious tunnel vision if you think it is all just in the code.


You are missing the point. I am addressing the 'inefficiency', not the details. Database in a very general sense - store of data. Not saying whether relational or key value or whatever.


Yo missed my point again, this was @ efficiency.


Brightest minds? Some bright core devs, but not some others. Some other people outside the dev circle also have their very own opinions, and they also fall on both sides of the debate.
So... we'll see.
And - to be a bit more constructive here - we could agree on a clean divorce than having a hardfork battle - how about that?
@_date: 2015-07-01 23:26:39


[This](  is also relevent here, I think.
Note that Adam is worried on the other hand about O(n ^ 2) - too many full nodes in essence.
If this isn't malice or intentional, he must have convinced himself that Bitcoin cannot possibly work, because it cannot possibly hold balance without forced interference. Black and white thinking lets one run over the cliff in any direction...
Yet Bitcoin works.
@_date: 2015-07-31 09:36:38
I think you are both right in a sense:
Because orphans are equivalent to (small) chain forks.
But your further description is a lot closer to the truth then theymos'. The 'fragment into lots of different sized networks' is only correct when user don't have a strong incentive to keep the network functioning. Which they have. The question is whether the current central committee deciding on blocksize is the right way for users to express their wishes. I don't think it is, I don't think we have a stable situation here with good leadership and sentiment in the community seems to reflect that.
Some core devs are right now working on making themselves irrelevant.
Some knowingly and willingly so (Gavin), some others seemingly without seeing what they are doing (the BS guys).
@_date: 2015-07-01 22:04:31


I certainly hope end users are in control because else it would be a drastic centralization of opinion and authority.
Regardless what your stance is on the blocksize issue.
@_date: 2015-07-07 15:48:14
People with shitty internets will still profit from Bitcoin running in countries with less shitty internets, though...
I'd say to ease censorship worries, it is important that Bitcoin is successful and *widely used* in jurisdictions that are somewhat at odds with each other.
The U.S. and Russia, for example.
@_date: 2015-07-24 10:40:25
His math is BS. He seems to not understand the difference between speed, time and bandwidth.
With a terabit link, I can send ~ 100 gigabits roundtrip within one second.
This is a totally meaningless figure, though. 
Because, luckily, Bitcoin is only constrained to 600s for eventual consistency. Meaning a sphere of 180 million kilometers of radius. Just past the sun. We're fine.
@_date: 2015-07-08 00:27:28
Thank you! Although should get the laurels, as he pulled the whole thing up.
But see what I wrote [here]( I just further thought about it.
Basically, the whole blocksize helps the network from being overloaded through attacks is complete shenanigans - it can only make an attack twice as expensive per bit, it *cannot* prevent any attack of any scale from occurring!
***To summarize: Because blocksize is totally ineffective in keeping attackers off the network, we have been riding along without training wheels all the time already, and it is just fine!***
 Blocksize cap only keeps us from using Bitcoin to its full potential.
@_date: 2015-07-11 15:08:06
I think it will be fine. Note also that there is still always miner power to reduce the size, with influence proportional to hash power.
And the Chinese will keep it at 8MB for the time being, so that should be make the near future pretty agreeable for everyone.
@_date: 2015-07-23 09:16:32
This would remove *the power and the responsibility* for setting this from the core devs and put it squarely back to the users - where it belongs.
@_date: 2015-08-15 23:40:29
How so?
@_date: 2015-07-08 02:18:31
@_date: 2015-07-02 21:52:04
That would be funny - if it would fit exactly into Gavin's schedule :D
@_date: 2015-07-02 08:45:12


We will see. If blocksize doesn't change soon, you'll get a hard fork. Because a progression of Schelling points describes Bitcoin better than *just* progressing ossification.
3txn/s will also prevent that (ridiculous) worldwide billions of nodes scenario...
 


How about not incentivizing by crippling?


More reasons to not limit the block size, if you think about it...
@_date: 2015-07-11 15:13:33


Yes, but there are wars fought about money, too. Just look at who's going to pay in what currency for natural resources, for example. It isn't yet completely clear to me, but Bitcoin *could* have a positive effect there.
I just hope that in the end game scenario, people will come eventually to their senses with hash power and reach mutual agreements on not having terawatts wasted in huge mining farms. We'll see.


Agreed. Will make it less scary.
@_date: 2015-07-14 13:25:52
So? Is it your position to arrogantly decide for the user what to do? The point is that s/he can choose already.
@_date: 2015-08-15 20:22:31
Exactly. Running XT is actually safest. Always longest valid chain.
@_date: 2015-07-08 09:03:31
If so, he had basically just shown and confirmed that [the blocksize cap is pretty much irrelevant with regards to network health.](
@_date: 2015-07-07 22:40:35
Good point - I am not up to speed on those details. In any case, making it a 100kB hard limit might make sense from a preventing a DOS point of view. When transaction verification gets linear in time, that limit might also be lifted - should the need ever arise.
And, different to blocksize, 100kB for each transaction is indeed plenty.
@_date: 2015-07-08 01:19:38


Yet we survived the attack with very minimal relay fees...


How so?
@_date: 2015-07-05 09:28:32


Apparently, &gt;50% of hash power (Chinese miners) only want 8MB blocks. The scare was always about the majority of hashpower ejecting the minority of hashpower out by sending bloatblocks. The bigger miners bloating blocks as an attack against the smaller ones. This data clearly doesn't fit into that picture.
I agree though that a (sufficiently-high, dynamic) safeguard limit might make sense to keep a hacked mining pool from suddenly producing bloatblocks and temporarily disturbing the network.
@_date: 2015-07-04 00:02:17
The question which one was first is only interesting for the risk profile of accepting 0conf transactions. If you flag your transactions as RBFable, you do not have to worry about this and can leave the people using 0conf alone. Thank you.
@_date: 2015-07-23 21:37:35
Realistically, this won't happen anymore. We'll have two coins.
@_date: 2015-07-07 19:39:02


If we know that, then we also know bigger blocks, IBLTs and so forth work from a technical perspective. As well as Justus Ranvier's ideas about paying full nodes for transaction relay.
All very sound ideas, with no obvious roadblock stopping implementation.
I also agree that LN will very likely work from a technical POV. But I also think bigger blocks will work. Even more likely so, at least for blocksizes that are proposed now, &lt;20MB. 
So please, lets do the easy thing first, increase the blocksize, until we get lightning and please, let's do something about the situation that some core developers have an obvious conflict of interest.
@_date: 2015-07-09 17:43:30
Maybe so, with a *really* determined attacker. But then there's no point in even believing the current network, or is there?
I think if you go further along that route, you'll soon end up in paranoia-land. I do not believe in that big conspiracies.
EDIT: And you also have to consider this as a dynamic process. If it appears that full nodes get concentrated in the hands of just a few people that you distrust, your desire/incentive to run one or pay your friend to run one will increase....
@_date: 2015-07-08 14:04:44
I am not confusing, it, I am pointing out the difference and how the blocksize limit is completely ineffective in securing the P2P network... if you'd actually read what I write.
@_date: 2015-07-31 09:14:30
How about something *more decentralized*?
Here's a suggestion: Make the blocksize solely the responsibility and the power of the user. Allow each user to select any blocksize with a configuration variable. Provide *no default at all* for this variable, to force each user to think hard about it...
Consensus and a working network needs to be build by the users anyways.
@_date: 2015-07-24 08:35:55


So this is basically just confirming that the risk profile is currently very acceptable for many transactions. 
I however completely agree with avoiding to instill any false sense of security for 0-conf transactions.
But I think it is OK to tell a merchant: Some random dude might pull back a transaction if it does not have any confirmations. This can happen with a certain likelihood and you should be prepared for it and only do small, frequent transactions on 0-conf.
And companies can estimate the risk for merchants. And I believe there are some that do that already.
So why the intend by Peter Todd and others to kill 0-conf? It does look completely childish and ridiculous.
Should Bitcoin eve *naturally* evolve into a state that no one uses 0-conf anymore (because, for example, LN turn out to be awesome and everyone *wants* to use them), it can be deprecated as a feature at that point in time.
@_date: 2015-07-24 07:59:46
Honestly, I am at the point where I see this just as tactics and maneuvering. 
He's simply trying to sell the PR to get it out and then he'll NACK any such comprehensive change that he dislikes.
@_date: 2015-07-08 00:57:19
Trying to say it once more, maybe even better:
The whole discussion about the blocksize keeping the network safe is arranging the discussion around a red herring.
Because the blocksize does nothing to prevent any attacks. It at most makes the cost to push data into the network twice as expensive. Because with a very limited blocksize, every transaction an attacker prepares will *not* be percolated through the network in a miner block, whereas without a cap, it would (with a high chance, depending on the will of the miners).
*Attackers* simply do not care whether the transactions that they send are actually confirmed. They only care whether they can bog down the network.
In other words:
The network is fundamentally unrestricted and open. The blocksize cap only reduces the *average* transaction rate for *honest use*, those people who care about their transactions actually getting confirmed. It does not reduce the rate, *neither average nor instantaneous* for those transactions meant as an attack. Even worse, it means that the attacker will get the money back for those transactions that have not been mined! 
Network attackers actually profit from a block size cap!
EDIT: Tried to work the point out even more clearly.
@_date: 2015-07-22 14:54:33
It is too little too late for him to even keep an appearance of being reasonable in this debate. And because 2MB is *not* going to be the last word with regards to blocksize, who wants to argue with him another whole round when saturation at 2MB is approaching?
Exactly. I rather go with 8MB+.
@_date: 2015-07-24 17:10:20
Fake confirmation attack? Please explain.
@_date: 2015-07-01 21:41:29
How about you point out where I am wrong technically?
... and separate the political from the technical - I have clearly stated my political opinion that Satoshi's initial goal of Bitcoin scalability is still very much possible.
@_date: 2015-07-08 19:13:58


I happen to understand that.


Yes, so? If they would have been included in a block, they would have been sent around the network and written to disk all in the same order. I am not asserting anything else...


Again, I am talking about the scenario that a higher blockcap would be in effect. And that would allow to clear out mempools ... and prevent the attacker from recycling his money ...
@_date: 2015-07-21 14:54:33
Is your worry that miners will try to extract as many transaction fees as possible, starving the whole ecosystem of affordable transaction space?
They can always do that - they can make their blocks as small as they want. They generally don't do that, though.
I think the only worry is rather that they are *unable* to produce blocks big enough to keep Bitcoin alive.
@_date: 2015-07-11 14:06:48
The problem is that we have completely different opinions on how Bitcoin should evolve now. I cannot see any way any longer to reconcile them. Some, like me, want a large layer-0, and some others want a small one.
Wouldn't it be best for both camps to agree that there should be two Bitcoins then? 
Bitcoin/QT with 1MB, and Bitcoin/XT with 8MB (and Gavin's schedule).
Instead of trying to destroy each other in a battle, we could split up Bitcoin into two 'Altcoins' and users can decide which system fits them better.
@_date: 2015-07-24 09:22:04
Note that you are probably a 'nice node' and most of your net load is providing extra chain download capacity.
But otherwise: Thank you! Been saying that all the time.
And validated UTXO set merkle tree roots are a very workable alternative to SNARKs.
@_date: 2015-07-08 03:33:50
See also [this](
Blocksize is completely ineffective in preventing network attacks on Bitcoin.
@_date: 2015-07-08 08:30:18
And that only because of the blockcap...
The blockcap impairs long term *use* of the Blockchain, but does nothing against preventing attacks on the network.
It arguably hinders - because it makes it slightly less likely that an attacker needs to pay for the transactions that he spammed the network with.
@_date: 2015-08-22 12:39:18
Huh? Your own link if it has any relevance at all points to that. Or are you telling me that central economic planning of the fee market is healthier?
@_date: 2015-08-16 20:47:48
Fully agreed. A totally sensible approach. And I lost a bit of my patience, yes: I think opposing it by saying it is the fall of Bitcoin (or equivalent) is indeed concern trolling or manipulation.
Maybe I should be more patient, but the whole discussion here wore me down.
@_date: 2015-07-01 11:24:41
The right approach would be an effective uncapped blocksize and then the top layers (LN or similar) competing with the bottom layer (real Bitcoin) on the smallest transaction cost. This way, level0 could expend as much as it makes sense to meet demand.
Apparently, the chinese miners with &gt;50% do not like anything bigger than 8MB at the moment anyways, and as they have all the power to restrict blocksize freely we won't get blocks bigger than 8MB for a while.
Miners have an incentive to not cripple the network with gigablocks now, because if it means a full node/merchant is lagging, Bitcoin loses usability and value - and that merchant can then not submit his transactions to the miners.
Satoshi's original incentives work out and have no need to be co-opted by special interests.
It is ridiculous to even make the argument pro small blocksize cap because the majority of miners cannot handle it.
And I am sure that there is, even with a scaling, true Bitcoin, still large and honest profit to be made for Blockstream in micropayment hubs, for example.
@_date: 2015-07-08 02:55:15
Fully agreed!
@_date: 2015-07-08 12:05:37
This is wording it wrong: There is a fee market in *any case*, because there will be natural fees as the miners need to get their blocks to penetrate easily into the rest of the network - favoring smaller blocks than infinity, with fees truly greater than zero.
The question is whether we want a *forced* fee market from the central blocksize planning committee, or a *natural* fee market from the technological limits.
I think it is insane to want the former.
@_date: 2015-07-14 16:57:53


The point is that mining large blocks would cost the spammer money, while just letting the full nodes fill up their mempools doesn't - and lets the miner recycle his transactions, causing even more spam.
Disk space is by far not an issue yet - and if becomes, there is pruning and we could do UTXO commitments/coalescing.
@_date: 2015-08-15 22:36:03
Though I guess most of that seems to be public now? I don't really know, though...
In any case, signing it proves it to everyone. You'd still need to trust a dev (and that is not necessary something you want to do these days) to vouch for his identity.
@_date: 2015-07-24 13:50:54
Yeah so if I am a merchant and get scammed twice for a cup of espresso at $2.50, I might think twice whether I accept 0-conf transactions from unknown customers. I'll still let my regulars and people down the street do 0-conf, as I know them to be trustworthy.
I do not go and sell a $10e6 yacht with a 0-conf.
This is all implicated in saying that 0-conf has a certain risk profile. Done.
@_date: 2015-07-17 08:39:31


[He has coded it.](  I don't know of any specific testnet tests so far.


Shouldn't matter. Any client that allows 8MB blocks should work and find consensus.
@_date: 2015-07-04 19:50:22
Fair point!
We should probably always do a best effort and think about potential failure modes - but also always consider that we can't imagine everything. IMO a case of tunnel vision from some of the core devs.
Gavin's proposal is just the right fit there.
@_date: 2015-07-10 10:11:17
Adams arguments have to stand on their own, just like everyone else's... argumenting from authority is misplaced here.
And also, code and CS is just a part of Bitcoin. Get that into your head.
@_date: 2015-07-08 18:58:03




What in specific is so insightful from Bram Cohen?
Sorry to say this, but it is a long winded post with a lot of simple opinion. You can see that even though he as written a successful P2P system, he's an outsider to Bitcoin.  There are much better candidates who could write a pro-blocksize-limit argument. Citing him like this very much smells like an argument from authority to me...
@_date: 2015-07-08 11:05:15
Question is: How?
@_date: 2015-07-07 09:41:54
What is the problem with every block being 'filled'? What does filled even mean in an open-ended scenario?
@_date: 2015-07-09 20:36:09
Hehehe. LOL!
@_date: 2015-07-19 13:28:41
Which result? suspected sock-puppetry and put up the bounty. In my analysis, I found no strong or clear signs of widespread sockpuppetry - and I think the data of the analysis in this thread here shows the same, when actually read correctly.
However, it should be noted that (a critic of the blocksize increase) paid 2.5BTC of his bounty to me (a strong supporter of the blocksize increase) already. So I doubt that he intended to divide the userbase, I think he was honestly giving the bounty out to look for signs of manipulation.
@_date: 2015-07-17 09:47:29
He's has to go and make a release?
@_date: 2015-07-01 20:25:25
If there is serious contention about RBF vs. non RBF vs. fss-RBF, I suspect it to be manufactured. Seriously.
Because the whole RBF/no-RBF thing could be solved for all parties in a way that makes everyone happy: Have a flag that indicates whether a transaction might be RBFed or not, safe or not.
Maybe do the same with child-pays-for-parent.
I propose to have an opcode that sets various flags, and then full nodes acting on those flags for all these minor contentious things like RBF.
If this is the way people are driven away from Bitcoin, this option of making it per transaction optional should be shouted wide and clearly.
After a TX is in a block, that data is thrown away anyways.
@_date: 2015-07-08 14:21:45


Agreed. This is a very common argument in this debate, though... that the blocklimit actually prevents the fullnodes from being overwhelmed bandwidth-wise.


Again, agreed. On the other hand, there are also clear costs with keeping the limit, as can be seen from the recent stress tests/attacks.


I agree this can become an eventual problem, and I think solutions to this are UTXO commitments, light full nodes and maybe, if it will become a big issue, UTXO coalescing after ten years or so - putting the burden of storage back to the users. If you look at my posting history, I worry that this might become a (solvable!) problem in the more distant future, too.
However, increasing blocksize allows more valid transactions to happen, allowing growth. Having a large UTXO set is in this view a luxury problem - that of actually having adoption. Note also that with 1MB blocksize, the set can be bloated by GBs per year in principle, but that actually does not happen. There is no reason to assume it would suddenly with bigger blocks. It would also cost a fuckton of money, more money with more bloat.
Which brings me to my next point: Isn't the blocksize a pretty imprecise tool to attack the much more confined problem of UTXO growth? If you have a hammer... 
The problem, if it appears, could be attacked with much more targeted approaches.


I disagree. I think we should allow all transactions to happen so that full node load decreases, less mempool stuffing - and making it more expensive to attack the network, because causing load on full nodes wouldn't be cheap anymore. And Chinese miners signal are only ok with 8MB. They are the hash power majority. So there won't be TB-sized blocks anytime soon.


SPV mining incentives work out so that eventually the miners will get a clue and implement something where they have a full node resetting an SPV miner going astray. Regarding wallets: Fair point, but again a blocksize cap isn't the solution, rather [ideas](  like  is proposing.


I agree that a high safety cap (e.g. Gavin's proposal) might easen worries, but I do sometimes wonder about this: 
There is a strong incentive to come to a consensus in the Bitcoin network, and I believe Miners are following this incentive, too. (Occasionally, there will be odd stuff happening, of course. But the long term average is the interesting case). The more I think about it, the more I think the network doesn't need forced developer input to come to a consensus on blocksize...


Yeah I know those worries. But again: How would those TB-sized blocks *not* orphan? Especially when 50% of the miners are behaving rationally in the sense that they want Bitcoin to survive, they'll not allow this. And we're dependent on the majority of the miners anyways.
I am now pretty convinced that the conviction that a block size limit is necessary is some funny mass psychology. A spontaneous symmetry breaking, if you want: Everyone wants some maximum block size, so it must be wrong to have no maximum block size..
I also explained my view further [here.](
@_date: 2015-08-10 08:54:22
1MB is a fork of 32MB Bitcoin. For consistency, I demand the 1MB fork to be removed from @_date: 2015-08-12 10:09:33
On chain? Where does he say that?
@_date: 2015-07-14 13:15:19


Why so? It is just an imperfect way to have your special miner mining the transaction. You are expressing a preference, as a user, but you aren't able to simply express a 100% preference (due to the obligatory fee for forwarding) for a single miner. Only asymptotically.


This isn't really on-topic anymore, but: By knowing your miner personally or from trusted friends?
@_date: 2015-07-10 10:36:41
Now it indeed looks like a good fix. I am also glad that the PR has been reopened on this. Looks like LTC serves at least the purpose of being an experimental platform for things Bitcoiners do not dare to implement.
Which brings me to another insight: Bitcoin has a quarter the effective maximum blocksize that Litecoin does. Yet Litecoin survives just fine with that big of a blocksize cap.
According to the limiters, it should be overwhelmed with cheap transaction spam...
That it isn't shows again that the whole blockcap worry is bullshit.
@_date: 2015-07-08 01:27:30


The negative trade off of the blocksize cap is obvious.  The security improvement, as shown minimal. (2x cost for attacker)


It makes forcing large reorgs easier because it makes mining more centralized and less miners are validating.
Why is that making force reorgs easier at all? If I am a miner, I can force a reorg, at *my cost*...
And the less validation is because SPV miners are not yet effective in what they are doing, as discussed elsewhere...
@_date: 2015-07-24 08:16:06


The payment processor will use a redundant, replicated data store, just like Bitcoin does, to keep his transactions safe.
I completely fail to see the inefficiency in Bitcoin. Rather, Bitcoin has arguably less regulatory cruft bogging it down.
And Bitcoin is an open network and protocol for payment processor, *at least*.
@_date: 2015-07-07 15:46:21
And even if on-chain micropayments never work out because it turns out that technological progress came to a complete standstill today...
It still does not make sense to artificially cap the blocksize to keep it from reaching physical or technological limits, just because one is fearful of Bitcoin's success.
If technology makes transactions cost the equivalent 10ct each in the long run with mass adoption, so be it.
If LN turns out to be a nice addition on top of a Bitcoin *with* effective *unlimited* blocksize... and Blockstream makes an *honest* profit from running some payment hubs in *that scenario*, so be it.
But crippling it to 3txn/s? Forcing upper layers on top?
Give me a fucking break.
@_date: 2015-07-08 00:17:21
Further thinking about it, this test makes a 
***Damn good case if not proves that no blocksize limit is needed!***
There is actually *no hard, enforced limit on the network bandwidth
used by Bitcoin*!
This means that anyone with enough money can attack Bitcoin and drive the
transaction rate that he puts into the network to an arbitrarily high
amount, except for his financial limitations and the technological
limitations of the network. 
The *block*size only matters here in so far as the *block*size can
cut down the amount of data that flows through the network ***at most in
That means any attack on the network gets at most twice as expensive! And to
any resourceful attacker, this is a ridiculously low barrier to overcome.
Again, this is important:
***The blocksize cap does not limit an attacker from injecting arbitrarily high
amounts of data into the Bitcoin network, it only makes it at most twice as expensive!***
That is a pretty weak shield of something which supposedly is so important
for network health. It does not prevent attacks except for making them just
about twice as expensive, it only prevents transactions from being confirmed, 
thus basically only *crippling* the network!
The blocksize fundamentally *cannot* stop attacks on the network, as it is *just denying regular transactions from happening*.
Yet, with the recent stress tests, we have tested what an attack can do, and
And given that the tests referenced in the submission rely on the network throughput itself being
truly unlimited (in the software - of course there are *natural* hardware
limits), I think the following should further be kept in mind:
Policies on rejecting transactions when the mempool is full and network enforced 
limits on bandwidth have to be seen *very suspiciously* as an attempt to sneak in 
another hard cap into Bitcoin, and as an attempt to hide the fact that the network might 
be able to process more transactions.
We need to be vigilant here, to avoid the blockcripplers and payment-hub
subsidisers taking away even the capability to show that the network
had a lot of foresight seeing the blocksize becoming a major
political issue, and I hope he and everyone else interested in a scaling
Bitcoin will see whenever something happens again to the code that might
be abused to serve some special interests.
@_date: 2015-07-08 02:46:12


Economy of scale always exists. Do not try to legislate against nature. Thank you.


Again, read Tier Nolan on this...
@_date: 2015-07-09 20:57:35


Then you are out of luck. But gladly, Gavin is trying to even keep it in reach for people like you and therefore proposes an increase schedule below estimated hardware and bandwidth costs for the future.
I would be fine with Gavin's proposal. But I think a Bitcoin that would be truly free market would be even better, for the reasons stated.


No one is taking away anything from you. There is no force involved. But if you want to keep a truly small Bitcoin, may I suggest an Altcoin instead? 
Bitcoin was always meant to scale, and hopefully economic pressure as well as intent of the users will make it scale. Because only a big, scaled Bitcoin will be able to serve the needs of the globe.
For some Altcoins, the only important difference that exists between the Altcoin and Bitcoin is that Bitcoin has a much higher network effect and thus market cap.
The blocksize determines the number of people that can be served and thus the size of the userbase. The size of the userbase is in essence the network effect. The network effect is extremely closely coupled to the market cap, price/value and penetration of Bitcoin (almost the same).
This means that if you do not like to increase the blocksize, it follows that you won't get the userbase, if you don't get the userbase, you don't get the network effect, not the market cap and not the value of a scaling-as-intended Bitcoin.
That choice is fine. But in that case, as you necessarily also forego the latter things, an Altcoin will not be a loss to you.


Every resource has a price. Think about it. Even when you connect to the network now, you can only do that because someone else is providing you the data freely. People do provide that out of altruism, curiosity or a strong sense of responsibility for 'providing for the network'. All I am arguing for is making price discovery and choice better.
@_date: 2015-08-15 22:19:44
This will BTW show whether is right in his analysis. I bet he is :-)
@_date: 2015-07-07 20:45:53
Interesting in this context how pointed out in 2010 that setting a temporary fixed blocksize is an issue and also that there will be contention due to people getting used to the wrong ideas, isn't it?
@_date: 2015-07-19 11:49:50


And here's our difference on deciding what is decentralization, and we're indeed back to a 'vision' for Bitcoin, even if you do not like it: 
I argue that a Bitcoin with larger full nodes in data centers (as seen to eventually happen by Satoshi) is still decentralized, if the user base is large and if those larger full nodes are in different jurisdictions. If Russia and the U.S. both have many Bitcoin users, the governments might well end up competing for open access to the blockchain, instead of crippling it.
IMO, an open ledger and open protocol for synchronization between big, even 'huge' nodes, even if there are just dozens of them worldwide, is still far from a paypal scenario. This is also what can arguably be inferred from his writings was Satoshi's goal. Size of the user base is a very important, if not the most important decentralization metric.
And from a technological, physical point of view, big full nodes in datacenters are indeed possible with today's technology even.
This isn't so much the point of the debate anymore, though, as I would be fine with the compromise is proposing: An increase schedule that has a good chance of keeping the average node within reach of a dedicated hobbyist or at least a smaller club, but still allowing Bitcoin to scale. 
And there would still be the chance to soft-forking it back down should Bitcoin lose its useful properties...
Again... isn't Gavin's proposal a very reasonable compromise here?
@_date: 2015-07-05 20:48:01


That is a weird way of wording it, though. The pool is in control only as long as people point their hashpower there. That is solely a technical level of control. Certainly worrisome in case of hacks and similar things, but does not reflect the true situation - which is the provider and owner of the mining power being in actual, physical control, so it is the other way around...
@_date: 2015-07-29 19:09:40
I think the point here is: LN *could* do quite a lot of good for Bitcoin - they *could* enable sub-cent microtransactions, for example, and they could generally take an order of magnitude or two off the Bitcoin transaction volume. They *need* to be implemented and tested first.
Increasing the blocksize *has* been tested already, and the network is known to support a peak rate of &gt;400txn/s.
That said, ***LN are not -the- solution to the scaling problem***. They are maybe part of it.
Gavin's schedule would give us 8GiB blocks eventually, which would allow people to move around their money every couple days directly, on-chain or open/close/clear/whatever LN channels with a similar frequency.
The LN paper argues that 133MB blocks are needed eventually for a transaction for every person on the planet twice per year. 
I think risking part of your money to be stuck for half a year is a lot less acceptable than losing access to it for a week or so, should a payment hub become uncooperative or otherwise unavailable.
So, in that sense, Gavin's increase schedule hits quite a sweet spot IMO.
It should also be noted that it is probably best if miners have the option to actually compete for transactions with the LN network and are not crippled by tiny blocksizes. Shifting fees from miners to centralized LN providers might also well hurt the security of the Bitcoin network and currency.
@_date: 2015-07-09 22:23:28
Seconded. Adam, we'll all gladly adopt LN if you can show us a concrete example, including usability and lightning network node topology.
If you can make a case that LN will allow a completely decentralized, hard-to-censor and zero-barrier-to-entry competitive layer just like Bitcoin layer-0 is right now, it will come, and you will be even more famous than you are already.
Think about it: You get a lot of heat, your conflict of interest is called out, and so forth. The best answer to all of this would be to push back by clearly showing us that LN will be awesome.
@_date: 2015-07-08 20:57:16
Who can do that? The miners?
@_date: 2015-07-09 22:01:09
So you say one can avoid locking in part of your Bitcoins for half a year or similar?
If that is true, that would indeed be news to me. A link would be greatly appreciated.
@_date: 2015-07-05 18:20:00
Agree, 0conf is the way to go forward. Just educate yourself on the risks of it.
There is a (for low values usually small) chance to be scammed.
@_date: 2015-07-06 19:49:23
They lose money by mining on the wrong side of a fork every so often. *Easily* more money than what running a full node that resets their SPV funny mining if it starts to happen would cost them.
I think they'll soon get a clue that SPV mining for multiple blocks can be indeed quite stupid...
@_date: 2015-07-31 19:17:46
Yeah, I think I wrote a couple posts in jgarzik's BIP100 announcement thread along very similar lines. 
So basically, take min(BIP100, BIP101) having a BIP100 without a 32MB limit?
I would be ok with that, too. I'd rather want to see a simple 51% majority of miners then, too, though.
@_date: 2015-07-10 10:02:33
If Bitcoin fails to increase blocksize, LTC does have a competitive advantage.
We Bitcoiners should make sure that this won't be the case.
@_date: 2015-07-05 21:44:26
Self-fulfilling prophecy if they continue crippling it...
@_date: 2015-07-29 20:30:11
The money they *could* make from crippling Bitcoin and forcing layers on top simply *is* a conflict of interest. It doesn't mean that they'll necessarily act on it. It just means that *it is there*.
@_date: 2015-07-08 16:00:32
Yes, indeed I am rejecting the idea that you can drive your false positive rate to exact zero.
And the whole discussion was in the context of the whole network - and there we simply do not have such a perfect filter.
Even if you can do it for your node.
@_date: 2015-07-24 09:29:07
Yeah - that's why I see the scenario of lots of tiny transactions with a vastly larger user base when the mining reward diminishes...
But I guess we disagree on that.
@_date: 2015-07-10 11:11:33


Hear, hear! So is an 8MB sidechain going to work or is it going to fail? What is it? 


Will try to honor this - as you very likely know, reddit does this automatically, even when quoting... and there is nothing particularly snarky...
@_date: 2015-07-09 13:17:29
Wladimir van der Laan is opposing the blocksize increase and is working for MIT as well.
So at least the group think cannot be too big at MIT.  Compare this with another place...
@_date: 2015-08-13 09:03:47
Is there a node implementation in existence that uses non-determinism as a side channel for a soft fork?
@_date: 2015-07-08 10:51:27
What do you mean by removed completely from We can still post here after all? This is evidence?
My knowledge about reddit rules is not very firm, sorry...
@_date: 2015-07-08 09:26:48
Also, blocksize cap is a practically worthlss tool against network attacks. Have a look [here.](
It does not serve any purpose any longer.
@_date: 2015-07-08 02:59:01
Security from a blocksize cap. Context, my dear.
@_date: 2015-07-08 11:39:12
Cool, am curious about that.
@_date: 2015-07-11 11:56:05
I think for Gavin to come back from vacation and push the hardforking code to Bitcoin/XT.
He indicated that he's going to do that - and there have been a couple weeks now without movement from the other side - so I think he *will* do that.
I'll load XT, and everything will stay the same until Jan2016, *regardless* of which software you are on.
Honestly, I think the most sensible, most adult approach to the whole blocksize debacle should be that we all agree to disagree and make it a policy that we inform people about the new status of Bitcoin now: That we are divorced now and that there exists a Bitcoin/QT and a Bitcoin/XT, with users having to choose their variant.
For all those going to tell me that this is very damaging: 
Please answer the question then whether a hardfork battle is less or more damaging than at least doing a clean separation and at least having the *chance* of not letting it evolve into a PR disaster?
Because the hardfork is inevitable now. It will come. There's no going back. And there's also enough 1MB people who want to keep their variant of the chain.
So why don't we instead make it agreeable for everyone?
@_date: 2015-07-07 22:07:32
BOOM. DONE!
Awesome. I feel bad for not spotting this immediately - this is the amount of psychological manipulation going on, the blocklimiters successfully bogging people down into all kinds of senseless discussions. And one loses the big picture.
@_date: 2015-07-30 18:46:34
But interestingly, a 10GB block has an insane orphan cost as well. Basically no other miner will see it because their network connections will not allow that block to enter their systems.
It is an interesting question whether running your super-duper mining hardware to produce a chain fork that cannot be transported at all (think truck full of HDDs) is actually ever part of the longest chain - and what exactly 'part of the longest chain' means.
With IBLT &amp; larger blocksize, a miner is also constrained in further bloating blocks, because he'll only tend to include transactions that propagate well and thus have been seen by all nodes.
I agree that makes sense to have a pure safety limit, though.
@_date: 2015-07-21 15:02:17
I guess this is just one of the many derailing tactics from the blockcappers?
Same way the 1MB block limit was supposedly helpful against spam. Yeah, right. /s
The way the blockcap increase side seems to get stopped is by (apart from being outright trolled) endless concern trolling with concerns which turn out to be constructed mirages.
Nothing adds up for the 1MB-blockistas anymore.
The core craziness is in the nonsense that an artificially, severely constrained system will somehow make Bitcoin successful. To support this 180-degree twisted BS, seemingly a lot of [psychological manipulation]( is needed, too.
@_date: 2015-07-08 08:55:38
You might like to have a look at [this]( given the recent events.
Blocksize does not prevent network spamming at all. A total red herring.
Thanks for your cartoons and other satire, btw. :-)
@_date: 2015-07-08 21:23:45


But the total here should cancel out. For every upload, there's a download. You are just being especially helpful to the network :-)
@_date: 2015-07-09 08:51:36
Well said. Same with consensus being 'it is in the code!1!'...
@_date: 2015-07-09 23:11:52
Then maybe ask Adam or Greg to ask Gavin?
I think in this context probably all core devs are against your idea, though.
Really, I also don't think it is that problematic - if at all, the hard fork itself is a bigger risk. (And I think that is very manageable, too)
@_date: 2015-07-23 14:28:42
I know that there isn't 'consensus' - [so how about the proposal on the ML to make it a user specified variable?](
Do not provide a default and make it mandatory to set this value - puts all the responsibility to build a functioning network back to the user.
Bitcoin is supposed to be a system that will eventually end up in a consensus state.
It should not be the responsibility nor the power of a core dev to set this limit in a decentralized system.
@_date: 2015-07-07 09:29:48
... also, total amount of fees collected by the miners increase when user base and blocksize grows.
@_date: 2015-08-10 08:27:10
Good point. So essentially showing everyone: This is a decentralized, uncontrollable beast?
@_date: 2015-08-16 18:12:42
Another one: Noticing that Bitcoin core is just a bunch of people - and not an authority. At most a self-selected one.
But we all know that in principle. 
@_date: 2015-07-05 10:02:48
^ This
And it can be seen as dangerous to go from game theory with a too narrow set of assumptions to forced limitations on Bitcoin. It is easy to foresee a market for full nodes and paying them to relay transactions. Paging ...
@_date: 2015-07-23 08:33:06




It would be nice to have full nodes on 8-bit AVRs. Won't ever happen, though. A scaling, successful Bitcoin will not run on your RasPi at home, so this is not a choice that can even be made.
That said, Gavin's proposal (which is heavily compromising on blocksize) which most on the increase side, including me, seem to back, has a good chance of always keeping a full node in reach of a dedicated hobbyist. What is not to like about that?


I am. See the comment I wrote just a moment ago [here](


Ridiculous to call this blacklisting. But I am happy that you are so watchful, exactly as I described in the other, linked comment. Extends from the devs to the opposed users, I guess.
@_date: 2015-07-07 19:47:29
Just a suggestion: If people like you would get away from the idea that full nodes have to keep transactions forever and run on a RasPi and a 56kbd modem, we might actually have a higher transaction rate on the network...
@_date: 2015-07-08 02:14:28
[Here]( is maybe a more understandable quote:


When someone sends a transaction, they send an inv message containing it to all of their peers. Their peers will request the full transaction with getdata. If they consider the transaction valid after receiving it, they will also broadcast the transaction to all of their peers with an inv, and so on.
Convinced? :-)
@_date: 2015-07-24 10:45:31
That doesn't answer your assertion:


Here's my very basic calculation: A full node costs at most a couple hundred bucks a year.  If you have one block per year orphaned that you could have avoided with a full node pulling you back on track, that's a loss of 25BTC, ~$6000. A lot more than a full node.
He had three orphaned in short order.
And, mind you, the full node would not prevent SPV mining (which is fine), it would prevent SPV mining running amok (which isn't fine at all, but incentives align and this type will actually cost the miner money).
You might be confusing free-spinning 'SPV mining' with proper SPV mining?
@_date: 2015-07-08 13:59:56
The capacity of the network is in how many transactions it can gossip around and validate.
This capacity has been tested. Turns out, &gt;400txn/s validated and flooding the whole network works with the current topology.
Whether actual blocks have been produced or not is not so much of interest here. (Except for the factor of 2 in bandwidth). So that means the network can process 200txn/s.
@_date: 2015-07-06 19:36:16
By the miners burning themselves and getting a clue.
Incentives work out exactly as intended. Basically, miners tried to optimize but only saw half of the equation.
@_date: 2015-07-29 20:37:03
I think git is an awesome system, too. Your point being what?
@_date: 2015-07-22 08:43:11
There is venture capital for companies trying to provide (centralized) services *on* the blockchain.
And then there is venture capital for companies trying to subvert the protocol to force money into centralized services.
And the latter is the problem...
@_date: 2015-07-31 10:43:38
They can fake already any chain fork below 1MB that they want. They don't do that, though. This whole 'faking it' argument is just smoke and mirrors.
@_date: 2015-07-08 10:37:34
Do you have an idea whether this could be reddit hiccup or is this a well known hint of some censorship from somewhere?
@_date: 2015-07-14 18:55:54
Yeah ... but if the majority of the miners want to screw us, they can. That's a core design decision of Bitcoin...
@_date: 2015-07-18 10:57:08
Yes.  That this doesn't happen yet are some social dynamics, people organizing themselves into groups etc.
@_date: 2015-07-04 19:27:41
Maybe better definition: Bitcoin is the blockchain that has the most proof-of-work (energy-wise).
Which currently is the same as most SHA256 proof-of-work...
Which currently is the same as 'largest market cap'...
The above definition works for determining Bitcoin consensus and what is real even in the unlikely scenario that the hash function has to be changed soon, and when there is the inevitable hard fork for blocksize, etc.
EDIT: I also specifically said energy-wise and not money wise, because in that case you'd have to base the definition on another (fiat) currency. Joules are the currency of the universe.
@_date: 2015-07-29 20:09:25


    Bitcoin will never be on a firm foundation, because hardforks are possible


If I am engaging in a straw man, it is your very own: 


I read this as you being interested in a firm foundation for Bitcoin. 


Sidechains that do not correspond to transaction clustering are simply not a solution for scaling Bitcoin. And those that map to transaction clustering are probably going to be a lot easier to censor.
And if you can make an 8MB sidechain, it would be much more appropriate, last but not least *to keep transaction volume down* (no cross chain txn), to make a 1MB mainchain...






You are defeating your own argument. The fact that Bitcoin *could* have been invented back in 2000 even just shows that it indeed needed a stroke of genius to bring this project to life.
Many tried, many came close - but yet not quite enough. 
@_date: 2015-07-15 22:02:04
It is not as bad as it sounds from your comment, because the LN payment hubs (when that thing comes out of pre-pre-alpha) might be able to freeze your money, but they won't be able (as far as I understand) to take any funds.
I think there are serious problems with LN only when it becomes a *replacement* for on-chain Bitcoin transactions - or when Bitcoin is bent by people with a conflict of interest to force LN into place.
That's why think Gavin's proposal hit a sweet spot: Low enough to probably always keep the blockchain in reach of a dedicated hobbyist, but eventually growing large enough to support a transaction every couple days per person on the planet. And enough reason for the blockstream people to continue what they are doing now.
If I can switch/update/(un)load/clear/whatever my LN payment channels and hubs every couple days by putting a transaction on the real Bitcoin network, that is very usable.
Not so much if I have to lock in the money I need for payments for half a year.
The only really ridiculous thing about LN is trying to force it on top of Bitcoin by crippling the blocksize. 1MB or 2MB are not going to cut for the next couple of years.
@_date: 2015-07-11 14:51:05


Understandable - so I think this is also why consensus is a bit misleading, see below:


Agreed. I think the consensus is only true in the technical sense. In a sense, the idea of longest chain of work created a whole new *battleground* (attached to having this technical goal) in which competing parties will fight 'wars' to win the majority.
Gladly, these wars are fought with hashpower and swaying users and not with nukes.
And I think *this* is the big advantage of Bitcoin. This is also making it a lot more peaceful than actual war. I also think this might, in 'end game' scenarios make countries fight 'wars' or create MAD with hashpower - that would be an awesome outcome.
But the recent dispute shows that 'consensus' is a very nice sounding thing for something that can be very much like a battle in the end. There's 'consensus' on who's the ruler after a country has ben conquered, so to speak.


I think this is exactly what is going to happen for people who are sitting on the fence. Better safe than sorry.
@_date: 2015-07-08 03:14:01
Indeed that needs to be something to be solved, and removing the blocksize cap would at least prevent the worst backlog here.
This is not really on topic with regards to the ineffectiveness of the blocksize cap, though?
@_date: 2015-07-06 21:34:24
Have an upvote. Small block social engineers out in full force today...
@_date: 2015-07-07 20:09:10
I read all of them and even participated in some. 
Ever saw that blocksize is approaching 1MB?
Ever thought that maybe there is indeed  a problem with keeping 1MB size?
Ever thought hard forking needs planning in advance?
I fail to see how this is a PR blitz...
@_date: 2015-07-08 02:51:25
Incentives work out. Big blocks will happen because people will pay for them. That is a *good* thing, not a *bad* thing.
You are arguing against big blocks as if you think it makes sense to artificially constrict Bitcoin.
@_date: 2015-07-16 08:21:21


Block crippling == more revenue for payment hubs.
Isn't that an obvious one?
@_date: 2015-08-15 22:01:11
You assume that longest valid does include the artificial nonsense limit of 1MB blocksize. Not for me. Bitcoin core has no authority for me.
And if you decide to stay on a chain that is easily 51% attacked, that is your problem.
@_date: 2015-07-07 15:21:09


I would assume that the size of the network and the transaction rate are to a large extend correlating with market penetration and usability, though?
Also, Bitcoin might be better for some markets than others. So it might be 1/100 of transactions on average in the mid-term, but it might be 50% of transactions for certain segments of the market. 
@_date: 2015-07-21 20:16:13
How about github.com/Bitcoin and bitcoin.org just being  umbrella websites that list a couple of competing Bitcoin implementations, such as Bitcoin/QT and Bitcoin/XT, giving each one equal representation?
This would squarely put the responsibility back to the user on which client to run on the network and how to make a network that will achieve consensus... where it belongs.
@_date: 2015-08-18 10:37:35


Only by waiting for time out.
Which means the time out needs to be sufficiently short.
Which means Bitcoin transactions should be cheap enough in the end game to at most lose access to your money for a couple days.
Which means BIP101. Lets do that.
@_date: 2015-08-09 17:43:54
That is true, most people on the planet with today's technology probably could live a leisurely live, but what we *cannot* support is all of us reproducing with a rate higher than replacement...
@_date: 2015-08-27 20:34:40
has a good argument, though, that 1MB should be the transport layer and not the consensus layer. 
Arguably, that it is now deemed consensus layer could be seen as successful consensus social engineering.
@_date: 2015-08-16 23:51:19
Yes. What a screwy situation to be in. Thanks for the explanation!
@_date: 2015-07-02 00:15:25
Sure. But this is not an urgent problem.
@_date: 2015-08-11 22:14:02
Gavin and Mike are some of the longest and arguably some of the most knowledgeable core developers. Gavin was @ Bitcoin before any of the Blockstream guys, for example.
And you must have missed that Bitcoin XT is just a small patch set *on top of* Bitcoin core?
@_date: 2015-07-01 20:33:26
Fine. Then let them openly and honestly state the conflict of interest that they are in. And let them be part of the debate after this has been made clear to all parties and can always be referenced. 'Yes, the blockstream guys gain from a limited blocksize'
The correct thing to do would be to remove yourself from the debate in such a case, but being forthright about it would be a start.
But they are rather deflecting from it. And point how they should be all kinds of trustworthy. THAT makes a lot of people suspicious. Rightly so.
@_date: 2015-07-08 03:05:36


And you fail to show that this is the case. You simply assume that, without any further explanation.


You make up FUD, that's all... :-)
@_date: 2015-07-02 09:42:51


The chinese guys have &gt;51% of hashpower and all the power to not make bigger than 8MB blocks happen. If they want to, they could stall the whole network and just mine 0-txn blocks all the time.
And if you assume the majority of miners is interested in a functioning Bitcoin - which you have to in the context of Bitcoin, no way around that - the majority of miners also have an interested in rejecting crippling GB-sized blocks from a single rough miner, should that ever happen.
And I won't even argue against some safety limits to prevent a sudden hacked miner from wreaking havoc on the network, should the other miners be too slow to react.
That's completely different from crippling the network with a hard cap, though.
@_date: 2015-07-02 09:08:07
Nothing foolish about switching to an implementation that a) does nothing at all yet and b) will be inert in any cases until and unless a supermajority of miners switches, too.
@_date: 2015-07-21 13:00:48
So.... as we can see many people now think that 1MB is somehow magic and intended.
Shouldn't we make sure that all references to 32MB in the BIP100 proposal are clearly mentioned as a technical limit and not an ought-to-be?
I fear we might arrive at the same deadlocked situation again, should people draw a conclusion from an agreement to BIP100 that somehow 'the historical 32MB limit is the golden ultimate blocksize limit'.
The problem is that this whole thing got insanely political already and I think it might be long-term very important what the specifics of the language in BIP100 are, should it become a success. 
@_date: 2015-08-16 20:21:33
Note also that Blockstream caused the 32MB network limit to be elevated to an *ought-to-be* in BIP100.
Which is, by the way, completely contrary to the market-based approach that jgarzik was arguing for in his BIP, it basically led to him contradicting himself in the BIP. 
See here, jgarzik's twitter feed: 
It really is that bad. XT is urgently needed. Dislike Hearn all the way you like, but the software is doing the right thing. When he does do any shenanigans (which I deem unlikely anytime soon), use what XT did to core - fork again.
@_date: 2015-07-05 17:24:11
Mmhm. Agreed. Also, have a look at [this]( Multiplying n with n becomes poly(n) x poly(n) for no reason whatsoever...
And Adam seems to be in the [ivory tower above everyone else]( too...
@_date: 2015-07-09 22:10:24


What straw man? My opinion is certainly close to a lot of other people's opinions...


Well then...
@_date: 2015-07-09 17:00:04


Excuse me? I am arguing for making the network more flexible, so that you can decide *yourself* whether you want to run a full archival grade node, an SPV client (*) or anything in between. Or outsource that business!
You need access to the network to spend your money. Full nodes provide you that access. You have to pay for that. It is better to have fine-grained control over resource trade-offs because it allows better adoption to market conditions. Easy as that.
(*)-  With improved security, hopefully something like [this]( gets implemented soon
@_date: 2015-07-08 02:10:03
The exact inner workings of the mempool are pretty much irrelevant in this discussion. Sit down and think about it, you'll realize it.


Oh, thank you.
@_date: 2015-07-24 17:12:26
You want to automate this highly political debate that is happening here?
Well, I think this is an interesting idea but I think there's way too many human factors involved in that decision still.
@_date: 2015-07-23 14:41:14
Yeah exactly. Or at least remove the whole contention about it out of core development and put it back to the users. Make it a limit that is mandatory, and Bitcoin will probably break into two factions, 1MB and 8MB. I think that is going to happen anyway. In any case, it would be up to the users to do that.
Some users would say ./bitcoind -maxblocksize 1000000
and some others: ./bitcoind -maxblocksize 8000000
As soon as a bigger block gets mined, the chain splits into two. Make it solely the user responsibility to keep it a working, meaningful system.
@_date: 2015-07-07 22:00:25
***This is a very good point.*** What you are basically saying is this, if I understand you correctly - please correct me if I am wrong:
The Bitcoin network got indeed 442tx/s for a  short while, filling up mempools.
*That means that nodes processed and validated 442tx/s for a while.* They only didn't write them to the block chain, to disk as valid blocks, because enforced protocol rules right now prevent that.
The *only reason* the average actual rate didn't go up to 442tx/s is because the hard blocksize limit prevents blocks from being that large.
Note that without IBLT being implemented yet, you'd have to cut the effective transaction rate in half. Mined blocks would about contain the same number of transactions that get put into the network.
But this still means that the current, as-is Bitcoin network can handle 221 tx/s for a short period of time.
I think it is thus *very safe* to assume it can handle at least a tenth of that (to play it extra safe) continuously.
That would be 22.1tx/s. ~7MB blocks.
Lets please remove the damn limit now.
@_date: 2015-07-16 08:24:40
A big block attack (and I pretty much see that happening only when a miner gets hacked) can be defended against with just 10x the average of last two weeks or so. A pure safety limit, never to be reached during normal operations.
@_date: 2015-07-03 10:25:07
Not before the price crashes and the media broadcasts 'it is just a toy system', though.
And at that point, unlike the 'Bitcoin CEO committed suicide' BS, they'd have a valid point.
@_date: 2015-07-08 09:24:48
Also, have a look at [this...](
@_date: 2015-07-07 16:05:46
Might actually be indeed pretty good *when it is ready...* 
Meanwhile, lets neither rely on vaporware nor force users into higher layers, but increase blocksize instead.
@_date: 2015-07-30 08:29:31
The byzantine generals problem is fundamentally unsolvable. And this was used to inform the debate on distributed consensus systems.
But Bitcoin introduced a good-enough approximation.
@_date: 2015-07-08 21:33:41


Yes... but in total, it should cancel out still. Think about it: Where should your upload end up, if not as a download into another full node?  (Ok, or maybe with small chance other part of the network, such as watch nodes)
@_date: 2015-07-08 22:29:30
Yeah, I think I got that now. I was looking for some point I might have missed. Worded it a bit too snarkily maybe?
@_date: 2015-07-08 02:19:37
This is it. How did we lose our minds so much?!
I wonder what thinks...
@_date: 2015-07-10 09:39:34
It looks like seems right in his assertion on BCT that you really went off the far end... :-)
@_date: 2015-07-07 15:06:57
Given the BIP66 situation, I actually have to agree on this one...
@_date: 2015-07-03 10:35:20
He could support flagging transactions to individually be allowed RBFed, fss-RBFed or full-RBFed .... or not! 
A merchant could then look at whether the 'RBFable' flag is set and deny 0conf's that are flagged this way.
That way, he'd keep 0conf transactions working for everyone but still allow RBF for those people that want it.
But what did he do? He went and (temporarily, gladly) convinced a pool to follow along with his plan on breaking 0conf - seemingly just for the heck of it.
Look at the reaction of the mailing list, too. This behavior was very unreasonable, to say it lightly.
@_date: 2015-07-31 09:19:25
I would guess one answer here is that the typical up/down ratio is the default configuration of your line that your provider sets (because most people use the Internet as a giant TV offer), and not so much a *technological* limit.
@_date: 2015-07-01 20:35:20
... or the payment hub.
And of course, they'd profit from a slightly increased Bitcoin blocksize in the mid term - to not kill Bitcoin outright.
But in the long term, it will make quite the difference whether consensus has settled on 100MB or 1000MB blocks in terms of the amount of off chain processing and thus market size for payment hubs.
Blockstream folks should all be very honest about this clear conflict of interest. But they are not.
@_date: 2015-07-29 20:34:08


***Very dishonest***, as it is taken completely out of context. Satoshi was arguing that people will be against putting BitDNS data, potentially lots of info similar to WHOIS etc. into the Blockchain.
Enough said.
@_date: 2015-07-05 16:01:50
But long term, they always have the incentive to stay on the correct chain, regardless of the games they are playing to squeeze out some more probably-valid-hashing per block.
I think the scenario that you are describing would come into play when CPU bandwidth in txn/s-validation is less than network bandwidth in txn/s-arriving. I think that is a very pathological case and also highly unlikely, as txn verification can be parallelized easily and so CPU power can be thrown at the task.
Only when verification time gets on average longer than block creation time would there be a problem.
But in that case, you'd also need to look at the other side of the equation: Whoever wants to make so many transactions has to construct them all - and pay a minimum fee on all of them to be valid. And get them to percolate through the rest of the network. And, and, and...
@_date: 2015-07-19 12:58:07
Well, and there is no new data that shows a blocksize increase is somehow impossible. Satoshi's original goal with full nodes in bigger data centers is still completely achievable - with very big full nodes even in the unlikely case that technological progress stalls completely and when we are left with today's hardware. Instead of no cap, I rather want to see what argument is left against Gavin's reasonable increase schedule...
But that is kind of off topic - about blocksize itself instead of sockpuppets in blocksize.
That said, I wonder whether I should be happy or worried that I am completely off the list of potential sockpuppet suspects? My writing seems to be unique...
@_date: 2015-07-07 15:56:24


Proof for that assertion? What are bloated blocks? How come bloated blocks into existence in an IBLT scenario?


That's all you have to say? 
@_date: 2015-07-24 07:43:35




 -- [Satoshi Nakamoto, August 4th, 2010](
One should also note that a cup of coffee is in no way a micropayment. That said, if Bitcoin can *freely* grow but will only grow to mid-sized blocks with LN payment hubs on top that are used for coffees and micropayments, fine.
But LN should not be forced on top by crippling blocksize.
@_date: 2015-07-10 09:18:14


Derailing bullshit, and you know it. The blockcap wouldn't be the spam limiting factor in this case, but PR6402 would.
Please try to troll more intelligently next time.
@_date: 2015-07-09 10:48:23
Also, may I suggest that maybe we want to eventually maybe hash (very) old transaction outputs together into a tree and store just the root hash?
That would put the long term burden back to the user to store the data needed for moving his coins...  and the UTXO set wouldn't grow unbounded.
@_date: 2015-07-09 10:51:18
Takes a while for those solutions to bubble up, though. 
I am pretty sure miners deciding to cap block size of their blocks even more isn't the best solution yet....
@_date: 2015-07-08 11:28:51
Ok. Would be nice to know whether staff/mod killed posts are marked in any way...
@_date: 2015-07-06 23:05:07
I have not seen the comprehensive game theory yet showing that big blocks are dangerous, only opinions disguised as game theory.
@_date: 2015-07-08 02:09:00
What attack, if not a SPAM attack, is the blocksize cap supposed to prevent?
Remember, it was only meant to be a temporary, *drum-roll*....
 ***ANTI-SPAM*** measure.
@_date: 2015-07-09 08:59:59


Well, of course, there's the orphaning due to a single block race and I think about half the nodes will not really see that, for example. But I think the focus on the whole network is correct here ... the chain state consensus.


One narrow but certainly valid argument - the careful approach would be to limit the damage through long transactions. This is why Gavin plans to limit to 100kB transaction size for his fork.
@_date: 2015-07-31 09:39:12
Thank you for putting it so clearly. I pointed this very thing out to all the people who think f2pool's behavior is proof that blocksize is already to big or some other BS.
@_date: 2015-07-09 22:56:58
And that's good. Point still stands - the blockcap does not help against spam in the network...
@_date: 2015-07-02 00:28:06


And as it has been pointed out repeatedly, decentralization is ill-defined.


There is another problem solved with Bitcoin. Control of the money supply is decentralized in Bitcoin, not so much in open transactions AFAIR.
And if Russia has some full nodes and the FBI wants to redlist transactions on U.S. ones, they would need to do so at the risk of breaking the network apart and full censorship of the U.S. internet. With a large *userbase* of Bitcoin owners, this will be a *very* unpopular move!
You make it alot easier for governments if you restrict Bitcoin to just a settlement layer - ***something it was never intended to be!***
And this is true irregardless of how big the nodes are, as long as they are in competing jurisdictions. And this is what Satoshi had in mind when he said - rightfully so - that Bitcoin can indeed scale to gigatransactions/day.
Many many transactions on layer 0 will also make it a lot harder to fish the 'right' ones out.


All fine. But what the heck has hash power to do with full node centralization?
If you are telling me big blocks will be used by big miners to drive out small ones and centralize hash rate, I have news for you: The biggest miners in China, with &gt;51% of hashrate, are only ok with 8MB blocks so far.
And because they can always constrain the number of transactions as much as they want to, we can all be very sure that there *will* only be at most 8MB blocks in the near future.
@_date: 2015-07-08 02:39:19
Exactly. This is fucking obvious now.
@_date: 2015-08-15 19:54:37
Yup. This one is telling, too. Way too crude for Satoshi.
@_date: 2015-08-15 22:18:26
Keep your 1MB node running. It will form a network. It might consist of just one node though. You can still transact.
Heck, you might even be able to ***CPU-mine*** again. 8-)
@_date: 2015-08-15 12:29:30
Yep. So basically, the results are that not increasing the cap keeps Bitcoin from growing and pushes users away. 
Who would have thought that? Why didn't anyone think of this? This is sooo surprising... /s
@_date: 2015-08-16 22:04:11
Yes, we do, agreed. And github.com/bitcoin needs to be redecentralized. bitcoin.org as well. We have to figure out how to do that.
@_date: 2015-07-08 02:42:29
I don't know if I actually understand you? 
More transactions allowed by the miners == more transactions that a spammer will pay for.
Other than that, the blockcap makes spamming the network only twice as expensive, correctly.
With a higher Bitcoin price and thus arguably higher investment in infrastructure, and a higher average transaction rate, spamming the network will obviously get harder, too.
@_date: 2015-07-07 08:56:01


But you could as well argue that a hard fork would make the non-updated nodes fall off the wrong chain completely, making a cleaner cut.
Overall, I guess I am simply not at all that worried about the things you are worried about... but if you want a testnet test of 8MB, for how long and how many nodes do you think would suffice?
@_date: 2015-07-08 12:10:16


Yes, sure! But what does a big block attack do to bandwidth use that a big unconfirmed transaction attack doesn't?
Exactly! Nothing!
The big block attack is actually benign in some ways compared to the unconfirmed transaction attack: No memory pools filling up, no chance for the attacker to recycle their coins for another attack, *less load on the full nodes in the network*. Exactly what the blocklimiters intend....
@_date: 2015-07-24 08:57:11
Good thing that a block arrives only every ten minutes, isn't it?
And no, SPV mining with a full node pulling that SPV miner back is not a problem.
 
Verification can also be done in parallel, on clusters. Miners have a lot of hardware already, they won't care whether there are a couple CPU righs between the mining rigs...
@_date: 2015-07-01 10:47:56
Ok. Usually, if I have invested or am dependent on a company that would potentially profit from actions I do in another, unrelated position that is supposed to be independent, I *at least* openly and widely state that I am biased and that I have a conflict of interest.
The *correct* thing to do would be to remove oneself from either position (Blockstream or being a self-assigned authority on Bitcoin).
What we hear from the Blockstream guys is how Blockstream 'can't be evil' (apparently their company motto? LOL...) and how they all care more for Bitcoin than for their company and that they can be trusted very much. 
This approach of denial alone makes me very suspicious - because a clear admission of conflict of interest would need to be in order here. It would be *honest*.
And there needs to be a very honest and forthright admission, too, that Bitcoin with small block caps is a clear deviation from its original goal of wide scalability.
Instead, all this is hidden behind scare talk and supposed impossibilities. See also [this.](
@_date: 2015-08-15 20:47:58
Works both ways in a long discussion - it will collapse all other posts, too. It is not hard to make reddit show all posts, though.
But [deleted] posts, I *cannot* get, unless I saved them in time or have a bot saving them running.
You know there is a difference and you are arguing from a losing position here. Downvote me all you like. But mod-deleting XT posts is another category. Shame on you.
@_date: 2015-07-24 17:21:19


Ah, that thing. So basically you are saying miner mines special fork and then returns onto the main chain.
Here's the answer to that one: Bitcoin allows this from day one. This is nothing new. That it doesn't happen is because incentives work out.


Can one be more arrogant and trollish at the same time?
@_date: 2015-07-02 09:09:06


Straw man. As if it is the same set of people.
@_date: 2015-07-08 14:54:23


A prayer? Are you joking? Is this a hidden reference to some guy around here? :D


Only when those resources are actually used - which would be a good thing.
Other than that, a higher blocksize allows more transactions to clear and thus makes attacks more expensive, lowering the damage that an average attack will do.
@_date: 2015-07-06 22:10:00
You're welcome. Glad that I could help.
@_date: 2015-07-24 14:29:48
The fees from your market won't go to the full nodes, though...
I think has it right: There is a reason already to run a full node and eventually full nodes will be in the business of transaction forwarding.
@_date: 2015-07-09 20:01:34
I am rather arguing that you should be able to build something in between JFK-sized airports and your local hospital's helipad.
But I guess I should just ignore 2 day old users as trolls?
@_date: 2015-07-11 11:08:15
Gavin's schedule ends at around ~8GiB blocks. With 500 bytes/txn and 7 billion people, that ends up at
***0.3 txn / (day * person)***
There is plenty of room for LN and similar to pick up the remaining demand for transactions, without forcing people to lock in their money for half a year into a (centralized?) LN hub and still keeping Bitcoin in easy reach of everyone. Accessing your money every couple days instead of twice a year makes quite a difference. And ***1MB would mean every person on earth accessing their money about just two to three times their whole life***.
And at the same time it has a very good chance of always keeping a full node in reach of a dedicated hobbyist, and *at least* always in reach of a smaller business.
It is a heavy compromise with the small-block side already - shouldn't it start to look reasonable to you, too?
EDIT: Typo fix.
@_date: 2015-07-24 13:08:31
No the bullshit and propaganda is telling people 0-conf is dead. (And, yet, we somehow need to get rid of it by forcing full RBF through)
'This guy double-spent successfully' as well as 'double-spends never happened to me' are *both* anecdotes.
As some say, the sum of anecdotes is data.
In this case: 0-conf has a certain risk profile and should never be marketed as risk-free to merchants. But it is absolutely ok to tell people that it works most times but there will be a certain number of people scamming, or trying to scam you.
Merchants understand risk. Else they wouldn't run a business.
@_date: 2015-07-08 19:00:02


What is nonsense? The high transaction rate?
The ineffectiveness of the blocksize cap in reducing this attack vecto? The inability for some stubborn core devs to move on the blocksize issue?
@_date: 2015-07-05 17:31:56


And who writes the code?
I have said it before and I say it again: A lot of the small-blockistas operate from some projected fear of the miner majority screwing them over. This risk, however, is innate to Bitcoin and you don't make it better by projecting it away and letting it evolve into all kinds of fancy and complex arguments against the miners (which you seem to tend to see as enemies). Instead, you make Bitcoin worse.
Accept that 51% of miners can always screw us and move on, please.
@_date: 2015-07-03 18:06:21
And I want to see how the US takes down full nodes in Russia.
Or rather, I don't want to. Because they could only take them down with nukes.
Or completely censoring and crippling the U.S. internet.
This is the success scenario for Bitcoin - larger full nodes in data centers. Not some 3txn/s cripplecoin.
@_date: 2015-08-04 22:51:05
You are missing the big picture. A single large block does only little harm. Sustaining large spam blocks is *costly*, and the constrast becomes drastic with IBLTs, because blocks that full nodes agree on (in terms of their mempools) are much easier to propagate.
Incentives work out and your argument does not remove orphan cost from the picture. 
At most, you are describing the unavoidable economies of scale.
@_date: 2015-08-18 11:49:34
Note also that it is easy to mold an email to follow a certain statistical profile...
This is a hoax. Sign it with a Satoshi key or go away, troll, as someone (paraphrased) said on the mailing list.
@_date: 2015-08-15 18:36:32


About two decades, to be more specific.
If Bitcoin is still running at 8GB blocks, it will be a wide success!
@_date: 2015-07-06 22:26:52
@ Propagation time: Is your worry that the network gets overloaded and out of sync? I am still trying to see what your line of argument is here.
       RE: scraping together money to run a few hundred full-load full-nodes: hardware is cheap, people are expensive. You seem to expect that companies will be willing to invest the time of their people testing something that may never happen (8MB of transactions every ten minutes). Maybe they would, but most companies are very busy trying to stay in business by attracting customers to their products or services. Scaling up is a good problem to have, and, in my experience, the way to be successful scaling up is to tackle problems as they occur.




Apart from the irony that otherwise tries to recklessly push RBF through, I have to agree, this might indeed not be the best approach from I think he convinced himself enough that big blocks are not a problem at all the same way that the blockstream people convinced themselves that big blocks are the end of Bitcoin.
However, necessary test efforts should also be put into perspective, compared to other things that have been changed in Bitcoin, and compared in how complex the changes are. How much as BIP-66 been tested, for example?
@_date: 2015-07-24 07:40:11


Is not math, is purely opinion.


Somewhat valid but becomes opinion by conveniently leaving out other factors: In a future scenario where transaction fees pay miners, being network-wise close to where the fees are generated can have a positive effect on miner revenue.
@_date: 2015-07-07 09:32:58
There will always be an economy of scale with regards to any endeavor. Trying to legislate (1MB blocksize) against it is foolish and will only make the market inefficient.
Other than that, mining centralization comes up again and and again yet the clear, proven, direct link to blocksize has not been established. Just a couple of hints that could as well countered with hints in the other direction.
@_date: 2015-07-30 14:24:56


If that is a common sentiment, we'll see a lot of new people working on stuff after the hard fork to 8MB+ and the increase schedule has been successful.
I am optimistic! :-)
@_date: 2015-07-04 09:47:07
Because, apparently, miners do not behave according to the narrow assumptions game theory that people ascribe to them.
F2Pool went back from full RBF to fss-RBF, so they must have some incentive to do that.
If it turns out that most or a significant fraction of miners will use RBF all the time, 0conf could still be phased out then.
@_date: 2015-07-30 10:13:10
Probably, yes. I personally think even unlimited blocks are fine, too. However, Gavin's proposal seems to be the most we can get through.
@_date: 2015-07-07 16:10:15
No one knows, and I haven't seen that money. Neither do you, you are just speculating.
All that could be extracted from your post is that a proof of stake opinion poll would be nice to have.
I am pretty sure that this poll will *clearly* show a vast majority for an increase in the blocksize, probably according to Gavin's schedule.
@_date: 2015-07-31 10:19:08


I expect that Bitcoin will take the VC route and rightly so. Because VCs are keen on making money - in this context, increasing the *value of Bitcoin*.
If cypherpunk ideals are more important to you than the monetary value of Bitcoin, you can go and create/use any Altcoin with a small blocksize limit. You won't miss the value, because you're apparently not interested in it anyways...
@_date: 2015-07-05 10:57:03


The question though is whether we really have that situation. I am not convinced we do.


Agreed. This might be the bigger issue.
I have to say though, comparing the situation to Linux vs. MS-DOS is a bit simplistic. Bitcoin the software is just a part of Bitcoin, the ecosystem.
@_date: 2015-07-08 10:32:43


It did, except for offloading the transactions from memory pool to disk due to them being mined. That is point.


If you have to point out that current network capacity is *only (lol) three times paypal*, we indeed have other things to worry about than restricting blocksize.
Such as figuring out who would profit from that.
@_date: 2015-07-24 17:44:38
0-conf as existing makes it less likely that the scammer can get off, though. So it would be a change in the risk profile for the worse.
@_date: 2015-07-02 00:07:22
Billions of people do not need to run full nodes. Not even millions of people need to.
@_date: 2015-07-05 22:12:23
A seatbelt would be a 10x average cap (or similar usually-stays-out-of-the-way) just in case a big miner gets hacked and causes some disruption.
1MB blocksize is neither seatbelt nor airbag, it is taking the wheels off your car so that you are safe from driving on the highway.
@_date: 2015-07-05 17:08:12
Same here... not a big holder (just enough to buy a when it goes to $10k/BTC :), but enough to keep me motivated to stop the small-block BSing here. Bitcoin, *just* Bitcoin.
Also, I have a lot of higher-level motivation, the whole system, software and ecosystem also genuinely interests me.
@_date: 2015-07-24 08:07:35


Sure it isn't at least 99.991% of users? /s
@_date: 2015-07-21 12:53:19


I agree @ 51%.
The 75%  actually creates instability, IMO. Because it might create a perverse incentive for miners to form a cartel, censoring a single rogue 25% miner blocking a block size limit change.
BIP100 without the 32MB elevated to an ought-to-be (that this happened should show you the politics going on...) and with 51% of miner power would be a nice low pass filter on block size and would give - as jgarzik seems to have intended - everyone a nice and comfortable (because it is reasonably slow) view on changes in the blocksize limit.
@_date: 2015-07-17 08:43:35
And the question remains what consensus block size is, then...
I think the only correct answer can be longest proof of work.
@_date: 2015-07-01 11:42:15
'Contributor to code' only overlaps partially with 'owns a significant amount of Bitcoin' set.
I'd really like to see a proof of stake backed opinion on this matter.
@_date: 2015-07-14 10:18:41
Further thinking about this ... couldn't we just implement this in wallets right now, in a different way?
Idea: 
- Miners publish a Bitcoin address that they will accept for incoming transaction fees
- Wallet creates transaction with *three* outputs instead of the usual two: ***Payee, Change, Miner***
- transaction is broadcast as usual, with just minimum fee
The miner addressed in the 'Miner' field will have a large incentive to mine your transactions, the other miners won't (they'd help their competition, as the fees would be paid anyways).
Very interesting dynamics! Now that I think about it, your post should indeed go to the top. This should make the the 'miners need to be a lot more decentralized' - people happy, as this would be a way to support their local neighborhood miner. And they could start a movement for locally grown mining blocks :-)
@_date: 2015-08-23 18:44:32
My point was that there is no way that this deal can be made because there is no way to check whether one runs a more efficient block relay protocol.
@ IBLT in P2P: Fully agreed, but it is in any scenario a lot less code than LN. And Gavin has test code up since at least a year.
And further, you can look at the fast relay network as crude approximation of this.
@_date: 2015-07-24 07:55:38
Hubris seems to be killing Bitcoin.
Hey folks we are rich1!! Hey folks, *we* invented the new bank settlement system, it is a given it will become that with 1MB blocks and $1000 fees. Some random dude had this idea of inventing Bitcoin as a widespread money/currency/payment system, how stupid of a fool! Banks are just salivating to submit to our will! Away with all the useless users!
@_date: 2015-07-11 11:25:18


You made that up, didn't you? Awesome technobabble: I need to set my zyngilirators to level 7x!
@_date: 2015-07-01 11:27:37


SPV wallets. Also: High full node count != decentralization. And he talked about *userbase*... 


Free market in action. Many users make this risk/convenience trade-off. 
@_date: 2015-07-07 23:23:29
The situation of inelastic demand meets hard limit (blocksize cap) hasn't been tested yet.
Maybe wants to clarify here?
@_date: 2015-07-24 12:57:03
The point is that UTXO commitments allow to choose from a wide scale of security - it allows people to run a full node with transactions from a year back processed for example.
And that is as good as full node security. Because you'd need a conspiracy of the full network lying to you for a year for anything drastic to happen.
@_date: 2015-08-11 22:41:38
Due diligence has been done. And raising the blocksize is indeed a solution, as nothing ever came along that proved Satoshi's vision of a scaling Bitcoin wrong. Just FUD.
@_date: 2015-07-01 21:56:19
As I said elsewhere: Why not make an opcode that can set a flag that decides whether a transaction can be RBFed or not? Another flag for FSS or full?
The best of both worlds and it would let the user decide. No need to make this topic another holy war.
@_date: 2015-07-24 09:03:47
It is relevant in the sense that there is already a fee market. 
@_date: 2015-07-05 11:41:24
Agreed. Make it 10x average then or something like that, and use a reasonably short time base (1-2 weeks).
@_date: 2015-07-08 10:48:18
Yeah... But where are the rules that state this is disallowed?
There is really stupid stuff upvoted into the stratosphere here on @_date: 2015-07-08 02:46:59
They will because they have an incentive to make blocks that are valid in the rest of the network. It is as simple as that, stop spinning this to your liking.
@_date: 2015-07-08 01:50:59
Again. Think about it. The transactions percolated, gossiped through the whole network. They have been validated by all nodes.
Yes, the transactions that *didn't* make it into a block. The bad ones. The spammy ones.
Most of the work has been done! Only thing missing is wrapping them up into a block by the miner, and that is all that is prevented by a blocksize cap!
@_date: 2015-07-06 07:56:22
You say 'wrong' as if you do not simply state an opinion. Exactly what many do not like you about you small blockistas...
Make it clear that it is your political opinion that we should have smaller blocks and change the course of Bitcoin. Do not socially engineer it.


SPV mining a single block isn't too much of a problem... it actually strengthens proof of work. Problem is when multiple blocks in a row do not get checked like in this scenario - but incentives work out, miners lost money that they could have easily avoided with a full node resetting their SPV chain. And they'll get a clue, problem solved.
@_date: 2015-07-02 18:39:06


Good point. OTOH, 51% of Miners can collude and censor the misbehaving miner out.
I think in this sense, BIP100 might actually *create incentives for a collusion between miners*. I am not so sure that we want that.
BIP100 with &gt;50% rule could work out OK. It would basically just be a damper preventing too fast block size changes.
But then you could also go and just put in an x times average of last n blocks dynamic limit or similar.
And if you then further simplify that, you end up with Gavin's proposal.
So I guess that one makes the most sense. If one believes that there needs to be a hard cap at all.
@_date: 2015-07-08 00:37:53
You mean submission?
@_date: 2015-07-09 13:32:37


In a way, we are fighting about consensus what a full node should provide for free. We have a set of people who want to provide 1MB blocks, and we have a set of people who want to provide 8MB blocks.
This is unfortunately strongly affecting the debate about the future of Bitcoin at a much larger and long term scale, where there shouldn't be one: 
Full nodes need to be paid for services eventually. And it does not make sense at all to constrain blocksize here, as effective payments for bandwidth, validation and storage can be worked out in a free market.
The *good* thing is that this market can exist right now,  already, just not with a very fine granularity what a node wants and doesn't want to take from or provide for by participating in the network.  
I do not see a path forward for Bitcoin that will include a significant fraction of purely volunteer free nodes in the long term. And I do not think that this is a bad thing.
@_date: 2015-07-05 21:01:25
I wonder whether the devs are the right place for that agreement, or rather the miners?
Miners are painted in very scary pictures in the Bitcoin ecosystem, yet they are the ones having very large exposure to Bitcoin. And thus a strong incentive for this to work out. No one wants a warehouse full of double-SHA256 chips with a failed Bitcoin.
Maybe the users should have a voice, too, bringing me back to the proof-of-stake voting poll I always wanted to implement.
And then, the minimum requirement will change over time etc. That's why I think Gavin's proposal is best, he makes the best guess at this minimum requirement and it will be a *predictable* formula. And should it be off by too much, blocksize can be soft-forked down...
@_date: 2015-07-21 22:06:09
And what part of selfish mining is actually related to the blocksize cap and hasn't be addressed already, e.g. [here](
@_date: 2015-07-12 10:04:24


The miner is just mining the txn and actually costing the spammer money. Because otherwise, the spammer is spamming them for free...
@_date: 2015-07-14 16:52:57
SPV mining is such a loaded word now - there is SPV mining with a full node pulling you back onto the real chain - and there is 'SPV mining' without that.
The latter was/is problematic to network stability, but it also made the miners lose a lot of money. Incentives work out to prevent this long-term.
The former, I do not see a problem at all.
@_date: 2015-07-09 09:13:44
But can you do that if blocksize is limited? The full node would need to know the modalities of all miners, basically. Hard to imagine that there will be 100% certainty ever.
@_date: 2015-07-24 14:34:05
Completely OT, but:


Feminists have their own economic theory now, too?
@_date: 2015-08-16 23:43:39
Thank you, and, yes, I am aware of this! 
I still have my personal set of preferences and think BIP101 is the most reasonable way forward.
And makes a success scenario possible with working LN (or similar) and widespread, *accessible* Bitcoin layer-0.
@_date: 2015-08-19 10:10:09
can you leak this encrypted email (in decrypted form, of course) from the blockstream employee in full?
Think about it: Yes, it might burn bridges, but it also will put disinfecting sunlight on something you strongly believe to be damaging shenanigans as well.
Thank you!
@_date: 2015-08-20 13:27:15
You have been censored or you deleted your post?
@_date: 2015-07-03 23:56:30
Default is no RBF though and 0conf works for a lot of people who take that risk...
@_date: 2015-07-19 15:28:16


Yes, I do. I guess that is the dirty world of marketing. Well, at least you are honest after the fact... :-)
@_date: 2015-07-08 15:53:15


In the scenario of a large block, transactions are more likely to confirm, thus more likely to cost the attacker money.
@_date: 2015-07-10 10:55:06
Yes... and bigger full nodes, too.
My point is simply this: Only Bitcoin has the promise to eventually scale *really big*.
A small-block-cap Altcoin would give you all the things Bitcoin, has if you are interested in such a thing. You could even make it a spin off or a nice clean 'divorce' in January '16, that might be better than a messy hardfork fight. If both sides of the blocksize debate 'agree to disagree'.
So there is no reason to prevent Bitcoin from achieving its goals or an Altcoin from achieving their goals.
But Bitcoin's goal is to scale - so let it scale.
@_date: 2015-07-05 20:36:55


Yet, as we can see, Miners use SPV mining. Network bandwidth for block headers - all that is needed for SPV mining, is truly negligible. So the large capacity miners would need to make big *invalid* blocks... but by doing that they'd be cutting themselves off the network first for that kind of attack, losing money by wasting it on invalid blocks.
And if 51% of miners are attacking us in this way, we have big problems anyways. This risk is inherent to Bitcoin and has to be accepted - no way around that.
If I wouldn't be so tired of this whole debate, my post here would be a lot stronger-worded. what do you think?
EDIT: Further thinking about this, SPV mining is a damn good argument against the whole bigger blocks mean forced centralization from big miners  BS and FUD.  Because a miner can always SPV mine with very high success rate on headers but have a full node running in parallel to do block verification to alert and reset the SPV part for the odd time that it diverges from consensus.
@_date: 2015-07-22 13:29:37




 -- [Satoshi Nakamoto, August 4th, 2010](
So, yes, medium-sized micropayments are out of the range of Bitcoin's intended functionality and should be done through things like changetip. But 1MB is not even large payments, it is *inter-bank settlements*, at most.
And a cup of coffee isn't a micropayment, by the way. And I do not even disagree with LN or similar on top of Bitcoin. If it is not artificially *forced* on top, by crippling the blocksize.
@_date: 2015-07-07 22:21:19
And, oh wonder, BIP101 will include [exactly that...](
@_date: 2015-07-08 02:26:13
It is not needed. You are believing in a mirage.
@_date: 2015-07-24 17:15:15
No, he's saying that it is safe to run an 8MB node but not an 1MB node. That is correct. Because the 8MB node will always be on the longest chain of work. Unless the miners want to increase blocksize even more than Gavin wants, which they stated already that they don't.
@_date: 2015-07-23 12:31:50
Keeping the 1MB limit for Bitcoin is an extreme measure.
@_date: 2015-07-21 12:48:36
Exactly. Orphan rate is indeed a red herring, especially when the big Chinese miners said they don't want to go over 8MB because of their bandwidth constraints...
There will always be some economy of scale and it is more than futile to prevent that from happening. Also, any altcoin can fill the niche of 'small blocksize' - only Bitcoin could fill the niche (rather position) of a worldwide blockchain with large blocksize.
In the whole blocksize debacle, some people want to do central planning of the Bitcoin economy, to preserve some wishy-washy centralization. All while ignoring the biggest decentralization factor of all - the size of the user base.
@_date: 2015-07-09 20:39:06


True! Equally, we shouldn't keep it on based on reasoning starting from a false dilemma.
I have seen a lot of folks - and I forgot that fact, too - who still think that the blockcap somehow constrains the amount of spam in the network. Yet it clearly doesn't, and it makes sense that it doesn't.
@_date: 2015-07-06 23:27:52


Well said. And spam that pays isn't really spam either...
@_date: 2015-07-10 19:49:46


... and we have seen about half of the bandwidth per transaction already in this test.
@_date: 2015-07-01 20:02:51
Instead of suspecting paid trolls, which I would expect there are probably a couple on both sides (but not too many, at least from my very preliminary analysis)... how about you guys all go and clearly state and at least admit the conflict of interest that you are in?
I understand the argument that Bitcoin with LN or SC might or even will profit from eventually somewhat bigger blocks.
But I fail to see how one could argue that there is no incentive at all to fuss with the block size in one or the other direction to please your investors or grow your company.
It might make, for example, *quite* the difference in fees that potential payment hubs receive long term depending on whether Bitcoin will eventually end up with a final 10MB, 100MB or a 1000MB cap.
Someone who invests in a company is usually not a charity, they want returns.
@_date: 2015-07-03 10:26:37
There are enough altcoins for your liberty. If wealth (~ price of the coins) is not important to you, use them instead.
No one is stopping you.
@_date: 2015-07-07 13:36:16
That incentive isn't that perverse. The stuff that they include must have percolated through the network of full nodes first, or else they cannot use the IBLT.
If they include transactions that have not been seen by the majority of nodes, they'll be back to risking an orphan for big data again.
In a way, it actually levels the playing field between miners and nodes even more, making miners reliant on the nodes having processed and validated the transactions that they want to include.
Again, have a look at what is writing about this.
@_date: 2015-07-05 10:32:48
But mining on a funny SPV chain risks a lot of money.
@_date: 2015-07-04 00:13:27


Why push them if they are fine using 0conf?
People just need to be aware of the risks. That is all.
@_date: 2015-08-11 21:53:27
Or as the [README for Bitcoin XT]( puts it, probably from Mike Hearn:


That irks me the most about some of the 1MB-blockistas. This insanely stupid arrogance and hubris and belief that 'Bitcoin is invincible with 1MB blocks'. *facepalm* x 100
@_date: 2015-07-08 15:45:32


Yes I know that. Longest in the metric of total, double-SHA256 work. I thought saying 'longest' and implying 'most proof of work' is just usual slang around here?


Yes, but only if I can communicate it to someone else. Just having it sit in my basement does nothing. What we generally subsume under just 'orphan cost' is actually a pretty complex set of dependencies and less than half of the picture; the miner has an orphan cost if it cannot relay the block to others, the full node a participation cost for staying in the network. The truck full of HDDs with a PB-sized block will immediately be orphaned by the network - because everyone would say: "Are you insane? I am not going to accept this as a block, go away, drive your block truck elsewhere.". Regardless of whether bitcoin-core has some constant in the code allowing this!
Do you see it? This behavior is the political and economic process of collectively deciding on blocksize.
What you say is that 'there must be clear consensus on what is acceptable'.
What I say is: Economic forces  can mostly work out such a consensus. People are interested in keeping Bitcoin running. I do not need to prescribe a size. It will work out.
Because all such sizes are always imperfect. Bitcoin is an alive, always evolving system.
With one caveat. And this is the area where political *opinion* comes in: The blockstream cryptopoliticians think that they have the moral right to prescribe a crippled blocksize here (for their own profits). 
And I simply *hate* that they frame it as a technical argument. Because it is not.
Yet, obviously, from a very distanced perspective, they are simply participating in this process of deciding on a blocksize, too. But with tactics and following incentives that I dislike and that I will call out.
@_date: 2015-07-03 18:14:41
You will *easily* see if there is widespread 0conf breakage by that miner.
And CS prof a clown?
Don't be ridiculous.
@_date: 2015-07-14 10:33:18
That's a first order perspective. But if you think about this further, it could create an incentive to break the economic chain of mining a block into smaller parts:
Your small miner could collect many small transactions that would pay him and then rent enough hashing power to finish that block.
This lending of hashing power could be automated happen at very short time scales (&lt; 1s). Furthermore, small miners could create profiles on where they want to direct their hashing power and could channel it to whatever pool they favor for a healthy mining ecosystem...
This would all be possible with the protocol as-is. The question remains how much of this fragmentation is actually wanted and needed by Bitcoin.
One point remains after all, though: Users, the most decentralized entity of all in Bitcoin, have all the freedom to choose their miner, if they want to support their local one. Just include an additional payment to your local miner in your transactions....
@_date: 2015-07-08 21:39:11
Information content of your post is approaching zero...
@_date: 2015-07-10 09:37:11
Thanks for writing this out. You are absolutely correct. Very well written! One of the gems that usually only appears deeper in the discussion and when the submission long moved off the front page of ...
Unfortunately, there are a lot of people who are able to just see the code and nothing else. And they think Bitcoin stays safe when 'we do not touch it'. And 'the code is the law'.
Gives a lot of credit to the idea that narrow thinking might be high-IQ but not necessarily all there is to being *intelligent*.
recently wrote an [interesting reply]( to me further exploring this.
@_date: 2015-07-04 17:05:38


I am not doing that...
@_date: 2015-07-24 12:51:43
Network effect. Litecoin might take over only in a very unlikely scenario.
Especially when Bitcoin is staying crippled and takes the whole ecosystem with it...
@_date: 2015-07-23 21:28:59


Exactly. I completely agree with the general idea of not making too many options available that allows people to shoot themselves in the foot. No one really wants that, I guess, for almost all defaults in Bitcoin, people are absolutely OK with the devs making reasonable consensus decisions.
But not so with blocksize anymore. That's why I think making this single argument mandatory would work. One has to decide which fork to be on anyways. Better make it explicit. And the 'wars' regarding blocksize can then be fought elsewhere and devs can concentrate on other stuff again - the code will run fine with 1MB or with 8MB.
@_date: 2015-07-08 01:55:36




Again, you confuse a centrally enforced 1MB limit for *validating* blocks with a per node *relay policy*...


Not at all. What a node relays or node is solely in the node's discretion already. Do not mix it with the block size.  Block size affects validation of blocks, relay policy *does not*.


Longer validation means more time spent on unvalidated mining. This correlates with block size, yes. But it is unproblematic and to be expected from miners. Have a look at the recent, [nice explanation from Tier Nolan here.](


That's a wide assertion, completely unfounded. Who's asserting stuff that is just wrong? :-)


I gave you the explanation, but you are unwilling to understand it yet...
@_date: 2015-07-29 19:13:12
I think you want to grow Bitcoin as much as possible in this still early phase, and given that miners still mine a lot of fresh coins.
Eventually, the *total* amount of value collected from fees *will* increase. But it should rather be tiny fees per transaction with a huge transaction volume than large fees with a small transaction volume.
Because in the latter scenario, there's not really a point to use Bitcoin at all.
@_date: 2015-07-09 20:12:09
But this is actually only true when the block size cap is an *effective* limit on transaction rate. Not when the block size cap isn't in the way.
Only when you go from purely miner selected fees based on market conditions to an artificial rationing will increasing the blocksize cap reduce fees.
@_date: 2015-07-05 10:06:32
Note though that, unfortunately, the same factors keeping the whole society from becoming your libertarian paradise might also easily keep Bitcoin from staying one (e.g. through introducing centrally planned blocksize crippling).
@_date: 2015-07-08 02:56:38
Miners will not SPV mine orphaning blocks because those cost them a lot of money. What part is impossible to understand about this?
That they did means they got burned and will learn. As easy as that.
@_date: 2015-07-11 10:57:17
In a long term scenario, hash power concentration at one particular spot in a country might mean an easy target for (e.g. military or terrorist) attacks.
There are completely non-altruistic reasons why it might make sense to spread the hash power out a bit.
That everything necessarily centralizes together isn't a law of nature when considered from all angles.
@_date: 2015-07-24 17:37:16
LOL. You are quite the comedian and spinster. 
How about honestly admitting that your 'fake confirmation attack' is, well, fake?
@_date: 2015-07-08 11:31:14
... and maybe ask a mod what happened?
Maybe this is some automatic spam filter or something....
@_date: 2015-07-21 21:55:20
Very true, Bitcoin doesn't support infinite transactions. That's why there is a natural limit, which will create a natural price for a transaction which is truly greater than zero.
So... how about letting the thing scale until it reaches a natural limit instead of erecting artificial walls?
And BIP100 (without the 32MB-as-an-ought-to-be BS) or BIP101 would make sure that things progress slowly with safeguards and no single miner can do something completely insane...
@_date: 2015-07-30 08:48:44
Yes. Thank you. I made this very same argument before as well.
The 1MB-blockistas seem to have a serious case of tunnel vision.
Also, if you have &gt;tens of millions of people regularly using Bitcoin, you risk something similar to a revolution by messing with their property.
If full nodes are in different jurisdictions, that is a large decentralization effect already. I fail to see how Putin will get his way with Obama when he wants to censor transactions, for example, and I fail to see it the other way around, too.
@_date: 2015-07-09 09:02:41


The infinitely large block with zero fees will never happen because block size and transaction costs will run into *natural* limits before. That economy would be informed by actual, physical and technological limits, as it should and as it is intended.
@_date: 2015-07-08 18:39:42
Issue is the blocksize cap and that wallets do not have good fee estimation code yet.
The blocksize cap is completely ineffective in stopping attacks on the network and clearly needs to be lifted.
That would also make the attacks more expensive.
@_date: 2015-07-19 14:47:57
And I think - as others said - you should have worded your submission *much* more carefully. 
Your list of candidates is indeed interesting. I did a very simple bag-of-words model and maybe I should go compare the list of likely sock puppets I found to your estimates.
If one would want to dig further into this whole mess, at some point one probably cannot get around manually check users whether they are likely sockpuppets. I do not know whether it is actually worth it, though.
@_date: 2015-07-07 09:28:23


And therefore infinitely small fees are impossible as well, meaning that there is a natural *fee market*. Equilibrium. Boom, done. 
Satoshi's incentives work out. 
@_date: 2015-07-22 19:13:29
The more I think about this, the more I think this is accurate. Reminds me of [this comic.](
Some people want to screw around with the code apparently just for ego reasons?
@_date: 2015-07-10 08:48:33
We have to stop the attack from within Bitcoin first, IMO...
@_date: 2015-07-02 08:38:51
Yes. 51%. If Bitcoin is in mass-use worldwide, you'll have a competition for hash power, too. - And a strong incentive to not let any one country have 51%.
EDIT: And this projected fear of 51% of miner power comes up again and again. That's just the risk one has to accept with Bitcoin.
@_date: 2015-07-25 08:47:31
And what is wrong about my comment, please?
@_date: 2015-07-01 20:06:00
There is indeed a saturation in number of transactions per unit of time. Repeating a lie doesn't make it true.
@_date: 2015-07-09 12:25:20
Filling blocks is not the same as spamming the network. And a larger block to fill means more fees to pay for the miner.
Also, consider this: With a hard cap on blocksize, a spammer can freely spam the network, *and he can recycle those parts of his money that didn't confirm for another attack*. Freebies for the spammer!
When those parts confirm, he lost money at least.
@_date: 2015-08-09 21:57:32
FWIW, there is also some *'spam filter'* operating on comments apparently. [This is a conversation](  with the mods I had about a comment of mine getting deleted.
Furthermore, there is free killing of comments when the mods dislike them. The link to the comment where I was called an idiot is  as you can see from the conversation ***it had been very likely killed by the mods***, because it disappeared *just* after I compared my comment to the one calling me an idiot to the mod.
Now, can some mod reply and explain policy here, or is this comment going to be deleted as well? 
@_date: 2015-08-15 22:16:49
How's this attitude different from Mike's hypothetical checkpointing?
@_date: 2015-07-21 15:12:29
I am certain Facebook would have become 'full', too if they would have only ever accepted one thousand users, total.
And they would be dead now.
@_date: 2015-07-05 16:42:59
I see that reversible transactions have advantages - but I think this can be solved with scripts and third parties and locking money for a while.
Consider that in the real world (TM), transactions are also practically irreversible after a while.
In any case, I cannot imagine a way to design Bitcoin that would make it reversible and still without a centralized authority.
@_date: 2015-07-08 02:59:58
Throwing away money to run hashpower costs more than using the same hashpower to *get some valid Bitcoin* ....
@_date: 2015-07-06 11:28:38
Yes... but which isn't what happened in the recent orphaning, though. Thing went on for so long, miners could have easily checked the chain for validity.
They would have prevented losing money by having a regular full node reset their SPV miners as soon as they went nuts, so I think they'll implement this, as they do not want to lose money.
@_date: 2015-07-09 17:14:23
Blocksize cap is not the right tool to prevent UTXO growth, though. 'If you have a hammer, everything looks like a nail...'
If you look above, I argue that UTXO commitments and eventual coalescing of old UTXOs are the way to tackle that problem.
@_date: 2015-07-08 08:34:01
Downvoters, then please go and explain how the blockcap prevents the network from being spammed and bandwidth being used.
@_date: 2015-07-09 12:18:26


You probably mean raising the block size *limit*? That that would lower the fees is a completely unproven assertion.


Also, it allows for much larger single transactions that can cripple the processing power of nodes, a 8 MB transaction can take several minutes for a node to process, during which time it can do nothing else, it's like a DoS
And Gavin actually thought about this and will limit *transaction size* to 100kB per transaction in the hard fork...
@_date: 2015-07-10 09:22:47


Agreed, I rather see this as damage control. There is a schism now in Bitcoin - some like me want a large layer 0, some want to stack many small ones. We could at least agree to disagree.
In a 'friendly separation' scenario, Miners could merge-mine both coins.
 


This might certainly happen, but when we at least get the good will in the community to not *intentionally* hurt each other, I think it would be much easier for the wrong side to accept failure and move on.
@_date: 2015-07-21 21:32:21


And "no or a larger artificial block size cap == no fee market" needs a citation, too...
@_date: 2015-07-09 13:39:39
With UTXO commitments and downloading the chain from a couple months or years back, I get *a lot more than SPV-grade security*.
@_date: 2015-07-03 17:46:58
And, gladly, f2pool reversed the decision.
Miners apparently do care about the ecosystem and/or their reputation.
This is P.Todd playing his 'developer authority' card to mess with a miner and the users of Bitcoin, for no sane reason whatsoever.
@_date: 2015-07-06 23:23:12
If this is true, it means that keeping 1MB blocks for Bitcoin is keeping a proven attack vector ... 
@_date: 2015-07-01 11:35:57
Exactly. Conflicts of interest don't just go away because someone (conflicted!) says they do.
Excuse me for some [copy &amp; paste](  because I think this is very relevant to the situation and no one has yet explained how any different approach would be good or honest here:
Usually, if I have invested or am dependent on a company that would potentially profit from actions I do in another, unrelated position that is supposed to be independent, I *at least* openly and widely state that I am biased and that I have a conflict of interest.
The *correct* thing to do would be to remove oneself from either position (Blockstream or being a self-assigned authority on Bitcoin).
What we hear from the Blockstream guys is how Blockstream 'can't be evil' (apparently their company motto? LOL...) and how they all care more for Bitcoin than for their company and that they can be trusted very much. 
This approach of denial alone makes me very suspicious - because a clear admission of conflict of interest would need to be in order here. It would be *honest*.
And there needs to be a very honest and forthright admission, too, that Bitcoin with small block caps is a clear deviation from its original goal of wide scalability.
Instead, all this is hidden behind scare talk and supposed impossibilities. See also [this.](
@_date: 2015-07-05 11:30:35
The hacker attack could simply be mitigated by a high moving average safe limit that otherwise does not impact blocksize at all.
@_date: 2015-07-08 19:27:58


He'd pay a lot of fees and that's it. No more recycled money hitting the full nodes.


Point taken, but I didn't see any large complaints about full nodes being overloaded all over the place, so the majority must have been just fine....
Also I did not see any hiccups in services connected to full nodes.
And no one ever said that Bitcoin runs on a calculator with avian carrier IP...
The network will always have some diseased, broken, nonfunctional nodes.


LOLOLOL. So you are arguing for the natural transaction floor due to orphan rate now? Even less reason to have a blockcap :)
And if you eventually come around to liking larger transaction rates, IBLT might actually one day be implemented.... :-)
EDIT: Oh and to repeat it again: The network saw more than a HUNDRED times the rate of transactions that can currently be mined due to the blockcap. It saw more traffic than Paypal has on average.
@_date: 2015-08-15 20:33:30
Which is spot on, as always... thank you :)
@_date: 2015-07-08 02:16:46
It worries me. It worries me that the Blockstream people do not see this or do not want to see this. Either explanation is very worrying.
Gavin looks like the good guy after all. I think if he think's more about it, he'll convince himself that indeed the blocksize cap is a pretty half-assed installation.
@_date: 2015-07-01 23:33:34


This is a good point. Not just gold - Bitcoin directly competes against current, trusted settlement layers. A small blocksize makes it uncompetitive. Very interesting angle.
@_date: 2015-07-19 13:18:00
It actually shows you the opposite - the method to do the machine learning seems to be o.k. *by itself* - the GIGO comes in by trying to infer anything about the absolute fraction of sockpuppets.
This inference can be done manually by looking at his list of most probable *suspects* (assuming similar writing style, a reasonable assumptions): And the high number of clear false positives actually support the notion that sockpuppetry in the blocksize debate is not a big issue yet.
As you know, I am a strong supporter of a blocksize increase, as you are. Note that clear opponents of the increase post pretty much the same argument in this thread, so maybe we actually all have something to agree on after all :-)
@_date: 2015-07-19 15:46:14
Exactly. Full RBF can be introduced as optional, by flagging some transactions as being RBFable.
Merchants who think 0-conf is good enough can keep using it. They should know the risks, of course.
@_date: 2015-07-05 17:19:20
This is a damn good point and should be a separate submission on the front page.
I think and hope the wave that finally flushes the blockstreamers and blocklimiters away is going to be the MSM picking up on the story and broadly (and correctly!) titling 'Bitcoin cannot scale, it is just a toy 3txn/s system'.
But this will at least prevent the next growth from happening, probably even crash the price into the 100s and *then* people who want to cripple blocksize will hopefully, finally stay quiet.
@_date: 2015-07-29 20:40:46
I think regardless of whether Mike is actually pro black/red/white-lists, he's not going to go into this direction because he knows that he needs to get the big block fork done first.
And as far as I remember, he argued that his banning proposals were mostly just 'thinking loudly'. I was opposing those proposals back then and I still don't know whether to trust him that he now knows that destroying fungibility is a bad idea.
Gladly, I nor anyone else needs to trust him more than necessary to profit from his (and Gavin's, and the other's) work. This is all in the open.
@_date: 2015-07-24 16:15:01
Did he ever talk about pools? Because he could have meant every user mining only in the form of pools - and many, many might be doing exactly that. We cannot know for sure. Because he definitely didn't plan for everyone to have a full node:


-- [Satoshi Nakamoto, July 29, 2010](
@_date: 2015-07-30 08:04:25
The 1MB-blockistas would use *anything* against Mike Hearn that they can grasp at. So they'll be very watchful. Watching for bugs as well as nefarious things. Which will be good for everyone else involved.
@_date: 2015-07-22 14:13:10
The point is not that Satoshi is a holy person.
The point is that Satoshi invented the game including most of its rules. Some people now want to socially engineer (with pretty underhanded tactics, one might add) a pretty serious rule change mid game.
No need to call anyone a deity.
@_date: 2015-07-08 19:11:01


Huh? There would be a natural blocksize limit? The Chinese do not accept &gt;8MB with &gt;50% hashing power...


No way. You are trying to socially engineer the 1MB limit into an *ought-to-be* that was never intended this way....


The attack was/is *ongoing* for a while with the small blocksize we have now, creating filled mempools and backlogs. What the heck is instantaneous about it? 


Do you understand that creating extremely large blocks can be considered damage to the network ? It doesnt seem clear to me that you do.
Miners have an incentive to not produce any bloatblocks, and they would lose money from orphaning them. Which would be highly likely in case they produce them.
And a single bloat block won't kill us, it will at most slow processing down for a while(*). 
And a continued attack is ensured to not happen by the incentive structure of Bitcoin.
(*) - Of course, I am talking in a situation where TX-size is at max 100kB with a higher/no blockcap, as intended by Gavin...
@_date: 2015-07-22 08:54:18
If Bitcoin is for everyone, that means GB-sized blocks *eventually*.
Which is totally doable btw. - with bigger nodes in data centers - just as intended by Satoshi.
@_date: 2015-07-23 07:57:47
Arguably, the idea is that the user base has grown enough at the point in time when the transaction fees take over as the main source of income for the miners.
But that also means Bitcoin needs to grow a lot early on.
Which means that we should remove the damn blocksize cap.
@_date: 2015-07-11 22:13:33


Why so? Simply introduce a flag to mark transactions as full-RBFable and everyone can keep 0-conf, but the full-RBF people are happy, too...
@_date: 2015-07-08 16:57:47
Thanks for reposting this. The block size is pretty much irrelevant in the context of maximum P2P network load. The two have almost no relation to each other. 
I made a submission yesterday that created a lot of discussion of this topic, but for some reason didn't really reach any prominent place here. [See here](
@_date: 2015-07-08 13:57:17
As I said, I think there will always be dynamic consensus: The network isn't ever in a perfect state. There is always an odd full node misbehaving, dropping off the network, having a suddenly noisy DSL line, etc.
So there will always be nodes like that even with the current 1MB block size. This happens, all the time.
An uncapped blocksize would mean that it *could* happen to *more* nodes. But then you also have to look at incentives: If 51% of miners are rational, they have no interest of this happening. They want validated transactions to arrive at the edges, and they want unconfirmed transactions to gossip through to them. And we have to rely on 51% of the miners being interested in a healthy Bitcoin anyways...
Also note that the odd megablock does not cause a permanent disruption, and a temporary disruption will clue all interested parties in on how to proceed from thereon.
The most extreme example, a &gt;51% miner hashing a petabyte block, would also be meaningless to the rest of the network. It would just sit somewhere and do nothing.
If you wanted to send it around, you'd currently need a high-bandwidth truck full of HDDs. [Funny how analogies to gold transfers come up here :)]
In other words, the orphan cost would be astronomical.
Bitcoin not only relies on the longest valid proof of work, it relies on the longest valid proof of work that can be transmitted through the internet, and the always existing incentive of avoiding orphan cost makes it work well, while potentially approaching the limits of technology - if it would not have a hard blocksize limit.
In other words: Accepting the idea of an orphan cost existing  also means that *inherently* there is indeed a strong incentive for a miner to get his block to all interested parties.
@_date: 2015-07-24 11:22:59


Yes, because it is completely irrelevant. Also, it is not true: Transactions can be processed in parallel.


Again, the only time limit for eventual consistency is 600s. As I said, that's corresponding to a sphere extending about to the sun.


And how can it be exploited to stress test indefinitely?
@_date: 2015-07-24 12:49:57
'will have to' is too strong, we could run solely on proof of work longest and UTXO commitments eventually. 
But I guess 'will', as in it is going to happen is correct. But emphasis on *some*. And people can pay for the services of archival nodes.
@_date: 2015-07-02 14:26:42
Yes I understand that. We were talking past each other: My POV was from a single miner's opinion on the size of his blocks.
XT will be scrapped when it will be long term inert and LN somehow creates something viable on top of 1MB. Very unlikely IMO.
@_date: 2015-07-09 20:09:16


If the blocks are indeed filled and a bigger size gives relief, they probably will. But OTOH, you could also as well have a larger adoption rate with higher blocks.


Agreed. The whole network spamming showed though that the network is able to handle &gt;400 txn/s - and that the blocksize is wholly ineffective at dealing with spam. This was a red herring placed into the discussion that urgently needed to be pointed out.
@_date: 2015-07-05 18:09:28






And I have given you a very clear reason why SPV mining for several blocks is not going to fly long term, for very rational reasons for the miners, and in very clear terms. They've burned themselves, and they'll learn. No need for you to evade here. EOD.
@_date: 2015-07-22 08:33:37
It is insane hubris IMO, to think that Bitcoin will sustain itself on 1MB blocks and $10 fees.
Especially when every other altcoin can come along and do better - heck even Litecoin has effectively 4MB blocks. And no, I am not a buttcoiner, just pointing out what is.
@_date: 2015-07-30 08:01:19
With the obvious difference that transactions are the intended use case for Bitcoin, giving it *value*. Sheesh.
@_date: 2015-07-09 21:27:12


Thanks for that link. The results are extremely positive for high block size caps in any case (we are not talking about 320MB!), so why do you put it into such a negative light?
And nodes in datacenters are not really that centralized. A single node in the US is.
But 10 nodes on each continent? Pretty decentralized and hard to censor. Because the Russians will not really take input on how to run a node from the Americans ... and vice versa.
@_date: 2015-07-05 16:13:28
True. A singler miner might say 'fuck it, I am going 0-txn blocks', though. Some apparently do exactly that.
If enough miners do that, transaction fees will rise high enough so that miners will have enough incentive to include transactions again.
Equilibrium. Transaction market. Boom. Done. Satoshi was far ahead of the curve...
Now please lets increase the limit as per Gavin's schedule... :D
@_date: 2015-07-03 19:37:33
No one said that 0conf is risk free. Again, you are thinking in black and white. Merchants can easily see whether out of 100, 10 transactions are failing, or just 1 or even less. Then can and will adjust accordingly.
Solution for implementing RBF is to make it per transaction optional so 0conf keeps on working.
@_date: 2015-07-08 20:53:27


Honest question: Has this been observed to persist in reality? I mean, the scenario would be that a miner produces a big block, it creates a chain fork for a while, but *eventually* the dust settles and one of the chains wins. Of course, there might be pathological behavior like fragments of full nodes flying off the network and clustering together to build their own small world of their own Bitcoin variant. Temporarily! But any scenario I can come up with, I fail to see one where there won't be consensus on one big chain eventually.
And all those scenarios can also potentially happen with network disruptions and a hard block cap as well.
And this disruption will immediately impact the bottom line of the miners so they have a good reason to stay clear of it!
It would mean that miners would be able to test the theoretical, technological *and* economical limits of Bitcoin, and adjust their rates accordingly.
I am not saying this is in any way *nice* in behavior and it might thus make sense to have purely a *safety* block size cap.
But I am not convinced that we have to be extremely fearful about this. Bitcoin is forking and orphaning often, after all. It is *designed* to survive that.
@_date: 2015-07-12 09:54:26
*You* said religious text...
@_date: 2015-07-05 14:47:53


What soft blocksize cap soft fork?




You missed my point: If they would do SPV mining for each block in advance but have a full node to correct them long term, they would avoid losing those 150BTC.
There is no real problem with SPV mining one block, there is one with SPV mining many blocks.
@_date: 2015-07-08 01:34:57
Yes, but they cannot censor transactions from entering the network and causing corresponding ressource usage. Exactly what happened during the recent stress tests/attacks.
Block size cap only deals with half of the problem. Making it only twice as expensive for an attacker by byte of bandwidth consumed, which is nothing for someone determined. It is not preventing anything. You are thinking about the long term rate of honest, in-block transactions, they are not of interest to an attacker.
***We have been riding with the training wheels off all along ... and we are fine!***
@_date: 2015-07-08 06:18:45
We're talking about resources here.
The hard cap doesn't do anything except 2x price / byte against the bandwidth resources that can be attacked... 
Whether confirmed or unconfirmed doesn't matter at all.
@_date: 2015-07-09 21:22:32
Yeah. Consensus would be nice but I am inching towards maybe just doing a clean separation?
In that case, we could at least try out both ways of achieving success.
If we would agree on planning this and put it into a positive light, the amount of damage it might cause to the Bitcoin brand might be controllable?
@_date: 2015-07-05 22:48:38
If it is a teardown due to correct technical reasons, that is ok. He's an intelligent fellow, after all.
However, most of his arguments in the blocksize debate are unfortunately not of that kind...
@_date: 2015-07-04 19:51:55
And the most ridiculous thing is that people come out now saying, ok, 1MB is too low, but 32MB is the intended limit!1!
There is no intended limit for Bitcoin.
@_date: 2015-07-21 12:55:52
I am very much pro-increase, [I do see a problem]( with the 75% miner majority now, and what I simply do not like is the political language around the 32MB limit.
The language should be very careful in the proposal to prevent elevating the 32MB to an *ought-to-be*.
 
Because else, we'll have the same stupid discussion again at that limit.
Other than that, BIP100 would be fine and a nice low pass filter on blocksize limits, giving everyone time to watch any changes in this regard.
@_date: 2015-07-08 01:47:18
Listen again: That problem exists right now!
EXACTLY this has been tested with the current setup. Transactions are VALIDATED before they are RELAYED.
The network did most of the work already. Only thing it didn't do is putting those transactions into blocks and on disk! (And receiving the transactions twice for this, once as a loose unconfirmed transaction (that gets validated before relaying!), and once in the block) that's why I said blocksize cap makes it twice as expensive at most)
Really. Think about it once more. I was confused all the time, too.
@_date: 2015-07-01 10:55:40
Yes, they do have made many valid contributions and improvements to Bitcoin.
But they are now intending to change the course of Bitcoin (from large layer0 to small layer0, all for a diffuse centralization scare) and *they are not forthright about it*. They are in a situation to potentially profit from a crippled Bitcoin and are *not forthright about it*.
And all their contributions still don't change the picture that the incentive system and overall architecture of Bitcoin is still to &gt;90% Satoshi's part. Bitcoin uses Adam's hash cash as one of its parts. But that doesn't mean Bitcoin is Adam's invention and he can steer it as he likes. Not by far.
@_date: 2015-07-31 10:45:57


Gradual being what? Continuous? Exponentials are continuous.
In that case you'd have a point: The doubling is discontinuous.
Not that it matters at all.
@_date: 2015-07-02 21:50:41
Does it? I never understood Peter's convoluted explanation of tree chains and I have not seen anyone saying he gets it, either?
I am thinking about just storing the root hash of a bunch of merkelized UTXOs after they are all a decade old or so.
Would also put the burden of long term storage back to the users, where it belongs. 
@_date: 2015-07-15 21:54:54
Downvoters ... look at the damn LN paper/presentation yourself: You have to lock in money before you can use it to pay. LN are an extension of the payment channel concept.
And an early, well-known implementation of the latter was done by one of the evil blocksize increase people, Mike Hearn.
We 'blockraisers' do not oppose LN, we oppose crippling Bitcoin to force it on top.
@_date: 2015-07-17 08:47:49


The cap was completely ineffective against preventing the recent network flooding attacks.
Otherwise agreed with your post.
@_date: 2015-07-05 14:51:50
I agree there are lots of (potential) problems, as Gavin says... Bitcoin is still an experiment.
On terms of error correction capability, do you mean reversal of transactions?
If so, I disagree with you there: Cash has no reversibility either.
EDIT: And with regards to the last point: With CHECKLOCKTIMEVERIFY and similar, one should in principle be able to build somewhat reversible transactions: I think I should be able to put in my transactions with a time-lock and name a third party trusted by both the sender and receiver of coins as someone who could revert the transaction for a while.
@_date: 2015-07-31 19:34:23
Ok, I see. Well, having a per-node relay police is going a lot further than I ever intended here... :D
@_date: 2015-07-06 23:15:17
Raising fees through artificial, centrally planned scarcity are about the most stupid idea coming from the blocklimiters, and this idea might apparently gain enough traction to kill Bitcoin.
@_date: 2015-07-22 14:17:35
2MB. Too little, too late. And that's about the max the BS guys want to support.
@_date: 2015-07-07 15:28:08
A 3txn/s toycoin system won't be used by anyone and is easy to shut down in any jurisdiction - as no government has anything to lose really.
A 30000txn/s system (that figure happening only *eventually*, of course!) with a couple tens of nodes of which some are in Russia, some in China, some in different parts of Europe and some in the U.S., as well as some elsewhere will be much harder to shut down. You would have a large user base that will not like interference, and you would have (almost) opposing jurisdictions competing for providing neutral Bitcoin access.
Assuming worldwide usage, a single country, lets say the U.S.,  would need to cut itself off the Internet completely to prevent their own people from transacting in Bitcoin - because they could otherwise always reach that full node and their coins in Russia. And enforcing artificial barriers on the full nodes in the U.S. would only mean that they get ejected out of consensus.
This is the scenario that Satoshi planned and foresaw, with, yes, bigger full nodes in data centers, and I think it indeed makes the most sense, also from a decentralization and censorship point of view.
@_date: 2015-07-10 14:36:23


What do you fear more: A hardfork battle or a clean divorce?
There would still be time to make it an orderly breakup and not a breakup fighting.
@_date: 2015-07-21 22:14:21


There is a) variation/spikiness in transaction volume that needs to be accounted for and b) this rate is probably adjusted by some 'spam' definition of yours...(?)
@_date: 2015-07-02 20:56:24
I mostly agree. Moving averages are indeed problematic. Someone on the ML I think said nicely: They care about the past, but not the future. Problem as always: No one knows the future. Well, maybe except things like demand for xmas/black friday shopping can be planned.
With regards to the size of the chain: Couldn't we seed pruned chains from the nodes? They can be validated and then the only problem that remains is the UTXO growth (which can and probably should eventually be addressed through UTXO coalescing or similar, but that is another whole discussion).
@_date: 2015-07-08 03:45:41
Yeah, so? I am pro removing the blockcap altogether now, as you might see...?
@_date: 2015-07-08 21:21:18
And I think you are actually right. Blocksize is a red herring there, too.
It gives a feeling of comfort. And that is probably all it does.
I might agree that it helps social help to give people some feeling of comfort. But I disagree on crippling Bitcoin's potentially awesome capabilities to please people who are constantly in fear anyways.
So this is in way the cowboy/punk/wild-west attitude to the blocksize debate: Whatever, eventually, the network *will* converge!
 I think it is also important to keep this perspective in all discussions, and to keep the "Blocksize cap [Overton window]( that large into this direction.
Because there is a tendency that IMO can be widely seen with some Bitcoiner's becoming being afraid of everything and all changes regarding scaling, clinging to blockcaps as keeping one safe from all woes that might happen to the network. Always shifting the scope of discussion to more 'security through blockcapping'. How many 'oh worry this, oh worry that' posts have I seen lately....
And that is fundamentally anti-Bitcoin and if this additional safeguard mindset overwhelms, it might be *very* damaging.
If we get purely safeguard block sizes, fine, but I think the cowboy attitude should be present in discussions at all times to keep Bitcoin moving forward.
@_date: 2015-07-09 21:37:54


I think this is seeing it too narrow, code-based. We ae also competing against altcoins and other payment systems. There are economic factors, too. Keeping 3txn/s for too long can make Bitcoin irrelevant...


I am not that worried. First because of the caching, and secondly, if this is becoming a real problem, it can be fixed - make creating UTXOs more expensive with a hotfix, whatever.
@_date: 2015-07-22 08:56:14
.. and that done by a rogue, 'experimenting' miner.
Now miners should have more of a clue. Which isn't to say they can't be hacked or something - but to prevent that, a high dynamic cap (10x average or so) as a pure safety limit could be implemented.
@_date: 2015-07-08 10:15:06
Why did you remove this?
@_date: 2015-08-16 09:26:43
Interesting point! I don't think the value accrued for Bitcoin is the value of the assets like bitcoin.org and though.
@_date: 2015-08-16 20:25:06
You say it all in the headline: Purportedly.
In other words, someone, certainly not Satoshi, spammed the mailing list. 
@_date: 2015-08-02 09:40:00
UTXO commitments are as good as full node security, for any realistic definition of security.
If you download all TXNs and UTXO hashes for a year back and you
end up on the wrong/invalid chain that means that the *whole network* conspired to lie to you for a year and build invalid blocks for this time.
In that case, Bitcoin can be considered dead since about a year. So arguably, UTXO commitments are a possible solution to blockchain storage growth - and with coalescing also to growth and size of the UTXO set. 
Bandwidth as an issue remains. Some people have GBit/s links to the home today...
Lets scale this damn thing up.
@_date: 2015-08-11 14:32:28
I think it would stay 'no limit'.
There might have been the odd megablock (mind you, one that also didn't get orphaned, so a GB-sized block is exceedingly unlikely).
Those blocks would just sit on the chain and do no harm. Because miners would have probably gotten a clue that bloating the chain costs them money in terms of orphan cost (see also what writes...) in addition to hurting the network, we'd have about the same situation as we do today, except without a 1MB limit.
Price in $/BTC would probably be *much higher* without that roadblock and accompanying uncertainty.
@_date: 2015-07-05 10:43:46


So they indeed have a strong incentive to validate the chain. And at least have a full node immediately resetting their invalid SPV chain if that happens.
Shows that incentives also align for 'SPV mining'.
I think it is simply an experience to be learned from for the miners. They make a mistake, get burned, and get a clue, done.
@_date: 2015-07-01 10:59:06
Look at what wrote. He has some very good ideas on paying for full node access.
Which would solve the whole debate altogether and could let Bitcoin itself scale with a completely open cap.
And even if the market for full nodes is not as close to perfection as it could be yet, if you have Bitcoin, if you want to participate, there is already an incentive to run a full node.
@_date: 2015-08-15 20:10:43
Core is deadlocked. Anything that is not 1MB will be an alt-coin to you mods now? What is your way forward?
@_date: 2015-08-16 19:55:48
That, and the results from show that economic blocks are limited *anyways*.
BIP101/XT, get core to follow, and be done.
Hopefully with a fresh set of people and less shenanigans.
@_date: 2015-07-07 23:05:13
@_date: 2015-07-08 03:03:37
Aha? I don't think so. In any case, you failed to show them. Instead, a really diffuse argument like this one:


Without being specific. 
@_date: 2015-07-19 14:36:16


So you want to compare to a baseline? Well, I think this is a matter of perspective: If we'd somehow be able to see a 50% fraction of all posts from sockpuppets in here, we would probably both conclude it is a *high* fraction. Or would you disagree?
If most of the candidates for sock puppets do not withstand a manual inspection (are actually separate users) and the classifier is assumed to have a low false negative rate, we can conclude that the sockpuppets aren't really that bad, or can't we?
Now, I concede that we might not have arrived at 'most' yet.
@_date: 2015-07-18 12:44:41


Does it? I don't see it... it just allows the spammer to recycle his money.... whereas completing blocks would miners make the spammer lose his money.


This was for several minutes. As far as I understood the methodology, it measured transaction flow at several independent endpoints. Because those transactions were validated, it is indeed a figure describing the Bitcoin network.


Putting the 'reddit-derp level' aside as some unwarranted arrogance, 8MB would be just about a tenth of the peak rate of 400txn/s - how about that?
@_date: 2015-07-24 08:19:19
Again, Bitcoin stores transactions on a replicated storage.
VISA and MC will do that all well to keep their transactions safe.


***This is very dishonest.*** Mining isn't only part of the payment network Bitcoin, Mining is part of the money supply. You are comparing VISA the payment network to Bitcoin, the payment network + money system.
@_date: 2015-07-11 11:48:17
No, he's asking to go back to plain old telephones with *just 3.7kHz* of true, analogue *bandwidth*, because digital peer-to-peer connections between people are just scary. People might *use* gigabit/s eventually, clogging up our tiny analogue lines!1! 
And we better go back in time when telephones were still a luxury and just the big, important 
people had telephones, because everyone having access to a telephone is equally scary.
The telephone lines between the important people are then enough to connect the big *hubs* and worldwide *centers* of important people, so they can make a *profit* selling stuff to the rest.
Sounds good! 
@_date: 2015-07-09 11:12:06
The complex problem of scalability has been tackled back in the day by Satoshi - and answered affirmatively.
[The hard cap helps nothing in keeping the spam off the network, only legitimate transactions.](
@_date: 2015-07-02 00:05:25
That's why pointed out the problem with a hard cap back in 2010 when it was introduced as a temporary anti-spam measure.
Without the hard cap, I think Bitcoin has few things to fear in terms of control - because if there is intentionally blocked consensus, there won't be anything holding the network up from operating efficiently. But Bitcoin running into the 1MB limit is Bitcoin not operating efficiently.
I think this hard fork is the last large change to Bitcoin. And the details of this will very much determine its course and will write history.
@_date: 2015-07-08 02:47:42
The thread is hovering at just 8 upvotes, was at 10. I wonder why that is?
@_date: 2015-07-24 17:33:44
I have never been opposing fees per se - just fees that are forced with an artificial hard cap, soviet planning style.


I am sure that this will happen. With UTXO commitments, there will only be a couple of archival nodes which probably will cost some money to query.
@_date: 2015-07-19 13:10:07
Read my [other comment]( - I spend some time doing my own analysis - that unfortunately got buried a bit. 
paid me 2.5BTC of his bounty for my analysis, and we are on opposite sides of the debate - I am strongly *for* a blocksize increase, and he's (strongly?) against. So I don't think that he's trying to stir up emotions here.
I found no clear signs for sockpuppets, and I think if you read the analysis here correctly, you'd find about the same. Because the machine learning approach used here can never be calibrated to predict the correct amount of sockpuppets due to lack of data. That is the dishonest part of this submission that I also called out. 
But instead, it can be used to arrive at an automatically narrowed-down list of possible *suspects* that can further be tested manually. 
... and, if you look at the testing that has been done here, no frequent posters have been identified to be sockpuppets - quite the contrary, the list contains many obvious false positives. This is giving a lot of support to the idea that sockpuppetry isn't really an issue yet in the blocksize debate.
Also note that this doesn't mean that sockpuppets do not exist - this means that they are not yet a *big factor* in the debate or that they are hiding *really well*. 
I personally want to add my anecdote that I have indeed seen a couple of 'new user' reddit trolls appear, seemingly (gut feeling) more often after the sock puppet bounty had been posted.
I think the reasonable approach is to use common sense when seeing new users around - and as I have showed in my analysis, the core of the blocksize debate actually happens between just a couple tens of people.
Does that make sense to you?
@_date: 2015-07-18 12:18:56
Again, you argue as if there are physical limits to what current's hardware could do in regards to transaction processing. Why don't we test those limits?
Ah right, you conflate physical limits with your vision of Bitcoin - and supposedly decentralization is damaged when all people on the world can use Bitcoin.
@_date: 2015-07-18 11:02:31


The blocksize cap does not help against filling up the mempool. The stress tests showed exactly that.
The stress tests also showed that the network has a peak capacity of over ***400txn/s*** .
What more arguments do you need for increasing the blocksize?
(And, yes, of course, blocksize will still be constrained by the miners)
@_date: 2015-07-31 16:48:18
Fake votes without a fake fork are meaningless.
@_date: 2015-07-02 14:29:17
The Chinese miners are agreeing to 8MB, and given that they have &gt;50% of hash power, they can constrain the block size however much they want, regardless of the actual hard cap on the blocksize. If more than 8MB shouldn't happen according to those miners, it won't.
They could go down to zero transactions accepted, stalling and crippling the network, killing Bitcoin.
They don't, though, because it is not in their interest to do so.
@_date: 2015-07-23 08:26:58
As in having to trust your bank using the 1MB settlement system?
Oh, it actually won't come to that, because 1MB-crippled Bitcoin will die well before that scenario is ever reached.
@_date: 2015-07-09 21:18:36
I think if that would be a problem, Gavin would do that in an instant - make it a ramp from 1MB to 8MB.
That worry isn't shared by any of the core devs as far as I can see (regardless of position on blocksize debate), and I don't share it either.
I would be fine with it, though. You could make just ask Gavin about this?
@_date: 2015-07-08 01:46:08
Lifting the blocksize cap fixes that.
@_date: 2015-07-18 22:46:05
It actually isn't - depending on the network layer one tries to maximize the activation of, it can/will produce learned features as the output - such as eyes, faces, etc...
@_date: 2015-07-24 08:13:47
At MIT, the group think can't be too strong: 
Wladimir van der Laan and Gavin Andresen are both employed by MIT.
BS does not compare favorably.
@_date: 2015-07-08 22:11:23
Well said! So in a way we are indeed in a very politicized system already...
@_date: 2015-07-24 18:46:11
Yes, but that is pretty much a useless tautology. Because the question remains: Is it the code, the rules, the ecosystem, the idea, the users, ...?
@_date: 2015-07-01 23:53:24
True, but consider that no one loses from the change if you make it compatible both ways.
Maybe there are really lots of people who want to lowball their fees and then bump as necessary. Weird, nothing I'd do, but I won't worry if they could - as long as transactions like that are flagged and the flag is considered by every full node.
In general, I agree with you though that there is lots of social engineering going on to make Bitcoin something which it was never intended to be.
@_date: 2015-07-04 19:38:35
Very good point actually. 
Also, I am not so sure about BIP100 anymore. It adds *a lot* of complexity and see also [this]( by Gavin's solution is the simplest of all of the solutions that would work - except or completely removing the blocksize cap.
@_date: 2015-07-24 09:46:34
Of course, there is *eventual* consistency.
Your speed of light argument is absolutely ridiculous. There can never be a global instant consistent state because it contradicts special relativity and causality.
Yeah, so?  This is not at all the point of argument.
The way you like to interpret the words 'propagating global state fully redundant decentralized network.' is absolutely ridiculous.
If you would be really pedantic, your 7.5txn/s would drop to *zero*.
@_date: 2015-07-07 14:47:56
Well, positive thing that can be said would be that some altcoins indeed have a higher blocksize?
I mean... they do slowly have a point now.
@_date: 2015-07-01 12:20:41
So what?
@_date: 2015-07-24 17:53:34


First of all, it is arguable whether those are 'fake' confirmations. What f2pool did was rather 'mistaken' confirmations.
That said, I am denying that incentives for their existence exist, yes. SPV mining without a full node for pulling back onto track is not profitable. For once, look up what Tier Nolan wrote on this issue. Thank you.
@_date: 2015-07-24 07:57:08
Care to elaborate?
Unless there's a 75% majority, XT nodes will just sit there, patiently processing the 1MB chain...
@_date: 2015-07-06 21:38:27
See the [other post by in here.]( The SPV problem can be solved. 
As said, paraphrasing: Take one problem, tackle it, take the next. Do not solve all problems at once.
And in the meantime, connecting to a couple of known to be good full nodes does solve it to a high degree already.
@_date: 2015-07-30 18:57:22
Way too conservative IMO. 8MB in 2030 is crippling it only very slightly less than 1MB forever.
Bigger full nodes in data centers are not a problem.
@_date: 2015-08-15 20:28:28
I read it as short for 'additional revenue'. what was your intent?
@_date: 2015-07-17 09:11:46


They can vote without a hard block cap, too...
@_date: 2015-08-11 09:34:29
To add to this list: The [O(n^2 ) BS and 'Bitcoin will ***saturate the internet***' scare]( kidding), and [psycho tricks]( regarding supposed inability to change blocksize. Add the circular reasoning regarding hard forks that was nicely pointed out yesterday, and maybe, if you are aware, a picture emerges about what the 1MB-blockistas really are up to.
And I am sure I one could largely extend this list of bad things.
@_date: 2015-08-15 20:07:02
I can go into my basement and change the blocksize limit to 100kB.
Explain how this is different!
@_date: 2015-07-08 02:23:50
Unproven assertion. That's all there is to say about this...
Oh actually, there is more: Didn't the mining centralization *decrease* a bit lately? With an *increase* in blocksize?
@_date: 2015-07-24 18:44:41
Huh? The point is about scammers, not your family members. At least I hope those are distinct sets ...
@_date: 2015-07-07 19:35:03
Gavin is playing it nice, I guess. It might effectively severe ties with the blocking core devs as soon as he releases the fork.
 I think he's preparing the release, though.
@_date: 2015-07-07 22:44:22
I believe in a *natural* blocksize cap, automatically progressing with technology. I am such a damn hippie :-)
Still better than one from the ministry for central blocksize planning and payment hub subsidies...
@_date: 2015-07-02 01:06:14
Arguably, blocksize is pretty closely related to fees that possible payment hubs (LN or otherwise) on top Bitcoin can collect.
Thus, having control over blocksize might mean having control over how much fees are collected for the miners on level0 (Bitcoin) vs. payment hubs (Lightning Network).
Someone who might profit from running payment hubs might want to adjust blocksize accordingly.
That is the conflict of interest.
@_date: 2015-07-30 08:27:29


They can set blocksize as low as they want. They don't need a hard cap to do that.
I bet, though, that in two years down the road after the successful XT fork, they'll be fine with 16MB, too.
@_date: 2015-07-05 15:23:44
I think what happened is that two Miners got burned, lost 150BTC and will change their implementation accordingly - such as running a regular, full node in parallel that will prevent any longer SPV-mined chain from forming. Because that is *a lot* cheaper than 75BTC/miner.
@_date: 2015-08-09 22:45:42
It is invite-only at the moment, though. That puts a heavy damper on it being picked up as an alternative to @_date: 2015-08-15 23:45:05
The claim of stalling cannot be proven as it is a negative. Is your skull especially thick today?
@_date: 2015-08-15 19:56:16
Which, as it has been pointed out, does not mean shit...
@_date: 2015-08-15 19:34:42
Ah, I see, thank you!
@_date: 2015-07-08 10:26:09
Thanks ... that's about what I thought you'd write.
After thinking more about this, I wrote a long post about how the blocksize cap is totally worthless to prevent network flooding [here.](
Though I feel this is only an older insight that got lost.
The blockcap only limits the rate of *confirmed transactions* in the network. The honest transactions. It does nothing about the spam that bogs down the full nodes.
It might even make it worse. Full nodes process a lot of transactions, but only few are included in a block. For those that aren't included in time, the attacker even gets his money back. 
Meaning the blockcap makes it easier for an attacker to cause load on all full nodes.
Which also unmasks the 'small blocksize for decentralization' argument: Because blockcaps *only affect the honest transaction rate*, but not the *spam rate* on full nodes, it directly means that wanting to cripple blocksize means wanting to cripple honest transactions..
@_date: 2015-07-09 20:32:50
Where is the contract that Gavin has with you to provide you with 1MB-limited node code forever?
ESPECIALLY when the social contract (look up what Satoshi wrote on blocksize early on) is clearly a Bitcoin envisioned to scale to gigatransactions/day?
@_date: 2015-07-01 23:38:39
It should also be noted that from a Miner POV with a long term interest in network health (ensured by his mining hardware only having value within Bitcoin, among other things), it is important to keep the network healthy and as inclusive as possible, too. If merchants drop off because their full nodes are not able to cope with the transactions anymore, the Miner will lose money from less transactions happening.
Granted this is not necessarily strongly influencing the actions of a single miner, but with something like Jeff Garzik's proposal, the miners will tend to vote for healthy block size.
And without that, they'll tend to do this anyways, maybe through a benevolent 51% cartel that would censor gigablocks that would hurt their revenue. 
@_date: 2015-07-08 19:50:16
Again - Due to the nature of the network, Lawyers have to adapt to Bitcoin and not the other way around. That would only happen in case Bitcoin becomes a failure.
@_date: 2015-07-05 20:29:12
Data centers are no problem, you definitely want them in many different jurisdictions, though...
It has always been hub and spokes between SPV and full nodes.
@_date: 2015-07-18 10:58:53
Great! So with Gavin's blocksize proposal combined with this, we can reach worldwide scalability?
Sounds awesome.
@_date: 2015-07-06 21:46:26
Bitcoin has orphans and thus forks all the time... just long ones are more uncommon.
@_date: 2015-07-03 09:49:41
And core devs are not agreeing on keeping the blocksize, either. Not at all. Some of the earliest devs, Mike and Gavin (and Gavin AFAIK is involved longer in Bitcoin than all Blockstream people), want an increase - and Gavin investigated whether it is possible - and showed it is.
A set of developers working for a company that could profit from fees shifting away from the miners towards upper layers due to small blocksize is blocking the increase. There simply *is* a conflict of interest. Adam wrote a [long post on conflict of interest]( I certainly appreciate it, but it doesn't really address the core of the issue - an open admission of this conflict would I think actually move the discussion forward. Any of his actual technical arguments get stronger as they can be seen as honest if he puts them right beside 'Yes, I would be profiting from this, too.'
Let's also remember that the guy who invented the whole system  - and Bitcoin is still to &gt;&gt;90% following his rules and ideas - saw no problem at all for Bitcoin to scale.  And I still fail to see what data arrived that suddenly shows this to be a problem now.
None of the people on the core team fundamentally changed any workings of the Bitcoin system yet. I am certainly grateful for a lot of bug fixes, performance improvements and so on.
But introducing the blocksize as a core economic variable in Bitcoin is altering its course.
I have no problem with people arguing for that (though still think they are severely misguided), I do have a problem of people hiding that political opinion behind seemingly technical arguments.
@_date: 2015-07-08 09:00:35
Correct - however, just one side of the equation. Miners need to (eventually) collect transactions to feed off from the rest of the network, too.
Something which is for some reason ignored all the time in the blocksize discussions.
@_date: 2015-07-05 20:52:26
You must have missed most of the debate here, then.
There are quite a few people (though I think still a small, but vocal minority) who argue all the time that a limit is necessary for economic reasons, for decentralization (political) reasons etc.
@_date: 2015-07-08 14:02:59
But incentives work against rational miners producing such blocks for any extended period of time.
Even if that is not convincing you, I have no problem having such a block limit just below theoretical capacity in place - assuming it is also updated with technological progress. (Wasn't that Gavin's proposal?)
Recent tests show that the limit is far below that theoretical capacity.
@_date: 2015-07-19 15:21:58


And a block size cap helps how? Those same entities said they do not want to process &gt;8MB blocks. So it won't be &gt;8MB anytime soon.
EDIT: And think about the scenario of a large userbase - with big startups betting on Bitcoin in the U.S./Silicon Valley, with a lot of users in the western world - there will be quite the incentive to even the playing field in terms of mining, lest the Chinese government can censor our transactions.
@_date: 2015-07-30 08:32:09
I am curious about this, too...
@_date: 2015-07-06 21:30:39
LOL, at the same time you are the guy black-and-white arguing for forcing RBF onto users... and now you tell me there are more variables to this comparatively simple picture?
I expect this behavior to be strictly temporary, until they have a setup in place that will make their SPV miners reset. If they continue this for months or years, you might have a case. So, wait and see if you want.
And this isn't even needing complex (and too narrow) game theory to reason about it, this is just a sound business decision. Running an alerting full node costs less than having these stacked orphans every so often. Because the three orphaned blocks cost some 75++ BTC  alone...
@_date: 2015-07-24 16:20:36
If so, they miners still need to sync with each other. As long as that happens with them being in many different jurisdictions, I am not so worried.
Also, this wasn't meant as an exclusive source of incentive for full nodes. There are other incentives to run a full node too (security, work on full block chain data etc.) and the miners still have an incentive to sync with them.
@_date: 2015-07-29 20:22:35
And you don't need git for Bitcoin.
@_date: 2015-07-08 10:40:28
What is going on? Why is the content of the post removed? I hope this is just a reddit glitch?
@_date: 2015-07-07 19:28:51


Can you explain that further? What is his PR blitz?
@_date: 2015-07-19 14:11:35
Yes, agreed on both points of your points above.
The silent assumption I made here is that his machine learning is actually good at spotting people with similar writing styles and that sock puppets have similar writing style. And that only sock puppets have that property.
And then the whole question comes in what sock puppets actually are... is someone with a work and a home account sock puppeting, or not?
@_date: 2015-07-03 17:41:27


Fine. Make it optional by flagging transactions to be RBFable.
No need to force this onto users.
@_date: 2015-07-08 11:08:56
Is there a way for you to prove that you didn't kill your own submission? Is there a way for the mods to see whether you or they killed it?
If you can make a timestamped screenshot of you still being able to see your own submission, I guess that might be helpful?
Again, I do not understand what is going on beneath the surface at reddit so much...
And don't forget to submit the SHA256 of your screencap to a timestamping service...
@_date: 2015-07-21 22:23:29
And Gavin/Mike going forward with XT ...
From what I read here, I can well imagine Jeff getting annoyed enough to eventually outright back Gavin, too.
@_date: 2015-07-01 11:39:16
No. What you are seeing are for example a lot of people who have strong incentives to protect the value of their investment/Bitcoin.
This will cause a lot of people to be very vocal here. And there is nothing wrong with that. Owning Bitcoin is not conflicting with furthering Bitcoin's goals, quite the contrary.
I am pretty sure a proof of stake vote would also show *a lot* of support for an increased blocksize and a Bitcoin following Satoshi's original vision.
@_date: 2015-07-08 02:25:26
Very much so! We basically all got hung up on Blocksize == actually helps for safety.
That this isn't really true after all needs some insight.
It only helped early on somewhat in not making a GB-sized blockchain right away. That time is LONG gone.
@_date: 2015-07-24 10:48:10


But full nodes have no incentive to forward those and miners no incentive to mine those...
Even without a blocksize cap, there is simply always a natural transaction fee floor - no need to have a central comittee for blockcrippling in effect.
EDIT: transaction floor -&gt; transaction fee floor
@_date: 2015-07-09 11:31:50
And that doesn't remove the spam from the network...
@_date: 2015-07-04 09:48:10
Merchants are not stupid. No merchant will accept 0conf car sales. Don't pretend it is like that. 
@_date: 2015-07-30 08:59:25
Exactly. Have fun trying to operate a Bitcoin node in North Korea.
The best that can happen is semi-open governments like we have in the west to warm up to Bitcoin, and be eventually pressured to compete with other governments to be more honest in their dealings with money/the money supply, for example.
It could also have the positive effect of shifting the current focus of anti-terror/security-BS , together with insanities like civil forfeiture and the authoritarian attitude and overreach that it entails etc. back to more honest police work.
But for that to happen, Bitcoin needs to be successful, to be *used* first. That won't happen with 1MB blocks.
@_date: 2015-07-14 09:48:41
I can rather see a scenario where *lots* of people small really tiny transaction fees than a scenario where an artificially restricted blocksize creates a fee market.
There are *natural* reasons why blocksize is limited (orphan cost, cost for miners to acquire transactions/cost of relay of fresh ones).
Those are technological and physical in nature. I rather want to let the free market evolve to find the optimum there. If it means there will be layers on top of Bitcoin to exploit clustering of transactions (e.g. due to geography) or other patterns and Bitcoin having only a medium blocksize, so be it.
But I don't think it is at all wise in any way to artificially, centrally plan a blocksize limit. Also in the long term. It is *also* a deviation from Bitcoin's original goals.
Have a look what writes [here](  about these topics. He's a guy who understands the technical side as well as the economical side of Bitcoin well and is much more eloquent than I am.
@_date: 2015-07-06 21:02:55
Ok, I'll try:
Mining means running your hardware trying to win the always ongoing lottery of finding the next hash that fits below the difficulty target.
A miner can in principle, of course, do anything with his hashpower, but because only the Bitcoin network (+ ecosystem) will reward him tokens (Bitcoins) for running his hardware according to the Bitcoin protocol, he has a big incentive to do this.
There is a longest, valid chain in Bitcoin, meaning with the most amount of cumulative hashpower that went into it and following the rules for transactions on the Bitcoin network. 
This chain can be checked to be the longest by just looking at the block headers. However, in addition to being the longest chain, it also needs to be *valid* in the sense that the transactions that are linked into it are fitting all kinds of criteria (do not create money from nothing, for example), to be accepted by the rest of the Bitcoin network.
By just looking at just the block headers, you cannot be sure that it also belongs to the longest, valid chain. But doing this verification of all the transactions takes some time.
So some miners take a shortcut: They start running their mining on just the headers and assume that the network fed them a fully valid block, meaning all transactions in the block are valid, too.
This *SPV mining*, so far, isn't really a problem. Because if they have a full node in parallel, they can let this full node check the transactions in parallel and, when done, alert the mining hardware that it should work on another block (the one that is actually valid) instead of one that actually has invalid transactions in them. If they spit out one invalid SPV block, that one would be rejected immediately by the rest of the network, because it is also very short (no transactions in it).
Now, BIP66, which changed some details of the protocol to make it overall behave better and more predictable,  was accepted by the majority of the miners (and also full nodes), thus now making it the enforced default on the network. But this also means that old software that isn't aware of BIP66 will create blocks that are invalid according to BIP66 and will be rejected.
And a couple of miners did not update. And they produced an old, invalid block.
But then, miners doing SPV mining started to build on top of this old block. Because they only saw that it is the longest chain, but didn't see whether it is valid.
 SPV miner a) created a block on top of the invalid block.  And then, another miner b) went and build on the invalid (but hashpower-wise longest) block once more. And so forth.
And this is where it broke down, and could have been done better by the SPV miners. Instead of having a full node that verifies in parallel, and alerts their SPV mining part that the longest chain isn't the longest *valid* chain, they did nothing! They simply didn't run such a full node at all.
Meaning they created a chain fork, a couple of blocks on the Bitcoin chain that were the longest, though invalid and rejected by the rest of the network. Because the rest of the network had the larger amount of total hashing power (&gt;50%), it eventually build a *valid and longest* chain overtaking the corrupted one. Gladly. Else, it would have been an (inadvertent) 50% 'attack', that would have crippled the network considerably.
But the above also means that the work that went into the corrupt one, the block rewards of 25BTC + all the fees from the transactions that those miners included, are lost to them, as this wrong fork was orphaned and replaced with the valid chain.
And this is where they'll eventually figure out that just running a full node doing a quick alert and reset of their mining hardware will save them from  wasting a lot of money.
Does that make sense?
@_date: 2015-07-31 18:39:44
Problem is that from jgarzik's [twitter feed]( one can infer that this was inserted on purpose and is not an accident, seemingly due to the blockstream guys...
So we are still deadlocked. Waiting for XT here.
@_date: 2015-07-31 09:45:47
Ok. I do understand all the worries you brought forward here.
But you (and the core devs) seem to take on the *responsibility and the power*  to make the decision for the user.
And there is an impedance mismatch. Which you can feel right now with all the contention that the blocksize issue creates.
Part of my argument is that blocksize is very much a ***political issue***.
By making it the clear responsibility and power of the user to set blocksize and create a functioning network, you would be giving up this responsibility and  power. And power and responsibility belong to the user. Because we are building a *decentralized* network, or aren't we?
It would just make the situation crystal clear and honest and would hopefully also move the fights about blocksize elsewhere, at least out of the hands of the core devs.
One further thing:


This is why I argue against any defaults. 
Make it an empty box in the QT client. Make it a mandatory command line argument for bitcoind.
@_date: 2015-07-30 14:05:01
It does not make view invalid, or your point valid either. Whether we'll have a trustless money system like Bitcoin that is successfully scaled up can only be found out experimentally.
Or you chicken out before and destroy any chance of it becoming anything...
@_date: 2015-07-25 08:16:23
Yes, sure, correct, the miner has to figure out what consensus he wants to build on. But's that supposedly just a tiny fraction of node operators due to all the mining concentration...
@_date: 2015-07-08 08:29:07
And if you read further in this thread, the blocksize does not prevent the network from being spammed.
Else we wouldn't even see these rates.
The blockcap is a red herring in terms of potential throughput of the network.
We always rode with the training wheels off anyways!
@_date: 2015-07-24 10:02:44
It was the answer to 'recorded permanently'.
That said: UTXO commitments as the next step. And/or SNARKs, should they become workable.
@_date: 2015-07-01 11:08:40
Conflict of interest, egos, group think, miner distrust, ...?
@_date: 2015-07-09 13:55:55
But arbitrarily close, same in the limit. 
If you'd take the UTXO hash from a year back and all transactions on top, you'd need to have the *whole worldwide network* lying to you, for a *full year*.
And that is all besides the point: If *you* want an archival node, *you* can pay for it. The rest doesn't need it or pays archival nodes for service on selected transaction data. 
UTXO commitments would simply allow a much wider scale of resources one can choose from when running a full node.
@_date: 2015-07-10 10:58:11
It looks like a real schism, though. I tend to think it might be better that we plan on having a clean divorce than a messy hardfork fight in January?
We could all agree to disagree, split up our shared assets (websites, repos etc.) and work on making sure that the world knows that there are two Bitcoins soon, Bitcoin/QT and Bitcoin/XT?
Of course there is a certain damage. But wouldn't this be better than spending our energies in a fight that cannot be avoided other than through a clean separation?
@_date: 2015-07-11 11:23:54
I think americanpegasus is missing the /s ... :-)
@_date: 2015-07-06 19:48:22
Which just means that a natural floor for the transaction fees exists, so a real fee market!
Because if the fee of a transaction is high enough, it will at some point overcome the risk of orphaning the slightly larger block.
The current affair shows that Bitcoin's incentives nicely work out and that we do not need a blocksize hard cap. 
And without a hard cap, there's no blocksize wall to be hit. Meaning that fees increase in a much more benign way when the network is loaded. And it gives people the option to upgrade the hardware for higher transaction rate (more fees), should the need arise...
@_date: 2015-07-19 16:07:33
Yes. But that should be for the affected businesses to decide!
When LN eventually becomes usable (and it is only at most an early prototype now...) and it turns out to be a great way to do 0-conf-like transactions *and* basically no one uses 0-conf anymore, I might agree that it is good to turn it off.
But that time isn't now.
@_date: 2015-07-24 08:24:32


Agreed with understanding the workings, I am not at all sure about him understanding the *purpose*.
Satoshi had a *very* different goal in mind with Bitcoin than a 1MB-crippled one.
@_date: 2015-08-16 00:16:49
That's exactly what I do, too. You are on my list :)
@_date: 2015-07-22 15:01:05


Neither one but you straw-man.




And here. Bitcoin with big blocks is a very different architecture to Ripple.
Also, another straw man in here: I am fine with Gavin's schedule. That will likely keep Bitcoin within reach of the dedicated hobbyist. Which isn't even needed for it to work in a decentralized way. 
@_date: 2015-07-09 23:49:36
If it is insignificant, it surely is *very loud*.
So if you are right, arguing for a divorce scenario might be the fastest way to silence that camp?
And if it is isn't, we might avoid some of the bad PR of an unplanned hard fork and can manage the situation better and present the true situation better - that of people having a choice between two Bitcoins then.
@_date: 2015-07-10 19:57:30
The 1MB blockcap opened an attack vector - simple as that.
@_date: 2015-07-14 09:13:04
... because in your opinion 0-conf is not working at all today....
... yet people are using it...
I guess someone wants to bend reality into whatever fits the argument... :-)
@_date: 2015-07-23 08:25:28
^ This. Thank you.
@_date: 2015-08-15 21:05:02
mining == only reliable way of voting in Bitcoin space. And adoption will be pro-XT as well. Look up what the big players say on this issue...
@_date: 2015-08-18 11:50:24
And how to sign messages with Bitcoin keys :-)
@_date: 2015-07-08 03:08:15
Didn't help, only got 8 upvotes, despite 100+ comments.
Ok, at least half of them from myself, but still..
@_date: 2015-07-09 20:17:43
I agree that we should be cautious - but I also think we need to be cautious not to leave the field to altcoins or other payment systems.
The 3txn/s limit and the contention about increasing it is holding Bitcoin back.
I bet that if there is ever a clear consensus that it will rise open-ended (BIP101 or BIP100 w/o 32MB political crap), you'll see *quite* the influx of people and money into Bitcoin.
@_date: 2015-08-11 17:21:40
If one party disagrees, is it controversial? What's the limit?
You will *never* get all parties to agree on anything. There are people who think 1MB is too large of a blocksize limit!
@_date: 2015-08-22 11:58:46
Redecentralization of decision making.
@_date: 2015-08-17 22:23:37


How so?


Can you explain this further? I do not understand who pays what in this 'coin reward per block'.
@_date: 2015-08-27 19:02:32
So how about having no cap at all?
@_date: 2015-07-02 08:48:50
The argument for not prescribing a hard cap?
@_date: 2015-07-02 14:22:48
If a miner doesn't want a block larger than 1MB, he doesn't need to produce it...
@_date: 2015-07-01 12:26:45
As it has been pointed out elsewhere repeatedly, pushing a small size capped (just as Adam is proposing) Bitcoin is a clear deviation from what Satoshi intended Bitcoin to be.
And this push is not declared honestly as a clear change in course.
And people are upset about that. Rightly so. 
@_date: 2015-07-07 08:56:51
Yeah, but they had a lot of orphaned work - what makes you think they'll not get a clue from that?
@_date: 2015-07-19 11:53:16
... and this in turn gives a lot of support for the assumption that sockpuppets are not a big deal here!
@_date: 2015-07-21 12:49:54
I definitely do not like that BIP100 elevates 32MB to an ought-to-be. The language should be clear here: It is just a technical limit, eventually to be lifted.
To those telling me it is 'meaningful' in the sense of 'meant to be': Which one is it, then, the 1MB limit or the 32MB limit? Do you see the ridiculousness of this argument?
@_date: 2015-07-03 17:44:10
One can really see where you guys come from by wanting to force users a) into the block cap and b) into full RBF. 
Make that damn thing optional with a flag and everyone is happy.
As points out, it works well enough.
And apparently, mining pools even care about their reputation.
@_date: 2015-07-11 11:02:11
Satoshi had said everything there is to say about scalability.
All that came after that is derailing FUD and trying to parasitically attach to Bitcoin to *forcibly* extract some money from layers on top. No fundamental new data showing that there is any fundamental technological or physical limitation.
 
I have no qualms whatsoever with Lightning *existing* and I might well use it if it solves the micropayment issue, for example.
But *forcing* LN on top of Bitcoin - which, yes, can scale to worldwide usage with today's hardware (with full nodes in big data centers, yes) - that just stinks.
And it isn't even as if the block-increase camp is recklessly wanting to make everything large and big immediately. Gavin's well thought-out schedule would with a high likelihood ensure accessibility of full nodes to hobbyists until worldwide adoption has been reached.
And he compromised a lot already.
@_date: 2015-07-30 14:32:50
Very true.
@_date: 2015-07-07 09:01:56
Usually, you use SPV mining to squeeze the last bit of hashpower out of your miners when you have the block, but only know it is the longest chain, but you still need your full node to take the time to verify it is the longest one.
As soon as your full node says go/no-go with regards to the block, it makes sense to direct your miners to the tip of the longest *valid* blockchain, redirect it off the bogus one, if necessary.
However, this is not what happened: The miner completely ignored validity of the chain, they had no full node checking their miners and simply mined what was the longest in total amount of hash power, as they could see from the block headers.
But this also means that they lost money which was preventable, because eventually, that fork got orphaned because the hash power majority caught up and replaced the longest, invalid chain with a longest, valid one.
And this costs the affected miners money (orphaned blocks with &gt;25BTC loss each) that a simple and low-budget investment in a full node would have avoided.
@_date: 2015-07-11 14:16:59
Well, then, the alternative is that XT *will* be adopted by people (just look around!) and might (quite likely, actually) eventually take over 75% of mining power and thus become 'the Bitcoin'.
But that also means the old 1MB one is squashed out. Do you want that scenario?
@_date: 2015-07-17 08:55:29
I agree that Moore's law *might* slow (however we have that worry since Moore stated it). 
However, with full nodes in data centers, Bitcoin would be fine running on today's hardware.
- CPU power: We are already at 4000 txn/s with modern CPUs. ASICs with today's technology could *well* scale that by a factor of 10-100x.
Multiple CPUs and a software change (transaction validation can be done in parallel) could be done even without new HW.
Also consider that chips are still basically 2D today - we haven't figured out proper nm-level 3D stacking of structures yet; that alone could keep Moore's law alive for another 2 decades.
- Bandwidth: Gbit/s connections exist today, Gavin's schedule would only reach ~133Mbit/s. Fiber speeds are still growing immensely.
- Storage: Pruning can be done now and UTXO coalescing might be needed at some point (after years of full 8GB node operation). [And also, there is still a lot of room at the bottom.](
@_date: 2015-07-22 14:38:52
Indeed. I hope that eventually there won't be any single central-looking entity anymore and just regular synchronisation meetups between different implementations on possible feature upgrades.
@_date: 2015-07-10 14:29:30
And you know the answer to that as well: That the 'risk breaking Bitcoin' is assessed in a completely different way by me and many others. And that micropayment on 'my side' isn't seen as paying for a cup of coffee.
Anyways..  I think you can in any case agree that bright people are on both sides of the issue?
So what do you think about this, something I just today posted on bitcointalk:
Given the deadlock, I honestly start to think it might be good to do a 'divorce'. As in, we actually agree to disagree, and all see that there are fundamentally incompatible opinions with regards to the direction of Bitcoin. But we can keep it friendly.  This might be less damage/cost than having a hardfork battle solving it in January 2016.
That way we can avoid the fighting and truly let the market work out the rest. We divide up the common assets of the web sites, code repos etc. between Bitcoin/QT and Bitcoin/XT. Make it clear that Bitcoin is now two Bitcoins, and that the user has to decide.
QT and XT simply seem like two people who shouldn't be married. Better have a clean divorce than endless fighting.
@_date: 2015-07-03 23:05:46
Because they might differ?
Simply allow RBF only if both are flagged as RBFable. Else, fall back to default behavior.
@_date: 2015-07-19 16:05:09


The problem is that no one knows what 'rare' is here. ... but as you said yourself, the value is in the ranked list, not the attached numbers.
As for the class imbalance, I see that one...
sklearn.svm.SVC(class_weight=True) is pretty much my default for getting a classifier to play with, should I need one... :D
@_date: 2015-08-23 18:31:17
IBLT might replace the fast relay network eventually. In any case, what is your point? Bitcoin consensus is about what is the valid blockchain, but people can do whatever network protocols they want?
@_date: 2015-08-05 10:27:32
The obvious one?
Block size limit is way too low. 
@_date: 2015-07-15 22:34:15
My point is, I think, that 2.) means that many of the smaller transactions would happen through LN, and might thus decrease on-chain transactions by an order of magnitude or so.
I do not think that LN will ever shave off more than 2 orders of magnitude of transaction volume from what has to happen on-chain.
@_date: 2015-07-09 22:11:42


I am not so sure about this anymore. Damage due to hard fork and accompanying PR-disaster (even if the network will resolve fine) might be more damage than a clean split. 
Yes, we should avoid it, but discussion is really stuck, don't you think?
@_date: 2015-07-24 11:10:34
@_date: 2015-07-08 03:29:33
[It should be noted that the blocksize limit is completely ineffective against network attacks.](
@_date: 2015-07-09 16:48:17
Thank you, I even reread the whitepaper multiple times...


And where is the evidence for that?
The 1MB limit is a temporary anti spam limit.
@_date: 2015-07-14 16:53:52


With Gavin's proposal, there is a very good chance that this won't happen ever.
@_date: 2015-07-07 09:54:35
And it should be added that one of the guys you listed simply *has* a conflict of interest in this debate.
The right, honest thing to do would be to step back from participating in this...
@_date: 2015-08-17 21:56:07
You seem to assume either theymos scenario or this not being different from removing MAX_BLOCK_SIZE.
Why so?
@_date: 2015-07-01 11:13:43


Bitcoin was always meant to have no effective block cap and scaling to gigatransactions/day.


No need to read anything into his intentions, he stated them clearly:


would be about 400 bytes (ECC is nicely compact). Each transaction has to be
broadcast twice, so lets say 1KB per transaction. Visa processed 37 billion
transactions in FY2008, or an average of 100 million transactions per day.
That many transactions would take 100GB of bandwidth, or the size of 12 DVD or
2 HD quality movies, or about $18 worth of bandwidth at current prices.


sending 2 HD movies over the Internet would probably not seem like a big deal. 


Yeah sure.. Because more transactions == less users?! Do you notice how ridiculous this is?
@_date: 2015-07-05 16:17:29
Fully agreed. Let's just not give in to the blocksize cripplers and keep to Bitcoin's original goal of it being able to scale a lot.
Hopefully the 'Bitcoin was always meant just as a settling layer'- social engineering stops soon.
EDIT: Typo.
@_date: 2015-07-08 02:48:51


That is a fear we collectively conditioned ourselves to have, all the time.
Here I argue it is unfounded, because the whole debate is framed the wrong way. We'll see.
@_date: 2015-07-08 19:40:18


The full nodes only get it once more - because of the mined block. If you'd have actually read what I have posted, you'd have seen that I address this in saying that a blocksize cap can *at most* make a spamming attack twice as expensive per byte of network bandwidth. And that only in the case of a zero-cap. And others have worked out (see long thread on BCT) that per node cost goes down when mining the transactions...


Then they wouldn't end up in the stats from all over the place either...


Excuse me? Those were valid transactions, else the full nodes wouldn't have forwarded those...
May I guess this is a semantics game that you want to play here? "*Those* transactions there are spammy?" In that case: No thank you.
@_date: 2015-07-09 22:15:08


There will  never be *unlimited* transactions and never ne *unlimited* blocksize, and thus fees will on average always be truly greater than zero, too.
 
Physical and technological limitations ensure this.
@_date: 2015-07-08 10:39:35
It is completely off the front of too...
@_date: 2015-07-19 14:28:55


Is it really obvious? Isn't this (mostly) the missing part?
@_date: 2015-07-06 11:06:28


If you'd have read the other comments, you'd have seen that I am well aware of this distinction. SPV mining in this context == SPV mining + full node resetting when SPV is going nuts. 
Oh, and they didn't do that, they actually did solely SPV mining and that's why they got burned.


SPV mining for one block increases proof of work in blocks. That's actually a good thing. 
With regards to the constant worries in all different ways about SPV wallets, all a [solvable issue.](


    attacker produces a large invalid block
    it takes a lot of time to validate it, so most miners will resort to "SPV mining"
    chances are that one or more blocks will be added on top of it
    attacker might now trick SPV wallets into accepting double-spends or even completely fake money
FUD and scare tactics. Obviously SPV mining in all forms happens with 1MB blocks, too. Nothing to be gained or lost with keeping 1MB blocks in this regard, except:


Wrong. Block subsidy + transaction fees. Unless you seriously believe that people will pay horrendous fees for a 3txn/s system, more room for fees == larger ecosystem == larger total amount of fees.
Making the attack actually harder.
       And if 51% of miners are attacking us in this way,


A well behaving majority can suppress even these attempts. A prolonged attempt needs 51%. Whether the former happens or not will dissolve into another whole back and forth about small and big games that has been here on reddit enough times...


Troll. Instead of pointing out what is wrong, you resort to this ad hominem...
EDIT: Fixed quote.
@_date: 2015-07-19 20:17:22
Yep. And those are fast. And relatively easy to understand IMO.
@_date: 2015-07-04 19:17:39




So is it safe, or not? Is it half safe? Not at all safe? Somewhat safe?
@_date: 2015-07-09 17:28:57


Yeah, it actually is somewhat ironic - it is just not that politically contentious and unlikely to become that way. I cannot see a business plan around a 100kB transaction limit, for example.


Fully agreed. Today I had the insight that maybe we are actually mainly haggling about the detail whether 'free' full nodes should support 8MB blocks or 1MB blocks?
I think if full nodes are mostly professionally paid for, we wouldn't see so much contention about the blocksize. But I might be wrong.
From a very detached perspective, I suspect this is the first time in history that people argue that passionately and globally about just a frickin' number :-)
@_date: 2015-07-07 15:31:59
I think people who argue against blocksize limits rather want to see Lightning Networks and regular Bitcoin *competing fairly* for transaction fees.
But it now indeed looks like some people want to use their power to *cripple* Bitcoin to *force* users into LN...
@_date: 2015-07-09 13:53:36
I think transactions need to be made O(n) validated first, but as soon as that is the case, the limit should be removed completely. 
As far as I understand, that is the sole reason for the limit.
@_date: 2015-07-24 12:47:54


Well, we are discussing Bitcoin, so your point is?
And a couple s for mempool sync is real time in any meaningful sense of real time, though. It certainly doesn't help that 'real time' is a somewhat ill-defined concept.


Sure. But mainly because of the blocksize cap prevented miner from mining the spam transactions. And a higher (or no limit) would prevent the spammer from saturating blocks as easily...
And that was 400txn/s with mempools mostly synchronized (of course, every new transaction took a little to get propagated).
By the way completely invalidating your 7txn/s BS...
@_date: 2015-07-24 13:43:20
And core is run by a soviet-era central-planning committee.
Funny though that *I can freely choose* between the two.
@_date: 2015-07-10 22:18:37
And the XT guys will not care about you trying to destroy the original vision.
We reached an impasse.
How about we do a clean divorce instead of a messy hardfork battle?
@_date: 2015-07-24 14:36:12
If we listen to the 1MB-blockistas, there won't be a widely successful, scaling Bitcoin layer-0.
Note that they also argue from a point of fear/doubt/uncertainty most of the time, so there is some analogy.
@_date: 2015-07-04 00:42:54
I think you missed the point: RBF for all transactions changes the risk profile for current 0conf without a need.
Per transaction optional RBF enables, well, RBF for anyone who wants it and leaves the current 0conf situation intact as it is.
It is being used successfully by merchants and users so why destroy it?
@_date: 2015-07-29 17:07:25


Gavin, certainly not part of any mob here, clearly said the hardfork will be implemented on XT and I have not seen any sign of movement of the other side so this is an event that is going to be expected. In either scenario, at least one chain will survive. So, again what are you worried about?


Optimism except for the results of a hardfork?
Please also note that intelligent people disagree with you, too.
@_date: 2015-07-05 18:14:26
@_date: 2015-07-21 14:48:33
All I am doing is calling out the blocklimiter-side to be honest - and ***clearly*** state that they intend the 32MB to be the final maximum limit for Bitcoin.
Instead, something sneaked in in weird wording into BIP100.
If BIP100 would outright state '32MB is solely a technical limit and should not interfere with a market-based miner vote on blocksize beyond 32MB eventually' that would be fine by me. 
If BIP100 would outright state '32MB to be never exceeded', that would be a proposal I would completely disagree on, but at least it would be honest...
@_date: 2015-07-07 14:43:06
And the miner has to accept it. Most aren't. What I am saying....
@_date: 2015-07-07 22:57:51
Ah, I see. I stumbled upon that a couple days ago. Will try it out. Thank you!
@_date: 2015-07-08 02:28:18
And full nodes + IBLTs would mean that they can set relay fees in addition to whatever other throttle they want to put on their network links. Decentrally. Individually.
what do you think?
@_date: 2015-07-07 15:17:19
The relationship to fee markets is that some people want to constrain block size to get a market with higher fees.
As [Gavin pointed out]( a fee market is existing right now. 
And I am not opposed to functioning fee markets, either. 
I think is saying some very intelligent things about also establishing a market for transaction relays/what full nodes are doing. He's an eloquent voice of reason against the central planning committees...
@_date: 2015-07-07 22:53:44
One can tag people here?
@_date: 2015-07-20 08:02:30
Yep! She's around here on reddit, too as 'girlwriteswhat'.
I am kind of surprised my comment didn't get downvoted into oblivion :)
@_date: 2015-07-30 08:45:03
One should add:
 ***Running XT is the best choice for anyone operating a full node. Because with Bitcoin XT you cannot be on the wrong chain for long***.
To explain:
XT will follow the hash power majority, whereas an 1MB node might cut you off the mainchain.
If blocks stay at 1MB for longer than expected, XT will know that and behave just like a full node from today. 
If blocks grow above 1MB, Bitcoin XT will know and will behave like a responsible full node in that scenario as well.
You risk more by *not* upgrading to XT.
@_date: 2015-07-30 10:11:34
Because it only works probabilistically?
@_date: 2015-07-31 09:06:54
And that we'll lose decentralization is a fear. There might be some truth to it, or it might be completely false. It probably doesn't harm to proceed somewhat cautiously, though. 
But *proceed*.
We have to take measured risks in any case. Because 1MB isn't going to do it in any way.
Gavin's proposal is IMO still the best compromise of all here.
@_date: 2015-07-06 21:20:14


I don't really see the issue either way (whether fast or slow propagation), but if you like fast propagation: Where do you put the fast relay network in this context? 
[It is also a bit funny that you say this because the worry of other blocksize limiters is often that we need either a limit or inefficient propagation for supposedly better fee markets. 
I am not at all saying that *you* do it here, but the way it is brought forward often makes it appear as a pretty manipulative argumentation tactic.]
Is sufficient testing otherwise your only worry? I believe I heard Gavin agreeing somewhere that it is good to set up a 8MB testnet... would that running for a while ease your worries?


Certainly, they don't have an interest in outright and immediately killing it. It is arguably rather more like a relationship on the  symbiotic-to-parasitic scale...
And I simply think that a competition between transactions on L0 and (eventually) Lightning will make the most sense. I am certain that the people @ Blockstream also believe in what they are doing is right, but there *is* a conflict of interest and being able to profit from something doesn't really cause you to question your beliefs more... 
And then, I think, there is NIH syndrome and the disease so common in CS of stacking complexity wherever possible playing a part, too...
And at some point, discussion can simply be so stalled that bitcoin-core will break apart into different, hard forking fractions. I certainly agree that this should be avoided, but avoiding it has costs, too...
@_date: 2015-07-11 12:29:52


Transaction fee is already mostly by byte, though?
And regarding UTXOs: Well, every full node is paying for them. *Not a problem yet, by far*, and not a reason to oppose a blocksize increase, but it might eventually become one. That's why I think full nodes should be able eventually coalesce old ones together, or a similar scheme.
@_date: 2015-07-17 09:09:59
Don't know why you are downvoted. 
One might add that sidechains that are not somehow mapping to the natural clustering of transactions (to keep as many transactions as possible happening within one sidechain) won't really help with scalability either.
You'd constantly need to shift your money around between sidechains and this wouldn't be better than a single chain in terms of scalability.
And if they map to transaction clustering, they'd most likely map to geographical clusters, i.e. are easily *within reach of a single government*. Much more to worry about than a large scale, but widely used single Bitcoin ledger, or isn't it?
Oh, and I tried to get Adam, who asserts that sidechains help scalability, engaged in a discussion, only to be downvoted into oblivion, [here.](
@_date: 2015-07-02 09:18:56
... and they could have formed a non-profit if the sole purpose of this money was to further the development of Bitcoin.
@_date: 2015-07-10 19:48:45
It won't even come to that - I think enough people are aware that currently Bitcoin == 1MB cripplecoin. 
@_date: 2015-07-04 19:47:17
Honestly, the way 32MB got elevated into something that can easily be read as an *ought-to-be* in BIP100 (Just read the draft) makes me really suspicious that the blockstream people have been pulling the strings there and inserting that - due to their conflict of interest.
I know that there are technical reasons to not make blocks larger than 32MB for now.. But look at it from another angle: 32MB is enough so that a lot of the blocksize increase supporters will be like 'me, whatever' if that would happen now.. only to have Bitcoin's protocol so ossified and cemented into place in some 5..10 years that Blockstream will make quite a lot of money from forced fees. With no chance to change anything then.
Gavin's proposal I think is indeed the best compromise.
@_date: 2015-07-14 16:51:04


OTOH, the devs can block consensus on things that (like said) should be a no-brainer...
I have not seen this effect yet - and I am rather worried about stagnation due to the blocksize limit. I also think that people will look very closely if it is about money...




I am not so sure I agree  - if miners are worldwide distributed, they do not need to be small at all. 
But in any case, Bitcoin mining has no barrier to entry - there will always be a couple of smaller miners, if only because people mine solo for the heck of it.
@_date: 2015-07-08 02:37:31


Agreed, but 32MB is not an intended *political hard cap forever* but just a current technical limit of the network code. We should keep that in place before the code has been fixed, of course.


If the block is very large, it won't get to any full node, will be orphaned. The miner loses money. Incentives. That's why I think it was aptly described as a temporary anti-spam measure. Because early on, GB sized blocks for no reason (whereas they have very legit reasons now) seemed cumbersome.


And as I have pointed out, internet bandwidth is *only constrained to 0.5x what it would be without any cap in the asymptotic limit of a zero cap.*


Fully agreed.


Flip the argument the other way around. The 1MB limit is too low to actually support all transactions happening in the network. The network is stressed *either way*. It is at most stressed twice as much by removing the cap (because then all the transactions would be confirmed). 
However, the transactions being confirmed would also mean that the spammer would actually *pay* for them!


And again,  the blocksize cap only makes them twice as expensive! ***It does not, at all, remove the capability to spam the network!***
Forget about the blocksize cap as an effective cap on the spam in the network.
That is just for honest transactions, actually meant to be included in blocks!
 
@_date: 2015-07-14 10:24:05
You are arguing that economy of scale will tend to favor the big miners. That is true - but is it bad in any way, and should it be counteracted by core devs? I do not think so; there's a lot of arguments to be made that this is very damaging soviet-style central planning by entities (devs) that do not necessarily have a better perspective than the users. Paging ...
He's basically giving people a way to channel their 'care for a healthy mining ecosystem' into action, by favoring their small, local neighborhood miners.
This should make everyone happy?
@_date: 2015-07-08 10:45:09
Yes, but maybe a better way to phrase this is that the mining reward schedule will incentive growth of Bitcoin and slowly transaction fees will take over (with many, many uses)?
As in: We do not centrally steer this?
@_date: 2015-07-08 14:37:52


This is a common argument. Have a look around.... just in this thread and you'll find a few examples...


A transaction goes into the network once, and *might* be seen another time by a full node, in a full block. With blocks including everything, this will happen, making bandwidth needed 2x as high compared to just unconfirmed transactions. That's the scale on which you can move maximum network load around by blocksize limiting.


Only if you do not relay and verify transactions...


As the submission title says, this is an argument why the blockcap is worthless in keeping the P2P network from overloaded. If you are not interested, stay out, maybe?


Which is fine and not the topic of discussion...
@_date: 2015-07-05 10:00:10
... or pay others to run a full node. Or pay several independent party to spread the trust.
A market for full nodes.
@_date: 2015-07-14 09:21:23
All it showed was 
- the blocksize was wholly ineffective at dealing with spam.
- And the network actually supports transaction rates spiking above ***400txn/s*** (higher than Paypal!)
So what again is the reason to limit blocksize? Centralized fee market steering?
@_date: 2015-07-05 10:51:19


So the majority of hash rate just sits there and lets others eat their share instead of kicking the blockbloater out?




And they lost some ~150BTC due to their practice. I think this is a case of lesson learned and done.
@_date: 2015-07-19 15:47:39
People can do stats about 0-confs, too? That makes it pretty much predictable, too?
I would take issue when someone tells me it is *without* risk.
@_date: 2015-07-08 10:46:29
I see the brigades as a reason, but as you say...


Exactly. It was benign satire. What the heck is going on here?
@_date: 2015-07-07 14:45:21


That's the point though, soft forking down is a lot easier than hard forking up.


Nope. Did you downvote me?
@_date: 2015-07-09 16:43:58
So what are the resources then that constrain us to 1MB?
@_date: 2015-07-05 17:34:31
'Within' is only 'within' because we let it be that way. I think the dev team needs to split into a couple of benevolent dictatorships. One led by Greg, one by Gavin and one by whoever is the head of btcd and maybe even a bunch more. That FOSS model of benevolent dictator seems to work the best in moving things forward.
In no way should the current crop of Block-the-streamers keep the brand and name recognition of bitcoin-core and bitcoin.org for themselves.
@_date: 2015-07-08 03:04:28
You are reluctant to actually *think* about this.
SPV mining only makes sense with a full node to pull the SPV miner back in case it goes nuts. See what Tier Nolan wrote on this. Really.
@_date: 2015-07-19 14:08:26
You are a non-native English speaker, too, right? Non-native english speakers probably all have their highly unique set of English language mistakes :-)
Which can be a bit scary when thinking about it...
@_date: 2015-07-09 19:47:48
Hey, thanks for the tip!
@_date: 2015-07-19 14:05:39
The value of his work is in a ranking of possible suspect account pairs.
With regards to an actually *proof of sockpuppets*, as in the submission title '... manipulation confirmed' are GIGO and just games with numbers. This has been called out be me and others (notably also strong opponents in the blocksize debate), and rightfully so.
Given that the machine learning-assisted, narrowed list of people with similar writing styles does not withstand an manual inspection - most people seem to be genuinely different and flagged as false positives - the correct conclusion should be: No signs of widespread sockpuppeting yet...
This is, BTW, the same conclusion from my own analysis.
@_date: 2015-07-07 14:57:04
Only when you small blockistas get their way...
@_date: 2015-07-03 10:16:31


Is there anything that in principle prevents seeding fully pruned data to new nodes, if they are OK with that?
It would still be fully validated, or wouldn't it?
@_date: 2015-07-09 23:35:53
He's 'replying' to luke-jr, I guess.
@_date: 2015-07-08 01:44:24


Yes, and those are a *per node policy*, nothing *centrally planned*.
I believe I remember that a well known guy here tweaked his nodes to only relay transactions that he deemed not SPAM.
This 'fracturing' of the network has been going on all along, but the state of affairs clearly shows that it is not a problem in keeping transactions from happening. 
Most importantly, this is a decentrally imposed policy, nothing that is centrally set.


Again, you confuse a centrally enforced 1MB limit with a per node relay policy...


Miners are SPV mining. Correct SPV mining means mining on just the headers and running a full node in parallel that pulls back the SPV miner as soon as the full node noticed that the latest block is actually invalid.
f2pool and the other one didn't do that. They just where spinning freely. That disrupted the network somewhat, but *most importantly*, it costed them a lot of money.
Incentives work out that this will not continue...


They have to forgo a block and its subsidy + transaction fees just to disrupt the network. That is actually harder with larger blocks.. more orphan risk, more transaction fees to lose...


Validation can easily be parallelized on CPUs. The miners run a lot of hardware already, a few CPUs more or less do not matter. And the risk of a single SPV miner finding a block twice in a row in 15s of validation time is negligible. And again, even then the miner has an incentive to minimize this, because those are *orphaned* blocks!
@_date: 2015-07-01 11:42:55
A problem on a RaspberryPi under your desk. Not a problem with full nodes in data centers. As Satoshi intended.
Decentralization is a large user base and full nodes in different jurisdictions. Not small, crippled 'full' nodes.
@_date: 2015-07-21 13:07:56
When Bitcoin stays stalled and deadlocked due to the 1MB limiters vetoing increases, Litecoin will take over. (Apart from the issue that the whole cryptocurrency ecosystem will suffer horribly)
Network effect is not a guarantee, it is just a way to describe inertia. And that can run out.
@_date: 2015-07-23 09:33:13
Bitcoin does not really care that much about the amount of your transaction, though...
@_date: 2015-07-03 17:47:24
The fact that they don't do that should tell you about your tunnel vision.
@_date: 2015-07-10 08:55:37
And that does not scare me at all.
1.) bigger nodes in data centers, as Satoshi intended and what is the goal for Bitcoin
2.) in some *20 years*
3.) use UTXO commitments and UTXO coalescing to keep the amount to store down
And, bandwidth-wise, ***filled, actually used*** (because Bitcoin would be *successful* in that scenario!) 8GiB blocks are going to be about *130MBit/s*. Just today, I read in the newspaper that some rural communities around here are going to have *200MBit/s* to the home next year. 
@_date: 2015-07-08 03:43:03


But so would this very attack choke some players. The blocksize didn't prevent that at all. Yet the network survived!
I think the point is that the network is never in a state of *perfection*. There will always be some nodes having problems.
However, dynamic consensus on the vast majority of nodes exists.
Because the network blocksize limit is wholly ineffective against attacks and spamming in terms of bandwidth, we have been actually running with the training wheels off *all along*, but didn't realize it!
And it works just *fine*.


Sure. And he probably did. But what did he damage?


With all respect, I think you might make a mistake here, the same I did all the time: 
The blockcap prevents a high network load due to large blocks. 
It does *not* prevent a high network load due to large number of unconfirmed transactions.
So it only addresses half of the network load. The other half of the network load is free to be driven to insanity by attackers! (Or rather, as apparently some average of the P2P network allows)
An *attacker* doesn't care at all whether transactions confirm (actually, it is even better if they don't as he can keep his money), and so he can spam the network as much as he likes, regardless of actual blocksize!
All the blocksize does is keeping the double load away from transactions being sent around again in the mined blocks. That is the factor of at most an asymptotic 2x in cost increase for a zero-sized blockcap that I was talking about.
@_date: 2015-07-09 20:24:17
I agree with you in principle, but I think we'll not see $0.02 on chain too soon. I think a coffee on the blockchain with a couple cents of fee is actually fine long term (hardware wise and with e.g. Gavin's blockcap schedule). But $0.02 is really deep in the realm of micropayments and I think if there's ever going to be widespread use for them, that would be another 2 orders of magnitude in hardware cost. Maybe doable, but I wouldn't count on it anytime soon.
There are working solutions were you can basically lock in a fixed amount of Bitcoin and then repeatedly send the counterparty a check with an incrementally larger amount, and at some point unlock the money again, clearing the amount. If you didn't come across this, google 'micropayment channels'. 
The whole lightning network idea is an extension of that. It is very interesting, but it shouldn't replace a widely available Bitcoin level-0 transaction network. Especially not by forcing it. It would be a great *addition*.
@_date: 2015-08-17 08:33:27
Yes, any mod that has the chance of leaking relevant stuff - please go ahead :-)
@_date: 2015-08-16 19:46:10
Good catch. Thank you!
@_date: 2015-07-09 21:53:37
I see it like this:
Bigger blockcap ~ more transactions allowed ~ larger userbase supported ~ larger network effect ~ larger value of Bitcoin.
And I think this chain is - if you believe in the possibility Bitcoin's success - really hard to break apart, for better or worse.
So if you restrict the block cap, you put Bitcoin into a niche. You will get less mined transactions for sure, but you'll also get a smaller network effect and less total value.
Which brings me to my actual point: It is not hard to either create, fork off or use an Altcoin that has basically the same parameters as Bitcoin has now, but has a smaller blockcap.
Using an Altcoin should be perfectly fine for a user who is interested in a small blocksize cap, because he will get the smaller market cap *anyways*, in either scenario. He's not losing anything that he or she cared about.
But allowing a larger cap for *Bitcoin* would allow *Bitcoin* to fill a 'niche' that *only* Bitcoin is able to fill: That of a network scaling up to worldwide usage.
Shouldn't we at least give it a try? 
@_date: 2015-07-10 15:37:35


Fair point. But this is still far from clear. With Gavin's schedule, we have a very good chance of keeping it in reach of a dedicated hobbyist. And you should also see that one can test implementations on a smaller testnet.
In any case, such a scenario would be a *wide success*. Do you not want that? Do you value easy developer access over success of Bitcoin?
Honestly, I don't. I want Bitcoin to become successful and increase in value. Long term. I see the value of Bitcoin (in terms of market cap) as the best indicator for success, and I also think it would be nice (as in it would be worth more goods) for that to happen. Call me an evil capitalist if you want.


The impacts of missing the train are much more permanent. As it has been pointed out before, it is easier to soft fork the blocksize back down, should the need arise.


I think it actually might buy us enough time until we're good. Meaning eventual worlwide adoption. Talking about Gavin's proposal here.


Having 'a good number' of layers is not a goal one should try to achieve, quite the contrary. If you can keep it KISS, that is usually better.
And I am also not opposed to Lightning Networks on top of Bitcoin. Why should I? That would be a nice *addition*. But I do not want them to be forced on top bit crippling the blockcap.
@_date: 2015-07-09 11:02:00


Well, but what are you complaining, you can still spam the network however much you want! 
The blocksize cap only keeps you from doing legitimate transactions!
EDIT: Yes, you might add an '/s' :-)
@_date: 2015-07-29 18:52:37


Bitcoin was managing billions of dollars in assets, before the world was
keeping close watch, and before anyone had even conceived of a more
sophisticated way than a hard fork to update the overall system, such as
2-way pegged sidechains.
Satoshi intended Bitcoin to grow. The 1MB limit cripples Bitcoin, quite clearly.
Bitcoin is a billion-dollar-system because it is expected to grow beyond 1MB
transactions blocks. Don't fool yourself.


came up with sidechains specifically as a way to upgrade the system
smoothly. There are minds as great as (if not greater than) Satoshi. Listen
to them.
I had several discussions with him here on reddit. You might have seen those.
His hashcash contribution is certainly an important part of Bitcoin's design,
but only the overall architecture, done by *Satoshi*, makes Bitcoin work.
Which, by the way, is beyond just the scope of computer science.
That said, no one sane would not listen to an honest, sane, technical
argument. However, Adam is not infallible. For example, in the scaling
discussion, he seriously talks about [Bitcoin traffic breaking the
This is hyperbole, irrelevant to the discussion of Gavin's patch which is only
*eventually* (decades) going to create network traffic that will always stay *way* below any
Internet saturation limit, even of today's internet. Andd I bet you can find a *quite* big set of
highly intelligent people and academics who'll find Adam's argument as ridiculous
as I find it.
Note also that people on 'my' side argue that *natural* limits will take care of transaction rate and price way before it will saturate the Internet...
But that's all besides the point ... in the end, arguments need to stand on their own,
regardless of who made them.


warning people to wait 30 more confirmations than usual. During
/that soft fork, there were times at which a majority of the mining power
/was put on the pre-BIP66 blockchain, despite the miners having advertised a
/vast majority in support of BIP66what an utter failure!


blindly; that's why there was a problem. Do you understand this? Do you
understand how frightening that is?
I understand this and I think I am not as frightened as you are. In the end
things worked out. The risk of a hardfork is a temporary failure of the Bitcoin system.
The risk of keeping an 1MB limit is a permant failure of the Bitcoin system.


Because they didn't think their incentives through. They lost money on their
orphaned blocks, more than the money needed to pay for a full node to pull them back in line. This has also been pointed on BCT and I believe the ML as well.


might be switched to the new blockchain, but merchants (who are more likely   
to use verifying nodes) might be on the old blockchain. Payments won't be     
received, because according to one history, they are invalid, or double       
spends might be made, because there are 2 histories at play.
Part of the the point of a 'divorce' would be to explain this scenario to
everyone in time. As a clean divorce is not going to happen, we're heading to 'war'. However much we both dislike it, I think it is unavoidable now.


software to avoid further difficulties in interacting with the economic
majority. Yet, at what loss? How much economic activity must be forgotten
and written off in throwing out the old blockchain in favor of the new?  
Again, you see the risk of a temporary disturbance. I see the risk of a
permanent crippling.


possibility of a hard fork (and a soft fork is not much better); the only  
right thing to do is to let each system stand on its own (Bitcoin Core,    
Bitcoin XT, etc.), but create explicit paths between them with what's called
a 2-way peg, so that users can transfer their coins from one system to the  
So, yes, Bitcoin will never be on a firm foundation, because hardforks are
possible. This is part of the design of the system - and goes way beyond computer science. It is also a good thing, because it allows for people to
route around damage, should certain parties try to hijack the protocol.   
@_date: 2015-07-08 22:27:55


Yeah. Those are the details. Might change the factor 2 a bit in either direction. But not really important for the discussion per se.


Why so? The stuff that you put in on your end of the TCP pipe goes out on another end, on some other dude's node. You shouldn't need much more to explain that equilibrium.
EDIT: Should -&gt; Shouldn't.
@_date: 2015-07-24 11:08:33


Yes, sorry, meant transaction *fee* floor, fixed it.


I don't see the DOS so much. Eventually, with IBLTs, pruning, UTXO commitments and tackling UTXO growth (with eventual coalescing, for example), full nodes will provide the service of  forwarding transactions to the miners in addition to being a transaction database. 
They'll only do that when it makes them money. And that's fine.
Note also that the decision what transaction to forward is completely decentralized and there doesn't even need to be network-wide consensus on this. A full node will not forward transactions that are unlikely to be mined by a miner it connects to. And a miner has a disincentive (orphan cost) of pushing a mined block that is not IBLT-able.
has some very good thoughts on the economics of a more distant but successful Bitcoin future.
@_date: 2015-07-08 09:04:38
And it all shows that a blocksize limit [is not effective in preventing these attacks.](
And that we basically run just fine without protection, since day 0.
@_date: 2015-07-09 21:35:22
Maybe we should do a clean separation into different Bitcoin parts then?
Maybe doing the whole separation into a Bitcoin/QT and a Bitcoin/XT  on purpose and full agreement of everyone would be a way out?
Maybe the damage to Bitcoin would be less in such a scenario than an unclear hard fork scenario. Some of us would agree to disagree...
@_date: 2015-07-09 22:41:55
The blockcap does practically nothing in preventing an attacker from consuming bandwidth and electricity.
It only cares about storage - and storage is not consider a bottleneck for scaling yet.
So again, what is the argument here?
@_date: 2015-07-19 13:15:41
The noise part is in this sensational BS submission.
The method itself seems to be a reasonable method to narrow down on potential sockpuppet suspects. I went (though not at all this far) along a similar path in the last part of my analysis.
In that sense, analysis is helpful - just forget all the probability numbers that come with it, all that is of interest is the ranked list of suspects, for which any scaling factors pretty much do not matter...
And, given the high number of obvious false positives, the opposite from his conclusions can be inferred: There is no clear, widespread sockpuppeting of the Bitcoin blocksize debate yet.
@_date: 2015-07-24 09:18:36
Also, there are answers to SPV client worries apart from crippling Blocksize:
Mike Hearn's 'one step at a time' argument comes to mind.
@_date: 2015-07-07 08:57:57
... the chances whether he can scam me or not depend on the overall miner and full node behavior in the network. Exactly my point that you are consistently ignoring...
@_date: 2015-07-07 09:21:28
You are so damn right. I have said it before and I will say it again:
Satoshi had an initial, long, hard look at scalability. It was also one of the first question from others that he answered - that about scalability.
And he answered *affirmatively*. Bitcoin can scale, *Bitcoin can scale up to gigatransactions/day*, and he had this insight in 2009. Bitcoin could scale to gigatransactions/day with 2009's hardware and network capabilities. In bigger data centers and not as a RasPi under your desk, *of course*. 
Nothing, let me repeat: ***Nothing*** in terms of new data arrived yet that shows that this view is invalid or wrong.
Only a lot of social engineering, trolling, [psycho tactics]( and opinions disguising as expert technical opinion (you know who I am talking about).
@_date: 2015-07-29 16:56:56


Yes, working, available LNs *could* help with scaling and take one or two orders of magnitude off of transactions. However, consider that the LN guys say 133MB blocks are needed for people to open a LN channel twice a year per person on the planet.
Honestly, I think a realistic goal is a LN channel every week or so, given that you are still relying on a counterparty for clearing of transactions to be available. [Before you go and want to explain to me that I won't lose my money in that scenario: I know the specifics, but inaccessible money is off *less value* so the discussion is a bit more complex here...]
If you think about every week instead of half a year, your 133MB scales into the GB range.
And that, I think makes a pretty good case why Gavin's 8MB+40%/y proposal is just right on the money (no pun).
@_date: 2015-07-22 13:14:05
Agreed. For me, a functional Bitcoin means that we at some point need the ability for everyone to have *some* transactions on-chain. More than half a year, probably, but also maybe not multiple times a day. If the latter happens because there's going to be further awesome scaling of computers, fine, but I wouldn't bet on it either.
I am pretty optimistic that technology and further progress on the layer-0 Bitcoin software will *eventually* allow that scenario.
@_date: 2015-07-24 09:06:30
Why not make it a PR then that actually changes the text? Bike shedding is happening regardless of whether you first remove and then add or do an remove-add in one.
But it is a lot easier to NAK or let others NAK after you removed the text.
@_date: 2015-07-22 14:20:43
Maybe make it an honor code that devs refrain from discussions in which they have a clear conflict of interest would be a better thing?
This would also be a change resulting from the learning experience with the current debacle.
@_date: 2015-07-07 15:34:44


The miners oppose anything above 8MB for now, so we won't get anything above 8GB, so you are putting up a straw man here..


Not at all. It just proved that the incentives work as intended and f2pool lost money for their carelessness.
@_date: 2015-07-21 20:10:05
Gavin is basically saying: 
Look at the bigger picture and let us get the damn blocksize setting out of the hand of the devs.
I think his behavior is quite coherent. If the 1MB-blockistas would have accepted his 8MB+40%/y schedule, we wouldn't even have that discussion and PR...
@_date: 2015-07-21 22:16:09


Quite obviously this has been false for almost all history of Bitcoin.
@_date: 2015-07-08 02:08:01
Thanks! Well, yes, I always felt this whole discussion is off and Bitcoin is being killed by this.
@_date: 2015-07-06 23:16:09
Which means that we do not need a hard cap.
Because the miners can reduce blocksize as much as they want... 
@_date: 2015-07-01 20:41:33
Huh? 1MB blocks means a hard cap on the number of transactions per unit time and when that number is reached, saturation is reached.
@_date: 2015-07-05 17:03:25
Interesting topic, actually: What is the stance of btcd and other implementations on blocksize? (I only know of btcd and actually only really follow corE)
@_date: 2015-07-09 17:35:42
Oh didn't see this until I replied to your other post.


Maybe so. Hopefully we can agree on things that make the trade-off between SPV and full nodes more continuous, with things like nodes started from UTXO commitments in the past or similar. 
If I would have the time and money to work on this (ok, yeah I have the time to frequent reddit), I'd LOVE to work on putting more validation data structures in blocks, for UTXO commitments as well as making SPV more secure.
@_date: 2015-07-21 22:28:08


Keeping the 1MB limit is very dangerous to Bitcoin, that's for sure...
@_date: 2015-07-22 08:50:36
Gavin could, and Gavin will when this drags on for longer...
Gavin and Mike have been some of the earliest core devs. Before Greg and Adam, for sure...
@_date: 2015-07-24 08:54:57


You create a straw man - as if we are talking about 3000txn/s tomorrow!
3000txn/s are ~900MB blocks. In Gavin's very reasonable schedule, ***we would have those only around in ~14 years.***
At that point Bitcoin, and with that transaction rate, would be *widely* successful.
And ECDSA verification ASICs would easily give you a factor 100x in verification speed *with today's technology*.
Even when assuming no technological progress whatsoever from now on, in such a wide success scenario for Bitcoin, such ASICs are to be absolutely expected to be on the market then.
@_date: 2015-07-07 15:39:38
Hmm... I read it just like the OP. The people in there are the very same ones arguing for small blocks and the currently disturbed fee market is arguably due to a limit on the block size.
@_date: 2015-07-02 09:13:50
Kind of tiring to do it again... but: It should be clearly pointed out that blockcripplers want to change the course of Bitcoin - and are not forthright about it. 
NOTHING came since Satoshi that shows that large blocks, even GB-sized blocks are impossible to handle with Bitcoin.
It can scale. It could have scaled to worldwise use in 2009. Of course, not with RasPis, but with big full nodes in datacenters.
Exactly as intended!
If you say otherwise, you are trying to socially engineer the debate by hiding your desire to change course (rather, to cripple Bitcoin for maybe even your own profit) behind technically sounding arguments.
@_date: 2015-07-09 13:20:03


UTXO commitments + eventual UTXO coalescing (after ten years or so).
Removes the burden for keeping the data needed to spend unspent outputs around from the full nodes and puts it back to the users, where this burden belongs.
@_date: 2015-07-24 07:31:15
Your 'evidence' is a load of opinion and nothing else.
@_date: 2015-07-01 10:59:42
And LN should compete with on-chain access... not by crippling Bitcoin to force it.
@_date: 2015-07-09 11:13:03


Not disagreeing with that part at all... 8-)
@_date: 2015-07-03 10:11:42


This so much. Even if you are worried about centralization through too-high full node operation costs, look at the situation honestly:
Bitcoin needs at least a somewhat open internet. You can reduce blocksize as much as you want in North Korea, won't mean you'll be able to subvert your government and run a node. Not at all. Bitcoin is impossible there, because that country is simply a dictatorship.
So fear about big, really evil governments in the extreme won't inform you about the blocksize issue. If governments really don't like Bitcoin and are really evil, they'll simply ban the internet. Full stop.
So, rather, you have to look at Bitcoin vs. the somewhat open, somewhat representative goverments of the world where reddit and bitcoin are allowed to exist.
And here, you have quite a different situation. If Bitcoin becomes a really popular way to pay on the internet, it will be very hard to shut it down or even censor it. Even with only relatively few but very capable large nodes in different jurisdictions (my favorite of an open market or no cap) or an order or magnitude or two more nodes of which many are located in middle class people's homes with high speed broadband in big population centers (Gavin's proposal).
If Moscow, Berlin, Being and Washington each have a couple tens of full nodes, no single country can shut down or decide what gets into blocks. Because each single country would risk a net split and due to the way hash power is validating the transactions, destruction of their own part of the payment system.
That is why you want a large user base - you want Bitcoin to *grow*.
@_date: 2015-07-11 10:34:40


Religious texts, huh? So did the *temporary* 1000000 constant become a religious constant to you?
@_date: 2015-08-16 10:29:27
Good. Leave it like this. I like the chaos more than the 'moderation'.
Chaos also means fresh creativity and a fresh look at what Bitcoin is. XT or not. I think Bitcoin urgently needs that now.
@_date: 2015-08-11 17:47:44
Another thing: There's  now.
@_date: 2015-08-16 19:15:50
Yes, QT would be an appropriate name, too. Unfortunately, bitcoinxt contains a bitcoin-qt GUI program, so there is some ambiguity.
@_date: 2015-08-15 22:29:42
You have a typo in there. 11.2*M*B. That could be the reason.
@_date: 2015-07-02 01:13:40


Disagree strongly. If your payment channel disappears, money is locked up for quite a while and you have to wait for it a long time to thaw up again. Might create perverse incentives, too...


Several? 1.4.


That is a change of course for Bitcoin. Bitcoin was never intended to be full nodes in every hand. And forcing this is central planning indeed.




Block size can be *soft* forked down if it doesn't work out. And it won't probably be 8MB blocks as soon as Gavin's hardfork takes oven - because usually blocks are not filled, unless Bitcoin is crippled to saturate.
@_date: 2015-07-02 09:22:13
Which most of the miners seem to honor for whatever reasons
... giving the 0conf transactions a risk profile
... that many people find then acceptable for small transactions
You'd lose something by removing 0conf. You probably also gain something by RBF. Putting in flags gives you the best of both worlds and lets the user/merchant decide on what transactions to make / to accept.
@_date: 2015-07-06 21:44:27


0conf *is* default, works well enough for many people and small amounts. Make RBF optional per transaction (by introducing a flag) and everyone would be happy. Except you and P.Todd, apparently.


Ok, I think I didn't get through what I was trying to say here and elsewhere:
SPV mining until validation arrives is totally fine, it even increases the total amount of proof of work. 
But what will make them lose money on average is SPV mining in sequence, without any validation.
This is what happened, and this is what they'll get a clue about.
The incentives align as intended here.
@_date: 2015-07-05 10:41:45
Understood; I was just pointing out the misunderstanding here. As others already suggested is a sock puppet of or vice versa.
@_date: 2015-07-08 15:55:17
You might be completely right. I might be confused by Blockstream propaganda to think there is even something political about it.
But it does feel eerily political for an otherwise very rational, rule based system. See what is going on in my mind in my other post [here.](
@_date: 2015-07-10 15:08:23


Not I am just stating the facts. I am not disparaging the devs - or if you want to see it that way, I would be disparaging Gavin, too.
It is, of course, important housekeeping. And, yes, I am thankful for that. And I said so, repeatedly. But this should also be obvious?


Again, you ignore my argument and fight straw mans while foaming at the mouth: All I am saying is that Bitcoin is a scheme for (almost) instantly creating a replicated datastore across the globe. And it is quite efficient at that.
Maybe you were held up by me using 'database'? Call it 'datastore' then...


What code matters?


You are getting angry. Not a good way to defend your position.
@_date: 2015-07-01 22:08:07
Agreed. Except that the language in the proposal should be fixed to be very careful to not elevate the current, technical 32MB limit to an *ought-to-be* for Bitcoin.
I think that is about the only thing that needs fixing. But it is a very important fix. Because It would be awful to have the same debate as we have today again at the 32MB limit.
@_date: 2015-07-11 14:27:12


Very true. And the hardfork only triggers at 75%. Most merchants, payment providers and miners are for a blocksize increase. You said


So what exactly do we have to worry about then? 
@_date: 2015-07-10 09:02:10


Ok... so you think an 8MB coin would actually work out? That is interesting news to me, why don't you go to Gavin then and get an agreement on 8MB?
Other than that, your scenario is still a red herring. We are talking about worldwide adoption here, and you are constantly worried that a scaling single chain is impossible in such a scenario.
Which means that you need multiple chains from your point of view. And if you want to  serve the world with multiple chains, how are you going to reduce cross-chain transaction rate except by exploiting natural transaction clustering?
And that's my point - those clusters are most likely mapping to geography. Economic reasoning would very likely make that the case. How are you going to eliminate regulation in that scenario?
@_date: 2015-07-22 14:29:35
I think the big issue is ***branding*** here.
As it looks, bitcoin.org will be very reluctant to even talk about XT's existence. Maybe Gavin might use his power over the core committer list on github as a lever here?
I would think having 'bitcoin.org' and github.com/bitcoin as umbrella websites, listing the different options with equal amount of representation would push the responsibility to build a working, functioning Bitcoin network back to the users, where it belongs.
I also think it would remove the focal point of 'bitcoin-core' and the whole heated debate and nonsense that is happening here as it would make people flock to the implementation that they like best.
Bitcoin/QT and Bitcoin/XT would be very neutral sounding names IMO. If people think that is not neutral enough, maybe SHA256 all the names of the involved committers in alphabetic order together and use the first 4 digits as an identifier? I.e. Bitcoin XT -&gt; Bitcoin/034d or something. I hope this latter part isn't really necessary, as it would mean the level of discourse would have gone so low as children fighting in a sandbox. But I do sometimes wonder now.
@_date: 2015-07-05 15:10:20
    why should that someone be me?


Isn't the correct answer here that analysis shows that there is a fee market, one of those ever-so-important things to exist for the blocklimiters? (And I actually agree it should exist - but I think it does so since the beginning of Bitcoin... )
So far, transaction fees are too low for Miners to consider the transactions, they are much more afraid to lose their 25BTC.
Over time, when rewards get smaller, they'll consider more transactions. But only as many as they can until they run into technological limit.
And I think it can be expected that the technology available to miners will about keep pace and vice-versa with the technology available to full nodes.
Even less of a reason to have a blocksize cap.
EDIT: I do agree with some safety limit though, as is suggesting.
@_date: 2015-07-08 10:34:58
... Interesting! My post, though picked up by many, has a constant vote battle going on. Still only at 12 or so, and if you reload the page, the count changes at least once per minute.
What the fuck is going on here?
@_date: 2015-07-30 18:44:01
They could. Arguably, it would make sense sense to at least have a safety limit common in all nodes. 10x average of last week's of blocksize or so.
@_date: 2015-07-19 20:33:21
And take a look at when you want to see a wholly different perspective on the whole gender relations stuff.
@_date: 2015-07-21 21:33:11
As pointed out, we actually do have a fee market already?
@_date: 2015-07-08 14:58:31
These ideas make so much sense, if only someone would actually listen...
@_date: 2015-07-19 11:14:14


Yes. And this is insanely hard to do here on reddit.  You need to train a classifier on a sample of a userbase that is agreed upon by all parties to be similar to the Bitcoiners here on reddit, and you need to know the real identities pretty much behind each person of your test case, to have a go at the true sockpuppet rate. 
 Just by trying to arrive at this user set, you'll probably bias your data because the true sockpuppets will start to go into hiding..
I think the much more reasonable assumption is no sockpuppetry unless there is overwhelming evidence otherwise. See also [my other post here.](
@_date: 2015-07-02 10:34:09
As you can see on how the debate is evolving, it is likely that there will never be consensus on blocksize with the current team of devs.
The hard fork is the way around this blockade. And every poll so far shows the vast majority wants the blocksize cap to be lifted soon.
It would certainly be nicer if it would happen with consensus, but the interests are diverging too much.
As long as SHA256 longest chain (total amount of work) wins, I think there is not too much to fear from a hard fork.
If hard forks would be that dangerous, Bitcoin isn't that secure anyways.
@_date: 2015-07-08 11:56:36


He is shooting himself in the foot with that argument, actually. Because the network running fine at high rate (up to 400 txn/s, several times paypal!) means a blocksize cap is not needed :-)
@_date: 2015-07-30 08:24:19


I didn't see that one. Thanks for pulling this up! That is actually quite funny and would explain why they even think that they, as the 'self-appointed wizards of Bitcoin' can make Bitcoin work by crippling it and somehow hiding its artificially introduced inadequacies behind a curtain, while at the same time extracting money from allowing a peek through their curtain...
Gavin certainly has the saner perspective in all this.
@_date: 2015-07-31 09:29:11


This is just making the point that full nodes should participate in the fast relay network or their own decentralized variant of it, too.  IBLTs or whatever it is going to be.
And that would mean that miners can only propagate blocks that are well known in the community of full nodes.
No sudden choking on megablocks is going to be possible anymore in such a scenario, regardless of blocksize limits.
@_date: 2015-07-09 22:08:55
For side chains to be effective in reducing network load, you'd need to have them to map actual relations of transaction likelihood between people. Because else you'd need to move amounts between sidechains/mainchains as much as you do actual transactions, and any advantage of sidechains would be lost.
This would *probably* mean that sidechains would map to geographical clusters, as most people do transactions in their neighborhoods.
This would also mean that censorship resistance and decentralization is lost in an important way, because now a sidechain is geographically constrained and thus a lot easier subject of heavy regulation.
So... even if we decide to go this route: Isn't that a change in course that should at least be clearly and openly discussed, including stating the involved trade-offs such as the above?
EDIT: Typo fix.
@_date: 2015-07-24 08:42:19


@_date: 2015-07-01 22:16:07
Exactly! That is what I am thinking about. A  little bit of cost of complexity, but very flexible. And as everyone can already see any RBF change is controversial already. Not as much as the blocksize, but why create a division here if we can have it both?
I haven't updated myself on the current list of Bitcoin script opcodes since a long while, if it isn't existing yet, flagging transactions in various ways could have other uses, too.
@_date: 2015-07-07 09:38:45
VISA and Mastercard will use some distributed immediate replication scheme, too, to keep their transactions safe.
From that point of view, I really fail to see how Bitcoin is inefficient. It does the same, with the only difference being that it is an open standard and everyone can be a credit card processor validation and data store node, so to say. Arguably, it is simpler, not having accumulated all kinds of regulatory and financial cruft along the way.
And regarding mining being inefficient: That part of Bitcoin doesn't really make sense to compare in the light of a payment processing network so much, rather in terms of the efficiency of money creation and care-taking of a money supply. And there are other institutions it competes against in parallel, in that sense - central banks.
@_date: 2015-07-05 15:50:33


What does that have to do with hash rate?


Nope. The problem is that as soon as a block arrives, validation would take time to execute. The mining hardware would lay idle in that time span. The miner rather goes and mines assuming that block is correct, as it will be in the far majority of cases.
No problem so far. 
Only when the miner disables all checks and assumes that incoming blocks are good by default (like in this fork) does he risk losing money by orphaning a long chain that he built on invalid data.
He has an incentive to run a full node to prevent this from happening in parallel, and I guess that after they burned themselves on this with 150BTC lost, they will.


On average, it actually does, because these things happen in parallel. In the scenario we have talked about a full node easily would have cut off the bogus chain. Stuff went on for tens of minutes at least. Enough time for any full node to say: Bzzt. No. Wrong!
With a very high chance (if we assume 15s of validation, p&gt;=97.5% for the whole network of hashing power) the full node is ready providing the 'this is an invalid chain' flag to the SPV miner to start afresh, per incoming block.
@_date: 2015-07-22 19:16:04
The only safeguard against blacklists is a worldwide, massive user base.
And the blocksize cap comes in as a very important parameter in this regard...
@_date: 2015-07-14 10:11:45
That idea is very interesting, but for a very specific reason:
We hear all the time about mining centralization and how it must be counteracted through changes in the protocol. We also hear about the long-term miner economy and how a centrally planned blocksize limit must be put into place to prop it up artificially.
The reason being, that the Bitcoin network will supposedly fall apart completely if not for these measures of economic intervention by the devs.
And what you describe would be a way to squash the stupid arguments along these lines completely! There will only be as much mining centralization as *users* will allow... 
And *users* are the most decentralized entity in Bitcoin. So the argument for centrally planning blocksize is unmasked pretty well as an argument from an 'I know better what you want' arrogant perspective...
@_date: 2015-07-02 09:02:29
Some of the core devs seem to have convinced themselves that Bitcoin cannot possibly work, the wheels will fall off in the end game (lots of transactions with tiny fees), the miners are all rogue and to be domesticated in whatever ways the blockstream guys see fit and that they know the only true path which is a *change of course* for Bitcoin. With the possibility to profit from this altered course.
Because, apparently, the lowly plebs ('sheep') are too stupid and too concerned for their own good, that change has to be socially engineered into place. As in 'use nonsense technical arguments in a *political* discussion about Bitcoin's direction'. [Use double binds.]( [Scare tactics.](
This whole discussion shouldn't even happen at all - as points out, rising the blocksize should have been a no-brainer.
The underhanded tactics is what just drives me nuts.
@_date: 2015-07-02 00:41:46
200MB with just 3 or 4 payment channels per person per *year*. If LN works out at all. Gavin's proposal would at least allow a transaction per person on the planet every couple days.
And you fail to recognize that a centrally planned hard cap shifts transactions fees around between layer 0 and the layers above.
And they are needed - one of the funnily ironic arguments of the blocklimiters by the way - on level0 to secure the network, much more so than any fees are 'needed' on *optional* higher levels.
@_date: 2015-07-05 16:10:06


So even less reason to install a limit? :-)


In any case, you are phrasing it now as if you shouldn't admit that you have been wrong... :-) 


It will always stay safe, unless you want to get ridiculous and talk about scenarios where network txn/s exceeds CPU txn/s. As long as CPU txn/s &gt; network txn/s, a full node will catch up eventually. And CPU power can easily be parallelized...
@_date: 2015-07-07 09:05:18
An O(1) invalid turd is still an invalid turd. And if it is valid, it means most of the network most of the full nodes accepted the transactions, exactly what you want!
Read what writes (he's much more eloquent than I am) about markets for transaction relaying.
@_date: 2015-07-24 08:12:30
Also: Bitcoin is the only coin that could potentially scale very big.
There are myriads of coins already available that can be used as blocksize-limited settlement layers below sidechains or Lightning Networks. If you don't like their parameters, make another one. Heck, tune it for SC/LN use.
But only Bitcoin could scale to become a very big layer-0.
So why do people want to prevent it from becoming that?
This really reeks. A lot.
@_date: 2015-07-08 20:56:36
But even if these lawyers are a scary and bloodthirsty and powerful, bordering on almighty pack - they still are facing the impossibility of pointing to anything specific. All there is is a globally distributed, decentralized network.
@_date: 2015-07-08 18:31:32
Oh I think it will happen. Especially if this whole 'stress testing' continues.
@_date: 2015-07-07 15:50:00


I don't know anyone. But it is a fair point that SPV security should be and can be improved. Gladly, these improvements are orthogonal to blocksize...
@_date: 2015-07-23 14:24:56
The point isn't to remove the limit, the point is to put the responsibility of setting the limit back to the user.
Make it mandatory to set the limit (but without any default specified).
@_date: 2015-07-24 10:33:39


To quote yourself:


Aside from your confusion of speed and bandwidth, this can, if one wants to extract anything meaningful from it, only be seen as a reference to global instantaneous state. If you didn't mean that, your comment is complete BS anyways. So, which way do you want me to see it?


Speed != bandwidth....
@_date: 2015-07-24 17:32:12
I don't think those arguments here are from authority, this is simply about the rules of the game.
About that quote: It doesn't argue for one miner for one person in Bitcoin as it exists, he's says that he was looking for something like that when creating Bitcoin but it simply doesn't exist.
And [in context]( it is pretty clear that he was looking for something that cannot be faked easily and corresponds to something close to the physical process of mining in certain ways.
@_date: 2015-07-09 23:04:41
Exactly. And if only a minority of miners is on some fork, that fork is at risk of a 51% attack.
@_date: 2015-07-08 01:16:05
The whole discussion about blocksize has been skewed. It is simply not effective in preventing an attack on the network.
It at most makes it twice as expensive for an attacker (in the asymptotic limit) and that isn't really a reason for anyone determined to back off.
@_date: 2015-07-01 20:30:38
People will get what they pay for. If they want trust, they pay for a full node.
No reason for you to prescribe anything and draw any lines.
Why do the blocksize limiters always remind me of soviet style central planning committees? 
@_date: 2015-07-16 08:48:57
... only with a blocksize increase ...
@_date: 2015-07-01 20:14:00
Please don't. Stick this fight out. I am still pretty sure 'we' will win this.
@_date: 2015-07-08 01:08:44
[See this. Explains it even better I think.](
@_date: 2015-07-01 22:36:08
Yeah, so?
@_date: 2015-07-21 22:08:18


Ok, what is 'as necessary' then?
@_date: 2015-07-08 09:22:51
[Have a look at this...](
@_date: 2015-07-06 19:40:28
As others have stated, they mine multiple blocks SPV. And that does not make sense in terms of maximizing profit for them.
I'll expect they get a clue as soon as they see the additional orphan rate from these forks.
@_date: 2015-07-01 23:57:40


I think this is seeing it too black and white: They might gain from a staged crippling, or some crippling. For example: Cripple Bitcoin so much to make LN mandatory, create a network of payment hubs, profit from it, and eventually switch of Bitcoin as the unnecessary underlying system. Of course, they won't profit from outright killing Bitcoin immediately.
@_date: 2015-07-09 22:16:15


Hyperbole, much?
@_date: 2015-07-24 17:14:02
And miners want value, they are paid in Bitcoin. Satoshi got the incentives right.
@_date: 2015-07-11 12:40:51
Or we make it a rule that full nodes coalesce old UTXOs together (after some ten years or so, because I think due to caching and technological progress, it won't become a problem before that UTXO set size is reached) and demand merkle branches as proof that a transaction is valid before forwarding.
This would also be a very decentralized policy that could be implemented by every single node. It would make sense, however, to get a consensus on figures (amount of time a UTXO output is stored as-is) as well.
10 years would be a good compromise, I think. With a staged scheme so that someone who wants to save BTC on the blockchain longer than 10 years has to update his UTXO merkle branches leading to his coins once every ten years - or rely on the capabilities and availability of archival nodes (for pay) to do that for him.
The data to save per unspent output would be miniscule - at most a couple hundred kB each.
How about that?
@_date: 2015-07-08 01:29:08
No that is exactly the point.
About half of the load on the network happens when a transaction enters the network.
The other half when it comes back across a full node inside a full block.
Capping the blocksize only prevents the second half from growing, but not the first. Meaning it is completely ineffective at stopping network attacks.
@_date: 2015-07-02 09:33:40
Exactly. As I also pointed elsewhere: What VISA/Mastercard are doing cannot *technically* be too different from Bitcoin's model of a globally distributed, instantly replicated database. 
Because they want to keep their transactions safe and on backups, too.
So Bitcoin cannot be so much less efficient than the VISA/Mastercard network.
HOWEVER, instead of a single party owning all the nodes, Bitcoin opens and specifies the protocol *between* the data base nodes and thus enforces a much more open ecosystem and no central part having control over the network.
Another ridiculous scare tactic by the blocklimiters trying to cripple Bitcoin.
@_date: 2015-07-09 10:58:37
A 'free' market with (completely unproven) side chains because there is a centrally planned, artificial hard cap?
@_date: 2015-07-07 09:47:55
He was cool with it in the sense of not seeing any major damage from it (and I agree), but he certainly wasn't encouraging illegal behavior...
@_date: 2015-07-16 15:53:06
And it should be pointed out that an odd 'megablock' will not kill the network. And this could be prevented against with a running average safeguard limit (10x average or so, to stay well out of the way). And emergency softforks, and ...
I'd be happy with Gavin's proposal, too: It clearly addresses the worries of the blocklimiters but would still allow for an eventually large growth of the ecosystem.
I also would frame your argument a bit differently: I think it is not so much whether Bitcoin might be overtaken by another Bitcoin-like system (a relatively minor worry at the moment), it is rather that Bitcoin is the only system that could fill the 'niche' of becoming a worldwide mass transaction system. Any other coin can fill the niche of 'limited blocksize' easily. But only Bitcoin could become really big.
Shouldn't we at least have a try at scaling it up?
@_date: 2015-07-30 09:03:48


OTOH, miners have an interest in Bitcoin being successful, and I am pretty sure they know (or they'll realize eventually) that this won't happen with 1MB blocks.
@_date: 2015-07-24 13:24:16


Again. You are arguing BS. If you are pedantic about it there is never a consistent state in the Bitcoin network, not even with that completely bogus 7.5txn/s number. Only with 0 txn/s is there ever a consistent state, Bicoin would be dead.
But as you can see, Bitcoin is currently working (though limited by the BS cap) even without such a 'guaranteed consistent state representation'.
The globally consistent state exists in the ledger, with a certain probability. For high probability, you have to wait until your transactions are buried deeper in the chain, and for lower probability, you go towards less confirmations.
@_date: 2015-07-21 19:17:29


I guess emotions run high in this regard and I won't say I haven't been trollish at all ever when talking about this issue...
But 1MB == final is indeed a ridiculous statement.
@_date: 2015-07-30 18:59:55
And I do not believe that. Especially not when we have efficient block propagation and a miner has an incentive to only create blocks that contain transactions that are sufficiently known across the network. 
@_date: 2015-07-21 21:41:35
Short term, I guess Mike has the say but will be careful not to push aside Gavin for obvious political reasons.
If XT becomes successful as in 75% of miners support it, core will follow or it will become a completely ridiculous camp of 1MB blockistas falling into oblivion.
It is the pressure relief valve and will allow users, miners and merchants to route around the 1MB damage.
@_date: 2015-07-01 21:40:23
This is a long and detailed answer and I very much appreciate you writing this, it certainly clarified some doubts from me. Thank you very much!
I also agree that there will be layers on top of Bitcoin in any scalability scenario - at least there will be micropayment hubs- and I also think it is fully OK to earn a living from that.
However, the central point still stands, and I think this is the main worry still in this debate: Is it possible for Blockstream to be profiting from payment hubs and their transaction fees (by running them or providing technical support), and could a change in blocksize or blocksize schedule change the amount of those fees that will eventually result in layers on top of Bitcoin. Can or will it change the amount of fees on layer0 (Bitcoin itself)?
Even after reading your certainly very helpful response, I fail to see that there isn't a possible pretty direct link between the fee structure and amount on layer0 and the fee structure in higher layers on top of Bitcoin - and the Bitcoin blocksize.
@_date: 2015-07-11 18:30:07
That is only for full, unpruned nodes, though. With pruning, it goes down to the UTXO set, and with UTXO set coalescing, that would go down even further. I think we need to get ourselves rid of a 'all transactions since day 0'-available addiction and instead concern ourselves with just a secure set of UTXOs.
And filling 8MB blocks will be more expensive to the banks than filling 1MB blocks.
@_date: 2015-07-23 08:24:30
The practical maximum is what the market will find and not what the developers prescribe.
That said, Gavin's 8MB+ proposal should be a very reasonable compromise for all sides. It seems that most on the increase-side are actually ok with this already strong compromise.
Coincidentally, this is also what is going to be included in XT.
@_date: 2015-07-05 11:40:06


And where is the evidence for this being the long term behavior of affected miners? 


That doesn't follow...


They gained more than the cost of running a full node in parallel that resets their SPV mining back onto the chain?
Please show me the calculation for that.
EDIT: And to make the last part more clear: Show me that the 150BTC that they lost and could have prevented by parallel verification with a full node isn't enough to run such a full node. Or two, as it affected two miners.
@_date: 2015-07-06 22:28:49


Then any mem pools are useless, too, lets get rid of them. Can you see where this is leading?
Again, 0conf is accepting a certain risk profile...!
@_date: 2015-07-24 13:47:36


I didn't say that you say that 0-conf is dead.
Apart from that your list is very appropriately classified as anecdotal evidence.
Note also: Double spends possible != double spends happening widely.
@_date: 2015-07-09 17:08:24
Simulations: Blocks up to 20MB can be validated just fine. [Proof.](
Real world tests: The network can validate transactions at  &gt;400 tx/s. It doesn't get any better than that. [Proof.](
Effects of filling up of blocks. [Proof.]( 
There is, of course, even more data available, if you'd actually care to have a look.
@_date: 2015-07-19 20:16:46
Topdog, I'd expect.... :-)
@_date: 2015-07-23 14:20:38
Yeah, exactly. This is also what I am thinking. It would also instantly kill the whole debate - or at least, shift it to somewhere else. Specifying the limit should be mandatory.
@_date: 2015-07-10 10:21:28
Bitcoin is the only system that has a chance of ever becoming a large scale layer-0. 
Bigger blockcap ~ more transactions allowed ~ larger userbase supported ~ larger network effect ~ larger value of Bitcoin.
And I think this chain is - if you believe in the possibility Bitcoin's success - really hard to break apart, for better or worse.
So if you restrict the block cap, you put Bitcoin into a niche. You will get less mined transactions for sure, but you'll also get a smaller network effect and less total value.
Which brings me to my actual point: It is not hard to either create, fork off or use an Altcoin that has basically the same parameters as Bitcoin has now, but has a smaller blockcap.
Using an Altcoin should be perfectly fine for a user who is interested in a small blocksize cap, because he will get the smaller market cap anyways, in either scenario. He's not losing anything that he or she cared about.
But allowing a larger cap for Bitcoin would allow Bitcoin to fill a 'niche' that only Bitcoin is able to fill: That of a network scaling up to worldwide usage.
Shouldn't we at least give it a try? 
[This is a slight edit of what I have said before [here](
@_date: 2015-07-14 09:22:28
"Blocksize helps against spam" was a well-placed red-herring.
Yet we had *lots of spam*. *Despite* the small 1MB blocksize.
Funny, isn't it?
@_date: 2015-07-07 15:53:18


He's explicitly talking about *scaling* the network, as a payment network, as intended. That is obviously related to blocksize, even though he doesn't put 'block size' into his submission. He also mentions it in the comments. I fail to see how you can read that differently.
@_date: 2015-07-08 00:35:50
Blocksize cap is a red herring. It only makes network attacks at most twice as expensive. Network bandwidth was and is *unlimited* in Bitcoin. Tests show that it survives even paid for attacks.
Except for making attacks twice as expensive per byte of attack data (which is pretty much nothing), all the a blocksize cap otherwise does is crippling the transaction rate for regular, non-attack use.
@_date: 2015-07-08 02:06:12
is right. They are checked right when they arrive. This is actually helpful so that you cannot spam the network by doing something a.k.a.
     socat /dev/urandom  TCP:&lt;bitcoin-node-you-want-to-attack&gt;
@_date: 2015-07-08 02:21:27
You're welcome. Honestly, you've been one of the few so far non-trollish people today...
@_date: 2015-07-03 17:49:36
Exactly. Tunnel vision and black and white thinking.
THE disease in Bitcoin.
Also, Bitcoin apparently works only if it is a network between stupid (small game) psychopaths. Kind of interesting perspective some people have...
@_date: 2015-07-29 20:14:14
And a more expensive attack is a harder attack with any meaningful 'hardness' metric.
@_date: 2015-07-05 16:50:01
LOL. Yeah sure... /s
I don't know whether I should point out that your account has been created just now? 
More transaction capability on Bitcoin == more chance for Bitcoin's competitors to succeed?
ROTFL. Are you fucking kiding me?
This is the way to spin it now?  Try better! You are damn ridiculous.
@_date: 2015-07-08 03:01:58
Economy of scale is a good thing...
@_date: 2015-07-25 20:10:12
The federated hub-centered sidechain can well be used as a payment system on top of Bitcoin that would profit from main-chain transactions to be scarce and expensive in relation to their system.
That's simply how it is. I do not fear profit at all.
@_date: 2015-07-24 23:36:47
Ok. Probably my fault then, too many trolls today. I apologize.
@_date: 2015-07-05 18:25:30
RAMTBF - RAM Through Before it Falls (apart) :-)
@_date: 2015-07-07 19:31:46


There is consensus of the network and then there is consensus of the people involved (or not, regarding the latter). They are unfortunately not the same.


Sounds good, but there hasn't been any movement towards a consensus for months, rather years now...
@_date: 2015-07-03 10:47:42
Don't be silly.
EGO is a much simpler three-letter-explanation than NSA.
The really broken thing about this approach is that he could enable RBF only for transactions that signal that the user wanted them to be RBFable - with a corresponding flag that could be set.
That he doesn't do that despite a lot of user opposition shows to me that he's indeed not a very responsible dev.
@_date: 2015-07-05 21:29:38


It is not working for securing the money supply. 
But I do not think POS in the context of an opinion poll is in any way impossible. But if you have a good argument that it is impossible, please give me a link.
@_date: 2015-07-09 13:23:04
I agree with the idea of trying to work out a consensus, but there is no 'unified' anymore. Blockstream devs are so stubborn on blocksize that this 'unified' broke apart.
Else you wouldn't see what you are seeing now.
@_date: 2015-07-06 19:31:31
Reduction of orphan rate? If their chain is invalid, it will be orphaned... like those three blocks, they're rather increasing their orphan rate wildly if this is going to continue to happen every couple days...
I fail to see how SPV mining otherwise affects orphan rate at all. Only because SPV mining means sometimes mining fast-propagating empty blocks is there any (indirect) relation to orphan rate.
SPV mining keeps your expensive mining HW from idling during those moments when a new block arrives but you are not ready yet validating it - you go and assume it is valid as soon as you receive the header and start running your miners under that assumption, getting a little bit higher effective mining rate this way. With the risk of being on the wrong side of a fork. However, a full node in parallel will immediately bring you back in line as soon as that one is finished validating. 
However, it looks like those SPV miners do not validate at all. Even after they could have easily (in!)validated a block, they keep on going on the wrong chain.
So... what says is correct: At some point they'll figure out that the number of useless sand castles they produce will show up in their bottom line; and it would be cheaper for them to run a full node in parallel that resets the SPV miners should they go off on a tangent, bringing them back on the main chain, making sure that the least amount of work is wasted.
That they do not do that yet doesn't really point to them being too professional and rational yet. Somewhat worrying from that POV, but I am sure they'll soon get a clue.
@_date: 2015-07-08 18:41:35
Ah I see. Thanks for the update!
Kind of a bad situation to be in, as a user on reddit. Basically no real accountability...
@_date: 2015-07-10 08:52:48
But bandwidth wise, Bitcoin is proven to support &gt;200 txn/s right now, &gt;400 txn/s with IBLTs...
***That is multiple times paypal***.
@_date: 2015-07-31 16:21:56
Miners can create fake forks already. But surprisingly, the basic incentives work.
The hard fork does not change the picture as long as your node is selecting the longest (hashpower-wise) block chain. I.e. running XT.
@_date: 2015-07-08 15:20:42
Thinking further about it: What scheme do you propose for selecting the arbitrarily large block safety limit?
Am I also correct in assuming that you could set it, in principle, quite a bit above average rate because those attacks would be rare (and, as I said, I think Miner incentives prevent them from doing this - except maybe when they are hacked or similar), so it averages out?
So a 10x running average cap - enough to not artificially constrain even during rushes, but prevent any sudden TB blocks, would that work with you?
What is your proposal there?
@_date: 2015-07-01 21:44:20
Exactly this. Please, just saying something like this:
"Yes, changing the blocksize might affect the profit of our company"
would be enough to stop all overboarding conspiracy and just make it clear where everyone's positions are in this debate.
I also think you will have a much easier time arguing here on reddit. I think it would actually build trust.
@_date: 2015-07-08 11:23:51
If you delete a submission, does that only delete the contents of the submission? Or also the header/title?
@_date: 2015-07-08 06:16:24


No, I don't. I rather ask the question: In what way are they different?
Disk space? That would be a valid point. Though far from problematic now. So we're talking about *bandwidth*.
And in terms of *bandwidth* attacks, the hardcap is completely inefficient and a red herring.
@_date: 2015-07-24 17:06:35
Well... if the older node is on a fork that has the smaller amount of chain length (in terms of total amount of hashing spent), it can not generate blocks that are valid on the larger chain.
The whole discussion shows one thing: 
It is probably most prudent, as a node operator, ***to simply remove all limits with regards to blocksize*** from the code altogether (ok, leave it at 32MB for the time being due to technical reasons until the networking code has been upgraded) and then simply rely on the network to form consensus on 1MB or 8MB blocks. This way, you are safe. Less assumptions about what comes in to your node means that you'll solely rely on hashpower consensus.
Nice, isn't it?
EDIT: Typo fixed.
@_date: 2015-07-09 12:22:04
    Raising it to 8MB or even 20MB isn't going to break things to such degree that the system can't self correct.


Only risk estimates can be done either way. You do not have evidence that keeping 1MB is not going to break stuff either.... and risk is clear: 1MB is crippling and just 3txn/s, not enough for any serious use of Bitcoin...


    You don't need everyone's permission.


Evidence? :-)


Evidence? :-)
@_date: 2015-07-01 11:01:34
And I have to say, I like the cartoon. :)
@_date: 2015-07-08 22:05:43


Small ones [happen pretty often](


Long delays for blocks propagation may raise the length and the occurrence of forks and that would have severe implications. The main risk with (even temporary) splits of the network is of course an increased number of double-spending.
Yes, there is certainly a risk, no doubt about that. Bitcoin is stochastic by nature.  The last fork of size 6 went on without any reports of double spending. As we have pretty well-synchronized mempools. 
However, I do not think any honest participant has an interest in this actually happening more often. And we rely on 51% rational, self-interested honesty anyways.
I don't think I can completely take your worries away but maybe consider this: 
*Regardless* of actual block size, of hard cap or not, there is a certain physical and failure rate and turnover in Bitcoin full nodes, their network links, their link speeds, number of links, CPU health, and so forth, even linked to price and adoption rate and so forth.
Yet the debate is framed as if there will be a certain breaking point for Bitcoin with regards to blocksize. But I have never seen a meaningful answer on what that should be the case.
It is a continuum. And by *allowing* miners and nodes to move around the continuum, the solution on to how the network should be organized can *emerge* instead of being *planned*.


I think we should be careful (of course), but not fearful. The fear is what is crippling us and holding us back. Given the status of the blocksize debate, I am rather worried that deadlock will cause Bitcoin to crash.
@_date: 2015-07-31 10:49:13
I have a problem with the language around the 32MB limit in BIP100. It makes it sound like 32MB is an ought-to-be. This is very dangerous as agreeing to BIP100 would set a precedent that 32MB is somehow magic.
And I think the supermajority voting (instead of simple 51%) could create a incentive for miners to form &gt;51% cartels to censor out miners that torpedo blocksize changes.  This is just a minor point, though.
Otherwise, I'd be fine with BIP100, too.
@_date: 2015-07-08 02:04:13
Exactly. This is very important. The whole debate has been (spun?) the wrong way.
@_date: 2015-07-08 14:11:30
No you are confused, but that is ok, I was for quite a while, too, due to all the social engineering around. I'll explain it once more:
Quote from  


When someone sends a transaction, they send an inv message containing it to all of their peers. Their peers will request the full transaction with getdata. If they consider the transaction valid after receiving it, they will also broadcast the transaction to all of their peers with an inv, and so on.


And this happened with a rate of indeed 440 txn/s for a while. And this means that the network is clearly able to process much higher rates than what we have now. Because unconfirmed transactions are send around and validated - causing real CPU and bandwidth cost to the network. 
And actually causing the bulk of the cost to the full nodes; Receiving the mined block is at most doubling the number of transactions in the network (as they go around once more in the mined block). The actual validation work is done at that point for most full nodes, as they have cached the validation results in their mempool and can simply look up which parts are valid of the block.
Does that explain it?
@_date: 2015-07-02 18:29:58
What you are saying in other words words is that Satoshi's incentives are enough to keep the miners sane - and thus there is even less need of a blocksize cap.
@_date: 2015-07-09 20:31:33


I don't think there is a lot of damage. UTXO database is cacheable, and as far as I can see, even the blockstream agree here.
That means this spam is going to be just another thing on disk.
The problem is *long term*: The UTXO set is going to stay or grow, even *with* pruning a node. That is why I am arguing for coalescing it eventually.
@_date: 2015-07-23 08:07:30
Because you have widely used nodes in competing countries pointing nukes at each other?
And you would have full nodes all over the world - China, Germany, U.S., Russia, South Africa, ... .
If you think only a small Bitcoin is censorship resistant - how about using an Altcoin for that - only Bitcoin has the chance to grow really big. By crippling Bitcoin, you are preventing it from going that path.
@_date: 2015-07-05 17:28:58
Participating in Bitcoin means accepting the power that &gt;50% of miner have.
They can even completely stop Bitcoin.
See the risk and accept it instead of trying to project it way.
@_date: 2015-07-01 23:28:21
RBF and non-RBF could be signalled with a flag in each transaction. It would give the best of both worlds.
@_date: 2015-07-05 15:12:48


Is this actually true? As long as overall average bandwidth available to those full nodes stays above average blocksize, those full nodes should catch up.
@_date: 2015-07-24 17:11:23
I think Gavin's 75% is a reasonable figure there.
The XT code is completely inert otherwise.
@_date: 2015-07-09 14:08:53
It should also be noted that relay policy is a completely decentralized decision, one each individual full node can do on their own without too much coordination.
@_date: 2015-07-08 11:02:30


Nope, I don't. That is what I was wondering. So it is off the index, but not completely removed from Reddit (as we can still post here).
Is there any rule book for posting to except the sidebar?
@_date: 2015-07-30 08:50:55
In paranoialand where the government replaced all your friends with sybils, the real Bitcoin is actually Litecoin and Putin the president of the U.S. /s
@_date: 2015-07-05 11:29:26
Hey Peter,
I have an eerie feeling of deja vu reading your analysis.  I think I remember reading a very similar analysis some years ago on bitcointalk.
[... when everyone was still optimistic, rightly so, about Bitcoin - and problems were actually to be solved along the way and not elevated to fears of success that lead to 'let's cripple the blocksize' and similar things.]
Again, this is just a deja vu feeling I have - but maybe someone with a similar feeling but better memory/more involvement in bitcointalk could dig up what had been written about blocksize earlier?
Maybe I am also just completely mistaken and your analysis is new.
@_date: 2015-07-22 09:37:03
Wrt. to technology, I think a lot depends on whether we're going to be able to make efficient 3D-stacked structures. 
That said, I wasn't trying to preclude your scenario by saying huge user base / tiny fees. 
If the HW for those TB-sized blocks becomes available, huge user base might have very tiny transaction fees and everything on-chain. If they don't, obviously we won't have TB-sized blocks.
But we might have GB-sized blocks eventually and still tiny (or maybe small) fees for lots of people.
And, sure, LN can certainly help to drop the transaction rate by another 1-2 orders of magnitude, which might be very significant.
@_date: 2015-07-19 13:42:11
Without touching the blocksize issue itself:
Do you think that deleted posts are a large fraction of posts in the blocksize debate?
I have certainly seen some, but not many at all. My gut feeling is: Most of them are buried further down in the discussion threads, are late and are unlikely to be designed to sway opinion.
And ones that are posted early on have a high chance of being replied to - in that case they'll show up as [deleted] in any discussion.
@_date: 2015-07-08 18:45:27


Assertion without any proof.


Not at all. Full nodes are easier to attack with a blocksize cap, because it means that the attacker can recycle some of the money he used for the attack. Because some of the transactions do not confirm. When they confirm and get written to disk, full nodes have a less loaded mem pool and the attacker less money. Easy as that.
And the blocksize cap is completely inefficient from keeping the P2P network from being spammed.
Or how would you possibly explain what is happening now?
@_date: 2015-07-03 12:38:49
It isn't perfect but has a certain risk profile that is acceptable to many.
Stop the black-and-white thinking.
@_date: 2015-07-19 11:26:31


^ this...
@_date: 2015-07-01 23:50:26
And the internet in 2002 easily could have supported hundreds of full nodes world wide. From [Wikipedia](


Thirteen years ago. See also [this response]( I wrote to Adam Back a while ago.
@_date: 2015-07-23 07:48:55
You know what? I am actually thinking it would be best if we get separate dev teams now. Because this will make sure that the QT-side devs (gmax &amp; co.) will watch *very* closely what the XT guys do (in context of the accusations wrt. black/redlists @ Mike Hearn for example) - and so will it be the other way around.
The QT side will use *any* thing they can get to attack the XT side. 
Win-win for the users. A distrust between two big developer parties means that the users profit from the respective other side being *very* watchful eyes on any potential conflict of interest / intent to screw up Bitcoin.
@_date: 2015-07-06 23:32:52
The limit was always meant to be temporary. Satoshi intended gigatransactions/day. And that is Bitcoin's future.
Stop social-engineering the debate now. Thank you.
@_date: 2015-07-17 09:32:26
The point of doing 0-conf is that they *eventually* confirm, which is about the same as credit card security.
Enough for many, many transactions, and there is absolutely no reason to destroy 0-conf to force through full RBF. 
Also, full RBF could be done by flagging transactions to be full-RBFable.
@_date: 2015-07-07 09:26:06


On the other hand, *growing* the userbase and size of the network means that these $50k/day increase as well, and as the block reward is going to zero in steps, I can well imagine there being a nice equilibrium with a (very low) fee market for the miners but most of the hash power in use in a Bitcoin success end game scenario.
And the amount of hash power will probably not be insanely big in view that Bitcoin would be a worldwide payment network. Which would be nice from an ecological point of view.
@_date: 2015-07-29 20:46:30
The point is that I have seen him arguing and I *have* listened to and read his arguments. Here and elsewhere. And I am simply pointing out that there is no need at all to put him onto a pedestal, given his behavior at times.
@_date: 2015-07-18 12:16:51
We are far from the physical limits yet, though. And today's hardware could well support full nodes for all of humanity - with very big ones in data centers, obviously.
This *might* cause centralization, and this is an arguable POV. But please do not conflate technological/physical limits with your personal vision on what Bitcoin should become. That is dishonest.
@_date: 2015-07-09 21:20:50


Why so? The access to UTXOs follows a pattern. More recent ones are accessed more likely. A cache should be able to see this pattern and optimize accordingly? 
@_date: 2015-07-08 17:12:37


@_date: 2015-07-19 17:00:40
@_date: 2015-07-24 13:15:47


And that is an assertion without proof.
And I am actually fine with a couple bigger nodes in data centers in different jurisdictions.
@_date: 2015-07-23 08:11:55
The problem is that it looks more and more like arrogance instead of an honest misunderstanding.
The most 'uber, 1337' Bitcoin channel is called ' ...
@_date: 2015-07-21 21:29:20


But that's my point: When XT becomes the 8MB+40%/y variant, it makes sense to present all choices to the user.
@_date: 2015-07-05 11:35:29
Supposedly people who want to increase blocksize are the CIA and the powers that be in general and want to centralize Bitcoin very badly so it will be used as another central control and surveillance method.
Ignoring, of course, that if Bitcoin is successful, it will be a worldwide success. And the CIA's say in the network will be constrained to the U.S. For example, russian nodes will be able to run however they please (or, at least, however Putin sees fit).
Honestly, I think the usual business of conflicts of interest (Blockstream, ahem...) is a much more common and actual threat than worldwide conspiracies involving waaay too many people and different jurisdictions.
@_date: 2015-07-08 18:52:16


Interesting wording given that the other side stalls forever and shows no signs of moving an inch... whereas Gavin came repeatedly down with the numbers in his proposal?
@_date: 2015-07-24 09:13:42


And Bitcoin is at least a money supply *in addition* to a transaction processing system...
Mining for example, is mostly about the money supply.
Other than that, the network of full nodes represent a quite efficient replicated, redundant data store.
VISA/MC will have similar schemes in place to keep their transactions safe.
@_date: 2015-07-06 21:45:42
The problem isn't SPV mining. The problem is SPV mining in sequence. And that makes them lose money. As intended.
@_date: 2015-07-05 16:25:05
We'll see. I am ready to run XT on my node, as soon as the patch goes live...
@_date: 2015-07-02 00:37:45
Understood. Very valid view and it helps to keep Bitcoin safe by slowing down crazy ideas before they are indeed validated and thoroughly checked. Unfortunately, blocksize limiters wrongly label themselves as 'conservative', too...
@_date: 2015-07-08 02:54:19


Security which does not exist. Did you miss what I posted?
@_date: 2015-07-07 13:33:13
Ok. Think a little more about it:
Coinbase and BitPay are listed as supporters of a blocksize increase and have voiced their support on their respective twitter feeds, and maybe at a couple places elsewhere.
The equivalent situation you describe would be listing Blockstream as an opponent of the blocksize debate and refering to their writings, tweets about it, simply that, nothing more. And I am sure people have already put them onto their 'opposing' lists.
But now we have core developers with access to github.com/Bitcoin among other things, and at the same time in a company that indeed would profit from a long term restricted block size, using their power to block consensus. And being very active in trying to steer Bitcoin, not just acting as stewards for the system.
Do you see the difference?
@_date: 2015-07-07 14:43:53
I know his writings and plans, as far as they are public - do you?
@_date: 2015-07-01 11:45:59
Is anyone you know working on a proof of stake for this?
I have a measly amount of Bitcoin compared to many others, but I am here also to protect my investment.
And I would be *really* curious about how such a proof of stake would turn out. I expect it to be *widely* in favor of a blocksize increase.
It just needs to be made *very sure* that no private key data leaks out.
@_date: 2015-07-14 13:23:54
Users are the most decentralized entity in Bitcoin. They can decide how centralized Bitcoin can become already, by incentivizing small miners - if they want to.
That they don't do that yet at large scale means that they disagree with your idea on how dangerous so-and-so much centralization is for Bitcoin.
The blockcripplers want to *enforce* propping up the smaller/inefficient miners.
And with regards to efficiency: Bitcoin is a worldwide replicated database. VISA/MC must have similar schemes in place to make sure their transactions are not getting lost. So I fail to see how people call it inefficient all the time.
@_date: 2015-07-24 10:00:47


Not disagreeing. Optional layer on top, could turn out very nice.
But lets stop crippling Bitcoin itself first.
@_date: 2015-07-09 13:13:46
This is a somewhat indirect conclusion, but I think I agree on this. 
Because widespread wallet estimation code would start to develop the fee market between miners and users and so would create the necessary equilibrium there?
I still think that after the blocksize cap is raised, we still need to (among other things, of course):
- make the range of what it means to be a full node even wider (that we have pruning now is great), by for example allowing to start a node from a committed UTXO set. 
- implement an easy way to pay for various full node services - relay and storage.
Note that there is in principle no problem with paying for full node access now, either by running one out of your pocket or by paying others. It is simply  not possible to do a very fine grained decision, leading to inefficiencies.
@_date: 2015-07-23 21:40:42


One can agree to that. I think LN will be a very nice *addition* to Bitcoin, though. We should counter the bogus argument that it is an alternative to the blocksize increase, but shouldn't let ourselves be divided (any more than we are already) and conquered on this. LN looks like a neat system in theory.
@_date: 2015-07-10 11:31:59
It should be noted that one should do an update and do this procedure again when XT with the actual hardfork code comes out.
@_date: 2015-07-10 11:24:57


 
Exactly. Have e.g. bitcoin.org state: Bitcoin is now two Bitcoins, Bitcoin/QT and Bitcoin/XT. Those are the differences [blocksize]. It is up to you to decide.
And then let the free market decide - and we can still all be friendly to each other - and all people involved before the split have coins on both sides of the fork.


I do think we are past the breaking point. But I surely hope that you are right.
@_date: 2015-07-09 12:19:39
Exactly. Does not help to keep spam off the network.
@_date: 2015-07-19 15:44:20
How about making RBF possible by making it possible to flag transactions to be RBFable?
That would keep 0-conf alive.
To maybe change your view: This is not so much about trust itself, this is about merchants being able to see a certain risk profile of 0-confs, and agreeing to it.
If someone markets 0-conf as 'safe outright' you might have a point describing him as pro-Trust/anti-Bitcoin.
But I think the point is rather that P.Todd is putting up straw mans of such people not only existing but this being a widespread misconception reinforced by people who should know it better.
If being careful and knowledgeable about 0-conf, I do not see a big problem with it. So I also do not see a reason to kill it, especially when you can go and make full-RBF available *optionally*.
@_date: 2015-07-31 09:10:10
SigNfe is a freshly created user by the way...
@_date: 2015-07-24 10:24:19


Citation needed.
@_date: 2015-07-24 14:05:55


Given the varying interpretations of 'consensus', this alone doesn't really make anyone any wiser..
@_date: 2015-07-10 08:46:18
Furthermore: How the heck is a node in the U.S. going to censor the blockchain when a node in Russia is not playing along because Putin has a bad day and does not want to give in to Obama?
The only way would be through hard forking an U.S.-only coin off. And with a large user-base and international widespread use of Bitcoin, that will not happen. A single country will be out of luck trying to mess with the blockchain.
@_date: 2015-07-07 19:26:47


Being nice, so far and still trying to work things out?
as far as I understand, would be fine with a little bit rougher approach.
Seeing what is going on here, I can't fault him for that view.
@_date: 2015-07-07 13:47:53
If that check means a full node putting them back onto validated footing, I think stuff is going to be fine.
@_date: 2015-07-09 08:49:17
Fair points; would still mean upload and download sum to zero, but with different usage patterns.
Makes me think that eventually, full nodes/full node will become a paid service - 'FNAAS' ;)
@_date: 2015-07-24 08:09:02


@_date: 2015-07-19 11:05:04
I certainly wouldn't go quite as far as that. 
There are some problems with the analysis (see below), but at the core I do see a ***large problem with the presentation of this!***
I have to say, though, that it is certainly a lot of work that has been
done. The result of this work is valuable, but only in the in the sense 
that it *could give potential candidates* for sockpuppets and might be a 
reasonable approach agreed on by all sides to reduce the search space, but it 
is not so much valuable as giving true bounds on the number of sockpuppets - which is the way it is
presented/marketed here.
It should be made *very clear* that this is just a raw candidate list of possible
sockpuppets, and I have to echo sentiment here (and is
by the way very much opposing me in the context of the blocksize debate!): 
The presentation
should be a lot more careful with accusations, given the *very noisy* output of the analysis
[BTW: I like to point out that I had my own try at doing some data
analysis in the context of the reddit Bitcoin user sockpuppet debate. My
analysis is
I was unlucky in terms of exposure here on reddit, and my post didn't get as much upvotes as the very first shot
at the issue by However, already paid 2.5BTC of the bounty to me for that analysis, and I think he
also paid another 2.5BTC. Which leaves the 5BTC from
BitGo which AFAIK have not been paid out yet.]
The main problem is that any analysis cannot possibly give a lower-bound
estimate on the number of sockpuppets. The way analysis presented here, 'up
to 85% accuracy', is as good as saying 'this laundry detergent removes up to
99% of stains'. It might well remove zero, while the marketing is technically
Consider the two extremes: Assume everyone is a sock-puppet a priori. The model 
sockpuppet(user)=true gives you 100% precision and 100% recall.
The other extreme: No user is a sock-puppet a priori, the model is sockpuppet(user)=false - again
100% precision and 100% recall on the corresponding ROC 'curve'. 
Yet those two models give you two very different answers on the sock puppet
debate - but they just mirror your a priori assumptions.
You simply cannot infer a certain lower bound on the fraction of sockpuppetry 
from the data itself, without making assumptions on the specific behavior of sockpuppets. 
From the analysis:


99%,for predictions on a uniform sample of the test set. Given our goal, this is
clearly not useful.
And here the bias unmasks itself, if you read carefully: The 'goal' seems to
be to be able to scream 'manipulation confirmed' eventually, by tuning the
point on the ROC and the sampling of L0/L1 until the desired results appear.
My bias is different: Now, I wouldn't expect there to be *no* sockpuppets in this debate (for any
reasonable definition of sockpuppet). But I'd never shout out 'manipulation
confirmed'. And also for the sake of keeping the debate civil, I would argue
someone is a sockpuppet only when there is *very* clear evidence of
I think this is much healthier in not letting everything devolve into a
fight of random sockpuppetry accusations, dividing (and conquering?) the
user base. 
Be watchful, but don't be paranoid.
That all said, I think applying machine learning the way did
makes sense in giving a list of *potential candidates* of sockpuppetry - which
then needs to be followed by careful, manual, human analysis. Without
normalizing the found correlations to any bogus probability values, it is
helpful in spotting *candidates* that can then be further looked at.
And, by the way, if you do that, you'll end up figuring out that there are a
lot of false positives in this list - exactly as describes. ***This
should rather give credence to the idea that maybe sockpuppetry isn't that
big of a deal here yet. If the most likely candidates from machine learning
do not withstand a manual inspection...***
Last, I wonder what the time frame of the collected data is. I might have
just overlooked the relevant paragraph. Recently, and
this is just personal anecdote, I indeed found that I interacted with more
users here that are 'reddit new users'. I tend to ignore those when they are too trollish, though, and I certainly now have a list of frequent blocksize posters in my mind - people that I take serious - a whitelist if you will. I guess this is similar for other participants in this debate?
@_date: 2015-07-22 19:22:11
I am not familiar with btcd at all - but it looks like this configuration file does only set the soft limits?
[They did some testing of bigger  blocks though.](
I don't want to put any words into their mouths, but the post itself suggests like they might indeed be completely fine, maybe even encouraging larger block sizes.
@_date: 2015-07-05 10:37:26


How do you eject the majority of hash power off the network?


But rather with few transactions in crippleblocks? I can see billions of tiny transactions eventually creating enough miner revenue - I cannot see 3txn/s doing that...


Huh? The miners who did the SPV mining lost money by creating an orphan chain. Incentives work out to not detach from the validating full node network....
@_date: 2015-07-08 14:45:02


I understand that - but consider that the definition is arbitrary at some point: If I make a very long, valid chain in my basement with my superminer, it would be the longest valid one, yet no one would think it is or has anything to do with Bitcoin.
So it needs to be public in some sense. And if you think hard about it, you'll see that all kinds of imperfection comes into the definition at that point.
I admit, I am a bit sophistic here; my point is, I guess, that Bitcoin is as much as social consensus on an idea (or meta consensus) as it is a technical consensus. And see no reason to give the consensus about blocksize to the core devs.
Note also that 'objective' means an agreement.
EDIT: And in the case of the guy quitting the game - to illustrate my point again: That the guy quits because he isn't able to handle the load anymore (which also is unlikely to come from just a single block; that block would need to be so big to cause a _long_ delay in processing), means that he's simply put out of the network.
Similar things happen with a full node dropping off now because it cannot accept 1MB blocks. That 1MB blocksize is *an ought to be for everyone* is purely political.
@_date: 2015-07-08 08:25:26
Not at all. The capacity of the network is clearly &gt;200 txn/s, according to the stress test.
 The blockcap does *effectively nothing* against preventing attacks on the network and using up bandwidth.
 
The problem is that you need to refocus your mind onto what is actually going on to see this fundamental truth again.
@_date: 2015-07-21 15:08:27


I don't think this is a large issue. If there is pressure to have larger blocks, the miners will vote adequately - and if not and the blocksize stays mostly the same, there is no harm done.
@_date: 2015-07-08 00:30:26
[See this discussion.]( Now that I think about it, it should kill the blocksize debate for good. It also addresses the testing - Bitcoin has been tested with much higher transaction rates already.
Blocksize being something important for health of Bitcoin is a red herring that we all believed in. 
@_date: 2015-07-10 09:41:47


We'll see. It will be a lot more money to spend to *fill* the blocks and then, we ran without an *effective* cap most of the time just fine. Miners can find the natural maximum blocksize that they can economically support...
EDIT: And, *of course* the spam problem is fundamental and will stay *unsolvable* in case someone has enough money to spend on causing the network to work on his transactions. *This* needs to be kept in mind all the time in this debate. The question is how much harm the spam does cause. And it causes more harm with a small blocksize cap.
@_date: 2015-07-09 23:28:15
As much as possible *total*. As seen from the miners.
The least possible, *total*. As seen from the users.
And in the middle, somewhere, they'll meet.
This will probably result in a high security chain. Due to all the value stored on it.
@_date: 2015-07-23 08:52:44
I think propping up both forks will do the opposite: It would create confusion in the market place, dropping the value of both chains.
So IMO people will generally refrain from doing this.
@_date: 2015-07-08 01:20:32
Sure they can? The rest of the network is validating and gossipping the transactions, the only thing that they don't do is receiving a block with those same transactions back from the miners.
But the load on the full nodes is still there...
@_date: 2015-07-05 10:13:29
And it should be pointed out that peer review is *not* being in the position of core dev on github.com/bitcoin and blocking consensus for whetever reasons...
@_date: 2015-07-09 17:20:22


Agreed, but we are talking about raising the blocksize cap to react to the increase in adoption. So the 'all other things equal' part does not hold.
@_date: 2015-07-16 16:01:20
I think point is that an agreement on blocksize isn't possible anymore. Maybe we could get consent for a clean 'divorce' into two blockchains (QT: 1MB, XT: 8MB+). But I think there is no way anymore for a consent to happen that will make everyone happy.
@_date: 2015-07-09 11:31:04
Tempting to think that, but it won't. The hard cap only keeps actually mined transactions down, not spammy transactions going through the network and bogging it down.
During these recent attacks/stress-tests, the network successfully processed transactions at [a rate of up to 441 txn/s](
@_date: 2015-07-01 11:05:35


Can be solved long term with UTXO commitments and/or coalescing old unspent outputs. Probably should be solved this way eventually.
@_date: 2015-07-07 09:40:23


Why so? Please see my other comment.
@_date: 2015-07-09 19:34:55
Ah ok. Well, then, thanks for the credit for giving you an idea :D
@_date: 2015-07-08 08:22:50
Nope. You are completely making this up. Bitcoin is a worldwide network and no one yet tested any legalities with regards to different transaction types respecting that situation - and there is *no one* to point to, either, because the whole thing is completely decentralized!
Are you going to *sue Bitcoin* because you didn't like you are only the 2nd transaction in a blog?
LOL. If the lawyers would own Bitcoin now, it would be dead. Gladly, this didn't happen and if we are careful, it won't ever happen.
@_date: 2015-07-22 14:08:51
My hope is - and I think Gavin's behavior so far also shows that he actually wants to remove all devs (including himself!) from ecosystem wide decision making as much as possible - that the blocksize change is the last controversial thing to happen with Bitcoin.
After that, I hope the ecosystem of Bitcoin protocol software fractures so much that no single entity will be able to mess with Bitcoin's implementation easily anymore.
@_date: 2015-07-08 02:03:15
Sure. [Here you go.](
Also, just think about it: Would it make sense for a full node to simply copy any bits arriving on one of its TCP sockets onto any other ones?
Of course not. A full node is so important (again, the blocklimiters agree here, as you might know :) because it actually validates everything.
And why should it validate individual transactions only when they arrive in a block?
Exactly. It doesn't. It tries to do as much validation work as it can right away.
AFAIR, a full node will only validate those transactions in a block that are genuinely new to it but pull the already validated-as-good ones from mempool.
 
@_date: 2015-07-15 19:40:17
You at least have to lock in your money to be able to use it - and you have to select a payment hub AFAIK. I have not seen a 100% decentralized hub concept anywhere yet.
And the LN people suggested 133MB for an access rate of *twice a year* for worldwide usage. I don't think locking your money for twice a year is really a usable LN. With Gavin's proposal @ 8GB, access rate would eventually be every couple days a transaction per person on the planet instead, that looks a lot more reasonable.
Do not get me wrong - I think LN are a great *addition* to Bitcoin. But I have yet to see that they are 'just a caching layer', as AFAIR Adam put it.
They simply aren't. It is a different layer on top, it needs integration with wallets, it is complexity and so forth.
Nice to have, but lets please make and keep Bitcoin itself usable first.
@_date: 2015-07-04 19:32:44
So, as I said, it has a certain risk profile and it isn't black and white.
Now, again, why do you want to prescribe users and merchants how to behave and whether this risk profile is acceptable?
Especially since it is in large use?
@_date: 2015-07-01 22:12:47
How about letting users flag transactions as non-RBFable, FSS-RBFable or fully RBFable?
This would allow everyone to decide on their own what is best, and avoid this being another (smaller) holy war in Bitcoin.
@_date: 2015-04-12 21:55:06
If you have a decent amount of money on your HDD, you should not sigh but still do the damn backup now and do as I described above.
Get help from a trusted friend who knows what a bit-by-bit image of a HDD is.
Sorry if this is sounding harsh, but the first thing you ABSOLUTELY have to do is STOP messing around with the device that carries your precious data. That means the HDD that your Bitcoins are on. Stop turning the computer  that those BTC are on. Keep it off and offline. Make a f'ing image of that HDD. THEN, look what you can extract. I believe there are tools that scan disk images for private Bitcoin keys. If not, ask someone to write it for you - depending on the amount of money lost.
At best get help from one of your friends who is hopefully some trusted Linux/UNIX and all-around computer geek.
With the amount of 'meh' and 'sigh' you display here with regards to losing 'life savings', you actually start to sound like a troll. No offense meant.
@_date: 2015-04-26 01:57:18
Well, it's very hard to further explain a 'they don't care', isn't it? You are very soon at that point very digging deeper doesn't really add anything...
@_date: 2015-04-12 15:54:45
Ah I see. Well, for the keeping up part, I think Gavin's projections are mostly right.
@_date: 2015-04-22 18:06:15
That's why the fork will only come into effect when a supermajority of the network agrees that it is able and willing to handle the new block size.
Add to that that the fork will be scheduled well in advance, and you'd be very ignorant of Bitcoin to miss this completely. Also, people who are running full nodes need to keep up somewhat with the developments in Bitcoin space anyways.
But you're right, there is a risk of loss of money. So proceeding needs to be done very carefully and (virtually) everyone needs to get the info about the coming hardfork.
@_date: 2015-04-18 20:32:14
I agree and I would say that POW is somewhat like a much saner version of 'proof of violence'.
The weapon is hash power and the battle is solely economic. 
Other than that, there are no additional weird 'rules' that could be either circumvented or taken advantage of. Many of the other proposed systems (POS etc.) are very difficult to balance in terms of incentives.
It is simple and it works. I agree that it might be wasteful in the long term though. I'd rather see mining coffee heaters and the like instead of farms full of mining hardware (but still a much better alternative to piles of weapons!).
I could even envision a situation (in a very long term positive scenario for Bitcoin) where I would support regulation/outlawing of mining data centers - because it would make sense in strengthening the decentralization and attack resistance of the network.
@_date: 2015-04-12 12:39:28
That's logically part of the assertion of the parent's poster:


Emphasis mine.
@_date: 2015-04-24 08:01:09
Thank you - but this isn't really him saying it. I am not too good at digging through twitter, do you have a tweet by himself that says that?
@_date: 2015-04-12 15:21:30


I think you misunderstood me here. I am proposing that the originator of the transaction supplies the full proof that the transaction is valid. A transaction (request) would look like this:
I want to spend UTXOs x,y,z: Here are the merkle branches leading to every input used in my transaction, referenced to block number H. You as a node know the history of the last N years. Block height H is part of that history. As a node, you can check that none of these inputs have been touched since block H until the current head of chain. Therefore, you can execute this valid transaction.
 
Nodes would keep, lets say, 5 years of transaction data. UTXO validity proofs would need to be updated by coin holders at least as often as that - or a rare, full archival node could be consulted for a fee.
EDIT: This could also serve as an incentive to run a node: You could add a certain list of addresses as a watch list to your node and ask it to keep the merkle branches leading to the UTXOs that you have under control up to date.
@_date: 2015-04-19 08:43:45
So how are you going to order society according to excellence, without resorting to violence?
@_date: 2015-09-20 10:18:06


You are doing an estimate about the future which might be right, wrong, or anything in between.
But assume that your scenario really turns out to be the case, ***why won't you let the free market run its course then?***
As someone else said: Constant factors matter. If transactions are very cheap, Bitcoin with O(n ^ 2) full network can well be competitive. And O(n ^ 2) would mean lots of full nodes which means a lot of decentralization.
***What the heck is the reason then to constrain it artificially, please?***
If you want to make any sense at all, you must be against a block limit, as Bitcoin is self-limiting.
@_date: 2015-04-12 15:50:23
If incoming transaction rate is eventually high but constant (Bitcoin saturation case), a constant length of history would mean a constant amount to store and process. So except for block headers, the whole problem becomes pretty much O(1) over time in storage space and computing time. 
I consider this an immense improvement.
@_date: 2015-09-20 10:11:54
And if the number of full nodes increases with the number of users, that is not a reason for a scare, that is a reason for being cheerful because it would mean the whole 'big blocks mean node centralization scare' falls apart. Yet Adam comes and says he's afraid of breaking the whole internet - how ridiculous is that, please?
The small blockistas can't have it both ways. Essentially, they are arguing against growing Bitcoin, which is *not in the interest of Bitcoin*.
@_date: 2015-04-30 12:03:23
Is this really possible with Bitcoin? I mean, the problem with Gold is that delivery is expensive and cumbersome. With Bitcoin, delivery is (almost) trivial. So if someone creates too many Paperbitcoins, it would be easy to pull the rug out from under their scheme.
@_date: 2015-04-22 20:05:37


Funny that you say 'keep' instead of 'get' ...
@_date: 2015-04-18 19:28:33
Lets hope can explain this more. NDAs signed by 'the Bitcoin chief scientist' would indeed be interesting dynamics.
@_date: 2015-04-10 08:12:07
How about putting the Trezor on battery power?
That would at least work well against the 'malicious USB-port in internet cafe' scenario.
@_date: 2015-04-18 00:27:17
Yes, but as he explains on his blog, he also envisions a gradual growth formula with the same exponential factor, not a jump every two years.
@_date: 2015-04-12 15:44:48
Well, but just keeping N years is scalable, isn't it?
Regarding sending the merkle branches for 'current' transactions, you're right.
I don't exactly remember my line of thought when I thought about that scheme. I believe a reason to do that was that you could also somehow compress the untouched-for-five-years UTXO set into a single hash on the nodes - should the UTXO set grow too much. I have to think about this once again.
@_date: 2015-04-06 08:31:36
I thought trading has started already? Why is no one selling for 355.10 $/BTC? Is it for some reason hard to thaw the shares of GBTC and sell?
@_date: 2015-09-20 19:28:59
I still fail to see - from an *engineering* perspective - what is wrong at all about Satoshi's initial plan. And that's why I believe the small-block side is pretty close to concern-trolling.
The big block side has a lot of engineers and engineering-types, too. For example is a physicist, and a computer scientist by training/education (if I am not mistaken).
@_date: 2015-04-12 15:11:58
Making the amount on the address part of the input to the transaction would prevent replay issues, wouldn't it?
I was not even proposing addresses and balances as the underlying mechanism. I just fail to see your argument that 'it doesn't scale'.
 
@_date: 2015-04-28 21:13:57
I kind of agree. One great thing about Bitcoin is its transparency.
Everyone CAN fully know how the ledger works, and all important constraints on this money system are visible and in the source code.
I would also argue that transparency of the protocol is completely orthogonal to any idea of 'bitcoin privacy'. 
It could be pseudonymous like it is now, or it could implement zerocash-like ideas. But as long as the core structure of the network and the rules for transactions are public and there is enough of a free internet to support free Bitcoin protocol discussion, it will stay transparent.
@_date: 2015-04-18 20:33:21
But democracy is certainly better than a situation where 'the better argument' equals the bigger gun... isn't it?
@_date: 2015-09-20 19:52:56
He described SPV - and ways to make it even more secure. 
Users might have alerted him, yet the system works well enough. Also e.g. wrote a couple of posts on very doable things to have more secure SPV. I think I remember others having similar ideas, but I remember his being the most well-thought-out so far.
@_date: 2015-04-24 08:04:59
Wait until they arrive at feminist frequency trading :D ... 
@_date: 2015-04-12 12:42:27
What about this:
Together with changing the protocol so that transactions have to eventually prove that they are a correct mutation on the UTXO merkle tree by providing a branch leading to a root that is less than N years in the past (for a sufficiently large N)?
That would make bitcoin's space requirements constant, except for the VERY slowly growing block headers. (less than a CD-ROM full of data in a century)
This would also avoid the 'ZK proofs are moon math' issue.
@_date: 2015-04-12 12:37:59
I thought bitcoin-core does have an internal UTXO index lately, too?
That's basically storing all adresses with non-zero balances.
Putting the burden of proof for any transaction on the originator of the transaction and just keeping the merkle root of the UTXO set for older data (lets say more than a couple of years) might actually be VERY scalable.
See this: 
@_date: 2015-09-20 10:30:20
No the burden of proof is on those who say that we need to centrally steer and *constrain* something that would run into *natural* limits.
All these system-wide big-O(...)scaling arguments are utterly inadequate for reasoning against a blocksize increase.
They rather point to people wanting to artificially limit Bitcoin's growth and thus value.
@_date: 2015-04-07 20:11:47
Timeline for upcoming protocol changes would be nice, if you can see through the fog enough to make a reasoned guess.
I am mainly talking about 'megablocks' but also two-way pegs.
EDIT: And thanks by the way for your awesome work.
@_date: 2015-04-12 21:49:36
Agreed. To the OP, a hardware wallet would probably help you to stay safe with your coins, too.
@_date: 2015-04-18 00:22:37


I think this has to be pointed out repeatedly to the '1MB is golden forever'-crowd: Blocks are not full even now! How the heck can that be if all this '1MB theory' supposedly shows that they should be absolutely full of meaningless spam transactions?
It can only mean that there are other incentives than the 1MB hardlimit that keep the transaction rate down. I can't really point out completely what it is, but the effect is real.
And that also makes me very confident that raising the block limit according to 'Gavin's growth formula' will be hardly even noticeable at first - and is the right way forward.
But if we stick with 1MB blocks, Bitcoin will stagnate, and then vanish.
@_date: 2015-09-20 19:43:57
And how the heck do you put people into this algorithm?
Transaction rate per user won't scale with O(n). Evidence says it is something like O(log n). So total network load per node will be O(n log n). Very doable, and there is nothing one might add, that says that O(n ^ 2 ) is always impossible in practice. It then really depends on the constant factors and size of the problem set involved! ***Heck, even exponential time algorithms are used in practice!***
The only algorithm of relevance in this is the gossip protocol, which gives you the O(n).
@_date: 2015-04-25 20:02:14
Deeper into what?
I'd say dig yourself out of the feminist propaganda hole...
@_date: 2015-04-24 07:55:41
Is it? If I invent quotas and other things to get women into the positions that 'they are capable of achieving'.. I am not really believing that they are that capable without the quota, am I?
@_date: 2015-09-20 19:49:42


In the context of a particular complaint (see below) this might be a valid criticism, but the sweeping statement of 'a series of cheerleading posts' would be in any case an unfair generalization here.


I don't see that. Care to point that out?
@_date: 2015-04-19 08:42:28
Added unnecessary complexity is a ***damn good reason*** not to implement something!
@_date: 2015-09-15 12:18:54
Interesting. There is the danger that this gets even more centralized and ivory-tower like, though.
Good thing is that there at least do not seem to be any BS-affiliates on board?
@_date: 2015-04-23 21:12:57
As in, he's a SJW? Do you have a link to something specific he said about this?
@_date: 2015-09-20 10:49:59
And BIP101 is exactly that compromise. Good chance of always keeping it within reach of hobbyists or small businesses. Remember the social contract and Satoshi's vision is Bitcoin scaling to bigger full nodes in data centers - another perfectly fine and reasonable outcome for Bitcoin.
@_date: 2015-09-20 09:55:22
Are you intending to troll? *Of course*, the number of transactions per user is scaling *sublinearly* with the number of users in the network. Saying that means 'that users use Bitcoin less when it gets mass adopted' is arguing against a straw man. A sublinear *growth* of transactions per users does not mean *less* transactions per user.
Assuming sublinear growth is not incorrect, it is common sense, backed by facts. Assuming they scale linearly with users is insane. Something like O(log n) could be expected, maybe. See also [here](
And wrote the very same, *correct* statement into his blog post.
@_date: 2015-04-12 15:03:53
Much better:
1. leave your computer turned off
2. Take HDD out of computer
3. Put HDD into external case
4. Seek guy/gal who is knowledgeable  and trusted enough in using dd and similar tools on Linux 
5. Do a bit-by-bit image of your HDD onto a new drive
6. Look at new drive with various undelete/forensics tools
7. Learn to use offline wallets!!! (EDIT: And proper backups!!)
If your wallet is indeed worth that much, the extra time and HDD
is probably worth it. Running any kind of tool on your current HDD risks losing data (=coins!). Even by just using your computer you risk losing your Bitcoins. Stop it now, turn it off and make a full disk image first!
EDIT: And when you do the dd, ***double check where the if= and of= goes***! Speaking from experience.
 
@_date: 2015-09-20 10:19:33
How about *letting it run into those supposed limits then*?
No need to do *centralized steering* then, or is there?
@_date: 2015-09-20 11:29:44


Thanks for being honest here @ keeping Bitcoin small. Bigger full nodes are still not centralized, though, or else the whole Internet would need to be called centralized...
And any altcoin can do your scenario. Why so keen on restricting Bitcoin - the only one capable - of filling that 'niche'?
To keep the price low? Is it that?
@_date: 2015-09-20 10:21:41
Indeed. This is so tiring here on Come over to the [redacted] subreddits...
 In this context, see also [this](
@_date: 2015-04-12 15:32:57


Ok, I must admit here I am not too deeply informed about ethereum - but I thought it had a bitcoin-like 'coinbase' mechanism for initial coin distribution, too?
How is such a script then valid in Ethereum?
This sounds like saying: "I just make this transaction that has no inputs and gives me N coins" and somehow I can execute that over and over again?
That surely can't be true?
@_date: 2015-04-18 20:26:15
Sure - I am just pretty curious whether he has any ties to any commercial interests that are so strong that there are NDAs involved.
@_date: 2015-09-20 10:08:58
It is OK even with yesterdays hardware and bigger full nodes in data centers - Satoshi's vision.
Gavin's BIP101 would make sure that there is a very high likelihood of Bitcoin always being within reach of a dedicated hobbyist or small business.
That is, however, not the needed nor intended decentralization scale for Bitcoin. It is, instead, the result of social engineering of a bunch of folks who apparently want to suppress Bitcoin.
@_date: 2015-04-30 11:26:26
Well, but in that case, the other side could wait 10s and see whether anything odd happens on the network!
EDIT: I think it is also completely ridiculuous that people would do elaborate fraud (double-spend) for something like micropayment access to an article.
@_date: 2015-09-22 21:42:57
***Per user***. Nothing wrong with that.
@_date: 2015-04-30 22:39:32
Isn't such a crisis happening mostly because trust is evaporating, catalyzed by the trust in the 'big-safe-government-money' evaporating?
I mean, sure, having your life-savings evaporate because the underlying asset disappears is certainly nasty.
But I think it is much more nasty (and much more likely to be the cause of starvation), if supply chains etc. become dysfunctional because there is no usable way to pay each other.
I could see Bitcoin being helpful in such a case, rather than detrimental or even destabilizing.
@_date: 2015-04-25 20:03:46
Insane how strong the feminist brainwashing is everywhere, isn't it?
@_date: 2015-09-20 10:03:39
So, are you fearing Bitcoin's growth and want to keep it artificially small?
That's kind of the gist of the whole small-blockist argument.
Very transparent, one might add.
@_date: 2015-09-20 10:27:49


No, I am aware of that and I am not scared. There's the other part which is more users = more potential full node operators. Honest would be to admit that no one knows *for sure* how full node count will develop.
In any case, I am personally not scared of somewhat dropping full node counts and I am definitely not scared about increases in node counts (the idiotic breaking-the-internet scare). The problem is  that the small-blockists do a double bind argument and say 'more transactions -&gt; less nodes -&gt; centralization!1!' and at the same time 'more transactions -&gt; more nodes -&gt; O(n^2) scare!! -&gt; bitcoin doesn't scale will break!1!'.
And that is disingenuous as hell.
@_date: 2015-09-22 09:13:04
You could do that ;-)
Well, I think - as someone else said - constant factors matter, too. O(n ^ 2 ) scaling is very doable if the constant is small and the problem set size n likewise. Saying O(n ^ 2 ) is never practical is saying that it is never practical to nest two loops.
Any image processing algorithm operating pixel-wise is O( n ^ 2), if n is the edge length of the picture. Yet, those are in use daily.
Gladly, we have something like O(n log n) (that is the best guess, also from the discussion of Metcalfe's law I linked above) for each node and O(n ^ 2 ) for the whole network *only if number of full nodes correlate linearly with number of users*. 
Again something that is questionable (and as so many are worried about full nodes, would be something good) but a) whole network is not a problem as no single user pays for the whole network and b) the scaling constant would be small (else people wouldn't complain about lack of full nodes and we certainly have more than 6000 Bitcoin users for 6000 Bitcoin full nodes).
All in all, the whole O(...) discussion is clearly settled as not being a problem in principle and that's what's important. And in practice, and seeing that we have very capable hardware even today and that there is a long way between a measly RasPi node (that apparently still can be used as  a full validating node) and perfectly valid big full nodes in data centers (as foreseen by the social contract), we have so much room to grow.
@_date: 2016-10-17 11:08:03
And even if you assume the exponential would have flattened a bit. The cap is in the way, loud and clear. There's no sane argument around that. And LN vaporware is /not/ a sane argument.
@_date: 2015-04-12 19:24:09
Ok, now I see the issue. Thanks.
@_date: 2015-09-21 14:26:23
Fair point. But you can always argue that O(log n) (and small-omega, too) has a constant and small upper bound (only so many people on the planet).
That's what he's alluding to with his 100 other people he transacted with. 
EDIT: And in all this it should be noted that the (log n)-part is harmless and the most benign of them all - arguing about O(n log n) vs. O(n) is pretty much nitpicking and won't change the scaling picture at all. 
@_date: 2015-09-21 08:27:42
       
    My number of transactions will rather be something like O(log n) with n users.


Quoting from TFA:


So you clearly put up a straw man and false statement ('O(n) [..] like Gavin claims') and argued against that. If this style of discussion would happen occasionally in the block size debate, that would be fine - we're all human after all. But it happens *a little bit too often* to not be trolling and derailing.
@_date: 2015-03-20 07:22:42


Shouldn't the burden of proof be the other way around?
@_date: 2015-03-16 21:44:26
^ this
And lets have sidechains for those who want to go the extra mile.
@_date: 2015-11-26 22:13:56
Only for ridiculous scenarios of attack. And at that point, you could as well imagine the whole internet lying to you and Bitcoin actually being Litecoin.
@_date: 2015-03-25 16:52:01
So let me get this straight: You are one of those folks thinking we're going to succeed with 1MiB blocks?
@_date: 2015-03-10 23:29:45
Well said. I think how the Y2K problem is seen in hindsight is another example of the psychological effect you describe.
@_date: 2015-11-21 17:22:10
EDIT: Oh, dammit, wrong subreddit :D
@_date: 2015-09-20 10:38:03
Sorry when I was obnoxious, I had too many arguments with small-blockistas to be too patient now.  
With regards to Metcalfe's law, [here's your proof.](
In short: Just because a country suddenly gets a bank system doesn't mean I'll transact with each and everyone in that bank system. My number of transactions will rather be something like O(log n) with n users. However, the *potential* user-user relations of course go with O(n  ^ 2) - and that is a good thing in any case.
@_date: 2015-03-19 21:08:01
Now? A lot. But I'd be optimistic here, too: Silicon is just reprocessed sand and right now, a large amount of money is needed to recoup the initial cost of building fabs. So even if there is not much progress wrt. transistor shrinkage anymore, I'd still expect price / GiB to drop.
@_date: 2015-03-21 18:56:41
They have ready-made binaries for pretty much all platforms of interest on their download page. I am using syncthing since a couple months now and am pleased, although it still has a couple of kinks here and there. (Not affiliated with them)
@_date: 2015-03-10 16:33:43
Gavin wants a final limit at 20GiB after 10 doublings. So no ever-increasing blocksize at all.
I am pretty sure that ***really*** high end hardware (cluster + SAN for verification and storage + dedicated fiber optic link for transmission) can handle 20GiB blocks today. So I do not think it is a stretch to say that this will be consumer grade HW in 20 years.
@_date: 2015-03-13 17:03:52
Maybe rather side chains soon to be able to absorb all new ideas that might help in more privacy features for Bitcoin?
Is there a schedule for any of the required soft-forks for sidechains?
@_date: 2015-03-10 23:20:00
Ah I see. Well, 2^42 / 600 s ~= 7.3 GBit/s. Certainly a lot, but there are already home connections offered in some countries at 1GBit/s. 
Considering that 1000x 7.3Gbit/s can be pushed through a single fiber with today's technology, I am pretty optimistic that we will get that factor 7 in bandwidth over the next 20 years, too.
@_date: 2015-03-25 13:15:34
I actually like to see a/the sidechain softfork ASAP, am somewhat worried that otherwise, Bitcoin will lose to other coins with more privacy soon!
@_date: 2015-11-26 09:31:13
Put a hash of the UTXO into each block...
@_date: 2015-11-27 05:51:26
No, that is not what I mean. Even Bitcoin has a trust root, that is the genesis block and consensus on what a valid transaction is.
Can you offer a realistic scenario where *I have to* validate the entire chain?
@_date: 2015-03-13 16:49:09
Ok, how about this:
We have 128GiB, i.e. ~10^12 bit micro-SD cards today. A micro-SD card is very roughly about 11 * 15 * 1 mm^3 = 165 mm^3. That is a data density of about 6.2 Gbit mm^-3. Stacked together into a cubic meter, that is 6.2 * 10^18 bits. Which would be about 27 years of full blocks at maximum rate.
@_date: 2015-03-17 12:51:49
And that stability is a good thing.
@_date: 2015-11-29 12:40:09
Kind of sad that we have to work around against the useless entropy introduced into Bitcoin/BS by ptodd?
@_date: 2015-03-16 21:43:18
It should be noted that Bitcoin ***as it is right now*** fully supports CoinJoin already.
Wallet support is lacking. But all the necessary infrastructure is there.
@_date: 2015-03-10 16:38:05
Shameless repost of something I wrote a couple weeks ago:
***Bitcoins and a bar of chocolate - Or: How I stopped worrying and learned to love larger blocksize***
A bar of chocolate is about 100g. The element carbon has a molar mass of about 12g. Assuming chocolate contains mostly carbon, it thus consists of about 8.3 mol carbon. One mol of carbon is 6.022 * 10^23 carbon atoms. Thus the bar of chocolate is about 5 * 10^24 carbon atoms. If we assume that technological progress in terms of storage density would eventually allow us to create an addressable bit per million carbon atoms, the bar of chocolate could thus store about 5 * 10^18 bits. I consider a million atoms to construct one addressable bit to be a conservative estimate of what technological progress in data density could do.
With Gavin's Growth Formula, the maximum block size will be 2^34 bytes, or 2^42 bits.
A block will be produced once every 10min, thus a total of 6 * 24 * 365 = 52560 blocks will be produced in a year. This amounts to a maximum of 2^42 * 52560 bits of data in a year, about 2.3 * 10^17 bits.
The hypothetical bar-of-chocolate data storage would thus be good for (5 * 10^18 bits ) / (2.3 * 10^17 bits per year) = 21.7 years.
Mind you, this is assuming filled blocks and no further progress with the software side of Bitcoin, such as various schemes placing most of the burden of transaction verification onto the sender.
@_date: 2015-03-21 19:05:07
And honestly, I think it is overdone. Making clueless people aware that Bitcoin exists != loudly promoting BTC at all times in all conversations. 
I think the former is a much more healthy approach, because, hey, I like BTC because I informed myself and understand that it is a very thoughtfully designed, even though still pretty experimental money system. I believe slow organic growth of the ecosystem is healthier than a sequence of hype cycles.
But somehow, the latter seems to be unavoidable with Bitcoin.
@_date: 2015-03-25 14:23:48
So? What does this have to do with my worry that Bitcoin might very well be out-competed through coins providing more privacy?
@_date: 2015-03-01 18:35:32
As Natanael_L said, either ZK proofs or 'certified inputs'. 
A valid transaction isn't valid just by signing some data with a key and showing that it is a valid signature. Valid transaction wrt. Bitcoin also means it can be included in a block and that means its inputs are valid, too - which means storing the UTXO set at least and better yet, the full chain since day 0. For a 'zero trust node', that means storing all Bitcoin history forever.
Putting the burden of proving a transaction valid onto the originator of the transaction would take that pressure away from the Bitcoin network and make nodes much more light-weight.
@_date: 2015-03-10 16:37:05
A scaling limit has been been discussed in various places - but having the simplicity of a fixed formula outweights the benefits IMO. Also, look around what Gavin and other devs wrote.
@_date: 2015-03-21 21:20:32
Bills have traces of cocaine on them ... do you reject them, too?
Also: What prevents someone nefarious from spamming other addresses with small amounts of 'bad BTC'?
Remember, you can not consent to being sent Bitcoins...
@_date: 2015-02-19 14:00:37
You former assertion might be true (I don't know) but your latter one sticks out:


Care to explain what exactly official authority is in the Bitcoin ecosystem?!
@_date: 2015-02-02 19:03:20
Gender is not stopping anyone from (not) participating in Bitcoin. 
@_date: 2015-02-02 19:16:42
Thanks for keeping a level head in the debate. I do agree w/ Gavin wrt. his increase schedule (which still limits the blocksize, only with an increasing limit!), but the doubters have some very good points.
Also, I have not seen Greg Maxwell argue for 'keep the 1MiB limit for all times', rather 'lets be very careful'.
 
@_date: 2015-02-03 10:45:54
There is bitcoin testnet. And sidechains are being implemented, I heard...
@_date: 2015-03-30 17:51:55
I think this will happen only when and coincident with Gavin getting a good agreement on the increasing-the-blocksize issue. Which I also think/hope will happen eventually, but I actually think the blocksize risk suppresses the price quite a lot. We'll see.
@_date: 2015-02-03 10:09:57
My terminology might be out of date then - CoinJoin in Greg Maxwell's initial post on bitcointalk.org AFAIR only considered how it was to be done on on the blockchain - nothing about whether servers are involved or some distributed scheme.
So replace CoinJoin with CoinShuffle wherever you like.
@_date: 2015-03-21 19:21:59
That's actually much more than I would have thought - ~12k SPV nodes on average.
Do you see a daily or weekly modulation in the number of SPV nodes connected, like there is in the rate of transactions? If there is a similar variation in SPV clients as there is in spending rate, that would suggest to me that 'manipulation' of the txn rate is either very sophisticated or not being done (much).
@_date: 2015-02-09 16:53:54
I would like to see a hard-fork protocol change that would make the merkle root of the UTXO set a mandatory part of the validated block header beginning with a certain Bitcoin version. Right now, this wouldn't really change very much, but it would allow verification schemes for transactions that do not rely on having the full blockchain history available.
@_date: 2015-02-09 11:54:51


In terms of protocol, this would only need Gavin's blocksize growth formula, don't you think?
Of course, you'd then have to make a couple million trezor-style devices for people to securely transact coins.
@_date: 2015-03-28 11:42:07
If you know yours has been fiddled with - isn't it best to NOT REFLASH IT AT ALL, but instead carefully disassemble it / read it out and look what happened to it?
I mean, this is a chance to get an insight to what is happening here, and by reflashing, you are potentially throwing all interesting evidence away.
@_date: 2015-02-01 12:06:24
What is construct, what is biology?
@_date: 2015-02-13 01:52:44
What do you think of schemes like [MTUT]( and putting the burden of proving that a transaction is valid onto the creator of the transaction?
Do you plan to make it a rule eventually that the merkle root of the UTXO tree is validated in the blocks?
@_date: 2015-02-04 17:06:47
Thank you. Is there some example code somewhere, though?
@_date: 2015-02-02 09:47:44
Most men are entitled losers? Which group are you comparing to?
@_date: 2015-02-09 13:33:24
That's why Gavin proposes a staging scheme that depends on a node seeing enough other messages/nodes with a new version to switch itself.
But is correct, this is far from risk-free. That why IMO we should be doing it soon.
@_date: 2015-03-17 12:51:10
The difference is that changetip works on top of Bitcoin whereas Paypal works on top of USD/EUR/..
Only in one case it is trivial to get your money out in tokens that you have full control over.
@_date: 2015-03-11 11:38:51
With the Russians and Chinese also building their competitors to SWIFT, Bitcoin looks more and more like a common neutral ground that all sides could use.
Very interesting times...
@_date: 2015-03-01 18:40:41
True, but I think the gist of the OP is that contrarian arguments would make for a more illuminating, lively debate. You don't get that with agreements/upvotes.
So I am very much in favor of seeing such a thing.
@_date: 2015-02-04 19:51:49
Yeah, as they said in the video - I guess I am somewhat impatient.
Quite interesting stuff is happening with Bitcoin!
@_date: 2015-02-03 10:12:35
Whatever sexism exactly is nowadays... anyways: Human nature is decentralized. What is your point?
@_date: 2015-02-10 12:16:16
**Bitcoins and a bar of chocolate - Or: How I stopped worrying and learned
to love larger blocksize**
A bar of chocolate is about 100g. The element carbon has a [molar
mass]( of
about 12g. Assuming chocolate contains mostly carbon, it thus consists of
about 8.3 mol carbon. One mol of carbon is 6.022 * 10^23 carbon atoms. Thus the
bar of chocolate is about 5 * 10^24 carbon atoms.
If we assume that technological progress in terms of storage density would
eventually allow us to create an addressable bit per million carbon atoms,
the bar of chocolate could thus store about 5 * 10^18 bits. I consider a
million atoms to construct *one* addressable bit to be a conservative
estimate of what technological progress in data density could do.
With [Gavin's Growth
the maximum block size will be 2^34 bytes, or 2^42 bits.
A block will be produced once every 10min, thus a total of 6 * 24 * 365 = 52560 blocks
will be produced in a year. This amounts to a maximum of 2^34 * 52560 bits of data in a
year, about 2.3 * 10^17 bits.
The hypothetical bar-of-chocolate data storage would thus be good for
(5 * 10^24 bits ) / (2.3 * 10^16 bits per year) = 21.7 years.
Mind you, this is assuming filled blocks and no further progress with the
software side of Bitcoin, such as [various schemes placing most of the burden of transaction
verification onto the sender](
@_date: 2015-02-12 13:22:14
Thanks! Log scale would be helpful for the earlier times.
@_date: 2015-02-06 09:00:07
Where did that happen? Tech-heavy bay area?
@_date: 2015-02-26 08:25:38
Just reread them all. Care to point out which ones? Pretty harmless discussion in here...
@_date: 2015-02-25 21:56:19
Many people have come to Bitcoin through the Internet, without any personal interaction at all. So I would argue the reason that just a few women are involved in Bitcoin can not be explained by 'too many unpleasant people around'.
@_date: 2015-02-03 10:47:30
Huh? Increasing the block size is a change in protocol. Allowing more script types is a change in protocol.
Certainly, the main economic variable, 21e6 coins total, is an integral part of the social contract and is not going to be modified.
@_date: 2015-02-02 18:58:14
Huh? Bitcoin is as decentralized as the Internet and noone is stopping anyone from doing anything. This is all very egalitarian in terms of participation.
The only reason I see for patterns to emerge that are different from the markup of the population is (lack of) interest in certain subgroups.
That's neither good or bad, just the way things are...
@_date: 2015-02-04 14:08:57
So in a way, sidechains would be able to customize their security guarantees against that robbing to occur from 0% to almost 100% bitcoin security, right?
Sounds like sidechains are a good idea then?
@_date: 2015-02-25 21:59:18
Feminism is whatever you believe in - the all-encompassing religion. Totally worthless, except for using it as a divide-and-conquer strategy by TPTB.
@_date: 2015-02-03 10:08:08
Well, wait a bit, but lets say 10000 people shuffle 1BTC each repeatedly and I only use that 1 BTC for a single pseudonymous transaction afterwards - then I am only linked by 1/10000th?
Of course, if you mix your old change and your shuffled coins, you are out of luck. I fail to understand why this is impossible, though. Move a certain amount through the shufflers, and then keep that amount separate and whenever you spend it, repeat shuffling with the rest.
@_date: 2015-03-25 13:55:33
Care to explain the downvote?
@_date: 2015-02-05 15:18:01


And is the addiction support group... :D
@_date: 2015-02-09 11:40:14
There might be a tipping point when:
A central bank realizes it can get a larger share of wealth by buying, adopting and pushing for (still cheap) Bitcoins rather than hyperinflating its own currency and therefore eventually printing itself into oblivion.
So, viewed in that light, it could happen right before a CB in a country with sufficient internet penetration would have to otherwise make the decision to hyperinflate.
@_date: 2015-02-09 16:50:36
I like the idea of Greg Maxwell and others to have the consensus rules for blocks executed/tested/validated with a formal specification on a VM - and let the rest of Bitcoin be as flexible as needed.
@_date: 2015-02-03 10:44:29
In my world women have agency, are not special snowflakes and are not put off by use of the word 'wallet'.
@_date: 2015-02-12 23:55:53
Interesting. From my observation, itt seems that the sentiment of the people in the E.U. would rather match a trajectory where the E.U. falls apart, though...
@_date: 2015-02-04 14:21:10
So is there a plan on when about we'll see sidechains as a patch to the 'official' client?
@_date: 2015-02-10 03:21:15
In theory, you could then have full nodes with practically constant space requirements, if the way of doing transactions would be changed so that anyone doing a transaction would have to prove that it is valid by including a merkle branch to the unspent outputs each for a recent-enough block header. (And the full node would have an index to know whether the referenced coins have been spent between 'recent-enough' and 'top of chain')
This would also require one to regularly update one's UTXO merkle branch information from the network to keep the coins valid, which I think makes such a change unlikely to be ever accepted.
But it would be great if schemes like these could be supported by hashing and validating the UTXO. As maybe in the more distant future, there could be only a dozen or so archival nodes worldwide.
@_date: 2015-02-04 16:59:00
Thanks! Do you have a link to an implementation/details of the federated version?
@_date: 2015-02-02 16:53:39
Well then... what exactly is the problem with coinjoin?
@_date: 2015-02-10 03:11:32
Maybe this is a good place to you ask then: Are there any other changes besides the maximum block size that will arrive with the upcoming hardfork?
@_date: 2015-02-20 11:31:00


Working on getting out of the rat race to do exactly things like that. Maybe Bitcoin will help and thus bootstrap itself ;)
@_date: 2015-02-03 21:58:22
Honestly, I don't see much of a problem either. Except for a few mild gender stereotypes in this very discussion thread, I do not run across too much, especially nothing that one would consider outright hostile.


    this is why we need to bring bitcoin payments to hookers
I actually think he has a point. Paid porn (arguably somewhat similar to prostitution) could be a big thing for Bitcoin. 
@_date: 2015-02-03 21:00:30
That's one way to see it, but even a softfork would be a change in protocol. Wrt. blocksize, I believe Gavin intends to implement it in the 'official' client by staging the introduction of bigger blocks with a client-version vote.
@_date: 2015-02-09 16:47:54
The O(1) block propagation is not really touching the core of Bitcoin, though. Just how nodes can talk to each other about new transactions. So I am not concerned about rolling this out later.
@_date: 2015-02-17 09:39:44
If you introduce other people to Bitcoin, maybe recommend a more simple to use light wallet.
I think it is good to know how bitcoin core works and how to download and that it is possible to download the whole chain. As in 'that's the system, here you can get the full details, all the history, the global village's ledger'...
But for most people, though, something like Electrum suffices 100%.  Most use firefox instead of wget most of the time (myself included), but it is good to know that wget exists.
@_date: 2015-02-14 11:09:19
Notice the language: "fears". As if they are somehow weak and have to fear a lot.
@_date: 2015-02-02 17:37:28
I understand that this would be even better. But with a large pool of shufflers, I fail to see why after a sufficient number of shufflings, you would not end up with a low taint level of 1/(number-of-shufflers-in-pool) asymptotically.
@_date: 2015-02-28 21:50:18
I think Bitcoin eventually still needs to change to a model where the originator of a transaction supplies proof that it is valid.
That would indeed be scalable in every way.
@_date: 2015-02-19 13:56:20
I wonder whether eventually the main client need to do mutual, statistical 'proofs of history' or similar to exclude these nodes and keep the network functioning.
what do you think?
@_date: 2015-02-03 10:28:42
Excuse me, are you serious?
@_date: 2013-11-21 04:14:57
A much bigger worry is that there is a 1MB blocksize limit which is about 7 transactions/s. And there seem to be a growing number of dumb people who argue 'bitcoin is just this awesome *store of value* and I want to run my own node no matter what'.
If that sentiment doesn't change or prevails too much, it will kill Bitcoin.
There must not be any static blocksize limit.
@_date: 2015-02-12 17:29:29
With regards to storage cost, let me shamelessly repost something I said in an earlier discussion:
**Bitcoins and a bar of chocolate - Or: How I stopped worrying and learned
to love larger blocksize**
A bar of chocolate is about 100g. The element carbon has a [molar
mass]( of
about 12g. Assuming chocolate contains mostly carbon, it thus consists of
about 8.3 mol carbon. One mol of carbon is 6.022 * 10^23 carbon atoms. Thus the
bar of chocolate is about 5 * 10^24 carbon atoms.
If we assume that technological progress in terms of storage density would
eventually allow us to create an addressable bit per million carbon atoms,
the bar of chocolate could thus store about 5 * 10^18 bits. I consider a
million atoms to construct *one* addressable bit to be a conservative
estimate of what technological progress in data density could do.
With [Gavin's Growth
the maximum block size will be 2^34 bytes, or 2^42 bits.
A block will be produced once every 10min, thus a total of 6 * 24 * 365 = 52560 blocks
will be produced in a year. This amounts to a maximum of 2^42 * 52560 bits of data in a
year, about 2.3 * 10^17 bits.
The hypothetical bar-of-chocolate data storage would thus be good for
(5 * 10^18 bits ) / (2.3 * 10^17 bits per year) = 21.7 years.
Mind you, this is assuming filled blocks and no further progress with the
software side of Bitcoin, such as [various schemes placing most of the burden of transaction
verification onto the sender](
*EDIT: Typo.
@_date: 2013-11-22 02:44:03
I do see your point! 
My troll remark was about the reaction to show the absurdity with my nail clippings, and I take it back. I was trying to show that there is a difference between nail clippings and Bitcoins:
I cannot attach value to nail clippings the same way that I can attach it to Bitcoins, *because they are simply not the same*. So in other words there is something that makes Bitcoin special. The same way that Gold is special. You can both see them as some goods to trade and abstract those differences and say well, people trade stuff with value and here I have XYZ theories how they do it. Up to a point, that works, of course. You can explain a couple of things. But eventually you will be talking about the price of Bitcoins, rather than its value.
I think it boils down to that 'all abstractions are leaky'. And this I think very much applies to Bitcoin, too. What Bitcoin can do, and how it operates, is what in the end defines a big part of its value.
@_date: 2013-11-21 03:37:59


I totally agree. **However** the number of people seem to rise that see a value in 'building payment networks on top of Bitcoin' and keeping the number of transactions at a rate like the current ridiculous 7/s.
I am very fearful that this sentiment will kill bitcoin if it gets stronger/prevails. We need to get rid of the transaction cap if we want to scale to payment networks and beyond. And I think it is ridiculous to assume that Bitcoin would serve as any kind of 'reserve currency' for 'bitcoin banks/bitcoin payment networks', to be used for clearing once a day between bigger institution or so. The existing systems can do that well already. Bitcoin is not needed in that scenario. Bitcoin would be great if it can be used as something that is the closest approximation of cash in the digital world.
@_date: 2013-11-21 22:50:40
I don't think that that is a good idea. Coupling it to the difficulty means that you would *need* a certain difficulty for a certain number of transactions. That is another artificial limit. And whatever formula one invents basically set the transaction fees in stone, rather than allowing it to be determined dynamically from what the actual hardware can handle while only avoiding the orphan cost.
I think the orphan cost in the end should be the only thing that is limiting the number of transactions. It is a natural, hardware-based limit and allows to scale the network to whatever hardware is available to throw at the task.
An 'n times median blocksize since last difficulty' rule would only limit the dynamic growth of the number of transactions, and that only to prevent 'big block spam'. Preventing spam was the only reason for having a blocksize limit to begin with.
I think that adding rules to artificially create a 'market for competing transactions, by having a limited block size' is not creating a market but crippling Bitcoin artificially to get more miner fees. But that is very short sighted. Miner fees will come in from the eventually vast number of transactions being confirmed, not by propping the fees up artificially. That will only kill Bitcoin because the network cannot grow.
@_date: 2015-02-08 00:49:01
True, but interestingly enough there is still an upward trend:
@_date: 2015-02-14 12:02:59
Also of note should be that blocksize is/was varying already by quite a degree, so lifting a limit that was meant as a temporary spam prevention method is a whole other thing (with limited, foreseeable consequences, especially as it won't be set to infinity but to 20MiB and growing) than changing the 10min time, which I think should never be done.
@_date: 2013-11-21 03:57:24
I think it would be a lot better to replace any static limit with a growth rate limit like a formula:
block size limit is n-times the median of the block size since the last difficulty interval, with n some value 2..10
If the blocksize gets upped to another static value, we might run into a scenario like it seems to happen right now, where the people who start to think there is some intrinsic value to bitcoin as a store of value (rather than the way it is, that the ease to transfer bitcoins gives it its value) start to argue crazily for economic incentives for a capped block size.
Like I said in another thread:  A 1MB limit, 7txn/s means 30 years of average waiting if all 7 billion people would use Bitcoin. Any static limit of the block size has to go.
Bitcoin's idea is to be used as an electronic cash replacement. Building stuff on top of Bitcoin is fine, but replacing credit cards is one of the major advantages of Bitcoin, and it would get lost if people start paying a significant fee for a $5 coffee. And having just a payment network on top of Bitcoin means Bitcoin would be dead, too. There is not reason to use Bitcoin as a clearing house rather than using the systems the banks currently provide already.
@_date: 2013-11-19 07:02:29
Also, as noted elsewhere, Bitcoin's bandwidth would be low enough even at VISA transaction levels to be able to run a full validating node as a rather determined enthusiast. That's democratic enough for me. The rest will be dealt with by the (albeit slowing) Moore's law and similar increases in CPU/network/disk over time.
I think we need to have some kind of algorithm like the following, which has been discussed and seems very reasonable to me:
At each difficulty increase:
(1.) determine median block size since last difficulty increase
(2.) Multiply by factor 2..10 (or something similar), to allow for transaction bursts like Christmas shopping
(3.) Set the new block size for the next difficulty interval to be the result of (2.). Make any change smaller than a certain percentage (~ +/- 20%?)
I saw ideas like the above being discussed. Having such a dynamic limit would guard against spam. Median instead of average to have more stability. What came out of them?
Again, I think it would be extremely foolish to hard-limit the blocksize to a fixed value longterm. That's basically saying: 'Bitcoin needs to stay small'. And that is basically saying: 'We desperately want to be killed by an altcoin, or ripple, or google wallets or whatnot, eventually'. 
@_date: 2013-11-19 06:47:19
Another thing which just came to my mind:
I am a little bit afraid of the dynamics of bitcoin. Most people buy because they speculate it WILL become a major currency. I am afraid that the number of transactions will rise violently, as soon as it actually IS a regular currency. And if the network runs into any kind of block size limits then it might crash it forever.
@_date: 2013-11-19 06:28:29
And I have the impression that your sentiment might very well kill bitcoin if it becomes the prevailing one.
Money needs to be liquid. People dislike banks and their controls. People like to use the QR code bitcoin wallet. That is how I introduced others to Bitcoin, that is how it is being used for payments. One can explain, can show that there are no middle-men. Offchain transaction right now means basically centralized institutions. EXACTLY what bitcoin tried to avoid.
Some people now seem to believe that the scarcity of bitcoins is enough to make it an investment vehicle. I fail to see how scarce numbers can be worth anything when they cannot be easily and cheaply transacted.
@_date: 2013-11-19 08:29:53
I disagree. Many merchants like to have no middlemen (VISA etc.) that take fees/delay transfers etc. A properly scaling Bitcoin would eventually allow for a huge market to exist for a well specified product (Bitcoins), with the validation of Bitcoin transfers as an extremely competetive market between the miners.
In a scaled variant of Bitcoin, it would not be necessary to switch between VISA, Mastercard etc. 
I really fear a future where a Merchant has to put a sign outside his door:
'Coinbase, Bitx, bity accepted here. Sorry, no American Express/Discover cards'
... and due to some weird scheme of vendor lock-in, you either have a Coinbase, or a Bitx 'card', or whatever, but not both. Like for example, earning points with your card, having incompatible readers etc.
I would like to have just 'Bitcoin accepted here', as it currently is. Having that globally, /that/ is the potential of Bitcoin.
@_date: 2013-11-21 05:12:23
Fully agreed.
I watch Bitcoin since 2009 and I read the initial paper and its prospects sucked me in. And I started to watch and compare the community a little between now and then. And I am quite a bit scared by people who argue for economic incentives for a statically capped blocksize (I think a dynamic cap like 2x median blocksize since last difficulty or so against spam makes sense) gaining a lot of mindshare. When I read the paper, I saw Bitcoin as exactly that: A great way to disrupt money transfer and payment networks.
There is no reason to even use Bitcoin when putting a payment network on top. That works as well with the current electronic payment schemes. And having a scaled up Bitcoin is like the internet - a simple (in relative terms) protocol layer which can scale to a very high degree.
Of course, I am not arguing against being *able* to build stuff on top of the blockchain. I am just really, **really** worried about the talk about keeping blocksize caps in place.
The sentiment of limiting the transaction rate apparently comes from some romantic notion that everyone needs to be able to run a full validating node. I think it is totally fine if *smaller groups* are able to run a fully validating node, if they want the trust. And I think tens of TB per year of disk space and tens of MBit/s of bandwidth is entirely doable nowadays for smaller groups/enterprises. And that would be &gt;1000 txn/s. More with better hardware over time. But no hard limits in the protocol, please.
@_date: 2013-11-20 06:40:30
I am talking about $50. And I know about the Java RNG problem. I am not talking about storing 100BTC or anything like that.
@_date: 2013-11-18 07:04:38
The MAVEN Mars mission is going to launch to tomorrow. That's kind of fitting :D Hopefully, there are two launches :P
@_date: 2013-11-21 04:54:11
I think that 'official release' thing is exactly the problem. I think the protocol is already set in stone almost and changes become exceedingly difficult. That's why I think bumping the block size to another static amount is a very dangerous thing to do.
@_date: 2015-02-05 22:17:55
Link, source?
@_date: 2015-02-05 23:52:57
Thank you!
@_date: 2013-11-19 08:33:48
And wrt. the clearing system: That exists right now. What exactly would Bitcoin offer that is so much better than the existing clearing system?
@_date: 2013-11-21 03:42:14
I can't agree more. There seems to be a rise in the number of 'gold 2.0' people who also seem to overlap a lot with the 'transaction cap is good' crowd. That's just insane. If we keep at 7 txn/s or any other static limit, Bitcoin will never be able to scale dynamically/globally and WILL be killed by another competitor.
7 txn/s means **~30 years** average per transaction if every human on the planet wants to get a transaction in.
And, **NO**, dear 'blocksize cappers', it does NOT make sense to 'create a competitive market' by intentionally crippling the growth of Bitcoins transaction level and saying 'well, if you pay a high enough fee, you will get in'. Bitcoins absolutely need to be as easy as cash to work well. And paying $5 fees for a $5 cappucino is a crazy prospect - and it won't happen because Bitcoin would die first.
@_date: 2013-11-21 06:02:12
Not when the blocksize and transaction rate is artificially limited like it is right now and not when people keep arguing for keeping it limited artificially.
I fear more and more that this is the next issue that actually decides about Bitcoins future. And I fear that if there is any static blocksize limit that is kept, Bitcoin is doomed.
@_date: 2013-11-30 23:55:33
Another thought: Do you plan to increase it to another fixed size or have some dynamic scaling like 'max block size x times the median of the last block sizes since difficulty?'
@_date: 2013-11-21 04:11:39
How do we overcome the resistance of the people that want to keep the/a stupid static blocksize limit?
@_date: 2013-11-21 04:50:58
THIS.  A HUNDRED TIMES THIS. Satoshi showed the path to VISA levels of transaction volumes in his paper already, and that was back in 2009. The blocksize limit has to **go**.
@_date: 2013-11-21 22:39:36
Why should both coins even exist for that scenario? Why do I need two different coins to achieve what you describe?
@_date: 2013-11-22 01:59:15
**Buying a coffee is not a microtransaction!**
Paying cents for reading an online newspaper article might be.
Bitcoin will be reserve currency only if it is easy to transact. Gold serve(s|d) the purpose of a reserve currency quite well and wasn't that easy to transact. So Bitcoin needs to differentiate from Gold. Bitcoin doesn't have the shinyness of Gold or the properties of Gold as a metal. The only thing it has is being easy to transact and very hard to forge.
@_date: 2013-11-21 21:01:16
Thank you. Yes, I also agree that offchain transactions are a nice additional feature. *But they have to remain completely optional.* 
**For bitcoin to succeed, I need to be always able to directly pay my $5 cappucino with bitcoins, without using a payment network. And with reasonable, low fees.** 
**That is the original promise of bitcoin. Destroy this promise, and bitcoin WILL fail.**
Of course, I am not saying that lets say microtransactions in some MMORPG economy should be settled directly. For those applications, it is totally fine to settle every couple hours or when the user wishes to pull out his or her money.
@_date: 2013-11-21 05:38:04
And what is the value of BTC then, if you can't transact it like cash, but rather just very slowly and expensively? As I mentioned in the parent post, I am worried about any static blocksize limit.
A static blocksize limit will drive fees up and limit the usability for Bitcoin.
Right now, I have an awesome user experience (disregarding volatility) using the android QT wallet. No one inbetween, just the network and me and the merchant. If this is replaced by a centralized payment provider, the advantage of Bitcoin goes away.
Of course, it is totally ok to add stuff on top of Bitcoin. But there must be a way for the blockchain to grow dynamically, and not hit a static limit. No one will do settlements in Bitcoins, if you can do settlements in USD just as easily (and you can). So why Bitcoins again?
Bitcoin is valuable because you can transact it freely, (almost) instantaneously and for low fees. Take that away and the whole crazy idea of this 'gold 2.0' value store will fall apart. 
As originally intended from reading Satoshi's paper, Bitcoin was intended as a cash and payment network replacement as good as possible using digital means.
@_date: 2013-11-21 23:00:10


There you are saying it yourself. You find the buyer only because Bitcoin can do more than my nail clippings. It can be transacted across the globe, quickly, without intermediaries and with very low fees.
@_date: 2013-11-19 06:37:44
Well, I wonder whether we have more than one chance at increasing it. It already became apparently VERY political to push through with another blockchain increase. I have the impression that it will be insanely hard if not impossible to change the rules with another hardfork after the current limit has been increased.
Is there any news on the direction this will go? Will it simply be a bump to 10MiB block size?
The original bitcoin paper AFAIR didn't specify any arbitrary limit and talked about VISA-level transactions speeds of thousands per second.
@_date: 2013-11-21 05:04:31
*If* the ones on top of Bitcoin would cooperate due to some agreed-upon protocol, maybe that could work. But Bitcoins fee structure and ease of use has still to be very close to how cash is used to be of similar utility.
But I think without a static blocksize cap, it would become a non-issue, as people will just use the blockchain as much as they want and mining fees will actually go to a healthy and stable, low fee &amp; high throughput balance. And maybe they only want to interact with their wallets once a week with a competitive and neutral payment network.
But I think any crippling by design will be rightly seen as a hard limit to Bitcoin's growth due to small-mindedness.
I also think that it is not *that* important for everyone to be running a fully validating, fully archiving node. I would be fine if there is an archiving node for every 10k or 100k people or so. And that is entirely doable with today's hardware and a Bitcoin network seeing similar amounts of transactions as cash does.
@_date: 2013-11-21 03:46:04
Not only that, it will fail if it doesn't scale. The current blocksize limit is ok **so far**, but it needs to go and be replaced with no limit or at most a dynamic growth rate limit against spam. Miners have an incentive to avoid the orphan cost of too big blocks. More and more people are saying: Hey I like it the way it is, let's just build a banking system on top of Bitcoin.
**NO**. That's not what Satoshi's paper promised. That's not what Bitcoin is intended for. That will absolutely kill Bitcoin. That banking system would work as well on top of USD. The idea is to have international cash and be able to pay a *low fee* transaction for the $5 coffee using just an android app and QR code.
@_date: 2013-11-30 07:55:16
A hundred times this.
**Bitcoin absolutely needs to scale to VISA levels and beyond to survive.**
@_date: 2013-11-30 08:04:17
Or it is just plain stupidity. But exactly this: I need to be able to use Bitcoin to pay for my coffee in the future. That means, yes, pretty big/huge nodes and more centralisation. Still much better than the centralisation of a payment network.
@_date: 2013-11-21 04:48:17
What are those reasons? Satoshi said and shown himself that it can scale to VISA-like levels, and that was 2009. Having bitcoin just as a 'reserve currency' is absurd.
Existing clearing systems solve all problems already -&gt; no advantage when using Bitcoin -&gt; Bitcoin dies.
@_date: 2013-11-30 07:38:22
The broadcasting of large blocks is currently done in a way that transactions in a 'warm node' that is lingering a while on the network get the transactions again in the block messages itself, correct?
Is there a way to have the block messages just as a small 'diff' of the transactions that are in the block but not in the memory pool of the receiving node? That would greatly help to scale bitcoin :-)
@_date: 2013-11-21 04:52:44
I would use the median instead of the average as it would no shift as much with large spikes, but otherwise I agree 100%.
@_date: 2013-11-19 08:32:47
The android wallet is very easy to use. I know enough 'computer illiterate' that I could explain it to.
As for the wallet service: I agree, there will be (and are) offers for safe storage of bitcoins. But the use case for Bitcoin is having the equivalent of $50 on your android wallet, easily spendable with NO MIDDLEMEN.
And if the $50 wallet gets lost/stolen, that is not different than losing a regular wallet. The phone would probably be the more expensive item. 
@_date: 2013-11-30 08:02:17
7txn/s is 30year average for ONE TRANSACTION if everyone would use Bitcoin. If the 7txn/s limit is kept in place, Bitcoin will die. That's how serious it is. Obviously, the limit is not a hard limit yet. But it absolutely has to go completely, and soon.
@_date: 2013-11-22 02:20:36
True @ $20 per Satoshi. But even with all the exponential growth over the years, and even believing it might become THE world currency, that feels like a stretch :-)
In the purely theoretical scenario that all coins would be distributed worldwide, and evenly, every human would still have 300 kilosatoshis. I guess that would make a satoshi worth a couple of cents/tens of cents in today's terms.
Wrt. the refactoring, I think it is a question about a) software, b) hardware and c) community. I am not so worried about a) and b), because
- with the current client, one could run a full node on a high end multi-core, many-HDD, fat network pipe machine already. Those machines are not insanely expensive. There could be several hundred of them worldwide, with eventually petabytes of storage and they could in theory already replace any other 'money' system. And would still be sufficiently decentralized.
- blockchain compression has not yet been implemented in the 'bitcoind router', but the path how to do it is clear
However, I am pretty worried about c). There seems to be a lot of growing sentiment that 7txn/s is just fine, because, well, I want to keep my own node on the network, and Bitcoin is just this awesome store of value. Which it won't be at all, if it cannot flow very easily. And a hard cap on the number of transactions is a crippling, artificial hard cap on the number of transactions. Arguing that it creates 'market incentives' to have higher fees for miners is insane. The orphan cost makes sure that transactions will cost something already! Sure it will drive up the price and thus the income for the miners. But in the long run, it will kill bitcoin, because, as I said elsewhere: 7txn/s means 30 year average if every human on earth would use it. And then, all other forms of payment look rosy in comparison. I also think that any kind of 'payment network' on top of Bitcoin is undermining its original promise. Which is: I can use my *very own coins* in my *very own wallet* on my *very own cellphone* and just pay for my coffee with low fees.
@_date: 2013-11-21 06:22:18
That is a very cool idea. I like it :D
@_date: 2013-11-21 05:23:40
It could fail because stupid people prevail that want static blocksize caps. It would fail slowly, almost unnoticeably first: 
a) So called offchain transaction providers will arrive on top of Bitcoin.
b) They become popular, paying in BTC through them becomes popular
c) They realize that they do not need Bitcoin, switch to USD/EUR/whatever
d) The offchain transaction providers become just like the regular credit card companies.
e) Bitcoin's value plummets and it eventually disappears
THAT is the scenario I am immensely worried about now, with all these stupid ideas about economic incentives for statically limited block sizes gaining traction. 
The orphan cost needs to be enough to keep the blocksize effectively limited and the fees reasonably low.
@_date: 2013-11-21 04:56:35
Transaction rate is growing exponentially. I would not be so sure.
@_date: 2013-11-22 01:58:31
Yes. But why did he want to buy Bitcoin? Because someone has, in the end, a value attached to Bitcoin, in the form of 'I want this'.
And this, funnily enough, does not happen with my nail clippings. And that's because I can't send my clippings through the intertubes and shave of a little bit.
And you know that and are just trolling now.
@_date: 2013-11-30 08:05:32
Look up 'block orphan cost'. That cost already is not optimal yet and can be made lower with protocol enhancements (which are needed, soon).
That cost will make miners always charge something for transactions. That cost should be as low as possible to cause as widespread adoption of Bitcoin as possible.
@_date: 2013-11-19 06:31:53
Right. I mean, sure there is a need for wallet services, at least as long as hardware wallets and the like do not become commonplace, foolproof and easy to use.
But this is one of the things that many people seem to like. Full control.
@_date: 2013-11-21 05:31:59
Why so? They are effectively banks. They will be in-between the customer and the merchant. Just like credit card companies! 
They will do all the fancy stuff banks/payment providers do, with all the fancy regulations bank have. There will be market concentration.
And nowhere are they bound to BTC. They could do all settlements in 'real money' as well.
@_date: 2013-11-21 05:51:45
The protocol currently has a hard limit of 1MB for each block (which comes about every ten minutes). That is about seven transactions/second for an average-sized transaction. That is all the network can handle at the moment.
The ten minutes are of course totally fine, and because they are intimately tied to the block reward/mining structure, noone in their right mind wants to change anything about the speed with which blocks are generated. And as seen from Bitcoins current use, that is fast enough for all uses. Small-value transactions can be done without confirmations and risking the negligible risk of double-spends in those cases etc. So no one right in their minds wants to touch that 10min rate.
However, seemingly things are now different for the block size limit. 
The 1MB limit came about in the early days when people were worried about others spamming a lot of computers with meaningless transactions. Satoshi in his original paper never intended such a thing and showed how the whole beast can scale to VISA level transaction rates, and that was 2009!
I can concede that unlimited block sizes and the risk of 'a sudden terabyte block appearing' might be somewhat of a worry still, but could be solved by having a rule that blocks can only grow to a maximum of, say, 3 times the median block size since the last difficulty increase. So that not suddenly terabyte-sized blocks are appearing before the network is ready for it.
However,  people now start to argue in funny ways that there is a necessary economic incentive for keeping the block size small! They argue that the miners would include all transactions otherwise. But that is not true. A mining pool operator that collects more transactions has a higher risk of producing an orphan block due to network and validation delays and that means that there is a *natural* limit to the number of transactions the miner would include.
Also, having no limit in the protocol would not mean that pool operators would necessarily allow anything. Miners are mostly pools now and no solo mining is happening. Lots of little guys have lots of ASICs. If a miner screws up/behaves badly, people will tend to join another pool, if at least half of them want to see BTC succeed. And if they don't, we have completely other problems to worry about anyways.  
@_date: 2013-11-28 10:43:19
I would also be very curious whether you are still planning on lifting the blocksize limit soon - ?
Also, what kind of techincal innovations do you have in mind to make block transmission more efficient? 
I see this as one of the big risks to Bitcoin - txn. fees that will grow so high that paying for the cup of coffee is cheaper with regular credit cards.
I would be interested in prototyping/helping out to make Bitcoin more scalable and keep fees cheap.
@_date: 2013-11-21 20:51:23
Well, I think it will mean a change in the network's structure, sure. It would also mean that less people are able to run full nodes. However, the money/equipment/dedication needed for credit card network levels, &gt;1000txn/s was already shown to be managable in 2009 in Satoshi's original paper.
For those levels, a couple of dedicated hobbyists (lets say a hackerclub-sized community) can run a full node (full validation + full blockchain). Sure, they need a multi-megabit/s stable internet connection and need to buy multiple TB-sized HDDs per year. But it is entirely doable. And then there is the whole intermediate range of just keeping a part-chain and the unspent transaction set, which greatly reduces the amount of storage needed. Nodes of this level do not really exist yet.
I think the problem is that a lot of people fear some diffuse sense of 'centralisation' on the Bitcoin network. And, sure, a scenario where it gets more unwieldy for myself alone to run a full node means ... just that: 'centralisation'. 
But I think *some* 'centralisation' is unavoidable. I think the big worry was always that there would be only big bad banks left running 'full nodes'. And I do not see that possibly happening, even with transaction levels that mean a worlwide adoption of Bitcoin.
I think this comes from a stupid, generic hate of 'centralisation'. Humans always achieved more by organising themselves into groups. This will also always happen, and I do not see anything bad with that. I can see that it can get bad when groups get oppressive and non-optional (states with big bad governments).  I think the extreme Libertarian nuts who see any kind of 'centralisation' as evil should realize that there will always be groups forming, and that this is to some level beneficial. If those groups are in the range of thousands or tens of thousands of people, and many hundred of those groups exist around the world, each having a fully validating and archiving node, that would still be arbitrarily close to what some Libertarians deem the  ideal of a flat and egalitarian society, wouldn't it?
For comparison, look at the internet. Peering points have a higher level of traffic than I can handle at home. That, in a way, is centralisation. But it is unavoidable for any kind of big, sucessfull network to develop that kind of structures.
I fear a lot more that any kind of additional payment provider (now euphemistically named 'offchain transaction network') on top of Bitcoin, instead of using the coins directly without additional, unnecessary complexity on top, would lead to the payment network becoming a 'banking/payment system oligopoly'. And bitcoins wouldn't be used directly, we would use credit cards, just as we do now. I rather like to use my android wallet directly!
... oh, and if they are a widespread banking/payment system, they do not have any need to settle in BTC. They can as well do that in USD/EUR!
I think the idea of having ASICs in many places where there is a use for electric heating does  actually make quite some sense. The  **main** thing that keeps Bitcoin in the hand of the people is the wide spread of the mining hardware, which hopefully doesn't get consolidated too soon. We already have an 'oligopoly' of pool operators, so to speak. *But they do not have* **any** *power without the miners in the pool agreeing with them*.
@_date: 2013-11-21 04:55:45
And I think Gavin is spot on. Any static block size limit (anti-spam 3x median since last difficulty would not harm) will cripple Bitcoins growth.
**It is meant to replace credit cards with digital cash. Not meant to be some kind of funny and useless 'reserve currency' for transactions on yet another proprietary payment network.**
@_date: 2013-11-30 07:49:29
7txn/s is a HARD limit. If everyone on the world would use bitcoin, it would take 30 years average for a person to get a transaction in!
So you are  right, right now, the blocksize isn't an issue yet (but efficient block transmission is, to lower the floor of the orphan cost). But it does have to be increased and the idea that 'there will be more competition for fees with the limit in place' is just crazy and will kill bitcoin if it becomes the prevailing opinion.
@_date: 2013-11-21 05:58:09
I still think the static limit is scary. Especially scary is that people are now seriously arguing for it, *artificially* limiting bitcoins growth.
A dynamic 'n times median block rate since sometime' rule, ok. But any static blocksize rule looks like a recipe for disaster to me. Especially as the protocol gets harder and harder to hardfork over time.
@_date: 2013-11-30 07:58:26
No it needs to be dynamic and basically unlimited. For bitcoin to really scale to worldwide transaction levels, the node structure and level of decentralization has to change. I think 'another jump' as a band-aid will just enforce the higher limit and over time it gets harder and harder to change the protocol.
I am not able to run bitcoind with a PB-sized blockchain at home. But I can get a few people together (10-100) and run such a node. That is still VERY decentralized but would allow (in principle) for bitcoin to scale to a level where all worldwide cash transactions are done through it.
@_date: 2013-11-22 06:34:35
That's true. I should have been more careful with my wording. I equated median confirmed transactions with median block size simply because I assume the distribution would be constant.
But of course you are right, it makes more sense to look at the actual size, as that is what needs to be stored/transmitted/replicated.
@_date: 2013-11-22 02:08:26
Ok, maybe we have different views here on the crowd. 
I have this impression that a lot of people buy Bitcoin because they think it has some value because of some circular (bubble) reasoning and 'gold 2.0' as a buzzword. I would be totally ok with your reason to call it gold 2.0. I guess some people saw the analogies to gold and started to use the that word and it got a buzzword and now it feeds on itself and ... oh well :)
In the end, and as you mention, the point still stands that it needs to be very easy to transact with it in small amounts.
@_date: 2013-11-21 21:54:20
And we need to get rid of static blocksize limits. The nodes have to scale up. We need to convince people that the currently rising sentiment of 'bitcoin is just awesome because it is a  store of value store' will kill Bitcoin.
**NO**. Bitcoin is awesome **because I can pay directly, without payment network (or 'offchain transaction network')** for my $5 cappucino with low fees. If we deviate from that promise, Bitcoin will fail.
@_date: 2013-11-22 02:03:48
I see! Thinking about this, I still would argue it is a better idea to base it on the number of confirmed transactions, though. Because those actually cost money and are thus not that easy to spam. 
And using the median of the blocksize instead of the average makes it more robust against attempst to raise it by a single, spamming entity. The spammer would need to constantly put confirming transactions into the chain, which eventually will cost him money.
@_date: 2013-11-21 21:26:44
Wanna have my nail clippings? They are also in limited supply. And they are real and not just some special numbers.
Your sentiment will kill Bitcoin if it prevails. Bitcoin has value because it has properties, **namely quick and easy low-fee transactions, worldwide**. Take that away like people do who argue for low blocksize limits, and Bitcoin will die.
Bitcoin was meant as a digital, decentralized and direct replacement for transaction networks and cash. Value store is secondary, not primary, and only works *because* it has these nice, cash-like properties.
@_date: 2013-11-30 21:18:53
I see your worry. However I would not extrapolate anywhere from the observation that the number of nodes are currently dropping.
Having the full chain available is allowing to be completely trustless wrt. to Bitcoin transactions. There is always a need for that, after all Bitcoin was invented to decentralize money.
I think eventually people will have to pay to be able to peek around in the full transaction history. Like a 'money library fee'. And I do not see anything wrong with that. And enough people will pay for it, as knowing the transaction history to some extent (but most people do not want to have the full extent) is needed to participate in Bitcoin.
I think in the end it is a good sign. It means that people find services trustworthy enough that they say: "You can run it". And I think the falling number of full nodes is partly due to this.
I will keep my (unfortunately very few) coins in safe offline storage. But others are ok with a web wallet. And some like a light node instead of a full node. And I think that is totally fine. I just think everyone should be educated about the trade-offs as far as possible. And as long as the option exists to go from online to offline wallet (let's say because someone 'got burned' by using an online wallet) easily, there is freedom. And if people freely choose centralized services, why should I or anyone blame them?
And wrt. to keeping Bitcoin safe by artificially driving the mining fees up with limited block sizes. That is kind of a funny argument. To rephrase:
1.) I believe in the forces of the 'free market' as the sole force in effect, stabilizing Bitcoin
2.) The pure free market will not provide any security guarantees in terms of hashing power
3.) Because of that, a limit to the transaction size is needed
4.) Limit the free market and thus limit Bitcoin with a purely artificial bandwidth cap
 
Don't you see how 4.) contradicts your assumption in 1.)?
@_date: 2013-11-21 22:08:18
That is only true if the stupid crowd that salivates when thinking 'bitcoin == gold 2.0' and 'inherent store of value' doesn't get too powerful and cripples the network by arguing for static blocksize limits and crazy ideas like 'a limited blocksize means more miner fees' and 'bitcoin has a value because it is limited'.
**NO**. Bitcoin doesn't have a value primarily because it is limited, Bitcoin has a value because it is cheap to transact it quickly, internationally and *without intermediaries*. That doesn't mean I am arguing against its deflatory properties or anything, I am arguing against artifical limits on the number of transactions. I am not talking about touching the 21mio. coin limit. No one in their right minds wants to do that.
But right now, we have a **temporary** limit of 1MB per block. That is **just seven transactions per second**. That would be **30 years average time if every human being on earth wants to get a transaction through the network**. That has to either go completely (no other static limit!!) or with something that scales the maximum block size with the median number of transactions since last difficulty or similar.
Bitcoin will not survive if the fees grow so high that a payment network (euphemistically called 'offchain transaction network') on top of bitcoin is neccessary to pay for my sandwich or coffee and have reasonably low fees.
We need to be able to use Bitcoin directly to pay for things like coffee/sandwiches, and we need to be able to do that with low fees.
@_date: 2013-11-30 21:04:27
I strongly disagree. Everyone I know who has ever been introduced to Bitcoin is liking it to a digital form of cash, **very easy to transact** in smaller (but not micro amounts). Like paying for a cup of coffee.
I think the high price of Bitcoin seems to have an intoxicating effect and people see it as some kind of reserve currency. It will not be a reserve currency if its original promise - cheap to transact digital cash - is not kept.
Gold and other things are perfectly valid as 'big money' already - and they do not have any funny 7txn/s limit.
@_date: 2013-11-23 21:06:36
Is there an update on it? Is there also something about transporting only compressed/summarized blocks over the network, exploiting the fact that transactions are usually widely known before a block is solved?
Because it looks like this is getting more and more of a pressing issue. And the current protocol has too high orphan cost to really drive the transaction costs down. And it seems very necessary to scale the network soon.
Oh, I also would contribute up to 1BTC to a fund/bounty that has the goal of solving this scalabilty issue once and for all.
@_date: 2013-11-30 07:52:17
You meant 'reduce' instead of 'increase' :-)
I wonder whether there are even more efficient schemes than just short hashes. Something like an 'rsync' for block between nodes. I think this is something that is urgently needed as it would also make mining larger blocks for pools much more profitable as faster propagation decreases the orphan cost.