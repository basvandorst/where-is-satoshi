@_author: no_scarcity
@_date: 2015-05-05 03:23:30
I see where you are coming from with this measure -- i too feel that a separate proof will be needed to verify ownership of a transaction without having to resign. This would need to be mathematical rather than protocol specific to a particular system. Have you considered an implementation of a DB with linked zero-knowledge proofs that can sync to the blockchain and with each person that needs an "ownership receipt" can contribute to the chain via a microtx into the DB. So the end user would just need to pay once from a deterministic seed and zero-prove it and only keep a payment channel open if he will need the receipt in the future. Think of it maybe like permanent storage that isn't stored until you need it, and when you need it, the cost to keep that record is settled.
 
@_date: 2015-05-10 01:35:48
That is any easy one.
I want to watch the current system burn like it deserves. 
Hopefully bitcoin pulls through - if not, fuck it, the bloodbath is also worth watching, clawing all the way through. 
@_date: 2015-05-03 13:22:38
So along the ideology of perfect accounting and settlement, would it not be wise for there to be an approachable limit to the amount of data transfer bound by a factor of time? Meaning the more important the transfer the higher cost within the bracket of the accounting unit. This is why there is block size limit. Recording-keeping is not free. Competing forces will drive more important batches first. The value of the network is subject to terms of its own transfer. Each transaction batch is directly proportional to the resources consumed in the POW for the batch - so do we need a limit as an approachable measure or release it and let the market point float arbitrarily? 
@_date: 2015-04-27 04:56:15
In distributed prediction markets an applicable use-case will be that which is not already available in the current model of prediction markets.  In a distributed model, we can potentially aggregate data that, though outside the original scope of 'prediction markets', provide an insight to statistics and data sets that are otherwise immeasurable. This can be extrapolated in a way that is optimized, so as a system we can continue to build a sum-of-subjectivity ontology of arbitrary interactions and the data set corresponding to those interactions. This can be used in re-targeting learning algorithms and machines. An example of this is two intersecting subjective rationalizations that have similar but not identical outcomes (from the user's perspective). i.e. 
mood = x;
while ( action_set -&gt; user_a ){
mood = y;
mood_ontology.update();