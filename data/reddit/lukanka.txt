@_author: lukanka
@_date: 2017-03-27 19:19:42
For the last 24 hours I have been observing BU's progress, on average every "last 144 blocks" have been around 43% BU. They are alarmingly close to throwing all of us (including themselves) over the cliff
@_date: 2017-04-01 08:54:35
quote: "If you ask people why they don't run a node, the reason is often tied to the block size."
Response: This is not very relevant to the issue of increasing block size. Yes, setting up a Bitcoin Node is a pain in the ass because it requires downloading a 115gb database right now, and then every time you want to send/receive funds, a sync is required which is tedious because it also takes time. However, this is already very cumbersome even with a 1mb blocksize limit. Someone who is willing to set up a node in the current environment and take the step from running a light Electrum-like client to downloading a 115gb database, is very likely to also be ok with a 200 or even 400gb database (if blocksize was slightly larger). It's the initial hurdle that is the problem with the low node count, and once a user decides to overcome it, I can speculate that a slightly larger database won't be an issue (especially since 4tb hard drives are now dime a dozen). 
Also, my proposal would optimize the blocksize 2 weeks a time according to actual network needs, so we don't need to speculate about just how much the eventual adoption of the LN may alleviate the on-chain transaction volume.
quote: "The cause for this is the time it takes to verify large blocks."
Response: this is an interesting tid-bit that I wasn't aware of and it certainly poses a problem. I am not clear on how the miners can skip validation, technically speaking?
@_date: 2017-04-01 09:28:31
The issue then becomes one of centralization - the attacker would aim and pushing off nodes run by average Joes in order to gain a majority stake of nodes?
But such an attack would last a while, as the dynamic blocksize algo would be rather conservative in the blocksize increase. The attacker would pay an exorbitant amount in transaction fees in order to spam the network to an extent that would increase the blocksize to such an extent that it would push off regular nodes. Again, if someone is ok with devoting 115gb to the blockchain, they would probably be ok with devoting even more than that. So then what is the threshold? 1TB? 2TB? how much spam would that equal in order to raise the blockchain size from the current 115GB to something much larger, considering it took 8 years of transactions just to get it to 115GB?
Not sure if such an attack would be feasible for anyone, and I am uncertain as to how exactly the mathematical calculations would be performed in order to determine the precise likelihood and cost of such attack...
@_date: 2017-04-01 08:32:21
Could you explain why the network can barely tolerate 1mb limits?
I tried to explain this but I suppose I am not very clear: the dynamic adjustment of a block size only happens every 2 weeks just like the difficulty level, and then sets a deterministic cap for the next 2 weeks. Because this cap is a product of the average block capacity of the previous period, any increase in blocksize is gradual and relatively benign, allowing for a slow growth or decline over a longer period of time. This also disallows the opportunity for any single block to be erroneously large (due to a spam transaction attack, for example).
@_date: 2017-05-11 04:05:59
You guys can bash him all you want, but the fact is, Bitcoin is not a user-friendly experience yet when it comes to real-life usage, which is further exacerbated by the network being clogged up. How is a newbie supposed to know about manually setting the "correct" fee? What is even the correct fee, at any given time? You have to go to a website every time just to see what the proper fee is in sats in order for your transaction to be included in the very next block. It's bullshit. As much as it pains, me, until we adopt Segwit or a similar solution, these kind of problems will further complicate a technology that is already above and beyond what most people can understand, and this will stifle user adoption for the time being.
By the way, my client is showing the mempool to be currently at ~93200 transactions and 247.63mb, this is getting out of hand...