@_author: buy_my_bitcon
@_date: 2015-02-01 03:14:15
BTW I noticed you skipped past the folding example I creamed you with
@_date: 2015-02-01 02:57:27


Yep, just look at folding for a demonstrated example


Nope; GPUs will increase just as fast as ASICs do, if not faster. After all, there are physical limits to how efficient ASICs can become, and if anyone is going to reach those physical limits, it's going to be the big multibillion dollar companies like AMD and nVidia first - not some amateur and his team of 12 in Norway giving orders to a Chinese manufacturing facility.
I'm saying that GPUs will have greater increases in hash rate at some stage in the future in comparison to increases in ASIC hash rates
@_date: 2015-02-01 03:01:03
Electricity consumption is already considered.
For the '$200 GPU needs to do 225 GH/s' figure, I use the electricity consumption of the GTX 960 which has an RRP at release of $199 and it uses 120W
Can't remember the exact figure I came up, but I believe it was a cost of ~$1000 over a year for electricity cost using the above - so I'm not even taking into account possible improvements in electricity efficiency over ASICs.
@_date: 2015-02-01 03:02:42


@_date: 2015-02-01 03:09:44
I don't need to imagine it - it's already demonstrated.
The Cointerra AIRE which *was* slated for release this March (Cointerra is now bankrupt) was 16nm.
Companies like Intel have already made and released for consumers 14nm processors.
You confused in your previous post technological development with the fact that AMD/nVidia are not currently gearing their GPUs towards hash rate. My point is that they could, if at the very least to a small degree.
I guarantee you that if AMD/nVidia/Intel made ASICs, they would be far more efficient than the ones currently available. In fact, I'd guess that they probably already have such machines, for purposes such as government or scientific research.