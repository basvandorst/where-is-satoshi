@_author: masterwit
@_date: 2015-08-02 13:31:35


*You or another might find the following industry example interesting or perhaps common knowledge. I'm not an expert in SAP / Oracle but figured the following might be worth sharing as that open ended question speaks well to what the database has shifted to today...*
*(Aside: articulating this below was good interview preparation for me later this week which is in part why my answer is a fucking novel.)*
SAP hits this very limitation being paired with an Oracle backend. 
Originally source code was restricted to 72 characters, but has since been increased to 255. ^^**(1)**


Today a form of a binary blob, SAP's "RAWDATA", is used to store revision changes within the release management system.  Customer code and select SAP source code are accessed today through a legacy process along with the newer ADT process which facilitates eclipse integration into the backend codebase. ^^**(2)**
But you may be thinking that 256 is no where near the limit (and you'd be right).  The actual value varies across databases slightly depending on indexing and various database operations require "overhead" to address or navigate data. 
A string of bytes / ASCII 7-bit is limited for a database in SAP running on an Oracle backend in a Unicode environment to 2000 character bytes (to my knowledge).  Here Oracle places the 2000 length sequence as the maximum indexable segment of type RAW without having the additional representation needed for the slower yet more robust LONG RAW.  
But my point is containers and facilities certainly exist on enterprise databases such as Oracle SQL to store and access larger data types depending on the update,   record size,  and other binary nuances that comprises Oracle's ROW ID in 256 bytes. 
Source code for SAP in a R/3 ERP on an Oracle Database uses **cluster tables** to store source code where the cluster index and series of tables C's can be sorted, hashed, etc. to minimize overhead or performance loss.  Obviously tying indices across tables in a cluster causes performance headaches for queries that require table scans or lookups on non sorted or hashed keys [represented within the cluster key].  This a piss poor explanation of a necessary area of DB knowledge for contextual purposes even as an application and back-end library programmer (such as myself for the time being).
SAP stores compressed source code using a cluster table approach as objects such as XML's, MS Word documents, pictures and more all are not accessed in the same performance heavy query driven fashion.  
The use case instead is put an object in and retrieve an object, not scanning this raw binary data. This is why compression and line length become irrelevant as source data is no longer the VARCHAR / exposed sequence of simple table indices. ^^**(3)**
Databases adapt today to different use scenarios one of those being the accessing of non indexed data as seen in cluster tables.  Storing data as raw blobs both saved and accessed through other indices is perfectly fine and safe with proper database management.  The issue with this moron and the codebase above is not about storage but the nature of the flaw in not establishing change management, release hierarchies, backups, and so much more. 
*I rambled too long above so I tried to footnote my meanderings and some areas of opinion from a non expert (me) before submitting this enormous reply.*
^^**(1)** *Released as a note (patch/hotfix) originally from 4.6, SAP note 367676, the changes were later incorporated somewhere in 6.*
^^**(2)** *(Transactions SE38, SE24, SE80, etc. requiring one use development tools built into the client frontend until ADT only recently.  ADT is still rough/incomplete and oftentimes particular operations require legacy access)*
^^**(3)** *Any good modern database system today (from my familiar non expert view) has been forced to facilitate storage for big chunks of [compressed] data as advantages and performance required within table indices and other data access restrict the nature of a database table itself.*  
*This philosophy stems in part first from the necessity for databases to gather and store data that may be impossible or at least inane to describe in a relational/logical database. One other driving consideration may be seen in the technical limitations of data access. Instead of mediocre performance for all data regardless of how it is used, its frequency, and queries, a database can be built to shift limitations in a lopsided fashion based on need. Relationally mapped data in a normalized database does not exceed the 2000 cap described above and would not normally even have data that large requiring such an index.  Similarly, a png image does not acquire meaning when sorted by stored bytes...but it may be immensely important as the result after queries, updates, and other calculations on the table index end.*