@_author: buttes_
@_date: 2017-11-13 20:38:31
1GB is a little over the top, I think it's obvious that would cause problems (storage bw etc). I'm speaking to the general case below.


You don't, past blocks are only needed to catch up to the network when you stand up a new node or something. UTXO is incrementally computed and stored in a database. When a node gets a new block they don't need to recompute all of the UTXO, only apply the changes in the new block to the UTXO pool, removing TXs that have been spent and adding new ones for the outputs.
The question is how quickly you can query a large UTXO pool and execute scripts on the resulting TXs. [It currently sits at about 2GB]( I'm not sure how Bitcoin Core caches the UTXO db but that would be interesting, it's probably a pretty big factor. The time it takes to execute scripts/validate signatures probably has more to do with CPU speed than anything. 
Bigger blocks won't cause validation to take more than 10m (well, maybe if they're *1GB* blocks haha), but it could still make block propagation unacceptably slow. My question is, how long is too long for a Bitcoin node to validate a block, at what point will it create an unacceptable amount of orphans and make the network materially less secure? 
The PoW itself shouldn't take any longer with more tx's, it would just take negligibly longer to compute the block hash I think.
@_date: 2017-11-13 19:31:46


What do you mean by "process" and "memory"? Mining? Tracking UXTO?