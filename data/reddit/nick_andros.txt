@_author: nick_andros
@_date: 2016-05-17 01:13:25
Glad I am not the only one thinking about this:
@_date: 2016-05-17 15:12:44
Overly simplistic thinking.  If I were the Chinese government, and I wanted to destroy Bitcoin (or had favorable relations with Russia to gain from destroying Bitcoin), I would take over or hack Antpool, F2Pool, BTTC Pool and BW.Com.   Next, I would use the resulting 70% + of the hash rate controlled by those pools to double-spend to unspendable addresses ALL bitcoins held by those pools; and any unspent outputs paid to existing pool members (but not yet spent or moved by those folks) as far back as posible, prioritizing from largest to smalletst BTC values. 
How many hours would this be able to continue until all that hashing power could be moved and absorbed by other pools?    How deep would the stack of bad blocks get?      Could the hashpower reorganize 18 blocks (3 hours) worth of such nonsense?   What if it took longer than that for things to get reorganized?   What if the attack were combined with DDOS activity against other nodes to further criple the network and slow the reorg process?  (Iand/or, DDOS to oblivion any miner who pulls its hash power off the PWN3d pools during the attack -- which prevents the loss of hash power from the controlled pool from being added somewhere else).  
I suggest the overall value of BTC destroyed might be small as a fraction of Bitcoins in existence but the value, trust, etc., of Bitcoin as a system would be hugely damage in the months of news stories and analysis that would follow. 
 And this is possible because of ASIC and pool concentration.  It would not be possible if mining were more diffuse. 
And this is not even considering more subtle attacks where, for example, a hostile government simply keeps the pools under their thumbs to affect what transactions are included in the blocks, perhaps based on something like chainanchor or whatever.   If all the governments got together and agreed to regulate ALL the pool operators to force ChainAnchor, where does that hashpower go?   Again, far better for mining to be diffuse.   The governments cannot stop 30,000  individual miners running fully nodes with mining.
@_date: 2016-05-17 01:26:52
Censor is such a generic term. 
Here's what concentration allows RIGHT NOW:
1.   Certain but not all exchanges can approach 4 or 5 poolmasters and make off chain deals promissing a monetary flow for preferential treatment in terms of inclusion of that exchange's transactions in blocks.  
2.   Developers, stakeholders, ASIC manufacturers, etc., can make secret agreements behind closed doors to 'orchestrate' the rollout of soft and hard forks.   I personally don't buy into all the conspiracy theories about Blockstream on one hand or Gavin Andreson on the other, but it is factually true that Blockstream can offer stock options, cash, or other incentives for miners to hold off on adoption of a disputed block size hard fork.   Gavin Andreson (if he actually worked for government agents) could treaten or cajole things in the other direction.   All outside the incentive framework meant to govern things under the protocol.   Now imagine if mining was not centralized as it is now?   Such theoretical coercive efforts would have to persuade thousands of people, instead of just the top 5 mining pools and mega mining operations.  
2.  The Chinese government could literally break down the doors at 3 or 4 locations and suddently have the ability to control far more than a majority of hashpower.  Their take over could be secret and under the radar.   Non democratic governments have a troubling history of simply destroying things they fear or decide they cannot otherwise control.
Time to fork to change POW.  pastebin.com/5MUHnYNL
@_date: 2016-05-26 22:31:59
Why would the devs agree to HF (with all HF issues) to bump block size, on any map, AT ALL, until the other solutions have their chance?
The only exception I can imagine is a panic/major dislocation event. 
Keep in mind:   
Block space is currently a perfectly inelastic supply curve.  You get a megabyte of space every 10 minutes, more or less.     No matter how high demand gets, the protocol does not supply more without changes (e.g., SegWit).
Demand for blockchain space is mostly elastic.   As long as the price of getting space in the next few blocks is under the curve, that works fine.   But if the price cranked up to a certain level some folks will drop out.   Maybe guys demonstrating Bitcoin to a college class won't do a dust transaction just to show how it works.   The spammers will finally have too much expense.   All that.   The only time there's a problem is for those who MUST use Bitcoin and MUST have reasonably fast confirmation (e.g., those who pay wages, on time, in bitcoin).   I suspect most will be able to wait, but if the ones who CANNOT wait, simply cannot wait, are stuck.. you'll see fees blow way up.   How much?  Depends on how bad they need the space and how backlogged it is.    It will take a LOT of additional demand before people will be willing to bid mBTC(s) per transaction.   So I don't think demand will get there before SegWit comes on line and then theres Lightning etc. 
But, a price shock could happen the other way:   The mining cartel could put out a press release that says their risk for financial losses after the halvening is such that they simply will not include transactions in any block unless those transactions pay 1300 satoshis per byte.   (That gets the miner that mines the block, if full, rough parity with its earnings before the reward halvening).    It may happen.  Cartels cartel.   Its how they work.   In fact, the the block limit and pool concentration makes that easy.    So it winds up being, what, just over a dollar for a 192 bytes transaction.....     Pretty big increase, but not end of the world levels.  (Nothing like what Uber does during demand surges)
If those situations had a big enough impact, you might see an emergency block size bump.   Just my opinion.  
@_date: 2016-05-26 22:09:27
I read somewhere (it was either Greg or Luke) that a tweak to the header format to freeze out things like ASIC-Boost could be done with a soft fork.    Problem is that whould be the devs literally protecting the existing cartel from competition.   
@_date: 2016-05-26 21:22:40
If/when a HF goes forward it needs to be a HF to change PoW, including memory footprint and storage hard tasks.   And sure, if there's still a scaling desire after SegWit and all the offchain solutions come out, fine. 
But there's a reason the miner conduct (including the passive agression by Antpool) sounds like a cartel.    Its because at least as to the 4 or 5 biggest pools, they are a cartel.    The very fact that you're having to send developers to HK to try and negotiate an agreement with the "miners" is very problematic from my view and says:  Yeah, we're pretty centralized at this point.    
@_date: 2016-05-18 04:58:33
Lets use the socratic method, shall we?
Do you acknowledge that I never claimed that the problem is being able to knock on 3 or 4 doors for find the actual physical hardware behind all that hash rate? 
Do you further acknowledge that hashing operations that simply point 'dumb' mining devices at a pool address of one of the 4 or 5 largest pools and are not actually building and validating the miners' own BLOCKS are placing at the disposal and into the control of the pools the benefit of that hashing power? 
Finally, do you acknowledge that a mining hardware operator that relies on a pool to assemble blocks and provide headers, difficulty, coinbase, etc., data for the miners' equipment to hash has no practical way to validate on an instaneous basis whether a hash just found by that miner's hardware and transmitted to the pool is actually in support of a valid block or some sort of shenanigans?
Why would it not follow, then, that for some period of time "X" after a hostile government secured control of the infrastructure of the four largest Chinese pools, that this bad actor could, if it wished, use the 70% of hardware miners' hash power at the disposal of those pools for bad ends until the owners of the actual mining power, on a trailing basis (probably reading about it on Reddit), learned of the problem and took action?   
@_date: 2016-05-17 13:57:12
There may not be a such thing as ASIC resistent, but we can try to get something in place that is ASIC "inefficient" 
To do that you have to ask yourself why ASIC concentration is so easy under CURRENT proof of work.  That's easy:  Current proof of work requires only 80 bytes of header data.   Of that, the biggest part are the previous block hash and the merkle root of the block being mined.   With this tiny data set fixed, a 4 byte nonce is twiddled until a hash meeting the difficulty target is found.   In short, once the content of the block header being worked is determined, and each ASIC tasked with its range of nonce values to try, there are no additional memory or storage operations and the ACIC can just power through hardware implemented SHA256 operations very quickly.   The SHA256(SHA256(Header) proof of work can VERY EFFICIENTLY be localized in ASICs, and that's why you see arrays of hundreds of chips with each chip having many many SHA256 instances per chip.  Mining modules have virtually no memory, zero storage, and are networked only to the extent to get the headers and send out winning hashes.   VERY very efficient.  
The idea of what I wrote up (and its just one way to do it) is to ensure every single increment of nonce during hashing requires the specific hashing core or cpu or cluster to have access to (a) a dataset of all prior block merkle roots (about 33 megabytes at present) plus large storage holding a "Depth" number of full blocks data.
What we're doing is adding STORAGE and MEMORY requirements to proof of work, while significantly increasing the QUANTITY and DIFFERENTIATION of the work per nonce iteration. 
In terms of storage, each increment of nonce produces a different Gadget, and thus every single nonce has us figuring out the WorkMask based on a different Target1 and Target2.    As I said in the brief write up, the Depth piece of this would require some testing to tweak.   But assume we would start with a Depth requirement that would yield a dataset that would overtop the available memory in currently available high end video cards by a material margin.  So were talking gigabytes.    Initially, I considered having the Depth be the full block chain (so each miner would have to store a copy) but the current write up envisions there may be some culling.    The newest hotness coming down the pipe from nVidia would have up to 16GB of super high performance video memory, so assume that Depth here would be such to require storage of at least 1.5 that much block data so that there would be a 1 in 3 chance or better that fetching Target2 would require a trip to nonvolitile storage.    Once the storage needed is beyond what can be parked in volitile memory, the storage options from fastest to slowest would be:  i) SATA on PCIe on the same system bus as the chip doing the proof of work; ii) SATA on striped raid or other fast disk controller on the same system bus as the chip doing the proof of work; iii) other local to the same machine disk options; and, finally, iv) network storage to be fetched over some form of network trunk. 
In terms of working memory, each increment of nonce requires a working data set comprised of i) the current block header being worked on [80 bytes]; ii) the merkle root of Target1 block [32 bytes]; iii) the full contents of the Target 2 block [varies but figure 500 bytes for hopefully rare single transaction block to whatever the block size limit is]; and iv) 4 bytes representing the time xored against the current nonce.   Remember that because Target1 and Target1 change based on the determination of Gadget, these will be different for every iteration of nonce.  Figure something approaching a megabyte worth of volitile memory will need to be staged as the working set for each iteration, (more if the block size limit is increased).  
Finally, we add operations and do it in a way that makes parallelization yield only marginal efficiencies:   For each iteration of nonce, in order to come up with the WorkMask, the miner:
-Does an SHA256 operation against the current header to find Gadget.
Then, after fetching and caclulating the data for the Blob (Target1 block merkle root, Target2 full block, and nonce xor time):
- Runs Whirlpool512 hash against the large Blob data to figure out the hi/low logic that decides whether LastHash is Keccak or Skein, and
- Runs either Keccak or Skien against the Blob to get the WorkMask
(And, finally, there is SHA256 (Block Header xor Workmask) to see if you meet the target.) 
There was some significant thinking put into the process, particularly in requiring the work of the slow Whirlpool hash against the Blob dataset to make an 'either or' determination between Skein or Keccak.   Aside from requiring any ASIC to implement three more full hashes (from different families) in silicon in addition to SHA256, there a practical limit to trying to parallelize things.    One might think:  OK, we'll figure out the blob and then run Whirlpool, and Skein and Keccak against the Blob in parallel and pick between the outputs of Skein and Keccak based on the hi/low logic of the resulting Whirlpool hash.  BUT, since we're dealing with several different families of hashes with different internal statesizes, different compression functions, very different operations and even different round counts, many of the silicon level tricks for parallelizing things will be of only limited benefit. 
Again, the goal here is not to pretend that ASICs cannot be used to optimize any type of process, but to create a proof of work that an ASIC focused solution is not EFFICIENT enough to be attractive or practical.   
OH, and I should add:   It still has to be relatively EASY for other nodes and wallets to figure out if the resulting hash is VALID or not.   Even with this added work, I say it is.   Another node simply looks at the block header, sees what the hash is for the given nonce, and rolls through the process a single time with that nonce and header to see if it results in a matching hash. 
Thin wallets looking at a new block header would run one local SHA256 operation on the header to figure out the Target1 and Target2 blocks, and then query the network and get the target blocks necessary to compute the WorkMask from full nodes, running the remaining operations to verify the hash locally.   Slightly slower but not by much.   Exchange run thin wallets (where there is already trust between the wallet holder and exchange) could even have the process of verifying blocks done locally on the exchange hardware and communicated cryptographically to the thin wallets. 