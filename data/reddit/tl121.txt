@_author: tl121
@_date: 2015-12-16 17:25:11
Recent node software has torrent-like software built in for initial block download.  Using bootstrap.dat will be slower, because recent node software overlaps block download with initial block validation which is very CPU and I/O intensive.
@_date: 2015-12-31 02:18:37
The stalling trick is to keep proposing new improvements that are, at best, only minor, rather than taking a single step that solves the problem.  This is a transparent game to anyone who has been around for several decades or more.
@_date: 2015-12-28 04:32:09
It seems clear enough how Coinbase and the other exchanges should handle coins in the event of a fork.  And this applies to Alice and Bob, too:
1.  Blocks must not pay Alice's coins if she required a signature and the signature made with the private key is missing
2.  Blocks must not inflate the currency by crediting coins that do not exist in the coinbase according to the schedule leading to 21 M.
These are both economic reasons necessary for community support. They are the only fundamental requirements as to whether or not a blockchain is valid. If there is more than one such blockchain, the one with the largest difficulty is the the one chosen by Nakamoto consensus.
This is a simple problem.  There is no reason to pay the slightest attention to the people promoting FUD and BS.
@_date: 2015-12-19 18:16:37
If the question is safest way to STORE bitcoins, I might agree with paper wallets.  However, when it comes time to RETREIVE bitcoins Trezor is going to be safer, because of operations security issues.
@_date: 2015-12-30 22:02:05
A block size increase will not affect the vast majority of bitcoin users who run SPV or web clients.  The problem is with full nodes.
There is no reason or excuse for anyone who runs a full node not to be following the bitcoin news. But even if they are totally out to lunch and fail to upgrade their node before a change, they will notice that their node isn't working and will learn that they have to upgrade their software.  Doing this will take a few minutes of their time.  Downloading and switching to new bitcoin software is less effort than many operating system updates.
@_date: 2015-12-28 04:16:05
If Coinbase blocks user transactions it's censorship. If they do it because of US government pressure, it is not Coinbase censorship, it is US government censorship. 
@_date: 2015-12-31 02:48:15
If the signatures are still needed in the block chain then they should be counted in the block size.  Changing the counting is a scam, nothing more.  There are reasons for changing the data structures around signatures (malleability) but these have nothing to do with blocksize, even less over the political debate around block size.
@_date: 2015-12-31 05:15:57
I think you are missing a few details here.  The key one is that if a single transaction is invalid it can invalidate a cascade of transactions over time, ultimately polluting a large fraction of the blockchain with erroneous data. There are also DoS implications if nodes forward blocks before validating them.  Even without attacks there are questions of error propagation.  There are trust issues, especially for users of SPV wallets which verify that a transaction is in a block but which have to assume that all transactions in a block are valid, since they lack the necessary context to do complete validation.
@_date: 2015-12-13 15:46:51
TANSTAAFL. It's BS that people expect free use of my electricity, which I use in mining and in forwarding transactions and blocks. Personal and family relationships involving people I don't know and don't want to know have nothing to do with my electricty bills.
Reasonable fees based on the actual costs of using the bitcoin network and writing data into many copies of the blockchain are a few cents.  There is no excuse for fees to deviate much from costs, and this won't happen unless there is artificial cartelization/centraliation.
@_date: 2015-12-29 17:38:32
The root problem is backward compatibility with insecure mag stripe technology, coupled with a requirement that the cards are inexpensive, hence requiring the user to use the keyboard and display owned by the merchant (or hacker).
@_date: 2015-12-30 19:15:51
The total cost to a 10,000 network of full bitcoin nodes to process a transaction is a few US cents, measured by today's marginal cost of storage, processing, and communications bandwidth. This figure does not change with increased block size.  Unless one is prepared to argue that the network should have more nodes, there is no reason not to settle transactions as low as $1.00 USD directly on the blockchain.  The user must tradeoff the value of his time vs. the complexity and uncertainty of fees, off network settlement, etc.. and the amount of attention equal in value to present transaction fees is only a few seconds at typical wage rates.
@_date: 2015-12-09 04:58:42
Thank you for doing actual engineering, ...
@_date: 2015-12-19 01:13:24
With respect to the China how much less does it cost to own/operate/lease a fab in China vs. elsewhere?  How does electricity and cooling cost compare to elsewhere?
@_date: 2015-12-31 02:13:33
Seg witness does nothing to reduce the bandwidth required for a given transaction capacity.  Calling it a scaling fix is dishonest.  It may be a good idea for a different reason, namely a bug fix to transaction malleability, but that's a separate discussion.
@_date: 2015-12-15 23:54:17
Who could possibly care? The "karma" is hidden.
@_date: 2015-12-06 23:55:43
There is no need for SPV mining to overcome delays due to the GFW. The miner runs two nodes.  A local, physically secured node on his low bandwidth network and a hired node on a high bandwidth low latency network outside the GFW.  The local node is not critical and does not need to run 24/7  Its only purpose is to (occasionally) verify that the hired node is (apparently) following the programmed policy.  The high performance well connected node runs proper software and does full verification.
@_date: 2015-12-31 02:38:07
This is a reason not to have stupid verifying code or to allow overly large transactions.  This is not a reason to limit block size.
It is a reason to look very carefully at all of the code and identify everything that is not O(n) or O(n)log(n) and exterminate it, if necessary by changing the block data structures, even if this requires a fork.
@_date: 2015-12-06 23:48:22
For each round trip in the protocol a 240 msec round trip amounts to an increase of less than 0.1% in the orphan rate.  There is no need for more than one round trip delay with an efficient protocol. 
@_date: 2015-12-14 15:55:09
They made a plan.  Plans necessitate "guesswork".  There is nothing arrogant about planning.  Not planning for the future is not only arrogant, it is stupid.
The block size limit is a safety mechanism to deal with rogue miners. It is not a fundamental operating parameter.  The fundamental operating parameter is the block size of each block and that is set by the miner who creates the block, at the risk of being vetoed by the next miner to score a block if the block is too large and arrives late.
@_date: 2015-12-31 02:14:55
It's not splitting hairs to note that a thousand line software change is not an effective solution to a problem that can be fixed with a one bit software change.  The simpler change is less risky and can obviously be deployed quicker.
@_date: 2015-12-09 04:55:00
I was not questioning your comments.  I agree with you.
@_date: 2015-12-14 03:32:16
This displays fundamental ignorance of the implications of priority queues and priority queuing.  Resource allocation decisions need to be fair and efficient for stable system operation and a decent user experience. This has been known for over 50 years.
@_date: 2015-12-31 02:56:43
My modest computing environment and poor network connectivity would have no problem running 8 MB blocks today using an Atom Nuc on a DSL link.  Quite possibly with the new signature verification code coming in 0.12 there would be no problem with 16 MB blocks.  My setup is sufficient for running a full verifying node.  It is not suitable for running a mining node with the current peer to peer software even with 1 MB blocks due to orphan risk. I mine using mining pools and/or solo mine with solo mining pools.  My mining operation is totally isolated and unaffected by blocksize issues.
@_date: 2015-12-31 02:26:37
Since SW doesn't reduce the number of bits that have to be transmitted and that's the problem behind larger blocks, there is nothing "responsible" about proposing SW as a blocksize fix.  In this context, it's just complexifying things so that new counting rules can be used to confuse opponents and/or save face.  
@_date: 2015-12-31 00:01:47
I suggest looking at two things to address two types of node usage. One is for miners, who need to solve the orphan problem.  The other is for full nodes that don't mine, but do want to perform full verification for security and privacy.  My network service lacks enough uplink bandwidth to run a mining node, even with 1 MB blocks, but would probably handle 0.5 GB blocks if all that was necessary was to track the blocks, based on the time it takes to download 500 MB files which I do from time to time.  Computing power and efficient software is another question, of course...
Obviously, because mining is the most difficult part of the problem, this needs to get priority, but I believe it would be worthwhile to demonstrate that residential computer users can also support huge blocks, with suitably fast consumer computers and improved software. 
@_date: 2015-12-31 16:52:55
There is no backward compatibility.  There is only backwards deceit.  Old clients will not be able to verify new blocks. Therefore they will not be able to operate correctly.  If they are given a block that is invalid because there is no valid signature for one of the transactions they will not be able to tell.  This means they will make a wrong decision, e.g. forwarding the block or showing their user confirmations for wallet transactions that aren't correct.  This is not backward compatible.
@_date: 2015-12-16 00:32:40
If there is no financial incentive for running a full node, why would anyone bother do do so?  (Other than as computer scientists or bitcoin geeks.)  In fact, full nodes provide specific value to their users, namely they validate transactions.  If one runs a full node that one trusts, then one can query it to find trustworthy status of transactions, as well as the overall integrity of the network (e.g. such as coin issuance is within the 21M schedule).
Full clients such as bitcoin core already provide this value to their users. SPV clients, such as Electrum can also do this, because they allow the user to select the specific nodes that they trust. It would only be a small step to go beyond donation support to charging for these client services.  Other SPV clients, such as Multibit HD, do not provide visibility to their users of the full nodes they are free loading off of, thereby avoiding any possibility of trust relationships or remuneration.  This could easily be changed.
@_date: 2015-12-31 02:43:29
If he knew the math and said it, he was dishonest.  If he did not know the math and said it, he was irresponsible.
@_date: 2015-12-30 20:56:19
I've not seen any significant error or omission in Peter r's research.  If you have anything, please provide links. The value of Peter r's work with subchains is not the novelty of the idea.  The value is the analysis and explanation of the idea, both of which are needed to turn the idea into reality.
Maxwell's work on privacy has value, but the papers you linked to not relate to the network performance issues associated with blocksize increase.   From discussions I've had with him I see no indication that he has particular expertise in network performance as it relates to protocol design and implementation, performance modeling and measurement, etc...  These are the critical technical skills needed right now to improve the performance of the bitcoin network.
@_date: 2015-12-30 18:58:44
It is not possible to make this decision "strictly by technicals". There are tradeoffs in different aspects of system performance, security, etc...  If there were a single dimension of "goodness" that everyone could agree with, then a strict technical choice might be possible. However, individual players in the Bitcoin ecosystem place different weight on the various dimensions.  This is why it is not possible for a community to make decisions based on strictly technical considerations.  Technocrats can make decisions whatever way they wish, but if their decisions don't meet the needs of the users they will be irrelevant (unless backed up by a dictator willing to apply physical force).
@_date: 2015-12-30 19:42:05
It would be useful for a few "huge" blockers to demonstrate the hardware and software and networking required to run GB blocks today.  I don't believe it's necessary to move into data centers to do this.  The needed bandwidth is becoming widely available even in rural locations if the goal is running a verifying node that can safely track the network in real time.  Being able to mine without high orphan risks requires a protocol change of some kind and would be more difficult, but it looks like there are solutions being proposed for this as well.
This demonstration, if successful, would go a long way to erase fears over modest block size increase.  In all likelihood it would also unearth and overcome obstacles that may be presently unknown (e.g. software performance bugs).
@_date: 2015-12-15 02:18:31
Correct.  Having more business than you can handle is, in the long run, far worse than not having enough.
@_date: 2015-12-31 02:29:30
Action is node operators deploying software.  Discussion of what might be done is not action.
@_date: 2015-12-16 17:16:38
A bitcoin node is serving other nodes.  It should be run in a stable environment.  If your electric power is unreliable then the computer should have some degree of battery backup, at least enough to ensure orderly shutdown.
If the DB is backed up regularly, it will be possible to recover from backed up files and not go back more than a few days.  Users who don't know to back up their data have no business running a bitcoin node, or anything more than a thin client to a web service.
@_date: 2015-12-02 03:36:52
No, he did not.  Gavin questioned Greg's actions.
@_date: 2015-12-16 00:12:35
This is false.  Some blocks are full.  On average, utilization is above 50 percent.  Based on the previous growth in volume, we are months away from saturation.  We appear to be fine for the moment, but the trend is clear to anyone with any vision or experience managing real-time systems.
@_date: 2015-12-16 00:01:24
In which fraction of situations would you be better off just using bitcoin directly?  This is a crucial question if one is advocating LN as a scaling solution, because it directly limits the potential scaling benefit of LN.  I have seen no considered and/or debated analysis of this question.
@_date: 2015-12-16 17:19:00
I have yet to see any detailed argument that connects the number of nodes to "network health".  Indeed, I've not seen any kind of definition of "network health".  
@_date: 2015-12-25 18:08:48
The core dev's are just a bunch of C++ coders.  Their creation of a tool does not give them authority to tell people how the tool should be used. Appeals to authority are inappropriate, even possibly absurd,  in the context a system designed to be decentralized.
@_date: 2015-12-30 21:28:38
Centralization of what?  Mining nodes?  Owners of hashing farms?   What matters for the safety of the blockchain is for there to be many owners of hashing farms.  It doesn't matter how many mining nodes there are.  And the block sizes being generated by the mining nodes does not affect the number of hashing nodes. 
@_date: 2015-12-30 22:08:50
There is a huge difference between the "centrally planned" blocksize and the "centrally planned" limit on coins.  A change in the blocksize  does not affect Alice and Bob who use SPV wallets or hodl coins in paper wallets. A change in the limit on coins affects the value of Alice and Bob's coins.
@_date: 2015-12-30 22:15:25
"Not to decide is to decide"
"If you choose not to decide, you still have made a choice"
@_date: 2015-12-15 23:48:11
To get any scaling benefit from LN, either there has to be a series of transactions between two parties, or there has to be an intermediary that does aggregation.  I have no problem with the first case; it's perfect for a series of micro transactions. Indeed, there is no third party.  However, in the other case, which is needed for scaling for arbitrary transactions, there is an intermediary and this intermediary require some level of trust.
@_date: 2015-12-31 02:09:41
A hard fork to increase the blocksize could be done successfully over night if there was sufficient will, as the necessary software already exists.  It requires trivial changes to full node software that amounts to patching a few bits of the source.  It does not require any change to SPV client software.  SW requires extensive new coding, including changes to client software, before it generates  (minimal) scaling benefits.
@_date: 2015-12-09 05:00:45
And if they have enough bitcoin to worry, they can always run a fully validating node themselves.
@_date: 2015-12-30 19:01:32
If Bitcoin were a product of a well-managed company, developers like these would have been shown the door some time ago.
@_date: 2016-03-03 22:34:36
Chargebacks can come months after a purchase.  From the merchant's point of view, how is this different from a bitcoin double spend?  In either case, a cheating user can defraud the merchant.
@_date: 2016-01-10 17:15:52
It's not just new nodes.  It's old nodes that have been offline for a few days.  However, there is a simple defense if one's upstream bandwidth is limited: configure QoS in your router.  It is also possible to limit network usage in the bitcoin software, and I have heard that this will be in the next release of bitcoin core.  It is already in other node software.
@_date: 2016-01-02 01:37:39
You should escape from under the thumb of the controlled media, a.k.a. North Korea.
@_date: 2016-01-05 15:29:48
Man wonders how any coder could be so stupid to propose something that causes hundreds of smart people to waste time debating a useless feature.  Man wonders how people continue to pay attention to others who do not see this.
@_date: 2016-01-05 15:31:40
If it's really necessary, then LN is such that it can not work even with RBF.
@_date: 2016-01-10 22:54:41
Unfortunately, helping the network may not have been useful in the past when the traffic load might have been light anyhow.  The problem with a node trying to catch up is that it creates peaky load.  Before I implemented QoS features on my full router, I could run well off a DSL connection with slow upload while still enjoying web surfing, videos, etc...  However, once someone started downloading old blocks my network was pretty much killed.  Router changes fixed that, but they were much harder to set up than simple commands in my bittorrent software.
@_date: 2016-06-13 13:02:38
Spam is a problem for email because it is free to send email.  If there had been a way to send microtransactions over the Internet there would not be a spam problem.   And there would be no damned advertising on web pages, either, which wastes *vastly* more bandwidth than the total bandwidth used by Bitcoin. 
@_date: 2017-08-10 21:23:57


This is where I jumped off.  The most competent developers of any product must necessarily include people driving development in the interest of the product's users.  Core has not done this since it was captured by Blockstream.  Regardless of their coding skills, these people have nearly ruined Bitcoin.  If these people are the most competent developers on the planet then Bitcoin is doomed, because it means there are no competent Bitcoin developers to be found anywhere.  And if the Economic Powers haven't been able to figure out that the Core developers are incompetent after the past year or two, then there is not much hope for Bitcoin regardless of who the developers are, since this implies that the majority of the economic powers are also incompetent.
Core has no sound long term plan for scaling and efficiency.  If they did they would have performance numbers and benchmarks for various computing configurations, and various model networks, including various plausible scenarios for how Bitcoin can scale both on and off chain and how far this is practical.  Depending on Lightening Network as part of a scaling plan doesn't look so hot, since two years have gone by and LN is not even a prototype network.  It's just a breadboard.
@_date: 2016-06-13 12:45:08
Where is you calculation of how much it costs for blockspace?
@_date: 2016-11-18 14:47:09
There are people who looked at Segwit and decided that it was a bad idea because of unnecessary complexity and security risks. This caused us to have problems with the group of people promoting and developing this software.
@_date: 2016-11-18 19:35:43
Your arguments do not refute my point that a new attack vector was created.  They are of the form, "Don't worry.  The necessary events won't happen for reasons:  X, Y, Z..."   
Experience with security sensitive software has shown that these types of assumptions, while usually correct, are often the source of creative and profitable attacks. This is particularly the case around complex structures intended to provide "backward compatibility" which from a normal user standpoint is definitely desirable, but which experience has shown leads to many security bugs.
@_date: 2016-12-05 22:09:23
I don't recall the reference or even have a good recollection to facilitate a Google search, but I recall one paper going into highly technical aspects (at least for me) of semiconductor fab processes and reverse engineering technology that involved only doping at a few specific points, which wouldn't have amounted to a mask change and would be essentially impossible to find if you didn't know where to look.  
One of the reasons why NSA was investigating adding Trojans to their chips was to evaluate the possibility that foreign supplies might be doing this to US military equipment.  This would be evidence to be used to justify procuring critical equipment from trusted (domestic) suppliers.  I have heard second hand that special FABs were set up for this purpose (among others) but I never investigated this because it seemed quite plausible in what I knew of NSA and their personnel and management.
@_date: 2016-12-16 18:37:50
"Nodes can leave and rejoin the network at will, accepting the proof-of-work chain as proof of what happened while they were gone. They vote with their CPU power, expressing their acceptance of valid blocks by working on extending them and rejecting invalid blocks by refusing to work on them. Any needed rules and incentives can be enforced with this consensus mechanism."
@_date: 2016-12-05 16:40:12
Luke may be surprised that I agree with him, but what he says about these CPUs is much more likely to be true than not.  And this is the "documented" features.  There can be undocumented features as well.  In most cases, the addition of a single wire without even any extra transistors is all it takes to void the hardware security on which an operating system kernel security depends. 
I will give two references showing how this was done back in the early 1970's.  It amounted to adding a single wire to the backplane of the processor and the wiring change (Trojan horse) was infiltrated into the manufacturing line and got into units subsequently produced.  This enabled a "tiger team" to demonstrate that the "secure" operating system wasn't.  
There have been papers published showing how an extra wire in a semiconductor chip could be effectively hidden from most laboratory analysis of the chip.  Most recently, defects in RAM memory design allow certain types of random errors to flip one critical bit and this has been used to gain control of an operating system. 
As to exfiltrating data.  There is a huge literature on ways this can be done through various side channel hardware and software attacks. 
The bottom line is that there is no reason to trust any piece of hardware if there is any possibility your machine may be a target of a nation-state security agency.
I suggest reading the newer article first. (Section 3.1 Installing Trap Doors.)
The original paper (which I read back in the day) is available here:
@_date: 2016-11-28 01:25:30
The present code base contains one arcane constant, which has a value of 1 MB.  The proposed Segwit as a Soft fork replaces this with two new arcane constants.  This is roughly double the number of arcane constants.  (I say "arcane" because these have no technical basis established by any documented research. They are just stupid wild ass guesses made up by a cult.)
I can't speak to the Core code base management of variables across all of its aspect, since my initial forays into the code base caused me to retch with disgust.  However, if there were a single set of global parameters incorporated into all relevant modules and enforced by competent development leadership, changing this single parameter would amount to a one line source code change, followed by a single recompile.   I am familiar with several much larger software projects (including complete operating systems) where there was a simple process to specify configuration parameters and build accordingly and I  know for a fact that a competent development team can produce a competent development environment where changing one parameter can be done automatically by typing no more than a few characters on a command line terminal. And after the build was complete another few commands would run the (updated) test suite.
There is actually no need for logic for activation, because there are already two parameters, one to limit the maximum block size that will be accepted by node and the other to limit the maximum block size that will be generated by a node (should it happen to be controlling a mining operation).  This already is sufficient to allow for an orderly migration to a larger block size, without any additional logic, which is, IMO optional, if not totally superfluous.  As such there would be no "activation".  The miners would simply go through a two phase process, upping one parameter and then at a later date upping the other parameter and actually generating larger blocks.  Nodes would have had the opportunity to upgrade the software.
And note.  There is no need for anyone to even make a source code change and recompile.  The necessary software has been released for several months.  The most anyone would have to do would be to make a command line change, and only that if they were running a mining operation.   The problem is purely one of politics, namely which software to run.  There is no development required.
@_date: 2016-11-23 21:32:09
You are making assumptions as to the behavior of the miners as to which version of software they run.  The problem here is, unlike previous software upgrades, the choice of software can expose user funds to theft.  This breaks the fundamental assumption of security that users expect, namely they won't have their funds stolen if they keep their private keys secret and don't sign any transactions they don't approve.  It is possible for funds to be lost with no chain fork at all, if for some reason all the miners were to revert back to pre-Segwit software.  This is a totally unnecessary security risk that would not have been necessary if SegWit had been done as a hard fork.  (In that case, if the software had to be reverted back for some reason no funds could be stolen, but any new Segwit transactions would be void and users would have to enter new transactions to repay any debts.)
There have been many cases in the past in security protocols where attempts to be "backward compatible" created security holes.  You can see today with all of the bugs associated with SSL, TLS and web security.  This is irresponsible design and totally unnecessary.  
As to conservative estimates:  nuclear power is safe, right...
@_date: 2016-11-18 14:46:06
Actually, it could be rolled back, because it has been executed as a soft fork. Nodes running older software would still interpret the chain and bitcoin would still run. The problem would be that coins sent to segwit addresses would be ripe for theft. The distinction here is between "can not" and "must not". 
@_date: 2016-11-28 15:10:36
The assumption is that after 95% start running Segwit there won't possibly be any need to roll it back.  I agree that is unlikely, but designing the system in such a way as to effectively make a roll back impossible (because of a security hole) is irresponsible.  Note that until Segwit activates users won't begin using it.  This ensures that any bugs that might be discovered by actual user usage won't be found until it is already too late to go back.  **This is an insanely dangerous practice, and amounts to burning bridges to keep the troops in line.**
@_date: 2016-11-18 20:30:05
This has nothing to do with zero confirmations.  This has to do with how pre-Segwit nodes process transactions and how this represents a risk if scripts are exposed to thieves that allow them to generate unsigned transactions that pay former Segwit users' funds. The scenario does require reverting a majority of hash power following a false activation of Segwit.
@_date: 2016-11-28 21:02:51
Non-standard transactions can be mined.  If they weren't valid, then it wouldn't be a soft fork.  So now there's another level of complexity involved in deciding whether something is safe.  How do you prevent an existing miner from mining a non-standard transaction and still call it a soft fork?
The entire concept of non-standard transactions being mined on a discretionary basis struck me from the beginning as ill-advised complexity. Building normal user functionality on top of this machinery seems foolish, at best.   I saw, and still see, little need for a "new electronic cash system that's fully peer-to-peer" to have this level of complexity, if only because it invites developers to think up clever ways of using it, rather than think of clever ways of making the basic mechanisms needed to move electronic gold blindingly fast.  But this is just my peculiar bias.  I would rather use something that is blindly fast and works 100% of the time than mess with something that sometimes does neat things and sometimes doesn't. 
@_date: 2016-11-28 15:04:39
"unable to pay" would seem to imply that the coding wasn't a "soft fork".  Please explain.
@_date: 2016-11-28 00:55:37
Baroque: "characterized by grotesqueness, extravagance, complexity, or flamboyance" 
Arcane: " known or knowable only to the initiate :  secret &lt;arcane rites&gt;; broadly :  mysterious, obscure"
These comments apply to the proposed capacity limits as rolled out by Core in SegWit as a soft fork, coupled with the attitude of the Core developers as to the competence of people who do not belong to their gang.
The present capacity limit comes from a single constant that can be changed in one line of code (for example 1 MB becomes 4 MB).  Done.
@_date: 2016-12-05 18:54:05
I do.  And I don't know how it occasionally happens.  Usually, I notice it by going to the forum and checking to see what the status is, but it didn't seem to work at the time.  I checked again after seeing your post, so I deleted the copy that didn't have any replies.  Thank you.
@_date: 2016-11-15 16:46:09


Your parenthetical comment is incorrect.  It is possible to use Bitcoin (send funds, receive funds, check balances) using an SPV client.  It is not necessary to run a node to do these functions.  Hence the use of "ie" is incorrect.
@_date: 2016-11-28 19:19:45


The security risk is **not** with the Segwit transactions, which are "nobody can spend" to the old nodes.  The security risk concerns the P2SH Segwit addresses that have been (or still are) funded.  These addresses have "hidden" behind them a script which can be used by a thief to create a valid **non-Segwit** transaction and steal the funds.  The "hidden" script may be hidden (at the time of creation) in the user's wallet, but it can not remain hidden to potential thieves the moment it is broadcast.  
@_date: 2016-12-17 16:43:49
The White Paper was not a design.  It was a white paper.  The actual design came later and was materialized as a program that did not have a blocksize limit.  (As to when the port number 8333 became a default, I have no idea, nor is it relevant.  It is definitely not part of the consensus protocol, which is what is relevant to the blocksize debate.)
@_date: 2016-11-18 14:45:14
It adds a new security risk to holders of bitcoins who receive coins sent to segwit addresses, because older nodes do not check signatures. If, for some reason, nodes with a majority of hash power were to roll back to older software, a thief could steal coins in these addresses.
@_date: 2016-11-18 19:38:41
The security risks concern the "anyone can pay" kluge that enables older nodes to accept Segwit transactions without checking the signatures.  This has been described elsewhere and I am not going to repeat my posts again.
@_date: 2016-12-16 18:54:13
This neglects the possibility of reducing the orphan window by protocol, software and hardware improvements, which is coupled with the reward structure.  Difficulty adjustment also comes into the analysis, since hashpower wasted on orphan blocks doesn't increase the difficulty. 
The mined empty blocks are not worthless to the network.  They secure all transactions in earlier blocks by making it more costly to change them.  In addition, these empty blocks cost almost nothing to the network (just the small overhead associated with the block headers) **except for the absurdly low block size limit**.  Without this blocksize limit any transactions that *should* have gone into the empty block could appear in the next block.
@_date: 2016-11-28 19:36:16
A full validating node is useful only to the node's owner and people who know and trust the node's owner.  Everyone else needs to run their own full validating node if they are to have a fully trustless system.  There will be no way for people to run a fully validating node if there aren't full storage nodes.  So to the extent that fully validating nodes and full storage nodes have significantly different operating costs there is a good chance there may be "too" few full storage nodes.
Possible conclusion to think about:  To the extent that the cost savings associated with discarding signatures is insignificant then the ability to discard signatures is irrelevant.  To the extent that the cost savings associated with discarding signatures is relevant then facilitating this capability may actually be a bad idea.  Here, the burden of proof ought to be for the people proposing added complexity to show that adding this complexity is actually a good idea.  (It may be, in fact, but it strikes me as marginal.)
@_date: 2016-12-16 18:44:19
Proof of work is fundamental to Nakamoto consensus which is fundamental to the operation of bitcoin.  If you think that this work is useless, you simply do not understand how Bitcoin works. 
@_date: 2016-11-28 00:19:24
Folks are interested in their transactions going through in a timely fashion at reasonable fees.  They are not interested in the subtleties of baroque block size limit calculations or arcane recoding of transaction formats whose only benefits are to save a few bytes per transaction.  None of this klugery constitutes an effective capacity improvement, since the only operative capacity limit is a purely arbitrary one that can be trivially changed.
 
@_date: 2016-11-28 13:51:15
Please read what I wrote.  I did not say that the present code base contains *only* one arcane constant.  There are many arcane constants in bitcoin, for example 100 and 2016.  
If there are multiple constants that need to be changed when changing the maximum block size then these should be all related, put together in one place and connected automatically by software that runs at run time or system initialization.  In some cases, the relationship is the result of disgusting bugs, technical debt that should have been removed years ago, such as the quadratic overhead.  (This is the result of a poor design, but it relates to the block size only because a limit wasn't placed on transaction size or complexity *ab initio* which if done would have been a separate constant, related in a clear way to the maximum block size.  And yes, obviously the maximum block size has to be large enough to hold the header, etc...) All the related constants should have been gathered together in a specification. 
What specification?  That's the problem with Bitcoin.  Bitcoin was a neat demonstration hack, but not an engineered piece of software.  At the $10B market cap and the better part of a decade of operation there is simply no excuse for its present state.  This is the main reason why I want to see the present development team excised from the Bitcoin universe and replaced by people who are software engineers, not mere computer programmers. 
What you call "bold ignorance" is deliberate on my part.  Any ignorance on my part reflects the poor design of the Bitcoin code base and the lack of good software engineering practices on the part of the Bitcoin Core developers. As to the btcd developers.  I sympathize with their plight.  They were given a hopeless task, to reverse engineer and produce a bug compatible replacement of spaghetti code.  I tried running btcd some years ago, but it was useless for me because it did not replicate the RPC interface and would not work with my Electrum server.  When I communicated to the btcd developers and they didn't seem to understand why this was important I concluded it wasn't worth any more of my time interacting with them.
@_date: 2016-11-15 16:49:53
The bug does not cause CPU requirements to grow *exponentially*.  The bug causes CPU requirements to grow quadratically. 
@_date: 2016-11-23 21:23:12
It is not possible to calculate these kinds of probabilities, because they don't represent random chance.  They represent decisions and actions by individuals, potentially hostile individuals.
The risk I was talking about comes from creating a Segwit address, funding it, and exposing the script by making a transaction to this address (or just broadcasting a transaction that gets seen by a bad guy, but for some reason or other not mined). Now the necessary "anybody can pay" script is available as an input for a thief to use.  He can keep this information until it becomes clear that a majority of miners are now (or have reverted to) pre-SetwitSF software.  Then he can create a transaction to pay himself all the funds in this address. It is entirely possible that all of this can happen without any chain fork.  This is a consequence of the "soft fork" and also a consequence of the very unsafe operation of using this encoding.  
Notice that this has nothing to do with 0-conf operation or SPV clients.  And the transaction that funds the "SegWit" address with the ill-advised script as well as the thief's transaction are not SegWit transactions.    
@_date: 2016-11-28 13:19:56


Satoshi's design was to have the miners collaborate, that's how Bitcoin works.
@_date: 2016-12-16 18:33:08


The constant was not even in the original design.  Of course it can be changed.  It was put in by central planning as a matter of fact, and so it can be taken out the same way.  It can also be taken out in a distributed fashion by running software that simply ignores this constant.
@_date: 2019-02-27 14:57:48
Fancy mathematical analysis of an incorrect model.
There are a hyper astronomical number of solutions meeting the bitcoin difficulty, as adjusted to allow one solution approximately every ten minutes. The size of the search space is vastly larger, such that the chance of any combination of player or players duplicating a guess during the search period is vanishingly small. Therefore there is no practical difference between sampling without replacement and sampling with replacement and hence no Matthew effect to be concerned about. Furthermore, were this not the case the design of miners, mining farms and mining pools would be completely different.
@_date: 2015-08-31 17:38:32
T-mobile could keep their business model by using fair and efficient congestion management policies. This would allow unlimited usage of idle resources while prioritizing lower users over heavy users attempting to share congested resources. Heavy users would still get unlimited usage when the network wasn't saturated while normal users would not get significant degradation of their service. (This won't happen if the CEO is ignorant of technology.)
@_date: 2019-04-08 15:36:33
I am not sure what you mean by "civilized discussion".  If a "civilized discussion" includes discussion between technically and economically competent people prepared to deal with the fundamental limitations of the Lightening Network concept and architecture, then it might be possible.  However,  I have seen no evidence that the proponents of LN meet this criteria. 
Indeed, after repeated requests over several years to LN proponents to answer fundamental questions it appears that there is no one competent to argue in favor of LN.  That being the case, the answer would appear to be "no".
@_date: 2017-04-27 00:06:54
This is an obvious technique to bypass DNS which anyone qualified to be running a mining farm would be aware of.  Manipulating host files is something that I've done many times when doing various network tests.  Someone who owns a big mining farm would have to be an idiot if the people he hired to run his infrastructure didn't have these basic network skills.
As to software that "phones home".  Just about all the operating systems and all the applications programs there are do that.  That includes all versions of Ubuntu and Windows that I run, all web browsers, email applications, office software, etc...  This is standard practice.  Of course it is possible to block (most) of this by appropriate firewall settings.  But software comes with so many security bugs that disabling the update feature adds more problems and risks.
@_date: 2017-04-07 15:29:59
Header changes just require different software for generating collisions. Header changes require lots of different software anyhow, so this would not be a big deal.
@_date: 2017-04-26 23:51:57


No, it does not require a recompile.  It just requires changing the /etc/hosts file.  This is trivial to do by SSH into the miner.
@_date: 2016-02-23 17:02:28
If I were a central banker and wanted to bring down bitcoin, I would arrange for some of my VC buddies to buy up as many foolish or corruptible bitcoin developers and attempt to sabotage the project.  I would then use the rest of my money to corrupt communications channels, bribe large miners, hire black-hat hackers and criminal gangs to DD0S nodes, etc...
@_date: 2016-02-27 01:21:54
This is a problem with how transactions are sent out.  It has nothing to do with how blocks are sent out.  Both functions need to be implemented efficiently.
There is no need for any node to receive any given transaction more than one time.  Anything beyond this is protocol overhead. There are various known ways that this level of performance can be achieved, minus a small overhead factor to account for protocol overhead.
The problem appears to be that "cryptographers" are working on a network protocol, rather than network protocol experts.
@_date: 2016-02-26 21:47:05
Run an SPV client.  It will give you proof that transactions you send and receive were confirmed.
@_date: 2017-04-27 00:24:53
They don't have the power to "kill" all of this hardware.  That would amount to rendering it permanently non-functional.
Instead, they allegedly have some software in place that could be used with some other software they haven't put in place that might be able to temporarily disable their mining hardware should they chose to do this.  Any serious mining operation wouldn't have been disabled at all by this "feature" and even those that were would have been back up and running in under an hour if someone competent became aware that mining had stopped.
What we are witnessing is a social media attack on Bitmain.  That's for certain.
@_date: 2016-02-26 21:43:19
If 88% of the bandwidth is being used for relaying transactions, something is definitely wrong somewhere, given that today all of those transactions end up in blocks and are not freeloading on the transaction transmission.   If this is the problem, then creating a fee market and RBF will only make the 88% problem worse by retransmitting transactions.  
@_date: 2017-04-07 16:44:34
The performance benefit is no more than 30%, and that's if finding infinite matches were free.  The cost of finding matches depends on details of the hardware and software used to compute matches vs. the cost of the hardware doing the hashing.  So the answer to your question is "somewhere between 0% and 30%."  The lower limit of zero comes from the obvious fact that if the cost exceeded the benefit it would be stupid to do the "optimization".
The match can be reused for many values of the nonce.  So a single match can speed up many hashes.  I'm not familiar with the details of how much this is.  It will depend on algorithms used to distribute work in parallel to the chip cores.
@_date: 2017-04-07 16:15:15
Finding lots of matches could be slower.  It's a point of diminishing returns.  If it costs nothing to find matches then the speed up is only 30%.  The cost of finding matches has to be less then the saving of this speedup, otherwise there's a net loss.
@_date: 2017-04-07 18:24:27
Please explain.  The header includes the Merkle root.  Unless I misunderstood you, how would you solve the circular hashing?
The "problem" appears to be inherent in the use of a hash function as the source of a parameterized proof of work.  Since Satoshi adopted hash cash as his proof of work, I guess you can blame Adam Back for this "problem".
I put "problem" in quotes because I don't see this as a problem that merits a fix.
@_date: 2017-04-27 00:35:12
For all I know Bitmain's compiled binaries have the ability to stop mining at block 500000 and turn all the mining engines on and turn off the fans and destroy the hardware.  And none of this appears in the source code.  ("On trusting Trust.")  And for all I know, North Korea (the real one, not "North Corea") holds the kill switch.
And the same applies to every computer in my automobile, including the engine computer and the anti-lock breaking computer. Ever had the throttle stuck open or the brakes fail.  I have, but both were purely mechanical having to do with a broken throttle return spring and rotten brake lines.
Lots of stuff to worry about, but this one is not one of the more serious ones.
@_date: 2017-04-07 15:46:15
Overt vs. covert are methods of creating matching data that is used in the chip.  These are software functions.  The chip inputs midstate information.  The terms "overt" and "covert" were conjured up by Maxwell to appear pejorative. You can see "covert" mode claimed in claim 14 of the ASIC boost patent application. **Nothing new here.**
I haven't seen any indication that the ASIC boost patent has issued in any jurisdiction.   Until this happens, there is really nothing to discuss.  If the Bitcoin protocol were controlled by a typical standards organization, then it would be reasonable for the standards organization to do the following:
1.  Ask the patent holder(s) to agree license the patent according according to reasonable and non-discriminatory terms.
2.  Make it clear that there could be changes to the standard in the future that would be used to make the patent ineffective or inapplicable if the patent holder(s) did not agree.
Since the patent hasn't issued yet, there could be other complications.  For example, the patent claims could be amended.  This could lead to all sorts of nasty complications that would greatly benefit lawyers and patent experts, but nobody else.
It appears that several groups independently developed forms of ASICboost.  This could also be used to challenge the issuance of the patent on the grounds that it would have been obvious to anyone with "ordinary skill in the art".  Certainly these kinds of collision based speedups were common in the design of hardware and software solving cryptographic problems, e.g. similar ideas were used in WWII by Turing in breaking the German Enigma machines.  In the late 1970's, Marty Hellman taught a cryptography short course that described many similar techniques. This speedup strikes me as "obvious" but then it could be argued that holding dozens of patents I am not one of "ordinary" skill in the art.
If I were the holder of the ASIC boost patent application, I would be working out a deal with Bitmain for reasonable license fees and making a public announcement to the community to this effect.  This could be a win-win for the entire community. It would also be a win-win for the inventors, since they would have something and avoid a lot of potential legal bills.
@_date: 2017-04-07 16:21:54
Sorry, which product(s) and where is the FPGA?
@_date: 2016-02-12 01:50:05
Without specifics, this is utter BS.
@_date: 2016-02-03 02:36:15
Correct.  The "patch" should have been made immediately after the problem was first discovered.  However, then a proper fix should have been made, such as a new way of mapping signatures to transactions, so as to fix the absurdly stupid quadratic behavior along with the absurd design of a system that uses changeable references to link transactions.  Set Wit appears to do this, but unfortunately it brings in much additional baggage, such as an entirely new data structure and the horrible soft fork kluge.
The basic, fundamental fault in Satoshi's design should have been fixed as soon as it was discovered and fixed in the simplest possible way.  
But what do I know, nobody is paying me $21M to fix (or not fix) problems? And bitcoin is an Open Source project and only coders make real contributions, according to the lines of source code the add to the respository.
@_date: 2016-02-28 19:11:14
The ones with the tools to do the hacking, ...
@_date: 2017-04-07 16:30:21
Thanks.  Do you know where the FPGA code is stored and how it is loaded?  That could affect how difficult it would be to reverse engineer how the FPGA works.  But it would be possible to put a logic analyzer on the communication between the ZYNQ and the circuit boards with the ASICs.  This wouldn't require expensive equipment, just a lot of fiddling.  This would be sufficient to show that the ZYNQ is computing the matches and, from examining the headers, what types of variation methods it uses.
There could be other practical problems as to why ASIC boost doesn't work as well as originally hoped.  If the ZYNQ has to work hard doing matching then it will impact the timeliness of work flow to the cores in the chips and this may affect chip performance, depending on how work queueing is implemented.  Just saying, I have no idea, other than shower thoughts a while ago as to how one would design an mining ASIC.
@_date: 2016-02-03 01:49:51
There are several obvious changes that could easily fix this problem.  The simplest one is to limit the number of inputs to a given transaction.
I would not call these people "incompetent" were it not for their claiming their unique expertise in bitcoin and how we must trust them.  Someone competent in computer protocol design, specification and performance analysis would not have made these elementary mistakes.
@_date: 2016-02-26 21:20:18
This 12% is a ratio argument.  This is one of the best known and most frequently used methods of lying using statistics, because it allows the creation of misleading statistics while leaving room for the perpetrator to weasel out, rather than get caught.
The correct way to present this argument would be to show specific numbers for transactions transmitted, specific bandwidth used for transmitting transactions, specific sizes and number of blocks transmitted, and specific bandwidth used for transmitting these blocks.
@_date: 2016-02-28 19:20:28
Gavin may be a model Bitcoin contributor, but based on his performance on the blocksize issue, he has not (yet) demonstrated successful leadership.  It may be that he has been too much of a nice guy to be an effective leader in an extremely hostile environment.
@_date: 2016-02-03 01:59:35
OK, number of signatures to the transaction.
The technical debt is there.  It is many years old.  It should have been fixed a long time ago.  It wasn't.  Instead we get new "fixes" that add new complexity.
@_date: 2016-02-03 01:30:10
This is a performance bug in the design and implication of bitcoin.  It has been known for some time.  It should have been fixed by appropriate changes.  One wonders about the competence of the people maintaining the bitcoin software.
@_date: 2018-03-08 22:21:58
The Bitmain patent claims several methods of regulating the voltage across series wired chips.  The patent would be invalid if someone could show prior art that all of these claims had been in use or publicly sold prior to certain dates.  That other people had wired chips in series with other methods would not invalidate claims limited to Bitmain's circuitry.  If some of the claims were in prior use, then those claims would be invalidated, but other claims would stand.
There is nothing illegal or unethical about filing patents that  makes claims that are not novel if the prior art was unknown.  It would be unethical and illegal to knowingly make false claims.  It is quite common for patents to issue with claims that are later found to be invalid by prior art. This commonly happens when the patent holder files a lawsuit for infringement and the defendant presents evidence of prior art. (I have a certain amount of experience searching out and documenting prior art for networking patents,  but not about power supply circuitry as this is not my area of expertise.)
Have you studied these patents in detail and compared Bitmain's circuitry to that of competitors?  If not, then would probably be unwise to accuse Bitmain of illegal or unethical conduct.
@_date: 2017-10-02 00:59:30
What ultimately matters is force.  Which kind of force do you want to use?  Hashpower, defined by investment in technology and electricity or government force, defined by lawyers back up by guns and ultimately nuclear bombs?  These are the alternatives.
@_date: 2017-10-02 00:56:50
The code behind the present Segwit based block chain has already been forked.  It is a done deal.
@_date: 2017-10-02 00:55:32
In November we will find out whether what people call "Bitcoin" is defined by Nakamoto consensus (majority of hashpower) or is defined by centralized authority.  At which point we can individual decide what matters to us and what, if anything, to do about the situation.
@_date: 2016-09-14 14:16:04
It looks like you are not an audiophile.
@_date: 2017-10-16 16:25:46
It is perfectly possible that the father of Bitcoin knew that keeping his keys secure was important, but that he didn't understand all the ways his keys could be lost or stolen.  It is also perfectly possible that he didn't particularly care back when Bitcoin had no significant market value.
@_date: 2017-10-02 02:13:21
Actually, I am quite happy and have ice cream most days of the week.  When I consider "force" to matter, I am just describing the laws of physics, namely the universe in which we live.  This is either the result of random action or God driven creation, your choice.  In either case I am completely happy that there is such a thing as "reality" and if you don't believe in it, then I am sorry for you that you have been type cast for such a peculiar role in the cosmic drama.  And yes, I prefer hash power to being nuked.  Do you?
@_date: 2017-10-02 02:10:35
Segwit 2X is a code change that has already been activated.  It is latent.  The rules have already been expanded.  You may think they haven't, but that is just because the present block numbers are below the coded in value.
@_date: 2018-07-03 15:50:10
The very complexity and confusion centered about how to measure the performance of the LN network belies the underlying complexity of the LN network.  This is a characteristic of over complex designs.  
@_date: 2018-07-03 16:49:49
There are ways to tell when something is over complex:
1 When the designers don't understand the implications of their design.
2 When others make mistakes in their analysis of the system
3 When there is an alternate solution that is simple and good enough to get the job done.  This is sometimes contentious up front, but obvious in retrospect.
@_date: 2017-10-19 23:49:35


Yes.  Bitcoin Core decided it was necessary to create a "fee market" to drive up fees for some reason or other.  They did it in a way that any fool could see would not work, i.e. either it would not result in increased fees (when the network was lightly loaded) or it would result in high fees (when the network was heavily loaded).  The result is an unstable and effectively unusable system.  
With a proper design (as was the case with the original bitcoin) wallets could set a fixed fee based on transaction size and there would be no need for user input and thus no confusion or opportunity for user error.
If the victim doesn't get his money back from the mining pool and he feels sufficient rage, then he should focus his rage at the people who created the fee market.
@_date: 2019-09-17 14:56:20
If you are planning to sell BTC figure out how many sales are you likely to want to make.  At times when the mempool is small move your coins into new addresses in a hardware wallet or cold wallet.  You want the funds you plan to move to be in a single UTXO,  so that you can send a small transaction to the exchange with a high fee per byte without spending a lot in fees.
Here's some advice on what not to do, based on personal experience.
You do not want to leave your wallet in many fragments, otherwise you will probably end up with huge fees, or worse lose the ability to hit the peak when it comes time to sell. In December 2017 I lost a lot of money in fees moving coins to an exchange because most of my funds were the result of small payouts by mining pools.  Worse,  I missed a lot more in potential revenue because the BTC network was busy at the time I tried to move my funds and the next day when I could sell the price was significantly lower.
@_date: 2017-10-02 15:20:35
The only relevant nodes are the nodes that are mining blocks.  There are many other nodes, such as mine, that will follow whatever blocks happened to be mined. (My node won't be checking Segwit signatures, but I don't care, since I don't care about fools that use Segwit addresses to hold their funds.)
@_date: 2015-09-05 02:33:41
There are criminal lunatic scum on the small block side.  I ran an XT node for a few weeks.  Two days ago one of these criminal scum ordered up a 10 gbps DDOS attack on my node.  It took down the main link into my ISP and killed all the internet service for six towns for several hours.  It also took out our long distance telephone service which was IP based. I could care less if my little node was attacked, but this was a terrorist attack and could have had life threatening consequences to innocent people.
@_date: 2016-10-23 18:47:55
Be warned.  It can take minutes for the server to shut down safely.  So if there are any problems (what can go wrong?) it may not shut down safely.  Unfortunately, if this happens it is necessary to delete the entire database and reload it from the central "foundry", because of incompetent database design and/or programming.
When you start the recovery you have to get a new database from the foundry.  You had better hope it's up to date.   How do I know this?  My Electrum server ran into some unknown glitches and wouldn't accept connections.  It went into an infinite loop.  Eventually I gave up and zapped the database.  Now I get these nice messages in the log:  ETA of 4300 minutes, about 3 days.  (And that assumes that the result is not DOA, if my patience lasts that long.)
Electrum is a typical example of what can be wrong with free and open source software.  It can have serious problems for years (that's the case here) that never get fixed. Of course the price is right.  (Actually not, because the time spent dealing with the hassles is not free.)
@_date: 2015-11-21 18:36:07
There are quantum resistant signature algorithms.  One method is Lamport signatures. Unfortunately, Lamport signatures are more than 100 times larger than the current bitcoin signatures, making them far from "good" from the standpoint of efficiency.
@_date: 2015-11-04 17:37:36
I doubt very much that the debit card transactions are free. (They are certainly not free where I live in the US.)  They may appear to be free from the consumer's point of view, but not from the merchant's point of view.  If the merchant is to stay in business he must price his goods to cover all of his costs.  Thus, in the end the consumer ends up paying fees to the banking system.  This works only because most consumers either don't care or are stupid.
@_date: 2015-11-30 01:29:21
Obviously, you are not a miner.  If you were, you would understand that mining does not come for free.  Or perhaps you don't pay your own electricity bill?
@_date: 2015-11-30 01:54:21
I read it that the dev's who rejected opt-in but approved the original pull were on the fence.  This goes to a reasonable determination of their motivation and/or games.
@_date: 2015-11-17 20:50:41
There is none.  The only mention of size appears in a discussion of the size of the block headers, where the overhead associated with the block size headers is shown to be minimal.
@_date: 2015-11-16 21:25:30
Why are the developers adding new features when they should be ensuring that the bitcoin network can continue to work over the next few years as usage grows?  Whose ox is being gored?
@_date: 2015-11-16 17:21:28
Perhaps you should reread Satoshi's white paper.  The  longest chain (measured by difficulty) wins.
@_date: 2015-11-17 20:51:27
Here is the relevant definition, from the end of the white paper:
"Nodes can leave and rejoin the network at will, accepting the proof-of-work chain as proof of what happened while they were gone. They vote with their CPU power, expressing their acceptance of valid blocks by working on extending them and rejecting invalid blocks by refusing to work on them. Any needed rules and incentives can be enforced with this consensus mechanism. "
@_date: 2015-11-03 23:02:45
(Not the only similar story)  This looks like the PTB are promoting bitcoin, attempting to pop the price so they can sell at a profit.  Enjoy, but take care!
@_date: 2015-11-19 16:45:00
Thanks for the info.  It will be interesting to see how this is orchestrated and fun to speculate as to who is doing the orchestration.
@_date: 2015-11-15 03:06:51
If the announcement concerns the D-Wave architecture it will be of no concern to bitcoin, because these machines can not run Shor's algorithm and pose no risk to the Elliptic Curve Digital Signature Algorithm used by Bitcoin.
@_date: 2015-11-16 02:21:45
I'd like to amplify your comment regarding Greg missing Peter's point.  The 21 million limit is a major function of the Bitcoin system that is visible to all users, even holders of bitcoin sitting in cold storage. The 1 MB blocksize limit is one of many internal mechanisms used for resource allocation inside the bitcoin network.
Drawing distinctions between functions and mechanisms is a fundamental part of the technical disciplines of software and systems engineering.  I would hope than anyone who fancies himself as guiding the direction of bitcoin would be able to make these distinctions. These distinctions are technical and not political.
@_date: 2015-11-01 19:01:14
There are probably quite a few additional nodes that keep up to date copies of the block chain that are not being counted because they are run privately and have no open port 8333.  (I can personally account for one of these.  It ran for 10 months with an open port, but has been running since August with a closed port after it was DDoS'd twice.)
@_date: 2015-11-24 22:24:05
A payment network is peer to peer if there are no trusted intermediaries.  If Alice uses one or more trusted intermediaries to send money to Bob, she is running client - server software and not peer to peer software. This is true whether there are one or more trusted intermediaries involved and regardless of how multiple trusted intermediaries deal with each other.
@_date: 2015-10-18 18:58:45
The designers of the bitcoin ATM should do some benchmarking of bank card ATMs, including time and motion studies of human behavior.  Then they should use this information to set a time budget for all the steps of the corresponding bitcoin transaction, so that it is significantly faster than the bank card ATM.  (If the bitcoin technology makes this impractical then they should be proactive in getting the bitcoin network to run faster.)
@_date: 2015-10-18 02:56:21
I run a private Electrum server on a dedicated small computer out of my home office. It ran fine on top of  XT, but I had to shut down my XT full node last month  after small block terrorists DDoS'd the XT node, taking down my entire ISP for several hours. Presently my Electrum server is running on top of a Bitcoin core node that has no open port 8333.
@_date: 2015-10-07 17:23:47
I have a friend who runs a local restaurant and uses a stand beside POS terminal.  He would not be able to use the BitPay solution, at least not according to the slides. The slides show the customer's smart phone looking at a QR code on the POS terminal. The problem is that the customer and his smart phone will be in the main dining room, while the POS terminal is in another room where the bar is located.  Also, there needs to be a convenient way to enter a tip from his smart phone and this has to make it back to the merchant so that the server can get the proper payout.
@_date: 2015-11-29 21:06:46
You may be back to the same level of safety, but you've got confused users, either in the form of unexpected behavior (set by defaults) or in the form of a more complicated user interface.
The problem is that RBF was done by a mechanism oriented coder who was not a systems thinker. A system thinker would start with a problem, imagine a user interface, and then work down to suitable mechanisms that could be coded.
The RBF situation is just another example that shows that Bitcoin Core is being run by junior people.
@_date: 2015-10-18 02:34:27
If you install Electrum from source Trezor will work.  I did that this afternoon in less than 30 minutes.  The main difficulty is that you have to install the dependencies (including Python, pip, Python-Qt, Python Trezor, Cython, and some USB software.).  There were a few glitches that had to be surmounted.  Mostly a matter of following your nose to the next error message and then Googling.  You need to be familiar with the Windows command line and have some knowledge of Python.
@_date: 2016-10-23 19:23:15
I think the important quality is the integrity, devotion and skill of the project leadership, not the corporate structure (or lack thereof), funding structure, or copyright status.  Linux seems to be excellent, as does some of its distros.  Other open source projects are having difficulties, e.g. Firefox and Thunderbird.
@_date: 2015-11-09 16:51:17
Related would be to power miners with micro hydropower. Mountain streams could provide 24/7 operation for much of the year, but this power might be uneconomical to connect to the grid.  Even a huge mining farm requires a minimum amount of internet bandwidth, a few kilobytes every 30 seconds is all that is needed when the farm uses a local stratum proxy. This would allow wireless communications of one form or another.
@_date: 2015-10-24 19:36:32
In the example, the 40% node gets control of the chain by partitioning the two 30% nodes.   However, the attack would not work in practice, because with three major players, each would already know about the others and would have configured their clients appropriately (addnode=w.x.y.z). It would not be possible to partition them by manipulating the node discovery protocol.
@_date: 2015-10-18 02:44:44
On days when I am in a bad mood I would say a user who didn't fail to write down or otherwise lost his seed deserved to lose his bitcoins.  He is an idiot who should stay away from computers.  He should be happy he hasn't been run over a bus.  Electrum is pretty much foolproof, but can't protect against Cretans. 
Before I first used Electrum I wrote down my seed.  I then transferred a tiny amount of bitcoins to my new wallet, deleted the wallet file, and restored the wallet from the seed I'd written down.  Then I took the paper with the seed and locked it up in a safe place.  Only then did I load the new wallet with a significant number of bitcoins.
@_date: 2015-10-07 20:25:58
To use bitcoin, one needs only download the blockheaders and run an SPV client.