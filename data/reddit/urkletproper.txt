@_author: urkletproper
@_date: 2017-08-28 23:45:20


What determines the naming?
@_date: 2017-08-27 03:37:22
Isn't ETH the one that had 1.6x more transactions than we did yesterday with an average tx fee of like 5 cents?
@_date: 2017-08-28 23:39:45
From some old post?
@_date: 2017-08-24 20:23:24
Working as intended is pushing people to other coins?  Other coins aren't even allowed for discussion in r\bitcoin.  How could that be the answer?
@_date: 2017-08-24 20:01:10


Lightning is new.  To use something to pay other people, other people have to be using it.  Lightning needs a network effect.  I can't pay anything with litecoin, or Lightning right now, and bitcoin fees are like 25% of the transaction
@_date: 2017-08-29 03:21:39


I mean... its 5 watts.  I don't think he's going to sweat it.
@_date: 2017-08-24 19:57:21
How is driving people out of Bitcoin a good thing?
People don't convert dollars into shekels before spending them on certain things.
@_date: 2017-08-24 19:59:56
Its all spam
@_date: 2017-08-24 19:52:39
Wouldn't that have something to do with the way that discussions are shaped in the subr?
It isn't really possible to have a balanced discussion anywhere right now, and a lot of non-extreme posters have been removed from here recently.
@_date: 2017-09-08 01:28:42
Can you measure it with historical syncing turned off?  Because that was 95% of the problem when I measured it.
@_date: 2017-09-07 20:48:52
Moore's law was about transistors per CPU, not cycles per second.  Its still on track as of last year when I looked
@_date: 2017-09-07 18:16:05


That's only true for transaction sizes, aka quadratic hashing.  If transaction sizes are capped, no quadradic hashing and the problem becomes linear.  2x already capped the quadradic problem to being no worse than it was in previous years.  Not to mention that constructing a very slow block to process requires a miner to give up all fees for that block, which is why it has been so extremely rare in the past.
@_date: 2017-09-07 19:08:18


What about SPV wallets?
And why's that really a problem?  Transactions are comparatively tiny...


I thought that was just mining security?  If not, how does the block reward relate to transaction volume?
@_date: 2017-09-07 17:47:14
According [to one guy with an older CPU]( 8mb only took 4.3 seconds.  So long as transaction size is capped / quadratic hashing is addressed it would be linear, that's 5 minutes on a 2008-era CPU.  So if someone were running a 2008-era processor in whatever year we reach enough volume to fill 555 mb, it seems like they could still keep up with 10-minute blocks.  No?
@_date: 2017-09-07 17:53:34


Well, that depends... How long would it take Bitcoin to grow to a level where we needed to process as more transactions in a day than Bitcoin currently processes in a year?  (FYI not sure where you got the 240k tx/day, that's our current volume)  What % of the world would have adopted Bitcoin by that time?


That might be true, but what level would it be at?  Could small businesses afford it?  How about medium size non-profits?  Early adopters who have retired?
It seems like these would be important questions to consider before we conclude that it would absolutely for sure cost too much.  And even if it does cost too much, a better question is... What cost is appropriate?  Where is the goal?
@_date: 2017-09-07 19:13:50
I'm not sure what my history has to do with it, but you can how it works here(also links to other places; the page on classic has the best explanation I think):
@_date: 2017-09-07 20:59:00


I didn't figure that.
No one needs 128 peers, and if they do have 128 peers, they don't need to send 128 blocks because then that would mean those 128 peers would also be sending the same blocks - over and over again - to you when you already have it.
In a perfectly optimal scenario, everyone needs to send and receive a block one time.  That would be 8 GB a month.  Bitcoin isn't ideal, but it isn't anywhere near the 50-fold numbers you're talking about.  I've measured a factor of about 8-10 sends/receives per block with 30 peers, including overhead - ~50-70GB/month.  The big numbers all came from historical syncing.


Turn off historical syncing and test it again(can do it with pruning or there's probably a flag).  I think you'll be really, really surprised what percentage of the number it is.


As a design, no, it is terrible.
But transactions are as small as 150 bytes.  How much data is in the paypal login page alone, which you have to visit to send a transaction there?  Over 100,000 bytes.  How much data does VISA spend on handshakes, reports, chargebacks, alerts, website, replication, etc just to be usable?  Bitcoin is many orders of magnitude more efficient than those because transactions are actually really, really tiny.
So it's not as terrible as it first seems.  The question is "how far can we scale and what does that cost" not "is it a good design for scalability."


I thought one of those papers recommended 4mb?  From Bitfury?  Segwit2x would be expected to be ~3.6-4.2mb, not counting progress in the year+ since the paper was published?
@_date: 2017-09-07 20:27:00


That didn't happen before?  And I'm not asking under the assumption of zero transaction fees.  Blocks should always be full and fees shouldn't be trivial, but they shouldn't be $5+ either.  There's no cap on how high transaction fees can grow, they could be thousands.
What I'm asking is what X level of blockchain scaling would cost, and roughly when that could feasibly happen?
@_date: 2017-09-07 20:43:02
Amazing math!  We need more people here doing the math.
@_date: 2017-09-07 20:28:48


Nodes have fees?
What's this based on what you're describing about the "mempool"?
@_date: 2017-09-07 20:39:58


Is this something we're in danger of?  How do we measure it, how do we know the danger and evaluate whether the economic protections are large enough?
@_date: 2017-09-07 18:10:42


The point isn't to compete with VISA.  The point is to not restrict transaction volumes arbitrarily.  Think about it.  Why should it cost $30 to run a full node for a year but $120 to transact once a month for a year?  And for the use cases that don't mind 10-minute blocks but can't handle a $10 transaction fee, why should those be pushed away into altcoins when Bitcoin can handle them by increasing node costs to $5 per month instead of $2.5?
Price should follow usecases and price does historically follow transaction volumes.  Why restrict adoption and give why give altcoins free adoption and huge price increases instead of keeping them on Bitcoin?
@_date: 2017-09-07 20:33:31


If that's the case then something is horribly horribly wrong because there's only 4383 blocks in an average month; Even if you sent every block to all 32 peers that's still only 140k, but that doesn't happen - Peers don't request blocks they already have / are already receiving.  With compact blocks many times blocks aren't even sent, only the information to match up blocks with known mempool transactions.
Are you sure you're not talking about historical syncing bandwidth?  Because that's another problem entirely, but it definitely seems solvable.


Surely someone's done the research on this though?
@_date: 2017-09-07 19:06:46


Ok, what blocksize could we have right now with those same requirements?
Not all peers get a full relay, generally less than half, then 16 peers * 1mb blocks = 70 GB/month.  What blocksize can the average Joe support?
@_date: 2017-09-07 20:38:53


We don't have to ditch it.  Ancient data can be relegated to a few full history nodes, services like blockchain.info that get ad revenue, and torrents to ensure it is never lost but not too costly.
Meanwhile, SPV nodes can become just as trustless as they are today by simply adding utxo commitments.  No history required for them.  Definitely still Bitcoin, and those who want to run a full validating node easily could do so.


What does difficulty have to do with the number of miners?  We have had about the same number of pools since 2013, and the number of miners has probably gone down over time consolidating into big farms.
Are you talking about mining security?  Because that's a different issue.  It doesn't relate to blocksize directly.  We definitely need enough security from mining to ensure that a 51% attack isn't viable.  How much security is that, has someone calculated it?
@_date: 2017-09-08 01:33:09


Wouldn't someone be able to calculate what the risk level is and figure out how much protection is needed?
@_date: 2017-09-08 01:30:33
What happens if the other blockchains do everything Bitcoin does and more?  Why would anyone need Bitcoin anymore?
@_date: 2017-09-07 21:03:00


But you just said they always choose profits over the benefit of the network...  Isn't the whole economic design of mining such that they focus on short term profits?  That would be small blocks.
Bitmain seems to support segwit via 2x, and has been trash talking BCH on twitter that I've seen
@_date: 2017-09-07 23:09:39
Effectively its a way for each block to record something un-fakable to state what the current UTXO set looks like.
Currently if a SPV client wants to find out if a given transaction is within a certain block, they ask a full node to prove it to them.  The SPV client knows the merkle root, and the full node simply needs to tell them where in the merkle tree it is, aka, what branches need to be hashed together to get that merkle root.  Since hashes can't be reversed or pre-calculated, the SPV client can know if the full node is lying to them.
Similarly, with a UTXO commitment a SPV client can similarly check an output and be certain - and not lied to - that that output existed and was unspent as of &lt;block containing commitment&gt;.
An even better advantage is that with UTXO commitments, we can mostly forget about historical transactions that were closed out; this is how fast syncing works for several other coins.  That doesn't mean the historical data will be gone, but only a few select nodes will care about it, like blockchain.info, and it can be moved to a torrent for archiving.  Essentially the way it works is you sync block headers only, which is like 25 MB a year of data or less(very small) from Genesis until you reach the current height.  You now know, for sure, that the chain you're on has X total PoW behind it; no one can lie to you.  You pick a historical time, say 4 months in the past, and you request from other nodes "send me the UTXO matching &lt;known UTXO commitment&gt;".  They can't lie to you; You already have the hash and validated it.  So they send it, and now you've sync'd a 140 GB blockchain for only ~3GB of data, plus 4 months or so of transactions (Which can generally be lower like a week or two).  The only reason for the 4 months of transactions is if there WERE a fork that you wouldn't want your fully-trustless-warp-sync node to follow, the creator of the software would have 4 months to update the software to reject that fork *even if it was the most PoW.*  This is very nearly the same thing that happens today because Core doesn't validate ancient signatures by default, it simply downloads them.  That's a tremendous waste of bandwidth that UTXO commitments could get rid of trustlessly.
@_date: 2017-09-07 21:00:33


I've done this many times and run several fullnodes.  It isn't a problem.
The problem comes from the historical syncing.  Which UTXO commitments can solve trustlessly
@_date: 2017-09-08 00:18:01


The only thing I didn't mention is that it is difficult to do UTXO commitments well.  A few people have tried to do them and eventually either gave up, or people couldn't agree on the best way to organize the data.
I'm somewhat fearful that Core's desire for perfection in coding systems will prevent them from ever reaching "good enough."  The principal problem is that every time you receive a new block (for some proposals; others only do this once per difficulty change, which is useful but not **as** useful) you have to re-hash the merkle structure of the UTXO structure.  If designed poorly, it would be a lot of hashes and that would be a negative.  Designing it to minimize the total number of additional hashes is hard.
@_date: 2017-09-07 18:19:41


How do you figure that?  Nearly all full nodes run by poolservers are on a gigabit connection, and can upgrade easily if they are not.  That's less than 5 seconds of transfer time?
@_date: 2017-09-07 20:24:03


Right.  So how much does it cost in bandwidth per month for 1mb blocks?  10mb blocks would simply be 10x that, right?


Verify its hash and signature?  Not sure what you're saying here.
@_date: 2017-09-07 19:03:47


But that happened for several periods before segwit was even activated?  That's why I asked the other questions
@_date: 2017-09-08 01:18:28


So basically just some fees minimum?  If the minimum fee was $0.001 would that be enough?  If not, what about $100.00?
@_date: 2017-09-07 19:16:21


You said they have to be processed by all nodes on the network, SPV doesn't do that.


The historical data isn't needed by average people.  All they need is the UTXO state, which is less than 3gb currently, even with the unfixed utxo bloat.  SPV users and light clients don't even need that.


Right, but fewer miners being able to run wouldn't affect transaction speeds or throughput.  The blocktimes adjust.
@_date: 2017-09-07 18:20:48


Hard drive prices historically have dropped pretty fast though... How many years would it take us to reach a point where we could fill 555 mb blocks for 13 days straight?
@_date: 2017-09-07 18:38:28


The mempool was a pretty noisy graph for the last year, but even so, the average and median fee per day haven't declined... And I haven't checked, has the daily average blocksize actually increased above 1mb?  Or the median?
@_date: 2017-09-07 20:43:59
Are small blocks more profitable for miners due to fees?
@_date: 2017-09-07 20:49:50
What if you dump it and it becomes Bitcoin?
@_date: 2017-09-07 19:19:05
But the average and median appear to have barely moved at all, and the mempool isn't empty?
@_date: 2017-09-07 22:49:32


Uh, what?
There are no clients that do this.  And nearly everyone is running core or a fork of core, none of which do that either.  How do you imagine that most nodes will do anything remotely like that?


There shouldn't be many of those or else Bitcoin is killing off usecases.  But yes, I am counting those in my measurements, they were raw bandwidth mesaurements.


All that matters is the cost of running your node.  The other nodes pay their own costs; decentralized.  Ergo, counting every single movement isn't useful for anything.


Eh, yeah, but 8x redundancy isn't quite "disgusting" especially in a completely decentralized network.  To me anyway.  Not quite "amazing" but definitely "good"


This attack is already economically discouraged, and even if a few blocks happen, a few of them won't have any negative impacts.  To do hundreds of those blocks, a miner would have to forgo hundreds of thousands of dollars of profit, and still have essentially no impact.
@_date: 2017-09-07 17:28:23
How did you calculate that?
And how much would that cost per month to run a node @ 555 MB blocks?
@_date: 2017-09-07 20:45:15
What does wanting low fees have to do with VISA?  VISA's just a transactions/sec number for comparison here
@_date: 2017-09-08 00:19:11


It doesn't make sense to me to try to protect against an attack that we can't actually explain.  For nearly everything else Bitcoin does there's a game theory and a payoff table... Or at least a mathematical explanation of the risks.  Can't we do that here?
@_date: 2017-09-07 22:55:08


Miners process block headers only.  Bigger blocks don't affect them.  For a poolserver, buying a top-end internet connection/server is a no-brainer because its cost($10k-ish at most) is trivial compared to their revenue($10-million-ish).  In what way can a larger block have any effects on other miners?


If miners start making things so big that regular users can't use the coin, the price would plummet.  The system is designed to economically punish them for bad behavior.
@_date: 2017-09-08 00:31:03


No, they do serve blocks, they act as a nearly fully-functional peer.  The only difference is that they are called "nonlistening" nodes; they can only connect to others, others can't connect to them first.  They still help the network and relay blocks, but new peers to the network can't connect to them.
Pruned nodes also contribute by relaying, they just don't provide historical data for download.


How high do transaction fees need to be for the system to be sustainable?  Surely someone has calculated this?


That's not what I was saying, that works out to be about how much more bandwidth is consumed than theoretically should be.


Last I saw, Bitmain was trash talking BCH?  And most miners only seem to be mining it when it is profitable now...
@_date: 2017-07-30 23:24:18


Who is making those claims as relates to a billion users?


~75% of the article is about disk reads using a horribly inefficient processing algorithm.  Why don't you just go back edit the article to state that the algorithm is inefficient to the extreme and that all of the math that follows from it is based on us reaching a billion users without anyone actually fixing a broken system, despite fixes being readily available and highly proven?
The point of the article, and its interpretation here, are that scaling to 365 billion transactions a year isn't possible without going to the second layer.  Despite this, the second layer you are touting as the solution is nonexistent and unproven.
How do you justify rejecting obvious proven engineering techniques like batching, caching, and indexing that have been around since the 80's in favor of unproven second layer technologies that are already threatening to drive many users away from Bitcoin **today?**
@_date: 2017-07-30 22:17:20


Apparently they don't have a clue.
The article assumes that each fullnode will load and iterate the entire previous day of blocks for every single spv client request, and all of the rest of the article was based on that one assumption.
No programmer that works on large-scale systems would ever build such a thing for large scale.  The issue is trivially solvable with batching and indexing, and caching layers.  The fact that several core developers even reviewed the article before publication and didn't point this out is embarrassing.
@_date: 2017-07-30 22:35:56


Remember Core's motto... If we can't get it perfect, we can't get it at all.  No way we can possibly estimate growth trends to a billion users...
@_date: 2017-07-31 16:14:15




And this is **factually wrong**.  Simple payment verification is STILL validation.  Light clients still do validation.  Your "observed fact" is flat out wrong.
Just because something isn't at the level of verification that **YOU** want does not mean it is not good enough for those people.


And I want them to be able to **actually use Bitcoin** without being forced to Lightning or sidechains by high fees.  Lightning and sidechains need to prove to the ecosystem that they are a good alternative BEFORE the ecosystem bets its future on them.  Not doing so will just drive people to altcoins if they don't work out great.  Especially since they are different from Bitcoin with different requirements, trade-offs and risk factors.


As covered above, SPV is still validating, and light clients still validate.  But even more than that, anyone can choose this at any point no matter what size blocks get.  Just because they have to pay a higher cost does not mean it isn't a choice, in fact, it puts the trade-offs for their own choices right in front of them.


Also flat out wrong.
The system is not "insecure" unless there is an **actual attack vector** that can be described, and we can then work out the game theory payoff table and protect against that.
The system has many different things that many different users value in it.  Being able to validate is not an important feature to many people.  Let them decide which features of Bitcoin are valuable and which are not, and stop telling them it must be the features you say are important.


If you refuse to let Bitcoin scale, you are telling people they can't use it.


No one is saying you can't validate your money.


"I wasn't telling them what to do" ... "I won't let it scale" ... "loud children" ... "Still not telling people what to do."
Do you actually read the words you write?
@_date: 2017-07-31 17:26:45


The FBI didn't seize the servers until July 25th.
@_date: 2017-07-30 23:45:26


I only see this in one place... Slide 34: Existing technology.
That is correct.  The technology for improving the inefficient algorithms(that you based your math upon) exists and has been proven.  He didn't say Bitcoin was ready to do it without any code adjustments from today's codebase.


That's not what the article said... The article said we can't scale to a billion users because &lt;500gb repeated reads&gt; and the solution is lightning, sidechains, and drivechains.
And "current state of the technology" is vague.  We know how to improve those algorithms, we've known how to do it for decades.


Premature optimization?  In the real world you don't block your company from growing until after you have the solution for every conceivable problem such growth might create.  Companies that try to do that fail.  Companies that solve today's problems today and next year's problems next year survive.
@_date: 2017-07-31 06:09:29


Nah.  The SPV nodes can simply add in a group of addresses that aren't theirs to each request, and shard their real addresses across different fullnodes.  The fullnodes will have no way to get a real picture of who owns what.


Also not true.  Bitcoin is remarkably efficient despite its flaws.  A few hundred dollars a month lets Bitcoin scale to an unbelievably high level, and as Bitcoin scales... Bitcoin prices rise.  Fullnodes can be run at future very large scales for less than 0.005 BTC per month.
@_date: 2017-07-30 22:14:53
This article is incredibly foolish.  Any engineer who has worked on any large scale project ever can see the problems with this immediately.


He's assuming that fullnode software would continue to scan every block in the requested time period for every single request.  No sane engineer or company would try design and support such a wasteful system at large scale.
This can be solved trivially with caching, batching, and indexing.  A engineers who have designed or worked with large scale systems would be able to point this out immediately.  The fact that several core developers reviewed the article and failed to point this out is really embarrassing.
@_date: 2017-07-31 00:36:58


AFAIK few people are using this at all today.
Moreover, there's no reason that SPV clients can't be changed to include a list of outputs they want to know about in their requests.  Its relatively few bytes for them and not enough to represent a problem for the full nodes even cumulatively.  They can get some level of privacy by randomly including other known UTXO's that aren't theirs so that the full nodes can't know for sure which UTXO's belong to whom, and by sharding which fullnodes they request which portion of addresses.
@_date: 2017-07-30 22:40:00


Don't forget, this article was reviewed by 3 major core developers.
@_date: 2017-07-31 06:00:17


Sure they are.  They just aren't validating it the way you want.  Let them validate how they want to validate, and stop telling them that they have to do it your way.


Nah.  Just add in a few addresses that aren't actually yours to each request and shard your addresses across a few different full nodes so that they never get a complete or reliable view of your addresses.  Problem solved.


No it isn't.  Thousands of different companies can run fullnodes and SPV clients can validate across different sources to their hearts content.  When 1 billion people use Bitcoin, the entire world isn't going to conspire against some little SPV wallets.


No, stop telling other people what they have to do.  Other people can choose what level of security is appropriate for them.  What you're telling them is if they can't do security the way you want, they won't get to use bitcoin at all because you won't let it scale for them to fit.  They aren't going to let you control them, they're just going to go use another coin and Bitcoin gets left in the dust.
@_date: 2017-07-31 16:15:50


Given when it was written and what Bitcoin was looking like at that time, I'd guess that his thinking was entirely based around cell phones with SPV wallets being the minority and node operational costs being a relative nonissue.  He probably wasn't trying to write it for 15 years in the future with the expectation that it would never be changed...
@_date: 2017-07-31 00:38:51


As our scaling causes fullnodes to develop new needs, they can require more information from SPV nodes in order to serve them, including easier to serve data requests.  This is naturally how ecosystems evolve as they encounter problems, we don't need to centrally plan for every conceivable problem that might happen in the next 15 years or else we'll never get anything done.  Premature optimization at its worst.
@_date: 2017-07-30 22:54:09


You can index on output addresses.  If nothing else, you can index only the output addresses modified in the last 24 hours to cut down on the size of the index and immediately reply "no change" or fetch the exact block/txindex required from the merkle tree of that block only, very few bytes need to be read.
Even if that wasn't possible, this problem can be massively, massively reduced by batching the client's requests together and then loading the block data in batches for the entire outstanding batch.  The fact that 3 core developers reviewed the article and didn't point out this obvious oversight is a bit of an embarrassment.  is correct, this analysis is ridiculous.
@_date: 2017-07-30 22:34:16


So why make a misleading article that implies we cannot do something when in fact the assumptions that went into the math were fundamentally wrong?
No one would ever set up a system to reprocess 500gb worth of data for every single request and expect it to scale to a billion users.  But your entire article was on why that can't be done because of the 500gb continuous reprocessing.  Why mislead nontechnical people into thinking it is insurmountable?