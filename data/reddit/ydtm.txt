@_author: ydtm
@_date: 2015-12-27 15:27:39
That's an important reminder.
Does anyone have historical data on this - showing the "relative" jumps in the past (I presume this means interpreting the jump as a percentage of additional hashpower rather than as an "absolute" amount of additional hashpower).
I heard that the difficulty jumped by about 18% a few days ago though?
Has a much higher percentage addition of hashpower been brought online since then?
@_date: 2015-12-07 21:15:14
This is the first time where I've seen a concrete similarity between the main p2p mechanism of BitTorrent (splitting a file into small "pieces") and a mechanism in Bitcoin (splitting transactions and signatures among nodes).
Intuitively, this seems to be the "right" approach to parallelism.
Today is the first day in about a year where I've felt optimistic again about the possibilities of Bitcoin scaling.
Also the first day in about a year where I'm "on the same page" as theymos and a Blockstream / Core dev.
Allow me to remark here that I have pretty much suspected all along that most people on both sides of the "great blocksize debate" have been arguing in good faith - ie, they really believe that *their* approach is the "best" for Bitcoin. 
It's just that most of the solutions seemed to involve different trade-offs along different dimensions - and this encouraged people to split up into different "camps", based on personalities and philosophies.
Math-based solutions like Pieter Wuille's Segregated Witness &amp; Fraud Proofs seem to be much more in the original spirit of Bitcoin itself: no trust required, if you understand the math itself, then you're good to go.
@_date: 2015-12-27 18:08:23
@_date: 2015-12-11 10:23:52
Hey Manhattan DA:
~~Bite me.~~The eagle flies at midnight.
@_date: 2015-12-11 13:42:41
Suggestion: only hide they-mos, and give us back our subreddit, and let us comment and vote just like any other subreddit. It works pretty good for the rest of reddit and it used to work pretty good for this subreddit too until some mod decided to override the existing rules.
The only reason they-mos is hiding votes now is because voting was the last way most people could actually *express* themselves on this subreddit (and because so many of his posts had hundreds of downvotes).
The reason reddit is so popular is because anyone can comment and vote. Take that away, and it becomes just another crappy blog. The whole point of reddit is to *have discussions*.
Stop twisting yourself into contortions worrying about whether an upgrade is too "controversial" to discuss, or worrying about whether a comment has to be flagged as a "discussion". Cripes, *most* comments are "discussion" - are we going to become such delicate flowers now that they have to be flagged as such?
Most people aren't stupid and can be trusted to engage in a forum with a *moderate* amount of moderation - which, remember, is mainly supposed to be to weed out spam, *not* to impose some moderator's personal opinions.
@_date: 2015-12-18 04:44:46
I thought I was asserting they are "xor" - not too sure what you meant by them being somehow "linked".
@_date: 2015-12-08 00:57:05
Read his post.
This is math, you can't just take the usual lazy reddit approach and read just the headline and recall that you "don't like that guy" because of some random thing he said a few months ago.
The link in OP is a well-written summary chock-full of major new scaling developments on a wide range of fronts. It's big news and I'm glad the work was done and I'm glad Greg took the time to summarize it (and theymos took the time to post the link to that summary).
I have been a vocal critic of Gregory Maxwell (and theymos) here in the past - but when these guys do something right, we also have to give them credit.
I'm still perhaps personally a little leery of the LN stuff (which goes under a different label in that post - something like "off-blockchain scaling", who knows, maybe Greg had a Public Relations guy guiding his word choices =) - but the stuff about Pieter Wuille's Segregated Witness and Fraud Proofs is great, plus the stuff on versionbits (evidently based on some ideas from Luke-Jr) also sounds like it could be very helpful as well.
Who knows, maybe most of these devs and early players really sincerely do want Bitcoin to succeed, but sometimes they just aren't so great at communicating, or complicated but wonderful scaling solutions (like SegWit) are "in the works" but not yet ready for release, so we flock to simpler, easy-to-explain stuff like BIP 101 and XT (which isn't the same kind of "scaling": SegWit actually crams more data into less resources, while BIP 101 and XT simply gobble up more resources). 
I supported BIP 101 and XT in the past *when there were no other options on the table*. Today, all that is changed. I still think BIP 101 and XT are "good" - but based on what I've learned today, I have already immediately revised my opinion that they are "not as good as Segregated Witness" (assuming all are actually available for roll-out - BIP and XT are already, and evidently Segregated Witness will be soon also).
Today, I'm feeling pretty positive towards *all* the devs. I've done some dev work myself, and when I do, I end up totally failing on my duties to communicate with people "in the real world" - including sometimes my users / clients (as well as friends and family). Who knows, maybe that's been the case here. Maybe Pieter Wuille has been so busy working on SegWit that he hasn't had much time to explain it to us. If so, that's fine, as long as he eventually manages to both get it done and explain it to us before it's "too late" (ie, before we've gone off and adopted some other solution that might not offer such real "scaling").
It's fine that we are "demanding" of our devs, and fine that there is debate among them (eg, perhaps BIP 101 and XT "lit a fire" under their butts, to make them release SegWit now). 
Hopefully the community of devs and users will someday become more united, the way it was back in the early years.
@_date: 2015-12-18 02:46:19
Playing it safe would be avoiding the network becoming so congested that people start abandoning Bitcoin - which they will do at the drop of a hat if "real users'" transactions can't get through.
Doing nothing when your system is about to hit a capacity bottleneck and scare away "real users" is the most radical and dangerous and non-conservative approach you can take.
All the decentralized nodes in the world ain't gonna mean much if they've been artificially throttled down to the point where people can no longer transact on the blockchain.
Plus you telling me where I should and shouldn't post is really unnecessary. If you're right, you should be able to demolish my arguments, without telling me where I should and shouldn't post them.
I appreciate that you want the best for Bitcoin - and I hope you appreciate that I do too. We just have different definitions of which dimension is most important to decentralize - which is fine, and which is why it would be best if we were able to openly debate this somewhere (say on this forum), in the hopes that one of us might eventually be able to convince the other.
@_date: 2015-12-08 02:01:49
I guess they were probably in-the-works but devs were too busy to clearly communicate to us about them, and re-assure is that they would work.
Also maybe they didn't think they'd get rolled out (because a hard fork would be needed) - until this new "versionbits" thing came along (which seems like it could allow more stuff to be rolled out smoothly via soft forks).
Not an ideal situation - but I'm tired of attributing things to malice on the part of the devs, when it could more easily be explained by lack of time / energy.
I'm hoping that communication will improve in the future.
But if I had to choose, I prefer devs who are great at programming and maybe not-so-great at communicating, versus the opposite (devs who are great at communicating and not-so-great at programming).
Someday hopefully we can have both - but if we have to pick only one, I think we all agree that the priority for devs should be programming over communicating.
I've been a vocal critic of many of the devs in the past - but not today. This is great stuff they're releasing, and we should give them credit where credit is due.
This is also a fascinating learning experience today, where we can see who is clinging to old animosities towards devs, versus who is swayed by the new stuff being released.
As I said - I have been highly critical of Core / Blockstream devs in the past - due to their prioritization of stuff like RBF, and due to their view of the blockchain as a settlement layer for stuff like LN.
But I saw the video and read the transcript of Pieter Wuille's presentation on Segregated Witness and Fraud Proofs, and I was instantly convinced. I turned on a dime, and I'm not ashamed to look like a fickle politician blowing in the wind today. New information became available, and I changed my tune accordingly.
I don't know, I guess it was just on a level where I "got" most of it. (I'm not an expert on Bitcoin technology but I try to keep up reading white papers, BIPs, debates etc., and I have a background in math and programming and basic notions about p2p, which is why I was impressed by Bitcoin in the first place, years ago.)
I was also a big doubter of even the need for the HK conference - I had all sorts of conspiracy theories that it was just some sort of delaying tactic to make people focus more on personalities and less on mathematics. Boy was I ever wrong. The conference provided a major opportunity to communicate important new features and proposals to users (and among devs), so I think it ended up being very positive. 
@_date: 2015-12-08 17:23:08
I think it could actually be argued that SegWit is (along some dimensions) perhaps even more KISS than stuff like BIP 101.
This probably involves differences of philosophy and perspective.
I think even us bigblock proponents (I used to be one - now I'm pretty neutral if we get SegWit) should admit that we did have *some* worries how the game theory and geo-politics would play out under BIP 101 - since the risk factors do after all include stuff like bandwidth limitations from ISPs, the Great Firewall of China, and rates of orphans among different types of miners (eg bigger versus smaller). 
In other words, when it was BIP 101 versus nothing, of course I went with BIP. Now that it's BIP and/or SegWit, I'm leaning towards SegWit ASAP, and the BIP 101 if / when needed (and actually XT's activation schedule doesn't have to change - but I just realized that what we now need is a version of XT that also includes SegWit!)
On the other hand, with SegWit, we at least would be getting pretty much an approach based on simply squeezing more performance into existing resources (instead of increasing our resource demands, so those risk factors (which all involve real-world resources) probably wouldn't come into play.
In this sense (and this maybe shows my philosophy, as an occasional programmer myself), I'd prefer to modify the software more in order to be able to modify resource demands less (or even not at all). So from this perspective, I see SegWit as being more KISS than BIP 101.
TL;DR: I see SegWit as more KISS than BIP 101, since BIP 101 involves increased resource demands which could interact with game theory and geopolitics, where as SegWit seems like it squeezes more performance out of the same resources. Yes, SegWit is more "code-based"- but it's code-based in the "right" way (an elegant refactoring of the merkle tree into a logical half and a numerical-textual half), so again it seems KISS to me in the programming sense (and actually SegWit is fewer LoC - lines of code - than BIP 101, according to Maxwell).
@_date: 2015-12-19 17:33:05
So... This means supports stuff like Hearn's XT?
@_date: 2015-12-07 23:49:35
After reading the transcript and watching the video of Pieter's presentation on Segregated Witness, I posted a "rave review" of Segregated Witness over on Some people may recall that I'm usually in the *big-blocks* camp, and have been critical of Blockstream / Core in the past for prioritizing RBF, and for viewing the blockchain as an (expensive) settlement layer with LN.
Today is the first day that I find myself supporting a major new feature from Blockstream / Core - and the first time after the past year of reading depressing block size debate postings that I actually once again feel optimistic about Bitcoin's future.
Once we get some real scaling solutions, I'm sure many more people will also be relieved and become much more positive again.
@_date: 2015-12-11 10:34:20
BitTorrent can either be regulated as a media files or as software files. What are the merits or demerits for either route?
@_date: 2015-12-08 02:10:12
Stop being so suspicious!
The simplest explanation is: he didn't support big blocks because he knew / hoped that something like SegWit would come along "in time" which crams more data into the same space, so big blocks wouldn't be so important so soon.
I was a major supporter of big blocks, because up until today, that seemed the best "scaling" solution which made sense to me compared to the others (the concept was ok, and an implementation was available).
Now we have SegWit - and I now prefer SegWit over big blocks, because it seems like the best "scaling" solution which made sense to me compared to the others (the concept is *great,* and an implementation should be available soon).
(Of course, some day we could need and use both SegWit + big blocks - they're totally different and independent and composable approaches to scaling.)
Stuff changes. We have to keep up.
My user name ydtm means YouDoTheMath. Study up on SegWit and see if you agree with the math. I do. We don't have to trust anyone if the math is right, remember?
@_date: 2015-12-27 19:50:27
I don't personally use Coinbase but I think it's wrong to censor Coinbase from bitcoin.org, preventing other people from finding out about them.
In general, we should be open and transparent (non-censored). People are smart enough to decide for themselves.
In particular, Bitcoin is "permissionless" and consensus is determined by CPU power instead of by centralized mechanisms such as censorship.
This includes letting people and companies choose which client software they prefer to run.
Without this freedom and diversity, Bitcoin would become fragile.
So it is important to:
- avoid censoring client software from sites which claim to be "about Bitcoin" in general
- avoid censoring people and companies based on which client software they intend to install
This should all be obvious.
@_date: 2015-12-27 18:59:27
Yes someone might get their fweelngs hurt - or might actually learn something about General Cryptocurrency Theory (very hard to do on a censored sub).
@_date: 2015-12-18 01:50:05
Real users who have real money on the line with Bitcoin don't have time to be nice to devs like Maxwell who are incapable of listening to user needs &amp; requirements.
This is simply about money, not about people's feelings. If a dev like Maxell doesn't understand how to scale Bitcoin, you work with a dev who does.
@_date: 2015-12-11 02:02:57


It's so weird - whenever you read a mainstream media article about something you already know a lot about, you catch all kinds of mistakes like this from the reporter.


That closing line from the WP article shows what reporters' priorities usually are: writing beautiful poetry, not informing us.
@_date: 2015-12-11 02:05:46
I'm so tired of reporters having only a layperson's understanding of the material they're covering.
There's hundreds of people out there who understand PGP (and Bitcoin) better than this reporter.
What is the point of reading WaPo if the reporter knows less than us?
@_date: 2015-12-18 01:47:58
Hey, he's been totally out of touch with users' needs &amp; requirements.
He really could cripple Bitcoin due to his gross lack of understanding of issues involving scaling and economics.
Read the message from J Garzik (and the reactions from investors) - this shows how Maxwell's inaction on scaling would hurt Bitcoin:


Of course, I doubt that Maxwell is really "gone" in any meaningful sense. He's probably still CTO at Blockstream - so he could have a lot of influence, telling other devs what to work on (and remember, they get paid by this corporation to follow his orders).
Then again, I think it's an illusion on our part to think that we have to debate or beg the Core / Blockstream devs for *anything*. They have no official status, and we are free to ignore them. More on this argument here:
@_date: 2015-12-07 23:39:56
Live Free Or Die.
@_date: 2015-12-18 01:57:24
I've been quite open-minded and neutral about the possibility of corruption on their part.
Basically I think that *they* think they're doing the right thing.
But I also fear that they have been turned into "useful idiots" by whoever is backing Blockstream and paying them.
Probably not all that hard for a few fiat billiionaire businessmen to socially engineer guys like Maxwell and Back - it happens all the time.
In other words, I'm almost 100% sure that Maxwell and Back *mean well* and their heart is in the right place.
But I'm also pretty sure they're being manipulated by other people - and they don't even know it.
They're both very principled guys and they would never ever consciously do something to harm Bitcoin.
They're simply not conscious of how harmful their influence is.
This also goes for Peter Todd as well. 
All great coders - but very easy prey for covert manipulation.
Look at how pissed taxi drivers are over Uber, and look at all the tactics employed by MPAA and RIAA (also enslisting the FBI) against things like Bittorrent and Kim Dotcom.
Do you really think the people who benefit from debt-backed fiat issued by private central banks would just stand by and let Bitcoin succeed?
Banning it outright would be stupid, so they'd never try that.
All they needed to do is find a "wedge issue" - preferably involving tradeoffs (and the blocksize is perfect for this) - and then use their usuall tactic of sprinkling their fiat among the peasants, destabilizing the debate, and sowing FUD.
There is only one side in this debate that's engaged in underhanded tactics like censorship and DDoS'ing: the people on the small-blocks side.
That shows that they're the weak and desperate side, and they do not believe in p2p.
@_date: 2015-12-27 19:44:59
(1) What kind of *webserver* is bitcoin.com running?
I've never used Apache, only nginx - really fast.
(2) Is bitcoin.com running some kind of *web framework*? That could affect speed too.
The TechEmpower framework benchmarks are good:
(3) What kind of *hosting* is bitcoin.com using?
Can all of the above handle the day when there's major media coverage and millions of users go to bitcoin.com?
@_date: 2015-12-11 01:52:23
I get the impression that Satoshi wants to do good.
I would imagine he would do something big and helpful for humanity with his coins - like try to fund ways to solve our most urgent problems, such as global warming.
@_date: 2015-12-07 21:05:09
As some of you know, I've been a rather outspoken opponent of many things from Blockstream / Core devs, such as their prioritizing of RBF, and their vision of Bitcoin as a settlement layer with LN.
However, I saw the video and read the transcript from Pieter Wuille today and instantly understood that Segregated Witness (with Fraud Proofs) is a very good thing because it is based on such a solid mathematical foundation that it actually is able to provide benefits across a broad range of aspects of bitcoin: 
- reduced storage usage requirements (blocksize), 
- reduced memory usage requirements (messages), 
- and even reduced network assumptions required (only need weaker "no censorship" assumption instead of stronger "no sybils" assumption).
At the above link I conjecture that this broad range of benefits may be due to Segregated Witness &amp; Fraud Proofs being in some sense a very deep but very natural (and inevitable) "refactoring" of Bitcoin's data structures: simply putting the (logical) validation in *one* top-level branch of the Merkle tree, and the (artithmetical) amount and (textual) addresses in the *other* top-level branch: a very neat and natural split along *types* of data - logical versus numerical, string - and along *uses* of data - validation versus amounts and addresses.
In turn, this quite naturally yields "refactorings" of Bitcoin along a surprisingly broad range of dimensions: 
(1) pruning validation-data from amount- and address-data in the blockchain (ie less storage needed)
(2) reducing network traffic by sending small messages for nodes that prefer this (ie less memory and bandwidth needed)
(3) reducing network security requirements and perhaps also permitting more distributed sharing of validation information
As far as I understand, (3) becomes possible due to the style of "refutational theorem proving" which I believe is similar to what is being used for fraud proofs, which seems to "invert" the type of messages which nodes need to listen to in order to perform certain aspects of validation, which in turn seems to permit relaxing the minimum required network assumptions (from the stronger "no sybils" to the weaker "no censorship").
All in all, I see Segregated Witness as a major win-win-win for Bitcoin. At a "meta" level, this is exactly the kind of direction I feel is most positive for doing upgrades and enhancements. I'm not an expert in Bitcoin technology but I have studied a bit of crypto, logic, theorem proving, etc. as well as lots of languages, and this whole concept and implementation of Segregated Witness &amp; Fraud Proofs from Pieter Wuille really sounds good to me.
Everyone (including myself) should also be careful of giving in to fear or suspicion, which has happened a lot in the past few months of the scaling debates with respect to block size.
For the past few months, I have been a very big supporter of bigger blocks (eg, BIP 101) and I expect that that type of upgrade is one of several we will take (eg, simply making more demands on the hardware, infrastructure - particularly bandwidth, storage, processing power).
At the same time, as of today, **I am also a very big supporter of Segregated Witness &amp; Fraud Proofs** - another type of upgrade which uses clever mathematics to reduce our demands on that hardware and infrastructure.
I have no idea why this sort of thing hasn't been more clearly presented before. I suspect many devs felt like it was a non-starter, simply because they thought it would require a hard fork. 
(This gives me a feeling that potentially other enhancements are also stagnating because devs might not see how they could be rolled out. Meanwhile there are cogent arguments being made that even when something *could* be rolled out as a soft fork, it can in many ways be better to roll it out as a hard fork (this is touched upon in the above reddit thread - I think Hearn posted an argument about hard forks being preferable over soft forks a few months ago on medium.com which made a lot of sense).
I also suspect that maybe some of the Core / Blockstream devs aren't such great communicators, or don't always have time to provide progress reports on this sort of thing to the public. Over the past few months I did read a few things on Segregated Witness on Github (in response to some posts I had made in favor of IBLT and Thin Blocks). So I think there often simply issues about communication where users don't know what devs are up to (and why) - and also issues about organization and basic clean-up / housekeeping (for example, there are probably lots of BIPs already implemented as soft forks which should be officially "rolled in" - but devs simply haven't had the time / incentives to get around to this basic clean-up and organizational work).
For the past year, I've been obsessively following pretty much everything posted on reddit (and sometimes other forums) regarding scaling, blocksize, etc. - and I must say that after seeing this presentation by Pieter Wuille on Segregated Witness &amp; Fraud Proofs, this is the first day in a year where **I again feel optimistic about Bitcoin and would recommend it to friends and colleagues** (provided this actually gets implemented - and for once, I'm *glad* this is coming from a Core / Blockstream guy, because it seems that that's the way to make sure it will get in =).
@_date: 2015-12-27 19:57:52
use a multi-reddit:
@_date: 2015-12-08 17:12:45
I bet SegWit gets made available pretty quick.
From what I understand, it's already pretty much written, and has been tested a bit.
And Maxwell says it's actually *fewer* LoC (lines of code) than BIP 101?
Plus SegWit is just such a fundamental improvement cutting across so many dimensions of Bitcoin. Click on some of my post from the past few days where I'm raving about it from a mathematical point of view.
SegWit didn't come from MaxwellToddBeck nor from GavinHearn - it comes from Pieter Wuille, and the more I think about it, the more it seems to be taking a fundamentally different and better approach than other stuff we've seen.
I think our evaluation of SegWit should be totally separated from our evaluation of BIP 101 - and our memories of the past year. 
I think SegWit will help *somewhat* with block size issues (but I still do need more details - I'm not sure it really reduces bandwidth requirements for the "most recent" blocks at all). 
Aside from that, SegWit helps with lots of other stuff - in a really elegant level-1 way. So regardless of where we go on blocksize stuff, we should add SegWit soon.
If it does turn out to help *somewhat* with block size issues, then our default situation would be that BIP 101 is still in the wings (it never went away), and it might not be needed so *soon*, but could still get activated whenever.
I don't mind if Blockstream / Core / smallblock proponents have "bought some time" with SegWit. SegWit (to me) seems "that good", that they deserve to be able to do so. It doesn't strike me as some desperate kludgy stop-gap - it's something that simply should have been there since day 1 (if Satoshi had modularized his data structures better), and so as far as I'm concerned, in it goes.
I guess the main thing I want to make sure of now is: **SegWith should get added to XT.**
Hearn has been noticeably silent the past few days. If he's as smart of a dev (and a person) as I think he is, then he sees what I see: SegWit is "that good" that every Bitcoin implementation from Core to XT (and BitcoinJ!) needs to add it. 
This is probably a lot for him to take in right now - ranging from perhaps a bit of bruised ego to see that someone actually managed to squeeze more performance out of existing infrastructure (and even possibly do it as a soft fork if versionbits actually works) - versus XT which was just a parameter bump implemented as a hard fork. 
He's smart at both programming and "group psychology" so I bet there's a lot on his mind now! =) 
In the end, I think he's going to realize that he's going to have to support SegWit and he probably already knew this as soon as he saw Pieter Wuille's presentation - who knows, maybe Hearn is already busy trying to figure out how to put it not only into XT but also into BitcoinJ!
@_date: 2015-12-19 14:11:27
I like the terminology: "independent currencies".
Nice and simple and positive-sounding.
@_date: 2015-12-27 19:34:59
Cool - I'm an eclectic kinda guy, I'll update my bookmarks!
@_date: 2015-12-18 13:09:07
I'm pretty sure that if Blockstream continues to be the *only* source of Bitcoin software, Bitcoin will not achieve much adoption.
This is an impact which Blockstream has on Bitcoin - but which would be easy to obviate, simpy by using software from other devs.
This is the correlation I was implying between Blockstream and Bitcoin: they are in conflict. Of course the mathematical "xor" is only a metaphor for that - more precisely, in human terms, we could just say that Blockstream is harmful for Bitcoin, which is why we need to route around them (by installing software from other devs not affiliated with Blockstream).
@_date: 2015-12-27 18:57:49
I recommend using a multi-reddit:
This way, you can see pretty much everything on a single front page.
@_date: 2015-12-08 01:45:30
Yeah I totally hear you on all this.
I've done some dev work in the past (just simple databases and web sites - nothing cutting-edge) and whenever I do, I get so snowed under by the work itself that I no longer have much time or energy to communicate with users (plus family and friends). There's just so much stuff involved whenever you do any kind of programming.
I have been critical of some devs in the past, but I realize that most of them are probably also snowed under by the work itself with very little time and energy left to communicate the how and the why.
I also appreciate any efforts that can be made to stay up to date communicating as much of this stuff as possible with the users - just so we can continue to understand and support what you do.
@_date: 2015-12-27 19:57:25
I was confused for a long time too - because there *are* real tradeoffs involved with nearly all of the various scaling / blocksize proposals.
But eventually, once you notice that only *one group* is in favor of censoring, artificial scarcity, even some people evidently DDoS'ing (against XT nodes) - it's pretty easy to come to the conclusion that Core / Blockstream / theymos are corrupt and weak and desperate.
Because if they actually *knew* they were right, they wouldn't have to cheat.
So while this whole thing shakes out, I recommend using a multi-reddit:
This avoids most of the censorship, and gives you a broad range of views so you can decide.
@_date: 2015-12-11 10:28:02
I have a morbid fascination with train wrecks.
@_date: 2015-12-11 00:24:45
Wow. Theymos lives in Wisconsin...
@_date: 2015-12-08 00:38:15
Hi - As a sometime notorious "big block" supporter, I have disagreed with nullc and theymos in the past.
However, I read the above post (and also saw the video and read the transcript of Pieter Wuille's presentation on Segregated Witness and Fraud Proofs) - and I am happy to find myself actually agreeing with stuff from nullc and theymos today!
My "rave review" of Segregated Witness and Fraud Proofs is here:
I think much of the cause of the never-ending debates over the past year has been due to the fact that most of us all sincerely *do* want "decentralization" - but there are actually many "dimensions of decentralization" (of nodes, of mining, of development, and of governance), and many trade-offs between them all - and people have different philosophies, so we ended up dividing up into camps, which often opposed each other rather vehemently.
I also suspect that many programmers don't always have the time or inclination to summarize their work for their users - leading to even more confusion. 
But I'd rather have devs who don't always communicate as well as they program (because the opposite is lethal: so-called devs who don't always program as well as they communicate =) 
So despite my criticism of Gregory Maxwell, Peter Todd, Adam Back in the past - I do think they are great at programming and do want to see Bitcoin succeed. (I also think Gavin Andresen and Mike Hearn also are great at programming and do want to see Bitcoin succeed.) If we didn't have Segregated Witness to squeeze more data into less space, then stuff like BIP 101 and XT would be more urgent, and would get rolled out *if nothing else was available.* 
Personally I'm neutral about which scaling solutions are adopted - although of course I do understand "scaling" to mean more something along the lines of "squeezing more stuff into less resources" rather than "simply gobbling up more resources". Translated into specifics, this means: [*faute de mieux*]( I supported BIP 101 and XT. But translated into further specifics this also means that that now that we have Segregated Witness and Fraud Proofs, I can turn on a dime and go back to supporting this feature from a Blockstream / Core dev! 
(Yes I am not ashamed to say that I really am that fickle - I just go with the best scaling solution *actually available,* which up till today seemed to be only stuff like BIP 101 and XT - but now seems to be Segregated Witness.)
Today is the first day in the past year of debates where I once again feel optimistic Bitcoin succeeding - largely due to the serious mathematics-based (instead of infrastructure-based) scaling features of Pieter Wuille's work on Segregated Witness and Fraud Proofs.
Regarding "versionbits": I hear that nullc already did a bit of work on SegWit (on a separate network), and maybe one of the reasons we didn't hear more about it was he might have felt unsure about how practical it would be to roll out (if it required a hard fork).
I hope that stuff like "versionbits" will provide a more graceful upgrade and life cycle management process for Bitcoin, so that devs who want to program enhancements will feel that there is a path for them to do so, without interrupting the user base.
By the way, there was also a post from Mike Hearn a few months ago on medium.com arguing that even when a soft fork is *possible,* a hard fork may actually be *preferable* (I believe it has something to do with at least making everyone on the network aware that optional additional semantics have been added via the fork).
I would be curious to hear a comparison on Mike's arguments re: hard forks on medium.com, versus "versionbits" (which I believe is based on an insight from Luke-Jr). Specifically, does versionbits address the issues raised in Mike Hearn's post?
@_date: 2015-12-27 15:24:50
This is really interesting.
At first I was thinking that they wouldn't be able to mine most of the 2016 blocks because the transactions to put in them hadn't been submitted to the mempool yet (the mempool isn't *that* backed up yet =) - but then I remembered that a certain type of "selfish miner" could mine a block with no transactions - just to claim the coinbase subsidy.
@_date: 2015-12-08 01:11:35
OK, thanks for that clarification.
From what I understand, refutation depends on "no censorship", while [whatever our previous approach was called] depends on "no sybils" - and "no censorship" is easier to guarantee than "no sybils".
I think it's great that this kind of work has gone into fundamental logical aspects of the proofs involves in validating Bitcoin transactions, and I am fascinated in particular about the opportunities for parallelization opened up by Fraud Proofs.
Some people have mentioned that this stuff was in Satoshi's white paper (something about "alerts") but maybe it wasn't so clear at the time.
Also it seems that Satoshi's original implementation may have been rather "monolithic" - and now we are seeing parts of it getting peeled away in a process of "refactoring" - the most obvious example being splitting the "signature" data from the "amounts and addresses" data, by putting each one on separate top-level branches of the Merkle tree.
All in all, this kind of stuff shows great progress, and I'm very pleased to see it happening.
@_date: 2015-12-08 00:05:59
The Fraud Proofs part of Segregated Witness is absolutely brilliant.
It seems like it really can makes some aspects of validation "embarrassingly parallelizable".
I believe it does this by proving something "bad" (fraud), rather than something "good". Thus it seems to be in the spirit of "refutational theorem proving", and this "inverting" of the proof obligation in turn reduced the assumptions required about the network from "no sybils" to "no censorship".
So Segregated Witness + Fraud Proofs appears to do a fundamental "refactoring" of Bitcoin's data structure (moving "signature" data into *one* top-level branch of the Merkle tree, and moving "amounts and addresses" data into *another* top-level branch).
This simple and natural refactoring in turn looks like it is enabling a surprising broad range of optimizations, including:
- reduced storage usage (less blockchain bloat)
- reduced memory usage (smaller messages)
- reduced network security requirements (only needs the weaker "no censorship" assumtion instead of the stonger "no sybils")
- allows distributing validation work in a more "parallel" fashion (a node can Fraud Proofs from various other nodes)
This is the most impressive and positive news for Bitcoin which I've heard all year.
This is also the first time that I find myself agreeing with posts by nullc and theymos, and with ideas from Luke-Jr (his insight about using version numbers to more safely support soft forking), and with features released by a Blockstream / Core dev. 
People may recall that I have been in the "big blocks" camp, *faute de mieux*
Big block may still happen someday (and proposals such as BIP 101 and Segregated Witness are of course totally separate and pretty much orthogonal) but they would certainly be less urgent for a while if Segregated Witnesses squeezes 2x - 4x more data into existing space.
Seriously I have been mega depressed and pessimistic about Bitcoin for the past year with all this scaling drama - but today after seeing Pieter Wuille's video on Segregated Witness &amp; Fraud Proofs, I'm optimistic and hopeful again.
My rave review of SW can be read here:
@_date: 2015-12-18 13:14:11
Seriously, I hate sites that overuse JavaScript.
@_date: 2015-12-18 18:23:05
I appreciate the attempt by Jeff, but there are still two big things wrong with this proposal:
(1) It's unnecessary meddling and micro-managing. Blocksize (and price) are actually supply-and-demand issues, which can and should only be decided by the market. 


Therefore, the market is the appropriate mechanism for setting them - not via some parameter in code.
(2) The increase Jeff proposes here would be additive / linear / arithmetic - but the growth in volume will almost certainly be *multiplicative / exponential / geometric.* 
In other words, even we were to accept the incorrect notion (see (1) above) that a dev *could* or *should* set this growth rate, any such growth rate would have to be a *percentage*, not an absolute number.
I can understand Jeff's failure to appreciate (1) - because it's a kind of subtle point which only recently has entered the discourse.
But his failure to appreciate (2) now makes me worried. Everyone knows that systems like this don't grow linearly, they grow exponentially. 
Frankly I'm kinda shocked that someone who considers themself to be a Bitcoin dev doesn't know that basic and obvious fact about how systems like this tend to grow.
@_date: 2015-12-28 14:41:00
What on earth are you talking about?
If I do a *static* site, I use raw HTML, and host it someplace like github.io or S3.
If I do a *dynamic* site, I use nginx like many webdesigners concerned about performance. 
This typically involves some framework (ie, the site is *dynamic*), which could be based on PHP or whatever. 
In that case people typically also use a PHP accelerator - I like the combo of Ningx + PHP-FPM.
Everyone knows how to customize their nginx.conf file so that it handles the static content one way, and the dynamic content another way.
Maybe to satisfy your picayune definition I should have said I use "a stack of technologies based around nginx" - but normally you don't need to be so careful about spelling it out like that, since everyone knows that already.
My point in saying "only nginx" (which should have been obvious from the phrasing) was that I had only used nginx and never Apache - ie I used "only nginx" and not Apache.
I always use Ningx + PHP-FPM with a PHP-based framework - or if I'm using a framework based on some lesser-used languages, I still might tend to use nginx as a reverse proxy.
What is it about this particular Bitcoin sub that brings out such angry trolls like you? Jeez a person can't even say they use nginx without getting attacked by some freak like @_date: 2015-12-08 01:52:32
Yeah I also need some clarification here.
I thought that SegWit simply would allow squeezing 4x more data into *existing* blocks - so if the block size would stay the same and SegWit gets rolled out, then than an existing 1 MB block would behave "as if it were" actually 4 MB. 
(Kinda like those new fluorescent light bulbs in some countries, which give much more power for the same number of watts, so they have one number on the package giving the actual watts, and then another number giving the watts this would be "equivalent to" for an older, incandescent bulb.)
Also, this would mean that if the "actual" block size increased to 4 MB *and* SegWit had been rolled out, then that would give an "effective" block size "equivalent to" an older 16 MB block - right?
@_date: 2015-12-07 23:19:43
I know, I'm really happy to be *agreeing* with Luke-Jr today.
I know there have been many "wars" over the past few months - but I sincerely believe this is because there are so many "dimensions of decentralization" - and different people with different philosophies are attracted to different dimensions, and I also sincerely believe that most people who feel that they are on different "sides" of various debates actually do want Bitcoin to succeed. 
@_date: 2015-12-08 00:11:27
I thought the presentation was excellent. I read the transcript and then listened to the video.
Pretty standard fare for this sort of computer science stuff.
Pieter spoke well and his slides were nicely done (using Prezi by the way).
Plus the ideas were amazing!
PS - I have been an outspoken supporter of big blocks, and an outspoken critic of many things from Bitstream / Core devs (eg their prioritizing stuff like RBF, or their view of the blockchain as a settlement layer with LN).
But math is about not having to trust people. I don't care who proposed Segregated Witness and Fraud Proofs - this stuff is great on so many levels, this is the first time all year I've felt optimistic about Bitcoin again.
More rave reviewing from me here:
@_date: 2015-12-07 21:31:50
Yeah, it's a big change - and it could have been included in the software at any point over the past 6 years.
It also seems like a really, really useful change - a simple and natural and elegant refactoring of the basic data structure (splitting the validation stuff off from the address and amount stuff - specifically at the two top-level branches of the Merkle tree) - which leads to major improvements across a surprisingly wide range of dimensions: storage, bandwidth, processing, even network security assumptions.
When something is a win-win-win like this (refactoring a data structure yielding storage, bandwidth, processing and security improvements), it *has* to go in, even if it's a major rewrite / upgrade.
I think what we're starting to see here is that, up till now, Bitcoin has been rather "monolithic". The code from Satoshi wasn't very modularized, and the participants on the network haven't been very specialized.
Over time, specialization of participants has occurred naturally (mining, pools, SVP, etc.) - and Segregated Witness &amp; Fraud Proofs seems to be a major step towards "modularizing" Satoshi's original monolithic data structures (introducing a clean separation between validation and amounts &amp; addresses).
The fact that this is being done so explicitly and naturally at such a "natural" location in the data structure itself (putting the signature data into one top-level branch of the Merkle tree, and leaving the amount and address data in the other top-level branch) is simply further indication that not only the *concept* is right here, but the *implementation* is also right. 
Implementing this "split" between signature and addresses/amounts at such a natural location will most likely encourage maximal simplicity (naturalness) when doing the further optimizations in code, networking, storage and memory usage, network security assumptions (distribution of proof broadcasting) - and I think we're already see some of this now, ranging from how SW supports a very natural and straightforward pruning of the blockchain, to how FP (Fraud Proofs) supports a very natural way of distributing and broadcasting validation data on the network.
Really I think SW is rather breathtaking in the range of issues which it addresses through such a simple and natural splitting of data into validation data versus amounts/addresses data. Bitcoin *has* to implement this (even though it does represent a non-trivial amount of work), and will be much stronger as a result.
@_date: 2015-12-18 04:12:25
The question of whether your org chart is deep or flat is actually beside the point given your obvious unwillingness / inability to listen to users' needs &amp; requirements.
Thus there are really only two possible outcomes, here:
- either Bitcoin becomes irrelevant, 
- or Core / Blockstream becomes irrelevant.
@_date: 2015-12-14 16:53:17
Why do these Core / Blockstream devs keep merging radical changes like this into Bitcoin with no debate or consensus?
@_date: 2015-12-28 14:44:05
Powering 25% of sites on the web, according to a recent slashdot article. 
Don't be such a snob. Look, I'm no fan of WordPress either, but I use it to make sure it's easy to find people to maintain the site. 
I've researched dozens of other frameworks and CMSs as well, but at some point, it does make sense to stick with something popular.
@_date: 2015-12-18 04:14:06
Your anger and your attempt to discourage me from posting here are merely evidence of your desperation - ie, if you actually felt confident that you were right, you wouldn't talk that way.
@_date: 2015-09-09 04:55:34
What's the shock and awe narrative you're referring to here?
Things are really simple:
- The block chain is close to getting congested.
- There is lots of extra capacity available (bandwidth, processing cycles, memory, storage) which could easily be thrown at the problem.
What would any sensible open-source project manager do in such a situation?
Just change a single parameter (the max block size) and leave the other 99% of the system unchanged.
Are you really against such a common-sense solution?
@_date: 2015-09-12 18:26:58
I totally hear the concerns being expressed here, as I have also shared them for many of the preceding months.
If I understand correctly, several people are saying saying that jurisdictional risk due to centralization, presumably due to larger bandwidth requirements due larger blocks, is unacceptable.
Regarding the apparent "gotcha" phrase from Gavin, it seems to me that he's saying "over-decentralized" as a shorthand for saying "more-than-sufficiently decentralized" - in this specific context regarding jurisdictional risks.
People making these arguments sound genuinely concerned and worried. I know I certainly felt exactly this way about this issue.
Eventually however I became more convinced by the counterarguments, such as:
(1) Centralization risk is probably due more to *other* factors, not due to bandwidth requirements due to block size. This has been convincingly argued by Mike in his blog, as well as by a miner with a mid-size operation in Washington State. 
(2) The current bottleneck of 3-7 transactions per second worldwide itself represents a kind of centralization risk. Mike has argued very convincingly about this in his blog, saying that probably the biggest risk to Bitcoin right now is this bottleneck, because adoption by an insufficiently high number of users could mean that the powers-that-be could still try to strangle bitcoin which it's still in its cradle, if they wanted to - without enough of an outcry to really stop them, simply due to this arbitrarily (and dangerously low) 3-7-transactions-per-second bottleneck.
(3) The only responsible approach, when dealing with so many trade-offs, is to always talk in terms of a *comprehensive* threat model, listing and prioritizing / ranking *all* recognized threats. Such a threat model has actually been set forth by Mike on the Google Group dealing with the newly released software version in question.
A lot of the arguments I've seen against increasing the maximum block size seem to be made in isolation: bigger blocks means slower propagation / higher bandwidth requirements, centralization is already a risk we want to avoid, therefore *no max block size increase*.
I agree that bigger blocks probably do tend - to *some degree* - to reduce decentralization - but this is a multifactorial situation we're dealing with here, and there is no sense in making an argument based on a single isolated factor. We have to weigh all the factors together.
So this is the kind of thing that "brought me around" from my initial fear: 
(1) mining is *already* centralized and the main driver isn't block size; 
(2) the 3-7 transaction-per-second bottleneck is probably the biggest centralization threat we need to address right now; and 
(3) the only feasible approach presented so far (to prevent the network from getting congested in the near future), *based on the track record of projects and programmers who have a track record of success in the real world*, is to go with guys like
-  Mike, who is taking a mature, managerial, comprehensive, practical approach here: 
-- attempting to alleviate the worst bottleneck, 
-- to address the most dangerous centralization risk, 
-- based on a comprehensive, prioritized threat model, and 
-- supported by an actual software release, 
-- consisting of a minimal modification to a single parameter (maximum block size), 
-- subject to a simple and carefully designed fork trigger (two weeks after over 750 of 1000 of previous blocks).
- and Gavin:
-- who seems to have a very clear notion of what "governance" (and "consensus") really mean in a this sort of situation, involving open-source p2p software running on a worldwide network involving financial incentives.
In other words, I have listened to pretty much everything from various devs and other stakeholders over the past few months, and the stuff from Mike and Gavin wound up *convincing* me (even though that had both quite definitely initially been on my shit list: Mike due to the passport thing, and Gavin due to the CIA thing).
Meanwhile, the other offerings are worse:
- BIP 100 (the voting thing from JGarzik) is not simple, and not predictable - so it could be inconvenient for managers, who like to do their capacity planning ahead of time.
- LN from Adam Back is still vaporware, and LN itself will be highly centralized (if and when it comes into existence) - in fact the "jurisdictional centralization risk" of LN seems (to me) to be much worse than jurisdictional centralization risk due to bigger blocks.
- Peter is excellent at coming up with hard-to-expect threat vectors and unusual game theory analysis, and I think he deserves our greatest respect for this, and he is obviously helping the network. 
Meanwhile, on the other hand, I think I can point to two examples of what one might call "over-analyzing" or "over-engineering" from Peter:, eg:
- his RBF / scorched earth thing, which seems like it can mess up zero-conf stuff while needlessly complicating the protocol
- his preference to implement DoS protection by deleting lower-fee transactions, rather than random transactions (the way Mike's 0.11B fix does)
I'm not saying that Peter's stuff is necessarily *wrong* - I just know that if I were a programmer proposing these solutions, and if I were working under a mature and experienced manager, then the manager would reject these solutions (and eventually I'd come around to accepting and understanding why).
In other words, Peter tends to introduce a bit too much complexity to the overall system, merely to handle lower-priority edge cases - both in his analysis and in his solutions - and I think experience tends to show that the simpler and more generally applicable approaches tend to be more successful.
So I'm a person who was initially wary of guys like Mike (because of proof-of-passport) and Gavin (because of the CIA meeting) - and I was way more into Adam (because he's a great cryptographer) and Peter (because he is able to think up all kinds of really hard-for-me-imagine threat vectors).
Now, I have hard that the proof-of-passport would have been some kind of anonymous zk-SNARK thing - so maybe it wasn't such a red flag after all.
And if I were a recent college grad living in Amherst (who is involved with local town meetings and such, as Gavin is) working publicly under my own name on major open-source financial software projects and I got an "invitation" to talk to the CIA - well, I probably wouldn't be too *thrilled* - but I'd probably end up going and trying to make the best of it. 
So what I'm saying is - I no longer feel paranoid about Mike and Gavin. They just seem like not-quite-radical programmer dudes, who aren't living in Belize or in a squat in London - and I feel ok using their open-source software (which is all auditable anyways).
Later, after reading Mike's posts on his blog, and after seeing that he actually released something workable - and after seeing Gavin's recent video on governance - and after seeing repeated occasions where Adam and Peter have tended to focus on more-complicated solutions for lower-priority risks (and meanwhile JGarzik proposed an overly-complicated and less-predictable scaling solution with BIP 100), i just feel that the simplest and safest next step is BIP 101.
By the way (in particular to if you are interested in forming an opinion about Gavin, I would highly recommend watching his recent YouTube video, where he gives a very decent explanation of his understanding of governance. It's not the kind of thing that seems really possible to argue *against* really: he's basically saying that given an open-source p2p project, governance is pretty much various developers doing their best to release the best software, and then seeing what the network adopts. 
Many of the other views he expresses are similarly low-key, realistic and pretty much common-sense and irrefutable. 
I know we're not supposed to need to "trust" anyone in this situation, be it Adam or Peter or Mike or Gavin - but I do in some sense "trust" Gavin and Mike much more now than Adam and Peter. I don't think Adam and Peter want to harm Bitcoin - I think they really want to help, and have and will continue to do so.
It's just that at this specific juncture - with the network starting to get congested in terms of transactions-per-second, some simple practical realistic solutions are needed, based some simple practical realistic approaches to governance - and we're seeing that from Mike and Gavin more than from anyone else - which is why I feel fairly confident that the next steps for Bitcoin should and will be in the direction outlined by Mike and Gavin - again, not *directly* because Mike and Gavin are some how "more cool" - but simply because they simply recognize the natural direction a network like Bitcoin is going to tend to take anyways - basically optimizing the tradeoffs between the most significant among the multiple threats and bottlenecks currently affecting it in the real world.
@_date: 2015-09-09 02:29:46
He doesn't need to join the conversation now - he already weighed in years ago - he said that:
- the current 1 MB block size limit was a temporary measure, and 
- the max block size should be raised, and 
- a certain amount of centralization of miners and full nodes was normal.
@_date: 2015-09-13 00:26:51
As I've said: my internet sucks, but I'm pretty sure I can download 8 MB or even 16 MB right now.
So using BIP 101 probably doesn't even mean I have to start using a datacenter - I can still run a wallet at home for the next few years, using the same code (core + BIP 101) which has been successfully tested over the last 5 years.
Anyone proposing anything different is proposing seriously messing with a billion-dollar network using untested and unreleased (non-existent) software.
Look, I'm often very tin-foil or idealistic myself. I actually do have the kind of personality which tends to favor complicated clever mathematical solutions which won't be available for years, and I'm pissed off at governments for snooping on people and ISPs for not giving us enough bandwidth.
Meanwhile, I'm pretty sure that even little ole me, with my crappy internet, can already download 8 MB and 16 MB blocks - so it just makes sense to leave 99% of the system unchanged, and simply upgrade to bigger blocks. 
This automatically lets me use Bitcoin for the next 3 or 4 years - with almost no changes to the code, and most locations still being able to run it.
@_date: 2015-09-09 02:57:33
This kind of broad-based knowledge and awareness that Mike shows in various fields ...
- ranging from doing scaling and security on major projects at Google, doing major essential upgrades to Bitcoin such switching from BDB to LevelDB or implementing BitcoinJ to pave the way for Android clients, and providing a realistic and pragmatic initial proposal for defining a "threat model" for Bitcoin (see the Bitcoin XT Google Group)
- to having a general understanding and appreciation of social, political, economic and game-theory issues such as the real definition of consensus (it's a network thing, not a dev thing), historical failures of consensus in politics (the Liberum Veto), the nature of hard forks versus soft forks (and the better security provided - surprisingly - by hard forks - see his blog entry on that one), plus being provide basic quotes such as this one from Churchill about how democracy sucks less than everything else
... these are some of the reasons why I'm personally very thankful and reassured that someone of his breadth and depth and calibre is working on Bitcoin.
Mike isn't just a coder - he's also a scholar and a gentleman and a manager. In the end, the involvement of someone like Mike will prove to one of the crucial elements to ensuring the success of Bitcoin.
Recently in addition to reading the various Bitcoin forums, I also make sure to look at this link every day now too:
@_date: 2015-09-12 22:43:50
Yeah I hear what you're saying (in a way) - it would be "corporations" running Bitcoin mining operations processing my transactions.
But in this case, they don't quite seem like "corporations" in the more scary sense - they're just a bunch of permissionless whoevers, maybe in China or wherever, who simply had to install a bunch of commodity hardware and start running some open-source code.
This is what we have now: a bunch of miners and pools around the world, not even very well known most of them (who are they? where are they?) running mining and pools.
The reason they don't seem like "normal" corporations is because they're all running the same open-source software and they're all competing against each other but still incentivized to protect the network (as a way of protecting their investment and future profits).
This was the state of affairs which Satoshi actually predicted. I don't see what's so wrong with it. It just seems like datacenters springing up like mushrooms in a hashing rate arms race trying to chase down profits on a very level playing field all running pretty much the same software - and nobody even seems to know very much about who or where these miners are.
It seems that the incentives built into the network are keeping them competitive and honest and motivated to continue to protect the value of their investments and their network.
I can't run a datacenter like this out of my house - but anyone with enough "boxes" can. 
This seems subtly but significantly different from a company like Google which also has datacenters all over the world.
Anyone with enough "boxes" can set up another bitcoin datacenter, but nobody can set up another Google datacenter.
So... in this sense, it doesn't really seem all that "corporate". Each datacenter may involve some kind of "corporation" - but there's evidently a bunch of them, and they're all running the same stuff, and they're all basically forced to cooperate to keep the system healthy (due to the way the incentives were designed).
So wouldn't you agree that Bitcoin mining datacenters are in this sense fundamentally different from Google search datacenters?
If so, there doesn't seem to be much to worry about. Joining the Bitcoin network is still "permissionless" which seems to be the main thing.
@_date: 2015-09-09 01:03:44
Hi, I just wanted to make this contribution today here in the "free speech zone": 
**BIP 101 is KISS ("Keep It Simple, Stupid")**
It's better (read: safer) to just...
- keep 99% of the existing code base, 
- change a single parameter (max block size) and 
- throw some more hardware and infrastructure at the problem (by adopting BIP 101)
... rather than inventing a whole new level-1 protocol such as Lightning Network. 
BIP 100 is more complicated than BIP 101 in terms of game theory, economics, or predictability - which means that BIP 101 is better - ie safer - than BIP 100.
LN is a non-trivial engineering task, with no guarantee of success. LN might be a great idea and we should be happy that people are working on it - but it sure ain't KISS.
**Evolving viewpoints**
A few years ago, I was actually suspicious of Mike (because of proof-of-passport) and Gavin (because of that meeting with the CIA) - and I preferred the idealistic and speculative mathematical and game-theory scenarios discussed by guys like Adam Back and Peter Todd. It was just so much more clever and hence more appealing to the side of me that likes complicated puzzles and idealized solutions in the realms of mathematics and programming.
I have the greatest respect for Adam Back as a cryptographer, mathematician and innovator. For example, his proposals for "homomorphic encryption" which he shared on bitcointalk.org could provide the groundwork someday for much more anonymity in Bitcoin.
And I have read and liked a lot of stuff from Peter Todd - from back when he dumped a bunch of his coins when cex.io got close to triggering a 51% mining threat, and his more complicated stuff regarding RBF. I like math and programming and I like clever complicated solutions!
But, after reading everything I could find in the past few months on both sides of the block size debate, I've finally been getting some serious reminders about how starry-eyed and idealistic and *impractical* mathematicians and programmers can be. (I include myself in this group by the way  although Im not *great* at math or programming, I have studied and worked a lot in these areas.)
Mike has a lot of practical experience dealing with security and scaling at Google (plus also implementing LevelDB in Bitcoin, and implementing a Java client BitcoinJ paving the way for clients on Android), and Gavin has a lot of practical experience from his time as the lead maintainer of the original Bitcoin client - and they both show the kind of maturity and practicality and common sense (and understanding of scaling and game theory and economics and threat modeling) which are most important for ensuring the success of an open-source project. 
**Some reference material**
If you have time, I recommend perusing Mike's posts at the link below, where you see that not only has he done some important coding on Bitcoin (changing from Berkeley DB to LevelDB, writing BitcoinJ) but he also has a solid experience and understanding of how to prioritize issues involved in major programming projects:
And I also recommend the recent video from Gavin, where he shows a clear understanding of governance and consensus on open-source projects:
Meanwhile, although I was enthralled for a while with some of the innovative mathematical ideas from Adam and exotic game-theory threats from Peter, I no longer think that these things should be *prioritized* - to use a word which occurs frequently in Mike Hearn's discussion of threat models on Google Groups:
I myself can get heavily into mathematics and programming (and can often spend way too much time pursuing things in those areas which are *not* a priority), so I appreciate the "managerial" common sense and maturity displayed by people like Mike and Gavin to establish priorities and deal with the most important things first.
I also think that Mike and Gavin have a much more realistic and practical understanding of "governance" and "consensus": namely, for a network running open-source code, there is *no such thing* as "developer consensus". *Anyone* (including an *anonymous* developer - anyone remember Satoshi Nakamoto?) can release *anything* they want - and then it is up to the *network* to decide to adopt it or not. This is the only real meaning of consensus. 
And, as Mike has stated elsewhere: if the code cannot be forked, then it's not open-source.
The biggest priority in the short term (for the next few years at least) is to *provide a simple scaling solution to support more user adoption, making maximal leverage of the stuff we already have: the existing code base and the available hardware and infrastructure*. This is the direction that Gavin and Mike have been focusing on. 
(By the way, as Mike has also pointed out elsewhere: *user adoption* itself is a very important metric of decentralization. If hundreds of millions more people start using the block chain directly in the next few years, this grassroots popularity really strengthens the system against many significant threats, such as interference from hostile state or corporate actors.)
Meanwhile, I think Adam's work on LN is something which *could* be important for the *longer term* - while some of the areas which Peter have focused on (such as RBF and "scorched earth") might be merely marginal improvements - or might actually make the system worse.
**Shower thought: What if Adam had been more of an early adopter?**
The other day I had a "shower thought" where I wondered what would happen if 1,000 people all donated 1 BTC each to Adam Back right now. 
Evidently Adam, while being a fine mathematician / cryptographer and the inventor of the Bitcoin forerunner HashCash, was actually *not* a big-time early adopter of Bitcoin, so he apparently doesn't have very much "skin in the game" in terms of actual stake as a holder **on the current level-0 block chain.**
I sometimes wonder how things would be now if Adam had "gotten in on the ground floor" as an early "hodler". It might make him more inclined to work on providing improvements to **level-0** (the block chain), instead of going off in some other direction trying to invent some complicated new **level-1** system (Lightning Network). 
Regarding the block size debate, we should apply the "KISS" principle here: "Keep It Simple, Stupid". 
BIP 101 follows this KISS approach, while LN and BIP 100 (and side-chains and other interesting proposed projects such as RBF) are non-KISS, and hence more risky, and hence should be *deprioritized* (or perhaps even *deprecated*). This is simply practical common sense based on the experience of successful managers of big projects including scaling open-source software. 
In a nutshell: if space on the block chain is starting to look like it might get congested in the near future, and more hardware and infrastructure are available right now and in the foreseeable future, then we have a choice between...
- *keeping the entire system and code base 99% the same* while simply increasing a constant (the max block size) and throwing some more hardware and bandwidth and memory and storage at the problem; versus
- inventing a whole new level-1 protocol which could raise a whole bunch of hairy and unknown engineering and game-theory and and economics and centralization issues (not to mention the very real possibility of introducing *bugs* in the expanded code base and *vulnerabilities* in the more complicated network due to LN)
...then it's a no-brainer that the first option is the safer choice to scale the network in the short term.
Yes I've heard all the arguments that increasing the maximum block size could lead to more centralization - but this is something we can keep an eye on in advance. 
- Specifically, there are about 6,000+ nodes right now. If and when we get close to hard-forking to allow bigger blocks, *we will know in advance* whether there are still about 6,000 nodes on the network - or some other number which is still in some practical sense "acceptable", plus also in diverse enough jurisdictional and geographical locations - simply by looking here:
- As Mike has pointed out elsewhere, a web page itself typically is 2 MB in size, and the major driver of miner centralization has not been bandwidth - it's been things like the ASICS arms race, cooling, and electricity costs.
Meanwhile, LN represents in some sense a kind of "hard fork" of an entirely different (ie much more complex) nature: Instead of simply changing just a single parameter while keeping 99% of the code and throwing more hardware at the problem, LN proposes making deep, major, "clever", non-KISS and unstudied changes in the architecture and topology (and game theory and economics) of the whole system itself.
And BIP 100 proposed a more-complicated voting procedure - giving miners too much say regarding the block size, and introducing more unpredictability (since the votes could lead to a wildly fluctuating max block size over the years).
Managers (and users and venture capitalists) *love* simple solutions where you can get 10x - 1000x - 10,000x scaling for the next few years (or decades) simply by throwing some more hardware and infrastructure at the problem, while leaving the existing codebase 99% unchanged. This is what BIP 101 does, and this is why people will adopt it instead of unproven, untried, untested, un-coded (vaporware) alternatives such as LN - or more complicated, unproven, untried, untested, riskier game-theory approaches such as BIP 100.
**TL;DR: BIP 101 = KISS = Keep It Simple Stupid.**
**Just change a parameter and throw some more hardware at the problem and don't change anything else - ie, go with BIP 101.**
@_date: 2015-09-09 04:31:21
Please explain how Mike's contributions (switching from BDB to LevelDB, writing BitcoinJ which allows Android clients, and now providing a simple upgrade to avoid congestion) constitute a "grab for power"?
Sound some power-grabbers and sockpuppets have actually gotten to people like you first.
@_date: 2015-09-12 19:36:48
Actually it is not appropriate to make the following equation or identification:
decentralist = small block
That kind of equation can at best be characterized as being glib, oversimplified, one-dimensional, etc.
This is because:
(1) There are *many* drivers which favor (or disfavor) decentralization (or centralization). In fact, we have seen several persuasive arguments (eg from a mid-size Washington state miner) who said that bandwidth was only about 2% of his costs - versus other things like electricity, which could imply that the argument "bigger blocks -&gt; greater bandwidth requirements" may not actually be valid.
(2) Convincing arguments have also been made that there are *other* important measures of decentralization - some of which may actually be *much more unhealthy* than "full node count" right now. 
In particular, Mike had a very persuasive blog post a while back arguing that "usage" (in terms of (a) how many users can use the network and (b) how many times per month or year can they use the network) is also a very very important measure of decentralization - and probably the one which probably has the worst "health" currently - ie, not many users now, and not able to transact many times on the network. 
Mike argued that anything which can easily support more users doing more transactions is top priority right now - because once we get a few hundred million users routinely using (indeed: *relying on*) the block chain a few times per month, then at that point, this provides a strong prevention against the powers-that-be daring to try to sabotage or prohibit the network.
I happen to agree with Mike's assessment here: the most anemic decentralization indicator of Bitcoin right now is the 3-7-transactions-per-seconc bottleneck. 
This is simply the aspect of the system which is in most urgent need to upgrading right now, using the simplest approach possible - eg BIP 101.
(3) A more mature, managerial approach to this whole thing is called "threat modeling" and again Mike, in addition to releasing working software, has also prepared the beginnings of a threat model for Bitcion. You can see it on the Google Group for the new software release.
The important thing about Mike's threat model is that it is *comprehensive* - ie, it attempts to enumerate all the "significant" threats to Bitcoin, and rank or prioritize them. This is the kind of thing successful managers of big projects do. And this approach has evidently shown that the centralization risk due to needlessly-small blocks (resulting in congestion which could cripple the network, or low usage which could make it easier for powerful agents to attempt to abolish it or damage it) is *worse* than the potential centralization risk due to big blocks (particularly if one listens to miners who say that block size and bandwidth are not the major constraints they deal with, and miners who say that they're fine with bigger blocks).
I've been following this whole block size debate closely for several months now, and saying "decentralist  (small block)" at this point just seems like either an accidental or wilful failure to communicate clearly, given the nuanced discussions above which have been loudly aired already.
Bitcoin is a real-world open-source p2p software engineering project, facing a variety of bottlenecks and threats. Any analysis or headline which does not take a more comprehensive approach towards ranking / prioritizing those bottlenecks and threats - instead focusing on a *single* bottleneck as if it were the *only* or the *highest priority* threat - is dangerously narrow.
@_date: 2015-09-09 04:34:13
Like you're so important. Please. 
What have you done for Bitcoin?
Mike switched from BDB to LevelDB.
He wrote BitcoinJ, which allows Android clients.
He has written up a threat model, including prioritizing.
He also has experience doing project management and scaling and security for major projects at Google.
How can you even have the effrontery to criticize him?
You need to seriously look at the psychology that has dragged you to where you are now.
@_date: 2015-09-12 19:52:37
You're correct to quote Gavin here. Actually you should have bolded this other part as well:


Now let's seriously deal with the question: What are we going to do about this?
I'll admit the situation is problematic and none of the proposed solutions are perfect.
But the number of transactions *is* increasing, and either we figure out a way to increase network capacity to handle them, or this whole majestic experiment could very well die soon.
I don't think the network automatically becomes "permissioned" simply because it might need, say, 10x more hardware and infrastructure to be a full node or miner.
This would only happen if there were 10x more usage. The thing would still be p2p and open-source, and anyone who wanted to jump in and set up a mine or a full node in order to handle this, could still do so *without permission* and in *totally decentralized fashion*.
If, say bandwidth becomes a major issue due to bigger blocks (and by the way this is still in no way proven - but it does seem to be "implied" a lot by the people favoring small blocks), then this could "shake up" the mining and full node ecosystem - possibly shifting hash power to regions with more bandwidth. 
It still seems that this is a simpler and more decentralized approach which would have a much higher chance of success - versus the complicated and centralized proposals such as LN.
Nothing is perfect here - but BIP 101 is simple and it's implemented and it is still quite permissionless. It just seems like the safest way to go here.
@_date: 2015-09-09 02:24:50
My definition of "reckless" and "contentious" is:
- "pinning my hopes on idealistic vaporware while failing to change a simple constant in the existing code base so as to maximally leverage already-available hardware and infrastructure - so that the network becomes so congested due to small blocks that it dies".
@_date: 2015-09-13 00:21:50
OK this LN stuff may be great - if and when it ever gets released and used.
However, remember that LN is a whole nother code base, and it *is* centralized.
Meanwhile, what is wrong with just keeping our existing code base, and increasing a single constant from 1 MB to 8 MB (and then 16 MB a few years out)?
This seems like it's the simplest approach, no new code base to audit, pretty much keeps 99% of everything unchanged. 
The only worry being: can I download 8 MB or 16 MB in ten minutes?
I have terrible internet, and I'm pretty such I'd still be able to download 8 MB or 16 MB in 10 minutes.
So I don't get what the problem is. The network is already getting congested (during the DoS attacks), and it probably will start getting congested for real in the next year or so.
So there are a couple of options being discussed here:
- Change a single variable, leave everything else unchanged, assume most people have enough bandwidth for the next few years
- roll out an entirely new code base, involving tons of new attack vectors, totally untested, still vaporware - and which will involve centralization anyways (ie: LN).
The choice seems pretty obvious. Keep it simple: go with BIP 101.
@_date: 2015-09-12 22:22:35


I have a really slow internet connection.
I'm not trying to mine, but I would like to be able to continue to run a full node, just so I can control my own wallet.
I'm pretty sure that this means I would only have to download 8 MB per 10 minutes to run a full node.
When I run utorrent (which I believe is a p2p thing similar to a bitcoin client), it has always maxed out around 70 kB per second on the download site.
So after about a minute of downloading a movie on utorrent, I usually have about:
70 kB / sec * 60 seconds
= 4200 kB
= 4 MB
So I can download about 4 MB a minute, or 40 MB every ten minutes, on my crappy internet connection. 
Based on these simple empirical results, as someone operating a full node to control my wallet on a crappy internet connection, the simplest approach to avoid congestion for the next few years would seem to be: adopt something like BIP 101, where I'd need up to 8 MB now, and 16 MB in a few years.
I know this might not be *perfect* but it seems like it would work for 3-4 more years for someone like me - and I feel fairly safe with this upgrade since it's only changing a single parameter in the existing code base.
EVerything else I've been hearing just sounds either overly complicated and/or vaporware - eg BIP 100 voting the max block size up and down, making things less predictable - or Lightning Network which is a whole massively different code base plus it IS centralized.
Honestly I've been following this whole block size drama for months now, and BIP 101 seems like it would work fine even for people with low bandwidth for the next few years (ie, full node operators like me) - plus the kind of mature and reasonable explanations I've been hearing from Gavin and Mike just seem to make the most sense.
Meanwhile, nobody else even has a working released software solution. Adam's LN is still vaporware, and also fairly complicated, and also will definitely be MORE CENTRALIZED - so I don't see why anyone's even paying any attention to it.
JGarzik's BP 100 is needlessly complicated - increasing but maybe also decreasing max block size, which would make it harder to do planning into the future.
Peter seems to go off on overly complicated tangents - the whole RBF / scorched earth thing seems like it adds a lot off needless complexity to a simple system which has already run quite well for 5 years - and most of the attack vectors he tends to focus on just don't seem to be top-priority.
In the end, the whole thing is simple:
- Bitcoin needs more transaction throughput capacity
- Most full node operators (even with crappy internet like me) can easily handle 8 MB or 16 MB blocks right now
- Gavin is the only one who has clearly articulated what I believe is the correct understanding of "governance" and "consensus" - ie, these are things that apply to the network itself, not to any particular set of developers - ie, developers develop and the network adopts what it prefers, so consensus is a *network* thing, not actually a *developer* thing
- Mike has not only released code - he has also released things like a comprehensive (prioritized) threat model - which is something which successful project managers know how to do.
So i'm fine leaving the theorizing to Adam and Peter. But in terms of getting out a simple fix which seems like it has the best chance of keeping things running smoothly for the next 3-4 years, Mike's solution seems to be the best thing we've seen so far, so that's what I would go with.
@_date: 2015-09-13 00:15:32
If I want to run a full node, can't I continue to do so using 8 MB blocks (or even 16 MB blocks) for the next few years - even if I have a crappy slow internet connection at home?
My internet sucks, but I'm pretty such I can download 8 MB or 16 MB in 10 minutes using utorrent - so I'm pretty sure I could do it using some bigger-block bitcoin client.
Maybe there's some technical issue that I'm missing - but isn't that all I would need in order to run a full node at home?
@_date: 2015-10-08 14:59:35
OK, I had heard of Segregated Witness - I just wasn't aware that it was addressing malleability.  I will read up on it later.
*Edit: Wow, I just read the link now on Segregated Witness and it sounds great! I'm not much of a C or C++ coder, but I've studied a bit of Proof Theory (eg systems such as Coq  which also uses the concept of "witness" in this sense), so I think I actually understood most of the words* :-)
By the way, I don't think we should be afraid of hard forks.
From what I've been seeing, there are definitely things which need to be tweaked in the code base, so hard forks are going to be necessary from time to time.
I hope we manage to figure out the appropriate "governance" around this process. 
I liked what I read recently about "rough consensus", although I'm still rather unclear on precisely how it might be applied given some of the specifics of Bitcoin - which seems to be rather unique as a p2p system providing global consensus on a blockchain.
@_date: 2015-10-08 14:07:38
So, if people want to do things with transactions *before* they are mined, why not use some truly unique identifier for the Txn - instead of a mere "suggestion" which can get "malleated" at any time?
From what I understand, the sender address and receiver address (and amount?) of a Txn uniquely identifies it. 
If so, why not use this (ie, some temp ID based on the sender address and the receiver address) - for those situations where people want to do things with transactions *before* they are mined? 
Or would this identifier be too long?
Plus: Are you basically saying that Txn (ID) malleability exists in order to support zero-conf?
In other words, if we got rid of zero-conf (ie, if people no longer expected to be able to do zero-conf with Bitcoin), could we get rid of transaction malleability??
@_date: 2015-10-08 14:36:07
Naw... that doesn't make sense.
Same sender-person and same receiver-person - yeah, sure.
But different sender-address and different reciever-address.
From what I understand, the first time I sent you, say, 0.1 btc, I actually had to *totally empty out* one of my addresses (say, an address containing 1.23 btc) - actually sending 0.1 btc to your address, and the "remainder" 1.13 btc to *a change address of mine*.
I don't think this is optional - I think Bitcoin always works this way (emptying the send-address, and sending any remainder back to a change-address owned by the sender).
So the *second* time I send you, say, another 0.1 btc, a different send-address would be involved.
It would be coming from the same send-person (me), but from a different send-address (because actually my first send-address got "emptied out" on the first send).
Can someone verify if I understand this correctly?
@_date: 2015-10-08 14:18:55
OK, so the TxnID is derived from the data/signature itself, which can be malleated.
This still might not be addressing the question of *who* sets the TxnID, (the sender of the transaction, or the miner), and *when* (earlier, upon sending - or later, upon mining).
Could the same existing algorithm be used to derive the TxnID - but instead perform the derivation *later* in the process (ie, not upon sending the Txn into the mempool, but instead upon including the Txn into a block)?
