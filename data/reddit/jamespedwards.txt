@_author: jamespedwards
@_date: 2015-02-18 20:51:54
Short Answer :)  Google Datastore, Redis/[Jedis]( ... REDIS!!!, AWS RDS MySQL w/ [jOOQ]( and [c3p0]( on the client side, Zookeeper/[Apache Curator]( [Dropwizard]( [Netflix Feign]( Firebase, [BitcoinJ]( Google DNS &amp; network load balancing, GCE, GAE, Java 8, Guava, Gson, and Gradle.
* **Datastore:** We use Datastore for storing and indexing our Block, Address and Transaction objects that get served up through the API.  The fact that Datastore abstracts away the indexing and storage for a NoSQL store is pretty amazing.  As long as the number of index entries for an object isn't crazy you can replicate most of the indexing features that something like ElasticSearch gives you, but now you don't have to manage two different services such as ElasticSearch for your indexing and something like Cassandra or Hadoop for your storage.  The only thing I don't like about Datastore is that if you want to connect directly to Datastore you have run from either GAE or GCE and those have two separate client libraries :/ .  If you want to connect from an outside server you have to go through a [Remote API]( proxy hosted on App Engine which is yet another separate client library.
* **Mysql:** To distribute the processing for clustering addresses I needed an ACID compliant database to prevent any issues when two separate jobs are trying to merge the same cluster.  We also use it to store Bitcoin network peer information and the relationship of which ones we are connected to.
* **Distributed Batch Stream Processing:** We needed a distributed processing framework for aggregating address transaction data, address clustering, and updating entries in Datastore.  I generally dislike when I see engineers reinventing the wheel or maintaining a not invented here philosophy, but I ended up developing my own framework on top of Redis and Zookeeper instead of using Storm or Spark.  I've used Storm in the past and I really enjoyed the software abstraction, but when it came to trying to deploy a cluster with the existing tools and my Java 8 codebase it was  a nightmare.  So rather than risk running into the same operational nightmare with managing a Hadoop cluster for Spark,  I went with writing something that I knew I could easily maintain.  Eventually, I would really like to try to open source it.  I think its key feature is its abstraction that handles redistributing time consuming jobs for you.  Often I run into the problem of being unable to efficiently know ahead of time if a job needs to process a couple of items or millions of items and this feature solves that problem nicely.
* **Redis:**  In addition to the stream processing, we use it for tracking API request rates by IP and user key, and keeping track of valid authentication nonces all in a single pipelined request.  For future projects we are using it to keep track of unconfirmed transactions and detecting double spent outputs.
* **Firebase:** Gotta love firebase, it has saved us a ton of development time for managing user accounts.
@_date: 2015-02-18 04:07:21
The real power behind Blockstem is the ability to retrieve data with simple SQL queries.  With the current blockchain API's you are limited to retrieving data if you already have a known key such as an address hash. With Blockstem you can find addresses based on metrics you find interesting such as balance, volume, or the average number of inputs per transaction.  
There are several additional metrics available as well which can support different business use cases.  Consider the average number of inputs per transaction for an address, this could be used to help classify user behavior. 
Also, the API exposes our internal address clustering which was inspired by the research/work of Michele Spagnuolo on [BitIodine](  Given an address of interest, this allows you to find other addresses that are probably under the control of the same entity. 
There are some scaling limitations with the current blockchain API's.  Consider the use case of retrieving hundreds of thousands of transactions for an address.  Instead of having to page through them all sequentially you can take a divide and conquer approach and page through different block ranges for an address's transactions in parallel.