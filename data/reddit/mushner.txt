@_author: mushner
@_date: 2016-03-05 02:24:25
Yeah, right ... keep obsessing about your definitions of words and calling them "arguments", I'm not going to waste another minute with this pointless exercise.
@_date: 2016-03-05 02:05:34
It is defined differently by Hayek, Marx or Brian Armstrong, get over it and focus on what's important - the actual meaning intended, which is not hard to understand at all.
@_date: 2016-03-05 02:02:21
He also said it should be increased eventually when needed e.g. now
@_date: 2016-03-05 01:51:28
Central planning meant by Brian Armstrong is obviously simply a planning happening at one center, meaning Core dev team ... there is nothing else to it, don't try to be too smart about it by redefining simple words.
@_date: 2016-03-03 15:10:32
That quote and his response is extremely relevant right now, to demonstrate how key people changed  their "sentiment" of what Bitcoin is and should continue to be, without bothering to consult the community about it.
51% of the hash rate became "the consensus"
hard forks became "contentious"
digital cash became "settlement network"
and so on ...
The whole idea of what Bitcoin is has changed in their minds and this post demonstrates perfectly just another aspect of this. If they want something different than the original vision of Bitcoin then that's fine, they can create an altcoin. But those who invested themselves in Bitcoin expect it's original vision to be respected.
@_date: 2016-03-05 02:12:36
Every transaction that contains a non-trivial fee is a "regular transaction", also if it's possible to cheaply mount a "spam attack" on the network, to the point that it substantially affects the "regular" transactions, the time to raise the block size has come.
@_date: 2016-03-03 14:02:25
Chain fork does not happen with a soft fork, everybody is working on the same chain with soft fork so the only thing that makes sense in this context is hard fork, therefore that's what you most likely meant back then.
But that aside, since you no longer hold the sentiment that disagreements about policy should get ultimately resolved by a split and the better policy will win, what do you currently propose to resolve policy disagreement?
@_date: 2017-03-02 11:49:44


And you consider that a positive change? That is exactly what we should strive to avoid as much as possible - by moving off-chain, you sacrifice the security and trustless model of Bitcoin and introduce a lot more complexity and confusion, Bitcoin is complex enough as it is.
We should strive to accommodate as much on-chain transactions as possible, as cheaply as possible to drive adoption *on-chain* while maintaining security - there is a balance to maintain, it's not uber security over anything else, uber security is worth nothing if it's not usable. This is what raising the blocksize does, without sacrificing any security in any meaningful way. Those who say otherwise are spreading FUD, it is technically no challenge to have 4MB blocks *TODAY*  ... I've seen no specific (with numbers) technical argument against that, only FUD in the sense of "derp, security", "herp, bandwidth" etc. etc. with no technical argument whatsoever.
@_date: 2017-03-01 16:48:07
So you're proposing that Bitcoin should be usable only for those moving $1,000+ worth of BTC? You're okay with pricing out those who would want to use Bitcoin for lesser amounts, which is probably the great majority - not the best strategy for driving adoption it would seem.
What you're talking about is WealthyCoin, not Bitcoin anymore - which is described as P2P digital *currency*. If you want WealthyCoin, there is nothing stopping anybody to create an altcoin for that as it's fundamentally different in it's goals than Bitcoin.
Currently there is nothing in technical terms that wouldn't allow for great improvement in on-chain transaction capacity. [Recent tests show that mid-range desktop can **process 4,200tps today**]( that is twice the VISA levels which we can do NOW. And we need only a small fraction of that to meet the current demand.
So these fees and 1MB limit is completely arbitrary and artificial, it has nothing to do with technical limitations.
@_date: 2017-03-13 09:04:32


This does not apply because BU is not an "attacker", it's an honest proposal to scale Bitcoin in a manner defined by BU and miners can freely choose to support that proposal or not. It is proposed in an open, transparent and voluntary way, just as SegWit is. If you would call BU an "attack" then you would have to by the same logic call SegWit an "attack", neither is.


So if BU provided a "clear transition point" you'd be OK with it? Because I'm quite sure that's exactly what they're going to do once they have enough hashrate, to give plenty of time for the Bitcoin ecosystem to prepare for that change.
They're not malicious, they want the fork to go as smoothly as possible. And If Core wanted the same, they'd raise the blocksize once BU gets 50+% hashrate, recognizing the vote of the miners and minimizing the risks associated with hard fork by raising the blocksize - to 4MB for example which is agreed is safe.
@_date: 2016-03-01 14:21:59
I think you have it the other way around. Bitcoin was ***designed from the start*** to be p2p cash - others who want to use crypto as "settlement layer" will just have to migrate to another crypto, not hijack Bitcoin.
@_date: 2017-03-02 07:45:29


You're shifting the argument, a little while ago it was:


Now it is that you need to also relay transactions and distribute blocks - so which is it? Is it not the decision of the node operator to decide what is the purpose of their node? If they want to just insure all the transactions are legit, they can do that - your friend can do that easily with the connection that he has, if they want to also relay transactions and blocks, they can do that too, possibly with setting how much bandwidth they want to dedicate to that - it's a question of improving the node software, not the block-size per se. I do not see a problem with both kinds of nodes coexisting on the network as long as complete full nodes are in the hundreds.


Do you mean miners? Then I do not believe this is the case, there are no higher resource costs, just lower profits - there is a difference because hash power is going to be increased as long as there are profits in doing so, sky is the limit. That doesn't mean it's necessary for insuring the security of the network, at some point there is no more significant security benefit for increasing the hash rate - I believe we are well beyond that point, 51% attacking the network is prohibitively expensive as it is.
tl;dr all I've seen are these vague references to security but no specifics or actually quantified limits which are needed to judge whether they are even relevant. And all quantified tests/numbers I've seen support the case for safe on-chain scaling right now.
@_date: 2017-03-01 17:59:02


A lot of that is actually incorporated in that test, however there are other limiting factors of course - do you happen to have an independent analysis of those factors where the bottleneck is identified and maximum feasible capacity devised based on that? I would be very interested in that as all I've seen are these vague claims about technical limits but I've never seen any specific analysis of them by those who claim they're a limiting factor.
I also have a hard time believing that 1MB is by some kind of weird coincidence the perfect limit to match those limitations and should therefore never be changed. That would be hell of a coincidence, don't you think?
Furthermore, all those aspects can be greatly improved for example by pruning, thin blocks etc. why isn't the focus there?


This is because he was relying transactions, not because of the block-size - he is free to turn that off and get all the benefits of locally validated chain, trustless validation by just downloading the blockchain which is just ~4GB/month ... the size of an average movie, hardly prohibiting.


You probably pay much more in fees if you're actively using Bitcoin than it would cost to get a decent Internet connection and hardware to run a full node. So to enable people to use those funds that are currently wasted on fees - which do not add to your ability to have a trustless model - those fees need to be lowered to free up funds to actually run a full node. There are millions spent per day on fees that could be used to run full nodes instead.
@_date: 2017-03-03 07:40:52


No no no. That is *EXACTLY* what I'm saying, that is does not! Great, we agree! That's why you can not compare VISA using server farms with Bitcoin, centralized transaction recording is easy, that's not what those server farms are for, re-read what I've said so far, that is exactly what I'm saying, that your original argument makes no sense, it appears we now agree on that.
@_date: 2017-03-02 15:54:22


Sure, these are two distinct concerns, but your original argument with your friend was this:




Considering that to do that (trust no one and validate the transactions youself) you need only ~4GB/month, not 300GB+/month, do you concede now that this particular problem your friend had is not related to the blocksize even with 8MB blocks (32GB/month)?


Sure, the question therefore is how many nodes do we need to maintain sufficient decentralization? The more the better for sure, but there is a balance there to be maintained with regard to capacity - we can not prefer maximum nodes over everything else, there are other important parameters of the network. Is the network too centralized now when we have ~6,000 nodes and not 12,000? Would it be too centralized if we had 3k nodes instead of 6k? What about 1k? What is the number of nodes you consider ok for the network to be considered sufficiently decentralized then?
@_date: 2017-03-02 14:39:22


Is it? Go try to make a MySQL DB, create some table with information about a transaction (sending/receiving address, amounts) and see how quickly you can populate it - it will probably be hundreds of thousands txs/s.
Recording the transactions, which is what Bitcoin does in a decentralized manner, is not the bottleneck in centralized systems, it's a completely different animal - you do not need server farms just to keep the record of transactions, you need them for all the other stuff which does not concern Bitcoin - that's why it's not a fair comparison in this regard.
@_date: 2017-03-14 15:04:12




@_date: 2017-03-02 06:53:48
So why was the proposal to raise the blocksize to 2MB rejected? And is still being resisted by the current development team?
@_date: 2017-03-17 08:19:55
I concur with LN *without blocksize incerase* is what could easily lead to that scenario. As long as on-chain transactions are affordable, we want LN to freely compete with on-chain and if it becomes well implemented and used, then great, I'd be glad to use it. The difference is that if it becomes overly centralized KYC/AML encumbered beast I can care less as long as I can still use on-chain transactions to avoid using it - but we need blocksize increase for that. We simply do not want to put all our eggs in one basket, that's highly risky.
LN needs blocksize increase in any case, it costs close to ~$3 to open/close a channel *right now* - say you want to dedicate $10 to microtransactions on LN, that's 30% fee to do that! Fees are already too high even for LN use cases.
@_date: 2017-03-13 09:12:50


And you arrived at this conclusion how exactly? Because the plain words of the white paper do not seem to support such an interpretation.


So now you're saying hard forks are inevitable, I'm confused - that's what I'm saying from the beginning and you were taking an issue with, why? If only contentious forks are bad, then will you agree that Core should raise the blocksize once BU gets majority of hashpower as to make the hard fork as smooth as possible? Because at that point, it would be the minority running Core that would make it contentious, so it's on them and their responsibility to remove that contention, no?


That is a completely different argument, do not shift arguments please - I said it's much closer to the original Satoshi vision, hard forks are inevitable anyway and majority hashrate decides what is Bitcoin. If you do not have any further arguments in this area then it appears we agree, great and thanks!
@_date: 2017-03-03 07:57:37


See? This is exactly the unreasonable approach that I'm talking about, this is what Core is saying also, which is just dumb. Adoption is very important also, it doesn't matter if you have perfectly decentralized network when it can not be used by people to make use of it.
It's like having PGP is not enough if people do not use it. There is a trade-off to as-much-decentralization-as-possible. It also doesn't help the network in practical terms when we get over certain number of nodes, after which the benefit becomes purely theoretical and it makes no sense to sacrifice other parameters of the network to theoretically "increase decentralization" with no practical benefits. My personal guess at "enough nodes" is *several hundreds*, at which point the network becomes practically impossible to disrupt.
With this reasoning we should cap the blocksize at 1kb so you can run a node on your smartwatch - that would be a great boon to decentralization, it would also make the network useless. It is the same with 1MB limit.


-- paraphrasing cypherpunk


When your brother has an Internet connection that is capped at 300GB/month, then he shouldn't run a full node, it's that easy. There a plenty of people who do not have capped connections and run nodes. He can still run a pruned node if he wishes to verify transaction history. There are also many people whose connections are capped at much lower limits, should we limit Bitcoin because of them? No.
We can not cripple Bitcoin to accommodate nodes on sub-par resources as long as we have enough nodes, which we do. Ultimately, if Bitcoin isn't crippled and is usable as a payment currency, many businesses will run nodes and users can be just users, this is how Satoshi always designed the network to be and we have to transition to that model. Transition from majority of enthusiasts running nodes, to majority of businesses running nodes, that is the future for Bitcoin that will ensure it's long term viability. We can not rely on enthusiasts running nodes forever.
@_date: 2017-03-02 07:53:51
VISA does a lot more than process transactions, they need to have connections to thousands of banks, comply with regulations, keep logs, be very fast, verify the cards, keep records of them, watch for fraud, authenticate them etc. etc.
If they only needed to record the transactions, they probably actually could do that on a mid-range desktop, the reason they don't is because it's a minuscule part of what they do, unlike Bitcoin so it's a false comparison in this sense.
@_date: 2017-03-13 08:41:46


So you'd follow the few guys that own the largest exchanges, or the one implementation of Bitcoin protocol? HILARIOUS!
Quote from the white paper


*That* is Bitcoin, it was designed in a way that POW decides - you may like that or not but that is a fact. If majority of hashrate switched to a protocol that you do not like, feel free to switch to an altcoin, that doesn't change what Bitcoin is. Bitcoin may well even fail from the protocol change, but it's still Bitcoin. It that case, good on you for switching to an altcoin soon enough.
@_date: 2016-03-03 13:21:22
Please provide these arguments, I've actually not seen any comprehensive analysis of why they're against this very small block size increase to 2MB, just off the cuff remarks that didn't make much factual sense ... thanks
@_date: 2017-03-01 17:01:30


I would be very interested in what kind of data is the basis for this kind of claim. As contrary to it, [actual tests show that current mid-range desktop can handle ~4,200tps]( (twice the VISA levels) and we only need minuscule fraction of that to meet the demand in near and even mid-term future (several years)
@_date: 2017-01-20 15:46:26
Do you believe doubling is sufficient? For how long? Why was then an increase to 2M block size rejected with the reasoning that it doesn't solve anything long term even when it's technically safe?
Does doubling it with SegWit somehow magically make it better than simply raising the limit to 2MB? Considering that LN is not ready anytime soon for production, how can SegWit address the capacity issue better than simple 2MB block size increase?
@_date: 2016-11-20 10:15:22
2.78% now, variance - read up on it
@_date: 2016-12-05 10:01:41
Great that you care about secure computing but I'd like a quote on your claim:


because this seems to just not be true, there is an attack vector but on a completely documented feature of updating CPU microcode, if an attacker would be able to get Intel's, AMD's signing keys.
It's good to be extra careful, but hype and deceptive descriptions do not help.
From the researcher that this story originated from:


Anyway, how do we know IBM's POWER8 CPUs are better in this respect? Do they not have this feature? How do we know they're any better than Intel or AMD?
The whole story on the "backdoors":
Edit: I see that you may have meant IME on Intel, that is a legitimate concern if this "feature" can be misused by hackers/NSA - but it can be disabled in BIOS and there is no specific information about it being able to be misused, of course it could be ... but then the question stands, how can we be sure there is no such thing that can be misused in IBM's processors? The FPGA helps, essentially running anything non-standard helps so in this sense, yes, it could be better security wise ... I'm just not completely convinced that the issues with Intel or AMD do not translate to other processors as well.
Edit2: After more careful examination - yeah, this is quite important project, sorry for my kneejerk reaction but I'm used to people hyping some things. The "backdoor" in Intel and AMD are however documented features and have never been shown to do anything malicious AFAIK ... but better to be safe than sorry and have a processor without this "feature", that's for sure
@_date: 2017-09-21 14:52:08
That could be true if you spelled his last name right ;)