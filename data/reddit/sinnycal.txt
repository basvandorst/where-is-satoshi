@_author: sinnycal
@_date: 2016-04-01 20:43:43
This technology really has a chance to transform a lot of lives
@_date: 2015-06-11 17:44:50
Well said
@_date: 2015-06-11 22:45:39
It could lead to "Tyranny of the majority" which is similar to what we have with fiat wealth distribution today. 
@_date: 2015-06-11 21:49:58




100% agreed. It's absolutely crazy that this view is unpopular in the community.
@_date: 2015-06-12 03:37:28
@_date: 2015-06-11 22:21:30
I agree with you overall, but a voting system based on the number of bitcoins you have could easily be abused. 
@_date: 2015-06-11 21:52:12
@_date: 2015-06-11 16:22:16
It's exactly as it is now (without this talk about dictatorship). There should be consensus first among experts who are chosen based on merit, and then consensus among major stakeholders who are mining, running nodes and supporting the ecosystem through business and technology.
Bitcoin should function more like an electoral college, not like a democracy or dictatorship. The majority of bitcoin users are not experts and are prone to "black and white" thinking, so no good can come from a noisy democracy. And the thought of a dictatorship shouldn't even be entertained as it sets a dangerous precedent that may not bode well for future stability
@_date: 2015-06-12 12:41:33
Sorry, I misunderstood your comment. But I'm glad I did - good discussion
@_date: 2015-06-13 07:28:41
What's this bounty all about?
@_date: 2015-06-13 15:48:34
To be clear, is the bounty still available?
@_date: 2015-06-12 03:15:22
@_date: 2015-06-11 23:00:45
That seems a lot more reasonable.
@_date: 2015-06-13 07:46:29
B.S.S is on point! Jerry seemed a bit stoned :P
@_date: 2015-06-13 07:40:51
Cool thanks. I'll try to work on something over the weekend
@_date: 2015-06-16 22:47:15
I'll update with a report in a couple days. I have some code to generate a database for the comments and posts. Let me know if you want a link
@_date: 2015-06-12 14:17:05
@_date: 2015-06-11 14:47:50
Consensus among panel of experts and major stakeholders.
@_date: 2015-06-12 03:21:26
@_date: 2015-06-11 22:55:12
I'm not sure that there is an ideal. Each will have its pros and cons. Quoting my response from another thread:
 &gt;It's exactly as it is now (without this talk about dictatorship). There should be consensus first among experts who are chosen based on merit, and then consensus among major stakeholders who are mining, running nodes and supporting the ecosystem through business and technology.


@_date: 2014-08-02 04:18:00
This is expensive but not entirely unreasonable. lol @ posters who expected this to be competitive with bank issued debit cards right off the bat
@_date: 2014-08-10 05:51:55
Let me take you on a feel trip! 
@_date: 2014-10-08 14:53:50
Search for his old handle "genjix". Plenty of threads showing his shadiness and incompetence. Here is one example 
@_date: 2014-08-05 05:33:47
Not sure. Most cards (that I know of) that are structured similarly, at least offer CAD as well.
@_date: 2014-08-05 02:58:33
My bank card does this
@_date: 2014-10-16 04:07:44
Damn OP, that's super sketchy. Thanks for the warning
@_date: 2014-12-19 00:35:29
We the people.
@_date: 2014-08-09 21:45:09
I hope this gets some life back into the namecoin project. It's been kinda quiet for a while...
@_date: 2014-10-13 16:55:45
Jesus. That was incoherent even for Keiser
@_date: 2014-09-18 07:03:46
What's that all about?
@_date: 2014-08-01 15:38:26
"ordinary" Japanese people
@_date: 2014-08-04 15:36:31
It seems you're the only voice of reason in this thread...sad. I would tip you if I could!
@_date: 2014-08-05 05:31:08
I got that from here 
@_date: 2014-08-05 02:56:15
They have three base currencies that you can choose from (USD, GBP, EUR). Foreign currency conversion applies to transactions not in your base currency. 
From what I understand, you could be in Britain and choose a base currency of USD if you wanted to, but all transactions in GBP would have that 3% conversion fee.
@_date: 2014-08-04 01:26:00
Those same questions were being asked back then too. Yeah, they haven't been entirely clear on the 100$ limit. My guess is it is relevant to insurance for your wallet and wallet transactions, but not your vault.
maybe can clarify
@_date: 2014-10-08 09:22:05
Amir scammed a bunch of poker players out of their bitcoin a couple years ago. I wouldn't trust this guy with my phone number
@_date: 2014-08-03 21:19:20
Seriously? They didn't even lie in the image. Did they give a fee breakdown prior to this? No.
Come on, this is getting ridiculous. What's up with all the hate for something that could significantly boost adoption? This witch burning is really just shooting ourselves in the foot
@_date: 2014-10-08 16:21:13
I actually knew him fairly well
@_date: 2014-08-03 17:49:16
ugh. All of this unwarranted Xapo hate is gonna make me unsub from this subreddit. Enjoy your circlejerk, haters
@_date: 2014-08-06 01:39:51
do you even day trade, bro?
@_date: 2014-08-04 01:59:49


I still don't see what's wrong with this though
EDIT: Isn't this just limiting their liability on events outside of those for which they are insured?
@_date: 2014-08-04 02:12:03
I think you are right
@_date: 2015-05-30 12:34:53
I dunno man. I'd just scoop it out.
@_date: 2014-08-04 00:43:55
Put away your pitch forks and chill out. Talk about overreacting much?


















@_date: 2015-01-07 13:04:56
beer's on me next time you're in Bridgetown!
@_date: 2015-01-27 19:29:49
What am I looking at?
@_date: 2016-01-30 14:26:29
Excellent post. Thanks Greg
@_date: 2016-01-28 12:03:42
I dont understand. What does that mean?
@_date: 2016-12-17 00:13:09
Spoke with him during the bounty as well
@_date: 2016-12-16 23:21:22
That's correct
@_date: 2016-12-16 23:47:55
Is the work linked in OP's post part of what you reviewed?
@_date: 2016-12-17 00:01:08
Neither of you paid. I have PMs showing I messaged you about the bounty, stating that I want to work alone. You asked "why alone" and never responded to my many PMs after. Then you paid out on bad work and now you're linking to my report? 
No problem, it's all open source :)
@_date: 2016-12-16 23:45:22
Communicated with you on Twitter and Reddit during the bounty. You just chose to conveniently ignore it?
@_date: 2016-12-17 01:54:18
Hi Mike, I posted here 
@_date: 2016-12-17 00:07:53
Link to their work then
@_date: 2016-12-17 00:18:34
I was 90% done the work when you decided to pay out. I wasn't going to stop working on it at that point. It took 2 weeks to even get the data in the first place. I PM'ed you with updates during the entire time. Either you didn't see them or chose to ignore them.
@_date: 2015-08-18 16:34:35
They didn't honor the bounty though
@_date: 2015-08-18 17:21:56
Thanks Greg
@_date: 2015-08-18 22:56:22
No. The goal of the analysis wasn't to identify sock accounts, but to show we are justified in believing that sock accounts are active in the debate. There's a subtle difference
@_date: 2015-08-18 22:59:04
The guy who originally did the bounty paid out while I was still working on it and didn't give me fair notice. 
I also tweeted at the Bitgo guy but never heard back.
@_date: 2015-07-19 14:51:25
No, I don't think so. That would mean that the ratio of Socks to NoSocks is 1 (an even split). Socks are incredibly rare, so reducing the False positive rate to at most 50% is a vast improvement as mentioned
@_date: 2015-07-19 04:25:34
I have the comments from deleted accounts in the database I posted. I noticed the accounts were deleted I identified them as probable socks. 
It's possible that some of these accounts were deleted after noticing the bounty, since bounty response made it to the front page.
I don't have a way to check which users deleted their posts. That data isn't available from the api
@_date: 2015-07-19 04:49:36
Read the full report. It mentions that the results will have a high (but reasonable) false positive rate. I chose the model that (imo) gave the most useful [precisicion and recall tradeoff](
@_date: 2015-08-18 13:12:36
All claims of psuedoscience were addressed [here](
@_date: 2015-07-19 15:40:35
lol! that's amazing! 
To your first point, just like its difficult to  prove you're not the same person as the others, it's even more difficult to prove a pair of users are socks - even if they made the list. 
" I encourage anyone who is curious to take a close look at all the posting histories"
@_date: 2015-07-20 17:28:35
The 85% claim and other performance measures refer to multi-account use, not sock puppets. The model is agnostic to the existence of socks, since that's not what its predicting.
To get the sock list, I made at least two assumptions about the results of the model, and I would agree that those assumptions might be questionable.
@_date: 2015-07-20 11:53:02
Yes and no. I used 100 topics as a reasonable cutoff. To use more would have been computationally expensive. 
The model is working with pairwise comparisons between all users on r/bitcoin. For the 3232 profiles in the training set, that Ives over 5 million comparisons. And each comparison between user profiles is computing similarity on a high dimension of features, including the 100 topic vectors mentioned. So given this, even doubling the number of topics comes at a significant computation cost. Scope had to be limited.
While more topics = better sensitivity to opposing views over all comparisons, it also means that the model will take several days to run instead of one day.
@_date: 2015-07-20 15:57:07


@_date: 2015-07-19 19:57:54
I shared the database on dropbox
@_date: 2015-07-19 14:38:38
Interesting, I didnt know the conclusion that Nick = Satoshi was reached using nlp.
@ your last comment: this analysis uses both writing style and content matching. Users are judged as similar if that have topic similarity (similar content) and syntactic similarity (similar writing style)
@_date: 2015-07-19 15:56:04


What I meant by that was "given our goal of building a useful classifier than can predict the rare cases of multiaccount use". The bias induced from tuning the sample ratio was reasonable - it is done in practice all the time to handle the [Class Imbalance Problem]( In fact, any sampling method that isnt uniform random sampling will induce some bias. Balancing the classes to 50% each is biased, in that it is an arbitrary selection of proportion. To train the model unbiasedly would mean that only 0.03% of the training cases would have {Label = 1}....this is just as bad as predicting False all the time. And remember, this was for training multiaccount detection - not sock detection
@_date: 2015-07-19 05:56:12
Ah, that's not a probability. Appendix 1 describes how those numbers are generated. 
I agree with you about Satoshi. It would be an interesting problem to solve, but no good can come of it  going by what happened to Dorian Nakamoto
@_date: 2015-07-19 15:46:16
Yes, you're right. 
There is a control sample for identifying multiple accounts. There isn't one for socks, since we do not know with full certainty who the socks are in the first place. 
@_date: 2015-07-20 13:09:11
Yeah you're right - it is too short. To be fair, I wanted to do this in less than a month and to make it longer with all the background knowledge you mentioned is literally a thesis. I even discovered that had paid out the bounty while I was writing the report, so clearly his expectations weren't for a 50 page thesis, and more for a quick analysis. I think there is enough detail in the report for someone who has a background in the field to reproduce all of the work. And for someone who doesn't have much background in it to at least get an intuition for some of the techniques used.
Yes, you're also right that I could have given an example of the author discrimination at work. I agree that would have been useful and would have gave a better discussion of the features and feature importance than what I gave. 
I disagree that my conclusion and summary depend on the Appendix though: the main report has nothing to do with detecting sockpuppets, just detecting multiple accounts. The Appendix is the main result as far as socks are concerned, but as I stated in the introduction: the assumptions required to detect socks with unsupervised machine learning, are too numerous and too subjective for me to feel comfortable including it in the main report. That's why the list will always be a starting point to look for socks, and nothing more. 
Agreed, 5e6 pairs aren't that much. Each pair has a large set of features including dense vectors of 100 dimensions, sparse vectors of 40,000 dimensions, and a other similarly large features (14 in total). Now, this isn't really the issue, because computationally, those are still relatively small. The complexity of the similarity computations was the problem. With each pair of comparisons, similarity had to be scored for each set of the 14 features. Some of the functions judging similarity were not trivial -  like Hellinger distance and the several Welch t-tests. So each comparison had high complexity. You're right that it was parallelizable, and I did do that! I used python multiprocessing for the relevant parts and churned away at it on an 8 core server for over a day! Otherwise it would have taken weeks. But again, the goal was to get the analysis done as quickly as possible and submit to and Mike Belshe. 


where do you see that in the report? 
The part I'm reading in the appendix says "Furthermore, since the classifier was shown to recover
at most 25% of all relevant instances of **multi-account use**,". Emphasis placed on "multi-account use". As you alluded to, we simply don't know who the real socks are. That's why the focus of the main analysis was to detect multiaccount use - we can learn some patterns correlated with that in an objective way. The value of 25% is known from the test data that the model was tested on - the induced cases of multiaccount use were known for this data.
As far as your personal comments go, I think they are spot on for the most part. In fact, I don't agree with many of the assumptions either - at least for detecting sockpuppets. However, the stated that the bounty will look at comments before June 10th. A lot of the low-hanging fruit you mention simply wasn't accessible since the analysis looked at retrospective comments, and not changes in the states of current comments. To be clearer, the analysis would need to look at active threads and measure how their state changes, to access better indicators of sockpuppetry like the common down-vote brigades you mentioned. I agree that monitoring active threads would be a much simpler and better way to detect socks. After an interesting exchange with I think I've found a feasible set of heuristics to write a quick reddit bot to do this. But again, to be clear, the premise of the original bounty was to use the historical data up to June 10. Therefore, a lot of information relevant to detecting socks was obfuscated.
What you said about the first assumption is spot on. A lot of users who commented on the list I posted, said things like ["that my coworker"]( or ["that person talks like me to an eerie degree of similarity"]( etc. This supports the view that the features used may have been adequate for detecting multiaccount use...detecting socks is another problem altogether. You're also right that on this subreddit, people will tend to talk similarly to the trend-setters - that's an interesting observation and I'm inclined to agree. To defend the techniques used a bit: some of the features of the model can capture more nuances in style than  just similar words and similar views. In particular, there is a feature of character 4gram sequences that should be sensitive to patterns/mistakes that would be common but unique to a user.
To your second comment about the assumptions: yeah, I agree. Socks can go out of their way to subvert the analysis for sure. I think "time" is another one that can be subverted though, or at very least made obscure. Someone like me who is typically working from home and awake a lot, would have 12-20 hours in the day to spread sockpuppetry over. I wouldn't be hard to use a few puppets effortlessly over a 12 hour period. Also, I suspect their posting time would more be a function of when threads trend the most or are most active, than an indication of the users schedule. But that's just a guess. In any case, everything I just mentioned implies (to me) that time is more useful for niche cases of sockpuppetry - those that follow from the assumption that sockpuppets are lazy. I don't think that would have been a good assumption to premise my analysis on. **But**, I agree it may have been a reasonable assumption to refine the final list...possibly with another score that would affect the ranking. 
Your third point is fair, but for classification it would have been a problem since accounts with less than 10 comments will have tons of missing features and will be ambiguous/non-informative.
Anyways, thanks for taking the time with this response! Good points overall.
@_date: 2015-07-20 08:10:13
If you have any criticisms about anything in the report I'd be happy to answer them.
@_date: 2015-07-20 17:32:39
Or am I misunderstanding your point?
@_date: 2015-07-19 07:15:31
You're right, but I do not stake my reputation as a "scientist" on the title and Reddit post. I would however stake it on the report :)
@_date: 2015-07-20 07:33:58
Yeah that's what it's doing, but it was trained on complete labeled data. I used users in a training set to generate two profiles per user. When those two profiles are compared they are labelled one. All other comparisons between users who aren't the same are labeled 0. The test set is generated in the same way, and from users not used in the training set. The results posted are predictions on a third set of data that was left 'unseen'
Correct me if I'm wrong, but I think you're suggesting that in training I wouldn't know a priori userA and userB are not socks, and therefore I don't know if the label for their profiles should be 0. If I understood that correctly, then yes, you're right, but this is the best we can do. If there are a lot of training cases where userA and userB are different user names but actually the same user, then that's not something we can control for. Furthermore, it would increase the False negative rate, since it's training the classifier to predict Label = 0 on some cases where the Label = 1
@_date: 2015-07-19 06:53:34
Yeah, I agree. This was more of a fun challenge for me than anything else. 
From reading the debate threads, it was pretty clear to me that at least a handful of users were sockpuppets. The "fun challenge" was to confirm objectively (or without my subjective bias) that they exists.
@_date: 2015-07-19 07:12:36
Exactly :)
@_date: 2015-07-19 14:42:51
Yes. The report states that judging socks will always be 'hand waivy', but, that "**If** we can reasonably say that examples 5.2.2 and 5.2.3 represent probable cases of sock manipulation, then it is likely that there are more cases in the suspect pairs from classification."
My claim of "manipulation confirmed" here is premised on that "If", and is independent of the conclusions of the report. The main analysis does not try to find socks, since as you know there is no easy and systematic way to do this with ML
@_date: 2015-07-19 06:48:49
This is true..
@_date: 2015-07-19 14:53:51
No. is right
@_date: 2015-07-19 06:55:00
Yeah thats fair. I was more careful with my language in the report than I was in this post. Many didnt read the report tho...
@_date: 2015-07-19 15:19:45
That's interesting
@_date: 2015-07-19 17:20:21
Yeah, Appendix 1 explains the score.
I'm surprised that you were both matched in that case. Usually the matching works for users who talk about the same things (topics) and in kind of the same way (syntax). The only thing I can think of is that you both talked about the same things, but the model wasn't sensitive enough to realize you had opposing views about those things.
@_date: 2015-07-21 07:21:44


Yes, I have investigated pairings it found and found them to be correct. Not sure if you read the report, but it clearly states that there are three datasets: 1) training data 2) test data (different from training and w/ distinct users from training) 3) investigation data (different data from both training and test). The training data was used to train the classifier. It has **known** cases of multiple accounts (these were induced). Similarly for the test data: they were **known** cases of multiple accounts that were induced. This is where the 85% comes from - the known cases. The model was used on the investigation data to predict cases of multiple account use - the cases were not known for this data. However, since the 85% represents how well the trained model generalizes to the test data of **distinct users**, it is fair to think that the trained model will generalize to the investigation data with a similar prediction accuracy. This is standard procedure in statistics and machine learning.


I'm not falling prey to base rate fallacy. I did not state the false negative rate because it wasnt of interest, and it can be infered from the precision and recall scores that are stated in the report. Here's how the data was predicted on a balanced dataset of (50% no multi-account use, 50% multiaccount use; note: this is not the true proportion):
Predicted:           0      ,     1
 
array([[552, 106],    0 (Actual)
 [113, 552]])     1 (Actual)
You can see the false positive rate there is clearly non-zero. How about on the dataset with the true proportions of multiaccount use to no multiaccount use? 
 Predicted:        0        ,       1
array([[762822, 143622],     0   (Actual)
[   113,    552]])         1  (Actual)
Whoa! The number of cases that are actually multiaccounts = 133+522 =  665, but the classifier predicts 143622+552 = 144174 of them as being 1. That's a huge False positive rate. Not useful at all if I had to scan the list of the 144174 pairs that it predicted as 1 for sock manipulation. 
Note well: The classifier still get's 84% accuracy, and this goes to show that judging prediction accuracy on the data with the true proportions is useless (as stated in the report). I could predict 0 all the time and still get a good prediction accuracy. That's why precision and recall are more useful measures in this case.
Choosing, or rather **tuning** the False positive rate to 50% was done on purpose. So that when scanning a list for socks/multiple accounts, you can simply say "at least half of these are wrong", instead of "at least most of these are wrong...and there are a lot of them to look through!". 
Lastly, the stuff about choosing pairs of users that commented in the same thread is misleading the way you stated it. This has nothing to do with the model of multiple account use. It was done after getting results from the multi-account model, as a way to rank the listed pairs. So the ranked list scores how often a pair of users commented in the same thread **conditional on** them being returned as a case of multi-account use from the model with a 50% false positive rate.
I hope this is clear.
@_date: 2015-07-21 07:30:42
Also, 


You're falling prey to the [association fallacy](
@_date: 2015-07-19 19:42:13
Thanks! The next stage for this would be to use an online algorithm to monitor threads for socks. A lot of information is obfuscated in looking at comment histories. By monitoring threads constantly, we can get onlineproof of socks :). I think the results of that would be more reliable
@_date: 2015-07-19 17:17:39
I'll check that out. Random Forest is typically the first classifier I try - since its proven not to overfit
@_date: 2015-07-19 05:48:41
That's what it **suggests**
@_date: 2015-07-19 15:35:49
Hey! I see you in a bunch of my subreddits alot (r/machinelearning etc.) :)
Skim through the report if you have 10 mins, everything is explained there. But to summarize it: I induced a ground truth by taking comments from known users and splitting them into groups, as if they were different users. 85% accuracy was based on a 50% threshold for a Random Forest classifier with 100 estimators.
I used a different threshold to get the results in the lists I posted. The results with the 85% accuracy had many more False positives than the lists I provided but they recovered many more of the relevant cases of multiaccount use...a standard precision and recall tradeoff
@_date: 2015-07-21 01:47:56
The goal of the bounty was to show evidence of sockpuppets. As to why you should care, well, that's up to you.
@_date: 2015-07-21 04:36:51
How is it random noise? What you fail to understand is that cases of multi-account use made up less than 1% of the dataset. My model predicts those cases with 85% accuracy. How's that random, or noise? Even 50% False positive rate is a 50x improvement over pure guessing, given the proportion of the two cases. Thats certainly not random nor noise
@_date: 2015-07-19 15:43:17
Those scores in the lists aren't probabilities, just scores to help rank the suspects. I should be more clear about that in the list. 
btw, who were the 5 matches?
@_date: 2015-07-19 15:21:06
@_date: 2015-07-20 13:29:59
Haha, thanks!
@_date: 2015-07-19 15:26:29
Yeah my title and original post are somewhat misleading. This was intentional to create enough interest for people to at least skim through the report (its actually a quick read!). In any case, as many people wouldnt have read the original post if I went into detail about judging prediction accuracy, why it's a useless measure, motivation for precicion and recall, reminders that the results are not certainly suggestive of socks, reminders of all the assumptions etc etc etc.
See what I mean?
by the way: I read your analysis but it got buried because it didnt click bait :)
@_date: 2015-07-19 06:58:58
That's interesting. Thanks for confirming
@_date: 2015-07-19 06:02:05
I try to keep a low profile but you can find out a bit about me on my website - old resume included. 
In any case, the analysis should be 100% reproducible, but computationally expensive. Full details are in the report and code
@_date: 2015-07-19 04:55:16
Thanks! Detecting socks will always be somewhat questionable - as I mentioned in the report. The challenge was to make assumptions that were useful, but not too biased or subjective.
@_date: 2015-07-19 05:27:08
Thanks. I hope it's a good read!
@_date: 2015-07-19 07:11:38
@_date: 2015-07-19 06:57:12
"It's especially egregious since he doesn't have any data of a known incidence of sockpuppetry (or known non-sockpuppetry) to control against."  
Yeah thats true, and I point that out in the report.
"he can fudge thresholds to reach any conclusion he wants."
No.. and this is why the list has so many False positives. 
@_date: 2015-08-18 19:20:17
The company was Bitgo. Neither of them paid
@_date: 2015-07-19 15:30:53
I didn't pass any of those "messages" in my report. I encourage you to at least skim through it before making bold claims about my intents. The title of this post and the original post itself were a summary of the results to encourage people to read further to see what the **actually** suggest. Many people clearly did not do this.
@_date: 2015-07-21 07:48:32
Thanks for understanding.
@_date: 2015-07-19 07:09:44
I'm not sure what you mean by taking the quoted comment replies into account. I removed all quoted replies from the comments, so they only contain what you said, and not what you quoted from the other person
@_date: 2015-07-19 17:21:02
Thanks bro! Represent!!
@_date: 2015-07-19 17:57:02
Because that's just one case. You would need many more to build a plausible model
@_date: 2015-07-19 15:17:40
Yes and no - the techniques are fairly well established but even the researchers that use them are cautious when interpreting the results. 
In spite of all the False positives, the model is still useful if seen in the right way
@_date: 2015-07-21 04:39:00
Also, I didn't claim to identify socks. I claimed to identify multiple account use. They are not the same thing. I showed evidence that suggests we are justified in believing there are socks. That is not the same thing as identifying socks. Come again
@_date: 2015-07-21 07:53:59
@_date: 2015-07-19 21:33:13
I know it's 50% because I had data that I knew the "truth" for. This was for multiple accounts however, not socks.
@_date: 2015-07-19 06:50:36
I didnt look to see what sides they users in the list were biased to. Just to be clear, the list is more of a starting point to look for socks. I am **not** claiming that the mentioned users are actually socks
@_date: 2015-07-19 05:09:35
pretty much?
@_date: 2015-07-19 05:23:29
are you sure you dont sleep-comment?
@_date: 2015-07-21 16:51:31


has posted his own analysis...no data, just results and code. Self-proclaims he has "confirmed" sockpuppets but nearly everyone who was claimed to be one proves they aren't. Also he's weeks too late.
You are a class act! I posted a database on my dropbox. 
**And your message to me is another example of how classy you are.**








  &gt;&gt;This is exactly why I did no analysis part, but provided a very good set of data. My data even avoids your "but since thread made it to the top one day many people might have deleted their posts" argument. Thanks for not mentioning myself or 's contributions.








Now, to stoop to your shitty level as a person, briefly: the fact to the matter is 1) you did shit all for the bounty 2) got paid for your shitty code and 'research' 3) Message me "fuck me" because you're a shitty person. 4) Rub it in my face that I didn't get paid and your undeserving ass did...how long did you spend on this again? what report did you do? what did it show?
Stay classy.
@_date: 2015-07-19 17:53:58
Yes, that's right
@_date: 2015-07-21 02:14:37
I'm trying to figure out if you have actual criticisms about the methods used, or if you just don't understand the point of it all (bounty).
@_date: 2015-07-19 20:30:53
Check the mirror in the OP. I'll reboot the host
@_date: 2015-07-19 07:37:38
I believe you :)
@_date: 2015-07-19 05:06:59
Thanks yo
@_date: 2015-07-19 16:55:29
To your point about the summary being inadequate: yes, it is. but see my response here 
I don't think you'd get much out of tweaking to model to the opposite results. The results wont differ much from what I've posted..
@_date: 2015-07-19 08:52:03
Hmm, using both would be better
@_date: 2015-07-19 06:23:14
Let me be clear: I'm not accusing anyone. I'm just stating the results based on the assumptions of the model. I know that there are a lot of false positives. However, it would have been dishonest to alter my results to remove them, and still claim to use objective methods. 
I see the list as a good place to start looking for socks without introducing too much subjective bias.
@_date: 2015-07-19 06:41:32
I was more careful with my language in the report than I was in this post. The 85% accuracy was for predicting multiple account use, but to jump from that to predicting sock puppets needed a few assumptions. I assumed that potential socks would 1) have been classified as the same user and 2) have a high thread overlap i.e. comment in a lot of the same threads.
Now here is where I was careless in this post: the model that gave 85% accuracy on predicting multiple account use, was not the final model that was used to answer assumption 1) as stated above. The model  used for assumption 1) was reasonable at recovering as many cases of multiple account use as possible without trading off too much on the False positives. In this case the False positive rate was around 50% **but** the model was able to recover up to 25% of the cases of multiple account use.  So those results - with 50% of them being false positives - were carried foward to answer assumption 2. 
All of that being said to say this: **At least 50% of the results will be False positives**. The goal was to find a good starting point to look for socks, without being subjective about is judged to be a sock or not.
**The results are not stated as "these are definitely socks". But, "given the assumptions, the model thinks these are socks**. I did not filter out those results I thought/knew were False positives because that would have been dishonest and introduced subjective biases.
"what's the number for the probability that it would give you the results you're seeing without widespread manipulation "
Thats a good question. I'm not sure how to answer that without first knowing **for certain** which comments are cases of manipulation
@_date: 2015-07-19 17:34:24
Ah. No severe retaliation I hope :)
@_date: 2015-07-21 01:12:56
**The point of the analysis was to show evidence of sock puppets.** If any one of the approximately 300 pairs on that list is a sock puppet, then we are justified in believing there are more. That's because the list represents at most 25% of the cases the model thinks are multiple accounts. 
@_date: 2015-07-19 08:49:40
I know you're not him because you didn't cuss at me :)
@_date: 2015-07-20 11:54:40
Yes, thats very basic, and my model does all of that as well. All of what you mentioned and more are features in the model
@_date: 2015-07-19 08:30:01
You would be in the database if you're not in the list
@_date: 2015-07-19 07:33:06
Yeah there are definitely a high number of False positives. The report discusses why in more detail. 
Just to be clear, I am not making any definite conclusions from the results, just stating what the model says. The list is simply a starting point to look for socks. 
@_date: 2015-07-19 14:48:11
Yeah, you get it. The main analysis did not confirm socks, only apparent instances of multi-account use under certain assumptions. My sock confirmation came from using what I thought to be a few reasonable assumptions about the results of the main analysis - this part was not scientific at all, and thats why it was left out of the main analysis and put in the Appendix. The numbers in the rankings are **not probabilities** but just a score to induce and ordering on the results from the main analysis.
I could have been clearer about this on this submission, but I did not want to post a wall of text explaining every little detail. That's what the report is for :)
@_date: 2015-07-19 19:15:09
Thanks for the link. I didnt know about that project. 
I use a technique called Latent Semantic Analysis. Some of the results from that are here 
@_date: 2015-07-19 05:57:55
It's still working for me
@_date: 2015-07-19 08:00:27
Not sure if you read the report but a few different models were trained. In the end, the "best" model was not judged on prediction accuracy for reasons mentioned in the report.
I agree that identifying socks beforehand and using it to train a classifier would have been useful. However, actual known cases of socks are hard to come by, and they cover their tracks fairly well. I would be training on a handful of cases which might not even generalize well!
@_date: 2015-07-19 08:34:30
That would explain why!
50% False positives isn't noise. Actual cases of multi account use made up less than 1% of the data. So 50% False positives suggests that many of those 1% cases are being classified as relevant, but some of the remaining 99% as well.
@_date: 2015-07-19 15:20:15
Exactly :)
@_date: 2015-07-19 06:49:01
Updated: 
@_date: 2015-07-19 17:50:19
@_date: 2015-07-19 17:55:07
Not just that but high topic similarity and syntactic similarity as well
@_date: 2015-07-19 05:01:57
I have a baseline set of comments for multi-accont use. The only way to get a baseline set of comments for sock puppets is to hand-label them myself....but this would introduce subjective bias
@_date: 2015-07-19 15:04:16
The bounty stated that the cutoff date should be 10 June 2015. So anything before that. I used June 2014 - June 2015 data to investigate socks, and the remaining data to build and test the models
@_date: 2015-07-19 15:13:40
Yeah, original post might have been phrased that way but if you skim through the report you will see that it makes no compromising assumptions to try to reach a specific result. 
Furthermore, the results are kept unbiased by highlighting that there are many False positives, and not refining the lists to remove the more obvious ones. 
@_date: 2015-07-19 18:00:10
Sorry la0ban! Appendix 1 in my report singles you out a bit. No offense intended, I was just trying to interpret the results I got from the model.
@_date: 2015-07-19 05:04:08
Spot on :)
@_date: 2015-07-19 16:56:48
"All models are wrong, but some are useful" - George Box
@_date: 2015-07-20 05:40:38
If you have any substantive claims to make against the report then I'd be happy to hear them.
@_date: 2015-07-19 15:00:45
Yes, it could have (and probably should have) been phrased that way in the conclusions of Appendix 1, although the way I phrased it isn't much different: 
"If we can reasonably say that examples 5.2.2 and 5.2.3 represent probable cases of sock manipulation, then it is likely that there are more cases in the suspect pairs from classification."
Is the glass half empty or half full :)
@_date: 2015-07-19 04:46:37
The details are in the full report so I wont go into them too much here. I used some of the data to generate a training set that has labeled instances of multiaccount use, and used that to train a classifier.
@_date: 2015-07-19 08:03:03
That's a great point! I'm inclined to agree.
Anyways, I'm glad someone understood the goal of the analysis :)
@_date: 2015-07-19 05:25:33
Yeah that could be done with a bit more natural language processing. The goal of the analysis was simply to show evidence of sockpuppeting. I suspect most of them will be pro blocksize increase...
@_date: 2015-07-19 06:06:50
Yeah for sure. The model only 'works' assuming that sockpuppets will have the same writing tendencies across accounts. An attentive sockpuppeter could easily get around this by trying to write like someone else
@_date: 2016-09-06 11:31:29
Yeah exactly. Given how long it takes them to respond to support tickets, I dont know if my funds will be locked up for a few days or a few weeks!
@_date: 2016-09-06 11:20:20
same happened to me today. I am seriously considering switching services 
@_date: 2015-02-21 20:11:26
It would earn bitcoin by day trading
@_date: 2015-02-03 17:21:59
don't tip me bro
@_date: 2015-02-14 07:37:00
Circle works just fine