@_author: ajtowns
@_date: 2015-12-24 07:35:28
Out of the last 2.5M transactions (between blocks 387721 and 389669, so 11th-22nd Dec) 41% paid a fee of 9999 satoshi, and 26.3% paid a fee of 10000 satoshi, independent of transaction size. These transactions paid a median of over 44000 satoshi/kB, with extremes of just 222 satoshi/kB and 103092 satoshi/kB.
That doesn't indicate a market, it indicates a hardcoded default that most people just haven't cared enough to change.
If there were a fee market in place (or, more precisely, if users/wallets calculated fees based on observed demand), the majority of users would be paying closer to recommended fees of 10,000 to 25,000 satoshi/kB, which is a 70% discount or more for 50% of transactions right off the bat (and I think estimates would reduce as actual fees went down too, so it would compound to some extent).
@_date: 2015-12-22 13:47:21
a transaction that uses only segregated witness inputs isn't malleable (scriptsig is required to be empty, the signatures become invalid if any other elements of the transaction is changed, so the txid is fixed). so a soft-forked segwit fixes tx malleability.
@_date: 2015-12-22 17:26:05
Why a fee market at this stage? Because people are paying too much in fees -- most transactions seem to be paying system defaults of 10k satoshi per transaction (over 4c!) when they could easily be paying something more like 6k satoshi per transaction (almost half price), and probably less.
Without wallets that support a fee market, it's easy and cheap to fill up blocks; just pay slightly more than the defaults, and have some well-aged coins to fill up priority space too. For 1MB and 4c per 200B transaction, that's a bit over $200/block or around $30k/day -- for which you can monopolise the output of the $1.5M/day in subsidy being spent on mining power.
Deciding fees by hardcoding them in the software is broken. Deciding on them by market mechanisms lets it be cheap most of the time, only becoming expensive when it's being attacked, and, at least for people who value their transactions more than spammers value their spam, lets you avoid being inconvenienced by spammers.
(It also makes it easy for fees to stay stable in real terms when the price of bitcoin goes up; and it prepares the way for bitcoins long term viability after the next couple of subsidy halvings make fees important)
@_date: 2015-12-20 04:36:19
The scriptPubKey isn't OP_TRUE, it's a PUSH of the sha of the witness.
@_date: 2015-12-22 04:42:39


Increasing is a tradeoff -- handling more transactions is good, increasing the risk of centralisation or outright failure is bad. Everyone wants the good side -- even if it's not necessary -- the argument is about how bad the bad side is. It's not a failure to find and support a new tradeoff that manages the same good with less bad.


segwit design minimises three of the risks of increasing blocksize (it's a small increase, and it doesn't increase sigop limits or worst-case UTXO rate of increase). segwit also has other scaling benefits (fraud proofs might let more people do more verification of the block chain at reduced cost compared to running a full node; making it possible to do any sort of improvement of the scripting language via a soft fork makes things like reducing bandwidth via signing-key recovery possible; malleability fixes makes wallets more accurate, lightning more efficient, and makes other use cases for bitcoin possible).
And meanwhile IBLT, weak blocks, secp256k1, all reduce the risks of blocksize increases, both the ones that have already happened (raising the 250k soft limits up to 1MB hard the limit), and ones in the future (the segwit effective block size increase or actual block size increases).




Segwit has a host of benefits, and is worth doing hard and fast even without any blocksize increase: efficient fraud proofs get back to Satoshi's original vision for SPV clients, malleability fixes make it easier for people to find their transactions reliably, scripting upgrades allow safely reintroducing at least some of Satoshi's original opcodes that were disabled.
Since it can be implemented by a soft-fork, and moves a bunch of data out of the base block and into a separate witness, it's also an easy opportunity to get an effective, if limited, blocksize increase, by simply not apply the existing limit to the witness (or, as is proposed, by giving witness data a steep discount). Personally, I don't see any reason not to take the easy increase.




When it's possible, a (safe) soft-fork is much better than a hard-fork. For full nodes, it's far better to be "degraded" via a soft-fork, than disabled by a hard-fork. For anyone who upgrades in advance, and for SPV nodes, there's no difference.


Gavin has posted in support of most of these ideas already:
* segwit: 
* IBLT:  
* weak blocks: 
* libsecp256k1: 
so I don't think working on those ideas is leaving him in the dust in any way.
Jeff's primary conclusion in the thread he started on segwit was "Bump + SW should proceed in parallel, independent tracks, as orthogonal issues." Personally I agree with that conclusion, and don't feel like I'm being "left in the dust".


I don't like the r/bitcoin moderation policy, but it's still my preferred bitcoin subreddit. I'm pretty happy with the bitcoin-dev list moderation policy and implementation, though I think it would be nice if bitcoin-discuss saw more traffic.




If you decide to do things based on what people will think of you, you might get popularity, but you won't truly get respect. AFAICS, the best you can do is what you think is right, which might at least earn you some self-respect. Alternatively: losing the respect of people you don't respect isn't much of a loss.
@_date: 2015-12-04 01:00:46
 has some draft code for soft forking it in as I understand it. It's a pretty invasive soft-fork though in that every tx using segwit will just look like an "anyone can spend" to nodes that don't support segwit. But I think it can still be rolled out so that people who care about segwit can use it, and everyone else can just ignore it.
@_date: 2015-12-22 13:15:15


Number of sigops scales linearly with blocksize, and total bytes-hashed for signatures potentially scales quadratically with blocksize. Both those provide some opportunity for miners to apply a denial-of-service attack against their competitors, which risks giving them monopoly powers over bitcoin. Segwit doesn't increase either of those.


I think there are further improvements to be made after and concurrently with segwit. Segwit's "accounting trick" is the best near term approach, though I also think it will be replaced long term.




Block propogation between miners is addressed by the relay network (already in place), and IBLT and weak blocks (included in the plan cited above). By constrast, both segwit and just increasing the block size make block propogation harder.




It is a clean design, and the discounts do give an effective blocksize increase. The approach making it a soft fork rather than a hard fork is surprisingly clean as well, in my opinion.


Hard-fork versus soft-fork is a fair opinion to have -- the code (or rather, the data structures) would be marginally cleaner that way. But doing it as a hard fork would significantly delay it, because it would add all the work to ensure everyone has upgraded before it could be activated. 
If there's independent work on a hard fork blocksize increase as well as segwit over the next six months, doing segwit as a hard fork would also likely mean tying the two changes together to avoid having two hard forks shortly after each other, which in turn means that delays in segwit would force delays in the blocksize increase.
Add those up, and I don't see how doing it as a hard fork makes sense.
Gavin's blog post seemed approving of using segregated witness data as a means of packing more transactions in a "block", but maybe he was more skeptical somewhere else?


Sure, I cringed when I watched it on the livestream, and much the same concern was being raised on irc during the talk. The questioner seemed pretty on-point -- expanding the effective blocksize via segwit only works if you're comfortable with ~4MB transmission sizes, and it's not 100% clear that is okay with existing technology. The difference between expanding the blocksize via segwit vs just changing the overall limit, is that it's only transmission size that you have to worry, without also upping the worst-case limits on other aspects as well (UTXO bloat, bytes hashed for signatures, number of signatures). It would have been good to have had that actually explained from the podium. But then, there were other talks on all those things anyway?
@_date: 2015-12-16 11:39:59
(I don't think it's good enough in general to just get your block to 8 peers, you need it to get to all other miners before you fully eliminate your risk of being orphaned, and that *could* increase it be a factor of four or so. I'm not sure how that would work out in practice, and there's already that much fudge factor in the choice of 8Mbps of uplink though)
(From what I can see, 33400 satoshi per kB is pretty much what the average fee is at the moment?  )


You don't just lose the block reward if you get orphaned, you lose the fees you were collecting too.
So if the average fee of the *n* transactions you've included is *f*, you're losing *25+nf* bitcoins, with (additional) probability *p=4/600k*. So your new transaction, paying fee *x* will want to be greater than *p(25+nf)*. But *f* is greater than *x* too (assuming you include transactions in fee order, and ignoring different size transactions), so *f* is greater than *p(25+nf)*, and rearranging gives *f* greater than *25p/(1-np)*.
Maybe more interesting, it also gives *n* less than *1/p-25/f*. Plugging in p=0.004/600, and f=17500 satoshi (current prices from statoshi.info), n has to be less than 7143, which means blocks should be under about 3.5MB. More fees would justify bigger blocks, but the relationship isn't linear -- small increases in (average) fees rapidly increase the number of transactions that are worth including under this framework, up to a maximum of 150k transactions (75MB blocks) -- transmitting a 75MB block at 8Mbps takes 1m15 though, and doing it to 8 peers takes 10 minutes, so the analysis has probably already broken down at that point...
I think weak blocks improves things by a factor proportional to how much easier it is to find a weak block -- ie, if you've got a 15/16 chance of finding a weak block that includes the additional transaction before the real block, you've only got about 1/16th the chance of your block being orphaned due to the extra transaction that you would have had without weak blocks.
I'm not sure if you can reason about IBLT in this framework. In that case adding a transaction only increases orphan risk if the tx isn't in the other miner's mempool, ie it's mostly due to factors that aren't included in this analysis.
@_date: 2015-12-24 09:02:22
If there's no backlog and miners are happy to increase blocks, I'd expect fees to be pretty close to 0, or at least some constant value in satoshi/kB reflecting miners' marginal cost for increasing the size of the block. Probably something like 5000 satoshi/kB if it takes about an additional second to transmit and verify a kB worth of transactions.
If the fee estimator were doing its job, fees would either vary or be roughly constant in cost/size which is how miners choose which transactions to include in blocks; instead they're roughly constant in cost, despite varying wildly in size. In short: most transactions are paying *far* more than the bare minimum necessary.
@_date: 2016-03-08 05:01:51
Yeah, but you'll note I took the easy way out there by just using "p" rather than trying to work out how *that* varied with the blocksize. :)
@_date: 2016-01-26 14:25:03
the listed CVE-2013-2292 describes a more maliciously designed 1MB transaction that (at least at the time) took about 3 minutes to validate; multiplying by four gives the "ten minutes" figure.  
@_date: 2016-03-08 05:00:44
That only works for small time intervals (h approaches 0 in the ch18 link) -- as the interval increases the approximation becomes more and more wrong. The relevant interval in this case is the entire time it takes to broadcast the block, and that increases as the block size increases, potentially significantly. (Conversely, if the time to broadcast a block is never signficant, then the orphan risk is pretty insignificant too, and thus not a constraint on the block size)
@_date: 2016-03-05 03:35:50


It doesn't scale linearly: imagine if every block you mine took ten minutes to propogate -- even then your orphan rate wouldn't be 100% (600/600), because every now and then, the entire rest of the hashrate wouldn't be able to find a block for half an hour or more.




A block is orphaned when someone else builds the next block on someone else's block rather than yours. That happens when they saw someone else's block before they saw yours. The likelihood of that is (roughly) the likelihood of someone finding a block in the time it took you to broadcast your transaction; which is "nys/b". Since it's a poisson process, we use the exponential distribution to calculate the probability, which gives a probability of (1-exp(-nys/b/600)). If you're comparing two different blocks you might mine, one with n transactions, and one with n+1 transactions, then the additional orphan risk you're taking on is the difference of those two probabilities:
    (1-exp(-(n+1)ys/b/600)) - (1-exp(-nys/b/600))
    = exp(-nys/b/600) - exp(-(n+1)ys/b/600)
    = exp(-nys/b/600) (1 - exp(-ys/b/600))
When the time it takes to broadcast a single transaction (ys/b) is small compared to the block target (ie much less than 600), then 1-exp(-ys/b/600) approximates ys/b/600, and the whole equation equals your figure (-ys/b/600) when the block is empty, at n=0. When n&gt;0, the marginal probability is less however (the right hand (1-exp(..)) factor stays the same, but the left hand factor decreases as n increases), so the impact of orphan risk actually decreases as the blocksize increases. Here's a graph of what that looks like with ys/b chosen to match b=8Mb/s, s=220 bytes, y=8 peers: 
If you control a significant portion of the hashpower (h=0.3 for 30%, eg), you could adjust the equation by changing "600" to "600/(1-h)", ie the expected time it will takes the rest of the network to find a block. Here's what that looks like:  . This captures the fact that you don't orphan your own blocks -- note how larger miners have lower marginal orphan risk at any block size.
(At least, I think that's correct; the curves actually converge around 700 seconds, then invert, which doesn't seem right?)
Anyway, orphan risk does still provide a limit -- if you're collecting 0.45 mBTC in fees for each kB of transactions, and you have 8Mbps bandwidth to 8 peers, then adding a 0.05 mBTC/kB transaction to go from 1MB to 1.02MB would be economically irrational.
But it's not a really useful limit, because when you work it out with the same limits, it's just as happy for you to do 60MB blocks with 0.24 mBTC/kB in fees (about 14 BTC total), even though 60MB blocks would take 8 minutes to broadcast, and you'd be orphaned something like 63% of the time...
@_date: 2016-01-18 13:01:48
Part of the segwit proposal/patchset fixes this in a much better way: 
This prevents hashing from scaling quadratically (ie, double the blocksize and the time to verify a block goes up by 4x) to scaling linearly (double the blocksize, double the verification time).
@_date: 2016-03-21 06:52:01
p2sh scriptPubKey looks like "&lt;hash&gt; OP_HASH160 OP_EQUAL", which previously was spendable by anyone who could work out what the hash preimage was (ie, what the script was) -- and once a transaction revealing the script has gone into the mempool (before it's mined), that's anyone. The p2sh soft fork changed it so the script had to be executed as well, which means signatures can be required as well, making it safe.
CLTV and CSV redefine NOP codes; a NOP code would have worked fine for segwit as well, there's no real difference. Luke-Jr's realisation was just that this technique could actually be used here too, despite the introduction of a whole new bunch of data making up transactions. (In retrospect it's pretty obvious; but that's true of a lot of great realisations)
@_date: 2016-01-29 03:11:03
The limit used in segwit implies the existing limit, so its not possible to satisfy the segwit limit but not also satisfy the existing limit, which makes it a soft fork.
@_date: 2016-01-27 03:09:33
At one level, yes -- if you manually built such a block and mined it as a small solo miner, you're mostly only screwing yourself over. I'm not sure how to build this into an attack, like selfish-mining, but that's probably my lack of imagination. My guesses are along the lines of:
Any node that then tries validating the block loses 10min of CPU, and if that node is also a miner, perhaps that delays them noticing that they've mined a block that would orphan yours? Maybe that gives you time to mine another block in the meantime and thus avoid getting orphaned?
If you can construct a standard transaction that takes minutes to validate, and get someone else to mine it, then its their block that becomes more likely to be orphaned, which is an attack.
If you're not trying to profit as a bitcoin miner, but just destroy bitcoin, then spamming the network with hard to validate blocks and transactions would be a good start.
None of these are crazy worrying though, because in the event of an attack actually being performed in practice, additional limits (like Gavin's patch in the classic tree) could just be immediately rolled out as a soft-fork. Fixing the hashing method just makes it all go away though.
@_date: 2016-01-05 18:21:29
This is the sort of improvement to script that would have required a hard fork (or massively unmaintainable kludging) prior to segwit. In future it will just require bumping the version. Pretty exciting!
@_date: 2016-01-26 14:18:05
It's even better than that -- the 25s transaction was ~5500 inputs (and hence ~5500 signatures) with each signature hashing most of the transaction (so a bit under a megabyte each time), for a total of about 5GB of data to hash. If you use Linux you can time how long your computer takes to hash that much data with a command like "$ time dd if=/dev/zero bs=1M count=5000 | sha256sum" -- for me it's about 30s.
But with segwit's procedure you're only hashing each byte roughly twice, so with a 1MB transaction you're hasing up to about 2MB worth of data, rather than 5GB -- so a factor of 2000 less, which takes about 0.03s for me.
So scaling is more like: 15s/1m/4m/16m for 1M/2M/4M/8M blocks without segwit-style hashing, compared to 0.03s/0.05s/0.1s/0.2s with segwit-style hashing. ie the hashing work for an 8MB transaction reduces from 16 minutes to under a quarter of a second.
(Numbers are purely theoretical benchmarks, I haven't tested the actual hashing code)
@_date: 2016-01-27 03:41:44
I think the only reason a developer would want to do it as a hardfork is to put the merkle root for the witness commitment in the block header rather than the coinbase transaction. It's more elegant that way, but it's purely aesthetic. It's the reason Gavin referenced in his blog post:  though he also wants to combine the commitment of the transaction tree and the witness tree at the same time. I guess as a practical matter that would save a bit under 40 bytes per block.
Some people claim to prefer hard-forks over soft-forks; Mike Hearn made that argument at  . I don't think that makes much sense at a technical level (as long as soft-forks only forbid behaviour that was already non-standard, which recent changes like OP_CLTV and proposed changes like OP_CSV and segwit do), which I've argued in 
Politically, arguing that hard-forks are better than soft-forks might work as a Trump-esque "Art of the Deal" maneuver to make selling a particular hard-fork easier: "okay, so look, maybe we agree to disagree about whether all hard-forks are better than all soft-forks, but let's at least compromise and agree that this hard-fork is okay so we can all come together to make the block size great again".
@_date: 2016-01-27 05:59:02
The "1.3" figure is from hard fork advocates and it's made by taking the estimated 60% gain for p2pkh transactions (which it turns out should be 75%-80% [0]) and ignoring the 100% gain for multisig, then dividing that by 2 based on an estimate that only 50% of wallet software/transactions will use it in the next 12 months [1], [2].
[0] 
[1] 
[2] The division by 2 isn't actually right mathematically given those assumptions, it'd actually be about 38% or a 1.38 multiplier with a 60% gain and 50% of transactions. But the assumption of 50% of transactions introduces has a much larger margin of error.
@_date: 2016-01-28 07:53:58
Different sort of spamming attack :) Having blocks that are slow to verify might make it harder on other miners, but a limit would prevent that attack. With a limit in place, you'd have a second way of preventing users from getting transactions into the blockchain -- you could fill the sighash limit instead of the byte limit or the sigop limit.
But that's only useful if you're doing it to blocks you don't mine (you can just arbitrarily set the limit on blocks you mine yourself anyway, no spam needed), and I don't think that works, because then you're limited to standard transactions which are at most 100kB, so your sighash bytes per transaction are 1/100th of a full block attack, and you can fit less than 10 of them in a block if you want to make the block less than full. Still, with 1.3GB limit and 100kB transactions, you should be able to hit the proposed sighash limit with 13k sigops instead of using the full 20k sigop limit (which might double to 40k with 2MB blocks anyway?), so maybe it would still be slightly easier to "fill" blocks with spam.
@_date: 2016-03-05 02:09:30
This should read "there's no added orphan risk if *everyone else* is SPV mining", shouldn't it?
@_date: 2016-01-05 05:02:21
Hey that's pretty interesting! Any chance you could do an analysis of the transactions? (How much data is it? Maybe you could dump your mempool to dropbox for others to analyse?)
@_date: 2016-01-27 03:50:04
I originally had a link the mtgox malleability stuff in the document, but Luke-Jr pointed out that while mtgox claimed that was the reason some funds were lost, it's actually disputed. A quick google turned up  which provides an analysis demonstrating malleability wasn't happening before MtGox's press release blaming malleability came out. So if the only reason to believe MtGox lost money due to malleability rather than some other reason is because you believe what they say...
@_date: 2016-01-27 03:28:56
The changes in segwit aren't any more complicated as a soft-fork than as a hard-fork -- the only improvement a hard-fork would allow is an aesthetic improvement for devs: the witness commitment could be moved from an OP_RETURN output in the coinbase transaction into the block header. The drawbacks of segwit as a hard-fork are the same as for any hard-fork -- the entire ecosystem has to be changed before the first new block can be mined, as they have to become willing to accept blocks they would previously have rejected.
The complexity comparison isn't between segwit as a soft-fork versus segwit as a hard-fork, it's between segwit via any method, and a direct increase in the blocksize via a hard-fork. That becomes a comparison between the difficulty of getting the code changes for segwit right versus the deployment challenges for a hard-fork. To me, that's kind of an apples/oranges comparison; it's ultimately a matter of taste as to what factors worry you more, and hence which you're going to end up preferring.
In a way, I'd say the biggest risk of the code complexity is that segwit implementation might accidentally introduce a hard fork. If it is a soft-fork, no one's money can be stolen, no new attacks are possible, etc just due to the fact that all those things are already stopped by the old rules, and those old rules remain in force in a soft-fork. But if there's a bug that makes a hard-fork possible miners can be mining on the wrong chain, merchants/exchanges can miss double-spends due to watching the wrong chain, etc. (if the worst case that can happen with one approach is the best case that can happen with the other, isn't it obvious which one is preferrable?)
I thought about covering costs/risks ("Who loses?") as well. When I tried, it was hard to do that without getting into the weeds of blocksize discussion, so I figured it was better to leave that out of this post.
@_date: 2016-03-05 06:39:48


Right, so here's how the numbers work out comparing to mining n MB with a given fee/kb rate of f, to just mining an empty block. It'll take you 1000ny/b seconds to broadcast to y peers with total bandwidth b, giving orphan probability of 1-exp(-1000ny/b/600). But at the same time if you don't get orphaned, you'll make 1000nf+r return. So your expected profit is:
    exp(-1000ny/b/600)(1000nf+r)
Again assuming b=8Mb/s, y=8; by my maths, that gives peak returns at n=100kB with f=0.334mBTC/kB, at n=950kB with f=0.3377mBTC/kB, and n=12.5MB with f=0.4mBTC/kB. 
Varying the assumptions makes huge differences. With a very small increase in broadcast speed, you get a large drop in fees needed: eg, b=24Mbps, y=16, you get n=12.5MB with f=0.25mBTC/kB. Likewise, subsidy has a huge effect, eg: once subsidy halves to 12.5, the same f=0.4mBTC/kB results in n=44MB. Conversely, with very small decreases, you just get empty blocks as the "rational" result: b=8Mbps, y=8, with an average fee of f=0.3mBTC/kB says mining no transactions earns the best expected payout -- and it's been pretty rare for fees to be as high as 0.3mBTC/kB, let alone higher...
I'm still not entirely convinced these are quite the right ways of reasoning about choosing block size based solely on orphan risk, but I do think it's pretty close.
From what I can see, though, the results just aren't impressive: it's very sensitive to bandwidth to other miners (your blog post already indicates this even with the simplified probability model), incentivising centralisation in a similar way to HFT on the stock market; fees are dominated by the subsidy parameter, which is more like "central planning" versus being decided by market demand; and the resulting blocksize is *very* sensitive to other parameters, which I think would make it hard to do capacity planning.
@_date: 2016-03-21 06:04:04
It's trivial to do any soft fork as a hard fork -- technically it's the same as asking if you could implement a feature in an altcoin rather than in bitcoin. Taking something currently seen as "anyone can spend" and limiting it is just how a soft fork works; that's how pay-to-script-hash was implemented too, and it's how segwit allows scripting upgrades in future: by making a bunch of things that will still be seen as "any one can spend" and can likewise be limited later.
Segwit doesn't introduce any meaningful amount of technical debt in my opinion; there's an extra 38 bytes that need to be added to the coinbase per block for the witness commitment, that if you were designing from scratch or doing as an altcoin or a hard fork you would probably just merge with the block's merkle header.
I don't know what it would mean to "be sent" a transaction that you "can't spend". If your wallet doesn't recognise it, it won't show up at all in your balance, just as if you'd never been paid. That's no different to sending a 1-of-3 multisig payment to your address and two others; if your wallet doesn't know about it, it won't show up and you won't be able to spend it, even if technically I "sent" it, and you have the needed key.
@_date: 2017-08-31 15:37:23
 seems to describe the same thing in Dec 2013:




     
     
I thought I read a post about some guy hooking miners in series up directly to something like mains voltage (with a cap and rectifier?) just for kicks, maybe with someone joking about how they hope no one opens the fridge and causes the setup to go undervoltage and melt or something, but I can't find it again (if it ever existed).  is a similar project, but it's from this year, and much more professional than what I thought I remembered...
@_date: 2016-11-14 08:14:27
Technically, 95% of the 2016 blocks in each retargetting period. If you had two retarget periods that were 1016 blocks against; 1000 blocks for; a retarget; followed by another 1000 blocks for then 1016 blocks against, that'd be counted as two ~50% votes, rather than activating after the 1916th block voted in favour. (That's different to how it used to work, or how the 2MB BIP109 hardfork worked)
@_date: 2015-08-11 17:19:08
It's not really zero risk -- there's the risk your server gets compromised and you lose all your channel funds, but there's also the risk that your channel doesn't get used at all, in which case you don't make any profit, and instead suffer a loss of a couple of bitcoin blockchain transaction fees.
The other thing with "rich-get-richer" is, I think, that you need to have the argument Piketty made, that "r &gt; g" -- that is the return on running a lightning channel has to be greater than growth in general. Maybe that's true (and a deflationary bitcoin probably makes it truer). But maybe it's false -- maybe running a lightning node is accessible to enough people that it drives the profit you make on running a hub below "g". I think running a lightning hub will be easy enough that they'll essentially be price takers, and won't be excessively profitable. More like putting your money in short-term treasuries versus putting them in high-growth stocks.
(Once bitcoin inflation stops, if bitcoin's still popular at that point, I don't see how you avoid the return on just sitting on bitcoin (r) being greater than economic growth (g), even without the extra return running a lightning hub would involve. I'm not as smart as Satoshi though, so I'm taking it on faith that maybe things will work out anyway somehow. YMMV)
@_date: 2016-02-04 23:55:09
When you send payments via lightning you can't see it on the blockchain explorer, but you still get a receipt that you can use as proof of payment.
(Technically, the merchant will quote you a hash to make the payment to; and when you make the payment, in order to collect the funds, the merchant will send you the preimage for the hash. Since the merchant decided on the hash, and the hash is a one-way cryptographic function, the only way you can get the preimage is if the merchant gives it to you, which allows it to serve as proof-of-payment)
@_date: 2016-02-21 15:21:08
Lightning channels are basically hot-wallets by necessity -- you have to have the signing key that would let you spend all the channel's funds (well, one of the two, it's multisig) available to be used everytime you update the channel. If you can do a buffer overflow or whatever to get control over the lightning daemon, you can send one side's share of the channel's money to yourself, most easily just by routing it over lightning directly.
@_date: 2016-02-21 14:54:34
From that link:




"Capacity increases for the Bitcoin system" links to this post:
The quote you're looking for is third paragraph from the bottom.
@_date: 2016-02-21 17:28:16
That's a totally valid concern, that I think everyone involved wants to avoid having actually play out.
There's a few limitations on trying to be a super-node like this:
* you need lots of capital to fund all those lightning nodes
* you may need to spend a fair bit on hardware/bandwidth to scale; maybe a mass of decentralised people get cheaper hardware/bandwidth, because they're just using idle cycles?
* if you raise your fees, other nodes/channels might become cheaper
But then again, maybe supernodes are okay? They're just providing a service (liquidity, speed, and low fees) and earning a return on that, they're not preventing anyone else from helping out (with improves anonymity and fungibility), and maybe that's good enough?
I don't know the answer. It's a really good question though :)
@_date: 2016-02-21 14:43:28
0.3% for credit cards, 0.2% for debit cards. "commercial" rather than "consumer" (whatever that means) is still 1.3% to 1.5% though.
* 
* 
@_date: 2016-02-20 06:34:57
Higher fees don't move transactions faster fundamentally -- blocks only ever come roughly every ten minutes, whether the average fee is $0.01 or $100.
The only way it's "faster" is in comparison to other transactions; and that's what things like bitcoinfees.github.io show -- the lowest fees you should choose to get in the next couple of blocks, or the next hour's worth of blocks, and so forth.
But at least for the values I'm seeing, the "market rate" that almost everyone is paying is already pretty much the value you need to get in asap -- you can still pay less if you're not in any rush, but for whatever reason (presumably "it's a couple of cents, who cares?") most people aren't doing that.
You can obviously also pay more (and people do, coinbase pays about twice as much for simple transactions eg), but the only difference it looks like that will make is that you're paying more; it won't actually get you what you want any faster.
The difference between this and normal economics is that the supply curve isn't really increasing with price -- you can have 1MB in quantity for (almost) any price, and that's it. That makes transaction fees mostly a zero-sum game, rather than a positive-sum game (though over the long run, higher fees also means more hashpower is being used to secure the chain, so it's not entirely zero-sum).
Whether "the fee market" is a "myth" depends on what you mean by "the fee market":
* If you mean "fees are determined by market forces" (which is what I mean by it), then I think I've shown it's somewhat true for a significant minority of transactions. I think that's new information compared to Rusty's post a couple of months ago.
* If you mean "higher fees will mean more transactions", then it's certainly a myth -- bitcoin's consensus rules cap the transactions per unit time by having a fixed blocksize and a fixed average number of blocks per unit time. 2MB doesn't change that, nor does 8MB or 20MB and doubling each couple of years. Limiting transactions per block based on expected marginal cost of orphaning might, though that gives a huge incentive for centralisation, and some of the flexcap proposals might also.
* If you mean "fees are sufficient to pay for hashrate as the subsidy reduces", that's an entirely different (and IMO more important) question.
@_date: 2016-02-03 04:46:31
He does -- he's put a limit on his limits, so the limits are limited.
@_date: 2016-02-21 15:06:01
Yeah, the running out of juice is what stops lightning from scaling infinitely. It's really just this: if you're running a node solely to make a profit by forwarding other people's transactions, then if you're taking 0.5% of every transaction, and everyone else has put, say $1000 on their sides of the channels, you'll have all the money after you've forwarded $200,000 worth of transactions.
(If you ever actually spent some of your profits via lightning things would start moving again though)
@_date: 2016-02-21 14:38:51


Because it was a round number substantially lower than the rates I've seen for paying by paypal or credit cards, but still high enough that you'd get a respectable return on investment for running a node. If I'd known that EU rates were dropping to 0.3% (!!!) I would've chosen 0.1% for the same reasons.
suggestion of 0.005% seems pretty reasonable for me in real life though. (Assuming that's per hop (so maybe 0.05% fees in total for a tx), by running a node you could get about 10% pa return-on-investment for that by doing just a couple of transactions and hour as long as your channel was only funded to about four times your average transaction size)
@_date: 2016-02-21 15:15:31
I call them hubs, even when Rusty and stark and Tadge tell me not to. :) As far as I'm concerned lightning hubs are just nodes that are trying to make a profit off of forwarding other people's transactions. There's no centralisation implied as far as I'm concerned: anyone can run a hub, just like anyone can by a 10-port switched gigabit ethernet hub from their local computer shop.
In the simulation, B/C/D are the only ones who'll forward transactions; that's contrary to what's assumed in most of the talks and the whitepaper, which figure that literally every node will be equal and end users will forward transactions too (eg, someone buying a beer might have their transaction go through Alice's phone, rebalancing her channels). I don't think that'll turn out to be practical, but I might be wrong.
@_date: 2016-02-21 16:57:23


IMO the answer to that is: because it's really hard for people who aren't one of the top twenty or so people with the best understanding of the code to go from "when it's safe" to any practical understanding of what that means. While it's technically precisely accurate to say "when it's safe", it's also ambiguous enough that you could keep changing the definition of what's "safe" so that it's never safe.
To put it another way: It's not a falsifiable to say "core will do a hard fork when it's safe" -- if you don't do a hard fork, you can always come up with a definition of "safe" that's not yet met. But even without working out in advance how to define "safe" exactly, it's easily falsifiable to say (eg) "core thinks it will be safe by $DATE1, and presuming that pans out will have code ready by $DATE2, aiming for activation on $DATE3." Dates aren't the only way of doing that (or even the best), but they're pretty easy for everyone involved to understand at least.
@_date: 2016-02-21 20:04:35
If lightning can do 0.005% fees per hop, then even if on-chain bitcoin tx fees were 5c per transaction, lightning would be cheaper for transactions under about $100. If it's merely comparable with EU credit card rates (0.2% in total), lightning would be cheaper than on chain transactions for anything under $25. (You might prefer to pay more and be on-chain anyway, of course)
@_date: 2016-02-21 14:19:31




It would also be enough if there were 272 times as many users buying coffees as there are now, if breaking even is your goal. Breaking even with current fees isn't enough to  pay for hashpower as the subsidy keeps halving, though.
@_date: 2016-02-21 07:49:36
Your fee is the standard 0.0001 BTC but your transaction size is large at 669 bytes, so you're only paying a little under 15 satoshi/byte (or 15k satoshi/kB). That's pretty low if you look at either:
Based on the fee, my bitcoin core node estimates the tx should be confirmed within about 19 blocks time, which is up to another 3 hours away.
@_date: 2016-02-22 04:46:29
Think of it like the cash you (used to?) carry around for daily purchases. If lightning works, it's usable for lots of things: coffees, bus fares, sandwiches, beer, paying for a haircut, buying an ebook, etc. So you fund a channel with $20 or $50 or $200 or whatever you're comfortable carrying around on your person anyway, and use the funds in the channel for lots of things.
If bitcoin suffers from too high a volatility risk for you to cope with holding $20-$200 for a few days, then everyone paying for coffee with bitcoin isn't going to happen, lightning network or not.
@_date: 2016-02-21 06:50:08
There are lots of goals for "bitcoin":
* decentralised: have a store of (transferable) value that can't be broken by inflation and manipulation by the powerful
* speedy: be able to send money anywhere instantly, rather than having to wait for banks
* cheap: be able to send money cheaply, without having people skim large chunks off in fees
* programmable: be able to have automatically enforced financial contracts
Long term, I don't think Bitcoin can compete on speedy and cheap: if you want to do proof-of-work to have security, then if you also want speedy, you're trading off against security which is generally a bad idea. Of course, the programmability lets you do payment channels, which gets around this. Cheap is also hard: on-chain transactions have to be stored permanently by everyone, which is fundamentally more expensive than you need for a receipt for a coffee, where just the buyer and seller might need to keep a receipt for a few weeks/years. Again, second-layer, off-chain solutions let you do this cheaply on top of bitcoin, but that's still not really "bitcoin".
That isn't to say bitcoin shouldn't be quick or inexpensive; just that there will be other things that will be faster and cheaper.
Lightning (assuming it works) will let you be fast/cheap but will only be as "decentralised" as bitcoin is, and will only be "programmable" in very limited ways compared to bitcoin. To me, if you want truly fast/cheap, lightning is the solution, not fiddling with the blockchain parameters. Of course, any solution on top of bitcoin is going to want bitcoin to still be "cheap enough"; lightning won't work if every on-chain tx costs $1000.
And if you want to retain "decentralised" then fiddling with the blockchain parameters is *hard* -- miners already worry about getting newly blocks downloaded, verified, and start building on top of them in a matter of 3 to 6 seconds (corresponding with 0.5% to 1% orphan rate, at 10 minutes per block), which is a bit nuts. Changing the blocksize to just 30MB (barely 5 doublings) might mean you'd spend 3 seconds just downloading over a saturated 100Mbps link, using up most of your leeway before you even start verifying the block, let alone mining on top of it. (The idea is, pools can always verify their own blocks instantly, so it only matters for other pools' blocks, and the bigger your pool is, the fewer blocks other pools mine; so if you can't do it in 3-6 seconds, a bigger pool can, and you're better off just joining that pool. Which leads to centralisation)
I'd be surprised if anyone working on bitcoin core (including Jeff and Gavin) didn't put "decentralisation" ahead of "cheap transactions", the question is more on where you draw the line of what's "good enough for practical purposes".
@_date: 2016-02-22 05:08:31
That's $100 per transaction; aiui lightning is designed to support transactions up to $10-$20, not $100 [0]. If your breakeven between bitcoin fees and lightning is above $100 at 0.5%, then bitcoin fees are at least 50c per transaction. So the assumptions don't quite make sense.
Nobody really knows how much lightning fees will be; maybe they'll be zero (funded by big data analytics or something?), maybe they'll be flat, maybe they'll be a percentage. Nobody really knows whether lightning transactions will mostly be micro transactions (internet-of-things devices sending each other a few cents?) or if it'll be useful for buying more expensive things ($100 jeans? $500 rent payments? $1000 tv? $20000 car?) as well.
The "costs" of running a LN are miniscule: the marginal CPU/bandwidth costs for a single transaction are on the order of fractions of  a satoshi [1] so 100 tx/s is on the order of 3e9 transactions per year, which will cost less than 1e-8 bitcoin each in real resources used, for a total of less than 30 BTC/year or about $14k. Other costs are bitcoin fees for opening and closing channels, time cost of money (could you do something more useful with bitcoin than hosting lightning channels), and the risk that your lightning node might get hacked and your money stolen. I think each of them would be more substantial than the CPU/network costs; but until lightning actually exists, they're pretty hard to estimate.
[0] 
[1] 
@_date: 2015-09-10 07:38:02
Why is the first point any sort of a problem? For comparison: "Think about what happens when you run out of cash in your wallet, you have to use another form of payment to fund it again (ie, go to an ATM, get a job)" Likewise for when you hit your credit limit, or your bank account the day before payday, etc.
But the thing that makes lightning **COOL** is that once you replace the hub-and-spoke banking system with a hub-and-spoke lightning system, you suddenly realise there's no actual limits on who gets to be a hub; you don't have to actually be a bank, you just have to be able to run a bitcoin node and the lightning software and trust it with some of your own bitcoins.  And then **BAM** nobody has to worry about delays signing up for a merchant account, nobody has to worry about chargebacks, you have enormous competition bidding transaction fees down, and maybe other nice things like censorship resistance or better privacy. Heck, if lightning works for everyday purchases, maybe it kills off consumer credit card debt.
(Well, you know, all that if lightning actually turns out to *work*... There's still lots of hard problems before there's an implementation, etc)
@_date: 2015-09-11 05:11:17
"A bank account is simple, your employer pays to it, you spend from it" -- and you can do the same thing with a lightning channel. If your employer doesn't want to pay directly -- just like most employers won't pay you in bitcoin today -- do it yourself, and if you can, find someone who will let you automate it.
The hubs / routing nodes in the lightning network earn fees by sitting on funds, without incurring much risk; and the math seems to work out at giving a pretty reasonable rate of return at pretty low fee levels (10% pa at &lt;1% per transaction). Add the fact you can close all your channels at any time and move your funds elsewhere, and that's a better deal than many investment opportunities, especially when interest rates are around 0%.
Personally, I don't think raw bitcoin works for consumer transactions because of confirmation delays (and the inherent risk in relying on zero-conf transactions). It's like accepting cheques over the counter; fine if there's really no alternative, but not knowing immediately if it's actually going to clear is still pretty painful.
@_date: 2016-10-25 05:31:59
I was thinking more like regtest than testnet even.
@_date: 2016-10-29 01:44:34
I find those bullet points pretty unconvincing personally.
 * "rule by technocratic elite" seems like nonsense; if you're referring to devs, it's just wrong; if you're referring to miners, it's only true in so far as it's true whether a soft-fork happens or not.
 * "not opt-in" is wrong, nobody is forced to use segwit transactions
 * "asymmetric / difficult to challenge" is wrong; you only need 5% of hashpower to challenge, versus 95% to support; and you could block segwit transactions with another soft-fork after the fact if it turned out to be a disaster
 * "user node security reduced" isn't true
 * "reliance on miners increased" isn't true
 * "ethereum example" is good at showing how hard forks can ruin your day...
 * "economic soft fork" is nonsense -- any soft-fork is going to have an "economic effect" if it's of any use whatsoever. you've referred in the past to doing nothing as being a severe economic change because fees will rise; if that's what you're referring to here, you're too late -- it's already happened. the graph software i was using in January has broken on me in the meantime, so i haven't done a blog post, but since March, dynamic fees have pretty much taken over -- from 40% to 95%, and fees have just risen linearly over that time -- you can get some sense of that from the images on  
 * "voluntary upgrade - long transition" -- that it's a voluntary upgrade means the transition is as long or as short as the ecosystem chooses for it to be. also, if you want to get a quick transition because a small group can force the upgrade, you might want to have an argument with whoever added the "technocratic elite" bullet point to your slide.
 * "capacity unlikely before January 2017" -- sure, it won't activate until Dec 2017 at best, big call to have made in August there. if the quoted "3-6 mo lead time" for a hard fork were true, that would be pretty much the same, but getting consensus for a hard fork will likely take *much* longer than that -- as far as I can see ,you've made negative progress on that over the past twelve months for instance.
 * "little user-visible benefit" isn't true -- it's an immediate 45% reduction in fees for simple transactions eg, but even if it were it isn't a cost or a risk
 * wallet/exchange/up-layer software doesn't need to know two txids, it only needs to know the same txid that's always been in use. the wtxid is needed by miners and by full nodes for transmitting witness data.
 * there aren't "two buckets", there's one bucket whose fee calculations are backwards compatible with old fee calculations, so pricing and fee bidding doesn't change either.
The bullet point I missed in the above, "increased software complexity" is addressed on the page.
But hey, that's just my opinion man. If you think you can make a persuasive, technical case that some or all of the above are real costs/risks, write them up and propose a pull-request to the page. That's all I did.
@_date: 2016-10-29 06:39:36




All that it means to "own" a bitcoin is that the blockchain has an entry spending some money to an address you have the private key for, and no later entry that spends that coin.
But what keeps someone from making a new blockchain that does have a later entry? In some sense "the rules" do -- but it's just software, anyone can change the rules, so that's not really helpful. If you mean the blockchain that you have on your PC, then you're fine -- no one can steal your money. If you mean the blockchain that someone else uses -- like coinbase or blockchain.info, well, you're back to the problem that they could change their software at any time they choose.
That's why hard forks are fundamentally more scary than soft forks -- they're the class of rule changes that can get money stolen. If you start saying "well, a hard-fork for a block size increase is okay", then do you also say "well, this feature didn't really get used, let's hard-fork to undo it" or "satoshi's probably dead and won't ever spend those coins, so let's give them to miners to help secure the network".
Technically, once segwit transactions are live, you could fairly easily come up with a chain that looks valid to old users that spends lots of segwit coins to yourself -- you need enough hashpower to make a block, but that's all. Most people will ignore it though -- there'll be a longer chain from the majority of miners mining segwit. That doesn't make your shorter chain disappear though -- you could give it a name and try getting exchanges to let you buy and sell coins on your chain for fiat, and you'd have an alt coin. You could equally well do that right now without segwit, but forking the code and changing the signature rules -- the limiting factor is that most people care about the longest chain, and if you try changing the rules to steal funds, you'll just be one of a million boring altcoins.
You can make more hypotheticals so that everything goes to hell -- like 95% of miners change their minds and switch to older/different code -- but they're pretty unlikely. If you had 10% of hashpower, and you switched to mining non-segwit blocks while 80% of other miners kept enforcing segwit rules (despite telling you they were going to switch), you'd lose all your mining income, and the other miners would get a 12% boost each. Do you risk that, or do you just keep mining segwit and hope someone else is enough of a sucker to give you free money?


Deploying segwit as a hard-fork immediately gives you two currencies, it doesn't avoid it. For everyone running the new version, they see the new currency; for everyone who hasn't upgraded, they see the old currency.
If you prepare the hard-fork well in advance (like satoshi's oft-quoted suggestion), this is less of a problem, because even if your software is a year old, it's upgrade ready. But that adds a year or two or three delay *after* the code is ready. We wouldn't be saying "segwit could be active as soon as December!", we'd be saying "segwit will be active in 2019".
@_date: 2016-10-28 17:06:51
No more or less than any other Debian package, but that's (arguably) somewhat less trustworthy than the official Bitcoin gitian builds.
If you want to rebuild it, you can; if you want to check the changes compared to upstream source, you can do that too -- they're fairly minimal, but they are there.  might be helpful. Like the Bitcoin gitian builds, the Debian package is (at least reportedly) reproducible -- 
@_date: 2016-10-28 13:18:43
The Debian packages are actually up to date for once this year, that deserves a post, right?
@_date: 2016-10-29 04:29:55
unstable only. you can rebuild the source in stable though.
 prevents it being included in testing and stable
@_date: 2015-11-23 20:22:43
I might be wrong, but I don't think $20 transaction fees are a realistic possibility. At $400/BTC, and 1kB per transaction, 1MB of $20 fee transactions would already be 50 BTC, which is twice the current mining reward, and four times the diff the forthcoming halvening will bring. With $5 or $10 per transaction alone, you could finance running an entire second bitcoin network that's just as secure (ie has the same hash rate) as what we've got now -- even if it was also limited to 1MB blocks and only funded by fees, no inflation subsidy!
There's three orders of magnitude difference there: going from being able to get transactions in the blockchain now at as low as 2c/tx, past 20c/tx and $2/tx all the way to $20/tx. Personally, I'd say $20 is implausible, $2 would be expensive and annoying but is possible, 20c is something to plan for, and to enjoy 2c transactions while it lasts...
For concreteness, to match the 12.5 BTC / $4050 USD block reward at the next halving with fees around 20c per 500B transaction, you'd need to have about 10MB of transactions in each block. At $2 per 500B transaction you'd be done with just slightly over 1MB. Those figures worry me a little bit for random bitcoin transactions I guess -- $2 is kind-of expensive, and 10MB every 10 minutes is a lot of transactions; but for opening and closing lightning channels, once a month or so, 20c is no problem and even $2 isn't a big deal.
@_date: 2016-10-25 04:37:26
Maybe: patch core client that mines its own blocks on top of mainnet, but much greater than 1MB (presumably dropping difficulty to 1 to make mining easy and marking blocks from the real chain as invalid to avoid following the most work chain), but keep mining valid transactions from mainnet. Probably even copy coinbase txns from mainnet to keep it as consistent as possible.
You could run that with 2MB, 4MB, 20MB block limits and see how quickly the mempool empties, and what the fee estimator ends up suggesting, which could be interesting.
@_date: 2016-10-15 17:57:03
A commitment of the witness merkle tree has to be included however it's implemented. If it were done via a hardfork, the witness merkle tree could be merged with the block merkle root for a saving of about 40 bytes per block.
@_date: 2015-11-23 08:19:12
If the hub you connected to directly (lets call it isn't scamming you and isn't non-responsive, you can just cooperatively close your channel. That will mean a transaction goes to the blockchain with three outputs: (a) some bitcoins directly to (b) some bitcoins that go to if he can provide some secret within 6 days, or will otherwise go back to and (c) some bitcoins that go directly to You can spend (a) immediately, and if (b) was a scam you'll never be able to spend it.
If your hub is hacked, or is just offline or whatever, you can't do this though. Instead, you have to sign and publish your last commitment transaction, which will mean there are three outputs, of which only one is relevant, namely: (a) which goes to after a day or two of OP_CSV, or if you're cheating by publishing an old commitment transaction. The "day or two of OP_CSV" is somethng you decided upon with the hub when you opened the channel though, it's not something that a random scammer can make arbitrarily long.
(There's a third possibility, where you're forwarding a transaction, which is timelocked, and isn't being cancelled by whoever you forwarded it too. The answer there is just don't forward funds for longer than you can cope with them being locked up -- running a node with the aim of making a profit by forwarding funds is more like putting funds in a short term deposit than in an at-call bank account)
@_date: 2015-11-23 08:31:18
Not really? It lets you pay someone quickly (seconds/milliseconds) without possibility of double-spends/chargebacks though, which is usually the goal behind trying to rely on unconfirmed txes.
@_date: 2016-10-29 02:30:58


A soft-fork is when miners add new rules -- eg "transactions from people with blue eyes must pay double the fee".


A hard-fork is when miners stop applying existing rules -- eg "transactions spending coins owned by people with blue hair can be signed by any key, not just the one they were supposed to".


If you add some rules, that's a soft-fork; if you later then remove the rules, that's a hard-fork.


If miners add some rules for a while (soft-fork), eg "transactions in a block will be sorted by the birthday of whoever sent them", but don't tell anyone else what the rules are (so that nodes run by users, exchanges, etc don't enforce those new rules), they can stop enforcing them (hard-fork) without anyone noticing, or any of the normal problems you'd have when deploying a hard fork.
(All my examples are deliberately unreasonable; hopefully miners don't know anyone's eye colour, hair colour or birthday in the first place)
When nodes have been upgraded to enforce the new rules, trying to mine blocks that don't abide by the new rules will have those blocks be ignored by the nodes (and wallets, exchanges, users etc using those nodes), so that even if you have a lot of hashpower you'll still end up with people seeing two chains and a lot of confusion, annoyance and cost as a result. That doesn't actually "steal" any funds though -- if you like the new rules, and run a node that enforces the new rules, the chain you follow will still acknowledge you as the owner of your funds. If you didn't like the new rules, and didn't make use of them, both chains will acknowledge all your funds as being owned by you.


A hard-fork doesn't "prevent their reversal" -- it's actually the opposite: a hard-fork is how you reverse them. Worse, without a fair bit of engineering or lead time or both, any hard-fork is kind-of "automatically self-reversing" -- any nodes that don't upgrade will just keep using the old chain and ignore the new one, so the "reversed" rules just automatically get a reasonable number of users just due to laziness.
Maybe some of that helps... Just being conservative and letting other people make use of new features first and seeing how that works for them isn't a bad approach though.
@_date: 2015-11-23 14:40:57
In the normal case, closing a channel and opening a new one is exactly the same cost and speed as any other blockchain transaction. Lightning just gives you an alternative to doing every transaction on the blockchain.
The blocksize is a whole other debate. Personally, I figure if bitcoin's unusable because of too small a blocksize there'll be some altcoin that's usable instead, and I'd just run lightning on top of that. But while bitcoin fees are in the 3c-8c per transaction range, that doesn't worry me at all. YMMV.
You don't need to wait for the HTLC timeout ("locktime") to reuse funds within the lightning channel; transactions that go through are available immediately, and transactions that fail will get quickly cancelled in the normal case. (For active attacks, there's been talk of "fining" the attacker, but I'm not sure if that actually works) And you can move funds from one channel to another either within the lightning network instantly, or via the blockchain with just the usual wait-for-n-confirmations delay. (If you trust 0-conf transactions, then I think you're crazy, or at least very risk-tolerant, though...)
@_date: 2018-04-11 13:11:59
What you think it was meant ot achieve certainly affects whether you think it was a success. There's a variety of views you could take:
* It was a plan to either get 2MB and segwit, or neither. That's the game theory you need for "let's make a deal" proposals, which was what segwit2x seemed to be to me. We got one but not the other (and the other but not the one if you count Bcash) so this plan was an obvious (double!) failure.
* It was an attempt to do two separate things: get segwit, and get 2MB; if so, it was definitely a partial failure in that it didn't get 2MB, and given every block complied with UASF rules, its contribution to getting segwit activated seems at best only minor (heh). If this was the plan, I think it's fair to say the "segwit" part succeeded, and the "2x" part failed, so complaining that people call it "just 2x" isn't very fair.
* It was an attempt to prevent bitcoin from having a permanent fork. It completely failed at this.
* It provided political cover for the UASF flag day that was already underway. If this was the goal, it seems like it was a success, but maybe one that's hard to admit in public.
* It was an attempt to fire core and move control of the reference client to a new, friendlier group. Total failure.
* It was an attempt to keep controversy going to keep attention on bitcoin, and keep eyeballs and awareness. Seems like a success?
* It was an attempt to keep controversy going to suppress the price prior to a big post-segwit rise. Huge success!
A second important reason to abbreviate segwit2x as "2x" is that everywhere people talk these days has character limits...
In any event, it's a dead proposal, there's no need to keep defending it. Treat it like you would any rejected PR: be grateful for the lessons you learned, take the feedback you got on board and try again later with a better plan that maybe other people will like, or if you think the rejection was wrong, work on a different project.
(If you want to defend the living companies: well, isn't it easier to point out that they did avoid the cliff of their own accord? It wasn't optimal, but it wasn't nothing, either)
@_date: 2015-10-03 08:43:28
Make it a bit simpler: one node sends 5 mBTC (0.005 BTC) along five hops. (Note that lightning transactions are limited to 0.04 BTC = 40 mBTC ~= $10 --  )
So that's A (the paying node) to B to C to D to E to F (the recipient node). Some number of bitcoin were locked up when those five channels (AB, BC, CD, DE, EF) were established, and each channel's locked bitcoin is split between the two ends; so if AB had 2 BTC, A might currently have 1.5 BTC while B has 0.5 BTC; likewise if BC had 3 BTC, B might have 2 BTC while C has 1 BTC.
When the 5 mBTC transaction starts, A locks up 5 mBTC on the AB channel that B can claim later; and B does likewise on BC, as do C, D and E on CD, DE and EF respectively, though presumably F will claim the funds pretty much instantly, so whether anything's "locked" on the EF channel is debtable. So that adds up to 25 mBTC across the channels, to make the 5 mBTC worth of funds. If this is happen simultaneously on 5 different 5-hop routes, that's 125 mBTC worth of locked funds.
In the normal case, that's not really interesting though -- they're only locked for the second or two the transaction takes to complete, and it only affects people doing things simultaneously. 
If you've got a peak of 1M transactions per second (compared to Visa at &lt;100k/s), an average route length of 5 hops, and an average transaction takes 5 seconds (which I think is all fairly high); but you have 100k nodes, each with an average of 10 channels, then the average channel is only doing ~50 simultaneous transactions -- and with each transaction at under 0.04 BTC that's only 2 BTC locked on each channel.
I think the bigger constraint is the initial channel funding (where you're locking bitcoin into the lightning network while the channel is open) -- ie, the 1.5+0.5 BTC that A and B put into the AB channel in the first place. If you want to pay $10/day ($5 for coffee, $2 for bus fare, $3 for internet), but only recharge once a month, your channel ends up being $310 (for the monthly figure) rather than $10 for a day; and there's similar constraints for merchants (if you get $5 for each coffee that's great, but if you want your channel to last for an entire day's sales of 400 coffees, it has to have $2000 worth of capacity). Refreshing the channel without hitting the blockchain is theoretically possible (if you're using an exchange to convert lightning directly to USD eg), at least.
@_date: 2016-10-28 14:14:08
All three variants!
@_date: 2015-10-03 08:54:53
500 BTC (~$100k) is way too big for lightning; and a 55 hop chain is pretty ridiculous (4-8 hops should be enough to get anywhere, and 10-15 hops is probably the point of diminishing returns for onion routing improving your privacy).
Otherwise, sure, the maths sounds about right.
@_date: 2015-11-23 05:16:40
If the initial anchor transaction (spending funds from either party to a 2/2 multisig address of both parties) is malleated by a third party, then your funds can be held hostage.
You can avoid that with OP_CLTV or OP_CSV (there's only minimal differences between them for this purpose) by making it be "pay to multisig, or if timeout has passed, pay back to me alone", but doing so puts a strict timelimit on your channel lifetime, and still means you've lost access to the funds for the duration if the channel gets malleated and your counter-party doesn't play nice. (This is a similar drawback to having OP_CLTV but no OP_CSV)
If I remember right, removing signature malleability as well means you can (trustlessly) outsource watching the blockchain for cheating, which might make it more plausible to run a lightning node without also running a full bitcoin node. The SIGHASH_NOINPUT sig type proposed in the lightning paper would make this possible; I think segregated-witness does too.
@_date: 2015-10-27 23:50:06




Yes -- lightning transfers bitcoin between people (or if you run lightning on top of an altcoin, it transfers altcoins between people). It just does it without hitting the blockchain for every transaction.
@_date: 2015-10-03 05:59:49


I think it's an open question as to how static lightning addresses will be. In order to change them you have to change your channels, so that's a cost, but maybe the benefit of decoupling payments makes it worthwhile? But onion routing means that your address isn't that tightly associated with the payments you receive, so maybe it's not that worthwhile?
If you're making a profit in the lightning network (either by collecting lightning fees, or being a merchant), you'll fill your channels up and the obvious way out is to close and reopen them. If you've established a few channels, and some are much more profitable than others, you'll want to close the unprofitable ones to free up the bitcoin in those channels. Also, if the channel's quite profitable and you want to make it bigger, you'll need to go to the blockchain and effectively close and reopen it.
(It's also likely that you'll need to keep a few hundred bytes of info for every transaction you've ever had go across your channel in order to protect yourself against cheating -- eventually that will add up to enough to make it worth paying a few cents to miners to commit to the blockchain and refresh the channel. But that's months or years)


The lightning channel is backed by an anchor transaction that's published on the blockchain when the channel opens; as long as the anchor transaction itself isn't forked out of the blockchain and its inputs doublespent, you should be fine.


To send a payment you first choose a route, ie "to get to Dave I send to Alice, who sends to Bob, who sends to Carol, who sends to Dave". Dave chooses an R value, and sends you its hash ( somehow. You then "onion route" the payment to Dave ending up with a message MA encrypted for Alice. This way:
 - Alice sees something from you, paying to R, and decodes MA to see it is to be forwarded to Bob with a message MB
 - Bob sees something from Alice, paying to R, and decodes MB to see it is to be forwarded to Carol with a message MC
 - Carol sees something from Bob, paying to R, and decodes MC to see it is to be forwarded to Dave with a message MD
 - Dave sees something from Carol, paying to R, and decodes MD to see it is for him; he reveals R to Carol to receive the payment
 - Carol sees R, and reveals R to Bob, to receive the payment from Bob
 - Bob sees R, and reveals R to Alice, to receive the payment from Alice
 - Alice sees R, and reveals R to me, to receive the payment from me
Privacy features: if you're not in the route, you never see the transaction; if you are in the route, you only know your neighbours are involved, you don't know where it came from or where it's going; *but* if people in the route collude, they can correlate a transaction (Alice and Carol can see the same  in the transaction, so can now it came through me and went out through Dave -- this is quite a bit worse than timing attacks with tor, but unfortunately seems unavoidable)
You know your neighbour's lightning addresses, and you have a connection open to them over IP (or similar), though that itself might be anonymised over tor.
@_date: 2015-10-27 23:43:18
I think that works rephrased as: "If miners would accept every transaction, no matter how small the fee, then rational users would set their per-tx fee to 0 or 1 satoshi, leading to zero or trivial revenue for miners no matter how many transactions they accept. Rational miners will thus limit the numbers of txs they accept to ensure fees don't go to zero."
@_date: 2015-11-23 08:07:57
Concerns I have:
* It's not implemented yet -- that limits adoption pretty seriously :)
* Working out how to scale route discovery could be really hard.
* Running a lightning node without also running a full (but pruned) bitcoin node might be hard, which would limit adoption assuming you don't want to run a bitcoin node on your phone...
* Tracking payments by R value might be too effective, making the onion layer pointless and meaning governments and well-resourced criminals can figure out everything you buy and sell. But it looks like that's solvable given a new OP_ECC_MUL or similar.
* Lightning wallets are "hot" by nature; phones are pretty insecure -- running a lightning wallet on your phone might mean you get pickpocketed a lot. This might not actually be worse than existing phone wallets though...
* Keeping the lightning network "balanced" might be hard -- if someone receives more than they spend on the blockchain, that's fine, they just get more UTXO entries; if they do the same thing in lightning, their channels fill up and they can't receive any more money; and this affects routing nodes as well as merchants. Not really a showstopper, even in the worst case, though
* Without third-party malleability fixes and OP_CSV, managing channels/funds will be pretty tedious. Fingers crossed for BIP68/112/113, and a segregated witness soft-fork soon.
Things I'm not concerned about (though YMMV):
* Miners not getting any money. Lightning nodes will have to pay them whatever it takes to close channels anyway; if they price out other uses of bitcoin, they'll only do that by paying miners enough anyway.
* Lightning will make blockchain fees super high, like $20 per transaction. I don't think opening a lightning channel can realistically cost more than monthly bank account fees, so $2-$5 per transaction would seem like an upper limit to me before lightning is too expensive compared to banks/paypal/visa/etc.
* Funds are "locked up" in payment channels, making them inaccessible. You can withdraw from a channel in three ways -- agreeing with the other party to cash out to the blockchain, which in the normal case should always be easy; unilaterally closing it, which will cost extra in fees and a delay of up to a few days (assuming OP_CSV) before you can access the funds; or spending the funds to yourself over the lightning network. I expect  those to be "good enough".
* Lightning is only good for micropayments, not big payments, like buying a house or a car or getting a salary or paying rent. (a) Okay, so what? Just use the blockchain directly for big payments, micropayments are interesting enough! (b) If you can pay someone a cent cheaply and easily, then just do that a million times, and you're starting to talk real money. Computers are good at doing the same thing repeatedly, so maybe good at micropayments means good at big payments.
* Blockstream/Visa/rich people will control everything in lightning: that might be possible, depending on how the "payment routing" system ends up working; but even if it does, it's just a matter of replacing the routing system with one that doesn't give the bad people control, and setting up your own lightning network, presumably with blackjack and hookers. Lightning seems like it'll be pretty interoperable, so doing your own thing should be pretty effective.
* Coding is hard. Yeah, it is; and a working small-scale demo on testnet is months away at best IMO, let alone something you want to use with real money. But that's okay, good engineering takes time.
@_date: 2015-10-03 06:20:16


Fees will be dynamic in that they'll be different for different channels, and adjustable without having to close channels. They'll probably have a rate limit though -- ie, you can only change your fee every 10s or every 10m or every hour or something.
It will be user-controllable, in the sense that you can choose not to forward transactions that don't pay enough fee, or you can choose for your transaction to only pay a particular fee and if that's not enough to have it fail (and be refunded, obviously).
I expect it to be a percentage fee (since forwarding a transaction has an impact on your channels proportional to how large the transaction amount is), rather than a per-kB or per-tx fee like the blockchain has. If the percentage for an individual hop is 0.1% (eg), and there are five hops, then that's 0.5% total, so sending more than 20 mBTC (about $5 USD) to someone over lightning would give you total lightning fees of more than 0.1 mBTC and it would be cheaper to go via the blockchain. (Cheaper lightning fees and shorter routes would raise that 20 mBTC figure; longer routes and higher fees would lower it)


Rusty's adjusted the resolution to operate in 1/1000th of a satoshi; so a 0.1% fee on a 1 satoshi transaction would work fine. (These get rounded off when you hit the blockchain though; so you want to have done thousands or millions of microtransactions to get any benefit) cf 


I don't think there's anything special here -- lightning nodes rely on the blockchain so will have to pay whatever it costs to keep the blockchain running, just like anyone else using bitcoin. If it turns out everyone uses lightning for everything because bitcoin tx fees are high, then lightning nodes will be the only ones paying fees and compensating miners, and they'll have to pay high fees to get high security. If lots of other people are paying fees, and there's competition for space due to block size, they'll have to pay high tx fees just to get in a block. Same as anyone else using bitcoin without the inflation subsidy.
If blockchain fees were really high at say 12.5 mBTC per tx (so 2000*512B txs per 1MB block pay 25 BTC matching today's per block inflation subsidy), and lightning fees were really low at 0.1% for the entire route, then sending 12.5 BTC or more (about $3000 USD at current prices) would be the point at which it's still cheaper to use the blockchain directly. So I don't think it's plausible to ever expect to see everything happening over the lightning network.
(Also, lightning only really supports payments to an address; if you want to use bitcoin's scripting features, you still have to use bitcoin... That's why lightning's running on bitcoin, after all)


I don't think a smartphone would be practical (you'd lose connectivity too often, you'd run out your battery life, and you don't have enough control of the OS to protect it from being hacked), but afaics it would definitely be possible. (The difference between being a node versus running a "light" lightning wallet on your phone is probably more in how you use it than how the actual software works)


Yes, you collect a fee for every transaction you forward.


I'm not part of "the team" (whatever that means), I'm just following the list and reading the code now and then, but I don't think this makes sense -- running a lightning node means putting bitcoin into channels and keeping the keys you'd use to spend those bitcoin available, which means the bitcoin could be stolen if someone cracks into your computer. I think the way it will work out is the more you risk the more profit you'll make; so if you're not willing to risk very much, you won't get much profit.
However, if you've got cheap and easy micropayments, you could just add fees for some bitcoin RPC operations -- "you'd like me to forward a transaction? sure, 1 satoshi please", "you'd like to see blocks 100,000 to 100,999? that's another 1 satoshi", "you'd like a proof that tx X hasn't been spent yet? sure 1 satoshi". Maybe that way full nodes talking to each other would break even, but they'd all get subsidised by payments from lightweight wallets?


Assuming OP_CSV, the money isn't locked for any extended period (ie it's locked up for a day not weeks, and it's not locked up at all if both sides of the channel are active), so I don't think there's much effect. You could equally well say that at present there can be at most, what, 10k input transactions per block, and since the UTXO set is about 33M, so on average any given satoshi is already locked for about 10 days (since it would take 33M/10k*10min ~= 20 days to spend them all).


As a result of coping with fees as low as 1/1000th of a satoshi, a single lightning transaction can only be about $10, so there's one limit (though you can just use multiple transactions to send larger amounts, of course). Otherwise, if you spend too much money in one direction, the channels on your routes will be lop-sided, and either fees will rise until you stop doing that, or the channels simply won't be able to accept your transaction anymore. And, as above, at some point, using the blockchain directly will be cheaper anyway.


Unknown at this point. Hopefully the normal case will be that you know within seconds or tenths of a second. The worst case is that a node goes down while forwarding your funds, in which case your funds for that transaction get locked up for a timeout period (many hours or a few days). It may be possible for the merchant to "cancel" that payment despite that timeout (the dead node can be routed around), so that you can safely retry the transaction.
@_date: 2017-06-18 07:44:14
BitMain's patent on ASICBoost is dated August 2015, three months before sipa's presentation on segwit, and five or so months prior to the Hong Kong agreement. The original ASICBoost patent was Nov 2014, a year earlier than that. 
Jimmy Song points out likely use of overt ASICBoost (by twiddling bits in the version field) on testnet in October 2014; presumably that's Sergio and Timo doing proof of concept rather than BitMain though. See:
@_date: 2017-06-19 12:20:17
The code says three months, so if sw2x activates just in time to avoid a uasf split, probably sometime in Nov.
@_date: 2017-06-20 04:27:22
That's three months from now, and would only happen (according to the code a couple of days ago anyway), of miners were already signalling bit 4 and 1. If miners don't signal til late July, that adds a month, and if they don't lock in segwit til after segwit2x is locked in that adds another two to four weeks, which takes it to November.