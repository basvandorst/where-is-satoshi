@_author: meshachblue
@_date: 2019-07-31 20:20:34
With things like LibraCoin and other big company backed monetary solutions out there Bitcoin will unlikely become the primary contender for worldwide monetary transfer.
Therefore it will likely sit within a niche, possibly much like gold does now.
Just for some back of the envelope Fermi maths, if we say about 1% of the population will want to hold $1000 worth, about 0.01% will want to hold $100 000, and about 0.001% will want to hold $1 000 000, given the cap of 27 million and let's say a population of 14 billion this estimates the price to be:
14000×(0.01×1000+0.0001×100000+0.00001×1000000)÷21 = $20 000
... No one who buys today is going to the moon, the rocket has already left. Those who turn up to the hanger will get on a toy rocket at theme park for a roller coaster ride.
@_date: 2017-03-27 02:41:37
There's a whole massive group of "untapped" people who would happily open up a webpage to mine but couldn't go any further.
@_date: 2017-03-26 21:24:47
A whole new kind of supporting websites via native speed mining on visitors computers?
@_date: 2013-05-22 10:06:19
How does this sound:
It has been seriously revamped with lots of help.
@_date: 2013-05-20 21:28:50
Yehh, the trust and legal issues might have to just be accepted and therefore this could only be used for limited purposes.
I am really interested on your thoughts if it was done this way instead:
@_date: 2013-05-22 10:04:04
I have updated the "how it is going to work":
What do you think?
I am considering reposting seeing as the idea has now been fleshed out a lot more.
@_date: 2013-05-20 21:16:14
It in now way will be used to help secure the network. Leave that to the hashers. This is made to value inject into bitcoin by offering miners using GPUs another choice as opposed to hashing.
@_date: 2013-05-25 01:16:19
It may take me 6 months before I have got onto step 2. But at that time, even if you were just willing to open the javascript website for a couple of hours every so often so I could do some testing, that'd be amazing :).
@_date: 2013-05-22 10:02:43
I have significantly updated how it works:
Want to see if your issue is addressed before I repost it.
@_date: 2013-05-24 02:02:15
Yehh, I was looking into OpenMM last night. I think though I may change tact. Maybe instead run it through a website using WebCl. (This would make the whole task a load simpler... and much more user friendly)
@_date: 2013-05-23 07:56:11
It would need to use OpenCl, and GEANT4 has been used on GPUs. The p2p nature has not yet been implemented. My hope is that I will gather enough interest and find people willing to help me code.
@_date: 2013-05-20 21:00:58
What if it was done on a GPU. Each user would therefore be submitting a thousand or so iterations per cycle on their GPU. That way only a few randomly selected results from their pool of results needs to be tested against others. Have a trusted node, use that node to repeat the calculations of others. But since each set of results would emulate some 1000 particles from each user (if done on GPU), then only one of these needs to be tested and compared.
@_date: 2013-05-20 11:54:41
It would be amazing if it was somehow integrated, but I fear that would be a lot of trouble and it would be better to get it working on the side first.
@_date: 2013-05-20 11:09:47
An alternative bitcoin "miner".
Pays the miner more bitcoins by giving the miner the option to run a researcher's Monte Carlo iteration.
From the researchers point of view, Monte Carlo simulations run many many thousands of iterations of code. Each iteration is independent of each other iteration and can be run in any order. This iterative process is perfect for distributive computing. Unfortunately due to limited hardware an iteration of 16 000 000 particles often takes days. This limits medical applications and research progress.
For applications of Monte Carlo simulations see 
CompuShare, when completed, would give researchers the option to pay bitcoin miners worldwide in an auction style, with bitcoins, for the ability to run their iterations distributively across the node network. ~~The code to be run would be an encrypted binary blob which would be run inside a sandbox. It's only network access would be to send a final encrypted result back to the payer in return for the bitcoin payment.~~
As a result researchers would have access to practically instantaneous Monte Carlo results. Particular radiation treatments could be implemented days earlier, the progress of physics research worldwide would increase, underfunded universities could have access to a distributive super computer for a very reasonable cost.
All money would be transferred as bitcoins to the bitcoin mining network. There would be several beneficial effects to the bitcoin economy as a result:
* This would [help --inserted] stabilize the bitcoin economy. 
* It would make it so ~~all computers~~ [GPUs] could once again effectively "mine" bitcoin. ~~(not just ASIC's)~~
* It would do leaps and bounds for legitimising the currency. 
* Make it exceptionally more liquid. 
* Allow users to very effectively trade electricity for bitcoins effectively bypassing exchanges. 
* The price would no longer be defined by a few exchanges and their traders, instead it would be decided by the value of computation (which is very valuable).
edit1: plugin was confusing
edit2: 
Use GPUs for the MC simulations. Have the code that is being sent open source. Since the code is being run on a GPU each set of data could represent about a thousand particles. Have a trusted node purely computing random numbers. This trusted node would test the trustworthiness of each node by sending one of these 1000 particles to be based on the same random numbers sent to a secondary trusted node. This would mean for every 1000 nodes you would need 1 extra trusted node. (and maybe quite a few more random number generator trusted nodes.)
Remove the idea of sandbox. Remove the idea of trying to make the calculated code closed source. Run on the above idea.
Eventually have it set up so that the user running the program on their GPU can automatically start hashing again if no computation is available that is worth it.
@_date: 2013-05-23 07:15:06
Due to the parallel nature of the Monte Carlo code, and the fact that it is being run on GPUs each node will send a large number of results back to the management node. Only a small random portion of this code needs to be checked. If at any moment a node submits false results they loose significant reputation. And are not paid for their entire submission.
It would not be worthwhile to cheat half the time as you would be very quickly found out.
@_date: 2013-06-03 02:34:22
My supervisor is aware of it. Thank you for the heads up.
@_date: 2013-05-20 14:22:12
I was under the false impression it seems that it is very hard to mess with a binary blob. Is that not the case?
What if a binary blob could be dissected and then was used against the payer. Hopefully the time taken to do this would be much longer than the total time for the network to finish the problem...
@_date: 2013-05-20 12:22:25
That is correct, it is confusing. Fixed.
@_date: 2013-05-23 07:26:54
It is abundant in some areas. But a lot of hospitals and scientists are still in the black. And I know many (my future self included) would be willing to pay very reasonable prices for this. :). Ideally this would like nicely in with current software such as GEANT4. As a result even the non-tech savy would eventually use this.
Often people are willing to give computing power for free, but it no where near quenches the huge demand.
@_date: 2013-05-23 10:03:22
Here we go... some better answers to your questions:
@_date: 2013-05-22 10:04:49
Here are some more details :)
Much more fleshed out.
@_date: 2013-05-23 21:38:28
I hadn't thought about it, but it should be as simple as just having the litecoin hashing sitting there as an optional task also. You would will be paid in bitcoin, but if there was no viable computation available you could resort to litecoin hashing if you choose.
@_date: 2013-05-23 07:50:26
Awesome, thank you. Did out want to contribute in the programming way? 
@_date: 2013-05-23 07:43:58
Thank you for your advice. The wording "trusted node" is actually quite misleading. They are just more the goto nodes for duplicate testing. All results "trusted nodes" are themselves also checked by duplication.
I shall change the wording and make it more clear. Thank you.
@_date: 2013-05-23 07:05:26
Thanks for the idea on the fiat btw. Once the proof of concept is complete I will approach exchanges for some form of automatic transfer to or from fiat (depending on if the user desires it or not)
@_date: 2013-05-20 21:11:33
No that sounds good. What my current thinking gathering from all the help here is the following.
Use GPUs for the MC simulations. Have the code that is being sent open source. Since the code is being run on a GPU each set of data could represent about a thousand particles. Have a trusted node purely computing random numbers. This trusted node would test the trustworthiness of each node by sending one of these 1000 particles to be based on the same random numbers sent to a secondary trusted node. This would mean for every 1000 nodes you would need 1 extra trusted node. (and maybe quite a few more random number generator trusted nodes.)
Remove the idea of sandbox. Remove the idea of trying to make the calculated code closed source. Run on the above idea.
@_date: 2013-05-20 11:33:10
ASICs would not be able to run this plugin. It would be a way in which people would once again be able to "mine" with their normal CPUs
edit1: and potentially GPUs... but my aim is to just get it working on CPUs atm.
edit2: But... I would leave GPUs to someone else. I mean eventually this could also be ported to Android as well, as high end phones would be fast enough, the main thing is just "A LOT" of computers on the computation end required before the "instantaneous" is achieved.
@_date: 2013-05-20 12:42:55
"porting existing software in an efficient way is challenging"
-- 
I'll stick to CPUs for now...
@_date: 2013-05-22 10:05:26
Thank you for your help and tips.
I have fleshed out the idea here:
see what you think :)
@_date: 2013-05-23 02:44:44
But you are commenting on speed not cost, sorry. It comes down to the code form of Monte Carlo simulations. The speed of the code running is directly proportional to the number of cores available. Generic supercomputing has many cores, but this is still limited in comparison.
@_date: 2013-05-27 23:44:55
I don't appreciate your accusation of misconduct. Sorry for bothering you, I will do so no longer.
@_date: 2013-05-23 02:42:55
Verification costs 1%, and management fees would be minimal. Even if they were also 1% then the total cost to the client would be $1.02. I just left it as $1. Is that confusing?
@_date: 2013-06-03 03:33:28
I told him what I was doing in my spare time, and he mentioned I could try and make that into my masters project.
@_date: 2013-05-23 10:28:49
Your point is very valid. I had been given the impression that GEANT4 had been implemented on GPUs. This only appears to be the case in a few very simple cases :/.
Unless I find somewhere that GEANT4 is run on GPUs this is partially dead in the water.
@_date: 2013-05-20 13:38:57
Your point about it being extremely inefficient to keep it secret from the processor is a very valid one.
It may have to be run in an "open source" format. Which would mean, if you submit your code to be used by the network it would need to be for research that doesn't make much sense without the context. Or the user is okay with the geometry of the inside of a particular machine to be public knowledge (as often it is anyway).
Unfortunately it would wipe out many other uses. But still plausible.
@_date: 2013-05-23 07:20:30
Some researchers are moving over to GPUs, but it is my understanding that for what ever reason they just book a slot on the local cpu supercomputer. Maybe, for most issues in the hospital/university CPU is what they want. Maybe not as many uses for the GPU. Not worth buying and maintaining them when there is something else already available? I am speculating. But from talking to people in the medical physics industry most use CPUs and as a result it takes days.
@_date: 2013-05-20 13:39:56
But that then brings in a hole of the cheating, because I was undoing the cheating by making the code encrypted.
@_date: 2013-05-20 23:58:38
The aim is not to verify bitcoin transfers. There will be no bitcoin verification being done.
Essentially this is just a value adding implementation.
Does the following address your issue?
@_date: 2013-05-21 03:24:45
The only bit that I can see being exceptionally difficult is the decentralized nature of it, is that such a difficulty that it would be not worth trying to decentralize?
I would really like to be able to make it decentralized as it would massively increase the uptake of the code by users. People are more willing to contribute if they own the network, not some central server...
@_date: 2013-05-20 21:14:35
Does this help the issue?
@_date: 2013-05-22 22:17:34
Thank you btw. :).
The repost is here:
@_date: 2013-05-20 21:17:53
The point is, the target users are GPU bitcoin miners. They would want to be paid in bitcoin.
At a later point it could be implemented to have a "back up process" be hashing if there are no better offers.
@_date: 2013-05-22 09:49:17
Thank you so much for all your advice throughout this. I have taken yours and other peoples advice into consideration and updated the github readme:
What do you think? Is it now worthy of a repost?
@_date: 2013-05-20 13:52:33
Kk, will iron this out, and repost.
Thank you so much for helping me get this idea straightened out in its baby stages.
@_date: 2013-05-22 22:11:14
Unfortunately I am currently limited to the newbie board. May I ask if you could post it to the alternative clients board?
@_date: 2013-05-23 06:59:25
Yup, hidden at the end (in the github readme) is a little bit saying that people could make add ons for exchanges for client side payments. Once more than just miners wanted to implement it, there could also be a transfer at the worker end also.
@_date: 2013-05-20 13:20:52
So I can imagine it would take a bit to distribute your code that takes 15mins. If there are 1000 computers on the network that have taken up your code and you want to run 16 000 000 iterations in order to test your new LinAc. Due to the central computer being clogged with requests you can only gain access to 6 cores. At this rate it will take you 24 hours for your code to run (once you are through the que). Assuming the cores in the supercomputer are twice as fast, each computer would run your iterations for about 20mins.
Lets assume these computers are 200W computers. And assume the price is 15c per kWh. That means each computer would need to be paid 1c.
This is a total cost of $10 to run the code. Essentially though, it would come down to 3c / hour / "core".
This is a comparable cost when  charges 5c / hour / core.
Even if the extra overhead made it take 20% longer this would still be value for money on today's market.
@_date: 2013-05-20 12:00:22
The code being run would be a binary blob, shared in a torrent like fashion. The binary blob would send an encrypted result back to the payer.
The real benefit of distributed computing over Amazon EC2 is that each iteration only takes a very small amount of time, but these iterations need to be done billions of times. Distributed computing would work much better.
An iteration of code would be shared throughout the network, the first node to pick it up would give it a "punt" and see how much computational effort it takes. If it is taking to long for its declared payment, it would just stop the computation and declare it not value for money.
If enough people take it up, and say it is worth it, other nodes will look at the selection of codes available to run and do a "cost benefit rank". The best bitcoin for computation will be run first.
edit1: And you are right, I am a masters physics student. I have a lot of deficiencies required for this. I understand the massive benefit it would result in. I am planning to do my masters thesis on this, and as a result will be doing a lot of the learning required. Would love any help I could get. If it is really not plausible I would like to know now.
edit2: Nodes would be listening to nearby nodes to see if there is any new code, or value for money code to be shared.
edit3: All code would only have the privileges to run inside a sandbox of sorts, and then send an encrypted result back to the user. 
@_date: 2013-05-22 23:43:28
At the moment it really needs to use bitcoin as there is a Catch 22 limiting its take off. Researchers won't pay for this unless it is significantly better than Amazon. It won't be significantly better than Amazon until there are a large number of nodes on the network. Nodes will only come to the network if it is profitable.
If this program has within it Bitcoin hashing, then GPU miners can mine bitcoin until the paying clients catch on to the service. Once paying clients begin to use it, it will snow ball. As more clients use the network the payout to the workers will increase. As payout to workers increases, the number of workers will increase. 
This needs Bitcoin to start. And because it will be firmly established using Bitcoin it will continue using bitcoin.
@_date: 2013-05-20 13:46:10
Hmmmmm... okay, that sounds much more plausible then. I was under the impression GPU + MC = hard. Will look into that and hopefully rejig my idea :).
Thank you.
@_date: 2013-05-20 12:44:55
It wouldn't be as efficient, and for the people running the code, if the only cost for them was electricity then sure it wouldn't be worth it.
But there are often huge opportunity costs associated with large wait times, and often the available computer in the local hospital is not sufficient.
It would also provide computing power to people doing research who would never be able to hope to gain access to a supercomputer.
@_date: 2013-05-20 11:23:38
Researchers would definitely pay for such a thing. These are applications for this kind of simulation:
Very lucrative.
No one is working on this. I am looking for people to help me.
@_date: 2013-05-23 21:30:44
Upon thinking about it, I actually think this is a much better idea. Will change it to this.
@_date: 2013-05-25 01:15:17
Awesome :) Thank you. Have a look at the updated readme. I have significantly changed it with the aim for massive simplification, and something that will still be useful when it is partially completed.
Threw away a lot of ideas, but the end result is something that I believe still meets the same ends. :)
@_date: 2013-05-22 22:20:50
CompuShare -- Proposal
A program to be used by GPU bitcoin miners "workers" that will either hash or run "pay-per-iteration" Monte Carlo simulations depending on which pays higher at the time. 
Would result in a worldwide distributed supercomputer that could be capable of running 30 000 000 iterations in 1 minute for the cost of $1.
For Clients
There are many [applications of Monte Carlo simulations]( Often there are time sensitive applications. Due to the [embarrisingly parallel]( nature of Monte Carlo iterations the time taken for the code to run is directly proportional to the number of cores available. If enough GPU miners run this code life saving operations could be tested computationally in minutes instead of days.
This is already available to clients via Amazon's Spot Instances. An example of an implementation is described [here]( The downside of using Amazon's service is that the price per instance is rounded up to the hour. A "pay-per-iteration" model would significantly benefit the client over current availability.
For Workers
Firstly, more Bitcoins for the computational buck. But most importantly the running code is all contained within CompuShare, the only files being downloaded are a geometry file, a bunch of random number generator seeds and the initial trajectory of the particles. Your computer would take those initial trajectories and propagate them through the geometry with the random number seeds given.
A difficulty for the miners is that they may need to install more RAM into their mining rig.
Some numbers for the interested
Assume 1000 computers online submitting computation requests to each of the management nodes. Assume each computer has GPU with 500 processing units. Assume each processing unit takes 1 second to process an iteration. This means this cluster would be capable of 30 000 000 iterations every minute.
Assume computer runs at 400W, assume price of electricity is 15c / kWh means 1 minute of computation would cost each computer 0.1c. This means the researcher would only need to pay a little over $1 per minute to access all 1000 nodes. 
Assuming network overhead means there is a 4 minute delay in data transfer this means for $1 this researcher has made a computation take 5 minutes that would have taken days on six CPU cores of the local super computer. 
If an extra cancer patient is able to be treated because of this time saved, the value of that $1 is immense.
Making it happen -- Brainstorming results
Begin with [Monte Carlo eXtreme]( for the running of Monte Carlo simulations on graphics cards.
CompuShare would contain within it all the executable code in an open source fashion. Only geometry files and initial particle trajectories would be downloaded onto workers computers.
This network would contain three categories of nodes, "untrusted", "trusted" and "trusted management"/"management". An untrusted node can become trusted by completing 100 000 000 consecutive iterations without error. If a trusted node makes an error it is demoted to an untrusted node. Trusted nodes are randomly given the opportunity to become managment nodes if the network is bottlenecking (work out how to define this). Management nodes become demoted to trusted nodes if they are completing three standard deviations less transactions than the mean of the management nodes for a sustained amount of time. Management nodes get a "star" rating from both the workers and the clients which is declared to all management nodes and is made public. It is also made public if a management nodes ledger is not up to date for a sustained amount of time.
The network would be run by the trusted management nodes. These nodes will:
* receive, sort and match computation requests
* receive submitted geometry files
* iterate each new geometry file in order to obtain its computational cost (save this iteration and use it to test nodes)
* distribute random number seeds -- have 1% of these be duplicates that are being run on a trusted node -- each set of data paid for must have at least one duplicate seed -- all nodes (including trusted nodes) must have 1% their computations tested in this way by a trusted node.
* record the average (time taken to return a result)/(computational cost) for each node being managed
* receive and hold onto exit particles until worker is paid
* contain the ledger of all the node labels and management node ratings -- the majority ledger is declared true
* trusted management node would compete against other trusted management nodes for management fees paid -- there is an upfront management fee to cover the computational cost of running the management node -- and a 1% fee eventually paid to the workers for running "duplicate tests".
Have CompuShare contain within it the Bitcoin hashing code so that idle GPUs will return to hashing.
Clients would submit their geometry and initial particle trajectories to a management node of their choosing paying a small upfront management fee declared by the management node, at this point the client could choose "cheapest" or "fastest". The management node would then calculate the computational cost of this geometry and declare its "price-per-computational cost" to all the listening nodes. Listening nodes would make their request to run the code and declare how many cores are available in their GPU, they would also have the option to be paid less should they wish. The management node will then assign a given number of iterations to each node. These would be weighted towards being an integer multiple of the number of cores at each node. The method of assigning would either be cheapest then fastest or fastest then cheapest depending on the clients choice. Once the exit trajectories are computed they are encrypted and returned to the management node. At this point the management node declares to the client the payment address and amount due to the worker. The client pays the worker. The worker declares to the management node that they have been paid. The management node sends the decrypted then re-encrypted results to the client.
@_date: 2013-05-20 21:19:45
It's more to do with the "take up ability". If it is all being run through me, or a server of mine people would be much less willing to run the code.
If I want to do something like this, for the best uptake by GPU bitcoin miners I would need to make it decentralised.
If the people own the network, they are more likely to back it up.
@_date: 2013-05-20 14:13:54
You have a point... :(
You can't do part of the code with homomorphic encryption because then they would just cheat for everything apart from that.
Cheating is the biggest hole. I figured I had got around that by making it a binary blob with encryption inside that. But people could still view that code and find a way to break it.
So you need to make it homomorphically encrypted as you stated. But that unfortunately is very slow. Slower than effective.
Is that a killshot to the idea... probably... will keep thinking about it. That is an unfortunate hole.
@_date: 2013-05-20 11:46:52
But you are right, it may just end being better not making it a plug in at all... but a separate "miner"...
Would be easier...
@_date: 2013-05-20 13:32:54
Thanks for the tips. Would it work to have a form of encryption written into the "hidden code" sent out, if someone made up solutions when the payer decrypted the result it would be nonsense? Enough nonsense submitted, then banned.
If code has to be run multiple times over it loses its cost effectiveness in the market...
@_date: 2013-05-26 13:20:42
Cheers :) expect a PM in about 3 - 6 months. :)
@_date: 2013-05-20 21:21:21
Yehh, definitely, much much better. :)
Would this work:
@_date: 2013-05-23 07:11:21
And yes, all code would be released. So definitely, other people could take this code and implement it in another way. If it's better people would use that instead :)
@_date: 2013-05-20 21:21:55
Would this work:
@_date: 2013-05-24 02:05:02
nvidia and an american uni are working on making it run on GPUs. But this will take ages. I am thinking of changing tact.
Using WebCl instead. Running it through researchers websites.
@_date: 2013-05-20 12:15:49
You are correct. It does confuse it. My hope is that it would eventually be taken on as a "miner" of sorts by the bitcoin community. This was my initial purpose for that wording, but in reality you are correct it just makes it more confusing.
@_date: 2013-05-23 00:19:32
Centralisation would be easier on the programming side. But I would really like to open up the market place. If just one person controls the central management server then the whole thing is limited by them (and I would have to upkeep hardware). I am studying to be a medical physicist. I want this network in place so that I can pay for it, not so that I can upkeep it.
Also, a network that is owned by the nodes is more likely to have code contributed by the nodes. The decentralisation would create ownership by the community and no one person would need to do hardware upkeep to keep the whole thing running.
@_date: 2013-05-20 12:32:50
It would be inside a sandbox. You would be paid to run it. And the only privilege the code would have is to send a final encrypted result back to the payer.
@_date: 2013-05-20 12:52:26
Any result that is not formed from the binary blob would be nonsense when decryption, given that the encryption would be embedded within the code. Therefore no payment would be made.
The sender of the result would also imbed the bitcoin wallet address as part of the decryption and declare that "I did this, pay me". If they did it, then the combination of the programmers key and the bitwallet should make a valid result.
@_date: 2013-05-20 14:31:27
Okay :), I surrender. Thank you...
Seriously though, appreciate the time you spent explaining this rather gaping issue. I learnt a lot :)
@_date: 2013-05-23 21:40:31
Or even just a tester while I play with a few things?
@_date: 2013-05-26 13:25:04
Been a few decent updates (and simplifications):
@_date: 2013-05-20 21:25:45
What if this was the way I did it:
I really love cryptography, not extensive knowledge, but I have 12 months to focus on this if it is possible, to get it started.
I am also still in contact with a few of the maths researches at my old uni, and could ask them for help when it comes to some harder problems.
@_date: 2013-05-20 13:49:50
This would become much more worth it though when 10 000 pick up your code as you can often only pay for hourly lots, and this way you would have your code completed in 2mins still at $10.
By the sounds of it though, I may be barking up the wrong tree. Start by keeping it centralised. Aim to have the code run on GPUs only (not CPUs), keep the MC being processed closed source and put the "anti-cheating" inside there...
@_date: 2013-05-26 13:23:55
Sorry for bugging you again...
How does this sound:
@_date: 2013-05-23 07:58:34
Simulations would be paid for enmasse by hospitals and physics researchers.
@_date: 2013-05-20 21:29:38
Awesome. Thank you.
@_date: 2013-05-20 21:27:24
More like a midnight disheartening... 
@_date: 2013-05-23 07:07:23
Yehh, not sure why they did that. I shall ask the writer of the paper. Maybe amazon doesn't sell GPU?
@_date: 2013-05-20 21:14:02
Does this address the issue?
@_date: 2013-05-20 21:30:26
What do you think about this:
@_date: 2013-05-20 13:35:14
The aim of this is not just for one researcher to be able to submit code, instead for anyone to be able to submit code.
I don't want one person having all of the computational power, instead anyone should be able to pay for it.
The aim is to make computation tradeable.
@_date: 2013-05-20 12:37:13
Yup, I have lots of friends who are exactly this :) And already doing it :) thanks for the tip :)
@_date: 2013-05-20 11:47:58
Exactly, it's funny trying to talk to both sides of the people here. I am talking to researchers and they are saying "would people really run their computer and just be paid the price of the electricity?"
@_date: 2013-05-23 07:09:21
Part of the job of the management node is to receive the results and then hold onto them until the client pays the worker. The management node escrows the payment.
@_date: 2013-05-23 08:34:38
Also making the code work p2p is the easy bit. It is as parallel as they come.
It's the general network set up that will be difficult.
@_date: 2013-05-21 05:43:26
Bugger :/ and thank you. Didn't know that.
@_date: 2013-05-21 00:00:12
Unfortunately, if someone had a botnet, there is a chance they could verify themselves. The data needs to be able to be trusted.
@_date: 2013-05-20 11:38:57
Yes, but it was money going to coinlab. This would be open source, and the money would go to the miners.
If you could earn extra bitcoin with CPUs at above electricity prices, then people will have CPUs as well.
At it would be set up in much the way miners are paid fees for transactions now. It would happen in an auction format.
(And people would use CPUs on purpose to leverage this. As a result it would be very useful)
@_date: 2013-05-23 02:47:05
In the case of Amazon, users can pay for many cores, but the prices per core is rounded up to the hour. Purchasing 1000 x 500 cores and paying for them for an hour when you only use them for a minute would be expensive.
@_date: 2013-05-20 13:48:10
Hmmm, it would be really nice if it was decentralized... Maybe though it would be worth to make a working code centralized and then battle that beast later...
@_date: 2013-05-20 12:02:49
Yehh, thank you. I understand how to implement Monte Carlo in a CPU fashion.
Geant4 is what I will try and have as the client (researcher) side front end...
So CPU it is.
edit1: I have been convinced otherwise -- 
@_date: 2013-05-23 21:32:32
Upon thinking about it, I actually think this is a much better idea. So simple and can be made to work more elegantly.
@_date: 2013-05-20 11:32:09
Researchers would be definitely willing to pay for electricity + computation time if it meant they could get instantaneous results.
In a situation I know of, LinAc's take months to have their radiation levels approved. These are expensive:
Every day they are offline is a lot of money down the drain in possible cancer treatments for patients.
If this medical physicist could pay for the computation instead of wait days, they would very happily compensate.
And, they would also have their computer have many days of downtime, which they could use to store up more bitcoins for themselves for later calculations.
edit: spelling
@_date: 2013-05-23 07:52:09
This actually benefits my ends. As bitcoin mining on GPUs becomes less profitable of there was another option of profitability available the current GPU network would very happily begin to implement it.
@_date: 2013-05-23 02:41:21
Yup, you have explained it spot on in a sentence. :)
@_date: 2013-05-23 07:49:06
Due to the massively parallel nature, any of this kind of scientific problem will by definition benefit from this kind of network.
This is still just a proposal. But it will offer massive benefits to clients. Every new patient that undergoes radiation cancer treatment must have their treatment simulated via Monte Carlo in this way. If that patient could be treated a day earlier that would be worth a lot. There are hospitals that have very little hardware that would greatly benefit from this worldwide. Especially third world countries.