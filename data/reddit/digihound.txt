@_author: digihound
@_date: 2013-04-17 14:36:38
That's about what I'd expect. The GTX 680 has eight SMX's, the GTX 660 Ti has seven. Toss in variable clock speeds depending on your vendor, and that's a fair ballpark.
@_date: 2013-04-16 21:34:17
Where? 
I've looked before. I know ASICMiner is going to have another sale. I'm not disputing that some vendors have shipped some hardware to large private buyers -- but I have yet to see a single retailer selling hardware on anything but a piecemeal basis. Typically with significant assembly required even then. 
If you know of someone selling plug-in boxes (not pre-ordering them), please share. :) 
@_date: 2013-04-16 21:49:53
Using GK104, I saw an increase of ~107 to 126, but performance is variable. CUDA hash rates 'bounce' from a low of 107MHash (same as poclbm) to the occasional 143 and 178Mhash. 
I allowed the kernel to self-configure. No special flags, save for the aggression level (7). You may need to install the CUDA 5.0 SDK (available from NV). 
@_date: 2013-04-16 22:58:19
Fuluffel, 
Good catch. Fixing that. 
@_date: 2013-04-16 19:39:50
The CUDA kernel linked in my story is based on that work. It's not the old one from years ago, but a new one (created about 9 days back) for Kepler. And it works under Windows.
@_date: 2013-04-16 18:24:58
As the author:  The point of the article isn't "Should you buy a ton of GPUs right now in an attempt to cash in on BTC mining?" The point is, "Why do NV cards lag GPUs, and can that gap be closed?" 
In the long term, ASICs replace GPUs as dedicated Bitcoin miners. But there are plenty of tech enthusiasts with discrete graphics cards who are also curious about BTC mining. 
@_date: 2013-04-16 21:29:07
I actually wrote my story because the BTC Wiki entry had gotten a bit outdated and I wanted something better. ;) The VLIW stuff doesn't apply to GCN and the number of shader ALUs isn't the issue. 
Kepler has far more shader cores than Fermi did, but BTC mining performance went backwards. That's one of the major points I explore. ;) 