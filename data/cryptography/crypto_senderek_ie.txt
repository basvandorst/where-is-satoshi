
@_date: 2013-12-15 21:19:05
@_author: Ralf Senderek 
@_subject: [Cryptography] The next generation secure email solution 
If you are so sure, can you tell us how the next generation secure email
solution will solve the "trust problem", please. How does the p2p daemon
find the correct crypto key, so that every user can rely on its invisible
      --ralf

@_date: 2013-12-17 15:03:37
@_author: Ralf Senderek 
@_subject: [Cryptography] The next generation secure email solution 
This is an interesting idea, because it provides certificates on demand for ordinary users, if they decide to sign up to a certain site. The
certs are then being used for other purposes, so the site does act as a
bootstap for using crypto. The one thing that this proposal relies on is
the availability of a common piece of software (user agent) that stores
the private key for the user. It's this part of the picture where things
get tricky.
                --ralf

@_date: 2013-12-17 21:35:02
@_author: Ralf Senderek 
@_subject: [Cryptography] The next generation secure email solution 
Doesn't that open the door for a DOS attack? By which means does the site
that maintains this list decide which certificates are valid and which are
not? Are we relying on a global PKI for this? The benefit of your proposal
was, that two relatively inexperienced users are able to perform the
initial steps of a trusted crypto relationship without having to trust
another third party except the one that issues them certificates. The MITM
check will expand this model into something, I cannot clearly define at
the moment, but seems to lead back to the (broken) system.
            --ralf

@_date: 2013-12-20 14:37:56
@_author: Ralf Senderek 
@_subject: [Cryptography] The next generation secure email solution 
I am looking into the future: Secure Email Identities (SEI)
The next generation secure email solution based on
eccentric authentication may look like this:
A user finds some website interesting and trustworthy and
signs on. He gets a client-side X509 cert from the site that is stored on his local computer (including unprotected
private key) under a globally unique name, that can be but
needn't be his own.
Users need their certificate to log into the website that
has issued the cert, there are no passwords any more and whoever
*has* the unprotected private key *is* the person and can
use the SEI. The user's local machine (the endpoint) is secure.
Any website can create these certificates for users as long
as SEIs are globally unique but only for local users.
The global-uniqueness of a SEI can be verified by checking a
distributed list of hashes, that enables anyone to see, if
a certain site issues unique SEIs or not.
Any duplicate SEI is proof of a problem and disqualifies the
site as a trustworthy CA issuer, and therefore invalidates
all user's certificates from the time a duplicate is detected.
It basically kills the site, as users need certs to log in.
So every web site has an incentive to do proper signing and
SEI verification before signing.
How does a user get a key?
Read: How does the software on the user's computer fetch the
correct recipient's public key without the user noticing what
happens behind the scenes?
The user knows the recipient's SEI, it's like an email address
stored in the contacts list. The machine searches for the
recipient's public key in the local key store, which contains
all keys for SEIs that have been used before. (like ssh's known_hosts file)
In order to accept a key to be stored in the local key store there
is a preliminary check with the global register of certificates,
where the uniqueness of this SEI is being established.
All SEI addresses that are not locally stored can be used in
the initial trust-establishing dance, involving the global
How can a Man-in-the-Middle attack be mounted under these conditions?
1) The classic attack is the case where the cert is not signed by
    the website's CA but the MitM during the sign up step.
    An attacker like the NSA can use quantum servers to take over the CA
    issuing process for a number of users as they have the ability to react
    to the https requests much faster than the legitimate web site can.
    This is certainly possible, but it would not weaken the encryption
    as they cannot replace the public key with some of their own.
    The user agent would notice and would complain at the registry.
2) If someone who is not the recipient, replies to my encrypted message,
    I need to verify the sender's identity (via a mandatory signature).
    The user agent has to perform sign and encrypt on every message.
    No problem.
3) An attacker induces false certificates with the attempt to ruin the
    website. He creates a fake CA and creates duplicate certs for existing
    users and sends them off to the global registry. As the registry is
    designed to accept all submissions, a number of SEIs will soon
    be unusable. The registry has no means to distinguish benign from
    fraudulent certificates, because the web site CAs are not signed by a
    root key or bound to any old-fashioned PKI.
    As I see it, there is no protection from this kind of malice.
And if the endpoint (local machine) is the place where the private key is
stored we will soon have another pile of SEIs becoming unusable, when laptops
get stolen, hard drives die and local OSes become unreliable without proper
backup procedures. This will happen.
So in the end we will have a mix of working addresses and dead addresses and
nobody can tell the difference.
            --ralf

@_date: 2013-12-22 13:53:29
@_author: Ralf Senderek 
@_subject: [Cryptography]  RSA is dead. 
Isn't the most obvious conclusion that no crypto tool can be secure if it
is not open source? Even if there is no guarantee that the code is
actually being scrutinized, the alternative - trusting the experts - is not really an alternative, if you cannot check what's going on.
                   --ralf

@_date: 2013-12-23 08:56:01
@_author: Ralf Senderek 
@_subject: [Cryptography] Why don't we protect passwords properly? 
And if someone dared to replace fast hashes with bcrypt or better, the
the uninformed user would wait for his password check a whooping second
instead of nanoseconds and will certainly think the site has a technical
problem and run to the competition.
             --ralf

@_date: 2013-12-23 09:40:20
@_author: Ralf Senderek 
@_subject: [Cryptography] RSA is dead. 
Of course open source is never a guarantee, I didn't say that. We should
not confuse a necessary condition with a sufficient one. But the RSA (Inc)
marketing implied that closed-shop trusted expert crypto is superior to
open source crypto products. And that is certainly false.
As Peter, Dirk-Willem and Jerry rightly pointed out, it is very difficult
to find crafted backdoors even in open source products. But just because
something is difficult, that doesn't mean it should not be done.
With open source it can be done. But some essential changes are needed.
Those who have the ability to check crypto code must be actively engaged
by the community / society. If there is no incentive nor any substantial
acknowledgement of this important work, if code audit is mainly seen as private activity with no financial rewards, then yes, we can forget
                --ralf

@_date: 2013-12-26 12:24:47
@_author: Ralf Senderek 
@_subject: [Cryptography]  The next generation secure email solution 
I am looking into the future again: (Email Encryption Preferences)
Phillip Hallam-Baker had proposed three pieces of change to make
email secure, I will have a closer look at the one piece that
IMHO has the most impact on future email security, the proposal
that a (receiving) user can declare his email encryption preference that will be honoured by the email transport infrastructure and not
the sender's email program.
Alice, the geek, has all crypto gear in place to decrypt X509 and PGP
encrypted email, while Bob, the sender, does not use anything to encrypt
on his endpoint (a smartphone) he might not even know what encryption is.
The fact that Alice declares her preference to receive PGP encrypted
email will be enough to trigger an encryption process done on the email transport infrastructure, very soon after Bob hits the send button
on his smartphone and the email goes out unencrypted.
How can this be done?
Bob uses the email address alice at example.com in the usual way, his clear
text email hits the first MTA. How does this MTA know reliably, that Alice
wishes to have her incoming email PGP encrypted? I can imagine the following
The MTA queries the website "example.com" with the URL
wget  at example.com
If unsuccessful, it will try http. If unsuccessful, it will send the unencrypted
email off as usual. That's what we have today.
If the query is successful, the MTA expects a (formal) message signed by the key
that is appended to the message. It verifies the signature and gets to know Alice's
email encryption preference. So anyone who is capable of storing a certain
message, with a self-signed key, in the file system of the domain's web
server can define (and change) Alice's EEP. If Alice runs her own web site
this is the most comfortable and secure way for Alice to express her
encryption preference to the world. From now on, encrypted emails are piling up in Alice's inbox, none of which have been encrypted by the sender himself.
This scenario allows for certain (easy) MitM attacks, but it spreads the declarations
over a number of web sites, avoiding one single key store. And if Alice wants
X509 encrypted email from next week, she signs and stores one single message
on her own web site, that's all.
Possible issues:
1) Alice wants her email in plain text, but someone stores a pre-fabricated
    preference message on Alice's web server, locking her out from reading her
    email.
2) The MTA has to decide which public key is to be used  without the help of
    any PKI, except the EEP messages. But this decision can be supported by some
    register, similar to the one we have already envisioned for the eccentric
    authentication model.
3) MTAs cannot be extended to support the new feature. What does it take
    to change MTAs like postfix, exim, sendmail etc? Is this a possible
    way to go that has any chance of success?
              --ralf

@_date: 2013-11-02 16:30:43
@_author: Ralf Senderek 
@_subject: [Cryptography] PGP Key Signing parties (Trust Link Grid) 
I can understand that you confront my proposal with the established CA-model of
hierarchical key verification. But my proposal does not intend to provide what CAcert
The Trust Link Grid proposal is an invitation to think outside the box to achieve something
the box does not provide. BTW I know the box inside out, as I ran a CA in a former life myself. There is (or at least easily can be) a huge amount of first-hand knowledge about
PGP keys that is currently inaccessible and therefore wasted. Bringing this first-hand
knowledge online in the form of trust link statements can help to create trust in keys
that otherwise cannot be linked to someone I know (via key signing parties) in the usual
In regard to the worthlessness of public statements on websites that may put the reputation
of the issuing individual at risk, I stick to a totally different point of view.
Before answering questions what can go wrong, I'd like to point out what you can gain
from the initiative.
Anyone who wants to create trust in his PGP key can collect trust link statements for his
key from nodes and can publish them on his own website. Of course one such statement
means nothing, as it can be the result of a conspiracy with one node. But with every
additional trust link statement (there are four nodes in easy reach of an individual)
the risk of a conspiracy reduces. There can be cross links between more distant nodes
and any of them add to the reliability of the key with their own TLS.
How would anyone succeed in establishing a faked key and trick the public into using it?
1) create a faked public key for the victim (easy)
2) hack the victim's website and replace all trust link statements with forged ones
    that verify correctly with all the right keys the nodes are using (requires conspiracy)
3) make sure that the victim does not upload an additional trust link statement from
    a different node as this would make the plot detectable.
Seems reasonably difficult to me. On the other hand it's easy for anyone to examine the
trust link statements online to make an informed decision whether or not a key can be
trusted, without having a direct trust chain (via key signing parties) which might not exist.
Worthless? Not at all.

@_date: 2013-11-23 13:30:13
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Yes, but it's about time we do something about that. Do we *exactly know why* it is such a failure?
People are using the internet, they are typing sensitive information into textareas and send them off, and on the other hand "/usr/bin/gpg" is installed on almost every server, why can't we make Johnny Average use it?
With the Web Encryption Extension I am going down that road.
    -- Ralf

@_date: 2013-11-23 14:19:30
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
this is an interesting proposal, worth to be examined.
Why don't we use openssl and gpg, the crypto is already there and tested,
but we have to make ordinary people use it, which is far more complicated than coding another piece of software.
This is essential because trust in keys comes from different kinds of facts, the best of which are not electronically induced.
For some people it's an automatic CA sig check they don't understand,
for others it's some first-hand information and for some it's a handful
of Trust Link Statements from people who are unlikely conspiring.
Why can't anyone simply get a globally unique key identifier (ADAEXA-G4UKAN-UADASXA-JQAGBS-XAA) from an online service and put it into
"gpg --genkey" or  "openssl req -new -key x -out y" to create his keys,
maybe with the help of an online application?
Regardless of what the proxy does to the relayed email, you have to make sure that the plain text sent is perfectly safe, so at least you need to
have TLS to your proxy. So you'll need trusted keys for that before anything happens.
Trusted public keys on the proxy server.
Policy verification relies on trusted public keys, so the proxy encrypts with keys legitimated by the key corresponding to the fingerprint in the recipient's email address.
The ordinary email user will be forced to sign policies to
ensure proper delivery of an email he could encrypt himself?
Is complicating things really helpful here?
Which will essentially be the relay proxy of your choice.
But should the ordinary user not be in control of the policy he uses?
What mechanism allows the legitimate user to influence policies and at the same time makes tinkering with the stored policies impossible? Don't we
use trusted public keys already?
     -- Ralf

@_date: 2013-11-23 21:45:18
@_author: Ralf Senderek 
@_subject: [Cryptography] (no subject) 
You seem to have identified the three reasons for the epic failure of
email encryption:
1) The concept of asymmetric key is unexplainable to your granny.
    Despite the fact that your granny is perfectly capable of making
    informed decisions in the similar complex fields like financial
    transactions and the assessment of her cancer treatment options,
    it's a pointless excercise to explain the concept of a public
    key in a comprehensible way.
2) All cryptographic software is designed by geeks and ridden with
    complexity.
3) Every secure email encryption has to take place on the grannie's
    laptop, not a well-maintained secure server.
I doubt all three assumptions.
    -- Ralf

@_date: 2013-11-25 13:08:37
@_author: Ralf Senderek 
@_subject: [Cryptography] Email is unsecurable 
Agreed, and you haven't even brought CRLs into the picture that nobody finds nor checks.
Today, the GUI may well be the browser, it's not clear to me that PGP
must fail here. Once we move encryption to the server, the GUI thing
becomes less important and some complexity evaporates.
While I wholeheartedly follow you on S/MIME, I think it's a bit unfair to PGP which was designed as a method to protect messages. It was used to protect email, but never promised to secure anything outside the message.
Put your subject in the message and leave the subject line empty. To secure email the way you wish, you'll need more than just encryption.
I hope the final word about that isn't spoken, yet. But waiting for the
big players to do it, certainly won't help as privacy doesn't fit into
their business model.
     --Ralf

@_date: 2013-11-25 13:29:29
@_author: Ralf Senderek 
@_subject: [Cryptography] Explaining PK to grandma 
IMHO the analogy does not explain encryption, because grandma's vision will be a box that she locks with a key, that's familiar.
But unless you can convince gandma that there is a box that you lock with
one key and open with another, she hasn't understood anything about RSA.
And the most difficult thing is still left to be explained: how does the key she has used to lock the first box get to the recipient? Even if
grandma is willing to use the new double-key box for this transport,
it'll dawn on her that she needs one of the two keys to lock it.
And getting that key is the problem. If she is tricked in using the
wrong one, somebody else, not the intended recipient is able to open it.
At that point granny will be crying out for something simpler, and we have to
tell her that we cannot make it simpler.
         --Ralf

@_date: 2013-11-26 15:37:42
@_author: Ralf Senderek 
@_subject: [Cryptography] Explaining PK to grandma 
Grandma wished it were that easy, but it isn't. With only public key crypto at hand, grandma gets some close key and has no means to check,
whether they have been sent by the one claiming to be the owner.
She is totally unprepared for the task to pick the trustworthy closing key out of the pile of keys she gets from everyone.
That's exactly what happens at a PGP key signing party, but grandma doesn't attend such stylish events.
      --Ralf

@_date: 2013-11-27 16:03:19
@_author: Ralf Senderek 
@_subject: [Cryptography]  Explaining PK to grandma 
Imagine Granny has a little box next to her computer that does all
the nasty crypto stuff she does not need to know about. Let us call
it the crypto pi. All she can do is plug a memory stick in to feed the
box some texts and pull another second memory key out to carry her
encrypted text off to her lappy.
What does the crypto pi have to do?
1) generate a RSA key pair for her, store the public part on the
    (output) memory stick.
2) check for new text on the input, try to find the public key for a
    recipient.
    Granny only says who it should be, giving an email address.
3) If found, use the public key on the text and write the encrypted result
    to the other memory stick. Inform Granny that the encryption is ready.
4) Check the input stick for new encrypted texts Granny might have stored,
    decrypt them with the private key inside the crypto pi.
    Granny does not know it even exists.
What's left to do for Granny?
1) Give her "thing" (from the output stick) to everyone who might send her
    secure mail.
2) Store incoming "secure mails" on the input stick and feed it to the
    box.
3) Store her messages on the input stick under the name of the intended
    recipient's email address.
    (finding the trustworthy pubkey is the pi's job)
4) Send the encrypted result to the email address.
I'd argue that even if such a box existed, finding a trustworthy public key to a given email address is not something we can take off of Grannie's
shoulders and delegate it to the box.
So there is another task for Granny:
5) Do good key management for the box.
    And this cannot be done without knowing about the risks and taking
    appropriate action.
    --Ralf

@_date: 2013-11-30 19:01:35
@_author: Ralf Senderek 
@_subject: [Cryptography] Can a machine do trusted public key management? 
As I see it, this is a common belief on this list. But the conclusion has
- IMHO - not really been thought through to the end.
Can the universal crypto box (UCB) take the responsibility from its users to
perform proper key management? I don't think so.
If putting the USB stick into the proper hole is all a user needs to do, then
the private key will be put in action by this event and the RSA decryption key
needs to be stored in plain text inside the box. So the UCB's user has to guard
his box carefully not to let anyone else stick some USB key in the box while it
is active. PRO: The user cannot forget a passphrase, the risk of key revocation approaches zero. CON: It's easy to get forged messages signed and to decrypt sniffed cryptograms, as no passphrase is involved.
If inserting a USB stick is all a user has to do, how will he/she decide who's public key is to be used? All the user can do is create an information on the
USB stick that says: "I want this text and attachment be encrypted so that I can send
it to this email address: xy at abc.com". The UCB now has to find the correct public key.
With no further intervention from the user, the box has to solve "the trust problem".
If at all, this will only work, if all users have registered their public key within
a perfectly working global PKI. At least a user has to understand that without some
setup, maybe done by experts for him, his box won't work and that it will cost him some bucks. The user will also need to understand that any recipient that isn't part of this game cannot be reached. A great incentive to work on his pals to sign on to this service.
Only in light of such assumptions you can claim that explaining PK crypto to a user is a waste of time.
If - on the other hand -  we burden the user of the UCB with the job of proper
key management, that does not necessarily mean that he needs to become an expert.
                       -- Ralf

@_date: 2013-10-31 17:17:33
@_author: Ralf Senderek 
@_subject: [Cryptography] PGP Key Signing parties (Trust Link Grid) 
I'd like to fuel the interest in a more closely knit Web of Trust for
PGP keys with a bit of history.
In 1998 a book was published (in very small numbers) that contained a few hundred
people's public keys with their fingerprints, mainly from academic circles.
It was called "The Global Trust Register 1998". The book certainly helped me to establish
a trust chain to the PGP key used to sign the PGP source code at the time.
As the book put certain prominent PGP keys in print it helped to make circulation
of faked keys more difficult. But it certainly was not able to provide an infrastructure
that anyone could use to gain first-hand-knowledge of PGP keys.
In 2001 I discussed a proposal with Ross Anderson that might have closed that gap
but it was not being advanced at the time, so I'll describe it briefly here for evaluation:
    The Trust Link Grid (TLG)
    "The initiative is based on the assistance of a number of volunteers to make sure that
    a reliable public PGP key is in reach of 100 Km globally, by establishing a grid of nodes
    that publish first-hand-knowledge about PGP keys, that cannot be forged easily.
    The TLG should provide a solution to this problem:
      "How can anyone gather enough evidence based on non-electronic
       first-hand personal knowledge to be sure that a key of whatever
       kind is really used by a certain individual to the best of
       the knowledge of those who published the first-hand information."
    I hope it will be possible to encourage individuals to act as
    a "Trust Link Node", as a contact person for others to confirm
    some first-hand information to them. I don't know how many
    volunteers would be needed but to cover Britain twenty individuals
    would make sure that a reliable key is no more than 6o Km away.
    Each node creates a Trust-Link-Key and verifies his key to the
    node in the north, east, south and west. That takes no more than
    400 Km of driving each, leaving a "Trust-Link-Statement" with
    every person contacted.
    Every node publishes the TL-key together with at least four
    TL-statements on a website, which confirm that there had been
    a personal contact and a key verification procedure that meets
    certain standards (like A-level keys in GTR).
    This should not absorb too much energy. And with every additional
    personal contact (at conferences or whatever) between two volunteers
    a node can collect new TL-statements to be published on their
    local website as well." (Jan. 2001)
This decentralized approach does not need a globally agreed-on standard
in contrast it only uses signed ascii messages stating first-hand knowledge
in plain english, that has been gathered without electronic means but is
verifiable by checking the TL-statements for any path you like.
The trust that lies in the grid is founded on the risk that a publicly stated
first-hand knowledge by a node turns out to be false. That would harm the online
reputation of the individual that is running the node, as there are always at least
four more independent TL-statements from other people for each such information.
   "The main purpose of the initiative is risk reduction.
    Unlike normal signatures on keys trust link statements have
    semantics they state facts that make it risky to cheat
    both for the nodes and for the local individual.
    The reliability depends on the consistency of the system,
    that some fact is independently confirmed by others whose
    keys can be verified in a similar way, relying on other keys
    which in turn have a number of independent verifications." (Jan 2001)

@_date: 2014-04-05 22:42:44
@_author: Ralf Senderek 
@_subject: [Cryptography] OpenPGP and trust 
You can easily solve this problem by obtaining a certificate that verifies
in almost all browsers for a few bucks per year, don't consider to use
a self-signed cert and as a consequence open up unencrypted connections
to your OwnCloud server. Instead you should configure your internet access
as HTTPS only !
Another reason to serve this content exclusively via HTTPS. Your non-technical users won't have any issues if you get an "ordinary"
X509 certificate for your web server.
No. This may be as bad as using HTTP for logins.
and that means signed PGP keys, which are called certificates as well.
This is the wider group.
In this core group you only have to throw a key signing party once and
make sure that the call-sign is in the name-part of the key, where the
email address is located under normal circumstances. You'll only need
name and call-sign in the key ID.
You'll do this with your signature under the core group member's PGP keys.
To extend this trust to keys that have been signed by Bob, there's two
things you have to ensure.
  a) people like BOB (core group) are bound to check the name/call-sign
     on every other key they sign with a proof of a valid license.
  b) your server needs to check the trust chain to allow a login.
You might restrict the trust chain to two hops as you might not be able to
ensure that Alice adheres to a) if she signs Carols key.
The key signatures "certify" the name / call-sign connection nothing else.
       --ralf

@_date: 2014-04-06 12:43:47
@_author: Ralf Senderek 
@_subject: [Cryptography] OpenPGP and trust 
Yes, given the context in which I wrote this (non-tech folk scared by
self-signed cert) your argument is pretty sound advice to leave
unencrypted logins to OwnCloud open as an option. Congratulations.
       --ralf

@_date: 2014-08-27 10:04:51
@_author: Ralf Senderek 
@_subject: [Cryptography] Encryption opinion 
very true, and it shows that it takes more than technical
changes to ensure protection against misuse of credentials,
no matter where in the process the middleman acts.
 	Ralf Senderek

@_date: 2014-12-12 17:36:34
@_author: Ralf Senderek 
@_subject: [Cryptography] North Korea and Sony 
CAPSICUM _is there_ for BSD and Linux for more than a year, but still has
not got enough traction so that applications really use it. In principle
capsicum provides a means to sandbox certain parts of an existing
application, but the application itself would have to be rewritten to use
the new features. Your email client example should be an interesting
candidate for such a rewrite.
In their one year old paper the University of Cambridge Computer Laboratory state:
"We hope to kick off a new batch of application adaptation in coming months ? as well as integration with features such as DNSSEC. However, we also need your help in adapting applications to use Capsicum on systems that support it!"
Looks, like the necessary help hadn't been there.
       --ralf

@_date: 2014-12-17 12:31:07
@_author: Ralf Senderek 
@_subject: [Cryptography] Any opinions on keybase.io? 
That won't help Johnny, because if encryption has to be transparent
to Johnny you claim (falsely IMHO) that he should have nothing to do with
ciphertext or keys. In this case someone else controls the encryption
key and can invalidate the encryption even if it actually happens.
The mere availability of an "off switch" should make Johnny nervous.
Without Johnny controlling (at least part of) the encryption key there is
no assurance of security for Johnny and that's why it cannot happen to
him transparently.
      --ralf

@_date: 2014-01-22 19:56:00
@_author: Ralf Senderek 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or encrypt-then-sign? 
In 1996 W. Unruh explained another good reason to avoid signing ciphertext
in his paper "PGP Attacks". Here is his reasoning.
    Chosen Cipertext Attack:
    An attacker listens in on the insecure channel in which RSA
    messages are passed. The attacker collects an encrypted message c,
    from the target (destined for some other party). The attacker wants to be able
    to read this message without having to mount a serious factoring
    effort.
    In other words, she wants m=c^d.
    To recover m, the attacker first chooses a random number, r < n. (The
    attacker has the public-key (e,n).) The attacker computes:
    x=r^e mod n (She encrypts r with the target's public-key)
    y=xc mod n (Multiplies the target ciphertext with the temp)
    t=r^-1 mod n (Multiplicative inverse of r mod n)
    The attacker counts on the fact that:
    If x=r^e mod n, Then r=x^d mod n
    The attacker then gets the target to sign y with her private-key,
    (which actually decrypts y) and sends u=y^d mod n to the attacker. The attacker
    simply computes:
    tu mod n = (r^-1)(y^d) mod n = (r^-1)(x^d)(c^d) mod n = (c^d) mod n = m
    To foil this attack do not sign some random document presented to you.
    Sign a one-way hash of the message instead.
Signing ciphertext directly has long been considered to be a mortal sin.
    -- ralf

@_date: 2014-01-27 06:03:18
@_author: Ralf Senderek 
@_subject: [Cryptography] The crypto behind the blackphone 
While the list hesitates to draft a clear picture of the next generation secure communication solution, Phillip Zimmerman has the NSA-proof, comprehesive tool ready for sale, the blackphone.
According to the news here
the blackphone is "a Smartphone that?s been designed to enable secure, encrypted communications, private browsing and secure file-sharing." And it is based on an open source OS called PrivatOS, derived from Android.
Does anyone know more about the crypto behind the blackphone?
Is it a proprietary solution or an attempt to solve the real problem?
          --ralf

@_date: 2014-01-27 17:56:02
@_author: Ralf Senderek 
@_subject: [Cryptography]  The crypto behind the blackphone 
My question is not difficult to understand:
We have the "real problem" that although we have the proper tools, almost nobody is using them today. Some of us find that undesirable and try to do
something about that. We've come to the point that we need something new
but don't know exactly what it will be and if the "old tool" will be used
in new ways or be replaced by something entirely new.
And now Phil comes up with the blackphone, claiming that "Blackphone provides users with everything they need to ensure privacy and control of their communications, along with all the other high-end Smartphone features they have come to expect.?
If you expect everyone to subscribe to a proprietary, in-house solution to exchange secure messages, that would be easy. Solving the "real problem"
will require much more than using a special device. A real solution must
work on all devices in a way that users actually use crypto.
That's why I asked what crypto will do that job.
         --ralf

@_date: 2014-01-31 07:16:16
@_author: Ralf Senderek 
@_subject: [Cryptography] Hard Truths about the Hard Business of finding 
In order to reach that place, we have to hammer out a clear strategy
for progress. It certainly does not suffice if we simply donate our
time to check certain parts of the "security environment" when we
cannot turn this efford into a trust-building result for all of us.
Also, thinking in different directions is fine, but competing with
a bunch of different solutions for the same problems wont get us
If anyone is willing to share his or her proposal for a joint security review strategy, I'd be all ears.
       --ralf

@_date: 2014-03-16 15:57:51
@_author: Ralf Senderek 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
It seems to me that it might make sense to get an open competition
going to elect a process of building trust in crypto that actually
works in practice and gets us out of the situation we're stuck in today:
The challenge is this:
"Show me the whole practical process anyone on this planet can use to
have a secure online communication with someone else."
The proposals must not be reduced to technical specifications but need to
show how exactly we can achieve the results of trust building. In this
process individuals must play an important role. As a precondition the
process must be entirely comprehensible and verifiable, so that a variety
of smart people - including the President and the God King - can expose
themselves to say "Yes, I've checked this approach, I know of N capable
colleagues that I know have scrutinized the code and the inner workings.
I might be wrong, but I sincerely would recommend to use this to my wife."
The successful winner of this competition won't be perfect, it won't
guarantee that the NSA cannot subvert it, it wouldn't even guarantee
that it'll be widely used in practice, but it would be a foundation
for the mammoth task that lies before us, to take back the internet.
Including the personal aspects, the need for a reliable framework that
shows all checks have actually been done in a way people can understand
might make this approach a success. I hope this can help to get the
ship going again, and I'm sure others will have much better ideas how
to achieve trust in crypto. Don't keep them to yourself.
     --ralf

@_date: 2014-03-17 15:42:38
@_author: Ralf Senderek 
@_subject: [Cryptography] How to build trust in crypto 
Guido, I think the clear outline of your proposal makes it a valuable
contribution to the competition as we can now figure out what it would
take to implement your steps.
One of you key points is the role your/our blog/web site plays in
preparing the ground for the trusted channel between two site users.
As I see it, there is an important element missing, which is context.
With a few messages signed on your blogsite, all I have is a public
key but no context about the individual behind it. Can I trust the public key? I don't know, because of a lack of context.
Bruce Schneier's new PGP key (EDACEA67) has only one signature, it's
self-signed. If I found some meaningless comment or blog post signed with EDACEA67 at myblogsite, I would not trust this pubkey to secure
a meaningful, secure contact to Bruce. It is the context in which
the key appears, Bruce's website, that induces trust even if he deliberately abstains from using any kind of PKI.
Your focus obviously is on anonymity, but I think that in avoiding context, we don't get trust. The introduction of your website as an
intermediary should not just be used as a technical match-maker, it
should also be able to reliably tie context information to the public
keys you are using as identities.
      --ralf

@_date: 2014-03-17 19:01:05
@_author: Ralf Senderek 
@_subject: [Cryptography] How to build trust in crypto 
To see what it takes to establish a secure online communication
it's interesting to look at the way Edward Snowden finally convinced
Glenn Greenwald, the journalist who published the NSA files, to
use crypto.
It took him about six months. [1]
In the beginning Snowden knew he needed a secure channel to Greenwald
but Greenwald's laptop was clear of PGP. In his first contact Snowden
asked for Greenwald's PGP public key several times but without success.
Snowden was then an anonymous contact, an untrusted source, no reason to
go through the pains of installing PGP/GPG, even though Snowden prepared
a video tutorial for him.
Snowden didn't give up, he knew about Greenwald's skills as a journalist
and his courage and contacted Greenwald's friend Laura Poitras who had
experienced some pretty bad treatment at border control that made her
a well-experienced user of PGP. This was the context Snowden knew about.
Now Ed had Laura's public key but his encryption key was somewhat
suspicious to Laura, because she could not rely on verified context
information about Ed. The man behind Snowden's public key could as
well be a girl from the NSA trying to entrap her. The working secure
channel was one-way.
Laura based in Europe needed to talk to Greenwald, but she had no secure
channel as Greenwald didn't use PGP, so she flew back to the US to meet
him. When both met and looked at the emails, their untrusted source
had sent, a picture formed and Snowden began to gain trustworthiness.
The idea of an interview was born, four months after the first frustrating
contact, initiated by Snowden.
Then a parcel arrived at Greenwald's door containing two USB sticks that
eventually enabled Greenwald to boot a pre-fabricated security distribution,
TAILS, to establish a direct, secure channel to Snowden. Using this channel,
Snowden revealed the first PRISM documents to Greenwald, still busy to sharpen his reputation as a trustworthy source to the journalist.
If anything, this may help to understand that building trust is not just
following a protocol, not just having the correct information, but a process
in which crypto plays one (important) role that is by no means independent of the context around it.
     --ralf
[1]

@_date: 2014-03-18 18:26:47
@_author: Ralf Senderek 
@_subject: [Cryptography] How to build trust in crypto (was:recommending 
Certainly this is no foundation for trusted communication.
Not at all, if I wished to have a secure channel to Bruce Schneier and
I use key the key from his web site, I'd want that the one who is
reading my encrypted messages is exactly the one that creates all
the stuff that I get on the 15th day of the month. And I'd want that only
this one person can decrypt and that this is done in a secure environment
which does not expose my messages to others. Short of that I wouldn't call
this a trusted communication, because the reason I'd start the contact is
my belief that - given all the context I know about Bruce - this endpoint
decryptor will actually respond to the content of my message in a way I
can predict to be what I want it to be. And (please) let me call this
       --ralf

@_date: 2014-03-18 21:03:35
@_author: Ralf Senderek 
@_subject: [Cryptography] How to build trust in crypto 
Not for a second did I claim this, you're missing the point I made.
I stressed the point that the value of a key originates in its
context that I can verify. Your examples are very much valid when I
have other objections than reaching Bruce (the man himself) in a
secure way. I used this case to point out a real deficiency of the
proposed "trust model" that tries to create secure communication
without context. This is impossible if the objective is to get a
secure channel to the one intended recipient.
There are of course other objectives in which trust is less important
and anonymity matters, but only with anonymity and without context
we're not getting what we need most, trusted, secure communication.
And the UI you are referring to is part of it.
     --ralf

@_date: 2014-03-19 12:18:08
@_author: Ralf Senderek 
@_subject: [Cryptography] How to build trust in crypto 
Of course there is a big difference between trust and risk but primarily
it has to do with what we know (context) as a foundation for our
While trust - accumulated over time - defines what I can truly expect
to happen under normal circumstances as a result of past experience,
risk calculation focuses on what I don't know, and what the consequences
are, if things go wrong. Reducing risks is fine, and it helps to build
trust, but what we need for a trusted, secure communication is the
assurance that our expectation of a private conversation is real in
practice and not only a faint hope.
To achieve this it does not suffice to look at one part of the picture,
as risk assessment does (which is important without a doubt!) but to
construct a process that we can know to work as we reasonably expect.
A process we can trust, based on context.
Getting the correct PGP public key is a hard problem in itself, but
it's still only part of the problem to be solved when the decryption
key (for instance) is stored in plain text on an insecure endpoint
(smartphone) or when we don't have any idea how intensely the code
running there had been scrutinized, or ... or .. or ... and we still
want a reliably secure communication. Without solving the whole
problem we cannot get trust.
The bad news is, that we have to attack the complexity problem, there
is no way around it. We need to think about how we can reduce both the
code base and the algorithmic dependencies in our solution to be able
to reach the point where we have enough evidence (context) to trust
the process.
I will have a look at it, certainly with the focus on solving the
complexity problem.
        --ralf

@_date: 2014-03-20 12:11:19
@_author: Ralf Senderek 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Which brings us to the question how hard it is to turn the recipient into
an oracle.
Sending PK-encrypted ciphertext is not a problem in itself, the problem
starts, when the sender learns anything about the results of the
decryption and when he can turn that knowledge into a plan for submitting
new crafted ciphertext to learn more. Careless handling of decryption
results by users is not an oracle, the oracle that works is a build-in
mechanism that can be used many times because it's automated and
independent of user action.
And because of this nature it also applies to AES. I recall an assignment
in Dan Boneh's excellent online cryptography course, where the university
provided an AES implementation that could be queried as an oracle.
Students using this oracle could decrypt an AES-128 encrypted ciphertext
without the need to recover the AES-key, simply by sending crafted
ciphertext that probed the padding at a certain character position and
learned whether their ciphertext was accepted as well-padded plaintext
or not. At a rate of 256 oracle requests, they could decrypt AES one
character at a time. So implementation matters, and denying any
information about decryption success is a very good idea.
This would have had worked as well if the plaintext was random (looking).
To answer the above question, it might be necessary to look for built-in
mechanisms in any scheme (symmetric or asymmetric) that leak decryption
success. Good concepts are important, but we're only really secure
when we have (and understand) the whole thing.
       --ralf

@_date: 2014-03-22 16:42:41
@_author: Ralf Senderek 
@_subject: [Cryptography]  Tamper-evident cryptographic systems 
And is working.
We as users won't notice, they may or may not do it, it'll be "transparent" to If it is done securely, _we will notice_, because to reliably encrypt data
at rest the decryption key must come from the outside (from the user),
it cannot be stored on the server where the data rests. So in principle,
if our co-operation is not required to use the data at rest, it is not
reliably encrypted. This may be a sign to watch out for that can help to
disqualify encryption that won't protect users and is only security
theatre or marketing.
The focus has to be on systems, not algorithms. Any signature scheme must
be tamper-evident in theory, but in order to detect successful tampering
in practice the implementation details of the system as a whole have to
be evaluated.
It's certainly counter-productive to focus on one approach only, but I'd
like to know what the direction of the development to secure the internet
might be. Let me (artificially) construct an alternative:
Is it the decentralisation of the internet, working to enable more people
to take things into their own hands by using less-complex, better
analysed and smaller systems, like a cryptobox or other well-audited
end-point solutions?
Is it the transfer of encryption into the cloud / internet backbone on
the assumption that a user's end-point will never be secure and therefore
encryption must happen without counting on the user's responsibility.
Signals of tamper-evidence in systems will be quite different in both
        --ralf

@_date: 2014-03-22 18:56:43
@_author: Ralf Senderek 
@_subject: [Cryptography] Tamper-evident cryptographic systems 
Yes that can happen, I didn't say we have proof of a reliable encryption
when our co-operation is necessary. My point was, that without our
co-operation we have proof that something went wrong. (sign of Today there are widely used web applications that store decryption keys
(or passwords for databases) in plain text on the server to save the user
the trouble to provide the essential part for protecting his data every
time it's needed in order to "improve" the user experience. This is not
(yet) considered as a clear warning sign.
           --ralf

@_date: 2014-03-26 09:01:41
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
It's not an inherent property of a cloud to be insecure.
As always, it depends on how you do it. As some servers
are much more secure than some end-points, putting all
encryption into the end-points may be a bad idea (TM).
       --ralf

@_date: 2014-03-26 23:19:11
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
I fully agree, but that only means that every attempt to strengthen the part(s) of the cloud where both interest match has to be made.
And I don't see why it should not be possible for users to take
control over their own part of the cloud.
       --ralf

@_date: 2014-03-27 12:22:08
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Let's be specific:
If you use strong encryption on the cloud server and the decryption
key _is not stored on the server_ but comes from outside (when needed), travelling to the server via a (working) TLS and the cloud server is
a dedicated machine in a data centre, why can't it be secure?
As we know, security is not absolute, so a threat model is required
to assess security:
1) attacks with no physical access through network:
    * the customer's database is stored encrypted, the key cannot
      be found on the server's file system.
    * malfunction of web server software are limited to what the
      web server process can access.
    * attempts to gain root privilege via ssh are blocked or slowed
      down
2) attacks with physical access
    * if data centre employees access the file system to modify it,
      which is possible with physical access, there might be visible
      evidence of this modification. That's why security is a process
      and not a product.
That's true if you assume that root access and invisible modification
of crucial software is possible. Otherwise there is evidence.
Now compare this kind of cloud server with the "zero-click encryption"
using plain text RSA private keys on smartphones and you'll see the
difference. If your customer's credit card info is being sold at black
hat sites, where did that information most likely have come from?
         --ralf

@_date: 2014-03-27 21:32:57
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
As you've read my posting carefully you'll know that I assume a dedicated
machine and not VPS nor shared hosting.
How would you possibly do that on a running dedicated server?
AFAIK you'd need to reboot the machine into your own OS utilizing your
physical access to it and stop my running system entirely. Of course
that can be done if one has physical access and motivation.
       --ralf

@_date: 2014-03-29 13:22:38
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
I very much doubt that. An attacker with physical access reboots my
server in order to compromise it. That's possible, I cannot prevent
that. But for instance I can send heartbeats through an encrypted
tunnel between another server I control and my dedicated server in
the data center.
While my server reboots for being compromised the magical software
emulation must create encrypted packets without having the key.
So I can _detect_ the (possible) malicious act even when it happens
and not only after my compromised system returns after reboot.
I don't say it's easy to secure a server, but it's certainly not
impossible to detect a compromise, so there is no "absence of effective
technical means to secure something" in special cases that involve
the cloud.
          --ralf

@_date: 2014-03-31 22:12:02
@_author: Ralf Senderek 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Of course it is a requirement, as any such VM is running on my dedicated
server, and in order to make a snapshot one has to be root on the
dedicated server, and that requires a reboot and a compromise which is
       --ralf

@_date: 2014-11-21 08:55:28
@_author: Ralf Senderek 
@_subject: [Cryptography] FW: IAB Statement on Internet  Confidentiality 
It might as well inspire someone on the list to give already existing
solutions that are out there for a long time a decent peer-review.
This may help to get those software out of its state of invisibility
they enjoy at the moment.
               -- ralf

@_date: 2014-09-25 10:52:51
@_author: Ralf Senderek 
@_subject: [Cryptography]  The Trouble with Certificate Transparency 
Given the powers of a post-snowden MITM, the claim in Greg's posting seems
legitimate. At the moment when the browser makes the connection it is
undetectable that the browser is being fooled, _unless_ the browser
keeps track of the certificates it's visiting over time.
Without this change in the browser any system that tries to verify a cert
can be circumvented, not only CT. If the browser was able to check the
cert (via CT log servers or other means) before the MITM comes into play
there is the chance of detection of a targeted attack. The MITM could
send the victim a forged revocation of the legitimate cert and send a
forged follow-up cert for the targeted domain name, but even if the MITM
is in possession of the CA private key of one of the many CAs in the
trust chain, the browser will be able to detect that the new forged cert
had been issued by a different CA. Discontinuity is the sign that
something may be wrong here.
I cannot see why CT alone will get us out of trouble.
      --ralf

@_date: 2014-09-25 23:22:50
@_author: Ralf Senderek 
@_subject: [Cryptography] The Trouble with Certificate Transparency 
And that is the problem. In the above scenario it does not help to be
able to detect the misuse after successfully being MITMed. Protection
against a MITM by use of certs must work when the act of misuse
happens or the damage is done already.
     --ralf

@_date: 2015-08-01 09:10:58
@_author: Ralf Senderek 
@_subject: [Cryptography] How to solve the hen-and-egg problem 
No, there is no SUID/SGID set on any of the scripts.
While the GUI is beeing installed the name of the user is
required and only this user will be put into /etc/sudoers
to be able to run the main script "cbcontrol", which has
(700 root root) permissions. This script calls all others.
There are a number of advantages:
   1) The masterkey can have root read-only permission when
      it is stored on the USB, so read access to the filesystem
      as the user would not reveal the masterkey to an attacker
      that gains access via the network.
   2) Using the cbcontrol program by an attacker that has gained
      execute permission as the user would require his login
      password (asked for by sudo via openssh-askpass).
   3) Anyone with the intention to subvert the installed
      Crypto Bone software would need execute permission
      as root, in which case the battle is already lost,
      in case all-in-one mode is used.
   4) In REAL mode, even then the Crypto Bone software is safe.
The cbcontrol script will handle the commands that are
generated by the GUI either by itself (if it is in
all-in-one mode) or sent it to the real, separate Crypto Bone
if it is in REAL mode. To do the later, the ssh private key
need to be used, which is also stored with (400, root root)
The next step might be to use a mobile phone as an decryption
oracle for the measterkey and the local key, so that
both can be stored encrypted on the local computer.
I hope that answers your question.
      --ralf

@_date: 2015-08-31 20:16:50
@_author: Ralf Senderek 
@_subject: [Cryptography]  Ratcheting 
You asked for a realistic threat model. In case of T2 when someone is able
to read substantial parts of the memory, there is not much left to protect.
But I think, the more realistic attack is the effect of malware on a user's
computer that allows "full remote code execution" with the user's access
permissions. In this case I find the idea interesting to *separate* the
secret key used to encrypt the conversation from the process environment
the user (and the malware) has access to.
There are several methods to perform this separation. One is to put the
message keys on a separate hardware (like the Crypto Bone) and another
method is to place the secret key into a space where a process with
higher access permissions can read it and allow the user (and the malware)
to authorize the use of these secrets by carefully crafted code that
elevated the permission for the required action. It is essential that
neither the user nor the malware can change this code. In this case
the user can be impersonated by the malware but the secret keys remain
unknown to the attacker, so that m2,m3, ... cannot be decrypted.
And of course there is a combination of both.
      --ralf

@_date: 2015-12-04 11:20:59
@_author: Ralf Senderek 
@_subject: [Cryptography]  "The Moral Character of Cryptographic Work" 
Phillip Rogaway's essay introduced here by Perry Metzger aims at changing
the mindset of current cryptography research (and practice).
I'd like to emphasize only three of the many excellent ideas from this essay:
First, the moral dimension of cryptography is not an accidental appendage,
it is a fundamental part, because cryptography can be used to empower people or
to take their freedom away. Phillip convincingly shows why mass-surveillance
is dangerous for society and its negative social effects on people will lead
to regression.
   "[...] our inability to effectively address mass surveillance constitutes
   a failure of our field." (Abstract)
Secondly, to live up to the moral obligations of cryptography we need a
realistic threat model and act upon this threat model efficiently, not abstract.
   "At this point, I think we would do well to put ourselves in the mindset of a
   *real* adversary, not a notional one: the well-funded intelligence agency, the
   profit-obsessed multinational, the drug cartel. You have an enormous budget.
   You control lots of infrastructure. You have teams of attorneys more than
   willing to interpret the law creatively. You have a huge portfolio of zero-days.
   You have a mountain of self-righteous conviction. Your aim is to *Collect it All,
   Exploit it All, Know it All*. What would frustrate you? What problems do you
   *not* want a bunch of super-smart academics to solve?" (p. 41)
Good question.
Is the answer really "How to repair the internet, how to fix protocol issues?"
Is it "How to restore trust in (to this point untrustworthy) online services?"
I don't think so, because people's dependence on a technical infrastructure they
don't understand nor control themselves is the building block of the insecurity
we face today. We shouldn't underestimate the frustration potential of a development
that would restore (or even start to enable) user's control over their digital lives.
Phillip Rogaway calls this "A cryptographic commons".
   "We need to erect a much expanded commons on the Internet. We need to realize
   popular services in a secure, distributed, and decentralized way, powered by
   free software and free/open hardware. We need to build systems beyond the reach
   of super-sized companies and spy agencies. Such services must be based on strong
   cryptography. Emphasizing that prerequisite, wee need to expand our *cryptographic
   commons*. (p. 41)
Popular services reflect real needs, something that has value for people who want
to improve their lives in their communities, it's not an abbreviation for mindless
entertainment. Phillip's own primary example is *secure messaging*, the need to
be able to communicate without fear.
Any solution would include a decentralized component, whose reliability and
trustworthiness is of paramount importance to prevent it from becoming the next
Wouldn't it be prudent to direct much more effort into developing, testing and promoting
such a crypto server under the user's own control?
    "But for cryptography, much is lost when we become so inward-looking that almost
    nobody is working on problems we *could* help with that address some basic human
    need. Crypto-for-crypto starves crypto-for-privacy, leaving a hole, both technical
    and ethical, in what we collectively do." (p. 24)
      --ralf

@_date: 2015-12-06 11:42:06
@_author: Ralf Senderek 
@_subject: [Cryptography]  Cryptography is not a science currently 
I cannot believe that someone who has read even parts of the 46 page PDF would be
able to write such nonsense.
This is the advice Phillip Rogaway gave in order to improve the current situation:
   "
     * Attend to problems' social value. Do anti-surveillance research.
     * Be introspective about why you are working on the problems you are.
     * Apply practice-oriented provable security to anti-surveillance problems.
     * Think twice, and then again, about accepting military funding.
     * Regard ordinary people as those whose needs you ultimately aim to satisfy.
     * Use the academic freedom that you have.
     * Be open to diverse models. Regard all models as suspect and dialectical.
     * Get a systems-level view. Attend to that which surrounds your field.
     * Learn some privacy tools. Use them. Improve them.
     * Stop with cutesy pictures. Take adversaries seriously.
     * Design and build a broadly useful cryptographic commons.
     * Choose language well. Communication is integral to having an impact.
   " (pp 34-42)
Especially the last advice has obviously not been given the attention it deserves.
     --ralf

@_date: 2015-12-09 07:33:21
@_author: Ralf Senderek 
@_subject: [Cryptography] Who needs NSA implants? 
Yes, but Jerry's point was that the original vulnerabilities are not
accidental. Does getting admin privilege on such a system allow for
installation of malware that survives a hard disk erasure in some
places or is physical access ultimately necessary to do that?
     --ralf

@_date: 2015-12-12 14:23:03
@_author: Ralf Senderek 
@_subject: [Cryptography]  Talk on encryption to non-crypto audience ? 
Well, if your audience is mainly non-cypto people
interested in how cryptography affects their digital
lives, I'd recommend to focus on the fact, that
cryptography is easy, but keeping encryption keys
secure is very hard. Try to figure out what your
audience needs to know to better deal with the latter
Make them understand that they'd need much less complex
systems and far better audited and checked software
including their OS of choice and prepare them for the
conclusion, that convenience - as they know it - has to go.
      --ralf

@_date: 2015-12-30 10:42:44
@_author: Ralf Senderek 
@_subject: [Cryptography]  Understanding state can be important. 
While my Fedora starts 220 plus processes, the Crypto Bone based on OpenBSD starts only 16. You might argue that this is still 14 processes
too much, but I think the Crypto Bone is a step in the right direction.
       --ralf

@_date: 2015-02-07 08:57:03
@_author: Ralf Senderek 
@_subject: [Cryptography] 
So, forget about this, 'cause he's sorted?
No. I mean we should be more aware of our own responsibility to help
keeping such crucial projects going. And there are a number of things
we can do apart of funding issues, that are necessary to prevent
such projects to become a very lonesome exercise, that rest on too
few shoulders.
Are we really up to par in organising the mutual aid for projects
that are worth supporting?
       --Ralf

@_date: 2015-02-09 08:40:01
@_author: Ralf Senderek 
@_subject: [Cryptography] Question on crypto implementation in existing python 
That's the problem with googling: AES-128-CBC is *not* relatively weak,
if you use a random IV, don't believe uncle Google.
In my opinion it's problematic to "look around for some crypto library"
that lets you call aes() from python directly. It's far better to call
you'd probably get wrong if you'd do it yourself, like message
integrity, strong encryption key, random IV, etc. And you'll benefit
from the 18 years of code review that went into gpg, which is something
you won't get from anything you stumble upon on the internet.
By the way, do you know where your encryption key comes from, and how
you'd keep it safe?
      --Ralf

@_date: 2015-02-25 17:57:15
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
The Crypto Pi has morphed into the Crypto Bone, as I succssfully
changed the OS to OpenBSD and use the Beagle bone as the hardware
platform now. These changes have made the original project more secure
and much more auditable.
But the question still remains, if the Crypto Bone's threat model
is sufficient or not. And I'd like to explain the threat model
for open criticism here.
To my knowledge, nobody has scrutinized the system as a whole, yet.
Obviously, decent peer-review is an investment of time and effort that
makes sense only when a certain level of maturity has been reached,
which is the case now, I think.
A technical explanation that defines what the Crypto Bone does can be
found here:
    The crucial assumption behind the Crypto Bone is, that for ordinary
users it is more secure to delegate the encryption of messages to an
isolated, personal device than to perform message encryption on their
endpoint devices. All design decisions follow this assumption to secure
processes on the Crypto Bone as much as possible and to let users
access the results with their (insecure) endpoint devices through
an encrypted tunnel only.
As a consequence, the threat model has to take several attack scenarios into account:
1) Attacks from outside the local network on the user's endpoint device
    (malware infection, browser misbehaviour, you name it)
2) Attacks from inside the local network on the Crypto Bone (by exploitation
    of the insecurity of the endpoint devices and the local network, i.e.
    compromise of the internet router via vendor induced back-doors, ...)
3) Modifications of core components like OS vulnerabilities or hidden firmware
    exploits (the recent discussion about SD card storage springs to mind)
4) Physical Attacks (manipulation of the SD card / storage medium, stealing the
    Crypto Bone, ...)
In short, the Crypto Bone responds to these threats by strict separation of
the keys that enable its use and the encrypted database of message keys.
Once set up, the Crypto Bone turns into a state of maximal isolation, prepares
for an incoming VPN connection over the network interface and waits for the
arrival of a masterkey through this VPN connection.
On the user's endpoint device a gui helps the user to locate his Crypto Bone,
helps to establish the encrypted link and helps to upload the masterkey via SSH.
The necessary secrets are stored on a memory stick, that the user provides to "unlock his Crypto Bone". These secrets will be stored away separately by removing
the memory stick from the endpoint device. This is the shortest description
I can think of.
Now, stealing the Crypto Bone won't let the attacker recover messages or message
keys, as the master key is not present, while the Crypto Bone is not working.
An uploaded master key will reside in a ramdisk not in the Crypto Bone's filesystem.
Stealing the master key alone (via malware) does not reveal message keys without
physical access to the Crypto Bone (i.e. the encrypted key database).
But stealing the vpn secret lets an attacker establish the encrypted link, if he can
use a local machine to set up the VPN with this secret. In my view, it is not
helpful to burden the user with the additional use of passwords to protect secrets
he needs to use the Crypto Bone. It might be more sensible to require the presence
of pico siblings, an idea developed at the University of Cambridge, to replace
passwords and to use these as additional conditions for the Crypto Bone to unlock.
      But anyway, if separation of keys and encrypted database is the way to go, the user
has to get the key into the Crypto Bone in some way to unlock it. With OpenBSD it's
not possible to insert the keys directly as the usb hardware on the Beagle Bone is
not supported by OpenBSD at the moment. I hope that'll change soon.
The only way today would be flashing the Crypto Bone OS to the on-board EMMC memory
and to use the SD card as the temporary key medium that has to be provided manually
by the user. I can imagine that many user would not understand the necessity to remove the keys and would rather leave the SD in the Bone "for convenience".
And all protection against 4) is thrown out of the window again.
That's why I prefer the VPN link, instead.
Are there any major problems with this threat model from your point of view?
      --Ralf

@_date: 2015-02-27 08:23:39
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
The system cannot be reset by accident for a number of reasons. First,
when in the initial step the
secrets are being generated, a symbolic link (as root) is created to
indicate that this Crypto Bone is
initialized. If an attacker is able to remove this link inside the
Crypto Bone from outside, then the system
is reset. Obviously, using a root shell inside the Crypto Bone must be made as impossible as possible.
The most likely event for this to happen is the exploitation of OpenBSD
code via smtpd or fetchmail or
cron, or ..., but it will never happen by inserting a SD media. The user
can get confused, if he uses
several different Crypto Bone SD cards for different identities or
purposes, but the web interface will
always tell him who he is at the moment.
I don't have a solution for a complete system reset at the moment at
all, because I don't want to
place a button in the web interface that removes the symbolic link. I'm
open for suggestions how
to do this securely, other than to insert a virgin, verified new SD
image and start from scratch.
In addition, OpenBSD does not support the use of "other devices", the
only path inside the Crypto
Bone leads through the network interface. To prevent that an attacker
gets a root shell via the NIC
is the main concern here.
I'm really interested to find out how verification can be implemented
apart from signing the mSD card
image file. When the system is in use a tripwire-like IDS may help, but
the user won't be able to make
sense of the scan results. What are your suggestions?
       --Ralf

@_date: 2015-01-01 08:09:11
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
The Crypto Pi
This year has had some devastating news about the state of the internet infrastructure in store and came with some
disillusion about the vulnerability of tools we use day by day. And BTW it had turned out, that Johnny still can't encrypt.
To change that, I've put some effort into developing the crypto pi.
While running on a minimalist Linux OS with only the necessary
tools installed, the crypto pi is different both from internet servers and insecure endpoints, and helps Johnny to establish secure
communication that is always encrypted, without burdening him with
complex tasks that would only make him avoid using secure communication.
The crypto pi is in its early stages of its infant life and although
it's working and looking great, it has not got the most important
ingredient for a happy life (yet), peer-review.
So I'd like to ask all of you, who think a well-designed, isolated
crypto box under the sole control of its user, capable of doing message
encryption reliably, may improve the situation we're facing today,
to give a hand and scrutinize the design and implementation of the
crypto pi. Let's make the crypto pi a success in 2015, together.
The concept of secure communication using the crypto pi relies on
several assumptions, not everyone will agree to:
   1) Johnny will be able to communicate securely with people he knows,
      if he had been able to exchange an initial secret information
      (on a piece of paper, via telephone, or some other way)
   2) Johnny's endpoint device is not trustworthy, as it runs all kinds
      of complex programs that are prone to attack the secrets on
      his device without notice in unforeseeable ways.
   3) Apart from feeding the crypto pi with the initial secret and an email
      address of the recipient (by filling out a form) Johnny has nothing
      to do with key management, but will be able to verify that message
      encryption has been performed.
   4) All secrets are stored on the crypto pi and messages leave the
      crypto pi AES-encrypted with a strong randomly generated key.
      The crypto pi does not use public key cryptography, there is no PKI
      nor CAs involved.
   5) Johnny uses one single secret that he alone knows to establish an
      encrypted tunnel to the crypto pi over which he interacts with the
      web server on the crypto pi to read and write messages.
   6) The local network in which the crypto pi works is not trustworthy,
      so all information that originates from the crypto pi is encrypted
      and only encrypted information that enters the crypto pi will be
      processed inside.
   7) Although desirable, ensuring anonymity is not a pre-requisite of
      the crypto pi's design (at the moment).
   8) All source code is licensed under GPL.
Fortunately, the crypto pi has a home (crypto-pi.com) where you can get
more detailed information about its fundamental concepts and implementation. Make sure, your criticism and constructive suggestions will be used to
improve this project.
Best wishes for 2015
     Ralf Senderek

@_date: 2015-01-02 18:30:19
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
I don't know how much more auditable the beagle bone will be compared to the Raspberry Pi.
To be honest, we don't have a platform that is completely auditable and probably won't get one in the near future. So we need to use something we have, even if it is not ideal.
Of course the core Crypto Pi software is not limited to Linux, it probably will run
out of the box with a minimalist OpenBSD, as long as the OS provides bash, python and
the essential tools (/usr/bin/gpg, hostapd, apache, postfix). If there was a mini or
better micro version of OpenBSD running on the Raspberry, I'd love to use this as the
foundation for the Crypto Pi.
Actually there are two reasons. First public key cryptography is not necessary for secure
message exchange, as I assume Johnny had been able to hand over a random secret to his correspondent at a meeting or using some other suitable way. Secondly, using a series
of symmetric keys makes recovery from a compromise much easier and reliable compared
to the necessary revocation of public keys when private keys have been compromised.
No, keys used to encrypt past messages are destroyed already as well as the messages.
Yes, if they can steal the Crypto Pi with the medium on which the secrets are stored, they can. If Johnny uses model B his secrets reside on a USB memory key that has to
be removed while the Crypto Pi is not in use. On the model A+ secrets are stored on
the SD card, which has to be removed like the memory stick.
The Crypto Pi's threat model won't protect against raiding Johnny's device while he uses it.
At gunpoint, it won't be of any use if the secrets on the Crypto Pi are encrypted, because a bit of rubber hose cryptography would suffice to get the secrets. What someone
can do then, is to act like Johnny, but Johnny could send his correspondents a message, telling
them that he's been "hacked", asking his correspondents to reset his key on their devices
and the stolen Crypto Pi will not be of any use then, except for the limited his correspondents get his notice and act accordingly.
      --Ralf

@_date: 2015-01-02 18:43:12
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
Thank you, for the links, If there was a mini or better micro version of OpenBSD running on the Raspberry, I'd love to use this as the foundation for the Crypto Pi. The criteria you mentioned would be very useful for a single binary that would run on the user's endpoint device where the attack
surface is very large. The Crypto Pi in itself is designed as an isolated
entity that is accessible only through a single encrypted tunnel, like a
VPN, so the attack surface is reduced. That doesn't mean that your suggestions should not be followed as much as possible inside the Crypto Pi.
     --Ralf

@_date: 2015-01-12 23:12:16
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
Do you know how trustworthy the onboard RNG on the Raspberry Pi is? Are
there any references that convinced you to put some faith into its
That's a cute idea, so to any client software the Pi's random number stream looks like a device file. But How do you encrypt?
The Crypto Pi uses /usr/bin/gpg for encryption, do you use something
different, as the client must decrypt the stream before simulating
So your solution replaces /dev/random on the client with the data
it receives from the Pi?
      --Ralf

@_date: 2015-01-13 08:14:43
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
No, you don't. In fact, the core software of the Crypto Pi project will run
on different hardware and (maybe) run on top of any *nix platform that provides
some basic tools. In its current version it uses a fraction of the functionality
of /usr/bin/gpg (AES) and reads 20 Bytes of /dev/random to get 160 bit of
"unpredictability" for each message sent. It is essential not to tie the Crypto
Pi to a specific pre-condition unless there is a reason for it.
That brings me back to my original question: Is it (in any sense) more secure
to avoid reading key material from /dev/random and use the hardware RNG
instead or in addition?
Does the rPI use its specific capabilities to enhance /dev/random?
     --Ralf

@_date: 2015-01-18 15:50:32
@_author: Ralf Senderek 
@_subject: [Cryptography]  The Crypto Pi 
The Crypto Pi needs a random key with at least 128 bits of entropy
for every message (AES). The desirable hardware platform would be
the beagle bone and the OS OpenBSD to make auditing possible.
But there is a problem with the randomness source on the beagle bone.
I've monitored the state of the kernel's entropy pool via /proc and
found that if you read 10 Bytes from /dev/random the entropy level
drops by 52 bits. A short time later reading another 10 Bytes the beagle
blocks for 54 seconds. Reading 20 bytes for the first time removes
116 bit of entropy from the pool and the second read blocks for nearly
70 seconds. The beagle bone needs 143 seconds to recover and to add
a 100 bits of entropy back to the pool. There's no rngd running.
On the Raspberry Pi, reading 10 Bytes drains 180 to 240 bits from the
pool, and reading 100 Bytes drains 960 bits of entropy, but the RPI
recovers rather quickly (with or without the rngd running) at a speed
of 100 bits per second.
OpenBSD will only be available on the beagle bone, but the questionable
random source on the beagle might justify the choice of the RPI as the
hardware platform for the Crypto Pi.
      --Ralf

@_date: 2015-01-25 17:35:41
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
Well, I'm referring to the output of :
When I read bytes from /dev/random with dd and immediately check this
file again, n bits are missing as a result of the read operation.
So, IMHO, the whole system now has less entropy to feed to /dev/random. Am I wrong, when I assume that if the content of entropy_avail drops to
zero, /dev/random is supposed to block?
      --Ralf

@_date: 2015-01-25 18:46:47
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
Okay, is there an alternative?
If entropy_avail is misleading, we're shooting in the dark while trying to
find out how much entropy is in a chunk of bytes read from /dev/random.
Are there any pointers to these experiments? Sounds as if that could
help the beagle bone quite a bit.
    --Ralf

@_date: 2015-01-26 19:59:08
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
I won't follow your advice, because for the Crypto Pi I want high-quality
keys with a reliable amount of entropy in each of them, not only
pseudo-random numbers. From which source I can draw the keys is unclear
at the moment, because /dev/random seems to work very differently on
the platforms in question. But without a reliable measurement of the
randomness there is no answer whether the key is good enough or not.
I'm not convinced that the entropy-avail file is entirely useless. though.
I agree that we're in implementation land already, and in the end we
have to assess a working system and not a mathematical notion.
     --Ralf

@_date: 2015-01-26 20:15:54
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
I'm not too happy with the idea to draw the randomness of the message
keys from /dev/random only. Including other local sources is desirable,
but if I use rngd to feed the hardware random generator output into
the mix it could easily become dominant compared to the other less
agile sources and that might help an opponent who has privileged
knowledge of its output.
      --Ralf

@_date: 2015-01-28 08:08:19
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Pi 
I've done that.
Yes, but the important condition is "well seeded". It's a chicken-and-egg problem, if you'd need /dev/random to seed the multiple userspace PRNGs.
This is a good idea, and fortunately the Crypto Pi will only need about 30000 bits of entropy per day. How to make sure that enough of these (128) will be in every message key is the challenge.
       --Ralf

@_date: 2015-07-09 23:50:12
@_author: Ralf Senderek 
@_subject: [Cryptography] The Mesh 
If the mesh is designed to ease a user's eminent pain ("remembering the
user names at all the 100s of Web sites they use") how does it work?
Is there any pointer you can give us? And how do you solve the
key management problem, practically?
    --ralf

@_date: 2015-07-10 20:45:21
@_author: Ralf Senderek 
@_subject: [Cryptography] The Mesh 
I understand that the mesh is something like a cloud storage for encrypted
keys the user needs on different devices. It just makes the keys available
wherever they might get used. But, doesn't that mean the secret keys must
be stored in the devices in plain text? Or, if you use passwords to protect them, you'd run in all kinds of key management problems, (forgotten passphrases etc.)
The administration device at least must store the personal PKI's root
signing key to be able to chain-in all other certificates a user might
need. If you'd use a password for protection here, that'll be entirely
necessary. So, if someone else's email cert has been accepted and
chained, signed and uploaded to the mesh, it is available on all the
devices, but how do you make sure, that the right cert is being
accepted by the personal PKI? I suspect you'll run into all the
well-known difficulties that makes key management an unfriendly
task for the ordinary user.
     --ralf

@_date: 2015-07-11 14:38:33
@_author: Ralf Senderek 
@_subject: [Cryptography] The Mesh 
That raises the question how the decision to include any particular
Signing Cert is reached. If for a frictionless use case the user
has nothing to do with this decision, how should he gain any trust
in the validity of his own personal PKI?
That's your most important selling point.
How did this verification happen? Why is it secure?
Again, I think the scheme does not answer the most important question of
how a user can be sure he's using the correct correspondent's key, if
all decisions regarding the validity of keys are done on his behalf by
"the mesh" and "a tool".
     --ralf

@_date: 2015-07-12 20:26:37
@_author: Ralf Senderek 
@_subject: [Cryptography] The Mesh 
In plain text?
and use this key to encrypt the message on the proxy.
Seems to me, that there are a number of questions to be
answered, if this is going to be pretty secure.
How will you convince the user that there is no "encryption
kill switch" with a remote control to NSA headquaters
on the proxy? It might be doable, I just don't see how.
     --ralf

@_date: 2015-07-13 10:30:01
@_author: Ralf Senderek 
@_subject: [Cryptography] The Mesh 
Ok, that addresses the main concern, i.e that doubts about the
trustworthy of servers is not at all paranoid these times.
      --ralf

@_date: 2015-07-29 14:07:04
@_author: Ralf Senderek 
@_subject: [Cryptography] How to solve the hen-and-egg problem 
It is usually being believed that the hen-and-egg problem cannot
be solved, but I need a solution for my particular version now.
The Crypto Bone has reached a state that it actually can be used
in practice, but it hasn't been subject to thorough code review, yet.
A code review has to focus on two programs (secrets and openpgp)
that I have written in C, which make use of Peter Gutmann's cryptlib.
And in a wider context a handful of ksh scripts.
People I've asked for advice, how to get code review, seem to say that
unless the Crypto Bone isn't used widely, nobody will have any reason
to look at the source code. In order to increase the user base
considerably, I've developed a software-only version of the Crypto
Bone that people can use who have no Beagle Bone.
One might think that this all-in-one version will be more vulnerable
to attack as it lacks the isolated, minimalist environment that the
real thing provides. And in a way that's true.
But it turns out, that it is still pretty safe unless an attacker has got
the ability to run arbitrary code as root on the computer that hosts
the all-in-on version. And although this is more likely to happen on
the user's machine than on the bone, it means the all-in-one Crypto Bone
may be safe to use, too.
I'd like to know if you think this is the right way to approach the
usability dilemma or if you have any idea how to approach my
hen-and-egg problem from a different angle.
      --ralf

@_date: 2015-07-30 09:10:17
@_author: Ralf Senderek 
@_subject: [Cryptography] How to solve the hen-and-egg problem 
Reviewing the two core files (secrets.c and openpgp.c) will not require
weeks, as they are both about 500 lines of code thanks to the very
good work you've kindly done already with cryptlib.
While static code analysers will work with C code, they might be less
valuable when it comes to reviewing the ksh scripts. These scripts
represent the logic of the message encryption scheme and a review
needs to focus on the security of the ideas, they're based on.
I think I'd add a few rows to my Lotto ticket ;-)
      --ralf

@_date: 2015-06-07 12:34:11
@_author: Ralf Senderek 
@_subject: [Cryptography] A "koan" about crypto 
like cryptlib-3.4.3
Such a framework is clearly missing at the moment. And this has nothing to do with laziness or lack of time, the primary reason is complexity. Most, if not all cryptographic programs can only be assessed in the context of the practical use case, where much more than the library itself comes into play. Under normal, unclear and confusing circumstances, it's hard to reach a substantial conclusion regarding the security of a system.
Without a far-reaching reduction of complexity, there's no incentive for well-educated people to contribute to such a framework, as the results
of their validation efforts may become invalid, because of some detail
they might have missed.
And then there is laziness and lack of time.
Maybe, one first step in the right direction is to fight complexity at all fronts, and to push yourself to volunteer, once there is a reasonable chance for success - and do it.
    --ralf

@_date: 2015-06-15 22:43:28
@_author: Ralf Senderek 
@_subject: [Cryptography]  Sunday Times Snowden Decryption claims 
Based on which evidence?
I'm amazed to read such an assertion on this list at all.
     --ralf

@_date: 2015-03-01 12:49:14
@_author: Ralf Senderek 
@_subject: [Cryptography] The Crypto Bone's Threat Model 
Ray Dillinger and Jerry Leichter have made suggestions on
how to reset an electronic device by the legitimate user.
Excuse my tendency towards applying these suggestions to the
Crypto Bone and its available features within its threat model.
A user wants to "reset" his working Crypto Bone, he does not wish to get a new, empty one, but for some reason he insists that the
vpn connection should use a new key and the message key database should be encrypted with a new masterkey and only he should have
these two new keys on a removable usb medium in his own hands.
Bear's suggestion (with more general demands on hardware design):
1) is equivalent to inserting a new mSD card, the database is lost, new keys are generated, this is not what the user wants. A reset should preserve the message keys. If need be, he could reset a single message
key in the usual way.
2) can be done by requesting the user (legitimate or not) to perform some
identifiable action on the beagle bone's gpio pins, like morsing SOS on a
pin by short-circuit pin x with pin y. Ther's no need for a switch, if you
don't insist that the action "cannot be triggered" by software. Software
surely will be necessary for that.
3) is no problem, as the new keys can be written to the ramdisk, internally
without replacing the working keys.
4) This is the crucial part that activates the new keys. And here the switch
alone does not suffice. I continue after Jerry's suggestion on that part.
Jerry Leichter's suggestion:
1) At the moment, there is no signing key in the Crypto Bone at all. It generates the
vpnkey and the masterkey on its own by reading from /dev/random. And it destroys the masterkey
as it must be provided by the user from outside the Crypto Bone through the secure link.
2) Once the mechanical interlock is triggered, the user must provide the old masterkey to
prove he is a legitimate user. Note, the threat model assumes physical access to the CB, so an attacker can morse SOS on the gpio pins but without the masterkey a reset would not be
initiated. (Separation of masterkey from message key database).
But we're talking about Johnny, who can't encrypt, so the new key must be generated inside the
CB as a result of running the reset-code.
3) N/A
4) If Johnny loses the masterkey, no reset will be possible, the CB will be stuck after boot
in the loop waiting on the masterkey nobody can provide, access to the message key database
is lost. Time to insert a new mSD card image and be more careful next time.
So, what can go wrong?
If the reset-code on the Crypto Bone is unmodified (as other crucial parts of OpenBSD)
the legitimate user can trigger a reset by performing the action on the board.
An attacker without knowing the masterkey cannot. And the masterkey is nowhere to be found on a powered-off Crypto Bone.
When keys get replaced and the database re-enrcypted the transfer of these secrets to the legitimate user's removable medium must take place in the same manner as if it were
a fresh mSD card running for the first time. If Johnny forgets to remove the mSD card
and does not transfer the secrets to his usb stick, then leaving the SD in the Crypto
Bone will destroy the masterkey rendering the system unusable. And that is exactly what it should do.
      --Ralf

@_date: 2015-10-11 12:32:48
@_author: Ralf Senderek 
@_subject: [Cryptography] Usable Security Based On Sufficient 
I am about to secure secret information using a "password" that can be
produced by a process on the user's endpoint device with administrative
*execute privilege* instead of using some information from the user's
brain. The idea is that a malware running on the user's behalf would not
be able to produce this password unless it has execute permission as the
root user.
Malware that has got read access as the root user could read any file or
information on the endpoint device, but as the production of the password
requires execute permission, all secret information secured with this
password is still safe until execute permission is gained.
Everything short of running code as root should not compromise the protected
information. If such a secret-producing process existed it would be a
substitution for user provided passwords and would increase the usability
of crypto considerably.
I'd like to hammer out an idea for such a process.
It's clear to me that such a process wouldn't work on all devices let alone
all operating systems. Unfortunately it wouldn't work on the Crypto Bone,
because people would restore their system from a common image file.
In any case where systems are being cloned, this is not an option, but on
the user's individual OS installation it should work.
In all cases in which the OS provides a properly secured admin account,
and in which the installation of the OS has produced a sufficient amount
of unpredictability, pieces of information can be collected by such a process
that are unavailable even to malware that has read access to the device as root.
I presume that root read access is a realistic threat on a user's endpoint
device due to the exploitation of the complexities of modern OSes by network
attackers without physical access to the device (heartbleed). And I presume
that getting code run as root is considerably more difficult than getting
read access, or in other words, in this case nothing can be done to save
the information stored on the device at all.
So, where will we find the necessary unpredictability to construct a secret
that is inaccessible to anyone not being able to run code as root?
I exclude everything that's stored in a file and anything that is prone to
change. But to the best of my knowledge, the inode numbers of specific files
cannot be read directly by a process with read access only. These inode
numbers are far from being random but they contain enough unpredictability to
construct a password that is as secure as anything a user could provide by
picking her brain in the traditional way.
My Fedora informs me that the inode numbers of files like /root/.rnd that
live inside a directory with 700 permission differ from the parent directory's
inode number quite a bit. One should not over-estimate the unpredictability
stored in the inode number as they are allocated sequentially to new files,
but conservatively guessing there will be at least 5 bits of unpredictability
in each of them.
So imagine in a certain OS there are say 5 such files (A-E). There are 5!
permutations of their inode numbers in concatenation that each would
make up a weak password if the sequence was known in advance. But if a sixth
inode number, unknown to the attacker, defines the sequence that's actually
used the resulting string piped through a bcrypt hash function should be
secure enough to replace a user provided password.
In fact some *NIXes provide very few such files, so it might be a better idea
to generate a directory structure for this purpose by creating a random number
of files to eat up inode numbers and then pick files you need and delete
all the temporary files again. This way the unpredictability of the left-over
files can be increased way above 5 bits.
For the process that deterministically computes the local secret the
unpredictability of the result is clearly zero, but for any network attacker
it should be sufficiently high to reliably protect secret information
bound to using that specific device by an ordinary user.
      --ralf

@_date: 2015-10-12 11:09:00
@_author: Ralf Senderek 
@_subject: [Cryptography] Usable Security Based On Sufficient 
quoting me:
Not all malware runs in a Turing complete environment. You're mistakenly thinking of all attack scenarios as happening inside one big computer.
It's much more likely that a network attacker gets access to information
she shouldn't be able to read by feeding malformed packets to daemon
processes over the network, or .. or .. without getting complete control
over the target machine. The world is not binary and there are increasingly
more ways to make bad use of the complexities in which the user's endpoint
device interacts with the rest of its network environment that may
in part be controlled or abused by a network attacker.
       --ralf

@_date: 2015-10-12 11:22:59
@_author: Ralf Senderek 
@_subject: [Cryptography] [Crypto-practicum] Usable Security Based On 
Exactly. And that makes it the perfect example to show that for
a network attacker the disclosure of information he should never
be able to get read access to and running arbitrary code as UID 0
on the target machine is not the same thing and by no means
equivalent. This is all about a realistic threat model.
       --ralf

@_date: 2015-10-12 21:11:30
@_author: Ralf Senderek 
@_subject: [Cryptography] Usable Security Based On Sufficient 
I believe firmly that this conclusion is false.
Of course.
Sure, but the execution would be under the users UID with access
permissions to files under the same UID.
I believe I can rely exclusively on execute privilege (UID=0)
if my analysis of the problem is not flawed and there isn't a
shortcut I cannot see yet.
Not at all! Because if 54 is the inode number of the file
in question, then running the machine code that hides behind
"ls -i filename" will produce this number and no file content
will reveal the information to calculate this number. [1]
My assumption is, that the input necessary to perform this
calculation (as root or user) is not available as the content
of a file. [1]
In the old days you could inspect the data block of a directory
with cat. That would reveal the file allocation table of the
directory with the inodes and starting data block, access permissions
and filenames. But in modern unix-like OSes (some? all?) the kernel
code prevents you from inspecting the directory data block which is
a good thing (TM) with respect to the solution I propose, because it
forces you to run the inode calculation code as UID=0 in the kernel.
Thanks for advancing the discussion of the crucial point.
        --ralf
[1] There is one obvious counter-argument, though. The special device
file /dev/sda13 is readable by root and if the malware can trick some
root process to read it and stream the whole raw filesystem to the
malware user. Reading this stream would "somewhere" reveal the bits
of information that make up the input of the userland computiation
mentioned by Ray above. But that's giving the haystack and asking
for the needle, to be found without running kernel code as root.

@_date: 2015-10-20 14:09:11
@_author: Ralf Senderek 
@_subject: [Cryptography] Other obvious issues being ignored? 
Yes, but that implies that we need to debate implementation ideas or even
details and that we have to look closely at *systems* not only concepts, hanging
in thin air. It's like a shift of paradigm that has to take place, if we
want to talk about security in a meaningful way.
The most important obvious question being ignored - as I see it - is how do we
make all that checking and debate actually happen in a way that things get better.
Neither hoping for the best nor expecting the worst will suffice. We need to
fight ignorance.
       --ralf

@_date: 2015-10-21 08:03:18
@_author: Ralf Senderek 
@_subject: [Cryptography] Other obvious issues being ignored? 
Let's combine two ideas from different posts:
Arnold Reinhold writes about "the list of issues the cryptographic community knows about, but keeps ignoring"
John Denker writes in addition to the list:
While ordinary users want the digital land of milk and honey *that works*,
there is no clear threat model for the assumed solution.
While on the other hand, the attempts to reduce complexity and to work with
a realistic threat model are bound to small niche markets if not entirely
ignored, we have all the ingredients for a perfect recipe to make sure,
we'll remain in this situation.
       --ralf

@_date: 2015-10-30 10:08:44
@_author: Ralf Senderek 
@_subject: [Cryptography] Hiding parties identities 
Peter Gutmann asked:
Christian Huitema answered:
While this RFC nicely summarizes the various post-Snowden threats and
categorizes them using clear terminology, it really does not address
the endpoint issues and consequently does not reveal a complete
threat model.
It is interesting that there is another document trying to give
"answers", that are omitted in RFC 7624 deliberately. This document
focuses on mitigations to the problems listed in RFC 7624.
As expected, these mitigations fall short of considering the role of
endpoint devices as "unwittingly collaborators".
       --ralf

@_date: 2015-09-03 18:26:47
@_author: Ralf Senderek 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
But you're comparing apples and oranges here. DH is only solid ground because the DH secret key is never used to sign messages. In fact DH
lacks all authenticity that RSA provides (if you have the correct
public key of course).
The worrying part of the talk at CCC is that it's possible to "exfiltrate"
a RSA secret key from a chip with only pouring some chemicals and poking
around with a flash light (and one single computation).
That would never happen with DH secret keys inside a chip as they're not
used for signing.
      --ralf

@_date: 2015-09-03 20:07:10
@_author: Ralf Senderek 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
And what makes you think that using ElGamal would not leak the secret
key under the same circumstances, i.e when the chip is made to dysfunction
due to a light injection? Why should an unsuccesful ElGamal signature
be immune to revealing secrets stored in the chip like RSA does?
      --ralf

@_date: 2015-09-29 22:10:33
@_author: Ralf Senderek 
@_subject: [Cryptography] Future GPG/PGP 
Wonderful, give me more complexity, we didn't have enough in recent years.
      --ralf

@_date: 2016-04-05 18:46:47
@_author: Ralf Senderek 
@_subject: [Cryptography] Secure universal message addressing 
Or a different public-key per server, all you have to add is a switch
"-i differentprivatekey" to your ssh command (and disable password access
altogether on the server).
    --ralf

@_date: 2016-04-12 20:31:04
@_author: Ralf Senderek 
@_subject: [Cryptography] Is storing a hash of a private key a security 
In order to keep the key material secure the truncated hash would be calculated inside the enclave. You give the stored truncated hash value
from outside and get a yes/no from the enclave code I suppose.
I wouldn't be concerned about oracle attacks unless the hash function
has any indication not to be one-way, but I would be concerned about
the reliability of the integrity check if the truncation has to be
substantial. The full hash value would clearly allow to decide whether
or not the key material is kosher. But if random key bit rot occurs
a substantially truncated hash value might not catch the first
of these bit flips reliably. So I think it depends on how catastrophic
an unlikely miss of a key change turns out to be.
     --ralf

@_date: 2016-08-11 01:27:11
@_author: Ralf Senderek 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
Maybe the Biologists, after all, remind us that in order to have a secure
system, at least one step must be performed non-electronically.
    --ralf
PS: No owls are harmed in the cryptobone project.

@_date: 2016-08-11 23:32:03
@_author: Ralf Senderek 
@_subject: [Cryptography] Public-key auth as envisaged by first-year 
Well RFC 1149 was way ahead of its time, but I still wonder how this RFC
may cater for a TCP "hand"-shake.
     --ralf

@_date: 2016-12-27 18:07:35
@_author: Ralf Senderek 
@_subject: [Cryptography] where shall we put the random-seed? 
It seems that we are far ahead in standardisation, as FWICT, all systems
I've looked at have it in /var/lib/systemd/random-seed including Fedora
and CentOS.
At the moment, we have a BIG TIMING issue. This is an excerpt from my
laptop's boot process (kernel 4.8.12-300.fc25.x86_64) :
[root at lap ~]# journalctl -b|grep random
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:53 lap.senderek.ie kernel: random: systemd: uninitialized urandom read (16 bytes read)
Dez 27 14:54:54 lap.senderek.ie kernel: random: fast init done
Dez 27 14:54:56 lap.senderek.ie kernel: random: crng init done
Dez 27 14:54:56 lap.senderek.ie audit[1]: SERVICE_START pid=1 uid=0 auid=4294967295 ses=4294967295 msg='unit=systemd-random-seed comm="systemd" exe="/usr/lib/systemd/systemd" hostname=? addr=? terminal=? The systemd process that loads the random-seed file starts 3 seconds
after the boot process gets grub's boot parameters at 14:54:53.
To make sure the kernel initializes the RNG at time=0 is yet an
unsolved problem.
     --ralf

@_date: 2016-12-28 20:12:53
@_author: Ralf Senderek 
@_subject: [Cryptography] where shall we put the random-seed? 
I cannot see any advantage in favour of overwriting the kernel image.
Once the kernel can get another parameter in form of a file name stored
in the same file system as the kernel itself, then kernel code can use it
to initialize the RNG properly from the start. In any case the bootloader
has to provide this information, be it grub reading a file in /boot or
isolinux in case of booting a CD getting this information from the ISO
file system. Making it a part of the kernel image IMHO only complicates
things and opens another attack surface on the kernel.
The main thing might be to teach the kernel to _expect_ such a parameter
and to evaluate the information provided as early as possible, certainly
within the first second after the kernel gets control.
      --ralf

@_date: 2016-02-20 12:17:31
@_author: Ralf Senderek 
@_subject: [Cryptography] Apple 3rd Party dilemma 
Exactly. And that does not only apply to hardware. People must also demand that they have the ability in software to reliably cancel access to
private information stored on the devices they OWN. If encryption is used
to protect their information the software they use (and don't own) must
give them sole control to the encryption keys, which in essence means that
they cannot use a 3rd Party to do the encryption for them with no role of
their own in the process.
     --ralf

@_date: 2016-01-07 22:31:50
@_author: Ralf Senderek 
@_subject: [Cryptography] Chaum Has a Plan to End the Crypto War 
If Chaum's solution is meant to be a solution, it requires that all
other methods to protect messages are successfully being declared unlawful.
To be able to decrypt a "bad guy's communication" by co-operation of some
nine super-trustworthy entities, it's essential that the bad guys abide
the law and use only allowed means of communications, which they don't
and never will.
So Chaum's solution is not a solution at all.
     --ralf

@_date: 2016-03-07 09:17:14
@_author: Ralf Senderek 
@_subject: [Cryptography]  EFF amicus brief in support of Apple 
You're claiming the realistic view here. But you're ignoring the fact that
these people don't really get what they *need*. They need protection and
what they get is an *illusion of protection*, because with closed systems
nobody is able to realistically judge if it "just works".
The point where you're getting off the track is that you seem to neglect
all the efforts to reduce complexity, to increase auditability and transparency
as fruitless exercises of a few misguided coders, instead of helping to
pave the way for them.
         -- ralf

@_date: 2016-03-10 14:03:53
@_author: Ralf Senderek 
@_subject: [Cryptography] Help with Raspberry Pi IoT initialization... 
identifying one well-known user (me)
which is the same on all incarnations of the boot media
which is on the boot media, but not in the boot partition
or else this would not work.
You seem to assume that the boot media can self-verify, which - as I see it -
is impossible. There has to be two separate parts (the verification tool and
the boot partition) and you can verify the partition with the verification
tool and the signature value. Put all three in one part and you'll end up
with an unsolvable problem.
The best you can do is to adapt the LIVE-image DVD approach. Treat everything
on your boot media as read-only. On first use of the boot media, run a Unix
in pure memory (like isolinux) that starts your verification code. Then verify
the boot partition, that has never been touched before, and then boot it After this point the boot partition will change and no further verification
will succeed with the old signature value.
But, this will only work if the part that contains your memory-Unix and code is kosher, you don't have a guarantee for that, as this part is unsigned.
No real solution, unless you get your boot media from a trustworthy source,
or unless you already possess a verification tool that can check the boot signature independently.
If you really want to erase the keys, your verification tool must create these keys in
pure memory not on disk, but then you'll have to extract this information
from the running device to use the device's temp. public key outside for of signatures the device creates.
If the signing keys need to be on the boot media in the first place, physical to the raspberry pi's boot media must be prevented. All you can do is make sure
that an attacker cannot inspect the raspi's file system using the network With physical access you cannot guarantee anything.
       -- ralf

@_date: 2016-03-11 08:18:20
@_author: Ralf Senderek 
@_subject: [Cryptography] Help with Raspberry Pi IoT initialization... 
Lacking any flash memory, the Raspberry Pi's firmware is (AFAIK) stored on the
boot medium (SD card) but in a separate partition, so that the boot partition
is not affected by firmware changes. Even though everything on the Pi is writable
a verification code, started from the separate partition, could - in principle -
verify the boot partition's fingerprint, BUT there is absolutely no guarantee
that this code has not been tampered with. And, using the boot partition for
anything but reading destroys even this possibility.
[root at lap ~]# md5sum /dev/sdc1
72cf5108ad7d989081c8f9664fa46694  /dev/sdc1
[root at lap ~]# mount /dev/sdc1 /mnt
[root at lap ~]# umount /dev/sdc1 [root at lap ~]# md5sum /dev/sdc1
606911db0d7d6d3444c2876005dfe56f  /dev/sdc1
So there's no way around an independent verification of the boot media, the Pi
cannot do it on its own.
      -- ralf

@_date: 2016-11-17 08:29:46
@_author: Ralf Senderek 
@_subject: [Cryptography] On the deployment of client-side certs 
Really? Or do you mean "I don't see anyone else *the size of Apple* trying"?
 for a start, there are others.
      --ralf

@_date: 2016-11-17 19:09:50
@_author: Ralf Senderek 
@_subject: [Cryptography] What does it take to make electronic communication 
There is a common misconception that we have to dump in order to make It's the idea of "transparent security" in relation to using the internet.
Some people think, that in order to provide secure email to ordinary
users in a way that they will actually use it, all change has to go
into the infrastructure not into the endpoint facing (and including) the People should do essentially the same thing they do today, but also be because an improved network in future (PKI that works, bullet-proof that have been fixed, servers that provide encryption, etc) will take care
of the security they need. Users don't play any role in providing this
security, it is transparent to them, if it happens, it happens because
experts have ensured it's there and works.
In this view, the user's contribution to security, and also their is close to zero. I'm quite sure this is *not* the way secure and usable
email will become a reality.
With zero involvement in the security of his email the user has zero
reliability too. The obvious conclusion is, that "using the secure
system" cannot be "as easy as using the insecure one".
If we can determine what the user's indispensable role is, what the
technical solution can expect the ordinary user to do before he might
be deterred from using it, because of its complexity, we can also what a secure and usable email system will look like.
I may be wrong, but these are the basic requirements in my opinion:
Prologue: Secure means authenticated and confidential message exchange.
 	  The two persons that exchange messages using the system must
 	  be reasonably sure that each message they receive was created by
 	  their correspondent and will be visible only on both screens
 	  connected to these people's endpoint devices and not anywhere
 	  else.
 	  This does not (necessarily) include secure storage of the
 	  information received nor the invisibility of their exchange.
Every user of a secure and usable email system must have the ability to
1) accept or dismiss the secure exchange of messages, deliberately.
2) actively enable a secure exchange with a particular correspondent.
3) prove that all the messages leave his endpoint device properly These are three abilities to control the system that the user must have
to develop trust in its reliability.
If a user cannot control (1), the system would continue to "secure"
the communication under circumstances where a compromise of a system has
become obvious. A user must be able to stop the exchange with selected
correspondents that have become unreliable, malicious or "hacked",
he must be able to pull the plug.
If a user cannot control (2), he cannot guarantee that only his own
decision makes the secure exchange possible. Without this control, the
message exchange may be readable by a number of other third parties in
addition to the intended recipient. The initiation of the secure exchange
must rely exclusively on what the user does at the beginning of an Once initiated, the system can change encryption keys as needed, but
the system cannot take the initiation out of the user's hand.
If the user cannot control (3), nothing can convince him that the intended
protection actually happens on his endpoint device using authorized
initiations by components that can be audited to do what they should do
and nothing else. Without this control, which includes physical control
of the device(s) a user needs to understandably produce the intended secret information can easily be leaked to network devices outside his It is my firm believe that we cannot design a usable and secure system if do not provide support for these three controls in a way that is as to the user as possible.
Now, what does a usable and secure system look like in technical terms,      --ralf

@_date: 2016-11-27 14:00:06
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
Both "grub.cfg" and "grubenv" are traditionally world-readable because
they are not perceived as containing secret information.
Of course grub could be changed to limit read permission to root, but
that brings us back to the question if there is a better way to secure
the secret seed than to store it in the file system?
     --ralf

@_date: 2016-11-27 14:11:08
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
When my grub starts the boot process:
       Nov 27 11:28:03 ext.senderek.ie kernel: Linux version 4.8.8-300.fc25.x86_64
it takes a whooping six seconds until
       Nov 27 11:28:09 lap.senderek.ie kernel: random: crng init done
the kernel completes the initialisation of the random number engine.
Everything done before that point whould have to deal with a lack
of entropy. So it's important to shift this event down as much as
       --ralf

@_date: 2016-11-27 16:08:24
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
This is IMHO a (common) over-simplification, though.
If the threat model includes faulty root code on the endpoint
device it may be prudent to limit the lifetime of the stored
secret seed to the very first milliseconds of the boot process,
so that kernel code can access it and then unmount it from the
ordinary file system. This way it would need full root access
permission (i.e running arbitrary code as root)
to re-install the secret information after the boot
process has finished. This can be a protection in case any
daemon (running as root) accidentally acquires read access
to a file because of an unfixed vulnerability, but the same
process is still not able to re-install the information that
was available to kernel code in the first second of the boot
Storing this information even in a separate partition (or
similar) would make sense, because the game is not over,
just because some code can be tricked into reading a
root-read-only file.
The Crypto Bone does something similar, though later in the
boot process with the filesystem already present.
Easier yes, but not as secure.
      --ralf

@_date: 2016-11-28 08:29:57
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
Would it be possible to extend this idea to information in the
file system, too? Imagine the Linux kernel would provide a new, special
file structure that has a property "read-once-and-then-never-again"
enforced by kernel code. This file could be read (once) in early boot
process and would not be accessible after that first read.
To update the secret stored in such a special file structure, there
must be a second property "write-once-after-being-read-once" enforced
by kernel code to make sure that the secret can only be changed by
early boot code once enough entropy has been gathered to safely
update the file content (once).
Would such a change to the kernel be possible?
       --ralf

@_date: 2016-11-29 10:15:33
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
The context in which I brought up this question was the idea
of storing secret information in the file system, in a new
way that ensures this information can be read once (in the
early boot process) and will then disappear reliably for the
rest of the time until the next boot. On the next boot,
the information --- maybe updated with a single write ---
should be visible again (first read i.e. by a deamon that
stores the secret in memory) but will remain inaccessible
even by root code after the first read, because the kernel
enforces this inaccesibility.
I'm sure a pipe or a socket wouldn't help here as they don't
preserve the information across boot processes. It would take
something like a file with a read counter being reset by starting kernel code. This is a rough estimate, details may
be more complicated, but if we had this, the storage of secrets
would become much easier.
      --ralf

@_date: 2016-11-29 12:30:48
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
[...]
not necessarily, the file system must be mounted already.
And that is what I'd want to have changed and supported by (minimal)
kernel code. I think the running kernel must decide whether or not
an open() succeeds (if it is the first) or fails (until the next boot)
on such a special file structure. It wouldn't matter if the file
must live in a dedicated part of the file system (/var/secure) to
get an easy way of making the distinction between normal files and
these special ones. I don't know if such a distinction imposes an
unacceptable performance hit or not. But the attack surface of
compromising secrets stored in such special files would be greatly
reduced for many services that need secrets, because the attacking
code must be run at boot time and wouldn't see the file content
during normal operation.
Yes, this sounds like something only the kernel can deliver.
      --ralf

@_date: 2016-11-29 12:53:22
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
BTW, my humble hack, using the ordinary file system to implement
the desired behaviour can be found here:
      And I'd really like to replace this with a proper kernel enforced
use of a secure read-once file. That's why I'm interested to
know if such a thing could be implemented (with the prospect to
become a reality).
      --ralf

@_date: 2016-11-29 13:01:45
@_author: Ralf Senderek 
@_subject: [Cryptography] RNG design principles 
BTW, my humble hack, using the ordinary file system to implement
the desired behaviour can be found here:
        And I'd really like to replace this with a proper kernel enforced
use of a secure read-once file. That's why I'm interested to
know if such a thing could be implemented (with the prospect to
become a reality).
       --ralf

@_date: 2016-11-29 21:37:06
@_author: Ralf Senderek 
@_subject: [Cryptography] read-once file, outside the filesystem ... or not 
It doesn't matter how many times the secret seed is read as long
as when the kernel sayes "boot finished" there's no more access
permitted by the kernel.
Yeah, and the point of attack is the kernel, the accusation
of security by obscurity is nonsense.
File permissions alone don't reduce the attack surface.
Trying to limit access to early boot is a valid concept,
even if you don't like it.
      --ralf

@_date: 2016-10-02 12:32:00
@_author: Ralf Senderek 
@_subject: [Cryptography] distrusted root CA: WoSign 
It's not. There's Cryptlib which has all the support for SSH you'll ever
want. (
For SSH (and even SSL) the configuration options are quite straight-forward:
You can use SSL/TLS with shared keys on any client using Cryptlib, see
page 115 of the manual. In addition you'll need to enable fingerprint
verification of the server (see page 121 of the manual), but as you have
control over all endpoints, that shouldn't be a problem in your There are (at least) bindings for JAVA, C/C++, Perl and Python for
Cryptlib. So these mechanisms *can* be used to build client applications.
A quote from the Cryptlib manual (page 233):
   "Certificate Trust Management
     In order to provide extended control over certificate usage, cryptlib
     allows you to both further restrict the usage given in the certificates
     CRYPT_CERTINFO_KEYUSAGE attribute and to specify whether a given
     certificate should be implicitly trusted, avoiding the requirement to
     process a (potentially large) chain of certificates in order to
     determine the certificates validity"
     more info (and code examples) can be found in the manual.
     --ralf

@_date: 2016-09-07 20:10:51
@_author: Ralf Senderek 
@_subject: [Cryptography] Strong DNS Names 
Isn't this a bit deceptive to call it "end to end", as the user's message will
hit the proxy unencrypted and will pop out at the other end's proxy for collection
by the other user's client software in plain text? And more so, because no user
has any control over what those proxies actually do (or don't do).
In my opinion, the main security issue with this scheme is the total lack of control
by users that rely on the information stored in online servers, which need to be
trusted but in (too) many cases may not be trustworthy. An ordinary user won't have
any assurance that his message will get out (properly) encrypted nor will a receiving
user know in which form the message was transferred, because he must delegate the
decryption to an online server he does not control.
Our recent discussion about trustworthy hardware made it clear that it is quite
difficult to reach any meaningful security assurances for user systems. But wiping
all user control off the table in designing a system, is - as I see it - going too far.
     --ralf

@_date: 2016-09-10 15:44:31
@_author: Ralf Senderek 
@_subject: [Cryptography] Secure erasure 
I read this as an endorsement of the idea of a "personal security server"
under the user's control. A separate hardware with a maximal level of
auditability that shuts itself (and its OS) off from network access
as much as possible, leaving only an encrypted tunnel to the user's
main machine which is the target of most malware approaching from
a huge number of sources.
For this "personal security server" the main issue will become assuring
its isolation and safeguarding the tunnel to its legitimate user, not
to ensure "secure erase" of secrets. In my view, the separation of
security critical actions (like message decryption) from the machine
the user (carelessly) uses for everything, is the most valuable step
forward, because we won't be able to defeat most attacks on the user's
desktop. But we may be able to strengthen a well-designed second system
to be much less vulnerable.
I know that it's quite difficult to ensure that only the legitimate
user can access the encrypted tunnel, because it'll require the
(safe) use of a secret. And so the "secure erase" problem must be
solved - one way or the other - on the user's main machine.
But the problem of running the whole bunch of safe crypto need not
be solved here.
    --ralf
PS: I assume that physical access to the "personal security server" can
     be provided, once it is under the user's control (no online server)
     and that primarily attacks through the network cable have to be
     prevented.

@_date: 2016-09-11 12:00:48
@_author: Ralf Senderek 
@_subject: [Cryptography] Secure erasure 
I have difficulties following this logic. Because if you continue to keep
traditional OSes insecure, which isn't very unlikely, easy attacks remain
and no-one will want (or pay for) a secure system?
Again, this logic presumes that every crypto is being done on the insecure,
traditional OS, so that a hole in the main OS devalues everything.
Why should an ordinary user not be interested in using a separate system
with faaaaar less vulnerabilies than the OS he normaly uses?
The norm is sending unencrypted email. Why should the desire to protect
emails better (the demand) suddenly appear once we've got good traditional
OSes, something we might wait for forever?
     --ralf

@_date: 2016-09-11 14:33:33
@_author: Ralf Senderek 
@_subject: [Cryptography] Secure erasure 
Jerry, I understand that argument, but I don't believe it.
The misconception here is the assumption that an attacker can gain
the same information easily as well as by subverting the security
You know that I speak in favour of a *separation* (normal,buggy
user machine vs separate. well-designed less vulnerable personal
security server) and here the information an attacker can gain is
considerably restricted compared with what can be exfiltrated if
everything happens on the (ONE) traditional device.
Anyway, the IBM/VAX story is well worth to be told (twice).
I just cannot figure out in what way it'll support your argument,
other than that it's crucial for the *separate device* to not
leave anything to chance, as much as this is possible.
      --ralf

@_date: 2016-09-11 15:12:41
@_author: Ralf Senderek 
@_subject: [Cryptography] Secure erasure 
I'd prefer a slightly less absolute adjective, because the unsafe
portion must interact with the (much more) secure portion in one
way or another. In my experience, the handling of access secrets
to the secure portion by the insecure part has to be guarded as
much as it is possible on the insecure machine.
In case of the Crypto Bone for instance, crucial secrets are visible
to a root-user daemon for a tiny time window during the first
stages of the (unsafe) booting machine and are not visible after the
boot process ended. This is not absolute protection of these
secrets but it raises the bar (enough), as you'd say to make the
attacker look for easier ways. To make sure, no easier way can be found,
that's what we can do.
     --ralf

@_date: 2016-09-12 07:22:43
@_author: Ralf Senderek 
@_subject: [Cryptography] Secure erasure 
Ray, I usually love your postings, but not this one, as you're
quoting totally out of context.
I agreed to Jerry's assumption that on ordinary (general-purpose)
OSes there are (and will be in the future) so many easy attacks
that it would be impossible to turn them (all) into a secure
I agreed to voice my opposition to the conclusion, that because
of this no-one is interested in (or will pay) for advanced
security measures, those you have in mind.
I did this to promote the idea of *separation* as a way forward
to achieve a (combined system: insecure user OS - more secure PSS)
level of security that cannot be achieved without a separate
device. This was the context.
If I remember correctly, heartbleed was such a critical thing
because of (at least) two facts:
a) Secrets, that are deliberately stored in memory like web server
    private RSA keys could easly have been exfiltrated, nobody
    knew if and when
b) the attack was possible for anyone sending a crafted packet to
    the server.
The way I propose the personal security server (PSS), b) would not be
an issue, because in order to send anything to the server you'd
need at least two different secrets. So separation would work
Long term secrets in memory are something that (IMHO) should not
be allowed on the devices I and Jerry had in mind, that's why
I introduced the daemon process that can access such secrets
only during a tiny time frame and sends it directly to the PSS
if it also has access to the secure tunnel secrets that are
stored encrypted in the file system.
But when it has to "secure erase" this secret in memory trouble
starts, because as many on this list have pointed out, your
secret might have gone anywhere and your code cannot control
(or prevent) this proliferation. But the separation would work
here too, because the database, holding message encryption keys
is stored on the PSS not on the user's device.
Isn't it strange that nobody addresses the idea of separation but argues to secure the one machine that is used for everything?
It might not be possible to secure the user's device but securing
the PSS can be less impossible.
So please folks, context, context and again context.
      --ralf

@_date: 2016-09-28 15:20:42
@_author: Ralf Senderek 
@_subject: [Cryptography] Use Linux for its security 
Everyone who complains about this situation should have asked himself:
"When did I last donate my time and effort to essential code review?"
(including efforts to reduce complexity).
And what are the alternatives? Use Apple for its security?
     --ralf

@_date: 2016-09-30 07:07:55
@_author: Ralf Senderek 
@_subject: [Cryptography] Use Linux for its security 
thanks very much for these links, as reading [3] (esp. section "progress")
gives a good idea of what has been done in the recent past and more
importantly, how to contribute to steady this progress.
     --ralf

@_date: 2017-12-27 19:47:43
@_author: Ralf Senderek 
@_subject: [Cryptography] Current State of Mailing Lists / Forums about 
It's my experience that some posts to this list resulted in a very
fruitful OFF-list conversation that in many cases lead to substantial
improvements of projects I worked on. But the subsequent request for
comment on these results were too often greeted with ignorance here.
I suppose the underlying problem is, that in this day and age, a consensus
on what constitutes a "solution" to a problem is no longer achievable.
There are holes and pitfalls everywhere and people putting their effort
into one specific project may be seen as following the wrong track as
they - as everyone else - are forced to rely on building blocks that can
be regarded as suspicious. There is no common ground any more, nor any
idea to agree on a way forward, that could claim any degree of promising
    --ralf

@_date: 2017-01-28 16:35:32
@_author: Ralf Senderek 
@_subject: [Cryptography] Is Factoring of large RSA moduli using Alternative 
The security of RSA depends on the infeasibility of factoring large
RSA moduli. Does this disturbing research paper change the state of
common wisdom about RSA's security?

@_date: 2017-07-12 07:01:46
@_author: Ralf Senderek 
@_subject: [Cryptography] A software for combining text files to obtain 
The original paper is by Ueli M. Maurer
"Conditionally-Perfect Secrecy and a Provably-Secure Randomized Cipher"
But there is no mention of the moon-image.
This reference is made in Schneiers criticism:
     --ralf

@_date: 2017-06-20 08:14:28
@_author: Ralf Senderek 
@_subject: [Cryptography] Predictions regarding Simon and Speck for 2019 
I've decoded your block.
The first line of the cleartext reads:
"Donald is no longer President of the United States."
And the last line reads:
"Maybe I'll have more luck with my predictions for 2020."
     --ralf

@_date: 2017-03-15 19:43:41
@_author: Ralf Senderek 
@_subject: [Cryptography] Crypto best practices 
In an attempt to enhance security the wise people at OpenSSH have changed
the defaults to only accept AES-CTR and AES-GCM in addition to some stuff
they've come up themselves:
chacha20-poly1305 at openssh.com,aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm at openssh.com,aes256-gcm at openssh.com
This makes it hard for a standard-complient ssh client implementation to
even talk to openssh servers without changing the defaults to include
AES-CBC mode. A gread idea to fuel cryptographic progress.
       --ralf

@_date: 2017-03-18 19:41:35
@_author: Ralf Senderek 
@_subject: [Cryptography] Crypto best practices 
As always, details matter. What we need are solutions to specific
problems, (like secure authenticated messaging) in which every
aspect of the solution can be justified as a necessary part of
the secure system's required behaviour.
And that does include the OS and its numerous shortcomings w/r/t
maintaining the possibility of secure code running as expected.
In other words, the only thing we have to worry about are secure systems
and not secure primitives in an abstract space.
      --ralf

@_date: 2017-03-19 09:54:47
@_author: Ralf Senderek 
@_subject: [Cryptography] Crypto best practices 
Looking at the system as a whole, robustness is a vital part of security.
It's of no use to have a system that's pretty secure in sunshine and light
winds, but falls apart at the first attempt to use it "creatively".
Building secure systems is what Ross Anderson called "programming Satan's
computer" decades ago:
      And if Satan demands the use of a certain proprietary OS, I'd rather
give up than fool people into thinking they can get a robust and
secure system.
Because of the fact that we have to deal with the complexity of systems
it is of vital importance that we reduce it.
      --ralf

@_date: 2017-03-28 20:00:18
@_author: Ralf Senderek 
@_subject: [Cryptography] "Perpetual Encryption" 
I stopped reading at the first bullet in the feature list:
    "Our solution is 10**2158 times more secure than existing Industry standards, meaning its Quantum Compute & AI secure."
No mention of how my data gets to their famous platform securely.
     --ralf

@_date: 2017-10-12 22:23:20
@_author: Ralf Senderek 
@_subject: [Cryptography] ? recommendations for secure communications 
No, I haven't. The Crypto Bone leaks the information that A has sent an encrypted
message to B via ordinary email.
     --ralf

@_date: 2018-08-09 23:11:49
@_author: Ralf Senderek 
@_subject: [Cryptography] PGP -- Can someone help me understand something? 
If you encrypt your plain text twice PGP (using AES) would produce a different cryptogram. That is because there is an element of randomness
introduced in the encryption process (google for "random IV").
So the key you don't know contains that random IV and the encryption "key"
so that every time you encrypt your plaintext another (unpredictable)
key is being used. That's why you cannot derive the key from the cryptogram knowing the plain text.
    --ralf
PS: Someone knowing the "key" does also know the random IV and consequently can decrypt the cryptogram. To understand this,
you don't have to delve into asymmetic cryptography, just make
yourself familiar with the practical uses of AES.

@_date: 2018-03-23 07:25:35
@_author: Ralf Senderek 
@_subject: [Cryptography] Avoiding PGP 
There is such a thing as choosing insecurity while there is no such thing
as choosing security. It's a bit like eating healthy. Everyone wants their
food being safe but even the most aware person can only try to avoid food
that'll harm with no guarantee of success. People who want more integration
between their nodes of communication have already made their choice.
The separation of private and non-private communication is a necessary
step but no guarantee of security. People who don't want that separation
won't even be in the draw for private communication no matter how blue
in the face they become by demanding other people to provide security
for them.
That's true and it took us a long time to learn this, but there are
solutions available that can be used without burdening the user too much.
The crucial point (apart from the initial choice) is whether these
solutions rely on a third party service or not.
     --ralf

@_date: 2018-05-07 20:23:06
@_author: Ralf Senderek 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Seriously, nobody expects a user to enter a (RSA) secret key into the computer to use it. What can be expected though is that the user enters a
sufficiently complicated passphrase which will be used to AES-encrypt
the secret key, and that this secret is NOT stored in the computer but
only in her brain. In additon it can be expected that without entering
this passphrase the secret key cannot be used.
To assume that critical comments require absurd procedures is - well - not       --ralf

@_date: 2018-10-15 19:39:15
@_author: Ralf Senderek 
@_subject: [Cryptography] Random permutation model for encryption as a 
IMHO no. It is not necessary to teach the inner workings of AES but at the very least the fundamental idea of a Feistel cipher and the problem of
using short keys (ie. brute force attack on DES since 1999) would lay
a more serious foundation for the understanding of modern cryptography
than fiddling with permutations.

@_date: 2019-04-15 10:04:32
@_author: Ralf Senderek 
@_subject: [Cryptography] Making scenarios realistic 
Now that selling key escrow seems to be the business model you fancy,
you may put this study
on the new enterprise's web site.
      --ralf

@_date: 2019-04-15 19:39:26
@_author: Ralf Senderek 
@_subject: [Cryptography] Making scenarios realistic 
In 1997 I happened to know people who already tried to broaden the user base of PGP keys in an academic environment including the improvisation of user interfaces to PGP. But the common mindset was the opposition to key escrow in any form, because key escrow is very different from key availabilty/backup which was a pain in the neck back then, and still is.
Even if your numbers were correct (in the open source community a handful of keys secure the integrity of a large number of OS packages, and almost all users are unaware of their "use" of GPG keys) the lesson to be learned here is that key management is the problem to be solved. But it has to be solved in a way that the user can contol himself, not by key escrow.
     --ralf

@_date: 2019-04-15 22:31:32
@_author: Ralf Senderek 
@_subject: [Cryptography] Making models + scenarios realistic 
And that's why I've asked for a threat model for the MESH.
     --ralf

@_date: 2019-01-25 22:18:01
@_author: Ralf Senderek 
@_subject: [Cryptography] Stupid question on S-boxes 
I'd say the answer to this question is a NO.
What is the difference between a shared CPU and a shared (isolated) co-processor ?
     -- ralf

@_date: 2019-01-26 13:47:35
@_author: Ralf Senderek 
@_subject: [Cryptography] Stupid question on S-boxes 
Of course using a co-processor does limit attacks, but the isolated
co-processor doing "safe crypto processing" has to be authorized to
do something valuable - like a signature - by the CPU on which the
attacker controlled code is running also. So even if there is less
of a risk of leaking key material, we're miles away from "the ability
to safely do[ing] crypto on shared hardware", which was my point.
      -ralf

@_date: 2019-03-21 19:47:11
@_author: Ralf Senderek 
@_subject: [Cryptography] Mix Messaging in the Mesh 
Answer: Maximum security and minimum complexity.
    --ralf
PS: And why do you think the other stuff is of any substantial importance?

@_date: 2019-03-22 08:12:17
@_author: Ralf Senderek 
@_subject: [Cryptography] Mix Messaging in the Mesh 
IF the "maximum security" approach really was a "complete" failure, then you'd have to consider the miserable job of easy-to-use key management,
the insane tendency to increase complexity in order to match desired features and - last but not least - the ordinary user's expectation, that security must be done automatically without any intervention on their behalf, not to mention the problems that reduce the security of the systems they use, which have been discussed here in detail.
The last thing that can be blamed for that failure is maximum security.
     --ralf

@_date: 2019-03-24 14:17:20
@_author: Ralf Senderek 
@_subject: [Cryptography] Clinton email issues 
I'd like to see a concise threat model for the MESH here.
If switching to a different email address ist all the user
has to do (plus using the proxy) then all the crypto has
to be done in that proxy on the user's machine and you'll
have to answer at least
    a) which crypto library you use, how and why
    b) how you intend to protect the encryption keys
    c) how key management is done on the user's machine
       without the user selecting keys
    d) why the user can be sure she is talking to the
       right person and no one else.
    e) how you will ensure proper performance of a-d
       on a bunch of different OS (some of them proprietary)
If the crypto is not done on the user's machine, then you are in "We're running an invincible server, give us your
plaintext via https and we'll do the rest to make you secure" land and then you should stop calling this thing
end-to-end encryption.
   --ralf

@_date: 2020-07-05 18:46:14
@_author: Ralf Senderek 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
In an off-list conversation with Benne de Weger I volunteered to
find out if the private key d would be large enough if I tried
to make the public exponent bigger than F4=e=65537L using CRYPTLIB.
In essence, I found that whatever value of e you assign (in the
limits that cryptlib permits) d will always come out as big as n.
I defined p and q to be 1024 bits so that n is 2048 bits and
the smalles d I got was 2040 bits in size. I had to castrate
Peter's code for this experiment and disabled every precaution
cryptlib has in store in order to manage to increase e over the
limit of 32 bits. I finally gave up when I reached size(e)=64 bits
because at this point I had to fill e with a bignum taken from
a hex value, which didn't succeed. So I told Benne, that his
assumption that d will almost never be much smaller than n
regardless of the size of n was compelteley in line with my
This IMHO may clarify the case for RSA key generation.
I can't wait to hear of the DH case.
     --ralf

@_date: 2020-07-05 18:51:05
@_author: Ralf Senderek 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
Sorry for the typo: I should have written:
that d will almost never be much smaller than n
regardless of the size of e (not n) was completely in line with my
    --ralf

@_date: 2020-06-04 22:44:59
@_author: Ralf Senderek 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
In which implementation do you think I found this?
typedef struct {
          /* Status information */
          int isPublicKey;                        /* Whether this is a public or private key */
          /* Public components */
          unsigned char n[ CRYPT_MAX_PKCSIZE ];   /* Modulus */
          int nLen;                                       /* Length of modulus in bits */
          unsigned char e[ CRYPT_MAX_PKCSIZE ];   /* Public exponent */
          int eLen;
    --ralf

@_date: 2020-06-05 10:33:07
@_author: Ralf Senderek 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
Well, everything can be overdone, because forcing a 2048 bit n to be used
with a 2041 bit e will give them quite a handy, small private decryption
exponent d.
But in a 2012 paper "Ron was wrong, Whit is right" [1] the researchers
found that in a collection of 11 million RSA-keys found in the wild
there were a disturbingly large number of moduli which were either
shared between different RSA-keys or which at least had ONE common prime
factor. They wrote :
    "Compared to the collection of certificates considered in [12], where
     shared RSA moduli are ?not very frequent?, we found a much higher
     fraction of duplicates. More worrisome is that among the 4.7 million
     distinct 1024-bit RSA moduli that we had originally collected, more
     than 12500 have a single prime factor in common. That this happens
     may be crypto-folklore, but it was new to us, and it does not seem
     to be a disappearing trend: in our current collection 3 of 7.1 million
     1024-bit RSA moduli, almost 27000 are vulnerable and 2048-bit RSA
     moduli are affected as well."
I don't want to re-ignite our anually randomness thread here, but I doubt
that the situation today is very different from 2012 and adding another
constant to the mix by fixing e to a commonly used sub-32-bit value does
not increase my feel-good factor much.
At least it's nice to know that it is possible to moderately incresase
the length of e if the implementation has made proper precautions.
     --ralf
[1]

@_date: 2020-06-08 21:58:33
@_author: Ralf Senderek 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
If I'm not mistaken then phi(n) = (p-1)*(q-1) is of roughly the same size
as n.
And if e*d = 1 mod phi(n) , then you *might* find a small d if e is large.
You may as well find a large d if e is large, but any small one that fits
the bill will decrypt your ossifrage.
    --ralf
PS: doesn't convince you?

@_date: 2020-06-09 12:54:34
@_author: Ralf Senderek 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
... I'll skip the Einstein quote about theory and practice because I have
no reason to dispute your findings above.
But I'd like to add the CONTEXT that has gone missing by now.
In an implementation that people really use with RSA keys it's not the
ideal world of mathematics. Florian complained about an implementation
of a crypto library that fixes e=3. The mathematical answer to this
(as argued above) is "don't worry there ar sooo many d's, all is fine"
Adding another piece of context, I myself referred to a 2012 research
that found a worring number of re-used prime factors and asked if in
such a situation fixing e to sub-32 bit is prudent. I did this after
Peter answered my observation that there are crypto implementations
that allow for a range of e values the size of n in their datastructure
with an anectdote that some people see this as necessary, which
led to my top-of-the-head example that sparced the mathematical view
expressed above. My point was, that it is not prudent to fix another
parameter to a low value if in the wild you can find X-509 RSA moduli
with common prime factors (and maybe worse).
I wonder if your answer with regard to the practical consequences of
real RSA use here and now is still "don't worry there ar sooo many d's,
all is fine"?
And adding a last piece of context. The original poster, John Gilmore,
brought up another implementation question, what's going on behind
the scenes in Zoom's crypto bulletin board that distributes meeting
keys? Is it possible for an adversary to deny a legitimate user access
to a meeting by posting some carefully crafted nonsense to the board.
I myself feel guilty to have deviated the postings away from John's
(in my view) still unanswered question. So fellows, let the context
be with you. Always.
      --ralf
