
@_date: 2001-08-25 18:40:19
@_author: Jon Callas 
@_subject: New Microsoft Security Server* (*runs Solaris) 
They also have a flash demonstration about the wonders of Windows XP and
how great it is. There are many pictures in it of happy people in front of
various PC. A couple of them are people sitting in front of Macs. (One in
particular is a tower that is obviously the 8600/9600 series, and the
people are typing on what is obviously a Saratoga keyboard.)

@_date: 2001-08-31 14:11:20
@_author: Jon Callas 
@_subject: CIA funds anonymous web surfing 
Maybe. Maybe not.
One of the bits of irony about Sklyarov's case is that the brouhaha was
happening at the same time was the Chinese jailing Chinese-American
academics for "spying."
On some level, "spying" and "infringement" are the same thing: both are the
misuse of information. Alice gives Bob some information, and Bob gives it
to someone Alice would rather not have it. Alice has a non-linear response
to this. Spying, infringement; infringement, spying. You say tomayto, I say
tomahto. If you spend five years in a foreign pokey for it, what *is* the
difference? The biggest problem of the DMCA is that it makes copyright
violations a criminal, not a civil offense. It also shifts policing from
the holder to the state.
If the Chinese put a price tag on information, and instituted criminal
infringement penalties, then WIPO and the US would probably have no choice
but to back them.

@_date: 2001-07-02 14:42:03
@_author: Jon Callas 
@_subject: Crypographically Strong Software Distribution HOWTO 
In our last episode ("Re: Crypographically Strong Software Distribution
HOWTO", shown on 7/2/01), Kent Crispin said:
Hans Dobbertin found some weaknesses in MD5 in 1996. I found two quickie
references, a note by Dobbertin on the issue:
and his paper on the weaknesses:
The short answer is that he found weaknesses in MD5 similar to the
weaknesses found in MD4 before it was broken. The message since then has
been "don't panic, but use a newer algorithm for new work." (In fact, the
Dobbertin note above says not to panic, but start looking for better
The answer is that you SHOULD (in IETF terms, see RFC 2119,
 for a definition of MAY, SHOULD,
MUST, etc.) use SHA-1. In plain language, what this means is that if you
don't know when to use MD5 and when to use SHA1, then use SHA1. If you pick
MD5, be prepared to answer people when they ask why you did. If for some
reason you don't want to use SHA1, look at RIPE-MD160. If you don't like
either of them, there are other choices, but we now start getting into
subtlety and taste.
On the other hand, in the intervening five years, we haven't seen a break
in MD5 appear. So maybe it's not as bad as we thought. Nonetheless, if you
have a choice and you don't know what to do, pick SHA1. At the very least,
no one will send you an email that starts, "Why did you use MD5? Don't you
know that Hans Dobbertin...."

@_date: 2001-06-22 12:58:10
@_author: Jon Callas 
@_subject: crypto flaw in secure mail standards 
This is a really good issue you've brought up, brilliant and creative.
However, like Derek said, this isn't a crypto problem. I'm going to go
further and say that it isn't even an engineering problem.
You demonstrate some interesting problems with secure messaging, but *none*
of them have anything to do with cryptography. They all have to do with
semantics, expectation, and human behavior.
Both of the scenarios you give are perfectly plausible. They could happen.
However, they don't *have* to happen that way and presume certain
conditions that are at best specialized.
Let's take the first one. This one presupposes that Alice's signed message
says, "The Deal is off." Note that if Alice had said a number of other
things, there would be no problem.
Suppose Alice's message to Bob is: "Dear, Bob, I'm sorry to send you some
bad news, but my company has had a reorganization, and we cannot pursue our
deal with XYZcorp at this time. I enjoy working with you, and hope that we
will be able to re-activate this deal at a future date."
Now there's no attack. If Bob sends *this* message to Charlie, then
Charlie's going to scratch his head and call Alice by phone. Then they'll
check the email headers, and see that it came from a hijacked IIS server in
The real problem here is that there are some terse messages that it's a
very bad idea to sign. For example, "The deal's off." Also, "Your mother
wears army boots," "So's your old man," and "Take a long walk off a short
Cryptography cannot solve the problem of appropriate use of the technology.
Let me give a related "attack." Suppose before she cancels the deal Alice
sends Bob a message that says, "I'm really glad I'm working with you and
not Charlie. He's a real twit, and I have to grit my teeth every time I
deal with him." After canceling the deal, Bob then sends *that* message
with Alice's signature to Charlie. Cryptography can't solve *that* problem,
either. My dear, late friend, Marin Minow had a maxim, and that maxim is,
"Don't send anything by email that you don't want to see attached to your
resume." That can be extended to really, really, not sending a signed
document that you don't want to see attached to your resume.
I will also point our here, that the attack you give needs no encryption.
This is why I say it isn't even an engineering problem. It works equally
well with a clearsigned message. Adding in encryption weakens your case.
It's a more powerful attack on signing alone because anyone who finds that
message can retarget it.
My response, simply put, is don't sign a vague message like this:
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
The deal's off
because you'll be subject to retargeting. There is nothing a cryptographer
or engineer can do to protect such an easily misunderstood message.
The next problem you give is more interesting. It's again, misuse rather a
crypto problem, but it strikes at the heart of two unsolved issues with
digital signatures:
The answers to those questions are in my opinion, "Whatever you want them
to" and "Yes." Again, your demonstrations are brilliant examples of how you
can misuse a signature into some sort of semantic attack.
The first question is a swamp, so I'll only dance around it. I know people
who regularly sign all their email. I know people who refuse to sign email
(or rarely do). Each of them has a good explanation for why they do what
they do. For full disclosure, I rarely sign messages. Since I rarely sign
messages, it's relatively easy for someone to forge one coming from me. On
the other hand, since I don't sign messages out of habit, I'm not going to
accidentally create a retargetable message. But what this shows is that if
you find a signed document in the wrong hands, the assumption that the
signer sent it is flat silly.
The second question strikes at the very heart of one of the biggest
fantasies there is with digital signatures: non-repudiation. I don't
believe that non-repudiation exists. This second example is not an attack
on cryptography, but a brilliant attack on the notion of non-repudiation.
Stan Kelley-Bootle has a marvelous definition in "The Devil's DP
Dictionary" for "GIGO" that is, "Garbage In, Gospel Out." Sheer brain-dead
fantasies having been run through a computer become holy and divinely
inspired. Similarly, people think that a digital signature makes real-world
considerations go away, and alas, the people who believe this most are
lawyers (who should know better).
Let's analyze that second problem. Someone goes to Alice and says, "Hey,
Charlie has a catalog signed by you." Alice says, "Who's Charlie? I've
never heard of Charlie. I've never sent sensitive company material outside
the company." We all know that it's true. Alice didn't. Bob did. But we
also know that it's plausible for the corporate investigator to think Alice
did because of this Garbage In, Gospel Out fantasy as applied to signatures.
When I go on my "there's no such thing as non-repudiation" rant, I usually
focus on the difficulty of securing a private key. I really like this
scenario, because it shows another attack on non-repudiation -- taking
things out of context. Thank you! This is the true attack, that it's a
semantic attack on what the message *means*, not how it's constructed. A
signed message (because again, it works just as well if it's signed without
being encrypted) out of context means nothing. Signatures do not grant
meaning, and in fact can easily misdirect or obscure it.
The real problem is not one of cryptography, it's one of belief. It is
believing that a message containing a signed object came from the person
who signed it. It is believing that cryptography protects meaning, not
merely bits. It is believing that real-world problems with the interactions
of people can be solved with a bit of fancy math. These are all ludicrous,
and thank you for coming up with another attack on them.

@_date: 2001-09-27 15:57:28
@_author: Jon Callas 
@_subject: collecting an Enigma? [was:  Antiques man guilty of Enigma   
Yes. They show up from time to time in various places, and a few times a
year on eBay, even.
A civilian Enigma like a NEMA should run between US$2K-3K. A military
Enigma should run around $5K-7K, less if it isn't in working condition or
is missing rotors, more if it has some interesting provenance.
You can get Hagelin M209 machines in the US$1500-2000 for ones in really
spiffy condition. There's a M209 canvas case up for auction now on eBay.

@_date: 2002-08-01 20:47:05
@_author: Jon Callas 
@_subject: Challenge to David Wagner on TCPA 
Is this a tacit way to suggest that the only people who need anonymity or
pseudonymity are those with something to hide?
    Jon

@_date: 2002-12-08 15:34:33
@_author: Jon Callas 
@_subject: PGPfreeware 8.0: Not so good news for crypto newcomers 
This is a bit unfair. PGP 5 could not comply with the OpenPGP spec, as it
pre-dated it. OpenPGP started with PGP 5, and then made a number of changes
based upon what the IETF working group wanted. RFC 2440 was finalized in
November '98, which was post-PGP 6.
It is, however, true that PGP 6.5 was not been as good as it could have been
in 2440-compliance (but neither was GnuPG in those days, either).
This, I believe to be partial mis-remembering. PGP 6 came out in July 1998,
and I don't think GnuPG existed then.
Nonetheless, thanks for the story. I go on and on myself about how important
software quality is, and your anecdote emphasizes this. Here we are four and
a half years later, and the bad taste left in your mouth by this bug causes
you to still be against the product.
All software developers can learn from this.
    Jon

@_date: 2002-06-25 20:57:09
@_author: Jon Callas 
@_subject: Ross's TCPA paper  
I think it even goes further than that.
I was giving one of my DMCA-vs-Security talks while l'affaire Sklyarov was
roiling, and noted that while that was going on, the US was being testy with
China over alleged espionage by US nationals while in China. At a high
level, each of infringement and espionage can be described as:
Alice gives Bob some information. Bob is careless with it, disclosing it to
someone that Alice would rather not see it. Alice has a non-linear response.
You can call it infringement or you can call it espionage, but at the bottom
of it, Alice believes that a private communication has been inappropriately
disclosed. She thinks her privacy has been compromised and she's stomping
angry about it.
At the risk of creating a derivative work, you say pr-eye-vacy, I say
pr-ih-vacy. Infringement, espionage, let's call the whole thing off.
    Jon

@_date: 2003-11-19 16:51:32
@_author: Jon Callas 
@_subject: Gresham's Law? 
This is exactly what I said in my talks and testimony about the DMCA. I referred  to Gresham's Law as it applies to security. I also have called the DMCA "The Snake-Oil Protection Act."
This is indeed the only case I know of where government has given protection and preference to inferior systems over superior ones.

@_date: 2004-08-12 15:27:07
@_author: Jon Callas 
@_subject: Cryptography and the Open Source Security Debate 
The relevant key generation code can be found in:
(those are backslashes on Windows, of course). The RSA key generation, for example is in ./pgpRSAKey.c.
You might also want to look at .../crypto/bignum and .../crypto/random/ while you're at it.
There is also high-level code in .../crypto/keys/pgpKeyMan.c for public key generation.
Incidentally, none of the issues that lrk brought up (RSA key being made from an "easy to factor" composite, a symmetric key that is a weak key, etc.) are unique to PGP. This should be obvious, but I have to say

@_date: 2004-12-16 22:58:33
@_author: Jon Callas 
@_subject: The Pointlessness of the MD5 "attacks" 
That's not what Ben is saying at all. He's saying that once you give the adversary the power to do the sorts of things that are required for this (like being able to replace a give C with C'), there are easier ways for the attacker to get the desired result than playing with I do, however, feel the need to be a bit pedantic and say that tables for state machines are seldom random (for some suitable definition of random). Nor are graphics, sound, nor video. Inserting the artifacts into them you need to make this work is really, really obvious for the same reasons that Shamir and Van Someren showed that finding key material is so easy.
I have an attack that I just came up with that pretty much proves Ben's point. I can, using this technique, make any MD5 preimage give you any desired hash value. It's trivial, once I can replace code C with C'.
Give up? Answer below.
Hint: it works just as well against SHA1. Or SHA-256. Or Whirlpool. Or pick your hash.
patch the md5 software. put in a table that gets searched -- when you see hash x, return y. if you want to be clever, obfuscate the check and the result. toss in some xoring so you don't have the direct target and result hashes there, so simple grepping doesn't give the trick away. But once you can replace C with C', why bother doing bit-flipping when you can just compile the code you want, and replace the code that rats you out?

@_date: 2004-07-19 14:05:58
@_author: Jon Callas 
@_subject: New Attack on Secure Browsing 
(Sent from this account, since I am subscribed from here.)
This is a favicon -- a logo icon for the site. Lots of sites use them. PGP has had this on our for a couple of years, now. I vaguely remember there being one in The Dark Days, but I could be misremembering. This is the first bit of confusion I've heard about it.
PGP's logo icon has been a padlock at least since the O'Reilly book used it in January of '95. This is before there even was an SSL. That particular icon is the very same one that was used as the tray icon in some version of PGP or other (we think PGP 7).
We're giving this all due consideration. Would it help if we changed the metal, perhaps from the current four-plane brass to eight-plane steel or even to alpha-channel Jolly Rancher iridescent translucent anodized titanium?

@_date: 2004-10-22 14:34:58
@_author: Jon Callas 
@_subject: Crypto blogs? 
Matt Hamrick's Cryptonomicon.net is good.
There are also my PGP CTO corner articles at

@_date: 2004-09-02 01:26:45
@_author: Jon Callas 
@_subject: "Approximate" hashes 
What you want is what's called a canonicalization function. You want to hash not T, but F(T), and F can de-tabify, and so on.
As has been mentioned, OpenPGP has a canonicalization for text and cleartext signatures (and we debate what it should be, even). XML Digital Signatures has one.
The major other issue you have to deal with is whether the canonicalization is an interpretation, a conversion, or an assurance.
If it's an interpretation, then you are hashing F(T), and there's always some slim chance that there's a collision between two texts that canonicalize to different values, but hash to the same. I wouldn't worry about it, myself. Much. (Assuming a decent canonicalizer, of If it's a conversion, then you're replacing the text with the canonicalized text. This puts the canonicalization in the face of the users, but removes the problems handwaved at above.
If it's an assurance, then if the text is not canonicalized, then it's not a valid message if it doesn't meet the requirements. A message with a tab, for example, just isn't a valid message. Don't even hash it, return an error. Or alert that it was an invalid message.

@_date: 2005-02-07 18:13:23
@_author: Jon Callas 
@_subject: Is 3DES Broken? 
Because if they did, some smartass would chime in and say that ECB mode is perfectly fine at some speeds.
For example, you could safely encrypt one bit in ECB mode, particularly if you permitted, nay encouraged the other 63 or 127 to be arbitrary nigh unto random. Surely you don't need to have an IV and padding and all if the small block were random-padded.
We'd then get into a long debate over how many bits can be handled in such a system. 32? 127? 128?
Then some other smartass would suggest that it's more efficient in such a case to just XOR the key on the data and effectively just use a one-time pad. And then we'd digress into a rambling discussion on one-time pads and how practical they are in "real" applications.
Finally, some uber-smartass would point out that you can even get rid of the OTP by taking those small bits of data and padding appropriately and using a public-key op.
By then, we'd all have lost sight of the fact that the main topic here is whether 3DES is "broken" and that the answer is a simple, "no." (And it's a good thing that this is Cryptography, not Cypherpunks, as then there'd be another digression about Nader and how good the Corvair was or wasn't, along with URLs of nicely restored examples on eBay.)
This is why no one has had the temerity to suggest that ECB mode is unsafe at any speed.

@_date: 2005-06-23 10:40:09
@_author: Jon Callas 
@_subject: Protecting against the cache-timing attack. 
One of the things to remember in all of this is that one of the reasons we picked Rijndael as the AES was its speed. (And yes, I mean "we." I was present at the conferences, and I filled out the little poll about which ciphers I liked and why. That means I participated and bear part of the responsibility, along with everyone else who was there.)
Rijndael is a fast little bugger. It is the fastest of all the candidates. This means that if we're forced to slow it down a little, then that isn't really so bad (assuming it can be done easily).
Bulk encryption on modern cpus is really, really fast. We're making servers that do a lot of encryption at PGP Corporation. We also do a lot of performance testing. One test that we did where we tested how many messages 10K in size we could do in an hour, and then 100K in size saw a performance drop of between 1 and 10%. Yes, between one percent and ten percent!
Bulk encryption is not the bottleneck for a protocol like PGP. The bottleneck is some combination of public key encryption and message invariant data formatting.
So let's conduct a small thought experiment. Take the set of timings T, where it is the timings of all possible AES keys on a given computer. (It's going to be different depending on cpu, compiler, memory, etc.) Order that set so that the shortest timing is t_0 and the longest one is t_omega. Obviously, if you delay until t_omega, you have a constant-time encryption.
I'm going to assert that if you delay only until t_omega-1 (by which I mean the first timing smaller than t_omega. It is possible that there is more than one key with the timing of t_omega. Actually, I think there *must* be a set of them through some basic combinatorics.) then the system is still secure enough -- that the attack is infeasible. By that I mean that the timing attack doesn't work. Let's just presume this is true.
So then there are two questions:
What is the largest t_n for which my assertion is still true? In other words, what's the minimum delay time?
What is the smallest t_n for which it's still feasible to make the timing attack?
"Feasible" is a squishy thing here, but we can get a handle on it. Take the example Perry gave -- a wireless router that you can suitably inject packets into. If I assume that it's using AES-256, and that the key is changed once a decade, and that the attacker consumes 100% of the bandwidth of router, then the attacker can only get P timings, and P is going to be much smaller than 2^256. That means that there must be some t_p that is a delay short enough that makes the attack impractical for a set of statistical parameters (like that the chi-square of my key guess is less than c).
Now if I'm completely wrong, and you *must* always delay to t_omega, that's *also* an interesting answer -- actually more interesting because it's counter-intuitive.
So -- is there a way to make some calculations about a small t_m, a delay that's kinda skanky, but short, and t_n, a delay that we'd all feel good about? Heck -- how do we even find out what t_omega is without having to enumerate the set? Or even more practically, if I just pick a t, how do I know that it's at least as big as t_omega?

@_date: 2005-11-09 14:27:46
@_author: Jon Callas 
@_subject: gonzo cryptography; how would you improve existing cryptosystems? 
But OpenPGP does. Here's an extract fro RFC 2440:
5.1. Public-Key Encrypted Session Key Packets (Tag 1)
    An implementation MAY accept or use a Key ID of zero as a "wild      or "speculative" Key ID. In this case, the receiving implementation
    would try all available private keys, checking for a valid decrypted
    session key. This format helps reduce traffic analysis of messages.
Now, there has been much discussion about how useful this is, and  there are other related issues like how you do the UI for such a  thing. But the *protocol* handles it.
You might also want to look at the PFS extensions for OpenPGP:
and even OTR, which is very cool in its own right (and is designed to  take care of the sort of edge conditions all of these other things

@_date: 2006-08-22 07:56:09
@_author: Jon Callas 
@_subject: A security bug in PGP products? 
The guy's basically confused. I wrote a long thing at the time to  bugtraq with lots of detail. He's got two basic claims.
The first is that if he makes a copy of a disk file, changes the  passphrase on the copy, and then uses a hex editor to paste the  passphrase reduction back onto the copy. Poof, the old passphrase  works again. This is like saying that you can use emacs to edit a  file and change "123" to "ABC" and then use a hex editor to change  0x41 0x42 0x43 to 0x31 0x32 0x33 and ZOMG! The change magically  vanishes! As Ondrej Mikle points out, the disk hasn't been re- encrypted. If you want the disk to be re-encrypted, you press the big  "Re-encrypt" button in panel.
The other thing he did was that he found some code that basically does:
if (user-types-right-passphrase)
And then he patches out the if statement and notices that the disk  will mount, but curiously is lots of random garbage. He leaves as an  open problem how to make the disk readable after patching out the if

@_date: 2006-12-04 13:44:36
@_author: Jon Callas 
@_subject: Can you keep a secret? This encrypted drive can... 
I just ran a speed test on my laptop. Here are some relevant excerpts:
Cipher    Key Size  Block Size  Enc KB/sec  Dec KB/sec
--------  --------  ----------  ----------  ----------
IDEA      128 bits   8 bytes      24032.09    24030.66
3DES      192 bits   8 bytes      10387.67    10399.30
CAST5     128 bits   8 bytes      29331.17    29459.49
Twofish   256 bits  16 bytes      20233.63    19185.82
AES-128   128 bits  16 bytes      44100.23    46266.98
AES-192   192 bits  16 bytes      39731.33    41228.87
AES-256   256 bits  16 bytes      36017.95    37302.43
Blowfish  128 bits   8 bytes      35347.34    38311.22
Comparing AES-128 and AES-256, encrypt speed is 1.2243959x and  decrypt is 1.2403208x. So that makes my lick-your-finger-and-stick-it- in-the-wind rule of thumb of 20% slower okay. I'll try to say 20-25%  in the future.
Of course, though, implementation matters a lot. I'm running a PPC-32  machine. You'll get different answers on an ia32, and different ones  an AMD64.

@_date: 2006-12-08 13:15:19
@_author: Jon Callas 
@_subject: [-SPAM-] Re: Can you keep a secret? This encrypted drive can... 
That was using pgp --speed-test. It's an algorithm-level test, but  it's calling the SDK so there's some API-level overhead involved. I  got the number from a 3.0GHz x86, and it was 1.36 for encryption and  1.37 for decryption. But I also got the numbers from a 2GHz Core Duo  laptop and it was 1.12 for encryption and decryption. On the other  hand, the fast machine was encrypting AES-128 at 66389.45 KB/s and  the slow one at 22217.39 KB/s, which means that the 3GHz machine is  running at just shy of 3x the speed of the 2GHz machine!
Obviously, there are other factors, such as cache, memory, and so on  that are huge differences. I'd take a "slowdown" of 12% to 40% if I  was getting a 300% base speedup.

@_date: 2006-12-13 14:38:40
@_author: Jon Callas 
@_subject: quantum crypto rears its head again. 
Thanks for writing your note at the bottom. Quantum cryptography is a  fascinating thing, but first of all, it's not cryptography. It should  be called quantum secrecy, or something akin to that. Next, its  proponents have a tendency to effectively say, "Oh, math, that's  something that could go bad. But physics, *that* will always be good!"

@_date: 2006-02-28 07:46:50
@_author: Jon Callas 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
I have to chime in on a number of points. I'll try to keep commercial  plugs to a minimum.
* An awful lot of this discussion is some combination of outdated and  true but irrelevant. For example, it is true that usability of all  computers is not what it could be. But a lot of what has cruised by  here is similar to someone saying, "Yes, usability is atrocious --  here, look at this screenshot of Windows 3.1." Someone else pipes up,  "You think that's bad, let me show you this example from the Xerox  Alto. What*ever* were they thinking?" And then someone else says,  "Yeah, and if you think that's bad, look at what 'ls' did in Unix  V6!" Then when someone else says, "Y'know, I'm using the latest  version of Firefox, and it's actually pretty good" the next message  says, "But what about the Y2K issues, and what happens when in 2038?"  I swear, guys, this thread is the crypto version of the Monty Python  "Luxury" sketch.
* Whitten and Tygar is a great paper, but it was written ages ago on  software that was released in 1997. Things aren't perfect now, but  let's talk about what's out there now. Even at the time, one of  Whitten's main points is how hard it is to apply usability to  security, because of how odd it is. As a very quick example, in most  forms of user design, you let exploration take a prominent place. But  it doesn't work in security because you can't click undo when you do  something you didn't intend.
* There are new generations of crypto software out there. I produce  the PGP products, and PGP Desktop and PGP Universal are automatic  systems that look up certs use them, automatically encrypt, and even  does both OpenPGP and S/MIME.
They're not perfect, and lead to other amusing issues. For example,  an hour ago, I was coordinating with someone that I'm meeting at a  conference. I got a reply saying, "I'm at the airport and can't  decrypt your message from my phone." I hadn't realized that I *had*  encrypted my message, because my system and my colleague's system had  been doing things for us.
I habitually send most of my email securely, but I don't think about  it. My robots take care of it for me. I tune policies, I don't  encrypt messages.
If you don't want to use my products, as Ben Laurie pointed out,  there's a very nice plugin for Thunderbird called Enigmail that makes  doing crypto painless.
* There are also new generations of keyservers out there that work on  the issues of the old servers to trim defunct keys, and manage other  issues. I have out there the PGP Global Directory. Think of it as a  mash-up of a keyserver along with Robot CA concepts and user  management goodness adapted from modern mailing list servers like  * A number of us are also re-thinking other concepts such as using  short-lived certificates based on the "freshness" model to constrain  lifecycle management issues.
* There are many challenges remaining. Heck, the fact that people  here apparently have not updated their knowledge any time this  century is part of the problem. But let me tell you that email  encryption is growing, and growing strongly. However, most of the  successes are not happening where you see them. They're happening in  business, where communities of partners decide they need to do secure  email, and then they do. This is another place where things have  changed radically. A decade ago, we thought that security would be a  grass-roots phenomenon where end-users and consumers would push  security into those stodgy businesses. What's happening now is the  exact opposite -- savvy businesses are putting together sophisticated  security systems, and that's slowly starting to get end-users to wake  I'd be happy to discuss at length where things are getting better,  where they aren't, and where some issues have been shuffled around.  But we do need to talk about what's going on now, not ten years ago.

@_date: 2006-05-23 13:08:00
@_author: Jon Callas 
@_subject: Secure phones from VectroTel? 
My guess from looking at the web site is that it's AES-128 counter  mode (but it could be OFB or something like it) derived directly from  a 1K ephemeral DH. My reading from some of the pages is that the four- digit thing is not that it's a PIN, but a Short Authentication  String, a la ATT3600, Blossom COMSEC phone, PGPfone, and Zfone.  Interestingly, they are doing the encrypted voice over the data channel.
The FAQ notes that they have perfect forward secrecy and no stored  keys. Sadly, they don't release source code and say there will be no  updates. Nonetheless, it passes the sniff test. The limitations on  its use give some further clues about implementation. Half-second  delay, slightly metallic voice, setup time of 10-30s. I have my  guesses on what codec, cpu, and other things they're using from that.

@_date: 2006-05-23 20:39:11
@_author: Jon Callas 
@_subject: Is AES better than RC4 
What problem are you trying to solve?
You're asking a question that doesn't quite map. It's a bit like  asking whether a Vespa better than a Ferrari.

@_date: 2006-11-08 11:17:09
@_author: Jon Callas 
@_subject: Can you keep a secret? This encrypted drive can... 
There is no requirement for it. However, as others have noticed, to  the casual observer, 256 is twice as good as 128. You don't want to  end up with a product review saying, "Product X is solid with 128-bit  encryption, but for the ultra-paranoid, product Y is using 256!"
Moreover, AES-256 is 20-ish percent slower than AES-128. That  difference can be completely irrelevant in the context of the entire  system. That means that there is coolness pressure pushing to 256,  and relatively little performance backpressure. The result is that  you use AES-256 except where the performance is so tetchy that you  really need to back off to 128.
I've been spouting off about how 128 is enough, but not fighting the  trend even an iota. It's not worth the bother. Besides, I find the  irony that AES is pushing us from debates about how 56 oughta be good  enough to why 256 is just inevitable in less than a decade to be

@_date: 2006-11-27 03:32:49
@_author: Jon Callas 
@_subject: RFID passport article in the UK's "Guardian" newspaper... 
I think that all this is just more reason that they should do what I  suggested ages ago -- 2D barcodes. Two facing pages in the passport  could be easily scanned, and would have plenty of information. it's  not sexy, but it would work. It also wouldn't have the inherent  misfeature of being able to be read from a distance through someone's  pocket. That would mean it could even be plaintext.

@_date: 2006-09-04 15:01:24
@_author: Jon Callas 
@_subject: signing all outbound email 
Take a look at DKIM (Domain Keys Identified Mail) which does  precisely that. There is an IETF working group for it, and it is  presently being deployed by people like Yahoo, Google, and others.  There's support for it in SpamAssassin as well as a Sendmail milter.
Go look at  for many more details.

@_date: 2006-09-05 13:42:40
@_author: Jon Callas 
@_subject: signing all outbound email 
Not precisely. It is *primarily* MTA-to-MTA, for a number of very  good reasons, like privacy. However, a number of people will be  implementing DKIM verification in the MUA, including Yahoo!. (I've  seen UI mockups, but they may have it shipping for all I know.) The  protocol itself is completely agnostic on that. The signature travels  with the message and the signing key is in the network. As long as  you have both, you can verify the signatures.

@_date: 2006-09-16 13:45:28
@_author: Jon Callas 
@_subject: A note on vendor reaction speed to the e=3 problem 
That is correct, it has the advantage of being merely a byte string  that denotes a given hash.

@_date: 2007-08-18 18:00:33
@_author: Jon Callas 
@_subject: a new way to build quantum computers? 
Via Farber's list:
Wow, that's one of the most egregious quantum computing-related
articles I've ever seen.  I'm not even sure where to start.
First off, let's point at the real research paper:
Coherent Optical Spectroscopy of a Strongly Driven Quantum Dot
Xiaodong Xu, Bo Sun, Paul R. Berman, Duncan G. Steel, Allan
S. Bracker, Dan Gammon, L. J. Sham
I read it.  It's an advance, but does not yet mean anything at all is
practical.  Their work is on the optical properties of self-assembled
quantum dots.  There are two major categories of quantum dots in
semiconductors, self-assembled and lithographically created (and
within each of those, many types).  The self-assembled dots are a
compound grown on top of a substrate of a different kind.  Differences
in the crystalline structure mean that the deposited material "beads
up", like water on a freshly-waxed car.  The quantum dot itself then
is a place where the motion of electrons can be confined to a small
two-dimensional area at the interface between the materials, creating
a place where quantum wave functions can behave like an "artificial
The work presented in the paper is some of the first solid
experimental work on the optical properties of self-assembled dots
that I have seen, though I'm not an expert.  Various groups, including
that of my adviser, Kohei M. Itoh (
 ), have been working for years
on the growth and mechanical characteristics (stress/strain, size and
shape, etc.) of self-assembled dots.  All of that has been very hard
work, and as far as I know no one has a reliable way to grow the dots
in a given place.  I wish they had a micrograph of the device, I'd
like to see it.
But the TG article talks only a little about the research itself; it's
mostly breathless pie-in-the-sky reporting on the possibilities of
quantum computers.
"Light pulses crack security codes within seconds," the title reads.
Wow.  Well, first off, it can't be done yet, and won't be done for
years, despite the present tense.  Second, saying it's done with light
pulses is like saying we compute today with electrons.  It's true, but
tells you nothing about transistors or computer architecture.  Third,
"crack security codes" is as vague and non-technical as it gets, not
to mention outright wrong (we'll come back to that).  Fourth, "within
seconds" presumes many things about a quantum computer that are not
yet defined to any level of precision.  This topic is the focus of my
research: how do you build a large-scale quantum computer out of a
given technology?  No one really knows yet.
Which security codes does a paper on the spectroscopy of a quantum
dot break?  Well, none, really.  But where they're headed with that is
obviously Shor's algorithm for factoring large numbers on a quantum
computer.  If the algorithm can be efficiently implemented, it is
theoretically capable of breaking RSA public-key cryptography and
elliptic curve crypto.
HOWEVER, the advantage may well be with the defenders on this one.
Shor turns a super-polynomial problem (factoring) into a polynomial
one.  Not coincidentally, the complexity of running Shor is similar to
the complexity of doing the encryption in the first place.  And
running an algorithm of the same computational class on a quantum
machine will probably always be harder than running an algorithm on a
classical computer.  So, raise your key length and you might be okay.
Shor does nothing to affect symmetric key cryptography, or any system
not dependent on the factoring problem.
I hesitate to mention this, for fear it will be misinterpreted, but in
my opinion there is still some small doubt about whether Shor can in
practice be scaled to large sizes, on theoretical grounds, let alone
the practical difficulties of building using any given technology.
The problem is the quantum Fourier transform (QFT) that is the key to
Shor requires, in the abstract, exponentially precise gates as the
problem size grows.  Most researchers believe that the QFT can be
truncated at some reasonable level and will still have a high
probability of success.  However, the several papers on the topic
(including one by a collaborator of mine) in the last decade have
taken different approaches to the calculation, and come up with
substantially different answers, making different assumptions about
the problem.  The theorists seem confident, but I will give only
provisional assent until I see it implemented.  Perhaps I'm just not
smart enough to fully grasp the arguments in the papers.
Breaking a code in seconds really depends on both the problem and the
machine.  A major factor is how many levels of quantum error
correction (QEC) are necessary, which is directly dependent on the
quality of the physical implementation.  QEC is a major topic of
research; USC is even sponsoring a conference on the topic in December
(  ).
Physical and logical clock speed, as well as the amount of parallelism
in the system, determine how long it will take to run the algorithm on
a particular problem, of course.  This fact has gotten too little
attention from both the experimentalists and the theorists, in my
opinion.  See  .
Enough for now.  I can talk about this topic all day, including what
the boundaries of my knowledge are; take everything I say with a grain
of salt.
        	   --Rod

@_date: 2007-06-10 15:04:43
@_author: Jon Callas 
@_subject: A crazy thought? 
I'm going to disagree with you a bit, Ian. If you take two X.509  certificates that contain the same public key, they are semantically  equivalent to an OpenPGP certificate with two signatures on the key.  PGP [1] does this; it takes public keys and images them into OpenPGP  and X.509 certificates, creating parallel structures.
Yes, most X.509-using software doesn't know diddly about multiple  certifications. In most cases, this doesn't matter, because you just  hand them one certificate they'll accept and they go on their merry  way. Yes, this introduces risk that Alan is talking about, but that's  *their* problem, not mine.
This was nonetheless likely a wise engineering decision because  OpenPGP supports this directly, and in X.509 you have to create a lot  of software to recognize that a set of certificates belong together.
Bridge CAs are also a way of putting web-of-trust concepts into  hierarchical trust systems as well.
That I agree with completely. You cannot create trust with  cryptography, no matter how much cryptography you use. A good  jurisdiction trumps technology.
[1] PGP is a registered trademark of PGP Corporation and refers to  software that it produces. The PGP Software Products implement the  OpenPGP protocol standard, as well as several dialects of X.509. It  also implements S/MIME, TLS, and a variety of other standard and non- standard protocols. Since I'm a founder and executive of that  company, I'm obligated to point this out periodically, despite the

@_date: 2007-06-21 17:24:27
@_author: Jon Callas 
@_subject: question re practical use of secret sharing 
PGP. I can tell you more gory details than you're probably interested in.  But you can go get a free trial and play with it.

@_date: 2007-06-21 19:15:20
@_author: Jon Callas 
@_subject: Blackberries insecure? 
There have been rumors for years that the BlackBerry protocol is  compromised by some government or other. I've heard them for years.  Ultimately, no one knows, and there's no way to know. It boils down  to whether you trust RIM or not.
There is a PGP software package for the BlackBerry that will further  encrypt the content before it's sent out. I use it, and it's quite  nice. It cooperates really nicely with one of my PGP Universal  servers, as well. It's one of the best integrations of crypto into a  mail package I've ever seen.
However, you still have to trust RIM. I've never seen any of the  code, myself. and to my knowledge no one outside RIM has. There are  any number of ways that the implementation could be compromised, with  or without RIM's knowledge.
Paranoia is the *unwarranted* belief that people are out to get you.  The warranted belief that people are out to get you is caution.  Personally, I think that this is pure paranoid rumor and innuendo.  That doesn't mean it's wrong, it just means it's unwarranted.
Last week, I got sent a posting on a web site that someone made that  said that he had secret knowledge that the USG could break RSA for  all key sizes that anyone uses, so you should just stop using any  cryptosystem that uses it. Of course, he couldn't tell us anything  more to protect the position of the person who told him that. I said  that if someone told you that an unidentified friend had secret  knowledge that banks were unsafe and so you shouldn't keep keep your  money there, your "I'm being scammed" hairs on the back of your neck  would stand up. But if some unidentified someone tells you that the  crypto's bad, it's met with complete credulity.
I have no doubt that people in various governments want to spy on  high-ranking French. Duh.
But what's more likely, that there are secret government compromises  of security, or that there's a secret disinformation campaign with  the goal of convincing these people that the crypto is compromised.  Of course, the really delicious theory is that they've compromised  the crypto and then started the disinformation campaign in order to  get people like me to discredit the disinformation campaign and thus  reassure people that the crypto isn't broken, when in fact it is. Is  this paranoid, or merely cautious?

@_date: 2007-06-22 13:17:10
@_author: Jon Callas 
@_subject: Quantum Cryptography 
Let me create an aphorism to sum up what Paul, Perry, and others have  said in detail before I address your comment:
     If Quantum Cryptography does what is claims, then it is
     strengthening the strongest link in the chain of security.
Now to your comment.
If you do a 3000 bit Diffie-Hellman exchange, you have a key exchange  with 2^128 security, to the best of our knowledge, assuming this and  that, blah, blah, blah. If you don't like 3000 bit integers, go to  elliptic curve.
I have in some of my talks, renamed Quantum Cryptography to Quantum  Secrecy. If the QC people would stop calling it cryptography, a good  deal of the hostility you find among us crypto people would evaporate.
Let me give an analogy. I will posit Quantum Message Teleportation.  Using QMT, Alice can write her message on a piece of paper, close her  eyes, and it will disappear from her hand and appear in Bob's hand.
This is cool. This is useful. It is amazing. It is also not  It also has all the problems that Perry points out in QC, like a lack  of authentication and so on. Like QC, adding cryptography to it makes  it even more useful.
The QC people should change their song to QS, and stop bashing the  mathematicians with arguments we can show are somewhere between  incomplete and fallacious. Then they might find us drift over to  supporting them because while Quantum Secrecy is not practical, it is  very cool.

@_date: 2007-06-26 13:53:32
@_author: Jon Callas 
@_subject: Free Rootkit with Every New Intel Machine 
They've apparently stopped shipping TPMs. There isn't one on my  MacBook Pro from last November, and it is missing on my wife's new  Santa Rosa machine.
If you want to see if a machine has one, then the command:
sudo ioreg -w 0 | grep -i tpm
should give something meaningful. Mine reports the existence of  ApplePCISlotPM, but that's not the same thing.

@_date: 2007-06-26 14:03:29
@_author: Jon Callas 
@_subject: Quantum Cryptography 
What does this "classical" word mean? Is it the Quantum way to say  "real"? I know we're in violent agreement, but why are we letting  them play language games?
Moreover, the quantum way of discovering passive eavesdroppers is  really just a really delicious sugar coating on the classical term  "denial of service." I'm not being DoSed, I'm detecting a passive

@_date: 2007-05-01 14:33:01
@_author: Jon Callas 
@_subject: can a random number be subject to a takedown? 
My tongue is slightly in my cheek as I say this: once a random number  is known, it's not random any more. An idealized property of random  numbers like keys is that there be no algorithm for producing it that  is better than guessing. I can presently guess this key with  probability greater than 2^-128 using this algorithm in a C-like  unsigned char* guess_key(void)
     unsigned
     char key[] = {0x0a, 0xFa, 0x12, 0x03,
                   0xD9, 0x42, 0x57, 0xC6,
                   0x9E, 0x75, 0xE4, 0x5C,
                   0x64, 0x57, 0x89, 0xC1};
     return key;
(Or it would if I'd put the actual AACS key in there.)
The question is if a *specific* key can be taken down. This is open  to argument, because the DMCA only applies to things that are  copyrightable, and one can argue that keys are not copyrightable  convincingly. (Sketch of argument: if keys were copyrightable then I  could copyright a list of all keys. I can't copyright a database, or  even a phone book, so the notion that I could copyright a list of all  numbers in the set [0..N] is absurd.)
As far as anti-circumvention goes, keys themselves can't be used for  circumvention. Assuming that the above were the AACS key, I couldn't  use it to circumvent because I don't know the right protocol to use.  Consider another scenario: one can use a brick to smash a window, but  possessing a brick does not mean you've broken windows. If I have a  proper key, but no software, I am not capable of circumventing.  Likewise, if I had software that could do the crypto, but no key, I'm  not capable. It is only if I have both the software and the key that  I have something that *might* be a circumvention device. Even things  that might be circumvention devices are not always. The test in the  DMCA is if its primary purpose is for circumvention. This is why  debuggers are not circumvention devices. It is only when you use the  potential circumvention device to circumvent that you've done the  equivalent of throwing the brick through the window.

@_date: 2007-05-07 18:53:28
@_author: Jon Callas 
@_subject: PRZ going in for heart surgery 
Phil Zimmermann is going in tonight (7 May) for heart bypass surgery.  He's not in immediate danger -- he's not having a heart attack, he's  not no in immediate danger, but they're pushing him into the hospital  quicker than any reasonable person would like. Obviously, that makes  for worries. He meets with his surgeon tomorrow morning, and likely  will have surgery tomorrow (8 May).

@_date: 2007-05-09 12:48:49
@_author: Jon Callas 
@_subject: Enterprise Right Management vs. Traditional Encryption Tools 
What problem are you trying to solve?
If you're dealing with a rights-management problem, such as how do  you give someone a document that they can read on the screen but not  print, you aren't going to solve that with a cryptosystem.
However, rights management systems have characteristics that are  Rights management systems work against polite attackers. They are  useless against impolite attackers. Look at the way that  entertainment rights management systems have been attacked.
The rights management system will be secure so long as no one wants  to break them. There is tension between the desire to break it and  the degree to which its users rely on it. At some point, this tension  will snap and it's going to hurt the people who rely on it. A  metaphor involving a rubber band and that smarting is likely apt.
One way this fails is the good old "analog hole." People can still  take pictures of their screens.
Another way this fails is for people to rely upon rights management  as a cover for sloppiness, anger, or mendacity. If you think you can  revoke a message or send Mission Impossible documents, you will.  Someday, someone on the receiving end will use the analog hole. Oops.  Imagine the case where a tech support person tells off an obnoxious  customer, who takes a picture of the screen.
Furthermore, there are subtle problems with rights-management and  policy. Let's suppose that I run an organization that needs to  archive documents. I therefore *must* reject documents that I cannot  I have personally stuck more to having crypto be a form of access  control (once you get to a document, you have it) than as use control  I think that the operational issue -- that rights management *cannot*  work -- trumps everything else, and turns the social issues (if you  can tell someone off and deny it, will you?) into -- into nothing  other than a information bomb. You're going to end up looking like  Wile E. Coyote, with a blackened face and stunned, blinking eyes.

@_date: 2007-05-10 18:51:31
@_author: Jon Callas 
@_subject: PRZ status 
He's out of surgery, doing well, and the doctors say he'll be better  than he's been for ten years.

@_date: 2007-05-11 21:50:38
@_author: Jon Callas 
@_subject: Enterprise Right Management vs. Traditional Encryption Tools 
Your comment of barring screen captures etc. is a bit like saying  that won't a bank be safe from robberies barring someone waving a gun  in a teller's face, etc. Yeah, sure, but doesn't that kinda miss the  point? DRM works if the attackers are polite. The less polite they  are, the less well it works.
DRM systems for media are probably more immune to "analog hole"  attacks ERM systems. Imagine that someone ERM protected an email  showing things that Gonzales couldn't remember when he was testifying  to Congress, or in some stock scandal, etc. A photo of a screen with  a cell phone camera would be sufficient. We have not (yet) seen an  attack where someone got a pre-release of a movie and then pointed a  camera at a laptop screen, but we will.
If you add in a TPM, it depends entirely on how impolite the  attackers are, as well as the construction of the TPM. One of the  recent attacks against AACS involved the attackers unsoldering the  chip and attacking it directly. That's pretty rude, but it worked.
If someone is so impolite that they'll put the TPM chip under a  scanning electron microscope, they can probably just read the bits  off. Very few smart cards can survive that.
Remember, this is all a trade-off between the cost of the device and  the devotion of the attacker. TPM chips have to be very cheap,  because the customer is ultimately paying for it. That means its  defenses can't be very thorough. Furthermore, while the owner of the  device is the attacker, you can't afford very many defenses. If a  music player, for example, went DOA because it it was dropped, went  over/under temperature, and so on, it would be a financial nightmare,  as you probably have to replace them under warranty. People who hate  DRM would buy devices, monkeywrench them, and then demand a refund.
ERM systems have the advantage that in general the attackers are more  polite. More people want to break AACS than rights-controlled analyst  reports. However, once something really juicy happens, like just  needing the content registration key for a document that will get a  politician in jail -- well, plenty of people can hack that. Now, all  of a sudden, the attackers won't be polite, and that metaphor I made  about a rubber band snapping will seem modest.
Really, you're much better off with real crypto and personnel policies.

@_date: 2007-11-01 13:52:28
@_author: Jon Callas 
@_subject: Hushmail in U.S. v. Tyler Stumbo 
I'm sorry, but that's a slur. Hushmail is not a scam. They do a very  good job of explaining what they do, what they cannot do, and against  which threats they protect. You may quibble all you want with its  *effectiveness* but they are not a scam. A scam is being dishonest.
You also mischaracterize the Hushmail system. The "classic" Hushmail  does not generate the keys, and while it holds them, they're  encrypted. The secrets Hushmail holds are as secure as the end user's  operational security.
I know what you're going to say next. People pick bad passphrases,  etc. Yes, you're right. That is not being a scam.
They have another system that is more web-service oriented, and they  explain it on their web site far better than I could. It has further  limitations in security but with increased usability. It is also not  a scam.

@_date: 2007-11-01 16:28:40
@_author: Jon Callas 
@_subject: Hushmail in U.S. v. Tyler Stumbo 
I don't know anything about this case, so everything I say is pure  Let's suppose you have Alice and Bob who are working together on some  sort of business, and they are using some OpenPGP [1] software to  encrypt their emails that pertain to that business. Let's suppose  that the authorities then decide to raid Bob. Let us then suppose  that they go to Alice's ISP and get a lot of encrypted email, by  warrant, subpoena, etc. It doesn't matter for our purposes what ISPs  Alice and Bob are using, nor what OpenPGP software they are using.
* Let us consider the case where Bob turns state's evidence. If those  emails were encrypted to both Alice's key and Bob's key, after Bob  turns state's evidence, the authorities can decrypt all the messages  they seized from Alice's ISP. It doesn't matter what Alice did with  her key or what Alice's ISP did with it. They can be decrypted  because Bob's key has been compromised.
* Let us consider the same basic scenario where all the messages are  encrypted to the sender's, but not the recipient's key. In this case,  the authorities can decrypt all of Alice's messages to Bob, but not  Bob's messages to Alice. After they have compromised Bob, all of  Alice's messages to Bob can be decrypted. The fact that Alice's  security is untouched is mostly irrelevant. Alice is likely toast,  not because of the cryptography, but because Bob has been  compromised, and Bob's key decrypts mail Alice has sent.
* Let us consider a slightly different scenario in which neither  Alice nor Bob are compromised, but Bob is detained. If the  authorities raid Alice's ISP, despite the fact that they cannot  decrypt the messages, they may be able to show a connection between  Alice and Bob. If they have been CCing themselves, then you'll find  the same undecryptable message in each mailbox. If they have been  using "reply," there's probably metadata in the plaintext headers  that shows that M_n is a reply to M_{n-1} ... M_1, and thus you have  a chain of messages. If there is other evidence, such as Bob sending  checks to Alice every so often, the cryptography may be moot or worse  than moot. (If those messages are harmless, why don't you decrypt  them? Yes, this can get into many interesting discussions like the  applicability of Amendments 4 and 5, but these are also not  cryptographic. I really don't want to discuss them because I'll bet  we agree.)
Cryptography is not magic pixie dust that you can sprinkle on a  security problem and make it go away. If your adversary is a major  national government, you have operational security issues, as well.  If your adversary is a major national government that has direct  authority over where you live, then you have a much larger problem.  The adversary is going to use forensic analysis, traffic analysis,  and anything else they can think of. They are also not dumb. You also  have to expect that third parties, including ISPs, are unlikely to  see why they should fail to comply with legal documents like  subpoenas and warrants because of what you did. Smart cryptographers  make sure there are no backdoors in the crypto, because if there  were, then every beat cop and two-bit mafioso will want you to break  just that one message -- or else. If the system is strong, it all  comes down to your operational security.
[1] I have to give a now-usual rant. PGP is a trademark of PGP  Corporation and refers to software it makes. OpenPGP is an IETF  standard that covers encryption, certificates, and digital  signatures. There are many products that implement the OpenPGP  standard. PGP software is one of those. But other products, such as  GnuPG, Hushmail, Bouncy Castle, and so on also implement the OpenPGP  standard. Futhermore, PGP software implements other standards than  OpenPGP. For example, PGP software implements the S/MIME and X.509  standards as well as the OpenPGP standard.

@_date: 2007-11-01 18:28:23
@_author: Jon Callas 
@_subject: password strengthening: salt vs. IVs 
Well, real hash functions are many-to-one. Consider the set of all 33- byte strings. Consider s', which is the set of all the 256-bit hashes  of all of those strings. It doesn't matter what hash function you  use, there will be duplicates. There must be duplicates.
The functions we used in those pre-bad-old-days included the AUTODIN- II polynomial and the Purdy Polynomial (I had to go look it up,  because those parts of my memory were put on the free list). AUTODIN- II had undesirable qualities, which is why things migrated to Purdy.  But based upon my quick research, Purdy seems to still be good for  its purpose, namely grinding up passwords.
The way we used Purdy had to be improved, as time went on. There was  a time in which you could bypass a password length limit by small  bits of cleverness. If you had your favorite three-character  password, and that mean old system manager set the minimum length to  6, you could bypass that by appending the string  "UUUUUUUUVVVVVVVVVVVVVVVV" (that's 8x 'U' and 16x 'V') to your three- character password and poof it worked again. Why this worked and the  fix are left as an exercise to the reader, but I'll note that the  underlying issue is something that hash function designers still have  to make sure they solve to this day. Joux and Kelsey have written a  lot about this very same problem, the length extension attack.
I mean precisely that. If you use a counter, the dictionary low  numbers is valuable. This is one of the many problems that came up in  Let's suppose you selected a "full-width" prime number, and your  counter incremented (or multiplied) by that prime. That's better than  0, 1, 2, ... but only if everyone doesn't select the same prime. Thus  you get back to using the RNG. If the width of your salt is wide  enough, you don't have to worry about birthday attacks. If you have 8  bytes of salt, the chance of a single collision is .5 when you have  about 4 billion numbers selected. 4 billion is a large number if it  is the number of accounts on your mail server. If you are fortunate  enough that it is not a large number, you can either go to 16 bytes  of salt, or weasel out of the issue by observing that even with 100  billion accounts, the number of collisions is not so large that there  is a clear advantage to the attacker who precomputes a single  dictionary. (And how do they know which dictionary to compute, a  priori?) When we're talking about precomputed rainbow tables, 2^64 is  a large number.
Simple is good. Why not just pull enough salt off of /dev/urandom and  make a small handwave about how big "enough" needs to be? If you tell  me that, I listen, nod, and we're done. With your scheme, I have to  think before I understand. Having to think before understanding is  not a feature. I think I can see a minor flaw, but I don't want to  spend the brain power on it. The RNG is your friend.
Eight bytes of salt is almost certainly fine. If you have to worry  about single collisions, use 16 or 32 bytes of salt. In general, I  recommend using a width of salt that is the size of an underlying  block size. If you're using AES somewhere, just go with 16, because  that's the natural amount.

@_date: 2007-11-01 20:20:41
@_author: Jon Callas 
@_subject: Hushmail in U.S. v. Tyler Stumbo 
I am only quarreling with the word "scam."
Yes, I have. They have their with Java option and their without Java  option, and even the "Express" option. Look at:
which has a very nice description of each system, and what  vulnerabilities there are with each. There are many things one might  say about the without Java option. They might include, "doesn't meet  my security needs" or "I can't imagine what would possess someone to  use the non-Java version." But it's not a scam.

@_date: 2007-11-05 15:05:11
@_author: Jon Callas 
@_subject: forward-secrecy for email? (Re: Hushmail in U.S. v. Tyler Stumbo) 
Forgive the additional nag, but that is OpenPGP clients. PGP clients  are my software. Mind you, I'm in favor of it, but (e.g.) Hushmail is  not a PGP client. It has nothing to do with PGP Corporation.
Well, we had some good news this weekend that RFC 4880, the updated  RFC 2440 is finally published. The OpenPGP working group has other  work it would like to do, including Perfect Forward Secrecy.

@_date: 2007-10-06 00:59:18
@_author: Jon Callas 
@_subject: Undocumented Bypass in PGP Whole Disk Encryption 
On Oct 4, 2007, at 12:37 PM, travis+ml-cryptography at subspacefield.org  Except that the guy who posted it was wrong on all of his complaints.  It's plenty documented. The "What's New?" section in the manual  documents it, the release notes document it and others.
We're examining how we document things. If you put in the release  notes for a product and the "What's New" section a description of a  feature and someone who is a security researcher can't find it there,  it calls into question how one organizes one's documents, of course.
If anyone has any questions, I'm happy to answer them.

@_date: 2007-10-18 12:49:40
@_author: Jon Callas 
@_subject: Quantum Crytography to be used for Swiss elections 
And we know they are trustworthy photons because they have  certificates signed by an accredited third-party boson.

@_date: 2007-10-22 13:21:05
@_author: Jon Callas 
@_subject: Quantum Crytography to be used for Swiss elections 
Boson. Bosons are force-carrier particles, as opposed to fermions.  Photons are themselves bosons, but there are other bosons that carry  other forces. There's the Higgs boson, W and Z bosons, and so on.  Gluons, the particles that hold atomic nuclei together are also bosons.
Bogons are, technically, bosons as they are the particle that carries  a quantum unit of bogosity. However, you yourself have criticized  people who discuss the role of bogosity in quantum cryptography [sic]  (I prefer the term "quantum secrecy"), and therefore I will say no  more about bogons and QC.

@_date: 2007-10-24 15:59:43
@_author: Jon Callas 
@_subject: Elcomsoft trying to patent faster GPU-based password cracker 
I agree completely. If the PTO does their job, they won't get it.  This is like claiming that once we know that making daiquiris in a  blender is possible, it's patentable to improve that by making pina  coladas. If you're skilled in the art, you know that this is pretty  obvious. Crypto extended to cryptanalysis is less of a stretch than  strawberries extended to coconut and pineapple.
Unfortunately, the PTO hands out patents for things like using a  laser pointer as a cat toy.

@_date: 2007-10-30 01:00:28
@_author: Jon Callas 
@_subject: password strengthening: salt vs. IVs 
On Oct 29, 2007, at 12:24 PM, travis+ml- Before the bad old days of using DES, there was the old days of one- way functions. These one-way functions were not hash functions, they  were one-way. They were in a sense related to hash functions, but  perhaps more directly related to redundancy checks and similar  The belief was that storing passwords in plaintext was a bad idea. A  related notion was that storing a password encoded through a  symmetric function was essentially storing it in plaintext.
The term salt comes from the metaphor of considering the process of  one-waying a password to be like making hamburger out of meat, or  stew out of ingredients, or some other cooking metaphor. The salt was  introduced to address the issue of dictionary attacks and carried the  observation that cooking is better if you add a little salt to it.  The salt was a sprinkling of an arbitrary constant into the function  to spice it up a bit.
The people who worked on these password-grinding systems had a  tendency to sneer at those who would use a cipher such as DES for  that because DES is reversible. Using a reversible function is  essentially storing the password in plaintext. Munging DES was seen  by those people as inferior to designing one-way functions that were  properly one-way. Eventually, these became a subset of what one would  use a hash function for.
The IV in a block cipher serves the same function as salt. It's  called an IV, though because of the different path of development.  The term "salt" gets used in other places, like with randomized  hashing, which is often also called salting a hash, too.
The question you had is how much entropy there should be in salt. The  answer is none, but that's a very subtle answer. Salt is -arbitrary-  as opposed to -random-. As it turns out, the best way to get a 256- bit arbitrary number is to pull it off your RNG.  Arbitrary numbers  like salt, don't have to worry about subtle issues that you'd want  key material to worry about. Arbitrary numbers are in general public  (or at least not secret), and key material is secret.
With salt, you want the number to be unique-ish, as the whole point  is to stymie dictionary attacks. A counter is likely not such a great  idea, because of collisions, but there are all sorts of things you  could do that would be very very bad with key material but are mostly  okay with salt. Nonetheless, the easiest way to get salt with a  system that has an RNG is to just pull the number off the RNG. But  that doesn't mean it has entropy.
Now as what to call it? I like "salt."

@_date: 2007-09-11 17:35:58
@_author: Jon Callas 
@_subject: Another Snake Oil Candidate 
I'm a beta-tester for it, and while I can understand a small twitch  when they talk about "miltary" and "beyond military" levels of  security, it is very cool.
It has hardware encryption and will erase itself if there are too  many password failures. I consider that an issue, personally, but it  appeals to people. The reason I consider it an issue is that I have  had to use a brain-dead-simple password I'm not going to forget  because if I get cute and need to try a number of things, poof, I'm  Yeah, it's using AES CBC mode, but that's a good deal better than a  lot of encrypted drives that are using ECB.
It also has their own little suite of Mozilla plus Tor and Privoxy  for browsing and they've set it up so that you can run that on  another computer from the drive.
It's not bad at all. My only real complaint is that it requires Windows.

@_date: 2008-08-29 18:13:23
@_author: Jon Callas 
@_subject: Generating AES key by hashing login password? 
The short answer is yes. A better answer is that you want to salt the  password before you hash it many times, to keep from having rainbow  tables created. Another better answer is that you want to hash many  times to slow down password crackers.
As others have mentioned, there are standards that can show you the  way. PKCS has a mechanism for this. OpenPGP does, too. They're  subtly different, and understanding the differences can help you roll  your own.

@_date: 2008-12-08 20:53:18
@_author: Jon Callas 
@_subject: AES HDD encryption was XOR 
No, this is simple to do.
What you is to start with a basic cracking engine. And then you add  another one an hour later, and then an hour later add two, then add  four the next hour and so on.
If you assume that the first cracker can do 2^40 keys per second, then  you're guaranteed to complete in 472 hours, which is only 20 days. And  of course there's always the chance you'd do it in the first hour.
For those who doubt being able to double the cracking power, Moore's  law proves this is possible.

@_date: 2008-12-30 13:40:59
@_author: Jon Callas 
@_subject: very high speed hardware RNG 
You are saying pretty much what I've been saying about this (and some  other things).
We don't have a formal definition of what we mean by random. My  definition is that it needs to be unguessable. If I have a random  number and the work factor for you to guess it is more or less its  randomness. It's a Shannonesque way of looking things, but not  precisely information-theoretic.
A deterministic, but chaotic system that is sufficiently opaque gets  pretty close to random. Let's just suppose that the model they give of  photons bouncing in their laser is Newtonian. If there's enough going  on in there, we can't model it effectively and it can be considered  random because we can't know its outputs.
However, on top of that, there's a problem that hardware people  (especially physicists) just don't get about useful randomness,  especially cryptographic random variables. Dylan said that to live  outside the law, you must be honest. A cryptographic random variable  has to look a certain way, it has to be honest. It's got to be squeaky  clean in many ways. A true random variable does not. A true random  variable can decide that it'll be evenly distributed today, normal  tomorrow, or perhaps Poisson -- the way we decide what restaurant to  go to. No, no, not Italian; I had Italian for lunch.
That's why we cryptographers always run things through a lot of  software. It's also why we want to see our hardware randomness, so we  can correct for the freedom of the physical process. Imagine a die  that is marked with a 1, four 4s, and a 5. This die is crap to play  craps with, but we can still feed an RNG with it. We just need to know  that it's not what it seems.
So yeah -- it's a glib confusion between chaotic and random, but  chaotic enough might be good enough. And the assumption that hardware  can just be used is bad. Hardware that helpfully whitens is worse.

@_date: 2008-12-30 14:34:25
@_author: Jon Callas 
@_subject: very high speed hardware RNG 
And that is, pretty much, the point. A formal definition that is no  guidance to people trying to build things is mathematics as art. Not  that art is a bad thing -- I adore mathematics as art, and my time as  a mathematician was all in the art department. It's just not useful to  the part of me that's an engineer. Pretty has worth, just different  worth than useful.
Exactly. You've described a chaotic but easily reversible system, and  that makes it unsuitable to be random by my "unguessability" metric.  There exists an algorithm that's faster than guessing, and so  therefore it isn't random.
Yes. Again, we're in agreement on this.
If I think some system is unguessable to 256 bits, and it's really  unguessable to 10 bits, then someone who knows those ten bits can  easily search for a state that I think is unsearchable. (At least in  principle -- ten bits worth of calculations that take a century each  is a different thing entirely, but that's only because centuries are  longer than nanoseconds.)

@_date: 2008-02-06 13:44:20
@_author: Jon Callas 
@_subject: Poor password management may have led to bank meltdown 
Yes, but get what? "It" is a vague noun.
The reporter showed some wit by using the word "may."
This was an attack by an evil (or crazy) insider. Evil insider attacks  are the hardest to protect against. If the insider decided that he was  going to start making trades for whatever reason, then he'd find a  weak point that would allow him to make trades, and use it, no matter  what it is. (My personal hypothesis is a variant of a mad-scientist  attacker -- "They laughed at me when I told them my trading theories!  Laughed! But I'll show them! I'll show them ALL!!!")
If this person had worked for 1000 hours to get a hardware token, he  would have just done the work. The result may have been an order of  magnitude more. High-security procedures tend to be more brittle for  psychological reasons. If you have the magic dingus, then you are  authorized, and no one ever questions the dingus.
Also, one must look at the economics and psychology of the situation.  Traders are prima-donna adrenaline junkies who trade vast sums of  money all the time and are not shy about expressing their  frustrations. Looking at the sheer economics first:
* A trader trades C units of currency every hour, with an average  profit of P (for example 5% profit is P=1.05).
* There are T traders in the organization.
* The extra authentication produces a productivity drop of D. For  example, let us suppose a trader has to authenticate once per hour,  and it takes 10 seconds to authenticate. This gives us a D of .9972 or  So the operational cost of your authentication is (1-D)*T*C*P per  hour. Divide ?4.9G by that, and you get the number of hours for the  raw break-even time on this.
Add to this the probability that the hassle will convince a trader to  jump ship to another firm (J), times the number hours of trading lost  until you find a replacement (H). We'll assume the replacement needs  no spinup time to become as productive as the previous trader. That's  an additional cost of J*H*T*C*P. This is the psychological factor. As  I said, traders are prima donnas who are used to getting their own way.
People have criticized post-9/11 airline security on similar grounds.  They observe that some number of people drive rather than fly, and  calculate out the difference in deaths-per-passenger-mile. I've seen  numbers that work out to a handful of 9/11s per year caused by traffic  displacement. They also observe that large numbers of people spend  extra time in lines, which works out to a "lost life" number. For  example, if you assume that passengers spend 10 extra minutes clearing  security and a life is 70 years, then roughly 6 million passengers  represents one lost life.
There's always much to criticize in these models. I could write a  reply to this message with criticisms, and so can you. Nonetheless,  the models show that there's more than just the raw security to think

@_date: 2008-02-21 07:13:10
@_author: Jon Callas 
@_subject: Interesting New Developments in SocGen 
An internal investigation into billions of euros of losses at
    Societe Generale has found that controls at the French bank
    "lacked depth".
    The results of the investigation also show that rogue trades
    were first made back in 2005.
    Societe Generale made a profit in 2007 despite a trading scandal      cost the bank 4.9bn euros ($7bn; ?3.7bn).
    The French bank said it made a net profit of 947m euros for the      although this was down 82% from 2006.
I think these two things are very interesting from a viewpoint of  security and economics. This fellow had been making unauthorized  trades for two to three years, and when it all came tumbling down, it  knocked off at most 82% of one year's profits. (I say at most because  it's reasonable to think that in a year of subprime issues, they'd  have been down 30-50%.)
Compare and contrast with Nick Leeson's sinking of Baring's, which was  a mere $1.4bn. We can even double-ish that to say $3bn to account for  the intervening time.
Both cases were unauthorized trades spinning out of control in  attempts to cover small losses, but Baring's was sunk for the  (adjusted) $3bn and SocGen merely loses 80% of one year's profits at  Does this suggest that what is really needed is a way to detect losses  that could spin out of control before they do, as opposed to direct  security mechanisms?

@_date: 2008-02-21 13:26:15
@_author: Jon Callas 
@_subject: cold boot attacks on disk encryption 
Umm, pardon my bluntness, but what do you think the FDE stores the key  in, if not DRAM? The encrypting device controller is a computer system  with a CPU and memory. I can easily imagine what you'd need to build  to do this to a disk drive. This attack works on anything that has RAM.

@_date: 2008-02-22 04:43:21
@_author: Jon Callas 
@_subject: cold boot attacks on disk encryption 
No. Apple (or anyone doing EFI boot, for example, someone doing WDE  for OS X) can easily modify the EFI boot to zero memory. It isn't just  the Air, it's any Intel Mac, but remember those are just Intel EFI  Note, however, that this does not completely solve the attack. If  someone hits the reset button or yanks power, then you don't get to

@_date: 2008-01-03 16:21:16
@_author: Jon Callas 
@_subject: Question on export issues 
They let strong crypto through all the time. I can't imagine what  *technology* you couldn't get through. Definitely, however, there are  *people* who couldn't get an export license because they've been bad  in the past.
So the answer to your questions is that they're vetting who you are  far more than what you're exporting.

@_date: 2008-01-04 12:41:07
@_author: Jon Callas 
@_subject: DRM for batteries 
Assuming that is your threat model, yes.
If your threat is that you're worried about a cheap battery flaming  in your brand of phone, that's more of a threat.
People who know how to make batteries have a so-so track record with  them. There have been many cases of off-brand batteries going into  If I were the CSO of a mobile phone company, I think I could  convincingly argue that this is a public safety issue. Let's face it,  if a battery takes out someone's leg or downs an airliner, the  headline will say my company's name, not the people who made the  If I really wanted openness, I could come up with an accreditation  process that allow responsible third parties who followed proper  quality standards to license the needed hashes.

@_date: 2008-01-06 17:23:56
@_author: Jon Callas 
@_subject: DRM for batteries 
Also remember that there is a specific exemption in the DMCA for  reverse engineering for the purpose of making compatible equipment.  It is there precisely to protect people like the printer cartridge  folks. That's why they lost.
Going back to the '60s, there was the Ampex case, where they made  compatible tape drives for IBM mainframes. IBM sued, and lost in the  Supreme Court. This is what gave us the plug-compatible peripheral  biz. My memories of this say that some judge or other said that  copyright is not intended to give a monopoly.
That doesn't mean that other companies can't pull crap and try to sue  competition away. But they're wrong, and the effect may help the  little guy, because by now, the big guys ought to be able to pay for  lawyers smart enough to know the precedent.

@_date: 2008-01-06 18:52:33
@_author: Jon Callas 
@_subject: Question on export issues 
I'm sorry, but I don't understand the question. I've read it about  ten times and don't know what you're asking. Let me try to answer by  talking around it.
If you look at the basic components we have, the ciphers, hash  functions, and so on, they're all secure enough that a major  government can't crack them. Yes, we know that there are weaknesses  in lots of hash functions, but by now, we have a pretty good handle  on that. We know about how broken they are, and there are  workarounds. Furthermore, if you look at the push to fix this --  where is it coming from? NIST, NESSIE, etc.
If you look at the medium-level functions, like HMAC, salted hashing,  tweakable cipher modes, and so on, they are *more* secure. For  example, even if you don't like SHA-1, a SHA-1 HMAC is still  considered secure.
If you look at the protocols, like TLS, IPsec, OpenPGP, S/MIME, and  so on, they're also secure, because they assemble the reasonably  secure components together reasonably securely. Yes, we can have  discussions about some of them, but again, we know lots about their  security, and can actually discuss it rationally. It was much harder  to do that ten to twenty years ago.
All of these things are freely exportable. It's just a matter of  filling out paperwork.
I don't have an example of a cryptosystem that I'd actually want to  use that is non-exportable. And I'm sure that if someone made  something that is custom, it's exportable. I have direct evidence of  Back in 1999, when we were at Counterpane together, John Kelsey and I  created a set of incompatible Blowfish variants. We were going to use  them in TLS so that Counterpane gear would have its own little walled  garden. We could have used a family key, but this was fun, and also a  test of the export regime. Blowfish, as you may or may not know, has  some initialization constants that are hex digits of pi. These  "colorfish" ciphers used different digits of pi for the  initialization. I constructed the family of: Blackfish, Brownfish,  Redfish, Orangefish, Yellowfish, Greenfish, Bluefish, Indigofish,  Purplefish, Whitefish, Silverfish, Goldfish, Octarinefish, and  Plaidfish. I sent them for export and there wasn't a peep. Nothing.  These days, British Telecom owns them.
There are cryptosystems I know of from non-Wassenaar countries that I  wouldn't go near. I don't think they're very good. I don't care if  that's a matter of competence or malice; I'm not favorably impressed.  I am, however, quite sure they're exportable.
I don't have an example of any crypto technology that I would think  wouldn't be exportable.
No, I don't mean that. Crypto is a "dual use" technology. Most things  are dual use. There are obvious dual use things, like nuclear  materials, but video games are also dual use, as are milling  machines, laser diodes, navigation equipment, and so on. Basically,  if it's fun, it's dual use.
You have to have export licenses for dual use items. Sometimes the  license is very easy to get. In some cases, it is nothing more than  giving them your web logs if they ask for them and there's nothing  requiring you to keep them. (Most open source software falls here.)  Other times, there's more. For some people, like Ivan at OLPC and me  (at PGP), we jump through hoops we don't necessarily have to because  we don't want to end up on the wrong side of things.
If you violate export rules, there can be legal and administrative  penalties. The administrative penalties can be much worse, because  they can essentially just decide you can't ever export anything.  These days, I suspect this would also be a good way to end up on the  permanent SSSS list for flying.
When I took a course on all of this, I was told about a guy in the  import-export biz who was known for being able to get things into  countries with sanctions. Eventually, he was caught and never  prosecuted, but the "administrative" penalties against him mean that  he had to find a new career. He couldn't get an export license to  send an Xbox to Canada.
The Treasury Department maintains a list of Bad People. It's on the  web. Osama bin Laden is one of them, and so is this guy.
That's what I meant.

@_date: 2008-01-08 17:47:51
@_author: Jon Callas 
@_subject: Question on export issues 
I'm going to disagree.
For Ivan, at OLPC, and for me, at PGP, we have to dot a few extra 'i's  and cross a few extra 't's. Think of what will happen in the press if  either of us have to deal with export messes.
And for what it's worth, Phil Zimmermann's whole brouhaha was started  by a competitor that filed a complaint that he was violating export.  It wasn't anything active by the government, they were just responding  to a complaint.
If, on the other hand, I were doing an open source thing, then sure, I  wouldn't be as anal about things. One of the ironies about crypto  export regulations today is that the  real risks come from trouble the  neighbors might make for you.

@_date: 2008-03-17 12:00:24
@_author: Jon Callas 
@_subject: delegating SSL certificates 
Yes, but.
If a browser handled unknown certificates similarly to the way SSH  does -- to alert the user when it sees an unknown, unrooted  certificate, and then only again when there is a mis-match, you would  have an incentive to get a CA certificate (because businesses don't  want their customers to see that scary message even once), while  supporting ad-hoc infrastructures.
This would require only software changes, not changes in the trust  models, CAs, procedures, etc.
A wicked person would suggest that this is because the present system  was designed to support the business model, not the security model.  I'm not a wicked person and would never suggest that.

@_date: 2008-03-18 09:46:45
@_author: Jon Callas 
@_subject: Protection for quasi-offline memory nabbing 
Such as Cold Boot, etc.
There have been a number of conversations among my colleagues on how  to ameliorate this, particularly with an eye to making suspend mode  In the Cold Boot paper, the authors suggested XORing a piece of random  memory onto the dangerous bits, so as to fuzz them. This is a clever  idea, but we didn't like it, particularly because XOR doesn't have the  best diffusion in the world.
The solution we came up with is to use EME mode (or equivalent) with a  fixed key. The outline is that you encrypt all dangerous data, like  volume key, key expansion, etc, with a fixed key into a chunk that you  keep to the side.
This relies upon the property of EME (and other large-block, tweakable  modes) that a single-bit error in the ciphertext propagates to an  error in the entire plaintext. Consequently, a very low rate of memory  decay turns into complete protection of that sensitive data.
Upon suspend, you erase and deallocate the active store, and on wake  you decrypt the fuzzed copy to get your keys and state variables back.
If you want to one-plus this, you could have a timeout on the drive so  that if it's idle for N seconds, you do same.
When we came up with this, we wondered if it was patentable. We've  decided that it isn't, that this is something that is obvious to  someone skilled in the art. Our reasoning is something like:
Cold Boot paper suggests XORing random memory
What has better diffusion?
(discard suggestions like lead, churches, and very small rocks)
Block ciphers have great diffusion
What operates like a block cipher on a large chunk?
Tweakable modes like EME.
The rest is just software engineering.
The cool thing about using EME (or equivalent) is that the larger the  chunk you create, the better you survive a Cold Boot attack.
Note, however, that an attacker who can grab memory with no errors in  it, such as someone who is playing DMA games, still gets the keys. To  protect against that, you have to have an authentication mechanism,  which is outside the scope of this -- we want something that is  transparent, but can make people worry less about suspending their  Also note that you don't really need a full cipher. All you need is  reversible diffusion that maximizes damage on a single-bit error.  However, the danger in coming up with another function is that you're  effectively designing special-purpose crypto. Yes, it's really special- purpose coding, not crypto, but it's a lot safer to use crypto. We  understand it better.
A number of people participated in our discussions and at least two  people independently thought of the core idea. The people include but  are not limited to (which means I apologize to everyone I forgot):  Colin Plumb, Phil Zimmermann, Hal Finney, Andrey Jivsov, Will Price,  David Finkelstein, and Bill Zhao.

@_date: 2008-03-21 10:38:51
@_author: Jon Callas 
@_subject: Protection for quasi-offline memory nabbing 
It is. That's something everyone should consider doing. However, I was  struck by the decay curves shown in the Cold Boot paper. The memory  decays in an S-curve. Interestingly, both the smoothest S-curve and  the sharpest were in the most recent equipment.
However, this suggests that for a relatively small object (like a 256- bit key) is apt to see little damage. If you followed the strategy of  checking for single-bit errors, then double-bit, then triple-bit, I  hypothesize that this simple strategy would be productive, because of  that curve.
(I also have a few hypotheses on which bits will go first. I  hypothesize that a high-power bit surrounded by low-power ones will go  first, and a low-power bit amongst high-power ones will go last. I  also hypothesize that a large random area is reasonably likely to get  an early single-bit error. My rationale is that the area as a whole is  going to have relatively high power 'consumption' because it is  random, but the random area is going to have local artifacts that will  hasten a local failure. Assuming that 1 is high-power and 0 is low- power, you expect to see a bitstring of 00100 or 0001000 relatively  often in a blob of 32kbits (4KB) or 64kbits (8KB), and those lonely  ones will have a lot of stress on them.)
Despite that my hypotheses are only that, and I have no experimental  data, I think that using a large block cipher mode like EME to induce  a pseudo-random, maximally-fragile bit region is an excellent  mitigation strategy.
Now all we need is someone to do the work and write the paper.

@_date: 2008-05-06 16:24:33
@_author: Jon Callas 
@_subject: OpenSparc -- the open source chip (except for the crypto parts) 
Yes, but.
I tend to agree with Marcos, Ben, and others.
It is certainly true that detecting an evil actor is ultimately  impossible because it's equivalent to a non-computable function. It  doesn't matter whether that actor is a virus, an evil vm, evil  hardware, or whatever.
That doesn't mean that you can't be successful at virus scanning or  other forms of evil detection. People do that all the time.
Ben perhaps over-simplified by noting that a single gate isn't  applicable to Rice's Theorem, but he pointed the way out. The way out  is that you simply declare that if a problem doesn't halt before time  T, or can't find a decision before T, you make an arbitrary decision.  If you're optimistic, you just decide it's good. If you're  pessimistic, you decide it's bad. You can even flip a coin.
These correspond to the adage I last heard from Dan Geer that you can  make a secure system either by making it so simple you know it's  secure, or so complex that no one can find an exploit.
So it is perfectly reasonable to turn a smart analyzer like Marcos on  a system, and check in with him a week later. If he says, "Man, this  thing is so hairy that I can't figure out which end us up," then  perhaps it is a reasonable decision to just assume it's flawed.  Perhaps you give him more time, but by observing the lack of a halt or  the lack of a decision, you know something, and that feeds into your  pessimism or optimism. Those are policies driven by the data. You just  have to decide that no data is data.
The history of secure systems has plenty of examples of things that  were so secure they were not useful, or so useful they were not  secure. You can, for example, create a policy system that is not  Turing-complete, and then on to being decideably secure. The problem  is that people will want to do cool things with your system than it  supports, so they will extend it. It's possible they'll extend it so  it is more-or-less secure, but usable. It's likely they'll make it  insecure, and decideably so.

@_date: 2008-10-24 12:42:59
@_author: Jon Callas 
@_subject: combining entropy 
It's within epsilon for a good many epsilon.
I'm presuming you want the resultant size to be X, as well. Otherwise,  the suggestion that Ben has, concatenation is obviously better, and  you can solve obvious problems.
Another solution is to hash the N pools together with a suitably  secure function. (Most the available algorithms are suitably secure  for this purpose.) The downside of this is that you are capping your  entropy at the size of the hash function. It's better than XOR because  it's not linear, blah, blah, blah.
However, if you had three pools, each relatively large, it doesn't  hurt anything to XOR them together. It's pretty easy to prove that the  result does not decrease entropy, but I think it's impossible to prove  that it increases it. XORing is really taking the max of the N pools.
You have to realize that XOR is bad if there's a chance to leak the  entropy pool, XOR is a bad function. If whoever produced pool X sees  X^Y, then they know Y. But you know that, too.

@_date: 2008-09-20 12:46:27
@_author: Jon Callas 
@_subject: Lava lamp random number generator made useful? 
A TPM has random numbers of arguable quality. I'm happy to argue  either side of it, but that's not what you asked.
A cheap USB camera would make a good source. The cheaper the better,  too. Pull a frame off, hash it, and it's got entropy, even against a  white background. No lava lamp needed.

@_date: 2008-09-23 00:09:46
@_author: Jon Callas 
@_subject: Lava lamp random number generator made useful? 
That's not at all what I suggested. There are so many ways that one  can creatively screw up reasonable cryptographic advice that I don't  think it's worth bothering with.
The point is that if you take a cheap 640x480 (or 320x240) webcam and  point it against a photographic grey card, there's going to be a lot  of noise in it, and this noise is at its bottom quantum in nature.  Thus, there's a lot of entropy in that noise. Photographic engineers  work *hard* to remove that noise, and you pay for a lack of noise.
I'm willing to bet that if I give you hashes of frames, knowing this  process, you can't get pre-images. I'll bet that you can't get pre- images even if I let you put a similar camera next to the one I'm  using. In short, I'm willing to bet that a cheap camera is a decent  random number source, even if you try to control the image source, to  the tune of 128-256 bits of entropy per frame.
No lava lamps are needed, no weird hardware. Just use the noise in a

@_date: 2008-09-24 16:57:40
@_author: Jon Callas 
@_subject: Fake popup study 
What you mean, "We?"
I said ages ago that you cannot produce trust with cryptography, no  matter how much cryptography you use. That's a bow towards Lao Tzu's  original, "you cannot produce kindness with cruelty, no matter how  much cruelty you use."
To quote Crispin Cowan on phishing, it (and other con jobs) are a  security failure on the device that sits between the keyboard and  chair. Until we can issue patches on that device, we're getting  nowhere. Even after, it's a long road ahead. I think you can prove  that it's impossible to stop cons.
What we *can* do is lower the number of them. But we're not going to  get anywhere when we blame the victims. I'm with Jim Youll on this,  the people who think the users are idiots just don't get it.

@_date: 2009-04-30 17:44:53
@_author: Jon Callas 
@_subject: SHA-1 collisions now at 2^{52}? 
Let me make a couple of comments, one from each side of my mouth.
* I would like to see an implementation of this result, producing a  collision. 2^52 is a nice number, but it needs a scale. I'm not  worried about 2^52 years. Or even seconds. I say this solely because I  expected a practical 2^63 collision by now, and have been wondering  about what the scale of that 2^63. I would like to see an  * What do you mean by "no longer theoretical"? The accepted wisdom on  80-bit security (which includes SHA-1, 1024-bit RSA and DSA keys, and  other things) is that it is to be retired by the end of 2010. The end  of 2010 fast approacheth. If you include into development time some  reasonable level of market adoption, one might convincingly argue that  the end of SHA-1 ought to be shipping this summer, or certainly in the  fall, and no later than the *start* of 2010. The need to transition  from SHA-1 is apparent and manifest. New results merely confirm  conventional wisdom.

@_date: 2009-08-26 15:12:29
@_author: Jon Callas 
@_subject: SHA-1 and Git (was Re: [tahoe-dev] Tahoe-LAFS key management, part 2: Tahoe-LAFS is like encrypted git) 
I have no idea, myself.
I have said many times effectively what you said, and there's always  the same hand-wringing.
I believe that it boils down to this:
They aren't software engineers and we are. We've designed  paramaterized or (that's or, not xor) versioned protocols before.  We've done upgrades.
They will inevitably bring up downgrade attacks, but come on. It is a  truism that there is more stupidity than malice in the world and if  you stupid-proof your protocol, you've also malice-proofed it.
And yes, yes, one has to be thorough in your design of plugable  system. I, too, can come up with a scenario where a simple version  number is not enough. It's just a software engineering problem, and  you and I and the other software engineers know how to do software  I think that again, they haven't in general deployed software to a  population large enough to contain stupid people. If they have  deployed it to stupid people, they haven't had the attitude that  stupidity is a fact of life and has to be fixed in the software, not  the person.
And after boiling it down, let me go further and reduce it to a  sticky, bitter sauce:
They don't believe it's important. They so believe the naive simple-is- better line that they end up believing that brittle is better than  resilient. They're so enamored with the aphorism that you can make  something so simple it's secure or so complex it's secure that they  forget the aphorism that you should make things as simple as possible  and no simpler. They're not engineers, so for them, upgrades are free.  Therefore brittle is simpler than resilient.

@_date: 2009-02-13 14:22:07
@_author: Jon Callas 
@_subject: Property RIghts in Keys 
For the same reason that phone books are not copyrightable. A  certificate is nothing more than a directory entry with frosting and

@_date: 2009-01-20 10:04:52
@_author: Jon Callas 
@_subject: MD5 considered harmful today, SHA-1 considered harmful tomorrow 
I've always been pleased with your answer to Question J, so I'll say  what we're doing at PGP.
We deprecated MD5 in '97. That was one of the main points of the new  formats that became OpenPGP was that agility has its own challenges,  but it's worth it.
We had a meeting recently to look at what we're going to do. Our first  thoughts were that we would scrub MD5 from the UI and be done with it.  Then we realized that we need to leave enough of the old UI so that  people can *remove* MD5 from their use.
We decided that we'll issue warnings in the annotations when we verify  MD5 signatures. We can't stop verifying them, but we'll do an  equivalent to what we do with 40-bit crypto in S/MIME. (40-bit still  harries S/MIME; it's really a pity that we have to deal with it. Our  solution is that 40-bit crypto is just a fancy form of plaintext. We  decode it the way we decode quoted-printable, base64, and other fancy  forms of plaintext.) We debated removing it from the APIs, and  concluded that that is asking for trouble, because someone will need  to do that for diagnostic and testing purposes.
We've started deprecating the 160-bit hashes. There will be comments  in the UI for both SHA-1 and RIPE-MD/160. We think NIST's advice for  phasing them out next year is just fine, and so we'll start really  phasing them out next year.
Lastly, we considered other options for hash algorithms. Presently,  it's too early to do anything, but we'll look at it again when we do  more work on the 160-bit hashes.

@_date: 2009-07-02 11:42:39
@_author: Jon Callas 
@_subject: What will happen to your crypto keys when you die? 
I'll point out that PGP has had key splitting for ages now. You can  today make a strong public key and split it into N shares, of which  two or three shares are needed to reconstitute the key, and hand those  out to trusted loved ones.
You can then use that public key for files, virtual disks, whole disk  volumes -- anywhere you could use an RSA or Elgamal key -- and be  assured that your data is safe in the absence of a conspiracy of those  loved ones.
It's there now, and has been there for a decade.

@_date: 2009-07-20 17:15:25
@_author: Jon Callas 
@_subject: XML signature HMAC truncation authentication bypass 
That's precisely what it is -- a denial of service to password crackers.
There are a couple of things I'll add, one in the OpenPGP standard,  and one in that implementation.
In the standard, the iteration count is not a count of hash iterations  as in (e.g.) PKCS but a length of output. So four million is four  million bytes of output. For SHA-1, that's a count of 200,000, and for  SHA-256 125,000 iterations. While this is a bit eccentric, it allows  you to use any size hash and any block size cipher. Even more  eccentric is the way it's encoded, as an 8-bit floating point value.
In the implementation, we upped the default because of more password  cracking, but also added a twist in it. We time the number of  iterations take 1/10 of a second on the computer you're using, and use  that value. The goal is to have the iteration count scale as computers  get faster without having to make software changes.
The downsides of this are left as an exercise for the reader (as are  the obvious workarounds).

@_date: 2009-07-26 14:44:35
@_author: Jon Callas 
@_subject: XML signature HMAC truncation authentication bypass 
You are of course correct, Peter, but are you saying that we shouldn't  do anything?
I don't believe that we should roll over and die. We should fight  back, even if the advantage is to the attacker.
You are wrong with this.
*Messages* don't have this property, so long as they were encrypted to  a public key. It is unlocking the *key* that has this problem.
That problem *only* exists when you import a key from a fast client  into a slow client. That problem can be fixed either through some  smart software (look at the iteration count and if it's higher than  you like, change it the next time you use the key), or the user can do  it manually. Set your passphrase once to the same thing it used to be.

@_date: 2009-07-27 17:11:34
@_author: Jon Callas 
@_subject: XML signature HMAC truncation authentication bypass 
Let's look at it the other way, and suppose that I said that despite  increases in processor power and distributed password crackers, we  would leave the iteration count where it was in 1997, because if you  increase it, it might go to small machines where that would be an  issue. Consider the tradeoffs. You'd call me daft for refusing to  protect the majority of people.
Okay, password-protected files would get it, too. I won't ask why  you're sending password protected files to an agent. I know you didn't  design this.
Sure, but. I think that "unintended consequences" is not quite the  right way to put it. We don't intend to cause slow computers problems,  but it was an intentional change with well-known upsides and  downsides. Despite that, the upside seems to outweigh the downside.
This change shipped in September 2006. It's nearly three years old,  and this would make only the second issue we had with it.
When it shipped, BlackBerries used signed math in computing the  iteration count, and got it wrong. We made a BlackBerry export tool  that reset the iteration count. That got fixed in 4.1 of the BB  software, as I remember it.
So with millions helped and one field problem, it's not bad.
By the way, do you think it's safe to phase out MD5? That will break  all the PGP 2 users.

@_date: 2009-06-28 12:53:40
@_author: Jon Callas 
@_subject: password safes for mac 
I would recommend the built-in keychain for anything that it works with.

@_date: 2009-05-02 13:02:57
@_author: Jon Callas 
@_subject: [tahoe-dev] SHA-1 broken! 
I'm not being entirely a smartass when I say that it's always in the  realm of possibility. The nominal probability for SHA-1 -- either 2^80  or 2^160 depending on context -- is a positive number. It's small, but  it's always possible.
The recent case of cert collisions happened because of two errors,  hash problems and sequential serial numbers. If either had been  corrected, the problem wouldn't have happened.
I liken in in analogy to a fender-bender that happened because the  person responsible had both worn-out brakes (an easily-fixable  technological problem) and was tailgating (an easily-fixable  suboptimal operational policy). It's a mistake to blame the wreck on  either. It's enlightening to point out that either a good policy or a  more timely upgrade schedule would have made the problem not occur.
The problem right now is not that MD5, SHA1, etc. are broken. It is  that they are broken in ways that you have to be an expert to  understand and even the experts get into entertaining debates about.  Any operational expert worth their salt should run screaming from a  technology that the boffins have debates about flaws over dinner.

@_date: 2009-05-13 09:37:19
@_author: Jon Callas 
@_subject: Warning!  New cryptographic modes! 
I'd use a tweakable mode like EME-star (also EME*) that is designed  for something like this. It would also work with 512-byte blocks.

@_date: 2009-09-17 15:14:35
@_author: Jon Callas 
@_subject: Biotech Based "Cryptogram Challenge" 
Yes, but it has nothing to do with biotech at all, except in the  presentation. The instructions say that the plaintext is represented  in the RGB values of each cell, along with the transparency (alpha) of  each color blob. So each character is represented (wolog, since we  don't know how deep the alpha channel is) by an integer value of ABGR  of each color blob.
Cute, but not biotech.

@_date: 2010-08-04 22:46:44
@_author: Jon Callas 
@_subject: A mighty fortress is our PKI, Part II 
That is because a tragedy involves someone dying. Strictly speaking, a tragedy involves a Great Person who is brought to their undoing and death because of some small fatal flaw in their otherwise sterling character.
In contrast, comedies involve no one dying, but the entertaining exploits of flawed people in flawed circumstances.
PKI is not a tragedy, it's comedy. No one dies in PKI. They may get embarrassed or lose money, but that happens in comedy. It's the basis of many timeless comedies.
Specifically, PKI is a farce. In the same strict definition of dramatic types, a farce is a comedy in which small silly things are compounded on top of each other, over and over. The term farce itself comes from the French "to stuff" and is comedically like stuffing more and more feathers into a pillow until the thing explodes.
So farces involve ludicrous situations, buffoonery, wildly improbable / implausible situations, and crude characterizations of well-known comedic types. Farces typically also involve mistaken identity, disguises, verbal humor including sexual innuendo all in a fast-paced plot that doesn't let up piling things on top of each other until the whole thing bursts at the seams.
PKI has figured in tragedy, most notably when Polonius asked Hamlet, "What are you signing, milord?" and he answered, "OIDs, OIDs, OIDs," but that was considered comic relief. Farcical use of PKI is far more common. We all know the words to Gilbert's patter-song, "I Am the Very Model of a Certificate Authority," and Wilde's genius shows throughout "The Importance of Being Trusted." Lady Bracknell's snarky comment, "To lose one HSM, Mr. Worthing, may be regarded as a misfortune, but lose your backup smacks of carelessness," is pretty much the basis of the WebTrust audit practice even to this day.
More to the point, not only did Cyrano issue bogus short-lived certificates to help woo Roxane, but Mozart and Da Ponte wrote an entire farcical opera on the subject of abuse of issuance, "EV Fan Tutti." There are some who assert that he did this under the control of the Freemasons, who were then trying to gain control of the Austro-Hungarian authentication systems. These were each farcical social commentary on the identity trust policies of the day. Mozart touched upon this again (libretto by Bretzner this time) in "The Revocation of the Seraglio," but this was comic veneer over the discontent that the so-called Aluminum Bavariati had with the trade certifications in siding sales throughout the German states, as well as export control policies since Aluminum was an expensive strategic metal of the time. People suspected the Freemasons were behind it all yet again. Nonetheless, it was all farce. Most of us would like to forget some of the more grotesque twentieth-century farces, like the thirties short where Moe, Larry, and Shemp start the "Daddy-O" DNS registration company and CA or the "23 Skidoo" DNA-sequencing firm as a way out of the Great Depression. But S.J. Perleman's "Three Shares in a Boat" shows a real-world use of a threshold scheme. I don't think anyone said it better than W.C. Fields did in "Never Give a Sucker an Even Break" and "You Can't Cheat an Honest Man."
I think you'll have to agree that unlike history, which starts out as tragedy and replays itself as farce, PKI has always been farce over the centuries. It might actually end up as tragedy, but so far so good. I'm sure that if we look further, the Athenians had the same issues with it that we do today, and that Sophocles had his own farcical commentary.

@_date: 2010-08-10 14:27:27
@_author: Jon Callas 
@_subject: NY Times article on Blackberry 
Indeed, but there are also other things not being mentioned.
One is that there is an OpenPGP package available on all RIM devices, and if you are using that, you get true end-to-end crypto. Another is that one of the things that the Saudis definitely want is control over whether young men and young women are talking to each other, which is a threat to society far more pernicious than terrorism.

@_date: 2010-08-13 11:12:53
@_author: Jon Callas 
@_subject: Has there been a change in US banking regulations recently? 
Possibly it's related to PCI DSS and other work that BITS has been doing. Also, if one major player cleans up their act and sings about how cool they are, then that can cause the ice to break.
Another possibility is that a number of people in financials have been able to get security funding despite the banking disasters because the risk managers know that the last thing they need is a security brouhaha while they are partially owned by government and thus voters.
I bet on synergies between both.
If I were a CSO at a bank, I might encourage a colleague to make a presentation about how their security cleanups position them to get an advantage at getting out from under the thumb of the feds over their competitors. Then I would make sure the finance guys got a leaked copy.

@_date: 2010-03-23 14:42:17
@_author: Jon Callas 
@_subject: "Against Rekeying" 
I think that if anything, he doesn't go far enough.
Rekeying only makes sense when you aren't using the right crypto, and even then might make the situation worse. Rekeying opens up a line of attack. From a purely mathematical point of view, here's a way to look at it:
The chance of beating your cipher is P1 (ideally, it's the strength of the cipher, let's just say 2^-128). The chance of beating the rekey protocol is P2. Rekeying makes sense when P2 is smaller than P1. When P2 is larger than P1, you've reduced the security of your system to the chance of a flaw in the rekeying, not the cipher.
As others have pointed out, it's front of Ekr's mind that there is (was) a major flaw in the SSL/TLS protocol set that came out because of bugs in rekeying. Worse, it affected people who wanted high security in more evil ways than people who just wanted casual security. Many people (including me) think that the best way to fix this is to remove the rekeying. If you need to rekey, tear down the SSL connection and make a new one. There should be a higher level construct in the application that abstracts the two connections into one session.
In most cases where you might want to rekey, the underlying system makes it either so trivial you don't need to think about it, or so hard that you can ignore it because you just won't.
Let me give a couple examples. First the trivial one. Consider a directory of files where each file is encrypted separately with a bulk key per-file. The natural way to do this is that every time someone rewrites a file, you make a new bulk key and rewrite the file. You don't have to worry about rekeying because it just falls out.
Now the hard one. Consider a disk that is encrypted with some full disk encryption system. If you want to rekey that disk, you have to read and write every block. For a large disk, that is seriously annoying. If your disk does 100MB/s (which very fast for a spindle and still pretty fast for SSDs), then you can do 180G per hour (that's 6G per minute, 360G per hour, and halve it because you have to read and write) max. That's about six hours for a terabyte.
If your disk only does 10MB/s, which many spindles do, then it's 60 hours to rekey that terabyte. You can do the math for other sizes and speeds as well as I can. In any event, you're not going to rekey the disk very often. In fact most of the people who really care about rekeying storage are changing their requirements so that you have to do a rekey on the same schedule as retiring media -- which effectively means no rekey.
A long-time rant of mine is that security people don't do layering. I think this falls into a layering aspect. If you design your system so that your connection has a single key and you transparently reconnect, then rekeying is just forcing a reconnect. If you make your storage have one key per file, then rekeying the files is just rewriting them. It can easily vanish.
And yes, obviously, there are exception cases. Exceptions are always exceptional.

@_date: 2010-03-24 15:36:48
@_author: Jon Callas 
@_subject: "Against Rekeying" 
Exactly, but they're at the proper place in the system. That's what layering is all about.
I'm not suggesting that there's a perfect solution, or even a good one. There are times when a designer has a responsibility to make a decision and times when a designer has a responsibility *not* to make a decision.
In this particular case, rekeying introduced the most serious problem we've ever seen in a protocol like that. Rekeying itself has always been a bit dodgy. If you're rekeying because you are worried about the strength of the key (e.g. you're using DES), picking a better key is a better answer (use AES instead). The most compelling reason to rekey is not because of the key, but because of the data size. For ciphers that have a 64-bit block size, rekeying because you've sent 2^32 blocks is a much better reason to rekey. But -- an even better solution is to use a cipher with a bigger block size. Like AES. Or Camillia. Or Twofish. Or Threefish (which has a 512-bit block size in its main version). It's far more reasonable to rekey because you encrypted 32G of data than because you are worried about the key.
However, once you've graduated up to ciphers that have at least 128-bits of key and at least 128-bits of block size, the security considerations shift dramatically. I will ask explicitly the question I handwaved before: What makes you think that the chance there is a bug in your protocol is less than 2^-128? Or if you don't like that question -- I am the one who brought up birthday attacks -- What makes you think the chance of a bug is less than 2^-64? I believe that it's best to stop worrying about the core cryptographic components and worry about the protocol and its use within a stack of related things.
I've done encrypted file managers like what I alluded to, and it's so easy to get rekeying active files right, you don't have to worry. Just pull a new bulk key from the PRNG every time you write a file. Poof, you're done. For inactive files, rekeying them is isomorphic to writing a garbage collector. Garbage collectors are hard to get right. We designed, but never built an automatic rekeying system. The added security wasn't worth the trouble.
Getting back to your point, yes, you're right, but if rekeying is just opening a new network connection, or rewriting a file, it's easy to understand and get right. Rekeying makes sense when you (1) don't want to create a new context (because that automatically rekeys) and (2) don't like your crypto parameters (key, data length, etc). I hesitate to say that it never happens, but I think that coming up with a compelling use case where rekeying makes more sense than tearing down and recreating the context is a great exercise. Inconvenient use cases, sure. Compelling, that's hard.

@_date: 2010-09-13 16:33:34
@_author: Jon Callas 
@_subject: Merkle Signature Scheme is the most secure signature scheme possible for general-purpose use 
I want to rewind the discussion a bit.
Things being said here range from the outright true, to the true-but-irrelevant, all the way to being a canard.
There is a core question that we're not addressing at all, and that is: "How much security does a hash function have?" There is no good answer to that question.
The base rule of thumb is half the bit size. If you were in my crypto group and you said, "Hey, I'm using an N-bit symmetric key, what size hash function should I use?" I would answer 2N. So for AES-128, use SHA-256.
There are many reasons for that, the big one being the birthday bounds (for which, incidentally, N/2 is an approximation, not an exact answer).
However, there are many uses of hash functions that have a security limit greater than the birthday bounds. When someone knows what they are, and when to use them, then they can.
Picasso said "when I run out of blue, I use red instead." Once you're a good enough painter that you know why  Picasso said that, you can use red instead of blue. Until then, you should use blue for blue and red for red. Similarly, until you know when a hash function has security greater than the birthday bounds, you should assume that it has birthday-bound security, N/2 bits.
When NIST started the SHA3 competition, they talked to many of us, and this very issue -- security greater than the birthday bounds -- was something they said they wanted. A number of us, including me, replied that we didn't want security beyond the birthday bounds because we teach our security engineers that hash functions have a security of their birthday bounds. In other words, we want them to use red for red and blue for blue.
Moreover, if you just use a hash function that way, it only needs internal state equal to its output size. There's a term circulating now about "pipe width" to describe that, but it's frequently a misnomer. It's not a good way to describe it in many circumstances, but that's not the rant I want to go on right now. I'll save that rant for later.
NIST asked for security beyond the birthday bounds, and suggested that a reasonable internal state size would be 4/3 the output size.
From a security point of view, this is kinda reasonable. From an implementation point of view, it sucks gravel up a garden hose. From an implementation point of view, 4/3 just gets rounded up to 2.
In my particular hash function, Skein, the security parameter we gave it was internal state size; Skein can have *any* output size, even one larger than the state size (and yes, I know what you're thinking, and you're right -- if it has state S that is greater than output O, it has a security parameter of f(S) not f(O), it's still useful in main places). The primary function is Skein-512, which with an output size of 256 is "wide pipe" to use that horrid term, and with an output size of 512 is "narrow pipe."
That's the reason why there is also Skein-1024. If you want a hash function with 512 bits of output and a security bounds that it beyond the birthday limit, you need more than 512 bits of state. NIST asked for that, and that is the reason for Skein-1024, or any other large-state hash function; it gives security beyond the birthday bound. The alternative was Skein-667, The Hash Function That Lives Across The Street From The Beast.
And this is why this is something of a rant. If you presume that a hash function has N/2 bits of security, this whole discussion dissolves into the traditional greasy black smoke. If you match AES-128 with SHA-256, then it doesn't matter for your Merkle signature that it ends up with 255.33 bits of security, because you're only claiming 128 bits of security. If you want 256 bits of security, use SHA-512. That's using red for red and blue for blue. The problem only crops up if you want a rigorous discussion of when it's okay to use blue for red and when it's not. It's also relevant to the SHA3 discussion (which is part of how we got here) because NIST asked for security beyond the birthday bounds whenever possible, and people who have hash functions with large state are trying to count coup upon the people who have hash functions with small state. The issue is relevant only because the rule of thumb is being broken. So to sum up -- if you assume that a hash function has security equivalent to its birthday bounds, you escape many of these quandaries. I will assert also that this is equivalent to using large state to get beyond-birthday-bounds security and also that we'd be better off with a "narrow pipe" 1024-bit hash function that we assume has 512 bits of security than a "wide-pipe" 512-bit hash function that we try to use beyond the birthday bounds.

@_date: 2013-08-29 22:31:55
@_author: Jon Callas 
@_subject: [Cryptography] Email and IM are ideal candidates for mix 
<521CE337.6030706
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Personally, I think you shouldn't worry about this.
The real sin is getting an attachment to a key. You are much better off developing a philosophy of key management in which you use it and then get rid of it regularly. If you do this reasonably well, it reduces the chance that a key will get compromised because its aegis, footprint, shadow, etc. is small. It also reduces the effect because most likely it takes more time to break the key than its lifetime; I consider hacking the key, stealing it, etc. to be a form of breaking. Stealing a key through a 'sploit is also cryptanalysis.
Be Buddist about your keys and have no attachments. (This is also a good philosophy about mail, but that's a different discussion.)
That's a fine step to a good attitude, but the effect on traffic analysis will be small or close to nil. Traffic analysis includes social graph analysis and any good social graph analysis will include probabilities that an entity will have different personae. Keys are just masks, too, just like a persona.

@_date: 2013-12-11 17:27:07
@_author: Jon Callas 
@_subject: [Cryptography] Size of the PGP userbase? 
Hash: SHA1
You should at the very least look at keyserver.pgp.com, which has very good numbers, because you have to re-verify your key there every six months or it gets deleted.
As Tamzen said, you're going to miss lots of keyservers that are domain-specific keyservers for all the people using PGP Universal (now known as Symantec Ecryption Server, because when you think of email encryption, you think of Symantec). You'll find those because they're (typically) using the hostname "keys.domain.tld" and have an LDAP server. I know there are several million users of that around the net.

@_date: 2013-12-12 15:29:11
@_author: Jon Callas 
@_subject: [Cryptography] Size of the PGP userbase? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I don't think my answer is lots different than it's been for the last decade.
First of all, people have to actually care. There are lots of systems that are extremely easy to use if people actually care.
For example, there's Enigmail for Thunderbird. But there's also the built-in S/MIME support in many popular systems. I can tell you from experience that the S/MIME support in OS X and iOS effin' rocks. I hear it's good on other platforms, too. The easiest email encryption I've every used is S/MIME on OS X and iOS, modulo a whole bunch of things -- like the out-of-the-box experience, which sucks. (I could mention other issues, but that's a digression.)
However, the whole model is one that weeds out the people who don't actually care. It's not something where someone just checks a box and turns on secure email, you have to prove you're *worthy* of it. For S/MIME, that means getting a cert, for example, which is surprisingly hard. It means renewing certs, which sometimes is amazingly harder. (At one time when I was doing S/MIME things, a certain commercial CA actually would not work if you tried to buy a new cert before your old one expired. Shame on me for wanting to renew. I solved this by buying from a different CA.) It means the way that the software actively discourages self-signed certs or private PKIs. For OpenPGP, this includes the difficulty of generating your keys, figuring out there to publish them, etc. Many of the community keyservers have no way to *delete* keys. I keep getting things sent to me encrypted to keys that I retired in the last millennium.
The bottom line is that the infrastructure for secure email makes it hard, because that's apparently good for you or something. There are plenty of things that make it really simple -- oh, like my aforementioned PGP Universal, excuse me, Symantec Encryption Server. I use that myself and it passes the "My Seventy-Nine Year Old Mother Can Use It" test because in fact, my seventy-nine year-old mother *does* use it. If it weren't for that, I wouldn't do secure email because, well, it's basically so hard that I can't be bothered, either.
The real underlying issue is that the people who are *creating* email security don't care enough to make it easy. The idea seems to be that only those who care enough to jump through the hoops deserve security.

@_date: 2013-12-12 16:57:26
@_author: Jon Callas 
@_subject: [Cryptography] Size of the PGP userbase? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Cool. Laudable goal. Even achievable. We did it at PGP Corporation. I'll add that despite the name "PGP Universal" it did S/MIME, too, and images a public key into both OpenPGP keys and S/MIME certificates.
There are lots of things you can do here. For example, the advantage of S/MIME is that certs are in the messages. You can sniff them out of incoming messages and cache them. So, for example, suppose Alice sends an S/MIME message to Bob. If Charlie then sends Alice a message, you can use that cached cert to get transparent encryption for Charlie.
You can use the convention we did of keys.* to be a domain name for a key/cert server, as well. Our SMTP proxy would go ask the recipient domain for relevant certs and use them. My 2003 "Self-assembling PKI" paper gives the basic rundown of many, many techniques. They work amazingly well.

@_date: 2013-12-13 10:27:18
@_author: Jon Callas 
@_subject: [Cryptography] Size of the PGP userbase? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
The stats on my server show that about 1.5% to 2% are encrypted/signed messages. Of that, about 85% are decryption/verify events. My results are obviously going to be more than the population at-large because I'm running an encryption server, but anecdotally, almost all of those are OpenPGP or S/MIME signature verifications.

@_date: 2013-12-13 11:19:05
@_author: Jon Callas 
@_subject: [Cryptography] Size of the PGP userbase? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Please don't think I am trying to discourage you. You're doing something fantastic here. I'm only trying to give some hints based on my own successes and failures.
The major reason email security has failed is that crypto is easy, user experience is hard. The developments have focused on the crypto, and only then on the UX. Even the best ones fall down on the most important parts of UX, the initial experience.
Every place I have succeeded, it's because we started with the UX and made the crypto work. The places where we let the crypto trump the UX, we failed.
Snowdonia is giving a spur to lots of people to finally get off their asses and do something. However, if they think to themselves, "Well, the NSA isn't after *me*..." then we're back where we were.

@_date: 2013-12-28 08:37:41
@_author: Jon Callas 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Me, I didn't see it.
I should confess that I have moderator privs for the list, too, but barely have enough time to read it. I've also been a more sucky moderator, letting in things that are Perry's peeves and not mine more. In the general problem, I think Ben Franklin's old maxim that three people can keep a secret if two of them are dead is apt. In general, you can't have a secure system that meets the properties of a mailing list or forum.
It's kinda like saying that you want a conversation that is secure only to the people within earshot, and as people go in and out of earshot, the security allows or denies them. You have to ask how "secure" is different from "within earshot."

@_date: 2013-12-28 08:48:49
@_author: Jon Callas 
@_subject: [Cryptography] deniable symmetric ciphers? 
Hash: SHA1
Under your definition, pretty much they all are. If ciphertext is distinguishable from random, then that's a flaw in the cipher. It may not be one worth worrying about, but ideally, ciphertext should be indistinguishable from random.
Known plaintext happens all the time. For example, the known plaintext '' or '\n\n<html lang="' are very common. If a cipher leaks an that XML header is an XML header, then it's just not a very good cipher.
I have to ask why you'd call this property "deniable." There are lots of things that produce data indistinguishable from random, but most of them carry metadata along with it. For example, compression functions ideally are indistinguishable from random, too, but they have metadata hints about that data. Compression functions *want* to be decoded.
If an adversary sees bare-ass nekkid "deniable" data, the first hypothesis about it is that it's ciphertext. A denial of that has to have a reasonable counter-hypothesis. If the na?ve attacker just assumes that more-or-less random data is cipher text, they win against this model. Yes, they get false positives, too, but they may not care.
If you want to have a model of deniability, that model has to create or encourage counter-hypotheses. Those counter-hypotheses are more important than the raw output because it's hard to hide data that's indistinguishable from random.

@_date: 2013-12-28 11:13:39
@_author: Jon Callas 
@_subject: [Cryptography] deniable symmetric ciphers? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
The output of a fixed key and ECB mode is indistinguishable from random (from an information theory point of view), but bad security engineering. Mistakes in IV, nonces, etc. are all still indistinguishable from random, even though they are massive security flaws that in many cases can even let a passive adversary distinguish *traffic*.
Even successes in this area have side effects. If everything's truly indistinguishable from random, then it's also impossible to distinguish a signal from noise (without credentials, of course). This includes spam from legit messages, DDoS packets from wanted ones, and so on. There are many legit scenarios where this cure could be considered worse than the disease.
There's also the underlying issue that there's no such thing as "security" as an abstract quality. There's protection against a class of adversaries under a class of conditions. If I'm willing to accept a vulnerability as a consequence of a type of protection, I win. For example, it's unlikely someone will DDoS me, but I know that nation states passively want my traffic. If that's not true, then I'm playing tradeoffs, and my tradeoffs might not be someone else's. Worst of all is a case where the same adversary will adaptively attack based on my observed defenses.
And of course, we're assuming that it's *possible* for all of us to hide all our data as indistinguishable from random.
But yes, the basic principle you're citing is the Tor motto -- anonymity loves company. It's true, it's just not so true that it's an overriding principle as opposed to a good rule of thumb.

@_date: 2013-12-28 17:00:27
@_author: Jon Callas 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
For starters, we need to stop using the word "perfect."
There are many, many sins created because of fetishism over "perfect." Look at all the people who try to figure out how to use one-time-pads, and end up with horrid systems because they were seduced by the word "perfect." Forward serecy is a Good Thing. But forward secrecy is a key management property, not a crypto property, and confusing key management with crypto is also the source of many errors. It's the same sort of problem as thinking about indistinguishability and then oopsing on ECB mode or nonce errors that I alluded to earlier today.
A long time ago, I was dazzled by a rant by Donn Parker. That particular rant was about the foolishness of "Best Practices" but really applies to any use of "best" or "perfect." A summary of his rant is that "best" is a comparative, not a standard. If you have three bad practices, the least bad is best, yet none of them are good. In contrast, if you have three good practices, trying to figure out which one is best is a waste of time. They're all good. Just pick one. His summary was that we should stop talking about best and talk about good.
"Perfect" has all the problems that "best" does -- and in fact, since "perfect" literally means that it cannot be improved on, it's merely a synonym for "best."
Forward secrecy is a good thing. If you're doing communications of any sort, it's relatively easy to get some degree of forward secrecy -- just throw the keys away -- and that's how all PFS protocols work. However, the more you focus on forward secrecy, the harder the authenticity problem is. There's less linkage between the messages, which is great if you assume a passive adversary. But if you assume an *active* adversary who might try to throw in a bogus message, then  linkage between messages has a different set of goodnesses. Independence of messages advantages the adversary.
If you look at a system like full disk encryption, the equivalent of forward secrecy is hard and harder the bigger your disk is because you get forward secrecy by re-encrypting the whole darned thing. But the key management is easy. If you build a per-file encryption system for storage, you get better forward secrecy, but a more complex key management system.
Many (most? all?) systems are ones where you have to pick your poison. Words like "best" and "perfect" shape thought into a certain set of reality-tunnels that imply and presuppose things that just aren't necessarily so.

@_date: 2013-12-30 07:44:48
@_author: Jon Callas 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
It was a speech he gave at some event I was at, and likely an ISSA event or ISACA. It was a life-changing experience.
I'm sure that's the print version, if watered down.
The minute someone uses the plural of "best practice" they prove Donn right.
Alternatively, the word "best" should be reserved for practices known to be not-good, but you have to do *something*.
Look at all the discussion of Perfect Forward Secrecy going on now. It's often just flat broken and counterproductive.

@_date: 2013-12-30 08:26:46
@_author: Jon Callas 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Of course not. The first rule of real-world security is that there are more threats than you can defend against. This is the whole reason we have "threat models." It's a way to scope the unsolvable totality in.
Actually, they're extraordinarily useful.
There are always people who get a bee in their bonnet about something that's real but unlikely. Pushing them off into any of the above has two advantages -- it gets them out of your hair of dealing with the real and actual problems, and when you make progress in solving the present actual problem, the next one will be some past real-but-unlikely problem. So you get a leg up on the next one.
In many organizations, this person is called the CSO.

@_date: 2013-11-23 21:48:56
@_author: Jon Callas 
@_subject: [Cryptography] Dark Mail Alliance specs? 
Hash: SHA1
Drop me an email.
If you have and I've lost it, I apologize. I've been busy, ill with bronchitis, and not successfully staying ahead of things I need to do.
I'm working on a Dark Mail white paper. Perhaps ironically, perhaps not, the biggest obstacle I face in doing it is all the people who want to know what it is.
But drop me an email.
To answer your salient questions, we're not doing either OpenPGP nor S/MIME. We're going much simpler.

@_date: 2013-11-26 08:46:32
@_author: Jon Callas 
@_subject: [Cryptography] Explaining PK to grandma 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Yes, "signature" is an unfortunate term. "Seal" would have been better.

@_date: 2013-11-26 08:50:16
@_author: Jon Callas 
@_subject: [Cryptography] Explaining PK to grandma 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Agree totally. It has to be so simple as to be invisible, even though that is suboptimal. Yup, it would be better if Gran cared enough to understand. She doesn't. (And yes, I believe that Gran *could* understand, and what's missing is desire, not ability. Obviously, better metaphors are great, if she cared.)
Also -- the way that you make the safety systems work in crypto for Gran is the same way you make the airbags work for Gran. They just do.

@_date: 2013-11-26 09:44:51
@_author: Jon Callas 
@_subject: [Cryptography] Explaining PK to grandma 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I hate to be blunt, but you're going to fail.
Teaching people the risks inherent in driving machines like cars has only happened because of legislation, active and passive regulation (insurance demands are passive regulation), and three generations of time.
Even something like seat belts in autos has this problem -- and the threat is *really* easy to understand: your face goes into the windshield. Without laws mandating it, the user uptake in seat belts is only about 15-20%.
You and I are in that group (at least for crypto), but we are the *vast* minority. The only thing that works is invisible, transparent crypto, and accepting the risks that implies. Add on to that continuous engineering and improvement -- heck, the same thing happened with cars to make them safer -- but even that only happened with regulation.

@_date: 2013-11-26 10:27:56
@_author: Jon Callas 
@_subject: [Cryptography] Explaining PK to grandma 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I think we're in violent agreement, then.
I think you can do better than mere transport security. I think you can secure the content as well.
I also think there's a blur in messaging from IM into email. Once you go from simple texting to multi-media attachments, to having subject lines, to wanting to file things into folders, and then you wake up and you have email again. And that's the way forward to get secure messaging.

@_date: 2013-10-15 19:32:24
@_author: Jon Callas 
@_subject: [Cryptography] PGP Key Signing parties 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Not me.
I'm not a fan of key signing parties, myself.

@_date: 2013-10-18 09:34:12
@_author: Jon Callas 
@_subject: [Cryptography] /dev/random has issues 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I say yes, this is a fine place, myself. How /dev/random, etc. work is at the core of lots of cryptography, from key generation to the appropriateness of whitening functions to real-world engineering, and at the end of the day, cryptography is an engineering discipline.

@_date: 2013-09-02 21:49:29
@_author: Jon Callas 
@_subject: [Cryptography] NSA and cryptanalysis 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
The real issue is that the P-521 curve has IP against it, so if you want to use freely usable curves, you're stuck with P-256 and P-384 until some more patents expire. That's more of it than 192 bit security. We can hold our noses and use P-384 and AES-256 for a while.

@_date: 2013-09-03 09:28:29
@_author: Jon Callas 
@_subject: [Cryptography] NSA and cryptanalysis 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
My understanding is that of the NIST curves, P-256 and P-384 are unencumbered and that P-521 was dropped from Suite B because of IP concerns along with MQV. I don't pretend to speak with authority on any of it. The niggling things often don't make sense. I'm just saying what my understanding is.

@_date: 2013-09-03 11:41:08
@_author: Jon Callas 
@_subject: [Cryptography] FIPS, NIST and ITAR questions 
Hash: SHA1
ITAR? Cryptography hasn't been under ITAR since way back in the 1900s.

@_date: 2013-09-05 16:57:30
@_author: Jon Callas 
@_subject: [Cryptography] Is ECC suspicious? 
Hash: SHA1
If there is a place to worry, it would be about the specific curves.
I had a lively dinner-table conversation with Dan Bernstein and Tanja Lange at CRYPTO this year, and Dan pointed out that there's been a lot of work on cryptanalysis of specific curves and curve families. We know, for example that anything over GF(p^n) is seeming dodgy, but GF(p) seems okay. There are recent Eurocrypt papers on said.
The Suite B curves were picked some time ago. Maybe they have problems.
I have a small amount of raised eyebrow because the greatest bulwark we have against the SIGINT capabilities of any intelligence agency are that agency's IA cousins. I don't think that the Suite B curves would have been intentionally weak. That would be a shock.
However, if the SIGINT guys (e.g.) discovered a weakness that gave P-256 something les than 128 bits of security, they might just sit on it. Certainly, even if they wanted to release that, there would be politics compounded by security compartments. Learning that they sat on a weakness would might be a shock, but it wouldn't be a surprise.
If there is an issue, that's the place it would be. Not ECC as a technology, but specific curves.

@_date: 2013-09-05 19:00:21
@_author: Jon Callas 
@_subject: [Cryptography] Suite B after today's news 
Hash: SHA1
My opinion about GCM and GMAC has not changed. I've never been a fan.
My objection to them is that they are tetchy to use -- hard to get right, easy to get wrong. It's pretty much what is in Niels's paper:
I don't think they're actively bad, though. For the purpose they were created for -- parallelizable authenticated encryption -- it serves its purpose. You can have a decent implementor implement them right in hardware and walk away.
I think that any of OCB, CCM, or EAX are preferable from a security standpoint, but none of them parallelize as well. If you want to do a lot of encrypted and authenticated high-speed link encryption, well, there is likely no other answer. It's GCM or nothing.
Remember that every intelligence agency has a SIGINT branch and an IA (Information Assurance) branch. Sometimes they are different agencies (at least titularly) like GCHQ/CESG, BND/BSI, etc. The NSA does not separate its SIGINT directorate and the IA directorate into different agencies.
I think the IA people have shown they do a good job, but they are humans too and make mistakes. Heck, there are things that various IA people do and recommend that I disagree with from weakly to strongly. I weakly disagree with GCM -- I think it's spinach and I say to hell with it, as opposed to thinking it's crap.
Would a signals intelligence organization that finds a flaw in what the IA people did tell the IA branch so people can fix it? That's the *real* question.

@_date: 2013-09-05 19:21:17
@_author: Jon Callas 
@_subject: [Cryptography] Suite B after today's news 
Hash: SHA1
How do you feel (heh, I typoed that as "feal") about the other AEAD modes?

@_date: 2013-09-05 19:19:12
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
I don't disagree by any means, but I've been through brittleness with both discrete log and RSA, and it seems like only a month ago that people were screeching to get off RSA over to ECC to avert the "cryptocalypse." And that the ostensible reason was that there are new discrete log attacks -- which was just from Mars and I thought that that proved the people didn't know what they were talking about. Oh, wait, it *was* only a month ago! Silly me.
"Crypto experts issue a call to arms to avert the cryptopocalypse"
Discrete log has brittleness. RSA has brittleness. ECC is discrete log over a finite field that's hard to understand. It all sucks.
And now we're back to the hymnal you and I have been singing from. It ain't the crypto, it's the software.

@_date: 2013-09-05 19:55:04
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Hash: SHA1
Yes, but. The reason we are using those curves is because they want them for products they buy. I think it might even go deeper than that. ECC was invented in the civilian world by Victor Miller and Neal Koblitz (independently) in 1985, so they've been planning for breaking it even a decade before its invention. I definitely believe (b). However, I also think that they aren't a monolith, and we know that each part of the mission is the adversary of the other. I don't believe that the IA people would do a bad job to support SIGINT. Once you start down that path, it's easy to get to madness, or perhaps merely evidence that they have time travel.
I'll add that they have a third mission -- run the government's classified computer network, and that *that* mission is the one that Snowden worked for.

@_date: 2013-09-05 20:54:47
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Not really. The Needham-Schroeder you're thinking of is the essence of Kerberos, and while Kerberos is a very nice thing, it's hardly a replacement for public key.
If you use a Needham-Schroeder/Kerberos style system with symmetric key systems, you end up with all of the trust problems, but on steroids.
(And by the way, please say "symmetric key" as opposed to "public key" -- if you say "private key" then someone will inevitably get confused and think you mean the private half of a public key pair and there will be tears.)
I have to disagree. You don't need a CA. There's a very long rant I could make here, and I'll try to keep it a summary.
Much of the system we have is built needing CAs, but it was only built that way. A long time ago, the certificate structure we're still vestigially using had as one of its goals a way to keep the riff-raff from using crypto. I remember when I got my first PEM certificate, I had to send my blinking passport off to MITRE for two weeks so they could let me encrypt the crapola that was sitting on my disk unencrypted. It was harder to get a cert than it was to get a visa to Saudi Arabia! So much of what we would have encrypted we just printed on paper and put in a file cabinet. Excuse me, I'm starting on that rant I said I wouldn't do.
The major problem one has with public key is knowing that the public key of the endpoint you want to talk to us actually the right public key. Trusted Introducers of any sort are one way to solve the problem. CAs are merely an industrialized form of Trusted Introducer and not ipso facto bad. The way that "Web PKI" (as it's now being called) is using Trusted Introducers is suboptimal, but ironically, we are on the inflection point of a real honest-to-whomever fix to them in the form of Certificate Transparency. That suggests even another discussion, one that I promised Ben I'd get to eventually.
The major problem with the certificate system is actually the browsers, in my opinion, because they actively discourage using certificates in any other way. If browsers, for example, allowed you to use a private cert with a user experience that was ultimately SSH-like (also called TOFU for Trust On First Use) as opposed to putting big blood-red danger warnings up, it would work out better for everyone including the CAs.
But anyway, there are other solutions. They range from some variant of Direct Trust being TOFU or even using a Kerberos-like system to hand you a key, or what we do in ZRTP, or lots of other things. The bottom line is that if you want to send someone a message securely and you have never talked to them before, you have no other way to deal with it than public key systems. (Or you can re-define the problem. Suppose I want to send Glenn Greenwald a message and his Kerberos controller gives me an AES key, I merely have to trust the controller. If we say that trusting him is the same as trusting the controller, then yeah, sure, it works. That's a suitable redefinition in which the KDC is isomorphic to a CA. But if we allow public key, then I could get Mr. Greenwald's public key from an intermediary who is not necessarily an authority, or even self-publish keys. It's done with PGP all the time.)
I concur that the way that browsers and web servers us SSL is suboptimal. This doesn't mean that a solution is impossible, it only means we have a widely-distributed suboptimal system.
I have to disagree on that one, too. The servers that have problems indeed have problems. Servers do not implicitly have problems. The solutions are not evenly distributed, but they're not architectural. Heck, a Raspberry Pi has a hardware RNG.

@_date: 2013-09-05 21:33:31
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
What am I trying to say?
I'm being a bit of a smartass. I'm sorry, it's a character flaw, but it's one that amuses me. I'll be blunt, instead.
There is a lot of discussion here -- not really so much from you but in general --  that in my opinion is fighting the last war. Sometimes that last war is the crypto wars of the 1990s, but sometimes it's WWII. Yeah, yeah, if you don't remember history you'll repeat it, but we need to look through the windshield, not the rear view mirror.
My smartassedness was saying that by looking at the past, gawrsh, maybe we're seeing a time machine!
The present war is not the previous one. This one is not about crypto. It involves crypto, but it's not *about* it. The bright young things of 1975 who went to work for the NSA wrote theorems and got lifetime employment. The bright young things of 2010 write shellcode and are BAH contractors.
There are two major trends that are happening. One is that they're hitting the network, not the crypto. Look at Dave Aitel's career, not your mathematician friend. Aitel is one of the ones that got away, and what he talks about is what we're seeing that they are doing. If you have to listen to one of the old school mathematicians, listen to Shamir -- they go around crypto. (And actually, we need to look not at Aitel as he left in 2002, but the bright young thing who left last year, but I think I'm making my point.)
The other major trend is that outsourcing, contracting and other things ruined the social contract between them and the people who work there. (This reflects the other other problem which is that the social contract between them and us seems to be void.) Nonetheless, Aitel and others left and are leaving because no longer do they tap you on the shoulder in college and then there's the mutual backscratching of a lifelong career. Now a contractor knows that when the contract is over, they're out of a job. And when the contractor sees malfeasance that goes all the way up to the Commander-in-Chief, they look at what their employment agreement said, as well as the laws that apply to them.
If you're in that environment and you see malfeasance, you go to your superior and it's a felony not to. If your superior is part of the malfeasance, you go to your superior's superior. If it goes all the way up to the CiC, then some sharp, principled kid who is just a contract sysadmin just might put a lot of files on a laptop and decide they have to go to We The People, who are, after all, the ultimate superior.

@_date: 2013-09-05 21:42:29
@_author: Jon Callas 
@_subject: [Cryptography] Can you backdoor a symmetric cipher (was Re: 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
There is also a theorem somewhere (I am forgetting where) that says that if you have a block cipher with a back door, then it is also a public key cipher. The proof is easy to imagine -- whatever trap door lets you unravel the cipher is the secret key, and the block cipher proper is a PRF that covers the secret key. I remember the light bulb going on over my head when I saw it presented.
So if you have a backdoored symmetric cipher, you also have a public key algorithm that runs five orders of magnitude faster than any existing public key algorithm.
This suggests that such a thing does not exist. We have a devil of a time making public key systems that actually work. Look at all we've talked about with brittleness of the existing ones, and how none of the alternatives (Lattice, McElice, etc.) are really any better and most of those are really only useful in a post-quantum world. It doesn't prove it, but it suggests it.
The real question there is whether someone who had such a thing would want to be remembered by history as the inventor of the most significant PK system the world has ever seen, or a backdoored cipher.

@_date: 2013-09-06 17:11:02
@_author: Jon Callas 
@_subject: [Cryptography] Suite B after today's news 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I confess that I might not explain very well a controversy that I lie on a different side of -- I'm using CCM, myself. My above explanation is what GCM proponents have told me -- that if you are doing multiple high-speed streams and have hardware you can throw at it, then it's what you want. There is/was an additional OCB issue specifically that there is/was IP around it. Univ. of California has recently relaxed them, but it's still needlessly complex. I confess I tend to think of OCB as a footnote -- the cool thing we can't use -- only.
My decision tree is that I think in a perfect world, one would use OCB, but the IP nixes it. CCM was created specifically because it's not OCB, and EAX as an alternative to the alternative CCM. GCM is too easy to screw up and is slow in software (yes, there's galois multiply on Intel processors, but most of what I do is ARM). There's nothing wrong with EAX, but CCM is there and standardized in a number of places. Other people might end up with a different place for their own reasons. I don't think that any of them are bad, including the decision of using GCM and just making sure you do it right.

@_date: 2013-09-06 17:21:18
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
This is why I try to say "public key" and "symmetric key" whenever possible.

@_date: 2013-09-06 17:58:33
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I'd be happy to give a different answer, like -- almost certainly not.
We know as a mathematical theorem that a block cipher with a back door *is* a public-key system. It is a very, very, very valuable thing, and suggests other mathematical secrets about hitherto unknown ways to make fast, secure public key systems. To me, it's like getting a cheap supply of gold and then deciding you'll make bullets out of it instead of lead. To riff on that analogy, it feels like you're suggesting that they would shoot themselves in the foot because they know that the bullet fragments will hurt their opponent.
That's why I say almost certainly not. It suggests irrationality beyond my personal ken. It's something I classify colloquially as "too stupid to live."
My assumptions about the NSA are that they're smart, clever, and practical. Conjectures about their behavior that deviate from any of those axes ring false to the degree that they deviate from that.
My conjectures start with assuming they're at least as smart as me, and I start with "what would I do if I were them?" I think they're smart enough not to attack the strong points of the system, but the weak points. I think they're smart enough to prefer operating in stealth.
Yeah, yeah, sure, if with those resources I stumbled into a fundamental mathematical advantage, I'd use it. But I would use it to maximize my gain, not to be gratuitously sneaky.
The math we know about block ciphers suggests (not proves, suggests) that a back door in a cipher is impractical, because it would imply the holy grail of public key systems -- fast, secure, public key crypto. It suggests secure trapdoor functions that can be made out of very simple components.
If I found one, it would be great, but I'd devote my resources to places where I technology is on my side. Those include network security and software security, along with traffic analysis.
If I wanted to devote research resources, I'd be looking closely at language-theoretic security. I'd be paying close attention to the fantastic things that have come out of there.
The stuff that Bangert, Bratus, Shapiro, and Smith did on turning an MMU into a Turing machine is where I'd devote research, as well as their related work on "weird machines."
I apologize for repeating myself, but I'd fight the next war, not the last one.

@_date: 2013-09-06 19:02:47
@_author: Jon Callas 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
Public-key cryptography is less well-understood than symmetric-key cryptography. It is also tetchier than symmetric-key crypto, and if you pay attention to us talking about issues with nonces, counters, IVs, chaining modes, and all that, you see that saying that it's tetchier than that is a warning indeed.
The magic of public key crypto is that it gets rid of the key management problem -- if I'm going to communicate with you with symmetric crypto, how do I get the keys to you? The pain of it is that it replaces it with a new set of problems. Those problems include that the amazing power of public-key crypto tempts one to do things that may not be wise.

@_date: 2013-09-06 21:31:33
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Actually, I'm doing the opposite. I'm starting with a theorem and arguing informally from there. And yes, I know it's informal. I'm starting with the math and then arguing as an engineer -- it doesn't make sense do go the way you're suggesting when there are other paths that are known to be worth doing. Oh, and we know that the NSA is doing.
I don't see that it is nonsense to argue that if I were an evil genius, I'd go here for these reasons and then note that that is actually the direction that we know the NSA is going.
I'm saying that the math and the facts on the ground point thataway.

@_date: 2013-09-06 23:50:26
@_author: Jon Callas 
@_subject: [Cryptography] Why prefer symmetric crypto over public key 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Point taken.
Bruce made a quip, and I offered an explanation about why that quip might make sense. I have also, in debate with Jerry, opined that public-key cryptography is a powerful thing that can't be replaced with symmetric-key cryptography. That's something that I firmly believe. At its most fundamental, public-key crypto allows one to encrypt something to someone whom one does not have a prior security relationship with. That is powerful beyond words.
If you want to be an investigative reporter and want to say, "If you need to talk to me privately, use K" -- you can't do it with symmetric crypto; you have to use public-key. If you are a software developer and want to say say, "If you find a bug in my system and want to tell me, use K" -- you can't do it with symmetric crypto.
Heck, if you want to leave someone a voicemail securely you've never talked to, you need public key crypto.
That doesn't make Bruce's quip wrong, it just makes it part of the whole story.

@_date: 2013-09-07 01:13:00
@_author: Jon Callas 
@_subject: [Cryptography] XORing plaintext with ciphertext 
Hash: SHA1
It better not. That would be a break of amazing simplicity that transcends broken.

@_date: 2013-09-07 17:14:32
@_author: Jon Callas 
@_subject: [Cryptography] ElGamal, 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
For as long as PGP has done DSA, it protected the signature nonce by hashing it with the DSA private key. These days, we'd do an HMAC, most likely.
There's now an RFC 6979 on "Deterministic DSA" now, as well. Phil Z, David Kravitz, and I started on something equivalent and then stopped when we saw what Thomas Pornin was doing. It's good stuff.

@_date: 2013-09-08 10:35:57
@_author: Jon Callas 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
I need to correct some facts, especially since I'm seeing this continue to get repeated.
Phil was never charged, indicted, sued, or anything else. He was *investigated*. He was investigated for export violations, not for anything else. Being investigated is bad enough, but that's what happened. The government dropped the investigation in early 1996.
The government started the investigation because they were responding to a complaint from RSADSI that Phil and team violated export control. As Phill noted, there was the secondary issue of the dispute over the RSA patent license, but that was a separate issue. RSADSI filed the complaint with the government that started the investigation.

@_date: 2013-09-08 10:41:55
@_author: Jon Callas 
@_subject: [Cryptography] [cryptography] Random number generation 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
There's also another way -- that it's a constant PRNG.
For example, take a good crypto PRNG, seed it in manufacturing, and then in its life, it just outputs from that fixed state. That fixed state might be secret or known to outsiders, but either way, it's a cooked PRNG.
Sadly, there were (are?) some hardware PRNGs on TPMs that were precisely this.

@_date: 2014-04-01 06:09:07
@_author: Jon Callas 
@_subject: [Cryptography] Dark Mail Alliance specs? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Probably not. It was a political and policy statement, and one where people are now going back post-Snowden and pointing out that they were warned ages ago.
Do I expect them to mend their ways? Nope. Quite the opposite. But you don't get to say "I told you so" or "you were warned" if you don't tell them so or warn them.

@_date: 2014-04-18 13:01:35
@_author: Jon Callas 
@_subject: [Cryptography] It's all K&R's fault (was: bounded pointers in C) 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
And it is, too, and others have said the sorts of things I would say about what an abomination of a language C is. But so what? You *can* use it reasonably, especially with any of a number of supplemental libraries do things like give you reasonable strings, buffers, and so on. The people on the other side of the tale are also right.
However, this whole conversation is mostly irrelevant to the actual issue. Here is a case in point from my own present experience.
One of the subsystems I use is a network server written in Erlang, which is a language that would meet many people's gross requirements for a better language than C.
Nonetheless, this Erlang server ended up needing to be updated for Heartbleed, because this system does SSL by using OpenSSL. Meow.
In contrast, we have another server which is written in C and pretty grody in a lot of ways. But it uses a *different* SSL package and therefore had no issues with Heartbleed. Meow. Other subsystems we use use yet other SSL packages (for example, Secure Transport on iOS), which each have their own issues.
Yeah, sure, C is an abomination. But much of its abominableness can be mitigated with static analyzers etc. Beyond that, unless you rewrite the entire stack you're on, from the OS up, you're very likely still using C even when you're not using C.

@_date: 2014-04-19 12:54:52
@_author: Jon Callas 
@_subject: [Cryptography] It's all K&R's fault 
Hash: SHA1
I'm sorry Ian, I have to raise an eyebrow at what is essentially saying "kids today are no damned good." When I was a kid, back in those days, kids were no damned good. C and its ilk were for lazy people who didn't want to learn how to program the *machine* dammit.
I'll get off your lawn now.

@_date: 2014-04-19 13:09:20
@_author: Jon Callas 
@_subject: [Cryptography] bounded pointers in C 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Yeah, that's most often said by people who make crap tools. If you were only a good workman, you wouldn't complain.

@_date: 2014-04-19 14:23:10
@_author: Jon Callas 
@_subject: [Cryptography] Apple and OpenSSL 
Hash: SHA1
It's an amusing read, as I was the developer who gave the 2011 WWDC talk. It's not completely accurate. Some of the places where it says "API" it actually should be "ABI" but it's understandable how you'd confuse those, especially when transcribing from the spoken word.
The fuller story is that OS X, like many modern operating systems is rather fond of dynamically linked libraries. It shipped with OpenSSL dylibs through much of the history. However, in and around the 0.9.8 to 1.0 versions of OpenSSL, there were flurries of activity in OpenSSL and long stretches where it remained stable.
The problem is, as I said above, that OpenSSL does not have a stable ABI. That's Application Binary Interface, and if that's not familiar to you, it means a number of things like that if you add a field to a data structure, you have to add it to the end, not the middle. If you add it to the middle of the data structure then code that dynamically links to that updated library will look at the old offset for the field, not the new one.
Consequently, if you made an OpenSSL dylib and people expected it to work, uh, you know, with dynamic linking, their code will break. This causes dismay. Some people express their dismay through the use of lawyers, or at least the threat of them. That lead us to think that Something Must Be Done.
We talked to the OpenSSL people and noted that we really needed to be able to be able to make and ship dylibs, and asked if there was anything we could do to help. Also at this time, we were culling security libraries. An open question was whether we should keep an Apple-written SSL package, or just start using something open source, and the favorite on this was OpenSSL. The consensus as I felt it was drifting away from chucking Secure Transport because that year's flurry of SSL bugs affected everything *but* Secure Transport. There was still a lot of sentiment for stopping supporting an internal SSL toolkit and devote resources to an external one, and that was part of that discussion.
I wasn't part of the discussions between Apple and the OpenSSL team, but I know it didn't go well. OpenSSL rebuffed Apple and I gathered that the rebuff was actually insulting. It probably wasn't literally, "why don't you go back and make lickable buttons" but that would have given a similar result.
One thing I know that OpenSSL said was that the unstable ABI was a feature, not a bug, and that anyone who really cares about security should statically link OpenSSL. The message we gave at WWDC about static linking was the OpenSSL team's advice to us. We expected that to rile people up, and I made sure that I said that static linking was the OpenSSL team's suggested fix.
Nonetheless, we were left with no solution to the *real* problem, which was that our own customers were on our asses because the dylibs were breaking, and we're left having to tell them that the OpenSSL team told us that it was our fault for being so stupid as to dynamically link OpenSSL. Thus, OpenSSL is intrinsically not supportable in system that likes dylibs. We had to fix that underlying issue.
So what fix? While thinking and researching options, we stumbled across Marco Peereboom's hilarious essay on the subject, "OpenSSL is written by monkeys"  (which he's updated to say, "Hah! I told you so!" recently). The camps divided up possible fixes into several main categories, which I'll summarize as:
(1) Be careful what you ask for, you might get it.
(2) Take OpenSSL out behind the shed, shoot it, and leave its corpse for the crows.
(3) Take OpenSSL out, shoot it, and give it a decent burial.
(4) Take OpenSSL out, shoot it, give it a decent burial, and hire "mourners" to dance on its grave.
Option (4) was really to fork OpenSSL with a team we fund, and fix things. It was dismissed as unworkable and would get a lot of bad press. (If it was workable and got bad press, it might not have been dismissed.) Option (3) meant to scrub OpenSSL from the OS, and that was unworkable because there was still the implicit need to maintain the existing dylibs. The decision went to option (2). That's what you see now. OS X still ships OpenSSL 0.9.8y (or at least that's the version on my laptop), and the entire system is as it was in 2011. Option (1) was to keep shipping an up-to-date OpenSSL, but stop shipping dylibs. That was the one I was in favor of -- OpenSSL may be written by monkeys, but it's written by monkeys that you know. The argument against is that you either have to support it or not, and that puts you in a position where you still have to do a lot of work, but the work is different from what you do with other supported libraries. Thus, inevitably, you revisit the decision every time OpenSSL is updated.
I think that someday, someone's got to give OpenSSL the decent burial. That OS X ships with 2011 OpenSSL command line tools and dylibs is a bug. Option (1) is also still viable, but likely far less viable post-Heartbleed.
On top of this, there's something I really want to point out. Heartbleed isn't a crypto problem. It's a software engineering problem. It's a software engineering problem related to the difficulties of writing, maintaining, and supporting a *system*, where that system includes things like ABIs and dynamic linking and the implied support into the future. Apple's decision to chuck OpenSSL was also a software engineering decision, not a crypto decision.

@_date: 2014-04-21 18:07:14
@_author: Jon Callas 
@_subject: [Cryptography] Are dynamic libs compatible with security? was: 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
What is the point?
Let's look at this purely as a software engineering issue. Any time you compile and link a program, you can either statically link or dynamic linking.
Dynamic linking lets you make a change to a library without changing any of the apps. When you are fixing a flaw, this is good. If you statically link, then you must replace all apps that are consumers of the library when you make s fix to the library. It's true that static linking prevents the case where a flaw is introduced and propagated across all apps. However, one major reason we have dynamic linking in the first place is because dynamic libraries reduce the flaw-fixing problem to O(N) from O(N*M). (Another major reason is that dynamic libraries let you share memory so that M consumers of that library take up O(N) RAM as opposed to O(N*M) RAM.)
I think that there are certainly places where it makes sense to statically linking an app and assume the cost of updating and distributing as part of the cost of doing business. I also think that it's going a bit far to say that security as general case benefits from raising the cost of patching to O(N*M).

@_date: 2014-04-27 13:07:22
@_author: Jon Callas 
@_subject: [Cryptography] The Best 419 Message I've Seen 
Hash: SHA1
I would love to send this along, but it would get eaten by spam filters. I just got a 419 spam alleging to be from the FBI. You see, in their worldwide surveillance, they discovered where I overpaid some bill, and if I just fill out these details, they'll return my money.
I laughed so hard. It's really brightened up my day.

@_date: 2014-04-27 15:49:32
@_author: Jon Callas 
@_subject: [Cryptography] The Best 419 Message I've Seen 
Hash: SHA1
Because the clever story is that it's part of widespread government surveillance of the otherwise allegedly encrypted communications.
It's because surveillance is now part of some cultural meme adopted by the scammers.

@_date: 2014-08-04 10:56:37
@_author: Jon Callas 
@_subject: [Cryptography] ADMIN: Periodic reminder about top posting 
Hash: SHA256
In another anecdote, recently a person who works with me noticed that I that I trim out extraneous stuff, in this style. This person thinks it's great and would like lessons on how to do it.
Now, there are some mail readers that detect replicated sections in a thread and elide them for you. Mail.app on Mac OS X does it.

@_date: 2014-08-11 11:03:30
@_author: Jon Callas 
@_subject: [Cryptography] IETF discussion on new ECC curves. 
Hash: SHA256
We're already there, Ian.
If you want to do serious work in Russia (banking, government) you must do a number of GOST things. If you want to work in Korea, you need to do SEED. If you want to work in Japan, you don't *have* to do Camellia etc., you can do AES, it's just that they'll pick a product that has those over yours.

@_date: 2014-02-18 13:05:52
@_author: Jon Callas 
@_subject: [Cryptography] The ultimate random source 
Hash: SHA1
It works just as well, perhaps better, if you put a lens cap on the camera. The ritual with the smarties just makes you feel better.

@_date: 2014-01-02 13:30:11
@_author: Jon Callas 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Hash: SHA1
It's nice work on a technical level, but I think it fails at its goal, that is to be a piece of polemic -- even mischaracterizing the Ferguson/Shumow CRYPTO Rump Session talk (their last slide explicitly said they were not suggesting a back door). Lest you think I'm saying something nice about DUAL_EC_DRBG, I'll repeat what I've said before, "Only an idiot would use it." It's slow, has biases in its output that hashes and ciphers don't, and cannot be proved secure.
Let me rewind to the first public key based DRBG/PRNG -- Blum-Blum-Shub. (DRBG is the name NIST gave to what you and I would call a PRNG, it's Deterministic Random Bit Generator. If you think of a complete design of an RNG, you want at least three major sections -- "entropy" collection, entropy pool management, and conditioned output. A DRBG is the output stage.)
Blub-Blum-Shub uses an iterated RSA-like operation to generate random bits. It was developed in 1986 and is a brilliant piece of mathematics. It was one of the very, very few bits of early crypto to have a sound theoretical basis.
Many people who haven't thought it through have sung its praises over the years, mostly because they got seduced by the sound theoretic basis. Blum-Blum-Shub has two of the three flaws that DUAL_EC_DRBG has: it's slow, and you can't prove it secure.
I'm sure you thinking, 'What do you mean, "can't be proved secure? Didn't you just say that it had a sound theoretical basis?"' Yup, and the sound theoretical basis gives you the good mathematical pseudo-randomness of the output. The slow part is pretty obvious. The inobvious part is that it can't be proven secure.
As an iterated, RSA-like operation, the core of it is two prime numbers, p and q. The security resolves down to the secrecy of the two primes.
And this leaves you with the question of how you get the primes. Well, if you generate them at run time, then you push the construction of your RNG down to the turtle below you. Your random number generator requires random p and q and you get those by using some other random number generator, one presumes.
Alternatively, you could use a fixed p and q, and then you have the *exact* flaw as DUAL_EC_DRBG -- you have a fixed private key that can be used to jimmy the thing open.
Matt Green wrote a great blog post last week at , which you should read. I'll summarize a bit and say that Micali and Shnorr did their own public-key based PRNG which also has Step 1 being "generate large primes p and q" and they helpfully gave test P and Q. I'm not merely being ironic. As someone who implemented the AES-CTR DRBG, having test vectors is really, really nice. NIST's test vectors for that are really, really annoying and I'll complain at length to anyone who cares.
Anyway, Matt Green identifies the Micali-Shnorr generator as a precursor to DUAL_EC_DRBG in both design and having a fixed key that you use for whatever purporses, testing or compromise. I have a couple points:
(Point 1) Any public-key based PRNG is going to have the issue that either you have a fixed key, or you have to generate a key using some other secure means. This is why I use terms as strong as "can't be proven to be secure" despite having a mathematical "sound theoretical basis."
I think there's a huge security philosophy problem here -- security proofs that are mathematical can have underlying engineering assumptions that render them insecure to the point of being silly.
I think that people get blinded by this as well, and if there's a mathematical proof they're blinded by it, if not cowed by the math and stop prodding at the engineering and operational security.
Going back to Blum-Blum-Shub, look at the Wikipedia article on it at . There are some interesting statements there, like the first sentence of the security section:
The generator is not appropriate for use in simulations, only for cryptography, because it is very slow.
That makes me splutter. As an engineer, I'd argue that slow alone makes it not suitable for cryptography. I'm tempted to argue that for simulations, speed isn't an issue, but really, if you slow things down enough, it's not suitable for anything. I also have a long-standing twitch at *any* security discussion that brings in performance. Performance is not security, and many security sins have their root cause in a performance worry, usually an artificial one.
The remainder of the section reads:
However, there is a proof reducing its security to the computational
difficulty of the Quadratic residuosity problem. Since the only known
way to solve that problem requires factoring the modulus, the
difficulty of Integer factorization is generally regarded as
providing security. When the primes are chosen appropriately, and
O(log log M) lower-order bits of each xn are output, then in the limit
as M grows large, distinguishing the output bits from random should be
at least as difficult as factoring M.
If integer factorization is difficult (as is suspected) then B.B.S.
with large M should have an output free from any nonrandom patterns
that can be discovered with any reasonable amount of calculation.
Thus it appears to be as secure as other encryption technologies
tied to the factorization problem, such as RSA encryption.
This is interesting because nowhere do they address the central engineering issue -- that a fixed p,q is not secure yet a variable one requires another RNG to seed the RNG.
Also look at the section in the Handbook of Applied Cryptography on "Cryptographically secure pseudorandom bit generation":
We find an RSA-based generator there along with Micali-Shnorr and Blum-Blum-Shub and *all* of these have a recipe that starts unironically with (essentially):
1. Setup. Generate two RSA-like secret primes, p and q.
This is a blind spot that's been there forever -- two of the three flaws of DUAL_EC_DRBG have been staring us all in the face since 1986. Despite mathematical brilliance, the security of public-key based PRNGs have always been flawed with something that could be used as a back door.
(Point 2) History looks different when you look backwards than when you look forwards. Everyone has a tendency to act as if things were predetermined when we analyze the decisions of the past. We also assign intent when we have to explain a WTF. Yet the usual answer to "What were you thinking?" is "They weren't." To quote the great philosopher David Byrne, "And you might say to yourself, 'My God, what have I done?'"
The general flaws have been there forever, and in general we still don't see them. In specific, the documents on Micali-Shnorr and DUAL_EC_DRBG were up front about the flaw all along.
Would the NSA exploit such a flaw? Hell, yes. We keep seeing this from leaked documents, over and over. It's clear that they have taken the hacker philosophy that nothing is out of scope or out of bounds to heart as an operating principle. You can see it in the recent ANT toy catalog, as well as the BULLRUN statement that sent us all into a tizzy.
We have *assumed* that the BULLRUN statement that they're after damaging standards means that DUAL_EC_DRBG is backdoored. People have said it so loudly and so often that it's part of conventional wisdom now. Yet until BULLRUN, it was part of conventional wisdom that despite the speed problems, mathematics made public key PRNGs more secure.
I know that one of my personal blind spots is that I'm a contrarian. I'm also a cynic who believes that stupidity is one of the fundamental forces of the universe. So I find myself in the ironic position of defending a thing I never liked because yes, really, people *can* be that stupid. We all defer to authorities, are cowed by proofs, and lose our critical thinking skills when faced with a standard. I am reminded of Markoff Chaney from Illuminatus! as well as Poe's Purloined Letter.
If we want to look at this as a root-cause exercise, we can go back to Blum-Blum-Shub and see the kernel of the flaws and blindness of them. You can see that despite it being there from the start, we didn't *understand* it. You can see the progression through Micali-Shnorr through the ANSI X9 committee, and then on to NIST.
I'm left wondering if something can really be a backdoor if it was there all along, we just didn't grok it. Was the purloined letter hidden? I can't help but feel that calling it a back door is just too cheap and easy and convenient. It wasn't that we were collectively stupid and snookered ourselves, it was demons and those in league with them.
This leaves the question of what they *have* been doing with that $250M, which is a good question. I lean towards private standards like those used in telecom etc. In the public world, we're good at doing it to ourselves.

@_date: 2014-01-25 15:43:34
@_author: Jon Callas 
@_subject: [Cryptography] Does PGP use sign-then-encrypt or 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
What do you mean by "progress"?
Lots of people in the security world wake up one day to a surprise and convince themselves that it's a bug.
Let's call this the same thing we used to -- which is the question of whether a signature should be inside "the envelope" (meaning the encrypted part) or outside the envelope.
As Derek pointed out, the strict syntax of OpenPGP permits either. However, most (all?) software puts the signature inside the envelope. It's likely an option in GnuPG because they're good about implementing all legal syntactic possibilities.
The major reason for a signature inside the envelope is that if the signature is on the outside, it cryptographically states to a passive observer that Alice is talking to Bob. It makes anonymous remailers and other things harder to do or impossible. The reasons for putting the signature in the envelope is to reduce the threat of traffic analysis.
PEM put the signature outside the envelope and *only* permitted it outside the envelope. At the time, there were plenty of dark things said about this. Similarly to today and a number of protocols, PEM was looked at as tantamount to insecure by design, and if there was a drawback in anything PEM did, many people considered it an unmitigated, intentional flaw.
There's no single answer here. Either side has plusses and minuses. I think that overall, you want the signature on the inside of the envelope and that has the drawback that you can send a decrypted signed plaintext message to third parties. I view that as a relatively small drawback. There is a difference between a surveillance system and a betrayal. Security can't stop a betrayal.
But -- I do see the other side. I just disagree.

@_date: 2014-01-27 08:20:31
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
I can answer, but I have no idea what your question means. Sorry. No, it's not proprietary, but I have no clue what you mean by "real problem" or the apparent opposition to -- something.

@_date: 2014-01-27 08:52:51
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
Yes, thank you.
Among the things on Blackphone is Silent Circle's services. You can get the sources for them at .
The phone is completely unlocked and can be used on any appropriate carrier internationally.
There's no way we can solve this problem for the first device. Baseband security is a huge problem.
Absolutely! Frankly, one of the things I consider most important is building the phone so that the OS can be updated for a reasonable life of the hardware.

@_date: 2014-01-27 12:57:28
@_author: Jon Callas 
@_subject: [Cryptography] cheap sources of entropy 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I was more blunt -- leave the lens cap on, even. There is quantum noise in the sensor, and if you take a frame and hash it, it almost certainly has more true randomness than the width of your hash function.
If you are in doubt, take two pictures and hash them. Or whatever makes you comfy.

@_date: 2014-01-27 13:01:19
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
I would *never* say that it is safe.
Okay, okay, after the Drudge Report called us NSA-proof, I was given a chocolate bar in the shape of a cell phone, and said that *that* was NSA-proof blackness and comes in plain or milk. There was an entertaining twitterstorm following, which was lots of fun.

@_date: 2014-01-27 13:10:18
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Thank you, very much for the vote of confidence.
But still, I've never said "safe." I've said security-enhanced, and lots of things like that, which are all true, but I'd never say safe -- because I know about the dangers of compromised hardware.
Reporters have to make a living, too, and many of them have written hyperbolic headlines. Well, okay, usually, it's the *editor* who puts the hyperbolic headline on the well-written story.
My truest personal goal for Blackphone is read an Android hardening guide sometime in the future that will give a list of the things you should do to lock down your Android phone, and at the end it will say, "Or you could just buy a Blackphone." I want it to come out of the box the way that serious people like us on this list would want it.
It will also have a set of software and services that people like us would like to have, which is part of the hardening, in my opinion.
It would be very nice to achieve that goal with a V1.0 product, but it would be hubris to suggest that that's going to happen.
(Also -- should there be some ad, web thing, or other communications from Blackphone that says something like it's "safe," mail me off list. I'll get it fixed. That's just another form of software and software comes with bugs, especially in its early days.)

@_date: 2014-01-27 13:14:00
@_author: Jon Callas 
@_subject: [Cryptography] cheap sources of entropy 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Sure, whatever.
Let me get down to brass tacks. Apologies for the minor handwave to make my point:
Assume a frame F_i taken from the camera. If the attacker cannot guess the *exact* contents of that frame, even with filters, in-camera JPEG, etc. with an advantage over guessing flips of a fair coin, then it's got one bit of entropy.
So take 512 frames and hash them. Poof you're done.
Extrapolate from there to the case of a frame having N bits of entropy.

@_date: 2014-01-27 13:28:09
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
Hash: SHA1
You cut to *precisely* one of my points. Usability is all. We have to have things that people will use.
Crypto matters to this in the same way that concrete relates to building strong buildings. It's often a good idea, not the whole solution, and nothing to do with whether people want to live in it.

@_date: 2014-01-31 13:13:10
@_author: Jon Callas 
@_subject: [Cryptography] The crypto behind the blackphone 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
We *are* using some of the Guardian Project's software. Also software that we're building for Blackphone will be available for other people to use on their own ROMs. And heck, you can go to Github, get the Silent Circle apps and put them on your own device. We're finally to the point that we've QA'ed people who aren't us building them and using them. (And if you can't, it's a bug.)
Let me answer your question with a question.
What's the difference between going to a restaurant as opposed to going to the grocery store and buying a bunch of ingredients and making the same meal? There are groups devoted to making food the way the Child or Keller might. You can't have a meal by Child because she's gone, but you could make a Keller meal as well as Keller's people can. Why go to the restaurant?
Now to comment on that line of both our questions, we all have a set time in this existence and some people might like to write their own compilers so they can write their own software, just as some people grow their own food so they can make their own meals. But some people don't want to do that, and every single one of us trades off the things we want to do against things we're happy to pay other people to do. No offense taken. I may be a smartass, but I like tough questions. If/when they do, I'd love to see it. I don't have time to make an open, secure baseband, but want to include one. The world needs one. Maybe we can arrange some sort of trade.

@_date: 2014-06-14 14:09:07
@_author: Jon Callas 
@_subject: [Cryptography] ghash.io hits 50% of the Bitcoin compute power 
Hash: SHA1
It would do the same thing that every other piece of software that has to do complex things and error detection on hard-to-simulate, won't-happen-in-the-real-world conditions does: fail massively in some spectacular way. Usually the spectacular way includes *not* throwing an error. I find it hard to think that the only people who screw up chain-checking are people who deal with X.509. No one *ever* gets that right in their early software, and then when it's finally right, it suffers bitrot years later.

@_date: 2014-06-26 17:47:31
@_author: Jon Callas 
@_subject: [Cryptography] a question on consensus over algorithmic agility 
Hash: SHA1
Well, Ian, in my opinion it works like this:
It's completely obvious that the most correct thing to do is to have only one set of algorithms.
And then when you decide that it's a really good idea to replace one of the algorithms, then it's completely obvious that you need another algorithm and agility is called for.
There's a very easy way to get around an apparent need for agility, and it's a simple process: Select an algorithm set that is going to give you adequate security throughout the total life of your protocol and then forbid people from using it for things you didn't anticipate or for longer than the life of your protocol.

@_date: 2014-03-11 15:02:51
@_author: Jon Callas 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Hash: SHA1
Yes, but the world is not Intel, it's ARM. Meow.
The world would be better served by CCM, which can be implemented well even in Javascript than more GCM, which is slow in most places, and is brittle.

@_date: 2014-03-12 08:56:45
@_author: Jon Callas 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Hash: SHA1
No, because there's no one thing.
If you're on a modern Intel processor, GCM is fast. If you're not, it isn't, and how fast it is depends many things in your execution system. If you're doing things in interpreted languages, it goes to hell quickly.
For me, I have a larger concern about GCM brittleness. It's the security properties, not the speed. There are many things that can go wrong and it's hard to get right. This is the essence of Niels Ferguson's commentary in .
CCM, in contrast is pretty straightforward. It just requires AES. It lacks the efficiency of OCB, but is *understandable*, which is a really nice property. For standards purposes, it's RFC 3610, and there are similar options for it in TLS as for GCM. It's not great, but it's free of intellectual property, and is understandable and reliable. You know what's in the box. (Yes, OCB is an even better choice, and eventually that might be the real answer. OCB's problem is one that can be fixed by ink on paper -- a better license, or the patent expiring.)
I'm a bit exasperated because yet again, a discussion of security gets turned into a discussion of performance. There are many places where GCM is a really fine mode, but there are many where it is not, and jumping from RC4 to GCM is jumping from a frying pan to another frying pan.

@_date: 2014-03-12 17:32:44
@_author: Jon Callas 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Hash: SHA1
I'm doing it only in software.
Yup. I ended up settling on it two years ago because it was in SJCL. Well, not totally for that, but it was there and that settled the debate.
This is about to inspire a rant. Yeah, performance is great, but I see a train wreck coming five years from now because someone misuses GCM.

@_date: 2014-03-19 16:13:48
@_author: Jon Callas 
@_subject: [Cryptography] Italians invent SHA-7 in 2011 
Hash: SHA1
I love the blur between has [sic] functions and ciphers. I rolled my eyes at the speed given in milliseconds, not cycles per byte.
I was struck by the number 3,628,800 for the increase in collision resistance. That happens to be 2^8 * 3^4 * 5^2 * 7, which is a really cool pattern.

@_date: 2014-03-23 21:56:59
@_author: Jon Callas 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
Hash: SHA1
There was a very important reason that Blake2 was not picked. It wasn't an entry. Blake was an entry, and Blake2 is a successor to Blake.

@_date: 2014-03-25 15:48:15
@_author: Jon Callas 
@_subject: [Cryptography] Dark Mail Alliance specs? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Funny you should ask, as I'm writing up some slides on it even as I type.
I'm giving an architecture overview at the Cloud Security Alliance meeting in Amsterdam next week -- April 1-2, at the RAI.

@_date: 2014-03-25 15:59:07
@_author: Jon Callas 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
What Ian said.
Most of the cryptographers I deal with are not engineers. They haven't built systems, and they spend all their time worrying about things that don't matter. They go around inventing banana attacks (Jean-Phillipe Aumasson deserves much credit for inventing the term). Remember, they get rewarded for getting papers published, so the incentives are all to write papers.
Remember, crypto is almost never broken. If you're smart enough and humble enough, you can build a decent system with the standard components. Yeah. Well, yeah. I wouldn't. You're most likely making the strongest link of your chain stronger, and convincing yourself you've improved a weakness.
It's all too easy to combine ciphers in ways that gives you the security of the weakest one. But see below.
Bingo. Take a halfway-decent content/message system and run it over TLS. Run your TLS over IPsec. Or better yet, run that through Tor (which is solving a different problem and that's why you use it).

@_date: 2014-03-27 07:01:02
@_author: Jon Callas 
@_subject: [Cryptography] Dark Mail Alliance specs? 
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
Yeah, why try to make the world better, when it's so much fun to just laugh?
Seriously -- the CSA are a good group. When "cloud" became a big buzzword, it was started by a bunch of people who said that lots of other technologies were designed with no respect to security, and then people started to jam the security in later. The idea was that *this* time, we ought to try to get the security in the first time.
Obviously, this is hard, because there's so few of us and so many of them. However, the CSA has been out there in promoting aggressive policies. I wrote the original encryption guidelines back in '09, and I put in things like a statement that if data in the cloud is not end-to-end encrypted, then it should be considered lost (which is a significant thing, as that triggers breach disclosure notifications). I was impressed that I got backing on that.
I stopped my involvement after I was in PGP as I was in The Village, but in the post-Snowden world, I've been invited back to beat the drum for real security. In next week's meeting , there's going to be a lot of good work. Yes, it's slow moving, but it's going in the right direction.

@_date: 2014-11-07 13:11:09
@_author: Jon Callas 
@_subject: [Cryptography] Wind River Security Features and Cryptography 
Hash: SHA256
I've been involved in producing and exporting crypto for close to twenty years. I've been involved in doing whatever is needed, from filling out paperwork to printing source code on paper.
Since 1997, there's been a steady liberalization of export control in the US. One might argue that it's not monotonically decreasing, but it's decreasing continually. Right now, it's easier to get an export license than it is to do your taxes. Similarly to doing one's taxes, you can do it yourself, but if it's at all complex there are pros who can do it for reasonable prices.
Collin Anderson's note was right on, and I almost feel sheepish for adding to it.
In many cases, there are outright exemptions for export control. There's an exemption for "mass market" crypto. There's an exemption for open source source code releases. I apologize for forgetting who said this on what list, but in this discussion, someone said that this was a reason for an anonymous source code release, and ironically a source code release is on the exemption list, so you don't need it to be anonymous if you're releasing source!
This means that in many cases, there's a "why bother" aura around export licenses. Let's say you do Javascript crypto (I pick this because the code is implicitly source) that you put on a public web site (which makes it mass market). You have two exemptions here. It can be argued that it only annoys everyone to bother with the paperwork. One can even argue that it would be a *good* thing for everyone who uses SSL on their web site to apply for export licenses because that chaffs the BIS. One can argue that BIS is chaffing themselves.
So that leads us to what we've all been saying -- we have no idea what is really going on here. We're all speculating. I think that Goodwin Proctor's speculation:
   This suggests a fundamental change in BIS?s treatment of violations    of the encryption regulations.
Is both speculation and something to strongly disagree with. Really? What leads you to this? Given that they're a law firm that does export filings, this speculation is a speculation that drives more business to them. This is like someone bringing up a case of food poisoning in hamburgers and saying this why you should pay me to inspect your kitchen. Consider the source. (And by the way, if someone needs to know a good export lawyer, write me off-list. I am happy to share, but feel it inappropriate to plug anyone here.)
There are other reasonable speculations, as well. Mine is that Wind River has been really sloppy about export control filings on many things, there's a lot of behind-the-scenes drama, and BIS decided to slam them for this because they admitted they're guilty. Effectively, this is the get-Capone-for-taxes story.
The huge irony in crypto today is that the more one is interested in the sort of crypto that is the general milieu of this list, the less one needs to worry about export licenses. Publishing your source gets you an exemption. The export definition of "mass market" means "generally available" not quantities or anything like that. If you have closed-source, secret algorithms, sold only to LEA/Military, you don't get any of the exemptions that we all do. Whatever's going on with Wind River, they are doing something that they admit they needed a license for that they didn't get.
So -- something's going on here and we don't know what it is. But it's almost certainly not what's being speculated about here. Yeah, yeah, if you're doing crypto professionally (by which I mean for pay), get an export license. Really, it's easier than doing your taxes, reasonably inexpensive, and if you ever end up in a different export battle -- for example, you didn't realize that those FPGAs or GPS chips might be considered "dual use" -- you won't get smacked with the crypto stick because they decided you needed to get smacked and that was the closest stick.

@_date: 2015-02-13 09:25:14
@_author: Jon Callas 
@_subject: [Cryptography] Crypto Trick Makes Software Nearly Impossible to 
Hash: SHA256
It also reminds me of a thing Intel did in the late '90s, as well. There was also a published paper on it in, I believe one of the early Information Hiding workshops.

@_date: 2015-02-20 14:36:55
@_author: Jon Callas 
@_subject: [Cryptography] trojans in the firmware 
Hash: SHA256
NAND memory runs faster when the hamming weight of the data is approximately even between zeroes and ones. You can speed up NAND flash by running the data through a suitable whitening function.
AES is a great whitening function. If you then go to the extra effort to do key management, you have security. It's a simple matter of architecture and programming. :)

@_date: 2015-01-05 12:25:55
@_author: Jon Callas 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
Hash: SHA256
Here's a quick bit of commentary.
The Lorentz machine is surprisingly good. Architecturally, it uses the same stream cipher structure we use to this day -- you set up a PRNG, XOR it on to the plain text, and poof, you're done. Its flaws are the same ones you see today -- the PRNG isn't quite good enough. The final nails in the coffin of RC4 that we saw a year ago are surprisingly similar to the thing that killed Lorentz. Lorentz is so good that they had to invent a computer to break it.
Engima is also much better than people give it credit for. It has the flaw that you can't encrypt a letter to itself, and this gives the cryptanalyst a huge jump on the game. From modern eyes, that's a fatal flaw and we can move on now because it's toast. Note that the Bombes were not full-fledged computers. But it's still a lot better than it's typically given credit for. Variants of the Enigma were used up until the early-to-mid-1990s, with the eleven rotor NEMA. It's also worth noting that the whole CRYPTO AG scandal was over these variant Enigmas.
The real problems were in key management and use protocols.
There was no good way to use a key per message in those days, so there was typically a key-of-the-day that all messages of that day used. That's the real downfall of both systems, because you now get a lot of places for there to be known plaintext. Known plaintext with repeated messages in stream ciphers is bad because it allows you to propagate what you know in one message to another. Since all messages were broadcast-encrypted (permit me that term) as opposed to point-to-point-encrypted (I'm trying not to say end-to-end as that's a modern term with context), it's the *system* that falls apart before the crypto does.
The Enigma protocols had the start of a message start with a three-character marker that's repeated twice ("ABC" || "ABC") and oh, by the way, that's the encryption key. With modern eyes, this moves you from "surprisingly good" to "facepalm" in one easy step. Since Enigma was used for trivial traffic, things like weather reports start being sources of known plaintext (they called them "cribs" in those days) as well as ways to collate and file all the messages encrypted with a certain key. When you throw the fact that a letter can't be encrypted onto itself with all of that, you get -- the Bombe, and the whole Enigma cryptanalysis program.
- From modern eyes, both machines have keyspace that's way too small. That's why NEMA went up to eleven rotors. One might speculate that in those days they kept the Enigma design because it was easier for the likes of NSA and GCHQ to cryptanalyze it as opposed to the essentially-modern design of Lorentz, which could be improved by a better PRNG. It's a pretty speculation, but it's just a speculation. I have my opinions, but they're just opinions.
It's really hard to get a stream cipher that's hard to crack with physical devices as opposed to bits. I'd say that's the biggest thing to answer your question with. This is the computer age, and let's use the fact that a computer is a machine that doesn't have physical constraints like you'd get with rotors or whatever.
Let's go back and consider RC4 for a moment, which is one of my favorite ciphers in the world. Despite it being broken, it is one of the prettiest, most elegant ciphers ever done.
You can consider RC4 to be a single self-modifying rotor. Imagine if you will, a rotor that has in each position a Scrabble tile. You do a little math based on the tile that are the contents of two slots, swap two tiles, and then output a result. Then you advance the rotor. Not only is it an algorithmic beauty, but as an intellectual bridge between the age of machine crypto into the age of computer crypto it's just drop-dead gorgeous. It would be really, really difficult to make a physical version of RC4, but you could. As software, though, it's a paragraph. Even the new, improved RC4 (I'm forgetting its name) is barely more complex.
The biggest problem with any rotor design is that you only get a character per click of the rotor. Or perhaps I should restate that and say that the "chunk" you encrypt is by necessity small. It is much harder to make a rotor with 100 clicks as opposed to 30. Modern ciphers get much of their strength from encrypting blocks -- to the point that these days you're seeing stream ciphers being better when constructed from a block cipher in counter mode. Block ciphers are software machines, not physical ones.
If you go further and look at a tweakable block cipher, you get even better stuff. I'm one of the co-authors of Threefish, and let's look at that for a moment. It has a large block size -- 512 or 1024 bits (I'm ignoring the 256 bit one) -- and runs at twice the speed of AES *because* it has a larger block size. In fact, the 1024 bit variant runs slower on an Intel processor than 512 only because the processor doesn't have enough registers to hold the state -- and even then, it's only like 10% slower.
The tweak lets you get over a lot of the problems with nonces, counters, IVs, etc. A tweak is a formalization of those ideas. The large block size means that you get less known plaintext. Look at an XML file, and its header is larger than the block size of AES. You start out with the same *issue* as with Enigma, albeit with a much better system. I call it an issue intentionally. I don't imagine it being a problem, but if you can fix it, it's worth fixing.
Large block ciphers are a good thing, for these old reasons. But anyway, summing up, those cryptosystems fell down more because they were systems rather than ciphers. Enigma+DH may not have been seriously broken. Lorentz had a PRNG problem and likely would have fallen anyway. An Enigma break with DH supporting it would have been different.

@_date: 2015-01-08 11:13:42
@_author: Jon Callas 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
Hash: SHA256
Yes. Dear me, it's the blinking *Military*! Admittedly it was the Allied forces in that war that gave us the term "SNAFU," as well as the tired joke about "military intelligence" but that's true everywhere. I offer as an example, this wonderful book:
"The Man Who Broke Napoleon's Codes" by Mark Urban. (Link not given because you can do that as well as I.)
You'll see there all the same human failures being made, but with code books etc. in the Napoleonic Wars.
I also offer up another classic, also a WWII one, but this book along with the above have been instrumental in my educcation:
"Between Silk and Cyanide: A Codemaker's War, 1941-1945" by Leo Marks.
Heck, while I'm at it, one more:
"The Story of Magic, Memoirs of an American Cryptologic Pioneer" by Frank Rowlett.
Sadly, that latter one is out of print and copies I found are spendy -- US$90 or more. I would argue, though that if you really want to understand capabilities, it's important.
The reason is that PURPLE -- the Japanese (handwave, handwave) Enigma-equivalent -- was broken by people who not only didn't speak the language but didn't have the character sets in common. Reading about that is really important, especially Post-Snowden.
It's also interesting that the Japanese OPSEC was so good that there are no surviving intact PURPLE machines, only the PURPLE-equivalents built by the cryptanalysts. They seem to have been seduced by belief in good OPSEC (which is the opposite of what usually happens) and continued using PURPLE even after the Germans told them it was broken, and even used it for a time after the end of the war (!). For what it's worth, the Russians also broke PURPLE.
It's also worth looking at because of the value of so-called "third-party intelligence" (in this case, the Allies got information about German things because they Japanese talked about them, thinking they were secure).
As Holmes said, "There is nothing more deceptive than an obvious fact."
Misuse-resistant crypto is *the* frontier. There are some nice things being done in algorithms themselves, but I think they need to be backed up with software design in many levels of the system stack. They crypto has to survive a programmer who read a crypto book. They're the worse ones of all. They've been deceived by all the new facts they learned most of all.

@_date: 2015-01-08 15:16:12
@_author: Jon Callas 
@_subject: [Cryptography] Imitation Game: Can Enigma/Tunney be Fixed? 
Hash: SHA256
That's about 45kg too heavy.
But in any event, you'd be scrod. Go read "The man who broke Napolean's codes" and see how one lone guy did it against Napoleon a century and a half before that.
ECB is Electronic *Code* *Book*. It doesn't matter how good the algorithm that produced the code book is, you just build the code book. You start doing statistics on the ciphertexts and look for duplicates. You make the guess of which messages are weather reports, and so on, just like they did against the stream cipher. Then you start piecing it together.
Also go look at the Venona history, as well as the fact that the three-byte "IV" in WEP led to a 24-bit attack against RC4 no matter what the key size was.
It's probably easier to break than Enigma.
On the other hand, at 45kg, it's so heavy that you wouldn't have as many machines and thus less traffic and less chance at playing statistics. Thus, ironically, the heavier weight would have been a problem for the cryptanalyst.
On the other, other hand, that would have also meant more plaintext traffic, too, and that's much easier for the cryptanalyst. Most people can break XOR-with-zero with pad and paper.

@_date: 2015-06-13 00:58:07
@_author: Jon Callas 
@_subject: [Cryptography] Anchor links in list digest 
Hash: SHA256
Sure! Submit your patches to the Mailman project and then everyone will get them.
I apologize for the snark in that, but I'm also serious. This list, along with others, uses an open source mailing list system, Mailman. Your request is reasonable -- heck, I think it's a great idea, but you're asking for features in Mailman.

@_date: 2015-11-20 11:12:33
@_author: Jon Callas 
@_subject: [Cryptography] ratcheting DH strengths over time 
Hash: SHA256
I don't really mean to point this rant at the person who wrote the paragraph below, it's really aimed all over the place, and even barely at this discussion. So I filed the name off.
Six months sooner than what?
In 2004, when Wang released her attacks, there was a big to-do because NIST had said that SHA-1 should be retired at the end of 2010. The worry was that SHA-1 would get really broken (like MD5), SHA2 was shaky, and we'd have to do a lot of scrambling before 2010.
You see, 2010 is the NIST recommendation to retire *all* 80-bit crypto. That means 160-bit hashes, 1024-bit RSA/DSA/DH, and other things like Skipjack.
As it turned out, SHA-1 held on 'til 2010, and lots of the people who have to follow NIST guidelines whined that six years wasn't long enough. Well, NIST held their nose and said, "Okay, kids, you can still use 80-bit crypto until the end of 2013, but we mean it really." That went by like a Douglas Adams quote: "I love deadlines. I love the whooshing sound they make as they pass by."
Here it is, the end of 2015 and we're still talking about this stuff. There's an real-ish attack on SHA-1 now (free-start collisions; as real as the Dobberton attack was on MD5 back in '97) and that's what's going to derail another proposed extension 'til 2017. I hope. WHAT THE HELL IS WRONG WITH YOU PEOPLE? NIST ***TOLD*** ***YOU*** TO GET OFF THIS STUFF A DOZEN YEARS AGO! THIS WAS BACK IN THE POST-9/11 DAYS WHEN NSA WAS PLAYING THEM FOR FOOLS!
Thank you, I feel much better now. Come on, Logjam and all that looks pretty clear with 20/20 hindsight, but with 20/20 hindsight, I can also see them snickering because they told us and we didn't listen.
It matters far less whether you use Integer DH or RSA or ECC as long as you use one that's big enough. 2K or 3K is fine, but obviously 3K is better. Dan and Tanja did a really shiny EC curve for me, 41417 that not only has Spinal Tap level of security (over 200 bits), but there are NEON implementations that make it faster than P-160.
What does it actually take to stop talking and do something? Come on, if we can't move off of 80 bit crypto for a dozen years after a warning from NIST we're going to do a ratchet? Really? I mean REALLY?

@_date: 2015-10-07 15:21:23
@_author: Jon Callas 
@_subject: [Cryptography] [openpgp] OpenPGP SEIP downgrade attack 
Hash: SHA256
So does CCM, and CCM has no intellectual property issues, nor GCM's brittleness.

@_date: 2015-09-01 15:13:03
@_author: Jon Callas 
@_subject: [Cryptography] mode of operation for file encryption 
Hash: SHA256
In my opinion, you should just go with XTS mode, which because you're doing to be doing disk blocks is going to fall out as XEX. It isn't perfect, but nothing is. It's fast, reasonably parallelizaeble, and usually good enough. I don't know what you mean by the GCM operation, myself. You can find plenty of people grumbling about it, but it's in my opinion the best thing overall.
If you're not fond of XTS, then I'd say go with EME. EME is cryptographically the best thing you could do, but it requires two encryptions. Modern AES-NI hardware makes this not horrible. You can still probably keep up with even a fast SSD with any reasonable AES-NI CPU. We were going to use EME at PGP Corp for our WDE, but that fell off the shelf for lots of reasons (including the advent of SSDs and thus speed).
EME is patented; at PGP we paid a license fee, but a few years later the state of California just opened up the patent.
The notes on Wikipedia: are pretty good. They're not up to date on the EME patent status.
There's another option that I will mention. That is to use a real, honest-to-god tweak able, wide-block cipher. My favorite is Threefish. If you use Threefish-512, you get 64 bytes per encryption, and (duh) 128 bytes with Threefish-1024. Threefish runs in software at over twice the speed of AES in software, but all in all, AES-NI hardware makes it really attractive to well, just use AES-XTS.

@_date: 2015-09-05 22:43:14
@_author: Jon Callas 
@_subject: [Cryptography] NSA looking for quantum-computing resistant 
Hash: SHA256
Another option, which I believe when Im grumpy is that after years of pushing us to ECC over RSA, theyve decided that the NIST curves have problems. Maybe the math crowd fessed up to the IA people about the break theyve been sitting on. Maybe some other issue.
And so at this point, theyre saying, "Look, over there, its Halleys Comet!" and getting us distracted from the real point, which is that maybe RSA 3K is okay, or get a new curve.
But yes, we have no idea. But I have a raised eyebrow.

@_date: 2016-08-22 17:06:21
@_author: Jon Callas 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
Hash: SHA256
They don't need a cryptographically secure PRNG. They do need a number of attributes that are easier obtained in a cryptographic PRNG than a lot of others.
My first advice is not to say "cryptographically secure" because that's going to frighten them off and set up their mental shields that you then have to knock down or go around. Then talk about goals that they pretty obviously want:
(1) They want a basic level of non-hackability. For example, it would be bad if knowing my cards I could then determine everyone else's cards in that hand. It would also be bad if knowing all 52 cards in hand N, you could then predict future hands.
(2) You want the hands to be evenly distributed at some level. Mathematically, I'm saying that you want the output to be pseudorandom, but that might be too much of a rat hole.
A linear-congruential generator fails both of those, badly, as others have stated.
LCG generators tend to generate in pairs. By that I mean that if you plot on a graph a bunch of (X,Y) pairs as points, you'll see a dark line go up the diagonal. This is bad because it means that you're creating skew in the hands. There will be clusters of cards that go together.
LCG generators can be hacked by knowing part of the output stream. It is *completely* reasonable that someone can see part of a hand and be able to tell the whole hand. It isn't even hard, go search "break linear congruential generator" and you'll find handy references complete with code to do it for you.
Moreover, as others have stated, 48 bits isn't enough. Not only is it a lot smaller than all the possible hands (and yes, that might not be strictly a requirement, but it sure would be nice to have), but it would be possible to precompute all possible hands and then just use that information to look up the hand based on a few convenient parameters like suit count and high card(s). I did a back-of-the-envelope calculation on constructing that, and it's less than a nice sports car today, in 2016. One of my parameters was that an 8TB drive costs costs $250. You can make it cheaper by using 4TB drives, or cloud storage. But more importantly, the cost of doing this is a lot less five years from now than it is today. The major point is that it's completely affordable for a cheating team to do it with each member contributing $10K-$25K. Worse, this would be a *great* Ph.D. thesis topic.
I feel I must also state that 5DEECE66D is not a large prime number, it is a only 32 bits in size which makes it not large, and it's not even prime. (Bigprimes.net can factor it in javascript.) Yeah, it's a nit and irrelevant.
The easy way to fix this is just to use /dev/random or equivalent on whatever computer they have.
If they want:
(a) A basic level of transparency. The ability to show the community your card dealing system isn't favoring anyone. (b) A basic level of reproducibility. The ability to be able to go back and say that yes, this hand was generated the right way and not tampered with. then /dev/random doesn't meet those goals because it will give non-reproducible results. I think some level of basic audit is a good idea, but I can accept that "we're the ACBL, you just gotta trust us" might be the way they roll. But if they do want those properties, there are easy fixes. My sketch is: * Take all the seeds they normally do: the hand record set number, date and time, and throw in something like a quote or witticism. * Hash them all with SHA-256 to get your first random number, which I will call R_0.
* Then for each R_{n+1} successor, hash N concatenated with all the seeds, and R_n.
I already gilded the basic lily of R_{n+1} = hash(R_n), and we could have a huge discussion of more ways to gild it.
Bottom line -- just toss out what they're doing and use /dev/random or equivalent, and you're good. The only problem you have is the auditability issue which they don't have now. They're already saying that a trusted team is fine. But as things stand, it's possible for a team to cheat by hacking the dealing system, either by hacking the generation system or just making a rainbow table of all possible hands.

@_date: 2016-08-22 18:17:31
@_author: Jon Callas 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
Hash: SHA256
Christian Huitema pointed out to me that 0x5DEECE66D is actually 36 bits not 32. I used  to convert it to 25214903917, and  reported it to be 7 * 3602129131 (in both hex and decimal) which I verified with bc. Bigprimes, however, says that 25214903951 (0x5DEECE68F) is prime.
I apologize for the error.

@_date: 2016-08-23 15:24:32
@_author: Jon Callas 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
Hash: SHA256
Yeah, I was irritated at the tone in which they said that they used the "large prime" because to my intuition, I might consider a prime that you'd use in secure-ish integer RSA/DH to be the bare minimum of "large" -- at least 2-3Kb. If I can paste it into a web page to check primality, it isn't large.
And well, these days an LCG just isn't appropriate for anything, because of how many better solutions there are.
The first suggestion should be /dev/random. Why not use it? It's within epsilon of a true random number generator for many epsilon. It's also darned easy to use. The major reason to go elsewhere is repeatability / audit which can be argued on either side. The argument that asks, "If we used physical decks of cards, you'd have to just trust us that we shuffled them enough, so why does this bug you?" is pretty good.
Past that, there are three basic paradigms. A cipher in counter mode (or equivalent), an iterated hash function, or something like the Mersenne Twister are all great. The choice of suitable PRP/PRFs is wide. I go for the iterated hash function because it's the most easy to set up and use. You can throw anything you want into the seed. Have your seed be some catchphrase like "The reign in Spain is plainly offensive to the Catalunyans" and you're good. If you don't think you're good, throw in the data and time to a hundredth of a second (like the ACBL was doing). Sure. No problem. Heck, pull 256 bits off of /dev/random, print them in hexidecimal, and publish the *whole* thing as the verifiable starting seed. Sure. Have at it. It's robust enough that you can do post-processing like toss all the bytes that aren't in [1->52] if that floats your boat. You can also pick your function accordingly. If you don't like SHA-256, use SHA-512 (which is *faster* than SHA-256 on a 64bit machine), don't like those, use Skein-1024, which is even faster than SHA-512. If you don't like that, well, there's Mersenne Twister.

@_date: 2016-08-25 15:33:15
@_author: Jon Callas 
@_subject: [Cryptography] Real-world crypto/PRNG problem:  Bridge 
Hash: SHA256
There has been a lot of research on this over the years. Myself, I did a model of shuffling for a statistics class many years ago. So many that it was written in Fortran. I modeled in it things like how many cards you'd drop in each flick of the shuffle statistically.
My numbers, which jibe with what I've read from other people doing the same is a minimum of five, and preferably eight. By the time you get to eight shuffles, it's hard to see badness any more. Since each hand is the set of cards that are all N mod 4, a card moving from the top (position 0) to position 3 is equivalent to it moving all the way to the bottom (position 51).

@_date: 2016-08-30 11:36:56
@_author: Jon Callas 
@_subject: [Cryptography] Strength of 3DES? 
Hash: SHA256
Many statements about the strength or weaknesses of crypto need little asterisk footnotes on the bald statement, because there are other assumptions.
When you cite Stefan, though, remember that his attack needs 2^56 memory. It's far more important to note that all 64-bit block ciphers are threatened with 2^32 memory. Long before you get enough memory to lower the key grinding, you can pry the thing apart with birthday attacks, key be damned. It doesn't matter how big the key is, there's an underlying weakness in the data representation.

@_date: 2016-02-18 13:00:51
@_author: Jon Callas 
@_subject: [Cryptography] XOR linked list & crypto 
Hash: SHA256
No, it isn't security through obscurity.
Security through security is typically a shorthand for a claim that something is secure because the design of the system is unknown. The strong antithesis of security through obscurity is Kerckhoff's principle that the secrecy of a system should be minimized. In cryptography, we typically say that we want only the key to be secret.
This is just an obscure implementation, that isn't supported by dev tools, etc. If it had become fashionable, it might be normal to us. There's no reason why tools couldn't support it, or hardware couldn't support it or whatever.
There are a lot of reasons why we do lists using the naive implementation of having the "next" and "previous" slots in the object to be the absolute pointer of the elements in the list. For one thing, you can just go look at the memory and see where it is.
But there are plenty of places where lists were done differently, as well.
Assume a naive doubly-linked list where a data structure by convention has as its first two slots the next and previous pointer. Note that a solitary element really ought to have its next and previous pointer being the address of the element itself. We can define an operation INSQUE and REMQUE that insert and remove an element from the queue. Very simple, and very powerful.
But let's suppose you want to do this in shared memory -- where two processors share some but not all memory. Modern multi-CPU systems share all memory, and they number it the same. In our system, let's suppose we want to have a co-processor that does crypto, and the start of this shared section may not have the same base address.
You can easily modify the previous system to use the offset rather than the pointer.
So foo.next is not &bar, but it's (&foo - &bar). It's still pretty easy to work with because  instead of saying
next = foo.next
you just say
next = foo[foo.next]
(Or something equivalent. Strictly speaking in the pseudo-C I just wrote, I've implicitly assumed that all the pointers are (unsigned char *) and I really should have defined it to be &bar - &foo for the offset, or used "foo[-foo.next]" in my fetch function. I did all of that for naive readability)
And foo.previous gets handled similarly.
In my shared memory, I also want to have an interlock so that I don't get race conditions where both processors munge the things at the same time. I also really need to have separate ways to handle inserting and removing at the head and tail of the queue, as well. So I end up having INSQHI/REMQHI and INSQTI/REMQTI for handling these self-relative queues. And note that conveniently, the lone element has zeros in its default slots.
But -- there's no reason I have to use subtraction. I could use XOR. I can store &foo ^ &next for the next pointer and &foo ^ &prev pointer. I can even observe that if I store only one thing -- &next ^ &prev, then as long as I know &foo, I can collapse the two pointers into one.
It's just confusing, and long ago, we decided that understanding is worth spending the extra slot in the data structure. It's also slower to do, because you're always doing a lot of math whenever you play with an element.
But you could do the whole thing in hardware -- make instructions for INSQUE and REMQUE as well as the interlocked ones. People even did that long ago. But they didn't optimize it into the XOR because -- well, it's not worth it we all agreed.

@_date: 2016-02-23 23:25:14
@_author: Jon Callas 
@_subject: [Cryptography] RIP Claude Shannon 
Hash: SHA256
British SOE used one time pads in WWII, printed on silk so they could be easily hidden and destroyed. Read Leo Marks's "Between Silk and Cyanide" which describes this and may be the best book ever on crypto UX. You may disagree, but I think so.
One time pads, of course have at least the operational issue that you have to have as much pad material as message material and you have to hard-stall if you run out. As a mental exercise, assume that you have a low-latency, high-bandwidth mechanism to transmit pads, something suitable for use on the Internet. Now -- why can't you optimize by instead of transmitting your pads with this, just transmit the message?

@_date: 2016-02-24 15:59:56
@_author: Jon Callas 
@_subject: [Cryptography] RIP Claude Shannon 
Hash: SHA256
My little thought experiment is saying that if you can securely transmit pads, you can securely transmit data.
The reason for "stockpiling" the pad as you put it is only that you're on a crap network, and don't have crypto.

@_date: 2016-01-01 22:33:43
@_author: Jon Callas 
@_subject: [Cryptography] Formal definition of lightweight crypto 
Hash: SHA256
Let me give an informal definition. Lightweight crypto is weak crypto that is still useful.
Let me handwave at an example. Suppose I was making a commodity in a factory  cosmetics, liquor, etc.  and you want to put an authenticity mark on the container. Imagine it either being an RFID or optical like a QR code, and it testifies that this product was made in batch 123, on date D, and this is all backed with a seal that you can check over the web to find out it's a legitimate good, not a counterfeit.
Now let us suppose that we construct a cryptographic puzzle as certification. It could be a MAC, a digital signature, some zero-knowledge whatever, and we all agree that someone can forge one of these for about $10,000 dollars worth of compute time bought from a cloud provider, and cracked in about four hours. But it provides the authenticity seal for a $100 bottle of liquor or cosmetics. One could argue that while this digital seal might have an equivalence of only 50-60 bits of crypto, it's still reasonable to be an authenticity mark for its intended use.
You wouldn't want to use it for securing a protected disk, or a communications link, or a contract, but as the special case of showing authenticity of a product, sure it works.
This is the idea of lightweight crypto. It is doing something useful  an authenticity seal that might otherwise be done by a plastic hologram seal or whatever  but it's not something that will keep a government out. It's just more expensive to make a counterfeit seal than the product is worth.
That's kinda the idea. Weak, and yet still useful. And more importantly it can be run on very cheap hardware as well.

@_date: 2016-07-19 09:01:02
@_author: Jon Callas 
@_subject: [Cryptography] The Laws (was the principles) of secure 
Hash: SHA256
I agree that it's overreaching. They're not laws, not in the way that a physicist or mathematician would consider laws. Relatively few are actionable. They're far more like aphorisms.
Is it? I know what you mean, and yet I have no idea what you mean. Well, Robert Morris Sr. had a fantastic principle that security boils down to a 2x2 matrix of how much someone else wants your data, and how much you care if it's known. If you don't care about something and they don't care, then we have something close to a no-op. If you don't care and they do, then they're going to get it and you don't care. If you care and they don't, you're wasting effort -- and this is itself interesting. It's only in the quadrant where they care and you don't want them to get it that things get *really* interesting. Part of good security is learning to divert effort from protecting things you care about from things they don't care about to things they do.
In any event, this law seems to imply that only that interesting quadrant exists.
That's clever, but the more I think about it, the less I understand it. What does "store" mean? Is a value in a register stored? Is data in transit stored? I think I can argue that they both are and that it isn't unreasonable to think that everything is stored. If everything is stored, then the law is meaningless because it becomes a false-implies-true tautology.
It also seems to be in contradiction to your I know what you're trying to say. I'd think something better might be, "talking securely to a snitch is still insecure." Or something like that.
Is this any different than the aphorism that the defender has to protect everything, but the attacker only has to win once?
Personally, I despise that particular aphorism because it is to me defeatist. It seems to basically say, "why bother, the attacker has already won."
But each of them, your  and the one I reference are to me trying to say:
Attackers adapt, nature doesn't. Friction doesn't develop countermeasures to lubricants.
Well, no. Your system has to have an attack model and part of the attack model is a declaration of what's out of scope. We assume that the attacker can't read your mind. We assume that a value kept in registers won't end up on the memory bus. We assume that pinned memory won't get swapped. Sometimes those assumptions are thwarted. For example, pinned memory might not be pinned in a crappy VM, the CPU might be emulated, the compiler might have optimized something away (like clearing the key). In such a case, the problem is real but the lesson is different.
The attacker will attack the weakest point. The attacker will do the least work to win.
But is also has more places to defend.
Complexity gets a bad rap for some good reasons. But let's also remember the Einstein quote that something should be as simple as possible and no simpler. In my experience as a designer, complexity arguments are the first refuge of amateurs.
In Don Norman's "Living with Complexity" there is much wisdom. He discusses what he calls "Tesler's Law" for Larry Tesler, that the complexity of a system remains a constant. You can shuffle it around but you can't eliminate it. He notes that two systems lauded for their simplicity are the classic "Macintosh" GUI and the Unix command line. The GUI is a few knobs that do a lot of complex things simply, and the command line is a lot of knobs each of which is simple but in combination are both powerful and subtle. Each is both simple or complex depending on your point of view.
I think this can be tighter. Words like "holes" are pejorative to start with. No one, no one puts in holes for the good guys. Even the LEA people whom I mock as wanting a "magic rainbow unicorn key" don't want a *hole*.
Usually? And what do you mean? Why not just state, "the only part of your system that should be secret is the key" (Kerckhoff's principle) and be done with it?
Here's my equivalent: The most secure system is the one people will actually use.
Compare and contrast with complexity. Simple systems are often hard to use.
Great advice. Completely non-actionable.
You have to work with the tools you have. You have to play the cards that are in your hand.
Not really.
This is in contrast to the old, "Security is a journey, not a destination." Booleans are destinations.
There are plenty of security systems that are good for X but not for Y. Threat models again are key.
At the end of it, security in general and cryptography in specific is about adaptation and picking the right thing. Let's also discuss other things like usability, scope, appropriateness to the threat, and so on.
"Security is not a boolean" is in my opinion an better law than "security is a boolean.
Worst of all, they're both great laws.
The old aphorism, "if something sounds too good to be true, it probably is" is better.
Well. Lots of things do. I could list lots of them. I can also list a lot of things I wish would go away.
Wouldn't it be better to just quote "Schneier's Law" as you put it, or to just simplify that to "it's easy to make a system you can't break, but hard to make one that others can't"? Here's a few of my own principles:
The first rule of security is "You can't protect everything."
No compromises is a compromise.
Layering is important. There's no sense is strengthening the strong links of the chain.
You can land a man on the moon, but you can't land a man on the sun.

@_date: 2016-06-22 12:06:08
@_author: Jon Callas 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Hash: SHA256
I apologize in advance if I'm being clueless on some piece of this, because I have *not* spent enough time to thoroughly understand Mok-Kong's protocol. (Furthermore, I have picky questions that I don't know if they're relevant about parts the protocol, like what properties should "signcryption" have?)
But anyway...
There are plenty of unsolvable problems that you can solve is useful ways. Yeah, they're "limited" but they're often limited to the real world and often because the full problem is unsolvable.
For example, it's often completely reasonable to solve the halting problem by setting a timeout. Split-brain problems are solved all the time with a quorum system. You can solve the Generals/Agreement problems either by allowing it to go a wait state or declaring an exit condition, etc. In short, these unsolvable problems are solvable when you put in an escape valve that says that when you get into the unsolvable state, you declare a solution.
Mok-Kong has this note at the end of the protocol:
   Note that after step (2) Alice cannot innocently refuse to perform
   step (3), since the pair (X,Y) stems from her. In other words, after
   step (2) the contract is de facto completed.
To me, it appears that "de facto completed" means there's an escape valve. If the protocol says that Alice does X, Bob does Y, and Alice is supposed to do Z, but if she doesn't, tough -- then the protocol terminates.
Of course, this might be unsatisfactory on some level and there may need to be software duct tape and chicken wire to make it work. But it sounds to me like there's the escape clause here.
Am I right or not?

@_date: 2016-06-23 11:11:17
@_author: Jon Callas 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
Hash: SHA256
Well, it sounds to me like you and I agree, but we're agreeing with me saying yes and you saying no.
I was observing that the protocol does not appear to be bound by Generals-class unsolvability because it has a property that is a termination condition.
You're observing that it isn't bound by Generals issues, but that it isn't fair. The property you're calling unfairness is the same property I'm calling a termination condition without further affect. So we agree.
But beyond that, it appears that he's been clear and explicit that this termination property isn't "fair" (that's the paragraph I quoted) because when the protocol enters a certain state, an outcome is presumed to be true, even if the rest of the protocol falls on the floor. So it really sounds like we have squishy language about what things like "fair" and "commit" or "promise" mean and there's a lot of imprecision in it (I noted that as well -- there are a bunch of terms that I don't know what they mean in the protocol), but it's *not* subject to Generals-style failures.
But yeah, it sounds like we don't have an agreed-upon definiton of what "fair" means, and it's hard to judge a fair signing protocol without knowing what fair means.

@_date: 2016-06-27 16:10:53
@_author: Jon Callas 
@_subject: [Cryptography] 40 years of "Diffie-Hellman" 
Hash: SHA256
Hear, hear.
Invention credit and scholarly credit goes to to those who publish or at least publicize. That there were some GCHQ people who wrote some papers about "non-secret crypto" is interesting, but it's more interesting for the lost opportunity and as an object lessons for others.

@_date: 2016-03-28 15:33:07
@_author: Jon Callas 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Hash: SHA256
Look at ARX constructions. Look at Threefish (part of Skein), the design of BLAKE, and others. Also, as Jim said, Simon and Speck. I really recommend the Skein paper, because we discuss precisely this. It was designed to run on a 64-bit CPU that has add, rotate, and xor.
Feistal construction is inefficient. You're only working on half the data on each round -- so you really just need more rounds, but it means that the per-round overhead is higher. The alternative, which is called an S-P network can also be thought of as taking the same concept and just doing both halves in a single round.
There have been a number of attempts to use pieces of AES as a lower-order function and they just really haven't been as good. If you want, I could say more, but I'm just repeating what's been said elsewhere.

@_date: 2016-03-29 16:09:19
@_author: Jon Callas 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Hash: SHA256
You might be thinking of The Hasty Pudding Cipher by Rich Schroeppel which is in my opinion the most brilliant of the AES submissions. My comment at the time was that it didn't meet any of the requirements NIST had, but it met requirements they should have had. It's also the first cipher that had what we now call "tweaks."

@_date: 2016-03-29 16:11:29
@_author: Jon Callas 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Hash: SHA256
SP networks often don't have S-boxes per se. Many of them have a set of rotation constants that function the same way. But in any event, the design is both hard and easy.
More coming.

@_date: 2016-03-31 15:46:00
@_author: Jon Callas 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
============================== START ==============================
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA256
Adding crypto into a data processing workflow.
Ideally, crypto would be a transform that can transparently turn on or off so you can build a *system*. But that's a long discussion.
HPC had what we now call format-preserving encryption built into it. It had variable output block length, and that lessened if not eliminated the need for chaining. Slightly differently from what Jerry said, a tweak is a generalization of an IV or nonce or even counter. It's not intended to be secret, and need to have full security even when ordered. There are no related key attacks with tweaks.
For example, an IV in CBC or CFB mode is not secure if it's a counter. You run into problems if you do something like use a block number of a disk block. You could use counter mode for that, but you lose the robustness of a block cipher. Counter mode is a bad idea for disk encryption because a plaintext leak turns into cipher text leakage.
Tweaks also make a lot of mode things easier or lessen them. They also provide the function of "family keys" as well.
A tweak is a generic parameter that you can do whatever you want with it, and it doesn't alter the security of the cipher, in contrast to a key, which is assumed to be a secret parameter.

@_date: 2016-05-06 12:05:36
@_author: Jon Callas 
@_subject: [Cryptography] Why two keys? [was: Re:  WhatsApp, 
Hash: SHA256
The concept here in a broad form is called "key hygiene." The idea is that you should only use a key for one purpose. If you're going to encrypt with it, you shouldn't also use it for integrity. Sometimes there are vague reasons for it, and sometimes specific. Sometimes there are weak reasons, and sometimes there are strong reasons.
Here's an example of a vague reason. With RSA, since decryption and signing are the same operation just using the public and private keys, using the same key for both turns each operation into an oracle for the other. I'm not sure that there's ever been a non-contrived attack, but there you have it. For Elgamal, there are generators that are desirable for performance, like 2. g^x is convenient to compute when g is 2. This is a fine generator for encryption, but trivially broken with signing. So don't use your encryption key for signing. In this specific case, it's also why people will say it's best not even to do Elgamal signatures. DSA, for all its own issues (nonces, hold on to that thought), is a better choice for an orthogonal reason -- signature size is proportional to the size of the hash rather than the key.
In symmetric land, particularly with AE modes and constructions, the weak reasons are that there are proofs of security that assume that the auth key and crypto keys are independent. If you do that, you don't have have to prove anything about the mixing. So it's a weak thing in that have statements about security if you're using the same key in different places. It might be secure, but we don't know. It might also blow up in your face.
Then there are the strong ones, where there is an actual attack on one part from the other. There are a whole lot of these in various places. In particular, this happens because we consider auth keys to be less valuable encryption keys for a lot of good and bad reasons. Let me contrive a hypothetical. Suppose Alice and Bob are talking and we're passively observing. Let us also presume that there's some sort of slow leak of authenticity through -- whatever. Timing, passive oracle, whatever. But let's suppose that they are on a noisy line and because of something we can learn the auth key with ~2^30 retransmits. This leak is somewhere between irritating and fatal to them. Let's suppose that the leak makes it so that we can know that Alice's message N is broken.  Well, we were going to learn that anyway, most likely, because it's going to get a retransmit. It's interesting, but not overly useful. Also interesting but not overly useful is that if we see a good message whiz by us, but then get retransmitted, we learn something about where the noise in their comm channel is happening. On the other end of the scale, if they're using the same key, the auth leaks turn into crypto leaks. In the random case where they leak the auth key, we can decrypt their messages. In the case where we learn *part* of the auth key, we learn part of the crypto key and thus get an advantage into decryption. These auth errors might turn an intractable crypto break into a tractable one. And of course, if we end up learning the crypto+auth key, we can impersonate either of them to the other, and gain all the fun from that you can imagine.
Beyond even this, we now have an *incentive* to stop being a passive listener and start injecting auth faults into the system. There's a systems break that happens because of bad key hygiene that escalates something unfortunate into something catastrophic, and potentially subtly catastrophic.
Here's a slightly differently contrived thing. Key reuse, as you know, is bad. It's sometimes necessary (like with block-level disk encryption), but it's never desirable. Sometimes it also just happens for one reason or another. And also often there's the attacker-level problem of how do you know that a key was reused? If you consider Counter Mode and key reuse, then known plaintext leads to a plaintext leak on key reuse, even.
Well, if they're generating a public parameter like an IV/nonce deterministically[1] from the key, then you're giving away a key reuse because the IV is often a public parameter. At the very least, an IV reuse broadcasts a reused key and tells the attacker where to look, and in some systems, like GCM, an IV reuse is far, far worse than a key reuse.
So all of this is why key hygiene is a good thing. There are plenty of places where it doesn't matter. In a perfect system, it shouldn't matter. But in real systems it can and does matter, from a crypto standpoint as well as engineering standpoints. It's good practice to get into. It's probably okay to use the same key a lot of the time. But it's always okay to have two separate keys, and sometimes maximally bad not to. There are many subtle issues that you just don't have to worry about if auth keys and crypto keys are independent.
[1] I'm using "deterministically" in an affected manner. You know what I mean.

@_date: 2016-11-13 20:25:13
@_author: Jon Callas 
@_subject: [Cryptography] highlights of crypto history 
Hash: SHA256
These are all military and diplomatic, which is only a facet of the whole thing, as well as all being 20th century.
It's certainly hard to beat on other scales, as well, because it was subterfuge getting it to the US, and Zimmermann didn't deny it at all.
Or not. If Yardley hadn't spilled the beans, then the USG might not have hired in the Friedmans, Rowley and others.
I'd put Pearl Harbor in there as well, because while the US knew that Japan was up to something, having broken the codes, they had no idea exactly what. When the Japanese navy set sail, they were completely radio silent, but a small army (navy?) of operators sent cover traffic to obscure that they were sailing for who knows where. The US was decrypting all the traffic and could only bite their nails. They knew something was up, but not what.

@_date: 2016-10-03 00:51:54
@_author: Jon Callas 
@_subject: [Cryptography] another security vulnerability / travesty 
Hash: SHA256
It's less odd than you'd think. HIPAA is the Health Insurance Portability and Accountability Act. Note that it's "insurance" and "portability" not what many people think, "information privacy." There are privacy parts of HIPAA, but it's a system to make electronic records work first. That implies security, but it isn't *about* security.
At PGP, we were fortunate to have Dr. Bill Braithwaite, who's sometimes called "the father of HIPAA" on our advisory board and I spent a lot of time talking to him about HIPAA and what it all means for security.
He explained that the purpose of HIPAA is to solve the insurance and health record problem. Before he started, there were literally hundreds of different forms across different insurance organizations, and also the spectrum of health care organizations in size and sophistication. You have huge organizations like Kaiser and Cerner at the top, all the way to the thousands of small practices that include dentist offices. These small practices are essentially small family businesses of four to six people where the IT person probably has graduated high school, but you can't count on that. So the rules have to be squishy enough to handle that wide disparity of sophistication.
A provider can essentially opt out of HIPAA by not doing portable records. That's exactly what the case you mention is. By using a fax machine and paper forms, they're more *exempt* from HIPAA than compliant, kinda the way that if you're walking along a road, you don't need headlights and turn signals. It's not using an amulet, it's saying "non serviam" to HIPAA.
Part of the challenge of it has been to make it worth people's while to go electronic, because too stringent interpretations that make it easier and cheaper for an organization to do things the old way help no one. Today that's not really an issue, but it was in early HIPAA days. I did a bit of searching, because Braithwaite is really good at talking about the subtleties of the situation. I found this testimony he gave to Congress in 2005:
But here's an important paragraph that sums up the huge problem:
   HIPAA also instituted severe criminal penalties to deter individuals from
   making knowing decisions to violate a patient's privacy for their own
   purposes or gain.  The fear that resulted in the healthcare industry made
   people pay great attention to the Privacy Rule and probably led some in the
   industry to take steps that are more conservative than intended.  For
   example, I often hear from frustrated providers who asked a hospital for a
   copy of the records on a patient that they were seeing in the emergency room
   and who were met with a 'stone wall' denying them access to the record
   without a signed release from the patient "as required by HIPAA!"  The HIPAA
   Privacy Rule has made it very clear that HIPAA does not impose any such
   requirement or restriction on a providers' ability to share or get
   information on a patient for treatment purposes.  There is much education
   still to do in this area.
And there's the point -- doxing, LOVEINT, using health records to tailor advertising and many other things ought to be illegal. On the other hand, as he says in the above paragraph, treatment demands that someone be able to get patient records because they happen to be the attending caregiver by fate and circumstance.
This isn't a problem cryptography can solve. The solution to that problem likely uses a bunch of cryptography, but it's not a crypto problem.
The HIPAA problem, though, is making health care use sophisticated computer technology. That implies controls. Those controls imply crypto among other things. Far fewer people are opting out by using faxes of forms now than they were. My dentist has an office system that has all the HIPAA stuff built into their practice, and that package they bought does all the right stuff for them. That's how it should be, and more and more it is. I can understand that someone who has been doing a small practice for the last two to four decades might want to just do it the way they've always done it. I don't agree, but I understand.

@_date: 2016-10-26 12:39:38
@_author: Jon Callas 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
Hash: SHA256
Here's a short opinion from one of the DKIM authors.
DKIM doesn't do what is claimed for verifying email athenticity. A DKIM signature is from the "administrative domain" which is not the same thing as the domain part of the sender. Virtual hosting, many other infrastructure things make it so that the administrative domain is neither one-to-one nor onto email domains in the general case.
It means that legitimate users of a given system can forge messages from some other user and they'll get a DKIM signature on them. Yeah, perhaps you can detect from headers and other things that the message was "forged" but perhaps you can't. I put scare quotes around forged because there are many situations where a user sends a message with some other name on it that are legitimate and in many cases this isn't a bug, it's a feature.
The DKIM signer simply stamps outgoing messages somewhere in the outgoing pipeline, it doesn't have user authenticity in it as anything other than guidance.
Moreover, the DKIM signing keys have to be sitting on some server that processes outgoing email.
This means that in a case where someone has hacked a system, if they have the email stores, they probably also have the DKIM signing key. If they have the DKIM signing key they can create whatever messages they want and sign them, with backdating and anything else they want.
If you're using DKIM signatures to verify a hacked mail store, you're (e.g.) assuming they have the user maildirs, but not the server config files.
Lastly, this property -- that DKIM doesn't provide author/message authenticity -- is a *GOAL* of DKIM. When we were making it, we were very concerned that the legitimate needs of spam fighting etc. would turn it into a tracking and surveillance system. DKIM is designed to make the connection between the DKIM signature and author authenticity tenuous at best.
Here's a short description of the DKIM use case: DKIM allows Gmail to know that a message for Alice from her bank was created by her bank, even when it is forwarded through her university alumni email address.

@_date: 2016-10-27 13:48:31
@_author: Jon Callas 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
Hash: SHA256
Sure, but the fact that these keys have been sitting on edge MTAs for ages means that they could have been hacked otherwise. The DKIM keys are low value keys, remember.
On top of that -- why would you doubt the plaintext as is? We all know that it's likely to be true. We all know that there's a chance that some of it isn't, and we know that they juiciest parts are the ones most likely to be a targeted forgery, which could have been done by trading to some other gang who have hacked some Google MTA.

@_date: 2016-10-27 14:34:53
@_author: Jon Callas 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
Hash: SHA256
By "dangerous" you seem to mean "meets its non-goals"?
The point of DKIM is that when Alice sends an email to Bob, it's actually a conversation between abc.com (Alice's server) and xyz.net (Bob's server) that takes place even (especially) when the message was forwarded through example.edu. It's so that if a phishing message shows up at xyz.net, it can go back to abc.com and say, "Hey, you have a personnel problem over there."
It is possible to use DKIM as a per-user signing system (and in fact, I would have loved to do more to prevent it, but there were some people who thought this was a good idea), but it's hard. You'd have to really want to, and the reasonable, easy ways to use DKIM (make a key and use it everywhere forever) degrade trust, and some reasonably sophisticated ones (like key rollover) can make it close to impossible.
When Bob's email amounting to a few gigabytes gets dumped onto Wikileaks, if Alice and Bob have been talking for years about how much they hate Fergus Laing, we're probably going to believe it because there's a lot of non-cryptographic evidence associated. You believe it because of the strong narrative, not because of the crypto. Is the crypto part of the narrative. Sure.
But if there's one lone diatribe about Fergus that seems out of character, you wouldn't say, "Wow, this is really out of character, but I guess because of the DKIM signature, it must be true."
In Manning's case, Manning made the mistake of confiding to an informant, and the transcript speaks for itself, again, no crypto involved. There are many, many opsec problems that reduce to talking securely to an untrustworthy or compromised person. Crypto doesn't solve talking to the wrong person.

@_date: 2016-09-16 12:24:34
@_author: Jon Callas 
@_subject: [Cryptography] Recommendations for short AES passphrases 
Hash: SHA256
I read what you're doing and my question is -- what problem are you actually trying to solve? You need to examine that, first and have a clear description. It's very easy to get lost in the weeds of details and lose sight of what you're doing.
Let me rewind all the way to the top -- a password (passphrase). Breaking that password, by guessing it or stealing it short-circuits everything else.
A password KDF like PBKDF2 has two purposes -- one is to whiten the password out to a full key size (if you need me to describe what I mean by "whiten" just say so), and also to slow down a brute force key search.
The iteration count slows down the search. We can also have a long discussion of that, in general. But 1000 is way too low. You really want to be doing much more. There's nothing wrong with having a significant fraction of a second in wait time there, but this also gets us back to what problem you're trying to solve.
You're asking us about solutions without stating the problem. The discussion might be interesting, but it's only going to help you by luck.

@_date: 2017-04-02 14:25:47
@_author: Jon Callas 
@_subject: [Cryptography] Regulations of Tempest protections of buildings 
I'll share the skepticism, but also provide a possible explanation.
Many years ago, I worked with a group that produced TEMPEST gear. That gear was not sold to the general civilian population. There's nothing that would stop someone from making shielded equipment, there's nothing that would stop someone from putting it through the same testings that TEMPEST gear went through. But your gear wouldn't be TEMPEST, because it hadn't gone through TEMPEST testing and approvals.
Similarly, there's nothing that stops you from making a shielded room; it's not illegal. There's nothing that stops you from making a shielded room that meets or exceeds TEMPEST standards. However, I would not be surprised if you couldn't get a TEMPEST approval for that room without some license or something from the government. There's a danger in conflating the testing and approvals process from the raw technology itself. I share the other skepticism that there is any authority by which the USG can control shielding technology, but can completely believe (though I don't know one way or the other) that you can't have your shielded thingie called TEMPEST without some restricted testing process that you don't have access to without a contract, license, or whatever. That's the subtle difference, and thus it can both be true that you need a license for TEMPEST and there's no restriction on making shielded whatevers.

@_date: 2017-04-04 17:14:04
@_author: Jon Callas 
@_subject: [Cryptography] Tempest and limits on receiving 
In the US, the 1934 Federal Communications Act allows anyone to listen to any frequency that is not specifically blocked. This isn't the case in other countries.
There are relatively few specifically prohibited frequencies, but those used by cellular communications are on the list. This is a result of someone mostly inadvertently picking up conversations between Newt Gingrich and a number of colleagues including John Boehner and others. The group of them were discussing ethics charges against Gingrich, and they were intercepted by people with a scanner who were playing around and just happened to pick up Boehner's car phone.
Here's an article from the time period by John Markoff
Here's a PBS News Hour article:
As others have noted, this has nothing to do with TEMPEST and is arguably the opposite of TEMPEST, but the upshot is that in the US the default is that you are allowed to listen, except when there is an explicit carveout.

@_date: 2017-04-26 15:28:47
@_author: Jon Callas 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
Personally, I would recommend that you *not* use counter mode or anything resembling it for writing files. There's nothing wrong with CBC mode for your purposes. (Or CFB, for that matter, but I really don't want to digress down that path.)
Counter mode creates a stream cipher. Stream ciphers nearly universally work by creating a stream of random bits (typically called the key stream) that is then XORed on to your plaintext. The first major issue with stream ciphers is that if you reuse a key, you give the attacker a trivial attack on your system. If there is any known plaintext in the first stream, you XOR that known plaintext onto the first ciphertext which yields you the key stream, and then you XOR the known key stream onto the second cipher text yielding its plaintext. In short, a stream cipher is an approximation of a one-time pad and the first rule of one-time pads is that you better use them at most one time. Put another way, when you use counter mode, you must not only keep the key secret, you must keep the key stream secret.
In a communications protocol, this typically isn't a big deal because you're going to pull a key from your random number generator, and if that isn't different, you have bigger problems.
However, when it comes to files, there's going to be the temptation to allow a seek operation. If they seek backwards, then it all starts to unravel. Worse, counter mode makes it easy to seek backwards, and thus increases the chance that someone will. If you seek, you're rewriting in place. If you rewrite in place, then you are reusing your key.  Thus, my advice is not to use counter mode for storage. Even if you do it right, it's like leaving a rake in the yard with the tines up. It's just asking for someone to come along and step on it.
Hence, just use CBC mode. Yeah, I know it sucks. Most of the ways that it sucks either don't matter when you're writing a file (because those problems come from using CBC in an online protocol, where you *should* use a stream cipher) or you can code around them (like using ciphertext stealing instead of padding). But CBC has the added advantage that it's a pain in the butt to seek in a file
But that isn't the question you asked, is it?
When you're using counter mode, it doesn't matter what the counter is. The basic construction is that you encrypt 0 and xor that onto plaintext. Then encrypt 1, then 2, and so on. It is, after all, *counter* mode. It doesn't matter at all what you start the counter with. It doesn't even really matter what you increment with. It only matters that you don't repeat a counter value. (And actually it only matters that no one knows you repeated a counter value, but don't tell anyone I said that.)
In fact, one of the really cool things about counter mode is that it doesn't matter what you start the counter with. This is unlike an IV for CBC, CFB, etc. where it *does* matter. Zero is a great counter. It's an awful IV. Counters aren't IVs, even when you pass them into the "iv" parameter in the api.
Since you're probably going to counter mode anyway, you might want to use an AEAD (Authenticated Encryption with Additional Data) mode. I caution against using GCM mode. It has a number of things going for it: it's fast (except when it is slow), and parallelizable. It is, however, brittle. It was originally created for Chuck Jones to use in the famous Road Runner cartoon, "Beep-Beep Blockchain" with Wile E. Coyote using in his famous attempt to take over the Road Runner's mining operation. As he often ended up doing with nitroglycerin, the Coyote disrespected a nonce, and even people who haven't seen the cartoon can guess what happened in the final scene. Either CCM mode or ChaCha20-Poly1305 are reasonable alternatives. OCB might be an option for you, too, but all of that's a different discussion.

@_date: 2017-04-26 22:35:39
@_author: Jon Callas 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
Thank you.
Well, ChaCha20 is a stream cipher. Everything I (and Jerry, thanks, Jerry!) said about stream ciphers applies to ChaCha as well.
If you want to do block-oriented work, what you want is a *tweakable* cipher.
Tweaks are a generalization of an IV/counter/nonce where you have complete security even if the tweak is under attacker control.
XTS mode is a tweakable mode for an AES-like cipher. It's great for things like disk blocks. AEZ is an amazing tweakable construction from Rogaway and others, and has great promise.
In shameless self-promotion, Threefish is a tweakable cipher that comes in either 512 or 1024-bit variants.
Thank you!

@_date: 2017-04-27 09:53:54
@_author: Jon Callas 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
Like I said, I didn't want to go down that path. The previous post was long enough as it was.
Nonetheless, yes, integrity checks are, in the abstract, always a good thing.
Definitely! I should have mentioned EME2. Before the patents were tossed away, my then-company licensed it, but we didn't do as much as we wanted because of performance. It's a different situation when you have AES instructions, except for some very high-performance situations.
The major advantage is that it's a wide-block mode, so you get something similar to authentication, as you say.

@_date: 2017-04-27 16:01:16
@_author: Jon Callas 
@_subject: [Cryptography] 
=?utf-8?q?_AES_Counter_Mode=E2=80=A6?=
I started at your comment with confusion and realize where I miscommunicated. I meant that zero is a good starting point for a counter, not a good increment. Obviously, with zero as an increment, it's just ECB+XOR.
Yes. I'm disappointed here, because really unwinding OCB issues would make it all go away. OCB is what we all really want and everything else is an approximation of it.

@_date: 2017-12-11 16:14:49
@_author: Jon Callas 
@_subject: [Cryptography] Rubber-hose resistance? 
Thank you for saying that. Usually, I'm the one who says that. Many times when someone says "plausible deniability" what they mean is "reasonable doubt." That was me. I had a wonderful conversation with a customs agent in New Zealand. I asked about TrueCrypt and he said, "Oh, we just ask you for the second password." I don't remember what I said afterwards, wanting to hear more, and there was a reply of, "Look, we don't look in your laptop just because you had an apple in your bag." (New Zealand has strict agricultural control, and if you do have contraband produce, it's something like a $200 instant fine.) I thought that was more telling than the first. The guy was telling me that he's got a job to do, and that involves stopping people who are going to overstay their welcome (they can and have refused people entry for not being able to show that they have a return ticket), people bringing in contraband, etc. If they want to look at your data, it's because they think that data is pertinent to *something*. If they find the equivalent of a suitcase that advertises as a feature that it has a false bottom, they're going to be intrigued as this indicates that whatever made them think they should be suspicious and probe more was right. And they're going to cut to the chase  show me the false bottom.
To me, that goes more to my objection to so-called deniable encryption  that it presumes a threat model in which your adversary is stupid (they don't know or don't care about your false bottom) or good (they are willing to play by the apparent rules of the game as described, and even if they think you're cheating, they'll let you go if they can't prove it. An alternate scenario is that they'll play by rules that say you have to prove you're *not* cheating.
In the case of border agents anywhere, if you're not a citizen of their country, they don't have to let you in. They can deny you entry (in which case you have to go somewhere else, most likely the port of departure that brought you there), or they can keep you in the non-state area between countries indefinitely. This advice is just silly. Of course they can accuse you of having hidden wallets. "Hey, I bet you have a hidden wallet!" In many instances, this is exactly what law enforcement does, to accuse someone of something to see what their reaction is.
The term "rubber hose cryptanalysis" was a term of art decades before XKCD.

@_date: 2017-12-17 23:49:17
@_author: Jon Callas 
@_subject: [Cryptography] Rubber-hose resistance? 
What's the problem with OTR?
I once had a long talk with Ian Goldberg about deniability (which is the topic here) and he said that all he really meant by deniability in OTR was that it wouldn't have *more* linkage than a plaintext log would have. That's fair enough, but the issue as I see it is that people ascribe magical properties to it, as if people wouldn't get a plaintext log and just presume that it's true.
Well, protocols do not have UX. Implementations have UX. I've seen lots of uses of OTR that had a great UX, which was essentially none.
Now, it *is* designed for continuous network connections, which is why it has issues with mobile devices, but that's not so much a protocol problem as an implementation problem. There's plenty of downstream UX issues, but they're not at all related to the topic of this thread.

@_date: 2017-12-18 00:18:13
@_author: Jon Callas 
@_subject: [Cryptography] Rubber-hose resistance? 
It's not a fine point, it's a coarse point. It's the difference between "I'm telling the truth" and "that's my story and I'm sticking to it." Plausible deniability is the latter of those.
Plausible deniability comes from statecraft, spycraft, and realpolitik. The idea is that an actor does something and denies it based upon some set of reasons, but those reasons are not exactly believable. Moreover, in most cases, part of the situation is that the actor knows that people don't believe their story and they stick to it, but often with a nudge-nudge as a way to convey as a subtext that the story they are denying is in fact true.
Plausible deniability is something you do when you have either power or impunity. In the cases we are discussing, the opposite is true.
If you're in a court room, plausible deniability is not going to get you acquitted. It might even get you convicted because it looks suspicious and the denial isn't reasonable doubt. "I have no idea what happened, I wasn't there, I was home along watching TV" is much better in the reasonable doubt world than a constructed plausible deniable alibi. There's a saying that once you say, "g^x mod p, yer honor, and that's why my client is innocent" then you've lost, because they're not going to understand and even think that if that's the best you can do, then you must be guilty. It creates unreasonable doubt.
In the case of something like a border crossing, it's even worse, because you don't have the normal rules even of a court room. If your denial is merely plausible, it's worse than no denial at all.
Moreover, there are rules in such a situation. If some border guard wants to look in your laptop and you say, "I'm a business traveler. That's my work laptop, and I'm not allowed to let someone who doesn't work for my company look in it, because it has customer lists, marketing information, and company intellectual property. I'm happy to leave it here with you. If you'd like to call my company's general counsel, here's the phone number" you're *much* better off than if you go into the g^x mod p routine.
It's Aesopian language just like "plausible deniability" is. It's a euphemism for torture or something like it.

@_date: 2017-12-24 22:31:41
@_author: Jon Callas 
@_subject: [Cryptography] (no subject) 
As one of the moderators, I apologize for letting this one through. I wasn't paying enough attention.

@_date: 2017-02-18 12:49:51
@_author: Jon Callas 
@_subject: [Cryptography] Security proofs prove non-failproof 
As a mathematical logician, I'm a huge fan of proofs, and yet sneer at a lot of security proofs.
We don't have formal definitions of what security even means. Our reasoning is based on concepts like "distinguishability" which are obviously flawed. Distinguishability is the same thing as saying, "Hey, we don't know if it's a duck but it quacks, has webbed feet, flies and swims." We also have a bunch of models like The Random Oracle Model, which is good as far as it goes, but sometimes it doesn't go as far as we'd like.
There are also a number of tacit assumptions that might be relevant in implementation. There is a marvelous proof of SSH, but Kenny Paterson and crew found a security break in it. The break came because of leaks from packet sizes, and the proof had an abstract definition of packets. Obviously, an implementation has to have the concept of a size and that's where the disconnect is.
As an analogy, assume an unpickable lock. But also assume that said lock is installed in a glass door, and by looking through the glass door and a reflection you can see information that lets you get the right key made. This isn't at all part of the model and likely the proof of security didn't have that in it. Their security model was different than the one where the lock was implemented.
(As a logician, what I would say is that the actual model is an elementary extension of the proof model that includes symbols in the formal language that lead to a contradiction in the extension that don't exist in the base model.)
In any event, this is the core of why proofs of security useful (hey, who doesn't want an unpick able lock?) and yet often don't matter in the real world (because it was installed in a glass door).

@_date: 2017-02-27 17:52:02
@_author: Jon Callas 
@_subject: [Cryptography] Lower bound for the size of an RSA key's private 
It's 3.
Here's my Gordian Knot construction:
Create an RSA key with a public exponent of 3. Make sure it's actually secure. Swap the public and private keys, publishing the formerly private key as the public key and keeping the formerly public key as the private one. The private exponent is now 3.

@_date: 2017-07-02 14:22:01
@_author: Jon Callas 
@_subject: [Cryptography] OpenSSL CSPRNG work 
Agree totally and them some.
Part of me wants to get fussy about definitions and say that an output function is not a RNG. It can be a PRF or PRP, but not an RNG. The other part of me just nods along and knows what you mean without getting fussy.
RC4 is not a good PR{F|P}. It has known biases, and those biases are so well studied that they are the basis of the recent break against it as a cipher. It takes about two megabytes of ciphertext to perform a break against it as a cipher. That means it's really not a good output function. It is the opposite of cryptographically, secure; it is cryptographically insecure.
AES in counter mode, any other decent block cipher in counter mode, lots of hash functions including the HMACified versions are reasonable.

@_date: 2017-07-02 22:16:17
@_author: Jon Callas 
@_subject: [Cryptography] OpenSSL CSPRNG work 
I'm sorry.
arc4random(3) is a function in stdlib.h. Look at the man pages. If you mean some suitable PRF/PRP to be named later, say so.

@_date: 2017-07-14 00:07:16
@_author: Jon Callas 
@_subject: [Cryptography] [FORGED] Attackers will always win, 
It can't run Apple ][ code, because it runs at about 1/16 speed of a MOS 6502, and there's a lot in the Apple ][ that has sensitive timing constraints, alas.
It is, however, very cool.

@_date: 2017-03-10 19:42:06
@_author: Jon Callas 
@_subject: [Cryptography] Programmable HPstyle RPN calculators (not HP). 
You need one. Just get it. You have my permission. If your SO complains, tell them it's all my fault.
However, I'd recommend considering the HP16. I have an original one around here somewhere, and you'll love it because it has lots of computer features in it like being able to do variable word size, operating in hex, octal, or decimal (binary too, but really that's more trouble than it's worth, except to input in binary for quick conversion to hex or oct), as well as being able to do twos-compliment or ones-compliment math. You can set it up to do 36-bit, ones-compliment just like a real Univac. Yay!
I'm sure you can find something to do with it cryptographically. Maybe you should get both. Yeah, definitely you should. It's all my fault you're going to get them both.

@_date: 2017-05-01 15:55:42
@_author: Jon Callas 
@_subject: [Cryptography] CFB/OFB/CTR mode with HMAC for key stream 
I believe that you're describing using an iterated hash function (HMAC is a keyed wrapper function) to create a key stream and then use that as a stream cipher.
Yeah, this has been called MDC for something as obvious as "Message Digest Cipher" and has, as Bill Frantz has mentioned, been suggested over twenty years ago. You can find descriptions in the original edition of Applied Cryptography.
There's no major security issues with this (assuming you use a decent hash function, and I leave *that* as an exercise for you). It is, however, slow and kinda superfluous. If you take the example of SHA-256, you're using its internal block cipher to create a hash function, which is then run twice to create a stream cipher.
You don't get CFB or OFB this way, those are modes around block ciphers and you're doing something different. Yes, you could do something somewhat analogous, or straightforwardly a way to do essentially  CTR. But why? You're getting something much slower than (e.g.) AES in counter mode, and if you were really paranoid you could even do something like Triple-AES. There's nothing wrong with AES-CTR.

@_date: 2017-10-15 15:30:33
@_author: Jon Callas 
@_subject: [Cryptography] filtering html 
I'm not quite sure I understand what problem you're trying to solve, so if I say something off base or even obvious, forgive me.
Let's rewind back to some basics. HTML is a markup language, a way to express both content and display metadata. There are lots of other markup languages, like Runoff and all its derivatives (nroff, troff, etc.), Scribe and all its derivatives (Scribble, Texinfo, etc.), TeX and its relations (LaTeX, ConTeX, XeTeX, etc). One can even arguably throw in Postscript and its derivatives like PDF as well as RTF, but relatively few people write them directly. There are of course newer markup languages like Markdown and all of its dialects and derivatives. Many of these are compiled as opposed to interpreted, and I suppose it's hard to even say what the difference is, because many of the languages are not anything like Turing complete. Even the ones that are full programming languages (TeX, Postscript) are typically statically run; one can make a TeX document/program that generates a different output document every time it's run, but in general we don't do that. I have a friend who wrote a Postscript program to generate her business cards and every card had a uniquely generated fractal as a logo, and this is exceptional enough to call it out.
Getting back to HTML, it's a derivative of SGML which was (is?) used for documents back in the day by lots of organizations. I worked for a company that used SGML extensively, and once HTML came out, I basically wrote HTML as if it were SGML with features removed. XML is also an SGML derivative, as is EPUB since EPUB is a derivative of XML.
HTML, as we all know has links, and external references. These external references are evaluated at display time, and it's not unusual for a re-display of a page to generate slightly different content. But yeah  you can easily separate the rendering from this display and render the HTML in sandbox and then display safer image type. The Amazon Silk browser does (did?) this, and as I remember, Opera did something similar.
HTML also can have embedded scripts for active content and this, of course brings in difficulties. But yeah, you can filter out some or all of the scripts, as well as other content. Ad blockers and content filters do this and they work reasonably well. You could even provide some sort of active separation in a proxy. It's not always an easy task, but conceptually it's no different than many other things like VNC, X Windows, etc.
For that matter, many browsers are moving to an architecture that's not unlike that now, even. Each browser tab contains a completely different rendering and execution system. Yes, yes, it's not got the same isolation as running it on different hardware, but conceptually the isolation of execution and rendering goes to similar principles.
So yes, you can make an HTML compiler. Heck, pandoc ( ) is a compiler / translator between many formats and you could (e.g.) compile HTML into PDF and then just display that. Heck, you could compile it into a PNG, for that matter. You could also print a web page and read it on paper, too.
Obviously, this ignores the active content. For HTML the way it is used today, this is an issue. I recently made a comment like, "It's impossible to use the web with Javascript turned off, these days" and while perhaps an exaggeration (okay, it's not *impossible*) it doesn't go all the way to hyperbole. There are a lot of ad blockers now that do at least some Javascript filtering.
Bottom line  sure you can do what you're saying, but it's easy to go into compilation to a degree to where you're no longer using a web browser. If the goal you're trying for is Safe Browsing, lots of people want that. I know I do, and it's a hard problem that lots of people are working on for some value of "safe."

@_date: 2017-10-17 19:36:07
@_author: Jon Callas 
@_subject: [Cryptography] Primary Sources for "The Woman Who Smashed 
I got both the dead tree edition and the audiobook; I've been listening to the audiobook on my commute and am about 2/3 the way through it. It's a fantastic book, a fantastic story, and everyone who's interested in the history of crypto should read or listen to it.

@_date: 2017-10-21 01:33:04
@_author: Jon Callas 
@_subject: [Cryptography] Signature Hashing Choices ... So Many Choices ... 
Yes, it's good enough.
Go look at the definition of an HMAC and note the similarities. HMAC is solving a different problem, but it's building on the same idea, which is that the properties of a hash function are such that you can substitute the hash for the data (or BLOB). That's really the whole point of signatures anyway  that it's impractical to sign the actual data and so you use the hash of data as a proxy for the data itself.
This idea is also used in tree hashing, hash chains like Merkle trees, and others. Tree hashing parallelizes hashing to speed it up by running a bunch of hashes of segments of the data in parallel, and then hashing all those hashes.
In the syslog-sign protocol that John Kelsey and I did, we play similar games by signing a message that is a bunch of hashes of messages. So instead of figuring out how you're going to sign a bunch of log messages, we hash each log entry, and then sign a set of hashes.
Now there is a limitation in this, and that is that if someone can get a collision or second-preimage attack on your BLOB, then your hash gets a collision in it that would otherwise be harder to do. The major way it's an easier problem for an attacker is that their alternate BLOB doesn't have to be the same size as the real blob.
There are ways that you could protect against that. Note my clever use of the term "guard against" as opposed to "prevent." For example, suppose you take your BLOB and cut it up into chunks and hash each one, along with a counter. So you hash 0 || chunk_0, 1 || chunk_1, ..., n || chunk_n (and note that the last chunk is probably short), all the while accumulating all of those intermediate hashes together into the hash context that will be the thing that you will *then* hash with your preamble.
This makes the job of an attacker harder because they're forced to create an attack on a fixed-size chunk as opposed on the whole BLOB.
I'll also add that if your basic hash function works correctly, this is unnecessary. However, it protects against classes of weaknesses like the ones that Merkle-Damgard hashes have. This is arguably gilding the lily, but it's also arguably a reasonable protection.
But anyway, answering your question, yeah, what you're doing works fine assuming the underlying hash function is not broken, and the exposure in your first-order construction has only a tiny opening for hash flaws to hurt you. If you care about this, you can do some type of chunking.

@_date: 2017-10-21 21:37:10
@_author: Jon Callas 
@_subject: [Cryptography] Does this keying scheme make sense? 
I know this sounds bad, but it does not behoove anyone who is an expert in the field to read someone else's patents. The reason is simple  knowing infringement is triple damages.
I can think of about ten ways to do this securely. If he invented an eleventh, I'd rather just award a golf clap from afar.

@_date: 2017-10-26 22:46:22
@_author: Jon Callas 
@_subject: [Cryptography] Has there been any good cryptanalysis of FourQ 
I will simply note that you wrote a post about performance, and then asked if it's secure.
You tell me. Is 17s secure enough for you?

@_date: 2018-02-14 23:09:34
@_author: Jon Callas 
@_subject: [Cryptography] Quantum computers will never overcome noise 
I'm a bit late to this party, but I liked the article.
I'm something of a soft quantum skeptic. Kalai is something of a hard quantum skeptic. We probably agree more than we disagree.
I have a number of friends and colleagues working on quantum computers and they think that the theoretical issues are complete and the rest is just an engineering problem. I even agree with that. My soft skepticism could be phrased, "You keep using that word 'just.' I don't think it means what you think it means."
Less squishily, I don't think that someone is going to make a quantum computer by 2050 that would perform at speeds that we predict a classical computer could do if Moore's Law continued until then (which it won't). Even less squishily, I don't think that there will be a quantum computer that can reliably break a generic RSA 4096 key by 2050. If I want to be a hard-ass on what "reliably" means, I'll say that that means that they can break one faster than a classical computer can generate one. If I want to be more generous, I'd back off to say one key per year. If I want to be provocative, I'd say a single generic key at all.
It's really easy to bury a lot in "it's just an engineering problem" and researchers do that a lot. Once you put Sputnik in orbit, cities on the moon are just an engineering problem. Once you have a transistor that you can make a hearing aid out of, mass-producing today's computers is just an engineering problem. That latter just happens to be a seventy-five year engineering problem.
The quantum advocates I know think that in ten to fifteen years, they'll be able to hit somewhere in the Shor brackets I outlined above. The first time I made my small bet was about a decade ago. At present, I'm merely saying that the engineering problem is going to take thirty years rather than fifteen to win. If they're right, I'll be shocked but not surprised. If Kalai's right, I'll also be shocked but not surprised. I'm in the middle of the two extremes, but really agreeing with Kalai. I think the noise, decoherence, and error correction engineering problems are a lot harder than the advocates think, I'm just not quite at thinking they're impossible. Impossible just means that no one's ever done it before. Just.

@_date: 2018-01-11 17:13:02
@_author: Jon Callas 
@_subject: [Cryptography] Spectre -- would an L0 for speculation-only help? 
Probably not.
Yes, there would, so long as you have a coherent cache, and without one, you blow speculation or have to introduce a whole new mechanism to restart if there's an incoherent cache, which is both hard and probably leaks.
Yup, that's basically the conundrum.

@_date: 2018-07-30 18:10:50
@_author: Jon Callas 
@_subject: [Cryptography] Signal double-ratchet vs. future breaks in ECC? 
Its not a dumb question. It shows that you are thinking more than many people are.
Heres really a simple explanation:
Naively, you use an algorithm like DH or RSA to do key exchange where we exchange random bit strings and use them as keys. But you dont *have* to use the thing you exchanged as a key, you could use it as  lets call it a *credential* and you then derive a key from there.
If youre using ECDH, you pretty much have to do this because youre not exchanging a number, youre exchanging a *point*, (x,y), and have to run that point through a KDF. (Strictly speaking, if youre using integer DH or RSA, youre also exchanging a point, it just happens to be a point on The Number Line and just as we all remember from elementary school, you can use points on The Number Line as if they were numbers. Or perhaps they really are numbers, but now were getting into Foundations of Mathematics which I adore as a subject, but its a digression.)
The double ratchet is really just a scheme for taking a credential and deriving a series of keys from that credential that has a bunch of desirable properties, but yes, if that initial credential exchange is compromised by any means from quantum computers to stupidity the whole security is blown. Game over.
And thats why no one is answering that question because its a case of the abyss staring back.

@_date: 2018-05-31 22:42:18
@_author: Jon Callas 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Heres what Im getting at, then, which is primarily about database theft. Phishing and keylogging are separate.
A servers database holds credentials. Typically, a site that you log into with a password has a stored credential that is F(password), the functional result of a password. Lets give them the benefit of the doubt and say that theyve done it correctly so youre using a decent one-way function, a parameter like a salt, and perhaps other things. If theyve done the right F() and youve picked a decent password, then if someone steals that database, they have to brute force your password, which might be harder than one would think. In many cases, the attacker doesnt want Alices password, they want *someones* password, and therefore they can breach the site by trying common passwords against the stolen database and thus break into some fraction of the accounts. Nonetheless, the database stores a shared secret, even though the protocol is that Alice gives her password and then they compute F(password) and compare that against the F(password) in the database. An ideally done password system has several interesting characteristics that include:
* If I lose the password, (phishing, keylogging, whatever), it is good forever, where forever is defined as until I change it. And of course, it could be used in other places where I reused the password. This is of course why we tell people not to do things like write down their password nor to reuse it on other sites; advice that has merit, but isnt as good as most claim it is.
* If the site loses the database, I might still be safe. This all depends on F() and my password.
Now lets look at TOTP. It is also an authentication based upon a shared secret. Typically, that shared secret is randomly generated and assigned rather than chosen, and thus cant be guessed the way that passwords can. The protocol also uses an F(secret) and that F has additional parameters that include the present time and a validity epoch. Any given password is only valid during its validity period (duh), but there are plenty of times where theres also a grace period as well. The TOTP system has its own interesting characteristics that include:
* If I lose any given password, my exposure is limited to the validity period (and any grace period). If you know that a TOTP password I used while typing this message was 830346, it does you no good now. Moreover, theres no easy way to derive future valid codes from knowing past ones.
* If the site loses the shared secret, Im hosed. The only recovery is getting a new shared secret. Arguably, this is easier than getting a new password, because I dont have to remember it, I just need to set it up in whatever app Im using. Also arguably, this is cold comfort.
Note that theres almost an opposite set of characteristics here. A password has nearly unbounded downside if lost, but bounded downside if the DB is stolen. A TOTP code has bounded downside if lost, but unbounded downside if the DB is stolen.
In operation, authenticating password+TOTP is both stronger than either and as strong as the weakest. The TOTP is nice against protecting against the loss of a password. If an attacker gets both my password and the TOTP, use of the combination is bounded by the life of the TOTP. Thus, operationally, I can be more careless with my password against an attacker who breaks into my session (by breaking TLS, using a key logger, etc.) because losing that data stream is protected by the security of the TOTP. However, if the attacker steals the database, the TOTP is toast yet they still have to brute force my password. If the site can somehow securely store my TOTP secret, this is mitigated, but usually if the password database is exposed to some access vulnerability, the TOTP secret is going to be right next to it.
Is this more clear?

@_date: 2018-06-02 23:48:24
@_author: Jon Callas 
@_subject: [Cryptography] Security weakness in iCloud keychain 
I agree with you, but I had already assumed that they were going to grind passwords well, which is a bit much to ask. I expect that the TOTP secret is going to just be somewhere.
Having the TOTP system running in its own place would be good, but its hard. Yes, the hardware would be cheap (heck, if you did nothing but have a server with a database and a REST service it would be most the way there), but you have to keep syncing it with the other database as well as deal with revoke-and-reissue, as well as the inevitable problem of that server horking up a hairball either transiently or for good.
You could certainly come up with a thing that did signing. An ephemeral key derived from the secret, or just use HMAC would work. It is, however, more complexity for an undefined benefit. I dont know what that actually protects against.

@_date: 2018-05-03 23:44:11
@_author: Jon Callas 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Perhaps I can help a little bit.
I realize that your comment above has nothing to do with the poor guy's problems, but logging your machines in to iCloud (and enabling "Find My Mac") doesn't *prevent* the anti-theft features, it *enables* them. If someone walked off with your computer and then you declared it to be missing, it would disable itself.
Before moving on, I typed "Mac anti-theft lock" into Google and found a number of articles about Find My Mac along with third-party alternatives. The flip side of that  the tale of someone who's Mac was stolen  is at:
These features cut both ways. As the above article notes, there's a lot of theft of computers, and a feature that that deters theft has all the obvious downsides, too, like the poor other guy's issue, where he bought a computer that later got marked as stolen.
Anyway, let's get on to the keychain.
Have you considered a longer passcode?
And yeah, that's the feature.
However, the password manager stuff appeared long before iOS 11. Parts of it were buried in the midst of settings and are exposed at a easier-to-find level with iOS 11.0 (not 11.3), and were extended so that a password that you have for a web site is now available in the associated app. I find this really convenient because it means I can have a long password on a web site (like Protonmail) and then when the app wants that password I don't have to go find it and laboriously type it in.
What it would have would be all the passwords from your account.
If you had a Mac in your house that a lot of people were using, e.g. an iMac with auto-login, then yeah, all the passwords that people typed into the keychain on that Mac would be there. But it's only ever going to be for that one account.
You could get rid of them from your iPod by "Erase Contents and Settings." They'll be gone completely.
Definitely iCloud Keychain.
Well, okay. I don't mean to gainsay you, but there's no other way for them to get there. It's possible that you turned on keychain syncing and then turned it off again from your Mac, but that would leave the keychains synced after you turned it off on the Mac. Incidentally, "iCloud Keychain" is perhaps a misnomer. The keychain items aren't stored in iCloud, they're synced directly between members of a keychain circle, end-to-end encrypted while they are in transit. They pass through iCloud as a transfer mechanism, but they're not stored there.
Personally, I recommend that you use iCloud Keychain.
No, they're encrypted to keys that only you have.
Okay. It uses elliptic curve keys and end-to-end encryption. More below.
Yes, it is known. I'm sorry you couldn't find anything on the web. Allow me to help.
The iOS Security Guide has some decent descriptions. You can find it at  (I found it by typing "iOS security guide" into google, myself.) Take a look at the sections on "Data Protection" as well as the keychain itself and the iCloud Keychain. The description is high-level, but reasonably complete.
I typed "iCloud Keychain" into google and the top hit is:
The second hit is:
And going further down the page there are:
And further down there are some tutorial videos on YouTube and more. Some of these articles are a bit old, going back to 2014 or 2015.
Help me understand what the scandal is. I'll file bug reports for you.
One has to turn on the iCloud Keychain. One has to enter each device into it with one's iCloud password, and go through a small ritual to approve every new device. I understand that you didn't do it, but someone did and that someone had your iCloud password.
Indeed, I agree that if you have an iPod that is syncing with your other devices and it only has a four-digit passcode, then that is a vulnerability. But it's also a backup, too, since if you had other devices lost you could reconstruct the whole thing from the iPod and all those passwords would sync back to your new device.
If you want to remove everything from the iPod since you did say that it's a sacrificial device, then erase it using "Erase all contents and settings". It's very, very secure. Look in the iOS Security Guide for the details of how this gets done.
Feel free to write me off-list if I can help with anything else.

@_date: 2018-05-07 23:04:48
@_author: Jon Callas 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Hear, hear.
I think that we really need some people to write software to make passwords easier to use and maintain.
Human beings have a tendency to think that security must also be hard. We instinctively think that if something was easy, it must be insecure, and that things that require learning and ritual are secure themselves. This is why we continue to be stuck with arcane command line programs and so on.
Any system that is ultimately a shared secret is equivalent to a password. Many two-factor systems are at their core no more secure than a password. Here's a quick example.
TOTP is just a shared secret system. It has the advantage over a naive password that intercepting it and reusing it is blocked off, but that's not the threat. The adversaries are not breaking the TLS that carries a password, they're hijacking the database and going from there. As a quick example, suppose a server has a password (a shared secret) and a TOTP seed (another shared secret). Together, they're just one larger shared secret. If an adversary steals the database, they can log in as any user. One presumes that the TOTP seeds were randomly generated, and providing that they were, it stops a loss of the password to other sites as they need both the password and seed (which is different on a different server).
However, in this case you could always just eliminate the password and just have a TOTP secret. It's not really any less secure. And in most cases, a randomly generated password itself is mostly good enough. Repeating myself, if you break the TLS connecting the client and server, you can replay the random password, but that's rare.
Let me wave my magic wand and create a password manager that generates random passwords automagically. It's almost as good as password+TOTP. If this manager could go out and change the password for you automagically as well, then as the life of any given random password approaches a single login, then that simple password system approaches the security of that type of two-factor, while gaining the benefit that a stolen database of shared secrets has ever-decaying usefulness, which lowers the incentive to hack that database in the first place. Single factor with automated change is arguably better than two-factor.

@_date: 2018-05-09 14:15:31
@_author: Jon Callas 
@_subject: [Cryptography] Security weakness in iCloud keychain 
Have you read the security document? Based on what you're saying, I have a hypothesis about what's going on, but you don't believe me.

@_date: 2018-05-20 22:11:18
@_author: Jon Callas 
@_subject: [Cryptography] Non-deterministic PRF as a MAC-and-Nonce for 
I confess that I don't understand what you're saying.
If you have a non-deterministic thingie, then it's likely not a function. However, let's say that you have a function F(x) and that for any given x, we can't predict what F(x) is, and that the output of F(x_i) is mathematically pseudo-random, but F(x_i) is always the same value, and it doesn't change every time you call F(). You can't predict what the function value is ahead of time, but once you compute it once, it's not going to change. Does that make sense to you?
If you have such a function, then it is precisely what we mean by a Random Oracle. Ideally, a MAC acts like a random oracle, but remember that a MAC is a function of a key and the data, so it's not exactly the same thing.
I confess that I don't know what you're trying to do. Yes, if you have a MAC, it should function as a random oracle, so sure, it's also an NDPRF as you named things. But there are NDPRFs that are not MACs, as well, even though I don't have one off the top of my head, I am sure I could construct some pathological function that met your requirements but wasn't secure. Again, what problem are you trying to solve? Why not just use a MAC as a MAC? They're hard enough to build.

@_date: 2018-11-20 17:17:19
@_author: Jon Callas 
@_subject: [Cryptography] Buffer Overflows & Spectre 
I understand what youre saying, but here, irony perhaps, but not betrayal, no.
Ive been dealing with this for a year now, and its not quite what your saying, if I understand you. Its nothing to do with buffer overflows, but with memory disclosures. Its a sidechannel or covert channel issue.
Your metaphor about automobiles doesnt quite hit the mark. In the case of the emissions testing that was an intentional thing put in to fool regulations. CPU manufacture is not regulated at all, and Spectre is so unlike emissions or even braking that its hard to comment about what would tune the metaphor. Its not fraud, by any sense, since theres no intent there. Moreover, there are no direct analogues of such a problem to automotive technologies. (Note that I didnt call it a bug; its not a bug, it is an emergent consequence of design.)
Out of order instruction execution first appeared in the IBM 360/91 back in 1968. In our world, it appeared in PCs with the PPC 603, and in x86 CPUs with the AMD K5 and the Pentium Pro. Back in the day, there were some Multics papers about covert channels (Paul Karger did some nice ones) related to speculative execution, but by and large no one really considered it an issue. Its been a mainstay of CPU design for well over twenty years.
If one wants to rail about something, it would be the misplaced use of metrics and the truism that if you cant measure it, it doesnt exist." Were in this mess because we care about performance (which we can measure) more than anything like security (which we cant). I think its far more like the present mess with C compilers where the compiler people have gone and created so much undefined behavior that C is no longer a low-level language. There was a time where the problem with C was that since there was little to no optimization in the compiler that it all fell to the author to write efficient code (which was often silly because the other major facet of C was its alleged portability). As C delivered on the portability promises and started getting into more optimization, that led us to the security problems of today where it is very hard to write a program with no undefined behavior. Were up to nearly 200 different types of undefined behavior.
Anyway, getting back to this, the problem we face is how to make systems that are faster without these side effects. The promises that CPU designers have been giving us and also we demand is what gets hand waved as Moores Law a shorthand for seeing continuous, regular performance gains over time.
On top of this, theres no easy way to fix pipelining to make this go away. Even getting rid of speculative execution doesnt make it go away, since it is timings that cause the memory leak. Even getting rid of caches doesnt fix the problem because RAM often has its own internal caches (which cause timing differences) and even bare RAM has timing differences caused by locality of reference. Getting rid of high-resolution clocks doesnt fix it, either, because a spin loop is a close enough approximation to a clock to make it work.
The only real ways to get rid of this is to go back to an architecture where the CPU clock is the same as the memory clock, there are no speedups in memory fetching nor in pipelines, and likely more. Ill bet even that doesnt do it.
There are people thinking about things that could fix this, some of which are outr, and some are complete rethinks of computer architecture, like flat address spaces with memory tagging.
This is far, far worse than most people think because it goes to the very core of the fact that any sort of a distinguisher is a side channel.

@_date: 2018-11-20 22:54:57
@_author: Jon Callas 
@_subject: [Cryptography] Buffer Overflows & Spectre 
Yeah, thats pretty much it.
Arbitrary, hostile code can dump all of memory. The end.
Thats one issue; or its an issue that has different facets. If youre running servers (etc.) then clients can spy on each other. If youre running client software (oh, like a web browser running Javascript), then you have to be careful that the JS doesnt manage to get a covert channel on other things.
I can think of other architectural changes one can make that would mitigate this, if were allowed to go back to square one on everything. You could, for example, separate out cores completely and have essentially one job-set per CPU and allow swapping across CPUs. You could come up with memory allocation and deployment schemes that get rid of the problems. They are all, however, unproven and they all lower performance. There are ways to tweak the existing architectures to get rid of the problems, but they all hurt performance, and even then they might not work, because were starting to understand that merely allowing two processes to share the same RAM opens up the potential for these side channels. Its possible that the only *real* fix is single-threaded, clockless architectures. I dont know, myself. Its all a huge mess, and the more one studies it the more interesting it is, where interesting is a euphemism for broken."

@_date: 2018-10-08 09:44:47
@_author: Jon Callas 
@_subject: [Cryptography] In memoriam: Edgar Allan Poe 
Less well-known, but arguably more important is Poes A Few Words on Secret Writing from Grahams Magazine, July 1841. The article can be found on the site of The Edgar Allan Poe Society of Baltimore:
Also, while there is a generation of cryptologists who were inspired by Kahn, before Kahn, the usual thing that pulled people in was Poe, either from The Gold Bug or Secret Writing.

@_date: 2018-09-05 15:56:52
@_author: Jon Callas 
@_subject: [Cryptography] WireGuard 
And others of us consider that to be its biggest feature.

@_date: 2019-12-11 14:05:08
@_author: Jon Callas 
@_subject: [Cryptography] "[CVE-2019-14899] Inferring and hijacking 
If there are two lengths, a packet length and a data length (note that looks a lot like a slice in Rust and good old-fashioned string descriptors) You can easily do the same thing, decrypt the head of the slice and then not decrypt the unused data. And of course, you need to at least clear the unused memory before encrypting.
Mayyyyyybeeee... In general, yes, but Gershwin's law ("It ain't necessarily so") applies. Note also, that this puts demands on your partner. If there are no hints in your initial handshake, then the decoder has more work to do. It also enables an adversary to play games that they couldn't otherwise play. Nothing here is a show stopper, merely that that goal introduces new considerations. One might ask which is better and not really have a decent answer.
Again, maybe.
A decade ago, Ricardo Bettati and others did traffic analysis on VPNs. They started by making a constant-traffic system and then they discovered they could attack their own system and others. They used timing -- just simply track each packet and the time it came in -- and discovered that it was often *easier* to peer into a constant-traffic network. They used their technique to be able to decompose a VPN flow. They could tell what portions of the VPN were web traffic, media traffic, and others. It was often easier to do the separation on a constant-traffic network.
The way Bettati described it to me, think of yourself sitting on a hill looking at a bunch of boxcars coming out of a warehouse. Some cars are full, some are empty. Even though they're coming out at a constant rate, it's still not an invariant. Inside the warehouse, you're sending out empty cars and then dropping in the full cars. Some work has to be done and it is very hard to do that in a way that has no side-channels. For example, you might notice that the inter-car distance between full cars is different than the distance between empty cars. The distances might glitch when you switch types and you can detect that glitch. They found all of those happening and could use it to categorize the traffic.
In short, constant-traffic networks give you a metronome, and if that ticking goes a bit rubato on you, that's a signal that a passive adversary can use. Perhaps even more counterintuitively, the better your clock the easier it is to tell when it wavers.
Yeah. Blurring is always at the mercy of large numbers. That's kinda-sorta the message above even, though perhaps with the polarity reversed.

@_date: 2019-02-21 15:22:16
@_author: Jon Callas 
@_subject: [Cryptography] How widely are the PSK modes used? 
I think this gets to the core of the issue. As John Denker points out, just about every WiFi setup in the world is using a single pre-shared key. You can get per-link keys, but you have to set up Enterprise WPA and do Radius and stuff like that. Its much easier to have a PSK and go.
In contrast, TLS is *easy* to use with ephemeral(ish) keys; its the usual way we all do it. You *can* do TLS with PSKs, but its hard to set up. Thus, you rarely see PSK with TLS for the very same reason you usually see it with WPA.
Thus, Im curious about what the question behind your question is. In TLS, its hard to set up PSKs, its not the normal user experience, and consequently its rare. If it were easy to use PSKs with TLS, wed see it a lot.

@_date: 2019-02-22 12:37:08
@_author: Jon Callas 
@_subject: [Cryptography] Best way to create a MAC from SHA3 
Each of the SHA3 finalists had as a feature that a keyed hash is as good as a MAC. Keccak and Skein explicitly had one-pass MACs in them. Keccaks one-pass MAC evolved into SHAKE and Im not an expert on where else it might be around.
Thus, I first ask, Oh, really? SHAKE is too much? then agree with you that HMAC is overkill (I mean, if you dont want to use SHAKE, you really dont want to HMAC), and observe that a keyed hash is pretty likely good enough, and I bet you can get a proof of security for it, even if the proof is a hand wave.

@_date: 2019-02-22 12:40:35
@_author: Jon Callas 
@_subject: [Cryptography] How widely are the PSK modes used? 
I only disagree with necessary and might say useful so yeah. I can also see some session-resumption things that could be layered on it.
My pleasure. Let me turn this on its head. Even though you and I dont see completely eye-to-eye, we agree there are a number of use cases for it, even if theyre not presently widely used. So why get rid of it?
The purpose of a standard is interoperability. A standard is a formalish language in which you and I can independently write software that we have a basis for expecting them to work well together. There is thus pressure to allow options people find useful (the MAYs) and a tight set of requirements for a useful minimalist implementation (the MUSTs). These are in tension, and the best result is something like the old Einstein aphorism of simple as possible and no simpler. That guidance is both excellent as a guide and useless as guidance. Its inspirational and not actionable. Its why we have these discussions, really.
From your examples, we know that PSKs are useful as a mitigation against lack of randomness as well as for higher-level protocols (like some fancy session resumption, optimization thing). Sounds like the case for keeping it has been made, to me. Theres another discussion about where it should be in the MUST/SHOULD/MAY cascade, but thats a different issue.
Whos using this? Is a valuable question. It shouldnt be the only one. There are plenty of awful security things that everyone is using and they should go the way of all things *because* everyone is using them. There are plenty of pretty bits of gingerbread that should be kept because of potential, or even as a safety net. A bit of (to me) obvious (and unfair) hyperbole is to take a vote on how many people are regularly using fire hydrants, and if we dont use them often, why not get rid of them?
It seems to me that the case for PSK is essentially that when you need it, you need it.

@_date: 2019-02-27 23:49:07
@_author: Jon Callas 
@_subject: [Cryptography] In the event of my death, master password 
Similar to what John Levine said, it should be on a piece of paper in a safe place, or an approximation thereof.
For example, a literal piece of paper in the safety deposit box with the will.
Im willing to consider some sort of computer thing, such as a secured text document or whatever. Yes, a number of these other options end up pushing the problem down to the turtle below you. (Whats the password for the encrypted text file? Well, its in the safety deposit box with the will.)
Definitely do not go anywhere near things like secret sharing etc. Those might work in some future world, but theyre just begging for trouble now. Remember that the rules of infosec are Confidentiality, Integrity, and Availability. You dont want to do something like maximize C and minimize A. Ive heard several stories about people who made that mistake and now do not have available the few thousand bitcoins they once had.

@_date: 2019-07-18 15:22:57
@_author: Jon Callas 
@_subject: [Cryptography] Digression: "Letterlocking" and URLs and avoiding 
List, I have a request for the future about pasting in URLs: please clean them up from tracking things.
I apologize for taking a specific previous post as my example here, but it's the proximate case for us. I, too, am a sinner too on this front. I try to do what I'm going to describe below and am often successful at it. Perhaps even usually successful.
Many URLs come with marketing tracking information in them. The base URL is everything up to the question mark, for example:
And if you click that you get to the site and everything is copacetic. I ask that when you send a URL, you simply cut off the question mark and everything after it. Bonus points for verifying that it still works. QA is important. That's it. Please and thank you. Please do this all the time, but especially here on Cryptography.
Yet let's look at what we trimmed. The rest of the URL is tracking information. Let me decompose the pieces:
The first piece tells us the it came from the Atlas Obscure Daily Newsletter (duh), and I too get the daily newsletter. Great to see another lover of Atlas Obscura.
In the second one, the hard work of Captain Obvious tells us that there's some hex stuff that's interesting in that it's 36 bits, not obviously ASCII or UTF-8, and wow, an email campaign for people not in NYC. I clicked on the link in my own email blast and this string was the same for me as it is here. So it's probably the lookup tag for this campaign.
I'll leave the meaning of the third one as an exercise for all us readers.
The fourth one has that previous string surrounded by two other strings. Interestingly, the prefix 0 has an underscore as a separator and the rest of it in dashes. I think this tells us something about the development practices of the organization that made it.
That first string, f36db9c480, is the same in my URL, so I presume it is also something global at least across the campaign, too. The third digit string, 63217145, is interesting in that it's not obviously hexadecimal. The one in my URL is similar in that it starts with 629 rather than 632 and also appears to be decimal. I'm going to guess that it's an account number or something like that.
The fifth, "ct" element is so redundant that even Captain Obvious moves on.
The mc_cid element is our old pal here for a third time, and I'll guess that it's a Campaign ID.
The last mc_eid, and probably the ID of something starting with the letter E.
On my URL, my mc_eid was different from this one, but it was the same across another link in the same email, and also the same in an email from July 16 to me as well. I'm going to guess that it's an opaque token for my email address, on no other basis than the invariance in my case and that email starts with an "e".
When I look at a URL from the July 16 email I received, the mc_cid is different, which I suspect, as it's a different campaign. The mc_eid is the same, as I mentioned before. The campaign string is "EMAIL_CAMPAIGN_07_16_2019_Not_Chicago" which is interesting and leads me to guess that they're doing some sort of A/B testing across metropolitan areas. The thing I guessed was an account number (starting with 629 for me) was also constant across the two emails.
Thanks for reading this far. Again, please clean up URLs by deleting from the question mark forward. Please do it everywhere, but especially here.

@_date: 2019-07-20 20:14:20
@_author: Jon Callas 
@_subject: [Cryptography] Our leader opines on cryptocurrencies 
I had a raised eyebrow, too. I know there's a *lot* of US dollars just lying around in one form or another. There's billions in change lying in drawers. There's less now than there has been historically. Those change machines in grocery stores, etc. put so much old coinage back into circulation that some number of years ago, the Mint furloughed 200 people because they didn't need to mint as much coinage. I spent a few minutes trying to search out a reasonable number and didn't get it. However, I did find that according to a 2011 article, there is/was $1.2 billion in dollar coins sitting around in banks, and that article says, "Officials expect the number of dollar coins sitting in storage to grow to $2 billion by 2016."
There's also a boatload of money sitting around internationally in $100 bills. I have no idea how much, but I'm sure it's huge on this scale, as well. If someone has numbers, please reply.
If the number of dollar coins sitting in the banking system's equivalent of the loose change urn on my dresser is 1/5 the way to a reasonable hat-eating bet, the scale here is the important issue.

@_date: 2019-06-11 11:23:45
@_author: Jon Callas 
@_subject: [Cryptography] Minimal secure boot 
Yes. You want something bigger than 3. Three can work, but you have to take care. It's simpler to pick something bigger.
Mathematically, 5 works just fine. Traditionally in RSA cryptography, 17 and 65537 are traditional because they're (2^n)+1. Knowing I don't know, why wouldn't you use OAEP or some equivalent?

@_date: 2019-06-11 22:48:43
@_author: Jon Callas 
@_subject: [Cryptography] Minimal secure boot 
Okay. If code size and performance are your major concern over security, why not just have an unsigned ROM? That will save you even more code and be even faster.
I realize I'm being a smartass when I say that, but we're talking security, and you're responding to security questions with answers that are performance and resource answers. If you respond to security questions with performance/resource answers, it means that your important issue is performance and resources. Since security is obviously Job Two if not Three, why not just say, "Gosh, I'd really like to have secure boot, but we can't afford it." That's a much better answer than mediocre security.
The problem with exponent 3 is that there are solutions to a number of cubics.
All exponent 2 systems are easily solvable with our old friend The Quadratic Equation. They're not secure.
There is also a Cubic Equation, which lets you solve cubics. The thing is, though, not all cubics can be solved, just some of them.  Thus, what you need to do to use exponent 3 is to show that your parameters don't have a cubic solution to them. I leave that as an exercise for you to go research.
Going on, exponent 4 is also not secure. I'm sure you can guess why. Euler proved that there is no polynomial solution for a polynomial of exponent 5 or higher (there are also exceptions here, like exponent 6, which I also leave as an exercise). Normally, cryptographic engineers don't like to use exponent 5 for really bad reasons. Those include that while they use number theory, they seem not to actually believe in it. That's why you typically see 17 or 64k+1. All of those  3, 5, 17, 64k+1 are of the form of a one bit, a series of zero bits, and then a one bit. Thus, they are faster than an arbitrary exponent. While there's nothing wrong with someone using 3 when they know how and why, you're much better off on the well-worn path.
Okay, there are places where PKCS is secure, and places where it is not. Which one is your case and why?
You would be much better off using a different exponent  like 5, 17, or 65537 (as stated above), which all have a hamming weight of 2, and OAEP.
Picasso is somewhat notorious for the aphorism, "When I run out of red, I just use blue instead." The thing about this is that if you're Picasso, you can use blue in place of red. If you're otherwise experienced, likely you can, too. You're coming to us and essentially saying, "Hey, I wanna use only blue so I can save money on paint." If you do that, there will be tears. Your painting is not going to look good. You need to learn how to follow the rules before you go break them. Picasso was also somewhat famous for periodically (every couple of years or so) doing an utterly straight, representational painting. He did it for the discipline, but also to how that he wasn't just doing Cubism or whatever because he no longer had it. If you came to us and explained that you were going to use exponent 3, PKCS and what all, and outlined a clear set of reasons why, and you wanted someone to comment, it's likely we'd ask a few questions and send you on your merry way. That's not what you're doing. You want to cut corners and not your fingers, and you want to start by cutting corners, not start by figuring out how not to cut your fingers. My advice is to come up with a completely secure, bog-standard implementation and *then* look at how you can optimize. Don't start off with optimizing. Start off with security. Be bold, and spend a bit more RAM and ROM on an implementation that you can describe in a couple sentences and a reasonably smart, non-expert person will find good enough. They're the ones who will be your customers and users, after all.

@_date: 2019-03-22 15:14:08
@_author: Jon Callas 
@_subject: [Cryptography] Best/simplest document encryption 
Best is such a hard word.
There are some decent GUIs put on top of GnuPG, which will encrypt to a passphrase. Theyre not *wonderful* from a UX standpoint, but its just fine and has decent encryption and compression.

@_date: 2019-05-02 14:18:48
@_author: Jon Callas 
@_subject: [Cryptography] Paper Link: 
I disagree, and even with my moderator's hat on. The Voynich manuscript is a document that people have puzzled over, debated if it's a real language or not, and many of the past analyses have used cryptanalytic methods, as well as the sort of information theory and reasoning that is common to discussions of cryptography. There are those who think that it is ciphertext. The question, "Is  ciphertext, plaintext in an encoding we don't know (and if so, what's the encoding), or just gibberish?" is a fine cryptographic question.
I wondered the same thing, myself.

@_date: 2019-05-10 12:36:48
@_author: Jon Callas 
@_subject: [Cryptography] peering through NAT 
With moderator hat on, I'm going to agree that extended discussion of NAT isn't particularly germane. Perhaps even more to the point, there are better resources than this list. (Also, I let through a top post I shouldn't have. Sorry.)
However, I don't think we've hit it yet. Yet.
I'll add in that NAT traversal has interesting security effects that are hard to characterize. It *changes* the way metadata leaks. Metadata always leaks: every packet has source and destination IP and port, and IP address is often a proxy for physical location. In the general case, if both Alice and Bob are behind horrid NATs (particularly nested NAT), it might be impossible to thread the combined NAT traversal problem, and the only way for them to talk is through a protocol relay, as it has a real IP address. There are many specific cases of complex NAT traversal that work well, and as time goes on, there are more of them, for all the reasons  stated by others. Router makers do a better job, client makers do a better job, and service providers are sensitive to the issues as well. There are thorny setups that used to be common but are not any more.
Naively one tends to think that relays are a security problem, but it is more complex than that.
If a service brokers a direct connection to Alice and Bob, then obviously, the service doesn't learn many details about their communications after brokering the connection. Whereas if they have to be relayed, the relay sees every packet and thus knows things like how long they talked, and the total data size. If they talk directly, the service doesn't know this.
However, if you consider the case where Alice is being surveilled and Bob is not, on a direct connection the surveillance system learns Bob's IP address at the very least, along with other things. If the connection is relayed, they know Alice is talking to a relay, but might not be able to learn who Alice is talking to.
This might matter in some scenarios, like secure VOIP (which is where I dealt with it), but also things like cryptocurrency protocols. It might actually be better for Alice to talk to a relay if one wants to protect Bob's metadata. It's possible that the trading protocols hide Bob's information, and yet a leak of his IP address gives the whole thing away.
In short, it's complex. There are counterintuitive things in all this mess, and dispassionate thought and challenging one's own assumptions is warranted.

@_date: 2019-05-28 17:33:03
@_author: Jon Callas 
@_subject: [Cryptography] The race to Quantum machines. 
As others have said, there's a lot to slip between the cup and the lip.
We should prepare for the eventual creation of quantum computers, of course, and this means continued investment in post-quantum cryptography, especially because as we deploy it, we're going to find interesting implementation glitches that have serious security issues. We'll get through them of course, but there's going to be a decade in which "hybrid" encryption where we use both conventional and post-quantum asymmetric algorithms will be *less* secure than conventional encryption because the weakest point is going to be the issues in the post-quantum.
Also as others have said, building bigger quantum computers is likely to get harder the more qubits one makes. The theory people say that the theory is all done and all that's left is "just engineering problems." Let us also remember that colonies on the moon, cities on Mars, and useful fusion power are also just engineering problems. Yet, let's cast those aside for the moment.
If we start with some basics, like IBM releasing 20 qubits this year, and assume a Moore's-Law like doubling of bits, we need ~20 generations of doubling to get to the needed ~20M qubits. If that doubling is every year-and-a-half, then we get there around 2050. If it's every two years, around 2060.
(For what it's worth, I have a standing bet that they can't break 4K RSA by 2050. I picked this because it's about where RSA runs out assuming Moore's Law continues in classical computers indefinitely.)
So there's where some reasoning can come from. Yes, a more efficient algorithm that decreases the number of qubits by 100x is good, but that's only 6.5 generations of doubling. The clock speed of the generation is more important -- if there's another 100x efficiency, but the doubling takes six months longer, then it's pretty much a wash. We're still talking about it really being practical in the 2050-2060 time period.
Far more important is that exponent. We assumed that it's 2 (doubling) per generation. If it's not 2 but the square root of 2 (~1.4), then we hit that point in the early 2080s, not the 2050s (assuming a 1.5 year generation). This is a completely reasonable thing for it to be; silicon has the nice feature that halfing the element size gets you 4x the transistors, not 2x. I made myself a spread sheet to calculate some of these numbers. I made variables for generation clock tick, generation increase, and just played around.
Now consider that their paper lets you break a key in eight hours. With a static TLS key for a site, that's powerful, but with ephemeral keys, there's still a pretty big haystack for those needles.
The bottom line, I think is that while yes plan for quantum computers, but it's not the cakewalk some claim it will be.

@_date: 2019-11-04 17:58:58
@_author: Jon Callas 
@_subject: [Cryptography] Very best practice for RSA key generation 
I agree that there should be testing, but it would be easy to construct a test that could give false or irrelevant answers. Let's assume that each word there is 15 bits of strength (in other words that the word list is 32K long). It seems obvious that the error rate on a 27 character entry would be higher than on 15 characters. Yet I'm sure that I can type "correct horse battery staple" with no errors (I just did it) more reliably than I could type 15 hex digits correctly. My brain can process 60 bits of entropy in words better than hex.
A naive thing people say about word-oriented shared secrets (I'm saying it that way for a reason) is that it's more secure, and you hit right to the crux of the matter, which is how security is measured in the face of user experience testing.
It's certainly possible to take that basic format and further make it easier. As an obvious example, you typed "Correct Horse Battery Staple" and it's obvious that "correct horse battery staple" is also valid. There's no reason why we can't accept reasonable typos or alternates such as "verum equus altilium stapulae" or "correct cheval batterie agrafe" or even "?? ? ?? ??" as each of those is just a different encoding of four fifteen bit integers. The tighter a coding is, the shorter, but also the less likely for human memory and typing to work well and the less for a helpful UX to error correct.
I made a quick check for an article I saw where people were noticing some very "high entropy" passwords that turned out to be really horrible passwords, but in some language and encoding the researchers weren't expecting.

@_date: 2019-11-05 09:58:24
@_author: Jon Callas 
@_subject: [Cryptography] Very best practice for RSA key generation 
Perhaps I lost the larger point in the rhetorical flourish, or perhaps I didn't go far enough.
The larger point is that we have a shared secret with some security value. Then we have an encoding of that secret. I picked 60 bits, and let's run with that because 60 is a nice number. That's 15 hex digits, or for a word passphrase, N words. If the list is 15 bits long, we need four words. We could do five words from a list 4096 long. Or six from a list 1024 long. We have no idea what the best thing to do there would be. The intuition that less typing is more reliable is questionable -- assuming you agree with my assertion that four words is *easier* than fifteen hex digits. It might be that words from a larger list (and thus more unusual) might be more memorable than more commonly used words. Or it might not. We don't know. That's a lot of what James and I were talking about.
I was adding to that that you might be able to add error correction to the typing task, and make the human's job easier. On a simple case of that, the sorts of error correction we get all the time can fix things. If I mistype the word "weird" (which is weird because it breaks the I-before-E rule of thumb) transposing I and E, the system knows what I meant. (And as a matter of fact, when I intentionally typed that misspelling here, it autocorrected.)
You could, for example, error correct "la pile" and "batterie" in French, but not error correct "cell" and "battery" in English. When I picked the Chinese words, the word for "staple" I picked was staple in the sense of a food, not staple in the sense of a metal thing you bind paper together with. Popping back to the larger point, a less efficient coding of a shared secret allows for assistive software. That assistive software can and should adapt to the human based on culture, language, etc. (As a side point, when I naively translated "correct horse battery staple" into French, the software I used considered "correct" to be a adjective modifying "horse battery staple" and threw it to the end of the words. This tells me that perhaps "correct" isn't a good thing to put in the French word list, as people are likely to misremember similarly.) So what we need is research, testing, and above all, some creative thinking from an interdisciplinary group, not just mathematicians doing coding.

@_date: 2019-11-07 14:10:43
@_author: Jon Callas 
@_subject: [Cryptography] Very best practice for RSA key generation 
Yeah, and to talk out of the other side of my mouth, you could always make them type it exactly. Certainly I'd code it up that way and then add in the error correction later.
Outsource that issue to someone else, then. There might be correction packages around or even OS functions you could use.
I found this:
Three thousand most common English words, listed in order, from newspaper statistics. Take the first 2048 of them, and then poof, you have an eleven-bit-workfactor/word list right for the taking. Or use them all, and then you have 11.55 bits per word.

@_date: 2019-11-07 14:22:36
@_author: Jon Callas 
@_subject: [Cryptography] Very best practice for RSA key generation 
And if you use only an English word list, you also have an internationalization issue, just a different one. I view internationalization as just another software engineering task that I get some qualified person for.
In our case, we have a relatively small one. We don't have to internationalize in a way that has semantic content, so whatever translation we do can be coarse. Yeah, there's still plenty of things to go wrong -- I am reminded of a hilariously bad manual translated into English, and one process needed you to turn a screw. The translation of "screw" was off-base enough that I childishly snigger over it to this day.
That's kinda what I'd do for a first approximation.
You only need to do it when there's a miss. Every time I type "battery" right, you get an exact match. It's only when I type something like "battere" that you need to guess. Or you could even just let me know that what I typed in isn't one of the words on the list.

@_date: 2019-09-17 16:40:52
@_author: Jon Callas 
@_subject: [Cryptography] TRNGs as open source design semiconductors 
Hear, hear. I use the word "unguessable," myself. I also repeat the apocryphal story that Von Neumann told Shannon to use the word entropy because no one knows what it means. Even if made up, it illustrates the problematic, faint, fuzzy thinking that we end up with because we don't have formal definitions.
Statistics itself has a lot of weirdness in it that alone will lead someone down a rathole of paradoxes and confusion. Let alone when you try to think about "entropy" or whatever else we want to call it.
A couple of weeks ago, The New Yorker had an article about statistics, Big Data, medical safety, and serial killers. As I was reading it, I thought it was valuable to this conversation:

@_date: 2020-04-03 17:15:55
@_author: Jon Callas 
@_subject: [Cryptography] "Zoom's end-to-end encryption isn't 
We are using Jitsi a lot at work, as well as Zoom. I find Jitsi to be perfectly acceptable. It's not as polished as Zoom, but a lot of that is because it is not regularized and is working with various devices of different kinds.
As a co-author of ZRTP, yes, it does end-to-end. That still doesn't tell you anything. If you have E2EE connections into an unencrypted mixing server, then it is not really, really E2EE.
At Silent Circle, we did mixing on one device. So one participant in the meeting was the mixer and everyone did a call in to them. Thus, you wanted the mixer to have fast Internet, but everyone else only needed one stream.
People have over the years, I don't know about recent things.
For what it's worth, FaceTime is also end-to-end encrypted, but you need all Apple devices, and it maxes out at 32 people.

@_date: 2020-12-19 15:20:44
@_author: Jon Callas 
@_subject: [Cryptography] BitCoin as Quantum Cryptanalysis canary. 
I am not quite sure I quite understand your question/comment.
Unpacking it, I see a few things:
(1) I think it is important to remember that bitcoin was created as a payment system, not an investment/speculation system. The Bitcoin Game as I've described it in the past, rewards people who create cashflow by giving them cash to flow (that's what mining is). I know a number of people who were early in the game who used their payouts to buy holiday presents from flat-screen TVs to cashmere socks (and this was in the days when the TV would have cost about 1000btc and the socks 20btc each), tuition for advanced degrees, needed yet "cosmetic" surgery, and so on. It's important to look at all those through the lens of thinking of bitcoins as a payment system that might fluctuate, but it wasn't an investment instrument.
If you're thinking that way, then orphaning an early block of coins is not really a big deal. Today, when people treat it primarily as an investment, and at best secondarily as a payment system, then you look at that stash as some sort of lost treasure. I think that while it is in theory treasure, it is almost certainly lost.
Think about it -- you want to set up a payment system and in firing up the servers, you glitch and lose the initial block of tokens. You can either start over or keep going. Starting over has a weird, scary reputation cost. A payment system is only as good as people's belief, and if people wonder if you might start it over then they wonder if they should stick their actual money into this thing that might just get restarted. It's turning coins into bits, and wondering if those bits are going to get zeroed. Also, let's suppose you don't realize you lost that block of until a day into having fired up the game. It's inconvenient to go back as well as a reputation hit. It's easier to move on. It's even easier psychologically because your mindset is that this is a payment system and early in the game they're like potato chips not only in value, but because you can always make more. It doesn't take long before it is literally easier to just make more than to go back and deal with the ones you lost.
Most thought about them presumes that they knew then what we know now. It's the classic problem that history is very different when you look back rather than forward.
(2) Should we get a gadget (like a quantum computer) capable finding lost coins, we also have a gadget capable of unlocking any chunk of coins. The effect on unlocking a few billion dollars of moolah, almost *must* be a collapse of bitcoin. Even the most cold, unpanicked rational analysis of this sets the maximum value of a coin. You end up with some sort of hysteresis and minimax of the cost to mine a coin and the cost to conjure the key to one (or a group of them). Bitcoin is a payment system, a speculation system, and also an information betting network on the question of the existence of practical quantum computers. Holding a bitcoin is making a bet that there are no practical, high-speed quantum computers.

@_date: 2020-12-21 13:15:36
@_author: Jon Callas 
@_subject: [Cryptography] Cryptographic archive format 
Also research tar files, and other zip-related things like jar, etc. There are lots of them around.
Research as well things like "bundles" in Nextstep and its descendants. Meaning macOS, iOS, etc. A bundle is a directory that contains subdirectories and the thing is treated as if it were a single file. An "app," for example is a bundle.
Also go look at chroot jails, and all their relatives like "containers" and even things like Docker, Kubernetes, etc.
I think you've argued why compression is not needed, not anything else. The compressed archive formats have two pieces, the archive and the compression. This is another reason why I think you should look at tar.
Actually, what you need are precautions in undoing the archive. As you note next, someone could create a malicious archive.
Yes. This is why you need work on pulling things out of the archive, not putting them in.
I think a reasonable rule is that when you unpack, you make a subdirectory and everything goes in it. It's legal to go down (making a subdirectory in your base directory for extract) and never up. If you do that, then you need a predicate that answers if the hypothetical target is in that subtree. If not, you put it in the base directory (or make an error directory and put all your errors there).
I think you're making it harder than it needs to be. Yes, you need an archive to work with in a subtree and never go out, I'm not sure you need anything else.
And really, I'm sure you can steal this from somewhere.

@_date: 2020-02-13 11:35:41
@_author: Jon Callas 
@_subject: [Cryptography]  
=?utf-8?b?ZSBjZW50dXJ54oCZIg==?=
Oversimplifying slightly, Rabin is like RSA but where p == q, meaning that it's p^2 rather than p*q.
All of our public-key cryptosystems have had growing pains that I'll call engineering considerations. The math hasn't changed at a math level, but at a security level, it's not quite as simple as the math. With RSA, for example, it took us years to figure out padding, and once we figured out padding, there were more years to get it right. (Look at the OAEP history.) There are DH issues with picking a generator (e.g. 2 is a great generator for encryption, but not signing). With ECC, we learned that the math done mod p is secure in a way that it done mod p^n is not. In plainer language, it would be really nice to do a "binary" curve, where we use normal binary bignums as opposed to mod some prime that's really close to some 2^n. NTRU went through lots of iterations before we thought we had it right and this grander issue pervades a lot of post-quantum work and is why cautious steps into hybrid schemes are a good idea.
By the time the fiddly bits in Rabin got sorted out, RSA was dominant. Zix email encryption used Rabin at one time and might even still.

@_date: 2020-02-15 15:31:29
@_author: Jon Callas 
@_subject: [Cryptography] 'The intelligence coup of the century' 
As I understand it, if you have Leet COBOL Skilz, you can make a really nice paycheck. Wikipedia says:
   "In 2006 and 2012, Computerworld surveys found that over 60% of organizations
   used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL
   was used for the majority of their internal software. 36% of managers said they
   planned to migrate from COBOL, and 25% said they would like to if it was cheaper.
   Instead, some businesses have migrated their systems from expensive mainframes to
   cheaper, more modern systems, while maintaining their COBOL programs."
   "Testimony before the House of Representatives in 2016 indicated that COBOL is
   still in use by many federal agencies."
So yeah, I'm quite sure. There is also COBOL-2014, so people are still working on it.
I can't say definitively, but VMS had a time system with a 64-bit value and that provided an easy update path for anyone doing COBOL on older systems. I know that COBOL migration to VMS was a big thing even back in the day for maintainability. The OS was spun out from HP to VMS Software Inc.  and they're working on an x86 port. They have a COBOL port on the task list.

@_date: 2020-02-16 16:21:45
@_author: Jon Callas 
@_subject: [Cryptography] SSL Certificates are expiring... 
He did say that, but the idea of a Trusted Computing Base (TCB) goes back to Orange Book, MULTICS, and many documents before that. That's the same use of "trusted" to mean that it axiomatically must work correctly or security fails, and therefore we have to trust it.

@_date: 2020-07-29 11:34:20
@_author: Jon Callas 
@_subject: [Cryptography] Cryptographically securing a two-phase commit 
I'm not sure this is even possible. Let's just model this abstractly. You have a bit stream, and the last bit is bad. We can't know the stream is bad until we receive and compute on that last bit.
Let's also assume that every bit is authenticated. This can let us know that it was precisely that the last bit and only the last bit was bad. (Ironically, in the case of bit-level, as opposed to some larger chunk, we also get error correction, since knowing the bit is bad makes guessing the correct value easy. It's only a factoid, but I still was amused by it when I realized it.) We still have to make the entire authentication contingent on getting and computing the last bit.
So my naive take is that this is just flat-out impossible. It is always possible for an attacker who can arbitrarily jigger your inputs to DoS you. However, I think that there are countermeasures that are at least possible. I think you could use some sort of TTL or timestamp or serial number to make this harder for the attacker. You can do some trick like your up-front authentication declaration and keep track of valid things seen in a time window. My mental design on this leads me to think I can restrict them to replaying only the last thing seen, which is easy to look for. If you were evaluating c_0, c_1, ..., c_n and and while starting to evaluate c_n, you can implicitly know that any replay of c_j where j < n is a replay and just reject it.
However however, consider the case where the attacker doesn't modify that last chunk, they just drop it on the floor. You've had to compute everything up to that point and now you see what is apparently a dropped connection. The attacker can then basically replay by saying, "Sorry about that, where was I? Oh, yes, I wanted to send you this nth commitment" and start the whole thing over again and then lather, rinse, repeat. They get to DoS your this way, by emulating an arbitrary broken connection and restart. In many cases, the attacker doesn't even need to emulate it, they just let you compute some section of the commitment and then force a restart. I can discuss more, but I think we have the point. The attacker DoSes you with the present connection and can in your chained system just keep replaying something to force you to hold state open.
So in the general case, I don't think this is possible at all; forget formats. I think you can restrict the attacker to putting you into a retry loop, which any sufficiently evil router can approximate anyway. That's cold comfort. That final thing is really the easiest attack (compared to replays etc.). Am I missing something?

@_date: 2020-06-29 10:23:43
@_author: Jon Callas 
@_subject: [Cryptography] Statement from Attorney General William P. Barr 
Yes, but that was for the purposes of export. Also, it was 23+ years ago -- encryption was moved from ITAR in 1997. Even if it were true then, it isn't now.
Encryption has never been regulated internally for almost all purposes. (Ham radio can't use encryption, but that's not only just for radio, but internationally it was. Even here, there are complexities that I can't describe both succinctly and accurately.)
However, even back in the day, my then-government affairs people said it was best not to go there, because we might not like what we'd get -- for example, we might get total unrestricted encryption, along with mandatory key escrow.
The present argument that they are making is that encryption should not be "warrant proof" and that would go away, too. They can issue a warrant to look for firearms and get them when they've been used in a crime. I don't see what would prevent the present attack on second amendment grounds.
Popping the stack back, though, encryption-as-armament was something that took place at border crossings, and was a designation that many other non-armament things had and even to this day have. Like GPS.
There was an old paper at I think UVA law school about how encryption should be considered an "ancient right" -- something that is so old in our understanding that it never had to be stated. I think that has far more legs than the second amendment. In any event, though, they are not banning encryption, they want to be able to get to information with a warrant. That's their viewpoint. It's not the encryption, it's the information, and they are arguing that the technology of encryption should not prevent a search or seizure when authorized by a warrant. For that, even, fifth amendment arguments are better and are even making traction. We just won one of those a week ago.

@_date: 2020-03-03 14:51:20
@_author: Jon Callas 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
You're conflating a wide number of things here.
Some of them are things over time. For example, there are things that were true a long time ago (e.g. most of the things people will tell you about disks were true in the 1980s. The typical thing one hears is absolutely true about RLL disks, which were the way things were done in the late 1980s and not at all today). For example, disks in years past recorded their magnetic domains digitally (by which I mean they were recording ones and zeroes) horizontally, but now they're recorded vertically using analog recording -- yeah, these days, it's closer to a modem. Today, there's no such thing as "layer by layer" on these media. Some of them are true for some types of media (like tape) but not about disks (and let's not get started about flash, which is a whole other complex set of things entirely). Some of them are true about recovery of deleted data and not true about truly overwritten data (many times people think they overwrote the data, they didn't), and really, really not true about a disk where someone has opened up the drive and done something as simple as removing the platter(s) from the spindle.
Modern considerations are very complex because modern drives write data so tightly that it's hard to read it back in a lot of cases. This leads the vendors to do a lot of things that both leave recoverable data all over the place and make recovery very hard. And of course, if some cryptography is thrown into the mix (like some form of full disk encryption) it gets even more complex. It's all so complex that I could write several thousand words on the subject and actionable advice would still make one's head hurt. If you go to Scott Moulton's site, , he's got a 21 page white paper, 90 pages on rotating media, and 100 pages on SSDs. They're all worth reading and I have learned lots from him over the years.
Anyway, categorical statements have enough caveats on them that they can quickly go from reasonable to not-even-wrong in the space of a glib clause in a sentence.

@_date: 2020-03-04 14:27:52
@_author: Jon Callas 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
For their purposes, I think it's a reasonable thing. However, it's reasonable more from a human-compliance issue than anything actual. Data destruction has two parts. The first is destroying the data; the second is convincing yourself and others that you destroyed the data. A bag full of metal shards is pretty convincing.
Modern disk drives become completely inoperable when you degauss them. The write heads don't have enough magnetic flux to be able to reapply the basic media format, so when you degauss it you've effectively destroyed the disk. The heads literally can't write the media. There is also no good way to re-format the platters because you'd to disassemble the drive, use powerful enough heads to format, which also need to match the read heads you have.
It's not uncommon for manufacturing runs to make changes for some reason to internal components. A challenge for data recovery is that often one needs a donor drive for components that is part of the same manufacturing run. (Anecdote: I made a large NAS server not that long ago, and talked to Scott Moulton for some advice and he recommended buying an extra spindle to keep on a shelf. The reasons are both that if/when you need a replacement you have one right there, as well as having a suitable donor should you need that.)
Nonetheless, imagine that I have two drives, one that has been degaussed and one has not. The only way for us to tell which is which is to hook it up to some system and try to use it. "Are you sure you fried it?" is a question that is hard to answer with degaussing, but if I shredded the degaussed drive, a reasonable person could tell it from the one that still has data on it.

@_date: 2020-03-04 14:49:51
@_author: Jon Callas 
@_subject: [Cryptography] Possible reason why password usage rules are 
I think that's a reasonable folk etymology. I also think that our conception of password use and how they're guessed has changed, and that has changed the way we look at the problem.
It used to be that the threat model for password guessing was a human sitting at a keyboard guessing things. The model for creating a password was making something that the owner can remember. It also included a model where one might have good reason to tell someone else one's password even though we'd cluck our tongues and shake our heads sadly at that. That resembles authentication words, and changing it forces the person to re-evaluate who they've shared it with.
Today, the threat comes from automated password crackers that have a dictionary of the umpteen gazillion most common passwords. The use model includes password managers. Thus, creating a password with bad human-UX and never changing it is a decent model.
I'll guess that there is at least parallel evolution from authentication words to password policies, if not outright inheritance.

@_date: 2020-03-04 15:17:46
@_author: Jon Callas 
@_subject: [Cryptography] Ex-CIA Joshua Schulte Describes His Data/Crypto 
That is indeed one of the considerations -- spare blocks for remapping -- and even rotating media does that these days. Drive makers have huge incentives to push what's possible, and large numbers of spare blocks cover many sins. It also gives a mechanism to signal to the OS or diagnostics that the drive is failing but not yet failed. Remapped blocks are almost generally being fussy about being written, but still can be read. Thus they have potentially dangerous data that cannot erased. On all bleeding-edge technologies, it's hard to say anything definitive because they're actively changing. My understanding is that SSDs in particular are still so bleeding-edge (especially in high-capacity, high-speed cases) that extrapolation from any isolated fact is hard. From rumors I have heard, your 7.3% seems on the low side. I've heard loose talk that some high-reliability drives might be much, much higher, particularly when there are manufacturing changes.
For example, let's suppose we have a manufacturing line that makes 2TB "datacenter" drives and another one that makes 6TB "desktop" drives. Let's also suppose that each drive costs $50 to manufacture and sell fors $150. Now let's suppose we want to start to build 10TB drives with a new process. If we put new firmware on the 6TB drives so that they have 2TB active storage and 4TB of spare space, we can almost certainly hit the reliability metrics of the datacenter drives using the same guts as the desktop drives. That's the sort of crazy talk I've heard that sounds plausible and while I have a raised eyebrow, I've seen wackier things in my day.

@_date: 2020-05-24 15:21:53
@_author: Jon Callas 
@_subject: [Cryptography] Improving MITRE's description for "CWE-329: Not 
You're pretty much right. This is an awkward assessment.
If we look at CBC mode, it's simple. We take the IV, XOR it onto the plaintext, and then encrypt that. The result is both the output and the IV for the next block. Lather, rinse, repeat. Let's also assume the key is just fine.
If the initial IV is zero, then, yes, that first block is essentially ECB mode. On the other hand, the output is completely unpredictable -- predicting it is an attack on the underlying block cipher. If we never reuse keys, there's not a lot of place for an attacker to get a foothold.
And yet, it is a flaw. In the immortal words of Bob the Angry Flower, "No! Wrong! Totally Wrong! Where'd you learn this? Stop doing it!"
Your intuition isn't bad. The comment about dictionary attacks is maddeningly off-base. A single encryption with a random key and sucky IV is still pretty secure. Nonetheless, don't do that.
I can think of examples where there might be a problem. For example, let's suppose we had a disk drive with 512-byte blocks and we used the block number as the IV. In this, we have *related* IVs, and it's not hard to think that there'd be a case where (Block_i \xor i) is equal to (Block_j \xor j) and that we could detect from the ciphertext. Moreover, since i and j are known, we learn something interesting about that first cipher block of plain text in the disk blocks. I can imagine some pathological cases, too -- suppose the first plaintext was a counter of the same size as the cipher block. In this case, the first cipher text would be the same in every block. It would also leak any time where the second plaintext block in two blocks were the same (and cascading onward). This could even be a surprise, because the people doing the plaintext data layout would never have considered that the crypto people would have done something as gawd-awful stupid as using the block number as an IV. Note, though, that we're still not leaking any (much?) plaintext, and hard to characterize how bad the flaw is. In all the cases where there are identical plaintext, we get identical ciphertext. If we know (or can guess) the plaintext, then there's the possibility we can extend this onward. A number of the flaws found in TLS's use of CBC mode, we get something like this happening. And yet, I'm flailing over here. The problem here is more like handling a knife unsafely. Someone's going to get hurt, and after the fact it's going to be clear what happened and everyone's going to feel really stupid. Hence the wise advice of Bob.
So as bad as that explanation is, it's kinda the equivalent of, "You're going to put someone's eye out!" when in fact they're more likely to lop off a finger, yet more likely just need a few stitches. Lots of safety advice is like that.
