
@_date: 2016-04-06 00:18:40
@_author: Jeff Burdges 
@_subject: [Cryptography] Secure universal message addressing 
It's probably best to reply to only *one* list if you reply to a cross
posted message like this, as otherwise people on multiple lists receive
the same message several times, possibly in different mailboxes.

@_date: 2016-04-09 23:37:40
@_author: Jeff Burdges 
@_subject: [Cryptography] What standards are there for post-quantum 
Arguably, we do not have the primitives sorted out well enough to deploy
post-quantum systems for all authentication purposes yet.  If you build a certificate chain based on SPHINCS, then that's 41kb per
link in the chain.  It's mostly all good otherwise.
There are many people who'd consider Ring-LWE too new for certificate
chains, but actually using it sounds roughly comparable to RSA, so
perfectly workable.  Isogeny based crypto is very new and the signature schemes have unusual
properties.  I donno about signature schemes using code based crypto but
the signature size would be worse than SPHINCS. Alternatively, one could attempt to find a domain specific hash based
signature scheme that avoids the "huge foot cannon" a typical stateful
hash based scheme represents, while still being more efficient than
SPHINCS. I'm doubtful that makes sense for certificates, but maybe another
application, like :
  A DVCs whose repositories are keys that continually signs itself.
If one were designing a new distributed version control system, lets say
because "everything should be rewritten in Rust" or whatever, then one
might wish to integrate signatures, not using an external signing tool,
but by making repositories themselves into keys.  In other words, all
repositories automatically signed their commits, other repositories with
whome they interact, like submodules or forks, and any builds they
produce, especially reproducible ones.
You want post-quantum today because the authentication might actually
matter decades into the future, but you do not like SPHINCS 41kb per
commit.  Is there a safe way that each commit could issue a new signed
hash based signature key valid for only a few future commits?  There are
however many trade offs one could try here like throwing away the old
SPHINCS signatures using the repository structure itself along with
newer signatures.  Is there any forward secrecy value in trashing the
old exhausted private keys though?  etc.

@_date: 2016-04-12 22:33:12
@_author: Jeff Burdges 
@_subject: [Cryptography] Is storing a hash of a private key a security 
It depends on the "huge amount of details".
I presume you mean the private key corresponding to some public key that
appears elsewhere.  Is that public key generally available?  If so, then a modern hash
function like SHA2, SHA3, etc. is far harder to invert than that public
key derivation function.  It's no weaker in any real sense. Is there are constraint on the hash function?  If so, what?  We're
worried about SHA1 for birthday attacks, but presumably an inversion
attack remains far fetched.  I donno if MD5 poses a risk, but maybe. Can you pad the key record inside the device with random noise known to
you?  Does the device just hash all its memory for example?  If so, that
extra entropy adds (information theoretic) security, strengthening even
a crap hash like MD5.  Please do tell us the hash function though. If you're still worried, then why can't you just do a trial signature or
decryption with a test vector?  It's slower, but you only need to do it
occasionally.

@_date: 2016-04-30 04:16:55
@_author: Jeff Burdges 
@_subject: [Cryptography] More speculation on cryptographic breakthroughs. 
Is this an endorsement of those Android pattern swiping screens?  ;)

@_date: 2016-08-07 11:35:12
@_author: Jeff Burdges 
@_subject: [Cryptography] Where to Find PQC Crypto Libraries? 
There are links to C implementations of New Hope, SPHINCS, and TESLA
at : New Hope is probably what you want right now anyways.
I'd think a single library cannot really provide algorithm agility right
now because underlying the algorithms remain too different and you
really might want to change algorithms down the road.  Post-quantum
cryptography is very much a research topic with basic questions like
authentication, chosen cyphertext vs plaintext security, etc. remaining
problematic.  In fact, there are attacks where malicious key exchanges
allow attackers to extract private key material, so keys can usually
only be ephemerally, or maybe require some sort of validation.  p.s.  I'll attempt to summarize my understanding of the state of various
post-quantum primitives with some form of implementation : SPHINCS - Hash based signatures algorithm.  Very good security
properties, long term keys okay.  Key sizes around 1kb, or maybe better,
but signatures are 40kb, so not usable for most applications. TESLA - I haven't read the paper yet.
McBits - McElise style Public-key encryption.  1mb public keys!  Adds
64kb to encrypted messages.  I donno if keys need to be ephemeral, etc.,
so read the paper.  Ask the authors about the code's status.
NTRU - Public-key lattice-based encryption.  Public keys are a few kb.
Aging now, but never subjected to enough crypto-analysis due to patent,
so maybe claims about treat modem get confusing.  Considered weaker than
more modern lattice based schemes.  NTRU' - Avoids NTRU patent, and improves in many ways, but many people
feel it suffers from many problems of NTRU, and big arguments about
parameter choices.  Ask authors about the code's status maybe.
New Hope - Ring-LWE based Key exchange.  2 kb public keys.  Acceptable
speed.  Ephemeral keys only.  This is what everyone is using right
now.  MS SIDH - SIDH Key exchange - Around 600 byte public key.  At least 300
times slower than curve25519, much more if you try to use key
validation.  Very new, so even the authors say not to use it for
anything serious yet.*  Also, there was an interesting paper on an LWE key exchange scheme
without ring structure recently, presumably their code is not production
ready, and their key sizes are much bigger than New Hope.  Yet, one
could imagine that a few years will see this growing into schemes that
seem more secure than Ring-LWE schemes like New Hope for applications
that can afford to pay more in key size. * I personally like SIDH long-term.  Amongst other reasons, it appears
you must do computations in a huge mathematical object to break it,
possibly adding an additional hurdle for quantum computers.  Yet, I've
never seen anyone make space complexity claims about attempts to break
it, so maybe that's just wrong.

@_date: 2016-08-08 18:38:05
@_author: Jeff Burdges 
@_subject: [Cryptography] Where to Find PQC Crypto Libraries? 
Yes, I'm not going to stand by that claim at all.  :)
I'll attempt to explain what I meant with this quote form page 18 of
 : "For both ordinary and supersingular curves, there is a natural
bijection between isogenies (up to isomorphism) and (left) ideals in the
endomorphism ring. In the ordinary case the endomorphism ring is
commutative, and ideal classes form a finite abelian group. This
property has been used by Childs et al. [10] to solve the ordinary
analogue of CSSI in quantum subexponential time. It is natural to ask
whether their algorithm can be adapted to the supersingular setting,
but, in this case, the endomorphism ring is a maximal order in a
noncommutative quaternion algebra, and the left ideal classes do not
form a group at all (they form a groupoid, though). Since the algorithm
of Childs et al. depends crucially on the properties of abelian groups,
we believe that no reasonable variant of this strategy would apply to
supersingular curves."
I am not overly sold on the idea that non-commutativity per se buys you
much resistance to quantum algorithms.  There are interesting quantum
algorithms for nilpotent and dihedral hidden subgroup problems, for
example.  Instead, I suspect our post-quantum algorithms real resistance comes
more from not even being based on a group.  And that quantum algorithms
cannot adapt themselves terribly well to structures like these
groupoids.  Intuitively most products do not even make sense in a
groupoid, so you attempt to say compute periods with a QFT then you just
get zeros.  I think you can embed the groupoid into a larger algebra
where I suppose the QFT makes sense again, but that algebra becomes very
high dimensional, so no longer useful for space reasons.  I am **not** an expert in this stuff, so do not take anything I say
seriously!  :)
p.s. There is something more concrete one can say about this in the
hierarchy of lattice-based systems : Unique SVP is reducible to dihedral
HSP, but apparently not in a way that's useful for breaking crypto
systems based on unique SVP.  As I understand it, LWE based crypto
systems without the ring, like that one in
 called Frodo, have no known reduction
to group HSP problems.  Again I do not understand this stuff well.

@_date: 2016-08-31 00:39:00
@_author: Jeff Burdges 
@_subject: [Cryptography] Capability Systems 
In this vein, there is a thin libc alternative called CloudABI that
attempts to impost a capability-like approach at the library level.  I
wondered if the C world might be the wrong audience for this though.. The Rust community otoh is both more security conscious and happier to
experiment.  And the ecosystem is developing rapidly.  And several of
the larger projects in Rust right now tend to be highly security
conscious, like the HTML Engine Servo or the kernel Redox.
I therefore suspect a capability oriented fork of Rust's standard
library could gain some real traction, possibly replacing the current
standard library, influencing future kernels, etc. It's imho the right moment in time for capability advocates to rebuild
the Rust standard library the way they'd want it.

@_date: 2016-06-06 20:49:59
@_author: Jeff Burdges 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
Taler is basically a modernization of Chaum's original blind signature
scheme from like 30 years ago.  Taler drops Chaum's trick by which double spending results in
deanonymization.  Instead, Taler does online detenction of double
spending, and so does not support offline merchants like Chaum did.  It
follows that a Taler exchange (mint) and merchant cannot be defrauded by
customers in the way Chaum allowed, so a Taler exchange does not need a
collections department the way Chaum and did, and Credit Cards do.
This, along with not using wasteful proof-of-work, makes Taler suitable
for "micro-transactions."  Of course, this also removed the most natural
category of deanonymization attacks on customers in Chaum's scheme.
Taler uses a RESTful API and runs in the browser :   You'll find the git repositories here :
 Chaum's blind signature scheme was arguably already taxable in the sense
that merchants not wishing their bank to see their income needed to take
risks to avoid it.  Taler's refresh protocol extends the risks to tax
evading merchants from merely legal to actually opening the merchant up
for fraud. As for Bitcoin, I suspect existing BitCoin exchange might simply deploy
their own Taler exchange as an off blockchain transaction system that
deals in BitCoins.  Taler would provide their customers with convenient
web based purchases with true anonymity.  I'd expect folks involved in
side chains will find Taler interesting too.

@_date: 2016-06-07 03:02:42
@_author: Jeff Burdges 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
I did not write it.   Christian, Sree, and others did.
I've tweaked it twice, once to use a full domain hash so that the proofs
of security against one-more-forgery attacks hold, and once to make the
blinding factor use the full domain of the RSA modulus to prevent
leaking a bit of identity information per coin.  (cute attack)
There are a few parts of the code that we import from GNUnet for legacy
reasons, maybe that'll get cleaned up eventually.  Our RSA blind
signature implementation based on libgcrypt is one of these.  You'll
find it in the file crypto_rsa.c and cryto_*kdf.c here :
  p.s.  We use RSA blind signatures firstly because Tanja Lange told us
to.  Additional reasons include : Schnorr blind signatures require an
extra round trip.  Pairing based blind signatures are pairing based,
making them no more efficient than RSA.  These alternative schemes might
be less susceptible to the RSA padding-like issues I dealt with.  In
cases, I found their proofs of security against one-more-forgery feeling
kinda "fast" though, while I found the RSA blind signature literate
lucid by comparison, and it seemed better studied.  And my tweaks were
easy once the issues became clear.  p.s.2  All these blind signatures schemes have post-quantum blinding
operations.  I think the new fancier zero-knowledge schemes like
Zerocoin, Anonize, etc. are generally not post-quantum.  Our refresh
protocol is not post-quantum either.  I'm working on a paper that fixes
that.

@_date: 2016-06-07 19:41:31
@_author: Jeff Burdges 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
At the moment, I think we're doing nothing about it, but it sounds like
I'm about to go fix that.  :) If I understand the attack you have in mind, it goes roughly :
First, an evil exchange creates a 2048 bit RSA key pq, but issues n = p
q r_1 r_2 ... r_k as say a 4096 bit RSA key where r_i is a smallish but
preferably not so obvious primes, like not 2, 3, or 5.  Next, our evil exchange detects and records when the various r_i appear
during blinding and spending.  As m is 4096 bits, then some always do
since we took the r_i smallish. Each appearing r_i factor leaks I think several bits about the
customer's identity.  If enough coins are involved in a transaction,
especially say through repeated transactions, then the customer will
quickly be deanonymized. Is that right?
I will fix this.  Thank you,

@_date: 2016-06-09 21:37:18
@_author: Jeff Burdges 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
I agree in general.  In Taler's case however, we've a RESTful protocol
without too many rounds that gives parties some flexibility.  We do not require that customers be online to spend money, only that
merchants must be online to prevent double spending.  In other words,
merchants must contact the exchange before delivering goods, or else
must institute their own identity checking and debt collections. At present, I'm unsure exactly how our merchant software behaves when if
the exchange is down.  In principle, it could continue carrying out
transactions, but mark the orders as pending, and then go collect the
coins and mark the orders as paid later when the exchange comes back.
Internally, I've mentioned this as potentially useful but maybe
something breaks currently. We do require that customers go online to withdraw or refresh money of
course.  Yet, Taler could be adopted to point-of-sale systems with
offline customers.  We could even batch withdrawal and refresh
operations to occur through a merchant's connection, so like when you
spend money at the corner shop you also get you freshing anonymized
change from the exchange.  Anyone interested in a point-of-sale system
protocol for that is welcome to contact me to chat about it.  :) Just an aside where I agree with you :  We need an offline or at least
highly asynchronous alternative to the web.  I'm thinking roughly
Yawning Angel's anti-captcha extension CFC for FireFox / Tor Browser,
but using distributed storage and an asynchronous mix network, instead
of the centralized servers of archive.is and Tor.  I'll clip message to
tor-dev about it and forward it below.  Arguably, the web standard could
be radically trimmed along the way, possibly losing security properties
of web-to-async-web gateways.  I think Taler could be adapted to such a
setting. -------- Forwarded Message --------

@_date: 2016-06-10 22:48:03
@_author: Jeff Burdges 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
I'm not sure I understand.
Taler should work well for handling most fungible assets. Taler would not necessarily be suitable for assets that incorporate an
obligation.  I suppose the financial world has invented assets that,
while still fungible, either entitle obligations themselves or have
legal obligations imposed on them.  As an aside, I once sketched out a bi-directional system where employers
pay employees using something like Taler while employees obtain a tax
receipt coin upon depositing wages that they return to their employers.
In this scenario, the employees and employers are not anonymous to one
another, but their relationship is hidden from their banks.  I'd think
such bi-directional schemes could handle many legally encumbered assets
In any case, I'm concerned that you might not be talking about
non-fungible assets based on : I'm dubious if anonymity makes any sense for non-fungible assets
anyways, so I'm going to ignore that bit and continue.  :) I have not quite understood what you mean by m-to-n, but..
Taler has an auditor party that helps determine what exchanges (mints)
the customers and merchants will use.  Auditors sign the denomination
keys issued by exchanges.  At most one exchange should be involved in
any given transaction, but it's okay if your wallet has coins issued by
many exchanges, even if they are both issuing Euros or whatever. A Taler exchange issues a separate denomination key for every value of
each asset type it handles.  If your exchange handles BTC, then it might
for example select a small array (x_i) of values such that x_{i+1} = 3
x_i and x_i BTC lies between $0.01 and $200 at exchange rates when the
denomination is issued.  It'd issue an "x_i BTC" denomination key for
each x_i.  We allow partial spending through the refresh protocol. If you're doing fancy financial instruments, then at some point I'd
think the anonymity set would become too small to be worth it.  If the assets are fungible but not divisible, then you'd never partially
spend, but the refresh protocol would still be used when either (a) the
coin's anonymity was tainted by failed a transaction, or (b) the coin's
denomination key is about to expire.  We've several dates attached to
denomination keys, issuing range, spendable range, refreshable range,
and maybe another.  After all that records no longer kept, at least not
on-line.  It's important that denomination keys eventually expire, not just for
the database, but also so that the exchange knows it was not defrauded,
by say an employee who stole the denomination key.  I suppose
denomination keys expiration might allow assets with some classes of
obligations, including some forms of taxes.

@_date: 2016-06-12 02:01:41
@_author: Jeff Burdges 
@_subject: [Cryptography] GNU's "anonymous-but-taxable electronic payments 
Interestingly, blind signatures need not favor extortion, or the black
market, at least not as strongly as bitcoin does.  Customers can
deanonymize merchants! If Eve attempts to extort coins Alice withdraws honestly, then Alice can
simply report those coins to the cops/exchange in advance, and they can
pursue Even when she deposits them.  If Eve attempts to refresh the
coins, then Alice can trace the refresh to the new coin, or give the
cops the coin's private key so they can do it. Instead, Eve must make Alice run a modified Taler wallet that does an
initial withdrawal operation from Alice's reserve (bank account) using
coins Even blinded.  And one could evade taxes in the same way.  In the
tax case, it's unlikely the bad customer trusts the bad merchant with
their reserve, so you're basically doing ordinary SEPA/ACH wire
transfers and lying about account owners.
If extortion becomes an issue for Taler, then we would upgrade the
existing withdrawal protocol to use a variant of the refresh protocol.
We simply replace the coin signing key in the refresh protocol with the
reserve key, and take the value for the issued coins form the reserve
instead of another coin.  Now anyone could trace all the coins created
from their reserve!  Our extortionist Even could still operate by creating Alice's reserve
for her and making her fund it with a wire transfer.  In principle, we
could prevent this by imposing restrictions on the creation of reserves.
At the extreme, maybe even asking customers to generate their reserve
key pair when physically in their bank branch office.  We're now back to your trusted hardware dongle for preventing extorion,
except the honest customer retains their anonymity.  In any case, we envision Taler withdrawals being regulated like ATM
withdrawals, meaning you cannot extort that money that quickly
anyways.

@_date: 2016-06-19 11:52:46
@_author: Jeff Burdges 
@_subject: [Cryptography] Digital currencies 
If you mean blockchains, then one big issue is the need to many crypto
currencies to waste more resources than traverse the system to protect
the system.  These proof-of-wasteful-work schemes like bitcoin wont
scale.  We could replace them by proof-of-social-good, like filecoin's
proof-of-possession idea.  And Lief Ryge's proof-of-storage and
proof-of-onion-routing ideas inspired by filecoin.  Also,
proof-of-science-work schemes will fail eventually too because the
science work is too fickle.
In general, a proof-of-social-good appears to only magnify the value of
a more pure proof-of-work system, so they might still encounter scaling
problems if used too naively.  I suspect you want a proof-of-burn type
system coupled with a proof-of-social-good system to really eliminate
all proof-of-useless-work. Imho, it's actually pretty great that providing free onion routing and
free file storage or file sharing appear to be the only realistic ways
to make something like bitcoin really scale up, as that could help an
onion routing system like Tor or a free file distribution system scale
up along the way.
p.s.  We've mentioned it here recently but you should probably check out
taler.net, which gives classical currencies many advantages over pure
blockchain based currencies.  You can use Taler with a bank that
operates in a blockchain based currency too though, so it's no real
advantage for classical currencies, but it closes the gap somewhat.

@_date: 2016-06-20 17:36:16
@_author: Jeff Burdges 
@_subject: [Cryptography] Digital currencies 
This is also true.  You might still imagine a social hierarchy where a vanishingly small
minority of powerful banks and ultra-rich individuals do their financial
transactions in the real BitCoin ledger, and everyone else uses some
sort of off-blockchain transaction systems like side chains or Taler or
whatever.  I'd expect a bitcoin style proof-of-work cannot handle even
this scenario though, as attacks on the proof-of-work component remain
easy compared to breaking other forms of security. Ignoring blockchains, there are still scenarios where one might wish
wish to add some third parties to a transaction who were selected in a
weakly random way.  Proof-of-social-good schemes like filecoin or
proof-of-onion-routing do seemingly make this much easier.

@_date: 2016-06-21 18:43:11
@_author: Jeff Burdges 
@_subject: [Cryptography] Digital currencies 
It's more precise to say "far less than all the electricity consumed by
the humans who operate the .."
At the transaction level, there are a spectacular number of humans
employed due to the fact that credit cards are based on debt.  It's not
simply debt collectors, but all the humans involved in advertising that
helps trick people into debt.  You cannot argue in favor of BitCoin based on this poor allocation of
human labor though.  Instead, RSA blind signature based payment schemes
like Taler remove the need for all those humans involved in handling
debt.    Agreed.  At the same time, there is an enormous cost built into Credit
Cards because they operate through debt.  It's extremely profitable, but
so is exploiting speculation was profitable for BTC investors too.
Both are bad ideas.  We should use transaction systems that avoid all
these problems.

@_date: 2016-06-22 00:55:50
@_author: Jeff Burdges 
@_subject: [Cryptography] Digital currencies 
At present, debit cards are handled through the credit card settlement
system.  I suppose they do not incur as much risk to issuers, although
probably customers can still contest charges, albeit with more
difficulty.  In any case, the credit card companies can still extract
huge rents from them though.
I don't think so.  A system suitable for micro-transactions cannot be
based on debt because if settlements involve a human too frequently then
transaction fees become too large.  Of course, micro-transactions are
more impossible with Bitcoin because the settlement costs involve too
much computation. ACH and SWIFT are different.  If their fee structure is not suitable for
your definition of micro-transaction, than a third party can simply
aggregate them until they're cheap, which is what a Taler exchange/mint
should do.  At present, these aggregating third parties like paypal
extract credit card like rents, but that can change with competition.
And this aggregator likely makes them more user friendly too.
This suggests you might want free software that can be deployed as an
aggregator to foster competition.  If one is doing that, then one might
as well provide anonymity for customers with blind signed tokens.  And
one quickly noticed that blind signed tokens do not play nicely with
debt either.  :)

@_date: 2016-06-28 00:35:10
@_author: Jeff Burdges 
@_subject: [Cryptography] RFC: block cipher randomization 
It's unclear if you're talking about using a single large block cypher,
but assuming yes..
It's unclear what you're protecting against.  I'd recommend reading Ross
Anderson's The Dancing Bear :
You could likely achieve whatever you want with either some grizzle
construction, or maybe some fixed none, mac, etc. encrypted along with
the plain text.

@_date: 2016-06-28 22:13:14
@_author: Jeff Burdges 
@_subject: [Cryptography] 40 years of "Diffie-Hellman" 
I dislike that phrasing as it might endorse plagiarism if taken too
It's simply that if you willfully fail to contribute to human knowledge
then we should probably not name a discovery after you.  This should
apply even if you were compelled by preexisting obligations, like a
contract or security clearance, although obviously one could imagine
exceptions. Imagine some GCHQ person had anonymously published Diffie-Hellman key
exchange first, so we would not be calling it Diffie-Hellman.  If that
person was Williamson, and we know that fact, then we should call it
Williamson key exchange.  If that person was not Williamson, then we
should probably call it modular key exchange, even if the anonymous leak
outed Williamson as the discoverer. We nevertheless always have an obligation to acknowledge other
discoverers if we know them.  Wikipedia handles this correctly with
Williamson being acknowledged in the third paragraph :  In the case of Diffie-Hellman, at least Diffie's stated motivations
represents a major early contribution to cyperpunk (political)
philosophy.  Williamson cannot lay claim to that, so the name and lion's
share of the credit unquestionably belong to Diffie and Hellman.

@_date: 2016-03-28 11:55:56
@_author: Jeff Burdges 
@_subject: [Cryptography] Mixing public key crypto systems? 
No.  First, Alice wants her security properties for the messages she sends,
not just the message she receives.  Second, we must assume that Bob's messages to Alice leak information
about Alice's messages to Bob, so using both PK systems like this gives
you the weaker security of the two.
Instead you should use both systems all the time.  For that, you need a
hash construct like HC(DH_1,DH_2) = H( H(DH_1), H(DH_2) ) and a proof of
security that:
Assume H has reasonable properties for a cryptographic hash function.
If we replace DH_i with an adversary controlled value, representing a
very bad break, but the adversary does not learn DH_j with j=3-i, then
the adversary gains no knowledge about HC(DH_1,DH_2).  In addition, if the adversary learns HC(DH_1,DH_2) too, then they should
learn nothing about DH_j, but that's simply pre-image resistance.
There should be general proofs this effect in the literature for various
HC constructions for when the DH_i represent either both Diffie-Hellman
key exchanges and for public key encryption operations, but..
I do not know where to find such proofs right now.  I'd love it if
anyone can fill that in, but otherwise I'll go hunting eventually. I've describe an HC as similar to double keyed HMAC because HMAC
provides some protection against malicious values like this, but some
current hash functions like SHA3 often make HMACs redundant, so simply
HC(DH_1,DH_2) = SHA3(DH_1 || DH_2) probably works.  A priori, if you use
SHA1 then HC should be a full HMAC construction, meaning like five
invocations of SHA1, but maybe someone can correct me.

@_date: 2016-03-30 10:09:17
@_author: Jeff Burdges 
@_subject: [Cryptography] Mixing public key crypto systems? 
I think that's an issue only if you're talking about a legacy protocol
wrapped in TLS like HTTPS.  A new protocol can do anything you like,
We have two abelian group based approaches, ordinary Diffie-Hellman
modulo a prime, and elliptic curve Diffie-Hellman.  It's always
reasonable to resolve conflicts over the prime or curve when designing
the protocol.  I hear OtR v4 will do both these key exchanges, but
that's probably anomalous.  There won't be a monoculture in post-quantum public-key cryptography
anytime soon because nobody knows what post-quantum algorithms to use.
We've only three approaches though currently :
- Code based like McBits requires 1meg keys and 64k encryptions
- Ring-LWE lattice based systems like New Hope
- Super-singular isogeny based
I think non-abelian group based systems have mostly failed to work out.
And non-abelian alone might not save you from the QFT really.  I think
these post-quantum systems have an element of "computational dead end"
which may kinda push Shor towards Grover or something.  In SSI, it's a
groupoid of isogeny classes of elliptic curves, not so sure how to
describe it for the others.  Also, if anyone has made that "dead end"
observation more precise, then I'd love to hear about it. This leaves us with at most five realistic public key types one might
use in one protocol.  I'd expect the reasonable configurations would
be :  (1) ECDH plus Ring-LWE, probably New Hope
 (2) ECDH plus both Ring-LWE and SSI
 (3) ECDH per message plus McBits with keys published elsewhere
 (4) ECDH plus Ring-LWE and SSI, and optionally McBits with ...
In practice, any modern protocol should make all public key systems it
employs mandatory, except possibly McBits whose key size might require
it be optional.  In principle New Hope and SSI are small enough for
ephemeral keys, but not McBits really. In (4), two parties could communicate even when neither had a McBits
key, but that's less secure.  Assuming a ratchet like Axolotl, a McBits
key could exist on only one side in (3).  A ratchet lets you alternate
between New Hope and SSI in (2) too, making it as fast as (1).
Anyways, I expect "Alice uses X while Bob uses Y" issue is very specific
to super-generic protocols like TLS, and to McBits.  In particular (3)
creates a situation where "Alice and Carol can talk to Bob, but not to
each other, because only Bob has published a 1meg McBits key." p.s.  I suppose the whole "crypto monoculture" discussion should really
be split into generic protocols like TLS that wrap legacy protocol, and
modern protocols like OtR that do one thing correctly.

@_date: 2016-03-31 21:36:53
@_author: Jeff Burdges 
@_subject: [Cryptography] On the 'regulation proof' aspect of Bitcoin 
Just a note on ransomware that might help get the discussion back on
cryptography : Blind signature based transactions are traceable by the sender.  GNU
Taler provides "taxability" this way : In Taler, there is a "hole" that issuing initial tokens could be done
fraudulently if the tax evading merchant or extortionist blinds the
tokens and tells their collaborator or victim to run software to mint
them.  We can defeat that for established accounts using a cut n' choose
protocol, just like we use to prevent giving change from providing
untaxable transactions.  It's harder if the extortionist can force their
victim to hand over their whole account, or make them open a new
account, but several tricks still work.  In any case, it's easiest to
plug this whole by simply having withdrawal limits like an ATM. At present, the refresh protocol reduces customers' anonymity from the
information theoretic security of the RSA blind signatures on the
initial tokens to merely curve25519, making it no longer post-quantum.
That's okay for giving change, well trash your small change if you
believe in quantum computers.  It's more concerning if we want to plug
the initial issuing hole too.  We could achieve post-quantum security
for both using Merkle trees of symmetric keys in a scheme inspired by
stateless hash based signatures.  I found this amusing because this
actually uses a Merkle trees for encryption, but it makes sense because
the encryption is ultimately for more signature like properties.
There is no question that public block chain signed by notaries would
work better than our current financial system.  That's great for large
transactions, but..
Individuals need privacy though, at least when spending typical amounts
money.  And blind signatures can provide privacyy guarantees of
information theoretic security, post-quantum security, or ECC security,
depending upon your concerns about taxability, extortion, etc.,
simplicity, and quantum computers.

@_date: 2016-09-02 12:08:56
@_author: Jeff Burdges 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
If your fault is random, then targeting the key is often more
profitable, ala Lenstra's attack on RSA.  I donno if this holds for
elliptic curves though.
I'd expect however that executable code should actually be easier to
target though since this attack depends upon deduplication.  There are
likely still limits in what you can accomplish though.
I suspect the answer might be improved schemes for randomized key
splitting, like this scheme for RSA from two Certicom guys :
That Certicom paper only really discusses fault attacks, but maybe some
could show that their scheme or similar improves on other RSA scheme at
timing attack protection more generally.  That might give the necessary
impetus to adopt it.

@_date: 2016-09-17 12:46:02
@_author: Jeff Burdges 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Recompiling massive amounts of C code without optimizations sounds
pretty painful too, making that option sounds unlikely too.  I suppose
one must argue that C made safer through limiting optimizations offers
either better performance or more safety than whatever the sandboxing du
jour scheme is.  Sounds dubious. Imho, there is more gain to be had by focusing on improving the Rust
ecosystem because at least Rust obtains reasonable performance along
with safety.  As I've mentioned previously, it'd be a good time to do
things say write a capability oriented standard library because the
language is quite young.  And Rust brings a wide enough array of
advantages to the table that programmers might actually use it, unlike
say yet another object oriented pascal. Anyway, if you want a feel for where Rust is going with respect to
memory safety and optimizations, then you should check out the issues
here:
