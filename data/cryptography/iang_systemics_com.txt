
@_date: 2002-08-19 09:19:53
@_author: Ian Grigg 
@_subject: employment market for applied cryptographers? 
just interested:  do you have a definition of what an
"applied cryptographer" is?

@_date: 2002-11-15 10:55:29
@_author: IanG 
@_subject: Fwd: [fc] list of papers accepted to FC'03 
I see pretty much a standard list of crypto papers
here, albeit crypto with a waving of finance salt.
What ever happened to Financial Cryptography?  The
organisers did say they were going to look at wider
accessibility for the coming year, but I see only
these papers that are, from the titles at least,
anything that speaks to non-cryptographers:
Even they're a stretch.  All are specialised, and
none are of interest to the non-deep-techies.
On a related front, how much interest is there in
running EFCE this coming June?

@_date: 2002-10-31 22:21:07
@_author: IanG 
@_subject: patents 
============================== START ==============================
No 'effing idea.  If you wanted to do blinded cash,
use Wagner, and then plug in something harder when
you get the rest going.  Any fuss over any patented
blinded method is the sort of fuss you get when the
blind lead the blind.

@_date: 2003-04-01 14:15:07
@_author: Ian Grigg 
@_subject: Russia Intercepts US Military Communications? 
Some comments from about a decade ago.
The way it used to work in the Army (that I
was in) within a battalion, is that there was
a little code book, with a sheet for a 6 hour
stretch. Each sheet has a simple matrix for
encoding letters, etc.  Everyone had the same
sheet, and they were created centrally and
distributed from there.  If any sheets were
lost, it was a major disaster.
All soldiers were taught to code up the messages,
it was one of the more boring lessons.  In
practice, corporals and seargeants did most
of the coding, but it was still a slow and
cumbersome process.
For most of the communications needs, soldiers
talked in the clear, using a set of code words
that never changed.  For example, Sunray is
the unit commander.  This wasn't for the purposes
of security, but for clarity.  Only reports
were encrypted.  Radios were huge, heavy, and
didn't have much facility.  They were always
giving problems, and soldiers for the most part
didn't understand their purpose (in the way
that they clearly understood what a weapon was).
I wasn't so much into professional crypto back in
those days, but thinking back, it would be a seriously
hard task to put net-quality crypto into tactical
Consider these difficulties:  it was *banned*
to use any form of comsec that wasn't centrally
approved.  No personal code words, no CB radios,
no knicknames, no nothing...  (In practice there
was some leakage, I recall on my last exercise,
logistics back to the battalion HQ in the city
was handled over a cellular phone!)
The standard radio had to be purchased from
a military supplier - like Racal - and the
procurement process was probably 4 years long
before the first units hit the troops.  During
that time there could be a revolution in the
way comsec could work, if one were to learn
anything from the lessons of SSH, etc.  Each
radio was meant to last at least 20 years...
Further, whatever was put in place had to be
handled by soldiers.  Count them as approximately
as technically adept as your grandma.  If she
can't be taught to do it on pencil and paper,
then the soldiers can't be either.
As we haven't managed to get our respective
grandma's using crypto on the net, yet, that
would suggest why the military hasn't had much
luck at the infantry level, either.
(Airforce and Navy are somewhat different of
course, as are armoured vehicles.  They have
portable infrastructure that infantry don't
Makes sense, the troops probably carried the
code books for the next 4-5 days, but comsec
probably ruled out any more than that.  Then,
when that "ran out" the staff discovered that
the new code books couldn't be distributed to
all the soldiers.  Without all of them on the
same system, switching to clear would have
happened like an epidemic across the force.
Exactly.  One of the things soldiers are trained
to do is, after a successful action, secure
the enemy's radios and try and recover their
codebooks or codes.  A fob or smartcard would
be just like that, a token to be captured.
Once captured, this would let one into the
net.  A big prize.
So, in practice, the commsec people would not
accept this solution.  They would know that
any pin would be listed in a plastic covered
page in the radioman's notebook.
Tactical security means where there is only a
matter of hours where the information should
be kept discrete.

@_date: 2003-04-04 12:50:51
@_author: Ian Grigg 
@_subject: Russia Intercepts US Military Communications? 
Your description fits, it sounds like DRYAD.
:-)  The reason was that sigint on the other side
could note particular differences from standard
procedure, and use that to track units up and down
the front.  For the same reason, all plan names
are generated randomly, from a dictionary program
in HQ;  sigint people could derive a lot of clues
from the personally picked plan names.
(Hence you can always tell when the professionals
have lost control, as the plan names become political.)

@_date: 2003-04-08 16:42:23
@_author: Ian Grigg 
@_subject: Via puts RNGs on new processors 
This could be solved by selling the chip
"as is".  With no guaruntee of performance,
leaving each user to make their own tests.
This is apparently a solved problem; there
is a ready market for faulty chips.  For
example, memory chips with half the memory
missing are shipped off to 3rd world countries
where they are sorted according to flaw, and
constructed into memory boards with matched
flaws, so as to deliver perfectly good strip
level memory from partial components.
To combine with the first point, run the TRNG
for a couple of seconds, compress the result,
and use the result to decide whether the chip
goes into the "good" bucket or the "no-TRNG"
No guaruntee, then no problem.  This seems
to be a userland problem, solved by some
user program that tests the output, run on
an application basis.
The main thing about use of random number
sources is to never trust a single source;
for this, a framework is needed to combine
several sources and measure entropy into
the result.  Kelsey et al wrote a paper
that described such a framework, called
Yarrow, and this appears to be the one
most used in the practical software world
(goodle on Kelsey and Yarrow).
would be a really useful input into Yarrow.

@_date: 2003-04-08 21:16:29
@_author: Ian Grigg 
@_subject: Via puts RNGs on new processors 
I think we have a difference of world views.
My world view would be that there is no such
thing as an acceptable off-the-shelf RNG.  Even
if one were to "rate" such using NIST testing,
etc, how does one know that the unit in hand
is up to the ratings?
If one is relying on some commercially acceptable
rating, then one has also to ensure that the
entire distribution chain - how you got that
chip - is also safe.  If there are such things
as "good" Via chips alongside "bad" Via chips,
how do we know that a bad chip wasn't substituted
in at the last moment?
A lot of this can be solved by making no such
assumption;  this is what the Yarrow framework
espouses.  For any serious (commercial?)
application, it would imply that no single
source is trusted, and many are needed.
whether the Via chip is NIST or not, or whether
compression is good enough or not.  And it
certainly doesn't matter what the marketing
blurb says - the situation is quite unlike
that of the snake oil claims of some cipher
manufacturers.  Effectively, we have solved
the issue of marketing noise in PRNGs.
What matters is that it provides at least some
"probably good" entropy, where good means
valuable as an input to Yarrow.  And, in a
bigger picture sense, what is good is that
there are more of these devices available,
as many as possible.
( About the only context where I can see a
one-stop-shopping approach being relevant
is in such things as USG purchasing, where
all governmental departments are *instructed*
to purchase according to stated NSA guidelines.
In that case, the departments get given the
kit, so it's not their problem.
But, no commercial operation should feel the
need to be constrained to that, and it would
probably find it more efficacious to rely on
an open source, Yarrow-inspired solution. )

@_date: 2003-04-09 14:43:02
@_author: Ian Grigg 
@_subject: Via puts RNGs on new processors 
I think the attack type is a lot harder.  That is,
attacking via the CPU involves a deep understanding
of what the code is going to be doing, and avoiding
any artifacts, as a squillion OS and application
instructions run before the special target pops
Attacking a RNG is simpler, as Arnold R. just
pointed out.  It is hard for an independant test
to determine if a sequence is random, or is a
PRNG with a seed known by someone else.
I "worry" only mildly about this for the CPU.
Switching our threat level up to 11, allegedly
there are special regions on Intel CPUs for
unstated special operations for unstated
special customers.  But, I can't quite see
how it is that, even if an attacker has
control of that, he is going to be able to
successfully futz with the your process.
It seems that as a minimum, he would have to have
a complete set of the binary instructions.  So as
to be able to target the attack.
Then, he would still need to leak information
somehow.  That is, he steals your key, and hides
it in the secret spot.  What is he going to do
then?  He then either requires a transmitter
of some form, or an ability to piggy back on some
other thing like an IP packet, or some sort of
special black bag job whereby the spook comes in
and sticks the special probe over the CPU pins
to extract the spoils.
That all seems like a very high cost attack.  In
contrast, a specially seeded RNG can be attacked
at leisure, just by looking at the actual work
product, which is presumably available.
(Mind you, if he had both a seeded RNG and the
CPU with "special ops" in, then he might have
all he needs :-)
Of course, all this is well into the paranoi level.
In practice, there are easy defences for both
attacks;  purchasing components randomly, and
random mixers.
(Of course, the real reason that we worry about
this issue is not that there is some nasty attacker
out there, but if the single sourced RNG breaks
and starts spitting out zeroes or some equally
poor output.  From an engineering perspective,
simply testing is insufficient;   mixing followed
by occasional testing is far superior because it
reduces the problem to a statistical issue.  In
comparison, for the CPU, breakage is detectable
in other ways.)

@_date: 2003-04-20 17:50:33
@_author: Ian Grigg 
@_subject: The Maginot Web 
The Maginot Web
The by-now infamous signed certificate was
deployed for two reasons, being, a) to stop the
MITM, and b) to stop the spoof. The MITM - man in the middle - is an attack
possible when the attacker is already in the middle
of two communicating endpoints, and thus has an
ability to change the packets as they go through
his service. Then, he simply presents one sequence of packets
upstream, and a second, distinct sequence of
packets upstream.  If those packets were intended
to set up a secure session, our attacker can set
up two secure sessions, one upstream, the other
downstream, and copy from one to the other. The certificate is used to defeat this, by using
an authenticated key exchange to bootstrap into a
secure session.  As the key exchange is signed (a
brief description not doing justice to the full
story) the MITM cannot change the signed key without
revealing the attack. The spoof, on the other hand, is where the attacker
tricks a victim into communicating directly with him,
by pretending to be someone else.  For example, a
spoofer pretends to be a site of some repute, rather
than going to that site directly. Once so tricked, the victim can set up the secure
connection to the pretended site.  But, again, as a
signed certificate for the site includes the real
address and can bootstrap a secure session, the
browser can avoid being fooled. As described above, at least, that makes for two
forms of attack - as defended by the cert.  What
makes the situation murky is that the attacks are
not totally distinct; the MITM could use his attack
to redirect to the same site or to a spoofed site, and
the spoofer might often pass the traffic back to the
original site, thus converting his attack into the
MITM. The essence of the difference then would seem to
be in the initiation; the MITM is already in the
middle, whilst the spoofer needs to trick one end
or other into letting him be in the middle. Either way, these are the definitions used here,
but have a care with them!  Switching between
threats and amongst purposes, aims, interests and
whimsical and personal desires is endemic within
the SSL community, and can suffer the outsider a
permanent headache. I looked at the MITM in a previous rant [1].  The
assumption is that the cert does stop your common
or garden variety MITM, as defined above.  I went
on to ask whether implementations need to in fact
defend against it, an odd question, given that SSL is
fairly universally deployed with certs in place [2]. The accepted dogma is "of course!" A slightly less quick and much more accurate
answer is: "a definite maybe." The frequency of a true MITM - one defined above
where someone has the ability to control an
intermediate node at low level and take central
position - is so low as to be difficult to measure.
Using risk analysis, there is no economically viable
support for mandating protection, so the deployment
of a cert should be optional if there is any cost
involved. What about the spoof?  In total contrast to the
MITM, spoofs are common.  As common as dirt, and
as equally unclean. E-commerce sites with real value for thieving suffer
spoofing attacks (or perhaps, to be more pernickety,
their users are attacked) on a regular basis. (I have most experience with the world of internet
gold (IG) systems, a small, rabid band of gold bugs,
libertarians, and other intellectual misfits that, to
their credit, actually made net payments work.  I will
rely on their experiences as a benchmark.) Within what we call the gold community, every week
or so, a spoof attack is launched [3]. Here is how it is done.  An email is sent to a
(big, targeted) list of addresses, encouraging the
recipient to click on a link.  The link takes the user
to a site that is not the IG, but instead is a mere
pretence at the targetted IG.  The user types in her
account number and password into the pretend site. Our faithful and persistent attacker collects the
details, logs into the *real* site, and scarfs up
the gold.  As there is a ready and reliable exchange
market, the funds are washed more quickly than any
response can occur in higher layers in the protocol
[4]. Does the Cert stop the Spoof?  Nope.  Well, of course
not - not as described above.  Obviously the user is
at fault for entering - clicking - the wrong address,
and not checking... Well, hold on there!  Wasn't the cert supposed to
stop the spoofed URL? What we have here is a clash of expectations.  The
security model claims that the cert will protect from
the spoof, in that the URL typed will be the one that
is reached. But, the user's actions derive the URL from some
email or other source; a click launches the URL into
the browser, and no typing is done.  The browser
accepts the 'click' and, as the user has already
signalled acceptance, there is no need of further
checking! But - say the security types - the user shouldn't
click on a URL that is wrong.  Um, ok, but users do,
because that's what their trusty browser asks them
to do! But (3) they say - the precise spelling should be
checked!  And, sometimes, our dear user does even
check the URL.  And, it is perfect, within the text
of the email.  And ... And ... The ways to create a spoofed URL are many and
imaginative [5].  Scammers have an infinite amount
of time, and the ways of the URL are complex,
flexible, artistic even.  Browsers and Email clients
boast much facility at hiding critical security
information, under the guise of user-friendliness.
Presenting users with easy choices and other such
marketing-driven imperitives rules. Perhaps, worse of all, these scams don't seem to
worry about SSL at all.  Well, why should they?  That
would only mean that the attacker would have to
fork out good money to get some nonsense string of
numbers so that the victim's browser didn't interfere
with the attack by giving the game away. What a sad state of affairs.  The CA-signed
certificate, far from being the key to browsing
security, is the Maginot Line that preserves the
masses in a state of blissful ignorance. It works perfectly against the attacks conceived
and theorised as the dramatic threat to mankind,
commerce and the Internet, a decade ago.  Problem
is, the attackers bypassed it, with as much disdain
as any invading army against the last war's dug-in
defence. Problem is, the security model had unreasonable
expectations.  Problem is, the users didn't subscribe
to their part of the protocol.  (To be fair, it's hard
to communicate to users that they are even expected
to be part of anything.) Problem is, the browser manufacturers that were
sold on the need for the certs also got sold on the
convenience of click and launch.  So, they turned
around and sold the security model down the river
faster than one can say "check the URL..." In the wider picture of the web, the goal of the
security model is to protect value and information.
Within that wider picture, the systems in place -
browsing and ecommerce and implementations thereof
- lost sight of the security model, paying only lip
service to the security needs, while scrabbling to
please the customer with more click-and-buy
models. The parallels with the original Maginot Line are
embarrassing.  The real Maginot Line was erected
by the French during the years between WWI and
WWII.  This marvel of engineering was designed
to defend against a massive attack by Germany,
Evidently, an event that France fully expected,
and planned for! At an initial budget of 3.3 billion francs, it stretched
from Switzerland to the Ardennes (an impenetrable
forest) for some 240 kilometres (150 miles). It might have done its job, had the German army
been so polite as to stick to the French security
model. Instead, the Germans bypassed it.  The main
invasion went through the impassable forest, and
the Maginot line itself was mostly ignored.  The
garrisons surrended a few months later, as a sort
of afterthought. There are some who say that Maginot Line did its
job.  In fact, the line created a sense of false
security; it placed all of France under what was
later termed "Maginot mentality." Likewise, the CA-signed cert.  It has created the
appearance of security, at massive cost.  The real
attacks on users go right past it.  Probably, if
one were to ask an attacker about the certificate
protection, it's odds-on that he would think us mad
for even suggesting that he pay attention to it. Not only do the attackers ignore the convention of
presenting the certificate, they have the temerity
to hack sites on mass, stealing whole databases of
credit card numbers. Which is not the effect we expected!  Precisely the
reverse, if the 100%ers intent were to hold sway. ( The notion of 100% cryptography is that if a
threat can be protected against, it should be.  This
is the underlying design principle that places the
CA-signed cert as the lynchpin of SSL security; the
final keystone that brings the arch of complete
browsing protection to a reality. ) Obscurely, if one has the gall to question the need
for the cert, one is informed that dropping it would
result in a "false sense of security," amongst
other crimes.  Yet, this is precisely the state of
SSL implementations today; those that have stuff
to protect are molested by attacks they do not
understand, over protocols they were sold as
secure. How to change the Maginot mentality of the SSL
systems; that is the question that faces us. It might prove hard to change the mentality, but
luckily it is not hard to fix the systems once that
change is achieved.  There are a few minor tweaks
that will redeem the protocol. It is not, as with the Maginot Line, that the SSL
implementations are a complete waste of energy;
more, that there is an imbalance in the security
model as deployed. The blindspot from the Ardennes and on across the
Belgian frontier was the scene of much hurried
attention from 1936, after the declaration to
independence by a weary and jaundiced Belgium. In the case of SSL, it is a likewise attention to the
blindspots that will improve the overall security.  It
goes like this.     1. Browsers now force the use of CA-signed
       certs on merchants, for some unmeasurable
       security benefit.        They should not!     2. Instead, Browsers should accept ADH and
       other techniques such as self-signed certs.        These should be deployed as intermediate-
       level security models.     3. The single binary lock of security, as
       presented by the browser, results in a
       compressed message to the user.  This
       forces a binary choice between what is
       accepted and what is unaccepted.  Hence,
       the false sense of security, as described
       above.        Browsers should cease this binary choice.     4. Browsers should present compelling
       imagery to show the user the nature of the
       security:           *   ADH is better than HTTP,           *   self-signed better than ADH, and           *   trusted third party (arguably)
              best of all.        The user may not understand the message,
       or merely choose not accept it, but, the
       message must be present as an initial step
       in encouraging the user to participate in
       the security protocol!     5. Browsers - not web sites - should show who
       signed the cert.  Not with a click, nor on the
       untrusted page, but, on some protected area
       of the browser. These steps are needed to fulfill the promise of
an increase in security; they would go a long way
towards preventing spoofs (which are now
practically unprevented). What would be the downside of this?  Certainly not
the decommissioning of the CA architecture, as
feared by some.  What would then be deployed would
be a graduated ramp of security, where each server
enjoys the best it can afford. No longer would the choice be: security, but only if
your merchant can spend the price of erecting his
own personal Maginot Line.  With the above series
of choices, security could be delivered according to
cost, the only way that we would know that it was
the right security [6]. iang [1] "Who's afraid of Mallory Wolf?" posted 23rd
March 2003 on cryptography list.
 [2] "How effective is open source crypto?" posted
15 March 2003 on cryptography list.
 [3] Here's several all collected in the month of
writing this rant:     *   -
       repeated several times over ensuing days,     *   - not
       really a spoof, but a harvesting, and     *   [4] This has been going on for a long time in the
e-gold community, but now seems to have crossed
over to credit cards:        New way to steal password. A
       Discover credit card customer
       receives an e-mail telling him that
       his account is on hold due to
       inactivity, and that in order to
       reactivate his account, he must log
       in to this phony Web site. The
       information collected includes plenty
       of data that would enable identity
       theft: Social Security number,
       mother's maiden name, account
       number, and passwords. Similar
       scams have targeted PayPal and
       eBay customers.
       or Cryptogram, 15th April, 2003. [5] It's probably worth stressing that these
attacks are extraordinary in their inventiveness!
Emails encouraging great things, warning of dire
consequences, alerting to supposed controls, all
sorts of things.  Wonderfully accurate sites,
convincing URLs.  It makes you want to employ
these people, as their resumes are impeccable,
and often, economically productive! [6] Analysis would go like this: If each spoof
managed to grab $100, and there are 2 per month
against that one site, he and his customers (same
thing) will incur losses of $2400 per year. So, spending money on a cert would be justified for
that site, assuming that the cert protected against
the spoofs.  To meet that assumption, however,
would involve fixing the browsers as described
above.

@_date: 2003-04-22 22:30:11
@_author: Ian Grigg 
@_subject: DRM technology and policy 
The basic argument goes thusly:  software product
can be copied for zero marginal cost (and some high
initial cracking cost).  Thus, the marginal cost of
the product should shrink as it is limited by the
cost of distribution and the cost of cracking
competing product.  Both of these are also zero.
So any free market would result in the costs of
software product shrinking to zero.
It's an economic proof, or theory, if you like.  The
real import is that it is practical and borne out
by experience:  There is no longer a plausible
physical foundation for the sysem of intellectual
property that is now in place to protect performances.
There used to be a physical cost for music:  the
cost of the making the tape (in both senses of the
word.  Before that, the cost of copying by pen the
songsheets.  The only thing that *ever* provided
meaningful protection for the intellectual property
of music was the cost of making a copy.  No longer.
(If you don't believe that, go to a birthday party,
and ask them not to sing Happy Birthday... still a
work under copyright.)
Those assumptions are being challenged, you can
no longer just assume them.  It is no longer cost
effective to protect performances.  If it is no
longer cost effective, why is it necessary to
pay these people?  As many have said, those old
assumptions don't hold any more.
Nobody's fooling here.  We are staring some
physical difficulties in the face.  There are
people on this list or close by that have
already spent millions on DRM (divx, hundreds
of academic papers, lots of failed systems,
lots of arbitrages...).

@_date: 2003-04-23 22:30:29
@_author: Ian Grigg 
@_subject: DRM technology and policy 
Not correct, as someone (you?) pointed out earlier.
The cost of a CD manufactured at the factory level
is about 40 cents, add the cost of the packaging,
then the distribution to the store, plus the
marketing blah blah, and you get about $5.  Add
the retailer's markup of 100% and we are at $10.
LPs were similar, as was sheet music.
This is not a "marginal cost of zero."  The cost
is low, but not zero.
Now add in the net + MP3s.  The cost of manufacture
of a perfect substitute (a "stolen" copy) is now
zero.  Zero cents that is, unless we want to count
up the cost of the harddrive, but even that is
really better allocated into fixed/sunk costs.
The cost of distribution is not the $1-2 to get the
CD to the store, but it's what it costs me in real
money to send this email:  Nix, nada, zip.
There is a big shift going on here:  marginal costs
just went to zero.  (Actually, not quite zero, but
they went below the noise or measurement threshold
for a lot of cases, so let's stick to the zero
Not at all:  Cost includes fixed cost and marginal
cost.  Marginal cost shrinks to zero.  Fixed cost
is still there;  both for the "original" author and
the "derivative" author (a.k.a. cracker).
But, and here's the difficulty:  business models
that support covering fixed costs across zero
marginal costs are very difficult.  It's not just
a matter of a better DRM.  It's a matter of a
different product.  There is no easy bandaid for
the music CD of today.  There is a need for a
real, new product.  (Assuming that there is an
artist out there that wants to be paid...)
No, it's nothing to do with size of market, it's
to do with the marginal cost - the cost of each
fresh copy *after* all fixed costs - shrinking
to zero.
Consider radio.  Once you broadcast, every
additional listener is free.  If you like, a
new product needs to be calculated on that
basis (consider Steve Schear's auction of
single copy paridigm).  But, that's a completely
separate business model;  there's no point in
asking for a DRM for that, until you can define
the business model, and thus the technical
That's correct.  Although, marketing does
require a substantial marginal cost in order
to tack on the various other contributions.
I.e., marketing doesn't really work in the
case of "free theft."  Set marginal cost to
zero and we have a problem.
Nah, one of the things about laws is that
they usually have a physical foundation.
That is, it could be considered a bad thing
that Alice says she is going to rob the bank.
But, as the law has a great deal of trouble
in proving the harm *before* any substantial
physical evidence is present, it is not a
crime to think of, or utter, the intention
of robbing a bank.
Same as copyright.  It's purely as a matter
of practicality:  copyright is built on the
marginal cost of copying being != zero.  So
as to give substance to the crime, to give
some way of showing what has happened.  If
we go back through time, copyright has always
had this underlying characteristic.  (Same
with other IP.)
Now, it's lost that.  It really is a paradigm
shift, it's not a matter of building a CD
player with crypto added in.

@_date: 2003-08-26 15:49:39
@_author: Ian Grigg 
@_subject: [Fwd: [fc-announce] FC '04: Extended submission deadline] 
*** Extended Deadline ***
Call for Papers and Presentations
Financial Cryptography '04
9-12 February 2004
Key West, Florida, USA
Conference Web site: Important dates:
Conference                       9-12 February 2004
Submission deadline (extended) **10 September 2003 23h59 GMT**
Author notification              15 November 2003
Pre-proceedings version due      15 December 2003
Proceedings version due          15 March 2004
Sponsored by the International Financial Cryptography Association
Original papers and presentations on all aspects of financial-data
security and secure digital commerce are solicited for submission to
the Eighth Annual Conference on Financial Cryptography (FC '04). FC
'04 will bring together researchers and practitioners in the
financial, legal, cryptologic, and data-security fields to foster
cooperation and exchange of ideas. In addition to novel scientific
research as in previous years, the program for FC '04 will include
sessions on digital finance and economics and on secure financial
systems and digital-cash architectures. For the systems and finance
sessions, submissions must have a visible bearing on
financial-security issues, but need not be exclusively concerned with
cryptography or security.
Submissions accepted to the research portion of the conference will be
published in full in the conference proceedings (up to 15 pages in
total).Systems and Finance Sessions: For the systems and finance portions of
the conference, the primary emphasis is on presentation. For accepted
submissions in these sessions, a one-page abstract will be published
in the conference proceedings.
Submissions to the systems portion of the conference may include
architectural descriptions and/or accounts of industry or technical
experience with implementations of secure digital commerce systems.
Presentations may concern commercial systems, academic prototypes, or
open-source projects for any of the topics listed above. Where
appropriate, software or hardware demonstrations are encouraged as
part of the presentations in these sessions.
Contributions to the systems and the finance sessions of the
conference need not necessarily include novel contributions in the
realm of scientific research, nor must they concern financial
cryptography or security exclusively. They must, however, reflect
careful thought and effort and provide valuable, up-to-date experience
that is relevant to practitioners in the fields of financial
cryptography and security. Submissions to these sessions may consist
of a short summary of work of one to six (1-6) pages in length.
Instructions for Authors: Complete papers (or complete extended
abstracts) must be received by 23h59 GMT on 10 September 2003. All
papers must be submitted electronically. (In exceptional
circumstances, paper submissions can be accepted, but special
arrangements must be made with the program chair prior to 1 August
2003.) Papers must be formatted in standard PostScript, PDF format, or
MS Word, and should be submitted electronically according to the
instructions at  prior to the deadline.
Submissions in other formats will be rejected.
Submissions to the research portion of the conference may include at
most fifteen (15) single-spaced standard pages in length. Submissions
to the systems and finance portions of the conference must be short
summaries of work consisting of at most six (6) single-spaced standard
pages in length. (As indicated above, for accepted submissions in
these latter sessions, a corresponding one-page abstract will be
published in the conference proceedings.) Author names and
affiliations on submissions must be explicit. In other words,
submitted papers should not be anonymized. Submissions must include on
the first page the title of the paper, the names and affiliations of
all authors, a brief abstract, a list of topical keywords, and a
conference-session category (research, finance, or systems). Papers
must describe original work. For the research portion of the
conference, submission of previously published material and
simultaneous submission of papers to other conferences or workshops
with proceedings is not permitted. Authors of research papers found to
be doubly submitted risk having all their submissions withdrawn from
consideration as well as other appropriate sanctions.
The conference proceedings containing all accepted submissions will be
published in the Springer-Verlag Lecture Notes in Computer Science
(LNCS) series after the conference. A pre-proceedings containing
preliminary versions of the papers will be distributed at the
conference. For accepted submissions, at least one author must attend
the conference and present. In addition, authors of accepted
submissions must prepare the pre-proceedings and final proceedings
version <96> a full paper or one-page abstract, as appropriate -- and
sign an IFCA copyright form. Questions about paper or panel
submissions should be directed to the program chair at
program_fc04 at ifca.ai.
General Chair: Hinde Ten Berge
Program Chair: Ari Juels, RSA Laboratories
Program Committee:
Masayuki Abe       (NTT Laboratories, Japan)
David Birch        (Consult Hyperion, U.K.)
Roger Dingledine   (The Free Haven Project, USA)
Niels Ferguson     (MacFergus, The Netherlands)
Thomas Frey        (Davinci Institute, USA)
Philippe Golle     (Stanford University, USA)
Tim Jones          (Simpay, UK)
Marc Joye          (Gemplus, France)
Kwangjo Kim        (ICU, Korea)
Arjen Lenstra      (Citicorp, USA and Technische Univ. Eindhoven, The
Helger Lipmaa      (Helsinki Univ. of Tech., Finland)
Dahlia Malkhi      (Hebrew Univ., Israel)
David Naccache     (Gemplus, France)
Tatsuaki Okamoto   (NTT Laboratories, Japan)
Benny Pinkas       (Hewlett Packard, USA)
Nicole Pohl        (Franklin and Marshall College, USA)
David Pointcheval  (CNRS-Ecole Normale Sup?rieure, France)
Bart Preneel       (K.U. Leuven, Belgium)
Avi Rubin          (Johns Hopkins University, USA)
Adam Shostack      (Informed Security, Canada)
Vitaly Shmatikov   (SRI International, USA)
Sean Smith         (Dartmouth College, USA)
Rebecca Wright     (Stevens Institute of Technology, USA)
Moti Yung          (Columbia University, USA)
 Ari Juels
   Principal Research Scientist      RSA Laboratories
      ajuels at rsasecurity.com
        tel. (781) 515-7069
          fax (781) 515-7010             fc-announce mailing list
fc-announce at ifca.ai

@_date: 2003-12-05 14:32:49
@_author: Ian Grigg 
@_subject: Larry Lessig on ending anonymity through "identity escrow" 
It seems as if Larry Lessig has figured out the fatal
flaw in anonymous or untraceable systems - that they
are not economically sustainable.
In the face of that argument, he does not propose that
they be banned, as Declan suspects:
What Larray proposes is that they be permitted, but
he also suggests - quite rightly - that psuedonymous
systems will have more "traction."  This is simply
because psuedonymous systems overcome the fatal
objection to totally anonymous/untraceable systems,
whilst providing some economic privacy that is
currently unobtainable.
In practice, most successful systems have been identity
based in some form or other, with psuedonymous features
at the edge.  E.g., a hotmail account is a psuedonym
that points to an IP number.  Systems that preserve
total anonymity or untraceability have not as yet
achieved any success that qualifies as survivability
(notwithstanding many brave efforts).
( To drag this back to the crypto context, psuedonymous
systems are easily modelled as each client generating
a private/public key pair and using it as an identity.
In that sense, they are a core and useful result of
crypto systems. )
PS: disclosure - my company builds psuedonymous systems.

@_date: 2003-12-06 21:05:28
@_author: Ian Grigg 
@_subject: Additional Proposed Hash Function (Forwarded) 
That is the guess we came up with too.  But, why does
NIST bother to standardise this?
Granted 2-key 3-DES is in widespread use, but it should
gradually switch across to other ciphers.  And, for a
stop-gap measure, if a protocol implementor finds a need
to match a hash size, why not just truncate a SHA-256?
Or, to take the alternate position, if there is a case
for "wierd lengths," then maybe this is a case that
SHA could be reworked to be of flexible length, at the
algorithm level?
Defining a new SHA seems to be a lot of detailed work,
albeit hardly challenging, for everyone involved in
producing standard crypto, and there doesn't seem to be
much of a payoff for all those people.

@_date: 2003-12-11 19:25:26
@_author: Ian Grigg 
@_subject: example: secure computing kernel needed 
Sounds like Eros & E & capabilities.  There have been
other efforts in this in the past, going back some
time, but it seems that Eros/E/Caps represents the most
advanced in general mainstream "prove this is so" comuputing.

@_date: 2003-12-14 12:56:31
@_author: Ian Grigg 
@_subject: example: secure computing kernel needed 
It is and it has been.  Just not so much in
North America, and not in sense of making
the PC secure.
In Europe, the smart card field routinely
decided that trusted devices were required
to access the smart cards.  Such devices
were created and distributed.  Smart cards
are very expensive, though, and "free"
Internet banking dampened the enthusiasm
When it came to Internet banking, there
was much more of an emphasis on cost control,
and a range of cheap challenge response
hardware tokens are used to authenticate
each transaction.
In both these modes, the banks used secure
computing, but they did it by providing a
secure computer other than the PC [1].
When it comes to the PC's operating system,
there is apparently no economic way to achieve
what you suggest - ensuring that it hasn't
been tampered with - so few bother to worry
about it.  If more security is desired, the
preferred method is to bypass the PC's OS
[1] Note that I use the term "secure" here
in a relative sense.

@_date: 2003-12-16 09:38:14
@_author: Ian Grigg 
@_subject: Difference between TCPA-Hardware and a smart card (was:  
In the late nineties, the smart card world
worked out that each smart card was so expensive,
it would only work if the issuer could do multiple
apps on each card.  That is, if they could share
the cost with different uses (or users).
This resulted in a big shift to multi-application
cards, and a lot of expensive reworking and a lot
of hype. All the smart card people were rushing
to present their own architecture;  all the user
banks were rushing to port their apps back into
these environments, and scratching their heads
to come up with App  (access control, loyalty...)
But what they seemed to miss was the stellar mental
gap between the smart card and the PC.  On the PC,
a user can install an app if she so choses.  On
the smart card, it was a proprietary system, and
no smart card provider (the institution was the
real owner) was going to let anybody else play.
But, they believed and behaved as if others could
As many suggest, the starting point is "who owns
the card/trusted module."  From that point, you
can predict what will happen.  If it is not the
end user, then a lot of expensive spinning will
eventually be thrown out.  Using an institutional
model, and today's understanding of PCs, it is
fairly easy to predict that TCPA hardware will
fail unless you can find someone to pay for the
entire rollout, and use it for one purpose with
which they are happy with.  And, it won't take
over the market unless you can solve the issue
of un-TCPA'd hardware.
The cost of the barriers created is daunting, and
generally outweighs any security gained.  There
is a reason why the PC slayed the mainframe -
transaction costs.
Dragging this back to crypto.  Sun recently set up
their Java crypto environment ("JCE") with special
"signed providers."  They then added International
Policy Files.  Even though Sun (for free and
without complaint) gives away sigs on signing
keys and allows anyone to download and install
the International Policy Files, the process has
slowed to a crawl.  The Number 1 Bug in Java
crypto is the lack of the Sun Policy Files [2].
And, the resultant effect will of course be more
and more insecure apps...
Exerting control has huge costs.  There had
better be a huge reason [3].  This is rarely
the case;  and almost all security concerns can
be done by being more careful in software, or by
fudged by bypassing the weaknesses with external
[1] We looked at putting gold currencies alongside
national currencies on smart cards in 1999 or so,
and found that not only could we not do this, we
could not even get access to the development kits,
the people, nor the smart cards.  It was an
institutional brick wall.  Part of the brick wall
was the doorway that permitted only institutional-
sized players through.  Nowadays, the gold currencies
do more transactions in a day than many smart card
monies did in a year.
[2] I don't blame Sun for this.  I blame us
cryptoplumbers for not recognising the bait
and switch.  At some point we'll ditch the
Sun architecture and get back to free crypto.
[3] Other examples of institutional plays that
stumbled over the transaction costs issue:
cellular phone apps that can now be installed
by the users, and CA-PKI model / HTTPS servers,
where some nominal security was available if
you paid for a cert.

@_date: 2003-12-16 10:53:51
@_author: Ian Grigg 
@_subject: Financial Cryptography '04 - accepted papers 
The Financial Cryptography 2004 conference has quietly (!)
announced their accepted papers:
    Read on for the full programme.......
       Accepted Papers
 The Ephemeral Pairing Problem
 Jaap-Henk Hoepman
 Efficient Maximal Privacy in Voting and Anonymous Broadcast
 Jens Groth
 Practical Anonymity for the Masses with MorphMix
 Marc Rennhard and Bernhard Plattner
 Call Center Customer Verification by Query-Directed Passwords
 Lawrence O'Gorman, Smit Begga, and John Bentley
 A Privacy-Friendly Loyalty System Based on Discrete Logarithms
 over Elliptic Curves
 Matthias Enzmann, Marc Fischlin, and Markus Schneider
 Identity-based Chameleon Hash and Applications
 Giuseppe Ateniese and Breno de Medeiros
 Selecting Correlated Random Actions
 Vanessa Teague
 Addressing Online Dictionary Attacks with Login Histories
 and Humans-in-the-Loop
 S. Stubblebine and P.C. van Oorschot
 An Efficient and Usable Multi-Show Non-Transferable Anonymous
 Credential System
 Pino Persiano and Ivan Visconti
 Electronic National Lotteries
 Elisavet Konstantinou, Vasiliki Liagokou, Paul Spirakis,
 Yannis C. Stamatiou, and Moti Yung
 Mixminion: Strong Anonymity for Financial Cryptography
 Nick Matthewson and Roger Dingledine
 Interleaving Cryptography and Mechanism Design: The Case of
 Online Auctions
 Edith Elkind and Helger Lipmaa
 The Vector-Ballot E-Voting Approach
 Aggelos Kiayias and Moti Yung
 Microcredits for Verifiable Foreign Service Provider Metering
 Craig Gentry and Zulfikar Ramzan
 Stopping Timing Attacks in Low-Latency Mix-Based Systems
 Brian N. Levine, Michael K. Reiter, and Chenxi Wang
 Secure Generalized Vickrey Auction without Third-Party Servers
 Makoto Yokoo and Koutarou Suzuki
 Provable Unlinkability Against Traffic Analysis
 Ron Berman, Amos Fiat, and Amnon Ta-Shma

@_date: 2003-12-18 15:37:33
@_author: Ian Grigg 
@_subject: Ross Anderson's Trusted Computing FAQ 
Ross Anderson's Trusted Computing FAQ has a lot
to say about recent threads:

@_date: 2003-12-20 11:03:57
@_author: Ian Grigg 
@_subject: I don't know PAIN... 
What is the source of the acronym PAIN?
Lynn said:
I.e., its provenance?
Google shows only a few hits, indicating
it is not widespread.

@_date: 2003-12-20 12:15:51
@_author: Ian Grigg 
@_subject: Difference between TCPA-Hardware and a smart card (was:example:  
<4.2.2.20031220065839.00acbef0
You may have hit upon something there, Lynn.
One of the (many) reasons that PKI failed is
that businesses simply don't outsource trust.
If the use of TCPA is such that the business
must trust in its workings, then it can fairly
easily be predicted that it won't happen.  For
business, at least (that still leaves retail
and software sales based on IP considerations).
It is curious that in the IT trust business,
there seems to be a continuing supply of
charlatan ventures.  Even as news of PKI
slinking out of town reaches us, people are
lining up to buy tickets for the quantum
crypotagraphy miracle cure show and bottles
of the new wonder TCPA elixir.

@_date: 2003-12-20 13:34:37
@_author: Ian Grigg 
@_subject: Difference between TCPA-Hardware and other forms of trust 
This was more than just a side effect, it was also
the genesis of the earliest successes with smart
card money.
The first smart card money system in the Netherlands
was a service-station system for selling fuel to
truck drivers.  As security costs kept on rising,
due to constant hold-ups, the smart card system
was put in to create stations that had no money
on hand, so no need for guards or even tellers.
This absence of night time staff created a great
cost saving, and the programme was a big success.
Unfortunately, the early lessons were lost as time
went on, and attention switched from single-purpose
to multi-purpose applications.

@_date: 2003-12-20 15:26:03
@_author: Ian Grigg 
@_subject: I don't know PAIN... 
Is this unpublished?  You might consider
submitting it to WEC:

@_date: 2003-12-22 20:17:30
@_author: Ian Grigg 
@_subject: Difference between TCPA-Hardware and a smart card (was:  
<5.1.0.14.2.20031220142208.02850190
Sorry, yes, each actual smart card is, at
the margin, cheap.  But, as a project, the
smart card is expensive.  There's a big
difference between project costs and the
marginal cost, and that generally makes
*the* difference.
I suppose the confusion is endemic;  as
everyone thinks about the project costs in
terms of "per person" and this is considered
by assumption to be one smart card per person,
but the cost per person is not the single 50c
per actual smart card.
Smart cards are a lot like Christmas, it's
not the gift, but the act of giving that
makes it special.
For example, yes.  So it all comes down to
whether you can afford to role out the hardware
to all the vendors, and all the associated
nodes.  At this point, the penny drops, and
smart cards start looking very expensive.
Hence, to date, only single-purpose projects
have succeeded - ones where the economics
where clearly based on narrowly focused,
single activities:  phones, transit systems,
etc, and they justified themselves on those
activities, alone, without relying on the
economics of unmeasurable and unmeetable
PS: all those Europeans with all those
smart cards in their pockets - ask them
how many times they use the smart card

@_date: 2003-12-23 11:32:42
@_author: Ian Grigg 
@_subject: Ousourced Trust (was Re: Difference between TCPA-Hardware anda smart  
Of course they don't.  What they do is they
outsource the collection of certain bases of
information, from which to make trust decisions.
The trust is still in house.  The reports are
acquired from elsewhere.
That's the case for D&B and credit reporting.
For the SEC, I don't understand why it's on
that list.  All they do is offer to store the
filings, they don't analyse them or promise
that they are true.  They are like a library.
International Banking Letters of Credit - that's
money, not trust.  What happens there is that
the receiver gets a letter, and then takes it
to his bank.  If his bank accepts it, it is
acceptable.  The only difference between using
that and a credit card, at a grand level, is
that you are relying on a single custom piece
of paper, with manual checks at every point,
rather than a big automated system that mechanises
the letter of credit into a piece of plastic.
(Actually, I'm totally unsure on these points,
as I've never examined in detail how they work :-)
Insurance - is not the outsourcing of trust,
but the sharing of risks.
Unfortunately, most of the suppliers of these
small factors in the overall trust process of
a company, PKI included, like to tell the
companies that they can, and are, outsourcing
trust.  That works well, because, if the victim
believes it (regardless of whether he is doing
it) then it is easier to sell some other part
of the services.  It's basically a technique
to lull the customer into handing over more
cash without thinking.
But, make no mistake!  Trust itself - the way
it marshalls its information and makes its
decisions - is part of the company's core
business.  Any business that outsources its
core specialties goes broke eventually.
And, bringing this back to PKI, the people
who pushed PKI fell for the notion that
trust could be outsourced.  They thus didn't
understand what trust was, and consequently
confused the labelling of PKI as trust with
the efficacy of PKI as a useful component
in any trust model (see Lynn's post).
I agree with this, and all the rest.  The no-
risk computing school is fascinated with the
possibility of eliminating entire classes of
risk, so much so that they often introduce
excessive business costs, which results in
general failures of the whole crypto process.
In theory, it's a really good thing to
eliminate classes of attack.  But it can
carry a heavy cost, in any practical
We are seeing a lot more attention to
opportunistic cryptography, which is a good
thing.  The 90s was the decade of the no-risk
school, and the result was pathetically low
levels of adoption.  In the future, we'll see
a lot more bad designs, and a lot more corners
cut.  This is partly because serious crypto
people - those you call the crypto-elite - have
burnt out their credibility and are rarely
consulted, and partly because it simply costs
too much for projects to put in a complete
and full crypto infrastructure in the early
We are looking at the same word and seeing a
very different meaning :)

@_date: 2003-12-23 11:50:37
@_author: Ian Grigg 
@_subject: IP2Location.com Releases Database to Identify IP's Geography 
These have existed for some time.  Google knows
where they are, although they were a little tough
to find.

@_date: 2003-12-23 14:33:45
@_author: Ian Grigg 
@_subject: Non-repudiation (was RE: The PAIN mnemonic) 
I would second this call for some definition!
FWIW, I understand there are two meanings:
   some form of legal inability to deny
   responsibility for an event, and
   cryptographically strong and repeatable
   evidence that a certain piece of data
   was in the presence of a private key at
   some point.
Carl and Ben have rubbished "non-repudiation"
without defining what they mean, making it
rather difficult to respond.
Now, presumably, they mean the first, in
that it is a rather hard problem to take the
cryptographic property of public keys and
then bootstrap that into some form of property
that reliably stands in court.
But, whilst challenging, it is possible to
achieve legal non-repudiability, depending
on your careful use of assumptions.  Whether
that is a sensible thing or a nice depends
on the circumstances ... (e.g., the game that
banks play with pin codes).
So, as a point of clarification, are we saying
that "non-repudiability" is ONLY the first of
the above meanings?  And if so, what do we call
the second?  Or, what is the definition here?
as "legal non-repudiability" or "cryptographic
non-repudiability" so as to reduce confusion.

@_date: 2003-12-26 19:35:38
@_author: Ian Grigg 
@_subject: Non-repudiation (was RE: The PAIN mnemonic) 
Ah.  Now I understand.  The verb is wrong, as it
necessarily implies the act of the human who is
accused of the act.  (And, thus, my claim that it
is possible, was also wrong.)
Whereas the cryptographic property implies no such
thing, and a cryptographic actor can only affirm
or not, not repudiate.  I.e., it's a meaningless
Would "irrefutable" be a better term?  Or non-
refutability, if one desires to preserve the N?
The advantage of this verb is that it has no
actor involved, and evidence can be refuted on
its own merits, as it were.
As a test, if one were to replace repudiate
with refute in the ISO definition, would it
then stand?
I think more is needed.  A better definition is
required, as absence is too easy to ignore.  People
and courts will use what they have available, so it
is necessary to do more; indeed it is necessary to
actively replace that term with another.
Generally, the way the legal people work is to
create simple "tests".  Such as:
  A Document was signed by a private key if:
  1. The signature is verifiable by the public key,
  2. the public key is paired with the private key,
  3. the signature is over a cryptographically strong
     message digest,
  4. the Message Digest was over the Document.
Now, this would lead to a definition of irrefutable
evidence.  How such evidence would be used would be
of course dependent on the circumstances;  it then
becomes a further challenge to tie a human's action
to that act / event.
PS: Doing a bit of googling, I found the ISO definition
to be something like:
But, the actual standard costs money (!?) so it is
not surprising that it is the subject of much
controversy :)

@_date: 2003-12-27 15:11:39
@_author: Ian Grigg 
@_subject: Non-repudiation (was RE: The PAIN mnemonic) 
Ah.  I did read your paper, but deferred any comment
on it, in part because I didn't understand what its
draft/publication status was.
Ben Laurie said:
You didn't state which of the two definitions
you were rubbishing, so I shall respond to both!
Let's take the first definition - your "technical
definition" (2.7):
  "Non-repudiation", in its technical sense, is a property of a communications
  system such that the system attributes the sending of a message to a person
  if, but only if, he did in fact send it, and records a person as having received
  a message if, but only if, he did in fact receive it. If such systems exist at all,
  they are very rare.
  Non-repudiability is often claimed to be a property of electronic signatures of
  the kind described above. This claim is unintelligible if "non-repudiation" is
  used in its correct technical sense, and in fact represents an attempt to confer a
  bogus technical respectability on the purely commercial assertion the the owners
  of private keys should be made responsible for their use, whoever in fact uses
  them.
Some comments.
1. This definition seems to be only one of the many
out there [1].  The use of the term "correct technical
sense" then would be meaningless as well as brave
without some support of references.  Although it does
suffice to ground the use within the paper.
2. The definition is muddied by including the attack
inside the definition.  The attack on the definition would
fit better in section 6. "Is \non-repudiation" a useful
3. Nothing in either the definition 2.7 or the proper
section of 6. tells us above why the claim is "unintelligable".
To find this, we have to go back to Carl's comment
which gets to the nub of the legal and literal meaning
of the term:
    "To me, "repudiation" is the action only of a human being (not of a key)..."
Repudiate can only be done by a human [2].  A key cannot
repudiate, nor can a system of technical capabilities [3].
(Imagine here, a debate on how to tie the human to the
That is, it is an agency problem, and unless clearly
cast in those terms, for which there exists a strong
literature, no strong foundation can be made of any
conclusions [4].
4. The discussion resigns itself to being somewhat
dismissive, by leaving open the possibility that
there are alternative possibilities.  There is
a name for this fallacy, stating the general and
showing only the specific, but I forget its name.
In the first para, 2.7, it states that "If such systems
exist at all, they are very rare."  Thus, allowing
for existance.  Yet in the second para, one context
is left as "unintelligable."  In section 6, again,
"most discussions ... are more confusing than helpful."
This hole is created, IMHO, by the absence of Carl's
killer argument in 3. above.  Only once it is possible
to move on from the fallacy embodied in the term
repudiation itself, is it possible to start considering
what is "good" and useful about the irrefutability (or
otherwise) of a digital signature [5].
I.e., throwing out the bathwater is a fine and regular
thing to do.  Let's now start looking for the baby.
Which brings us to your second definition, again,
in 2.7:
    To lawyers, non-repudiation was not a technical legal term before techies gave
    it to them. Legally it refers to a rule which defines circumstances in which a
    person is treated for legal purposes as having sent a message, whether in fact
    he did or not, or is treated as having received a message, whether in fact he
    did or not. Its legal meaning is thus almost exactly the opposite of its technical
    meaning.
I am not sure that I'd agree that the legal
fraternity thinks in the terms outlined in the
second sentance.  I'd be surprised if the legal
fraternity said any more than "what you are
trying to say is perhaps best seen by these
sorts of rules..."
Much of law already duplicates what is implied
above, anyway, which makes one wonder (a) what
is the difference between the above and the
rules of evidence and presumption, etc, etc
and (b) why did the legal fraternity adopt
the techies' term with such abandon that they
didn't bother to define it?
In practice, the process of dispute resolution is
very strongly oriented towards addressing evidence
and moving it to a supported conclusion.  A
digital signature is evidence, and conclusions
can be supported based on that evidence;  what
we techies should perhaps draw from this is
that our efforts to define any new terms and any
new procedures are childish in comparison to the
logic and procedures developed in the forum of
the law over the last few millenia.
Next para:
    Such a rule may be imposed by law, as for example this rule:
       The person making the return to the Controller
       shall be presumed to be the person identifed as
       such by any relevant feature of the electronic
       return system.[2]
I disagree [6].  That clause creates a presumption.
Nothing in the clause states that this cannot be
repudiated.  In fact, careful examination of the
preceeding clause concerning the time of the return
indicates that "conclusive presumption" was preferred
in this alternate time context.  Thus, repudiation
of the party is anticipated, and repudiation of the
time is "ruled against" [7].
Further, repudiation as a word or concept does not
appear in that act.  What happens is that the act
ties down the event of the return;  it does not
state that the return cannot be repudiated (although
I grant that might occur elsewhere).  In Section
(4L)(a) it specifically raises a case of potential
The second (surveyors) clause/example is the same -
it creates a presumption that can always be repudiated.
In practice, the repudiation is an uncertain thing, as
is all repudiations.  But, it is not possible to
conclude, AFAIK, in law, that the clause is non-
repudiable, simply because it states so.
Which all leads to this:  I don't think you have
nailed down any legal definition of non-repudiability.
Or, if that is what it is, the legal fraternity also
knows that the techies' definition is a chimera, a
mere hope on which to attach the use of dig sigs, in
which case, this logic needs to be explained within
the paper - that the definition doesn't exist.
In essence, I can imagine a lawyer saying "yes,
we already do what you are aiming for (presumption),
but, your technical non-repudiability is impossible
under the law because the law doesn't think in those
Nor does the paper nail why it doesn't make sense.
It omits the killer argument that the process of
the dispute resolution moves from evidence to
application of law to ruling;  a statement of
non-repudiability is meaningless in that context.
(A lawyer would need to make this argument more
carefully - I am not such and am conscious that I
also haven't nailed it myself :)
I note the English & Wales legal context.  If there
is a law that covers non-repudiation, by technical
means, that would definately effect the nature of
the discussion.
[1] ref: Lynn's post that pointed at the ISO SC27
[2]        1.To reject the validity or authority
         of: "Chaucer... not only came to
         doubt the worth of his
         extraordinary body of work, but
         repudiated it" (Joyce Carol
         Oates).        2.To reject emphatically as
         unfounded, untrue, or unjust:
         repudiated the accusation.        3.To refuse to recognize or pay:
         repudiate a debt.        4.
             a.To disown (a child, for
                example).              b.To refuse to have any
                dealings with.
For most relevance, examine 3., 4.a.
[3] This is in part an assumption, as I am assuming
here that AI and similar things cannot enter
into contracts, or, if they do so, they are
then persons, and thus the model stands.  We
can test this by determining as to whether
these "new persons" can take standing in a
forum of dispute resolution.  Lawyers might
like to comment on that, and as I say, it's
an assumption!
[4] Agency problems are ones where a principal
delegates powers to an agent, and those powers
are used or abused according to the incentives
and circumstances.  Canonically, a shop owner
employs an assistant to mind the counter;  does
the money taken in by sales made by the assistant
go into his pocket, or her cashbox?
[5] Irrefutability I proposed in an earlier
email, and as yet lacks any credibility.
[6] Link reproduced:
[7] ruled against more firmly, perhaps.  There
is nothing stopping a repudiation of the time,
and presenting the reasons to the judge.  That's
part of the process.

@_date: 2003-12-28 14:00:08
@_author: Ian Grigg 
@_subject: CIA - the cryptographer's intelligent aid? 
I would agree that CIA reins supreme.  It's easy to
remember, and easy to teach.  It covers the basic
crypto techniques, those that we are sure about and
can be crafted simply with primitives.
CIA doesn't overreach itself.  CAIN, by introducing
non-repudiation, brings in a complex multilayer
function that leads people down the wrong track.
PAIN is worse, as it introduces Privacy instead of
Confidentiality.  The former is a higher level term
that implies application requirements, arguably, not
a crypto term at all.  At least with Confidentiality
it is possible to focus on packets and connections
and events as being confidential at some point in
time; but with Privacy, we are launched out of basic
crypto and protocols into the realm of applications.

@_date: 2003-12-28 15:35:23
@_author: Ian Grigg 
@_subject: Repudiating non-repudiation 
In response to Ed and Amir,
I have to agree with Carl here and stress that the
issue is not that the definition is bad or whatever,
but the word is simply out of place.  Repudiation is
an act of a human being.  So is the denial of that
or any other act, to take a word from Ed's 1st definition.
We can actually learn a lot more from the legal world
here, in how they solve this dilemma.  Apologies in
advance, as what follows is my untrained understanding,
derived from a legal case I was involved with in
recent years [1].  It is an attempt to show why the
use of the word "repudiation" will never help us and
will always hinder us.
The (civil) courts resolve disputes.  They do *not*
make contracts right, or tell wrong-doers to do the
right thing, as is commonly thought.
Dispute resolution by definition starts out with a
dispute, of course.  That dispute, for sake of argument,
is generally grounded in a denial, or a repudiation.
One party - a person - repudiates a contract or a
bill or a something.
So, one might think that it would be in the courts'
interest to reduce the number of repudiations.  Quite
the reverse - the courts bend over backwards, sideways,
and tie themselves in knots to permit and encourage
repudiations.  In general, the rule is that anyone
can file *anything* into a court.
The notion of "non-repudiation" is thus anathema to
the courts.  From a legal point of view, we, the
crypto community, will never make headway if we use
this term [2].  What terms we should use, I suggest
below, but to see that, we need to get the whole
process of the courts in focus.
Courts encourage repudiations so as to encourage
all the claims to get placed in front of the forum
[3].  The full process that is then used to resolve
the dispute is:
   1. filing of claims, a.k.a. "pleadings".
   2. presentation of evidence
   3. application of law to the evidence
   4. a reasoned ruling on 1 is delivered based on 2,3
Now, here's where cryptographer's have made the
mistake that has led us astray.  In the mind of a
cryptographer, a statement is useless if it cannot
be proven beyond a shred of doubt.
The courts don't operate that way - and neither does
real life.  In this, it is the cryptographers that
are the outsiders [4].
What the courts do is to encourage the presentation
of all evidence, even the "bad" stuff.  (That's what
hearings are, the presentation of evidence.)
Then, the law is applied - and this means that each
piece of evidence is measured and filtered and
rated.  It is mulled over, tested, probed, and
brought into relationship with all the other pieces
of evidence.
Unlike no-risk cryptography, there isn't such a
thing as bad evidence.  There is, instead, strong
evidence and weak evidence.  There is stuff that
is hard to ignore, and stuff that doesn't add
much. But, even the stuff that adds little is not
discriminated against, at least in the early phases.
And this is where the cryptography field can help:
a digital signature, prima facea, is just another
piece of evidence.  In the initial presentation of
evidence, it is neither weak nor strong.
It is certainly not "non-repudiable."  What it is
is another input to be processed.  The digsig is
as good as all the others, first off.  Later on,
it might become stronger or weaker, depending.
We, cryptographers, help by assisting in the
process of determining the strength of the
evidence.  We can do it in, I think, three ways:
Firstly, the emphasis should switch from the notion
of non-repudiation to the strength of evidence.  A
digital signature is evidence - our job as crypto
guys is to improve the strength of that evidence,
with an eye to the economic cost of that strength,
of course.
Secondly, any piece of evidence will, we know, be
scrutinised by the courts, and assessed for its
strength.  So, we can help the process of dispute
resolution by clearly laying out the assumptions
and tests that can be applied.  In advance.  In
as accessible a form as we know how.
For example, a simple test might be that a
receipt is signed validly if:
   a. the receipt has a valid hash,
   b. that hash is signed by a private key,
   c. the signature is verified by a public
      key, paired with that private key
Now, as cryptographers, we can see problems,
which we can present as caveats, beyond the
strict statement that the receipt has a valid
signature from the signing key:
   d. the public key has been presented by
      the signing party (person) as valid
      for the purpose of receipts
   e. the signing party has not lost the
      private key
   f. the signature was made based on best
      and honest intents...
That's where it gets murky.  But, the proper
place to deal with these murky issues is in
the courts.  We can't solve those issues in
the code, and we shouldn't try.  What we should
do is instead surface all the assumptions we
make, and list out the areas where further
care is needed.
Thirdly, we can create protocols that bear
in mind the concept of evidence.  That means
we use various techniques such as signed
receipts, logs, sharing of records and chains
of signatures to create pieces of evidence.
We use the careful techniques of protocol
design to marshal sufficient evidence of
strength to make it easy to resolve any
questions;  before they become disputes,
and ideally, before they leave the protocol!
And, when these questions do become disputes,
we try and make it easy (read: cheap) to
present strong evidence to those resolving
any dispute.
[1] It was highly instructive, and I'd almost
recommend all to get in trouble with the courts
at least once in your lives, if only it wasn't
so darn destructive of ones life!
[2] It's even worse than the signature.  At least
there is some resemblance between the process and
result of a digital signature and a legal signature.
With (non)-repudiation, however, cryptographers are
saying that the entire meta-concept of the court
is wrong.
[3] Courts actually have a rule, that, only claims
made up front can be heard - so you had better
get your repudiations up there in the beginning!
[4] This is a characteristic of the no-risk school
of cryptography, but even economic cryptographers
fall into this trap with regularity.

@_date: 2003-07-02 12:47:27
@_author: Ian Grigg 
@_subject: Mozilla tool to self-verify HTTPS site 
I'm currently reading Eric Rescorla's SSL&TLS book,
and a significant proportion of the problems within
the SSL/TLS protocol seem to come from the assumption
that the cert should be supplied *within* the core
protocol, and not outsourced to a higher layer.
I.e., if SSL/TLS was re-written around this simple
separation into two separate sub-protocols:
    1. get the/a/all certificate(s)
    2. use the key within
a lot of the complexity would disappear.
(I understand the argument that SSL/TLS does not
"require" a cert, but to all intents and purposes,
everything and everyone assumes it, AFAICS.  As a
practical issue, as it effects the implementations
out there, I'm not sure it makes sense to even
consider SSL/TLS without certs.)
It seems to me to be a developing principle.
We are all agreed that the delivery of (any/the)
cert is a very hard problem.  We are mostly agreed
that it is an unsolved problem.
So, as a corollary to the "hard problem," the key
to use as the starting point for any crypto protocol
should be provided to it, not bootstrapped within.
(I wonder if there is a pithy way of stating this
principle?  Good crypto divorces bad PKIs?  Cost
effective crypto starts with an assumed key?)
(I'm not quite sure what the issue here is with
PGP ... it works fine without any certification,
and it works slightly better when 3rd party sigs
("certs"?) are added by the user?  Although I
grant you that the key structure is .. costly
to code, to the point of being impermeable to
new implementations.)
Yes, my company's protocol (SOX) "extends" the
certificate layer by using OpenPGP.  Configuring
the issuance of a new monetary contract is a bit
of a bear, in no small part due to the chain of
signatures in the OpenPGP "PKI" that we use.  But,
it works, and it doesn't feel as though the big
costly "PKI" process built out of OpenPGP slows
down adoption any.
[ We tried x.509 for a while, but it was a
mistake;  it lacked cleartext signing (minor
point, we hacked our own) and its fixed PKI
doesn't map to financial relationships, which
are based on WoT, not centralised permissions. ]
SSH chooses the simplest solution - opportunistic
crypto - create the certs on demand and caching them
for future checking.  That is the best success formula
I have seen so far.

@_date: 2003-07-08 14:24:39
@_author: Ian Grigg 
@_subject: LibTomNet [v0.01] 
It's not just you.  The field seems to be evenly
divided between those who view SSL as a mess, and
those who view it as the only sane choice because
so much attention has been put on it.
(That's just my seat of the pants feel for it, in
gauging the public and private responses to the
series of rants on SSL I've written.  And it isn't
just a recent development, I've known other far
more competent (than me) cryptoplumbers who were
dissatisfied with SSL, going back as far as 1997.)
Using SSL as a base for a new set of requirements
seems to be about as complicated as a competant
cryptoplumber doing his own.  Obviously, SSL will
give you a jumpstart in security over your homegrown
crypto, but less obviously, the complications and
misturns built into SSL make tuning it to your
application a much harder task, and achieving a
unified security model is difficult because it's
not a simple starting point.
The main thing that reduces SSL's applicability to
real world problems come down to the assumption of
certificates as part and parcel of the security
model.  Also, the threat model is unrealistic, and
the consequent security properties seem more to
derive from "what we can do" rather than "this is
what your application demands and needs."
It's definately not just you - but one of the reasons
that it feels like that is that the SSL supporters
tend to protect their franchise very aggresively.
Which is odd, really, I haven't myself worked out
why the supporters of a particular protocol are
so adamant that one should not experiment in a
field as complicated and challenging as crypto.
Their attitude is religious, it is tantamount to
saying that you shouldn't dare to assault the ivory
tower.  SSL is the officially sanctioned way of
doing Internet crypto.  Capice?
Which is a total crock.  If SSL can't make up its
credibility in the open market place, then it isn't
worth idolising.
If you looked at it - and you say you did - and
concluded you could do better on your own, then
more power to you.  And us all.
An entire generation of crypto engineers have been
fed this notion that they needn't bother with their
own, which has had the net result of reducing crypto
knowledge, reducing security, and leaving the net
reliant on an infrastructure that just can't meet
its own needs, let alone the needs of users.
Somebody said we were the A-team.  John Gilmore, I
think, but that's from memory.
Nonsense.  We aren't even up to being the C-team,
we don't make the team.  And we won't ever until
we cast off the shackles of rote acceptance, and
start challenging SSL on its inadequacies.
Tom, you are not alone!  Dabble on!

@_date: 2003-07-08 20:07:44
@_author: Ian Grigg 
@_subject: LibTomNet [v0.01] 
Right.  But better is not a binary choice in real
life.  SSL is only "better" if it exceeds all
requirements when compared against a product
that has only those requirements.
One needs to look at the requirements.  Tom's
requirements didn't include message integrity,
if I saw correctly, because he had something
in there at a higher layer that covered him
there.  That's good.
Does Tom require certs?  No, or *even better*
he explicitly outsourced that requirement to
another layer, thus allowing the protocol to
be simpler.  This is a great thing, and my
reading of the protocol of SSL - from Eric's
book - indicates that SSL would benefit from it
Does he require replay protection?  Is he worried
about MITM?  What about authenticity?  These all
need to be established before you can compare any
The whole world doesn't want or need perfect
channel security.  That's because some parts of
the world have different needs.

@_date: 2003-07-08 23:07:16
@_author: Ian Grigg 
@_subject: Fwd: [IP] A Simpler, More Personal Key to Protect OnlineMessages 
I'm not sure that's the case!
There are some markets out there where there are some
contradictory rules.  By this I mean, all messages must
be private, and all messages must be readable.
Now, the challenges that these markets must meet point
them in the direction of having a central server doing
key escrow.  But, the central server is not allowed to
escrow the messages or be able to read the messages.
A further challenge is that these markets are full off
leakages, and so what is needed is a way of taking the
crypto capability away from users.
This solution seems to do this latter part, in that it
achieves the contradictory requirements of making every
message unreadable, but crackable, and it - in theory -
does not give users any ability to do their own crypto
and thus bypass the system.
A (purely hypothetical) example, to clarify what this
market looks like:  Imagine the NSA had to outsource
its encrypted comms.  They want all messages to be secret
because .. that's kind of their mission.  But, they are
worried about moles in the organisation, so they want
to be able to open up the whole shebang somehow and go
trolling for data.
So how do we rationalise all this?  Simple - the people
who use the system are not the people who buy the system.
The market for this system is not "users" but corporates
with special needs.  In fact if we look at the website,
it's oriented to selling into 4 markets:  corporates,
financial, health, and government,  If we ignore the
first as a catchall phrase, the remaining three all have
special needs when it comes to privacy.  And those needs
aren't so much to do with the user as with the organisation.
It was for these markets that companies like PGP Inc put
in their fabled alternate decryption key, and companies
like Hushmail sell "corporate packages."

@_date: 2003-07-09 13:09:55
@_author: Ian Grigg 
@_subject: replay & integrity 
Welcome to the applications world!
Integrity:  Financial protocols that use crypto
(as opposed to ones abused by crypto) generally
include signed messages.  The signature provides
for its own integrity, as well as a few other
Replay:  One of the commonest problems in HTTPS
sites is replay failure.  The solution is well
known out in the real world - you have to have
replay prevention at the higher layers.
(Credit card processors have replay prevention
So, some protocols don't need replay prevention
from lower layers because they have sufficient
checks built in.  This would apply to any protocols
that have financial significance;  in general, no
protocol should be without its own unique Ids.
I wouldn't say that this is a good reason to take
these features out of SSL.  But assuming they are
"needed" is a cautious assumption, and assuming
that SSL meets the needs for replay & integrity
makes even less sense when we are dealing with a
serious top-to-bottom security model.
It's simply the case that a serious financial
protocol would have to have its own replay &
integrity, because its threat model and failure
model is so much broader than SSL's.  For example,
a serious payments scenario works across end-to-
end, and assumes that nodes on both end-points
can be compromised and/or faulty.  And, it's not
only just faults, many higher layers actively
replay as part of the protocol.
SSL just doesn't address the security needs of
protocols as well as all that.  Where I've seen
it used, the core need for it is privacy of the
data stream, not anything else.
(As a sort of oxymoron, a payments or similar
protocol that didn't have its own replay & integrity
would not work.  Ideally, a good test of a payments
protocol is to see if it would work over unprotected
UDP or email.  Some do and some don't.)

@_date: 2003-07-10 10:49:28
@_author: Ian Grigg 
@_subject: SSL 
Most people seem to think the RFC is unreadable,
so ...
I am reading Eric Rescorla's book at the moment,
and if you are serious about SSL, it is worth the
price to get the coverage.  It's well written,
and relatively easy to read for a technical book.
It costs a steep $50.  It's not a "For Dummies."
You have to be comfortable with all sorts of things
It's giving me the intellectual capital to attack
the engineering failures therein and surrounding
the deployment of same.  Maybe Eric will offer me
$100 for my annotated copy just to shut me the
f**k up ;-)   I've so far discovered .... well,
that's another story, and I sense Perry's keenness
to be less redundant and stay more ontarget.
PS: next step is Ferguson & Schneier's recent book
which has been described as "how to re-invent SSL."

@_date: 2003-07-14 15:47:54
@_author: Ian Grigg 
@_subject: Announcing httpsy://, a YURL scheme 
I agree that the last part is a little aggressive.
"Unnecessary" is so ... final.  What this does is
to place the URL as the point of trust, rather
than placing a CA at the point of trust.  It also,
as an aside, allows the CA to remain as a upstream
point of trust.  (Or, so I read it?)
I'm not sure whether the intention is to allow
filtering of URLs and caching of hashes found
therein!  That seems the obvious intention; to
scatter a thousand URLs out there, and by the
time the browser has a desire to upgrade the
connection, it has already established a record
of hashes.
Is that the case, Tyler?
Essentially, trust is established at the point of
"when you trust this URL, you can trust the connection."
That's quite valuable.  It separates out the trust
into two separate protocols, which makes both much
stronger.  In fact, I think it is essential for a
sound security model.  It's what makes SSH so efficient,
and it's what makes SOX so simple.  Obversely, I
suspect, but am not totally convinced yet, that it
is at the root of SSL's issues as a protocol.
It can be seen by inspection that the second part;
booting from the URL to a secured connection, is, on
the face of it, viable.
How is the URL trusted, is the big question.  If
the answer is that it is no less trusted than the
ordinary http URL, and less trusted than a https
URL, than that's still valid, and a vast improvement
on today's situation of 99% untrusted browsing.
Well.  I worry that your criticism rides on a circular
To unwind, it is a statement of definition that if the
threat model is not covered, then the communications
are insecure.  If the threat model *is* met, then the
communications are secure.
So the question devolves to "what is the threat model?"
1. Clearly, the threat model does not include MITM.  That's
quite practical in today's world.  See my rants on the
subject for my logic.
2.  Revocation of a private server key seems an unusual
need.  Granted, in a PKI of some dynacism (?), this would
be a real shortfall.  But for a website server, I don't
see the need for revocation.  It's not as if private
keys are stolen anytime, much, and if the server cert
changes for operational reasons (end of year, etc),
then that moves the YURL into the domain of "broken
link."  So writing revocation out of the threat model
seems like a practical decision.
3. Spoofing, however, seems unavoidable as an issue.  It's
trivially easy to construct a link which purports to
be something else.  All the YURL does is to prove that
it is the real underlying link, without helping any
in the spoofed presentation problem.
That is, if I mail a million BoA account holders with
my YURL and some standard text, it will work as well
as if I didn't use a YURL, so adding the security
aspect might create a false expectation, and nothing
Is the YURL considered to be a generic replacement for
links?  If so, I don't see how it changes the big failure
of secure browsing, which is the spoof attack.
Mind you, I would, in the spirit of addressing the real
security agenda, applaud any attempt to propose fixing
up the current bad-and-getting-worse situation.  It would
seem that sharing and spreading the hash of server certs
is definately a viable direction.

@_date: 2003-07-14 17:13:30
@_author: Ian Grigg 
@_subject: Announcing httpsy://, a YURL scheme 
OK, so that's YOUR threat model.  It isn't mine,
and I'm guessing it isn't Tyler's (I can base
this on Tyler's new post, where he implies that
his threat model is "like PGP & SSH").
The issue then becomes, why is your threat model
more relevant than mine, or v.v.  That has not
been addressed in this thread, merely implied.
(I grant you, this is a really big issue, and
cannot be addressed in a simple thread.  But,
that doesn't make it any less valid!)
Yeah, all that is doing is assuming that this
threat model is "the one!"  That has no basis,
although, I grant you that it is too commonly
done by by far, and is good enough to fool most
of the people most of the time.  (Including, to
be fair, by Tyler himself, as "like SSH & PGP"
is not a threat model.)
It makes no sense to talk about this threat model
or that one, if one assumes that all threat models
are the same.
We ask "what's your threat model" - WYTM - for a
reason:  because threat models differ.
To jump from the empty set to "must have spoofing/
MITM/compromise" to be a threat model is to say
that there is little merit in the concept of threat
models.  That is, in effect, attempting to capture
the meaning of independant and validated threat
models to push ones own view of what is the "one
big threat model."
E.g., that's marketing, not science (and nowhere
near engineering, which is where we should be
heading, IMHO!).
To take a hypothetical example:  a smart card has
a key inside it.  It's really hard to get the key
out.  Even if you get it out, you can't put it back
into a smart card without substantial difficulties,
because everyone looks at the cards before talking
to them.  So, we conclude, no-one can even make a
pretend card without spending more than it's worth.
Then, under those assumptions, why would you include
"compromise of the key" in your threat model?  Far
better to ignore that case, and wait until it happens.
(As I say, simply hypothetical.)

@_date: 2003-07-14 20:29:07
@_author: Ian Grigg 
@_subject: [Fwd: BugTraq - how to coverup the security] 
Over on BugTraq, there is a new security flaw being
demonstrated that allows a page to cover up various
of the security components for an IE browser.
I can't see them on my browser, but what I saw on an
IE equipped browser was good enough to fool some people.
It's worth checking out!  It really did open eyes
over here!  It's not actually clear to me that any
of the ideas we've discussed here - caching of self-
signed certs, enhanced security displays, etc - will
overcome this.
Just yet more evidence that that the attacker is not
playing by the rules laid down in the secure browser
security model :-/
---------- Forwarded Message ----------
Microsoft Windows users that use Internet Explorer versions
5.5 and up (including 6.0) will want to pay special attention
to the information contained in these recent posts on security
related mailing lists:
of particular interest to SSL secure site (such as e-gold.com) users
is the capability to overwrite the location bar and padlock (with
a borderless popup window).
the notice gives a workaround of "Disable Active Scripting", which
you should consider. another option would be to choose an alternative
web browser, such as Mozilla available at jay w.
jrw at e-gold.com
You are currently subscribed to e-gold-list as: To unsubscribe send a blank email to l
Use e-gold's Secure Randomized Keyboard (SRK) when accessing your e-gold
account(s) via the web and shopping cart interfaces to help thwart
keystroke loggers and common viruses.

@_date: 2003-07-15 20:56:29
@_author: Ian Grigg 
@_subject: [Fwd: BugTraq - how to coverup the security] 
I apologise for the snippety email last night,
I obviously missed the point completely!
That is significant!  Was this code not
folded back into Mozilla?
Oh, my!!  That is a significant effort.
a browser with a security model, and
*tested* it against users.
That implies a *validated* security model
built against realised and known threats.
That's pretty unique!
I've only skimmed it so far, but it looks
like you are well ahead of us here.  I'm
curious to hear how successful you have
been convincing the Mozilla people to
adopt this?

@_date: 2003-07-16 10:34:56
@_author: Ian Grigg 
@_subject: Announcing httpsy://, a YURL scheme 
That's an extraordinarily good idea!  It reminds
me of the technique for determining banks SWIFT
codes.  It seems that the banks often don't really
know themselves, so if you do a google search on
the bank name and the word 'SWIFT' you will find
lots of merchants that already quote it on the net!
Now, one thing that could be done against such a
situation is to poison the search engine with false
URLs in advance of some mailing.  This is relatively
easy, although, will result in a lot of trails which
might give indicators to the perp, so I'd count that
as an expensive technique, and thus, the utility
of the URL searching still remains high.
YURLs are meant to be cached by the browser, I found
that somewhere in the documents but do not recall
where.  The same obviously goes for Simon Josefsson's
crypto-URLs, as mentioned by Trevoer Perrin.
This is
the really neat part, in that when we start to think
of server authentication as a volume & correlation
problem - as expounded on by Mark Miller - rather
than a one-supreme-quality problem, not only do we
achieve sufficient security for most purposes, we
do it with no more than the free net resources.
And, it has the additional benefits of matching
real life, and returning our Internet back to a "no
permission needed" society.

@_date: 2003-07-16 11:53:28
@_author: Ian Grigg 
@_subject: 3 more good ideas: cryptoURLs, SFS, eternal resource locator/WAX 
Below follows a paragraph on each idea to distribute
key hashes within existing web practice, with examples.
This is really good stuff.  The first observation
is that it is more general than Tyler's approach,
in that it explicitly incorporates URIs and also
the existing CA-cert regime.  In this way it will
actually add to the current situation, rather than
require the dismantling or ignoring of the CA-cert.
(Much as I think the CA-cert is 'mostly harmless'
as far as security goes, there is little point in
trying to remove it.  It really needs to be
incorporated into any and all plans to add security
to the browsing experience.)
One observation immediatly springs to mind is that
the above URLs simply don't work with existing
conventions.  That is, when I click on them in my
(admittedly not security conscious) email agent,
the result fails.
To rectify this, one could reverse the location of
the hash:
  crypto:[x509_sha1=4ca4.b169.587f.7258.9f6b.f9ee.bd6e.d7cd.cd6a.d551]:
and similar with the mailto.
Alternatively, use a wrapper convention such as:
  crypto:[x509_sha1=4ca4.b169.587f.7258.9f6b.f9ee.bd6e.d7cd.cd6a.d551]
which would have more defined properties, and
also be more aligned with crypto practice in
A great start!
Turning to Zooko's recommendation:
FTR, here is an example I cribed from the paper
(click on the top right of the above referenced
page to get the actual paper):
  /sfs/sfs.lcs.mit.edu:vefvsv5wd4hz9isc3rb2x648ish742hy/pub/links/sfscvs
suggest that if a URI form can be constructed
that is compatible with existing URIs then this
will have deployment advantages over the above.
Looking at Adam Back's post on "eternal resource locators":
Cribbing again from the paper:
 here which indicates an HTML only approach (the page
pointed to also includes a duplication of the
tiger hash value).
Again, I suspect an approach that expanded the
use of URIs in a compatible fashion would have
more merit than the above, albeit an approach
that is more structured.

@_date: 2003-07-16 12:56:43
@_author: Ian Grigg 
@_subject: Announcing httpsy://, a YURL scheme 
Well, I would have said it suggests a different
Instead of regimented, hierarchical and fixed
key management - an idea of poor track record -
the key management issue here is pushed back to
the user & client.  It relies on browser assistance
in caching, and correlation between many introducers.
In comparison to the CA regime, there is additional
trust from the people who send out URLs with this
information embedded.  If you trust the introducers
then you can trust their information.  That doesn't
mean that it is perfect, it just adds more security
than an absence of information.
I grant you that an either/or approach is a tough
sell.  The YURL would be improved if it explicitly
included a method to reference the hash of a CA-cert,
and to incorporate that additional information into
its trust display to the user, IMHO.  More information
is generally better.
(In comparison to the HTTP regime, as opposed to the
HTTPS regime, this is much better method than having
no trust at all.)
I don't understand this.  If Alice sends Bob a URL with
a hash in it, then Bob can apply the same trust that
he would apply to anything that Alice sent.
The same metric applies to an attachment.  If Alice sends
some thing.exe around, Bob will have the same procedure
in deciding whether to trust it as not-a-virus, or in
how to increase your level of trust.
Alternatively, if the URL with hash arrives from some
unknown source, yes, this is untrusted!  But, it is
no more untrusted than the previous scenario sans hash.
How can that be a problem?  We've gone from untrusted
to untrusted in one seamless step.  I wouldn't pay
for that as a feature, but I wouldn't grumble about
it either.
Right.  I don't think the YURL really meant for
people to read the things.  It could be better
explained, the browser has to record and correlate
the hashes.
Which is, really, how it is now for most use cases,
as we see long complex URLs all the time (click on
amazon for example) and without the browser to
interpret these things, we in userland are lost
I think it is a mistake to compare the URL+hash
idea to some existing security model such as is
purported by, for example, https.  It really sits
closer to raw HTTP - which is where most of the
live usage is, and where all of the problems lie.
relative senses.

@_date: 2003-07-16 18:02:17
@_author: Ian Grigg 
@_subject: Announcing httpsy://, a YURL scheme 
We were indeed talking about different things.
I'll address that here then.
That's true as it is basically stated, but one
needs to consider that URLs without any crypto
suffer from exactly the same problem.  That is,
bookmarked or distributed or page-published
URLs all lose their relevance over time.
This is from different causes but the effect
is the same, people need to do a little bit of
maintenance to keep links fresh.
I grant you that a user could be stuck with a
URL that has had a key change underneath it,
but I can't see that this is too traumatic,
as the website can simply indicate that the
key is changed.  A bit like a 404 for keys,
except that the original page might still be
That is indeed a huge issue.  Recall that the
paper mentioned yesterday did in fact research
this they built a browser according to a properly
constructed security model, and they claimed good
X> > The question at hand is this:  if secure browsing
X> > is meant to be secure, but the security is so easy
X> > to bypass, why are we bothering to secure it?
X> >
X> > Or, if we should bother to secure it, shouldn't
X> > we mandate the security model as applying to the
X> > browser as well?
X> X> Exactly.
X> X> That was the whole point of our Usenix paper last year
X> X> E. Ye, S.W. Smith.
X> ``Trusted Paths for Browsers.''
X> 11th Usenix Security Symposium. August 2002
X> Right now, this is the only reference I know
of, and it does at least indicate that users
*can* participate in the security process with
some success.
It may be that these guys were lucky and had
some smarter users - they talk about that -
or it may be that any work done on the current
model of brower security was unlucky for some
But, whichever, user particiption in the security
model should not be ruled out if that paper is
anyything to go by.  Also, see below.
I would rephrase that as "I don't like bad system A,
and let's look at other systems, however bad, so that
we can think about how to come up with a good system."
Like you, I think the YURL system is so incomplete
that it won't move onto the tarmac, let alone fly.
But, that doesn't stop me picking over the bones
looking for some good ideas.
Well, only Tyler is 100% behind this idea, the rest
of us are either damning it or treating it with as
much hope as it can justify.
I'd definately we'd all like to hear more ideas.
(That's why I took the trouble to skim those links
of this morning and summarise by example each of
their methods.)
I still don't understand that - who said that I would
treat the second URL with as much trust as the first
one?  If that's what Tyler Close is proposing, then I
can't see it either.
But I certainly didn't pick up that impression (mind
you, we are back to the doco situation there).
OK.  That's correct.  If I send you a trusted link that
we both agree has been vetted and certfified and orange-
booked and all, and you then go to that place, you have
a good trust path.
If you then cd/click/refer around a bit, well, the trust
is watered down.  I can't help you with that, all I did
was send you the first link.
Ahhh...  We are talking about user choice to
participate in the security process here.
SSH improves life because in the 99% of cases
where the key hasn't changed, it goes through
easily.  So we have the benefit of a widely
distributed tool that is available to all.
In the 1% of cases where the key changes, there
is a horrible warning displayed [1]
Then, the user has a choice:  ignore it, and face
the consequences.  Or find out what the hell
(When it happens to me, I generally yell at a
sysadm and he yells back that such and such a
server was reinstalled.  Good enough for me.
Translate this to the web.  This happened recently
in the gold community.  Messages went out on the
mailgroups asking what had happened to a particular
group off SSL keys that were mucking up.  The sysadm
came back within the hour and said they'd changed.
Good enough.)
Many users will ignore it perhaps because they
aren't so lucky to be able to yell at the sysadm.
So we are left with an economic decision:  those
that can't easily figure it out have a choice of
ignoring it or not carrying on.  They need to
balance the value of their work against the risk
of an attack.
To cut a long story short, the risk of an attack
is infinitesimal.  Not zero, as emails after that
rant of mine pointed out [2].  But still infinitesimal.
So, most users choose to ignore it.  And they
thus benefit.  Because they get their work done.
Choice is a wonderful thing.  I wouldn't want to
take choice away from users of the Internet, as
that would be to poison the very lifeblood of
the Internet.
[1] I don't know whether it is 1%, that's just
    a number that feels ok.
[2]

@_date: 2003-07-23 20:23:03
@_author: Ian Grigg 
@_subject: [Fwd: [fc-announce] FC '04: Call for Papers] 
Call for Papers and Presentations Financial Cryptography '04 9-12 February 2004 Key West, Florida, USA Conference Web site:  Important dates: Conference: 9-12 February 2004
Submission deadline 1 September 2003 23h59 GMT Author notification 15 November 2003 Pre-proceedings version due 15 December 2003 Original papers and presentations on all aspects of financial-data security
and secure digital commerce are solicited for submission to the Eight Annual
Conference on Financial Cryptography (FC '04). FC '04 will bring together
researchers and practitioners in the financial, legal, cryptologic, and
data-security fields to foster cooperation and exchange of ideas. In
addition to novel scientific research as in previous years, the program for
FC '04 will include sessions on digital finance and economics and on secure
financial systems and digital-cash architectures. For the systems and
finance sessions, submissions must have a visible bearing on
financial-security issues, but need not be exclusively concerned with
cryptography or security. Possible topics for submission to the various
sessions include:
Research Sessions: Submissions accepted to the research portion of the
conference will be published in full in the conference proceedings (up to 15
pages in total). Systems and Finance Sessions: For the systems and finance portions of the
conference, the primary emphasis is on presentation. For accepted
submissions in these sessions, a one-page abstract will be published in the
conference proceedings. Submissions to the systems portion of the conference may include
architectural descriptions and/or accounts of industry or technical
experience with implementations of secure digital commerce systems.
Presentations may concern commercial systems, academic prototypes, or
open-source projects for any of the topics listed above. Where appropriate,
software or hardware demonstrations are encouraged as part of the
presentations in these sessions. Contributions to the systems and the finance sessions of the conference need
not necessarily include novel contributions in the realm of scientific
research, nor must they concern financial cryptography or security
exclusively. They must, however, reflect careful thought and effort and
provide valuable, up-to-date experience that is relevant to practitioners in
the fields of financial cryptography and security. Submissions to these
sessions may consist of a short summary of work of one to six (1-6) pages in
length. Instructions for Authors: Complete papers (or complete extended abstracts)
must be received by 23h59 GMT on 1 September 2003. All papers must be
submitted electronically. (In exceptional circumstances, paper submissions
can be accepted, but special arrangements must be made with the program
chair prior to 1 August 2003.) Papers must be formatted in standard
PostScript, PDF format, or MS Word, and should be submitted electronically
according to the instructions at  prior to the
deadline. Submissions in other formats will be rejected. Papers should be
submitted electronically according to the instructions at
. Papers may be submitted through the submission form available at
. Submissions to the research portion of the conference may include at most
fifteen (15) single-spaced standard pages in length. Submissions to the
systems and finance portions of the conference must be short summaries of
work consisting of at most six (6) single-spaced standard pages in length.
(As indicated above, for accepted submissions in these latter sessions, a
corresponding one-page abstract will be published in the conference
proceedings.) Author names and affiliations on submissions must be explicit.
In other words, submitted papers should not be anonymized. Submissions must
include on the first page the title of the paper, the names and affiliations
of all author, a brief abstract, a list of topical keywords, and a
conference-session category (research, finance, or systems). Papers must
describe original work. For the research portion of the conference,
submission of previously published material and simultaneous submission of
papers to other conferences or workshops with proceedings is not permitted.
Authors of research papers found to be doubly submitted risk having all
their submissions withdrawn from consideration as well as other appropriate
sanctions. The conference proceedings containing all accepted submissions will be
published in the Springer-Verlag Lecture Notes in Computer Science (LNCS)
series after the conference. A pre-proceedings containing preliminary
versions of the papers will be distributed at the conference. For accepted
submissions, at least one author must attend the conference and present. In
addition, authors of accepted submissions must prepare the pre-proceedings
and final proceedings version - a full paper or one-page abstract, as
appropriate -- and sign an IFCA copyright form
. Questions about paper or
panel submissions should be directed to the program chair
(ajuels at rsasecurity.com).
fc-announce mailing list
fc-announce at ifca.ai

@_date: 2003-07-17 16:27:52
@_author: Ian Grigg 
@_subject: invoicing with PKI 
============================== START ==============================
Does anyone know any instances of invoicing and
contracting systems that use PKI and digital orders?
That is, purchasing departments and selling departments
communicating with digitally signed contracts, purchase
orders, delivery confirmations and so forth.
And, the normal skeptical followup question, do they
work, in the sense of delivering ROI, or are they just
hopeful trials?
(I don't mean vendors of the stuff, I mean users who
are have got in and seen some benefit.  I've suddenly
had two queries for help on this, and I'd like to
check whether the field has matured past the trial
phase without me noticing.)

@_date: 2003-06-01 11:18:06
@_author: Ian Grigg 
@_subject: "PGP Encryption Proves Powerful" 
Certainly, if all the cell members had a PDA,
with IR, then that would allow a much more
robust multi-factor system.  But...
This sounds workable in theory, but in practice,
one has to work with the skills base of the users
and the stress of the work.
Terrorists are generally not adept at technical
work.  They are not really chosen for their
skills;  more their loyalty, their anger, and
often their simplistic belief in "some other
bad guy" stories.  Terrorists are like soldiers,
mostly drawn from the lower echelons of society,
with a small smattering of bright sparks who
rise to the top (if they survive at all).  If
they could master technically challenging tools
like crypto then they'd not be terrorists, they'd
be out there making a living.
Giving them a complex technical tool means an
awful lot of training.  Which means:  they may
be able to master this, as they are not totally
dumb, but, this means they are not training in
some other thing.
There is a reason that the AK47 is the weapon of
choice:  it is an extraordinarily simple weapon.
Training is probably about half the requirements
of say the M16.  That makes a difference, much
more so than, say, the increased accuracy of the
There is a huge premium in a simple tool.  In
practice, I'd suspect that a single factor crypto
system would win out in the end, as anything more
complex would bog down under fire.  (In fact, I
am surprised they are using crypto *at* *all*,
I'd be very nervous about the amount of data that
could end up being compromised by a lost PDA and
a tortured terrorist!)
There is this pervasive image that terrorists are
technologically adept.  I don't think I've ever
seen much real evidence of that.  I think there
are two factors in this unrealistic belief.
1. The media love to portray terrorists as a wiley
enemy.  I can only put that down to a need to
explain how they managed to do this terrible
thing to us:  mentally, we feel better if the
enemy is really smart, a challenge to us, as it's
ok for him to win once or twice.  (As long as we
are smarter, and can rise and win in the end...)
Recall, we all love and admire the Germans because
they were a smart adept enemy in the first half
of the 20th century.  We have almost as much
admiration for the Japanese, but pretty much no
admiration for the Chinese and the Koreans, who
resort too quickly to human wave tactics.
(The Vietnamese, and Russians, we feel quixotic
Phsycologically, it makes us unhappy to realise
that the 911 attackers were actually quite simple,
so we don't.  We build up Osama bin Laden to be
a mastermind, a sort of James Bond-qualified evil
guy who constructs plans of insidious cunning.
2. Also, the counter-terrorist forces have a
vested interest in presenting the terrorists as
more capable than they really are (hence, that
article, as many have observed).  This is a simple
and pervasive technique to get more support for
their activities.  For example, it's now pretty
much clear that a lot of the threat assessments
of the Soviet Union were routinely exaggerated
dramatically by money-seeking companies and generals.
Also, you can't really be "wrong" and embarressed
if you over-exaggerate the threat.
All this is a long winded way of saying your
average terrorist is much more like your grandma
when it comes to tech.  Highly competant in the
kitchen, but can't send an email to save herself.

@_date: 2003-06-02 10:09:06
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
A lot of the tools and blocks are too hard to
understand.  "Inaccessible" might be the proper
term.  This might apply to, for example, SSL,
and more so to IPSec.  These have a lower survival
rate, simply because as developers look at them,
their eyes glaze over and they move on.  I heard
one guy say that "you can read SSH in an hour
and understand what's going on, but not SSL."
(This was the point raised by the chap who
recently wanted to role his own from a pouch
of fine cut RSA.)
Also, a lot of cryptosystems are put together
by committees.  SSH was originally put together
by one guy.  He did the lot.  Allegedly, a fairly
grotty protocol with a number of weakneses, but
it was there and up and running.  And SSH-2 is
apparantly nice, elegant and easy to understand,
now that it has been fixed up.
(SSH is the only really successful net crypto
system, IMHO, in that it actually went into its
market and made a mark.  It's the only cryptosystem
that is as easy to use as its non-crypto competitor,
telnet.  It's the only one where people switch and
never return.)
PGP was also mildly successful, and was done by
one guy, PRZ.  The vision was very clear.  All others
had to do was to fix the bugs...  Sadly, free versions
never quite made the jump into GUI mail clients, so
widespread success was denied to it.
I'd say that conditions for Internet crypto system
success would include:
  1.  One guy, or one very small, very close team.
  2.  The whole application is rolled out, ready to use.
  3.  Crypto is own-rolled, tuned to the application.
  4.  Concentrate on the application, not the crypto.
  5.  The application meets a ready need, and
  6.  The app is easy to use.
  7.  User doesn't need to ask anyone's permission.
These aren't very strong indicators of success, if
only because there have been so few fires, for so
much smoke.
Counterexamples are speakfreely, which was again
one lone hacker (John Walker?).  Maybe it stalled
on latter points.  (One doesn't hear much about
crypto phones these days.  Was this really a need?)
My own "interested" protocol (SOX, done by Gary H,
not me) trys to meet the above criterion and hasn't
succeeded, like all other money protocols.  I leave
speculation on why success is still just around the
corner to others :-)
So, I'm with Scott on that.  When it comes down
to it, there's an awful lot of smoke, and precious
little real life crypto success out there.  It's
no wonder that people roll their own.

@_date: 2003-06-02 14:48:51
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
That presupposes that one can do "better"
using SSL because SSL is "better".  It is
a challenge to translate SSL's strong peer
reviewed heritage into a secure crypto
In practice, if the tool is hard to use, an
implementation opens itself up for problems
in its usage of SSL.  There can be bugs in
the interface, bugs in the architecture
reflected by the complexity of the interface,
and there can be bugs in the underlying tools.
It may be that the SSL underlying code is
perfect.  But that the application is weak
because the implementor didn't understand
how to drive it;  in which case, if he can
roll his own, he may end up with a more
secure overall package.
Yep.  But the application got up and going,
he didn't wait for the protocol to be perfected,
which mean that the the application had a much
greater chance of ultimate success, and many
more scenarios were protected than otherwise
would have been.
Now it's a good protocol (Peter G reports that
it is highly analogous to SSL, but with its own
packet formats).  It's hole-filled first effort
doesn't seem to have done it so much harm.
[ Someone commented before that v1 was not deemed
serious (Marc A?) and v2 was the more acceptable
starting point (Weinsteins?). ]
Let me clarify.  SSL - the protocol - was not
designed by committee, but, the size of the teams
involved in the crypto systems was in excess of
the people who were intimately familiar with the
protocol.  For the most familiar example, browsing,
there were, it seems, many people involved in the
overall grafting of SSL into the original HTML/HTTP
system.  Hence, SSL as a protocol might be a fine
piece of work.  SSL as a browsing application is
flawed, and that's partly because too many different
people and agendas were involved.
(I think the design-by-committee criticism would
stick more strongly to IPSec.)
Sure.  If someone does roll their own, then they
should get it reviewed.
I know this is the mantra of the field.
Quesion is:  which PRIMITIVES?
1.  RSA?
2.  SSL, written from the RFC?
3.  OpenSSL, the toolkit?  EKR's fine effort?
4.  RSADSI security consultants, selling you
    theirs?
5.  ...
It's true that if there is a perfectly good
alternative available, it is probably more
expensive to roll your own than to use the
perfectly good alternative.
But, that assumes an awful lot.  For a start,
that it exists.  SSL is touted as the answer
to everything, but it seems to be a connection
oriented protocol, which would make it less
use for speech, media, mail, chat (?), by way
of example.
It's also very much oriented to x.509 and
similar certificate/PKI models, which means
it is difficult to use in web of trust (I
know this because we started on the path of
adding web of trust and text signing features
to x.509 before going back to OpenPGP),
financial and nymous applications whereby
trust is bootstrapped a different way.
Then there is understanding, both of the
protocol, and the project's needs.  I know
that when I'm in a big project and I come
across a complex new requirement, often, it
is an open question as to whether make or
buy is the appropriate choice.  I do know
that 'make' will always teach me about the
subject, and eventually, it will teach me
which one to buy, or it will give me a
system tuned to my needs.
In contrast, using a black box is always
a big risk.  Which black box?  There are
always 10 experts for every black box out
there, and they are all asking for lots of
bux to say they're right.  And, traditionally,
when you buy in a black box, chances are,
it's a grey box, or it's a black bucket, or...
still seems a very plausible decision to
roll ones own crypto.
Has anyone read Ferguson and Schneier's
_Practical Cryptography_ ?  Does it address
this issue of how an outsider decides how
to "make or buy"?  I just read the reviews
on Amazon, they are ... entertaining!

@_date: 2003-06-03 08:40:05
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
True, although, that begs the question as
to how they learn.  Only by doing, I'd say.
I think one learns a lot more from making
mistakes and building ones own attempt than
following the words of wise.
OK.  Then I am confused about the post that
came out recently.  It would be very interesting
to hear the story, written up.
Ah, good point:  There should be some
point on that list about building ones
cryptosystem outside the domain of an
institution, which tends to have too
many conflicting requirements, and
cannot limit itself to a simple system.
(And, yes, some protocols don't get
peer reviewed.  I wasn't debating that.)
I knew I was in trouble on chat, that's
why I stuck the interrogation mark in
there :-)  We recently added an email-like
capability to our (homegrown) crypto
system, and intend to expand that to
chat.  But, in order to do that, we
have to expand the crypto subsystem
(SOX) to include connection-oriented
[ Hence, an open question floating
around here is "why don't we use SSL"
which hasn't been definitively answered
as yet. ]
I think it would be very interesting to
do a study of all the cryptosystems out
there and measure what succeeds, what
doesn't, what's secure, and what's not.
What cost too much money and what saved
One of the issues that we see is that
too many security people assume that
"insecure" is "bad".  What they fail
to perceive is that an insecure system
is often sufficient for the times and
WEP for example is perfectly fine, unless
you are attacked by a guy with a WEP
cracking kit!  Then it's a perfectly
lousy cryptosubsystem.
It's like the GSM story, whereby 8 years
down the track, Lucky Green cracked the
crypto by probing the SIMs to extract
the secret algorithm over a period of
many months (which algorithm then fell to
Ian Goldberg and Dave Wagner in a few hours).
In that case, some GSM guy said that, it
was good because it worked for 8 years,
that shows the design was good, doesn't
And Lucky said, now you've got to replace
hundreds of millions of SIMs, that's got
to be a bad design, no?
(Lucky might be able to confirm the real
story there.)
Different ways of looking at the same
thing.  They are both valid points of
view.  To work out the difference, we
need to go to costs and benefits.  Who
won and who lost?  I never heard how
it panned out.

@_date: 2003-06-03 08:45:32
@_author: Ian Grigg 
@_subject: Ntru suffers 'chosen ciphertext attack' 
Ntru gets into trouble when their proprietary
crypto hits a security bug...
    "The technology was perceived to be better, but it's not good enough to
    overcome the objection that no one gets fired for buying RSA [Security
    Inc.products]," said one person close to Ntru.
:-)  Apropos to the roll-your-own debate.
-------- Original Message --------
June 2, 2003 Crypto Maker Changes Course
By Dennis Fisher
New leadership at security developer Ntru CryptoSystems Inc. is hoping a new services and consulting strategy will help mitigate the damage caused by problems with the company's core encryption algorithm. Once one of the premier cryptography companies in the United States, Ntru in the past six months has undergone a nearly complete face lift, replacing its CEO, moving away from its main business of licensing its cryptographic algorithms, slashing its staff by a third and placing many of the remaining employees on part-time status.
The changes at Ntru stem from issues surrounding the company's main intellectual property, the NtruEncrypt algorithm. The algorithm is the heart of the company's Neo security tool kit line and is the basis for the Ntru public-
key cryptosystem. Last fall, the company discovered there were problems with the parameters it had been recommending to customers to improve bandwidth when using the algorithm. Specifically, the problems caused random messages to fail to decrypt.
As a result, someone could mount what's known as a chosen ciphertext attack, which gleans small amounts of information from each failed decryption. Over time, the attacker would be able to amass enough data to decrypt an entire message, which would call into question the security of every other message encrypted using that key.
Although Ntru discovered the problem with the algorithm on its own, several groups of security researchers found the same weakness at roughly the same time and notified the company.
The problem was an obscure one-affecting just one in 1 trillion messages-but it was serious enough to compel Ntru to disclose it to all its customers and partners while the company's engineers began working on a new tool kit. Ntru executives maintain the problems didn't cost them any customers, and several customers contacted by eWEEK refused to comment on the issue. But, unfortunately for Ntru, the security community tends to have a long memory when it comes to such issues.
Ntru's Shrinking Partnerships
Advanced semiconductor designer
Secure Internet media delivery
Contactless security technology maker
Internet security products vendor
Maker of smart cards, smart-card operating systems
Smart-card industry group
Worldwide electronics manufacturer
Signal processing technology vendor
"The technology was perceived to be better, but it's not good enough to overcome the objection that no one gets fired for buying RSA [Security Inc.products]," said one person close to Ntru.
"We got a new tool kit out, and we've written some papers on this problem," said William Whyte, director of cryptographic research and development at Ntru, based in Burlington, Mass. "I think everyone understands that this is how things go. We're working on new parameters, and now we have provable security."
But, as the furor surrounding the algorithm problems began to subside this spring, Ntru executives decided to refocus the company's efforts on its nascent consulting business. That decision led to a round of layoffs in February that slashed the company's payroll to 20-and left many of the remaining employees as One high-level casualty of the reorganization at the same time was Scott Crenshaw, the former CEO who had been asked to take a diminished role earlier in the year. Crenshaw left Ntru and is now attending graduate school at the Massachusetts Institute of Technology.
"We looked at the financial picture and found that the skill set we had was geared toward getting our [intellectual property] licensed," said Ed King, Ntru's former vice president of sales, who is now the company's general manager. "But we needed to get more consulting. The layoffs were a one-time deal in my mind. By no means are we de-emphasizing the Ntru [intellectual Much of the consulting work at this point is in the form of custom cryptographic algorithm development and security audits. But Ntru is also doing work with Microsoft Corp., sources said, which could turn into a larger project.

@_date: 2003-06-03 11:38:50
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
OK.  Learning more every day :-)
Indeed!  Although I trust that we can also look at many
different ways of measuring success.
In order to *compare* success, like for like, we have
to start with an understanding of the marketplace for
each system, and assume that the marketplace for each
application is its universe.
I (arbitratrily) define the marketplace for SSL as
browsing.  (I.e., HTTP, as used between a browser
and a webserver.  The SSL protected part might be
referred to as HTTPS.  This of course ignores all
the other users of the protocol.)
There, we can show statistics that indicate that SSL
has penetrated to something slightly less than 1% of
servers.  It would of course be interesting to see
what the bandwidth figures are like, for example,
but I wouldn't be surprised if they are also less
than 1% (think about all those yahoo monsters that
overflow your POTS).
The fact that a user of SSL is neither aware nor
capable of being protected by SSH is irrelevant,
neither is a sysadmin concerned in his job with
protecting his work with SSL.
(Actually that's not true;  there was an SSL terminal
system for a while, as an adjunct to SSLeay, but
that is a dead or dying protocol, rapidly replaced
by SSH whenever the two entered competition.  Which
is a good thing, the SSL terminal was a nightmare
to get going, due to its insistance on hand crafting
SSL's 1% penetration into the browsing market doesn't
strike me as successful.
If I was "selling SSL" as a business, I'd be looking
at the other 99% and wondering why it's just sitting
there, not being sold.  As there are big expensive
companies doing just that;  then I guess they have
Have a look at the penetration reports on
On the other hand, SSH, as a cryptosystem, as an
application (think: replacement for telnet, not as
competitor to the SSL protocol) penetrates its market
very well.  I have no more than anecdotal evidence
for that, but any sysadmin knows that once they
started using SSH, they would never go back to the
alternate unless forced, kicking and screaming.
It would be very interesting to find out what SSH
v. telnet traffic looks like.
That's what I mean by success.  Within its market
place, SSH rules.

@_date: 2003-06-03 18:42:59
@_author: Ian Grigg 
@_subject: CDR: Re: Maybe It's Snake Oil All the Way Down 
Getting back to the world of users, there is a
threat out there:  idle listeners.  For the
famous and the vulnerable, there have been countless
scandals whereby private conversations have been
recorded and dumped on a shocked and titillated
GSM stopped that one cold.  It wasn't ever meant
to be encrypted to stop LEOs listening in, and
that never would have been an issue anyway, as
taps are more conveniently put at the base station
(assuming legal behaviour by LEOs).  (And, we can
pretty much assume that the encryption wouldn't be
allowed any further than the basestation ... in
fact, I'm given to understand that there is a
reason that the microwave links were never
encrypted ;-)
What was a real issue was that people who had
something to hide wouldn't use the phone.  And,
those people with something to hide, *wanted* to use
the phone.  It was actually economically sensible
to give all those scandalising lovers secure phones
so they could romance away the hours safely, because
the charging was per-minute.
The other issue was phone spoofing, which was a
massive industry in Europe with the older analog
devices.  Again, the crypto in GSM phones killed
that little loss leader.
I'm guessing here that neither civil litigation
nor Murdoch papers are much seen in Finland :-)

@_date: 2003-06-03 23:43:42
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
I've seen it a lot.  Not that I pay much
attention, but I'd suspect it is less
than 10%, but much more than 1%.  Also, a lot
of credit card numbers get delivered by email.
These are all to small time merchants who have
MOTO agreements without the net part, but take
the CCs anyway.  After all, a sale is a sale,
and nobody ever heard of a credit card number
being lost over the net...
OK, I'm teased by this:  how many sites use
open unencrypted CC delivery?  I went to google
and searched on:  " Virtually all deployed browsers support SSL, except a few
Total SSL servers 131,566.  Now go to here:
Total webservers 10,432,910 (derived by 5280096 / 0.5061).
That gives SSL penetration as 10,432,910 / 131,566 == 1.26%
(Darn!  I was wrong, it's slightly more than 1%, not less.
I should be stoned and cursed!)
Hmmm...  You might say that, but I would have said it
was the other way around!  There is - surprisingly -
not much of a threat model for eavesdropping of credit
cards (and - shockingly - even less of an MITM threat
It's easier for a crook to break in and hack the DB, and
pick up tens of thousands than to haunt the net looking
for an elusive 16 digit number out of a browser page.
But, there is a big personal cost with reputational
information.  Few people would want to see my credit
card info, but I can think of lots that would be keen
on seeing my adult browsing, my gaming addition, or
my participation in my kleptomaniacal therapy group,
not to mention anything embarrassing I might get up
What I find curious is why all those open source people
worked so hard to build in the crypto to protect credit
cards, but didn't want to protect anything else.  I can
understand Netscape programmers - they wanted to sell
secure servers for cash.
But I don't understand why Apache and KDE and Mozilla
deliver software tuned to protect credit cards.  It
would make sense if they were all paid to do this by
the credit card companies ... but they aren't, are
they?  What's their incentive?
We run a dozen or more web servers here, and
I can never tell the difference between the
unprotected ones and the protected ones, so
I'm not sure what to make of the argument
that SSL should be reserved for "important
credit card numbers".
I think CPU has gotted so cheap that running
out of CPU is a great sign of a successful
business, no more.  The last time I made a
serious business decision based on CPU
horsepower was back in 1989.  We are almost
at the point where raw PCs can do 1000 RSAs
per second.  Companies like Visa and Mastercard
process in the order of 1000 - 10,000 transactions
per second.  Which means if they were using an
efficient payment system - one or 2 RSAs per
transaction - they could be now thinking about
putting their entire crypto processing on one PC.
Maybe it's only an issue if one is serving
continuously... in which case, maybe one could
either "use less crypto" like switch back to
smaller keys - way more secure than no keys -
or buy a faster box?

@_date: 2003-06-03 23:48:12
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
Certainly, in measureable terms, Tim's description
is spot on.  I agree with Peter's comments, but
that's another issue indeed.
Design wins!  Yes, indeed, another way of measuring
the success is to measure the design wins.  Using
this measure, SSL is indeed ahead.  This probably
also correlates with the wider support that SSL
garners in the cryptography field.
That would depend an awful lot on what was meant
by "dollars-secured" and "data-secured" ?  Sysadmins
move some pretty hefty backups by SSH on a routine

@_date: 2003-06-06 14:30:04
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
Nope.  Cellphone companies are big slow moving
targets.  They get their franchise from the
government.  If the NSA wants weak crypto, they
do weak crypto.
There is literally no point in hoping the cell
phone company - or any large franchise holder -
will help you in your fight against big brother.
OTOH, what you can do is argue for reasonable
(Similar to GSM's.  That is hard to attack,
there is AFAIR no 'trival' attack, you have to
get access to the SIM or you have to probe the
phone with another phone over a period of hours.
I.e., the attacker leaves tracks, and he does so
in a way that will move him on to another mode
of tapping, such as purchasing a straight listening
Now, it seems that the US standards didn't get
even that.  There's definately a case for arguing
for better crypto in the US.  And, market forces
and all that, one would think that this would
happen in due course.
But arguing for strong crypto end-to-end - save
your breath.
John Kelsey (paraphrased):
(I edited the above to broaden the assert!)
Opportunistic crypto - that which uses the tools
immediately available and delivers crypto that
is the best available right now - is the only
crypto that will work for *you* the user in any
application.  Anything that defers security off
to some external party has a result of slowing
or killing the application, or delivering less
or no security than if you'd gone ahead in the
first place.
This isn't saying anything new.  It's the Internet,
after all.  On the Internet, one doesn't ask for
permission to participate.  That's no accident,
it's a core reason for its arisal.  Any protocol
that has a step of "now ask for permission" is,
IMHO, breaking one of the major principles of the
I guess there's no reason why you couldn't load
up speakfreely on a custom Unix box with a flashed
OS, put in the USB headset, and sell it as an end
to end encrypting phone.  The software's all free,
a cheap machine is $300 at Walmart, some enterprising
crypto guy could ship out a network appliance for
(Or, put it in a PDA that's got the right hooks?)
Half the price of your old Comsec, wasn't it selling
for $1000?

@_date: 2003-06-06 18:08:34
@_author: Ian Grigg 
@_subject: Maybe It's Snake Oil All the Way Down 
Derik asks the pertinant question:
My view, again, IMHO:  ignore Microsoft.  Concentrate
on the open source solutions:  KDE, Mozilla, Apache.
These groups will always lead in security, because
they are not twisted by institutional conflicts;
they can examine historical security model from the
point of view of interested professionals, rather
than commercial actors trying to preserve this or
that revenue stream.
The trick is to understand whether HTTPS as it
currently is can be improved.  If it can, then
those above guys can do it.
Once the improvements are shown to work, Microsoft
will follow along.  They are a follower company,
not an innovator, and they need to see it work in
practice before doing anything.  As Derik suggests,
the vast majority of users will have to wait.
Along those lines, there's one piece of excellent
That's fantastic!  I never knew that.  How does one
set that option on Mozilla?  (I'm using 5.0 / 1.3.1.)

@_date: 2003-06-17 18:19:39
@_author: Ian Grigg 
@_subject: Session Fixation Vulnerability in Web Based Apps 
Having read all these discussions and having looked
in my own PHP code and the PHP documentation, I have
to agree with James D.  This cleverness challenges!
I knew how to start and maintain a session, I think.
(That was no easy task.  The PHP documentation is
a mess, and over the last several versions different
ways started and stopped working...  I'm sure the
obvious answer is to use a better tool, but I'm a bit
stuck with a huge dose of reality at the moment, being
one of the million or so PHP developers, and can't junk
the man-years of habit just this month :-)
I just spent an hour or so skimming the doco for PHP,
and apparently, there is an ability to set another
session id with a call called session_id(), oddly
enough :-)
Which only leaves the problems of a) inventing a new
session id, b) rewriting the code so that it carefully
implements the unclever notion of setting this at the
new login, c) deleting this at logout, and finally d)
praying that this works as expected.
On the face of it, PHP doesn't appear to have much
support for this.  It will require each developer to
(re-)implement their own solution.  I'd love to be
wrong in this:  does anyone know how the easy way to
secure a PHP website against session_fixation?  Or is
it another case of "you gotta write it all yourself
Please.  How does one get access to that in PHP?  That
would be a wonderful answer to a) above.  Which would
only leave me with b) thru d)   :-(
PS:  Steve, thanks for the aviso!  Very interesting

@_date: 2003-06-18 20:15:34
@_author: Ian Grigg 
@_subject: An attack on paypal 
It's certainly enough - IMHO - to take the wind out
of the sails of the current rash of pirates.  If
the "placebo box" were to present the number of times
connected, and showed this in a graphical fashion,
I think it would be something ordinary users would
E.g.,  bright and bold and pulsing for 1st time,
with a big frowny face and the number 1.  Warm and
fuzzy for 10th time, with a big 10 and a smiley
face.  It might even put the fun back into browser
programming :-)  Certificate caching is a far more powerful idea
than, say, CA-signed certs.  If it were added
to browsers, and servers initialised with self-
signed certs, then the security of the net would
go up immensely.  Integrated with some of the
ideas that people have suggested concerning WoT,
publically distributed certs, and individualised
displays (amounting to local secrets keyed on the
cert), we could actually start to see people using
secured browsing when they wanted to rather than
when they were forced to.
It might even raise the stock of the profession
above the current "maybe it's all snake oil" rating
that some skeptics have applied :-)
(Oddly enough, the market for CA-signed certs
would also increase, and the factory signers
would make a killing, but that's a rant for
another day.)

@_date: 2003-06-24 19:16:29
@_author: Ian Grigg 
@_subject: Mozilla tool to self-verify HTTPS site 
Fantastic news:  coders are starting to work
on the failed security model of secure browsing
and improve it where it matters, in the browser.
This plugin for Mozilla shows the SSL certificate's
fingerprint on the web browser's toolbar.
It's a small step for the user, but a giant leap
for userland security.  It means that someone is
thinking about solving the hacks against secure
browsing.  Caching and distributing techniques
for certificates can't be that far off...

@_date: 2003-06-25 07:15:41
@_author: Ian Grigg 
@_subject: Mozilla tool to self-verify HTTPS site 
Apologies, last night's answer was too brief to
be useful!  Here's the more detailed and coffee
charged explanation:
Printing out a fingerprint allows a PGP-ite to
feel comfortable, but we all know there are
precious few of those on the planet.  So the
expected benefit to security is fairly low.
We don't get much bang for our buck here, so
we're agreed on that point.
In reality, the importance of the tool is that
it signifies - to me at least - that the browser
maufacturers (which I conveniently enlarge to
include plug-in makers :) are beginning to address
the security failures in secure browsing.  In
small steps, but they are now facing towards the
threat, at least.  Maybe.  I hope.
Also, SSLbar isolates and addresses what I percieve
to be a questionable design feature in SSL:  the
certificate and its delivery as an integral and
assumed part of SSL.
Here, this tool specifically challenges that
feature and allows for out-of-band checking of
the certificate.  It ignores or supplements the
debatable assumption that browsers make:  a
certificate is good if and only if it is signed
by a known CA.
That's a good thing, IMHO.  Tying the certificate
into the core crypto protocol seems to be a poor
design choice;  outsourcing any certification to
a higher layer seems to work much better out in
the field.
(E.g., PGP, SSH, SOX, Eric B's cryptophone.)

@_date: 2003-06-25 07:21:33
@_author: Ian Grigg 
@_subject: New toy: SSLbar 
None taken here, and I doubt that the author
of the tool (who has just joined this list
it seems) would take any!
It's an issue.  I think the answer requires the same
analysis as always:  someone would download this
plug-in if the result were likely more security in
the overall browsing experience.
So, the question then arises, could this plug-in
give more security than the exposure to an
untrustworthy party warrants?
On the one hand, the plug-in isn't likely to be
terribly effective, as is fairly obvious, as has
been pointed out.
OTOH, one might be downloading a trojan.  Well,
that's possible.  Is it likely?  I don't think
so, and here's why:
If this were an attack, it would be unlikely to
be effective.  There is a known site (albeit
with a masked identity) with a webpage, etc.
So there are tracks, and angry emails to the
owner of the site will incur a cost for the
Few people use keys, making this an obscure
approach.  I suppose if the target really *was*
keys, then the challenge would be to target
those key users ... against which, the users
of keys are likely to be more security conscious
than other victims.
If the person was indeed a crook, why would he
use open source?  And, even though Javascript
may have a poor security record, that's to do
with bugs in its model and code efforts and
potential security breachs, not with crooks
acutally inserting code to steal value.  I.e.,
theoretical breaches of security, not actual
breaches of security.
Also, to impune the plug-in arrangement is to
impune all plug-ins, and to impune the download
from an unknown is to impune all downloads from
unknowns.  What is the risk of downloads being
trojaned, and the risk of plug-ins being aggressive?
These are unknowable risks, a priori, so we
have to resort to statistics and cost-benefit
to work out the probability.  And here,
statistics is on our side.  In practice, an
attack is rarely initiated via a download,
or via a plug-in.
I.e., "download this fantastic tool" which
just so annoyingly includes a trojan from the
person who manages the site doesn't seem to
occur as a real attack with any frequency.
(Partly because it takes a long time to find
the right victim, and partly because it
leaves the attacker static and vulnerable,
I'm guessing.  In comparison, it seems that
attackers get much better results by using
targetted mass mailings tools to deliver
their EMD.)
So on balance, I won't download the tool,
because its effectiveness is low.  But so
is its risk.  Other people might come to
other conclusions, but I personally don't
buy the argument that just because I don't
know the site, it shouldn't be touched.
Life is full of risks.  Only by taking
risks do we understand what works and what
doesn't.  Real-life security is like that,
as in practice, we know that not all can
be covered in security, as it is simply
too expensive to be 100% safe.  So we have
to take some risks in some areas.
EMD - emails of mass destruction?

@_date: 2003-03-15 11:05:14
@_author: Ian Grigg 
@_subject: How effective is open source crypto? 
How effective is open source crypto?
One measure is to look at how effective the
open source crypto regime is in getting
product out there.  From the above, it is
fairly easy to suggest that strong crypto is
totally available to all, probably thanks to
the efforts of open source crypto providers.
How effective is the SSL cert regime?
Last page showed 9,032,963 servers.  This
page shows 112,153 servers using certs.
That's right, folks.  In the particular
case of web browsing, the USAGE of crypto
has been relegated to 1% of potential
(Pprobably much less than that due to other
factors, but 1% makes for a nice soundbite.)
Why?  Because a) it is relatively hard to get
a server configured with a cert, and b) the
browsers discriminate against self-signed
certs, forcing administrators to go the more
troublesome, costly and frustrating way of
requiring purchased and "approved" certs.
(For no measurable added value to the security.)
(So they don't.)
I suggest that open source crypto has won
the crypto wars, and the implementations
of SSL have bungled the peace for us.
It is ludicrously easy to encourage more
use of crypto, by repairing the browsers
and servers in these two ways:
Fix 1. browsers should not negatively
  discriminate between self-signed,
  CA-signed and unprotected HTTP.
  (For example, browsers might show one
  icon for the self-signed and another
  icon for the CA-signed - maybe a
  branded icon from the CA.  There
  should be no FUD warnings when going
  from totally unprotected HTTP to
  connections secured by self-signed
  certs.)
Fix 2. Apache and other servers
  should be configured out of the
  box automatically with SSL enabled
  over the default site.
  (Which means, a self-signed cert
  [unencrypted on disk] and the server
  listening on its port.)
(There are plenty of minor fixes as well,
such as renaming the self-signed certs
to be self-signed.  At the moment, they
are sometimes incorrectly labelled as
"snake oil", thus confusing the users by
implying that that are not definitively
better than unprotected HTTP.)
To conclude, open source crypto has not
shown itself to be effective, at least
within the one protocol examined above,
but could easily be so with some changes
to the implementations.

@_date: 2003-03-23 23:10:22
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
Who's afraid of Mallory Wolf?
By common wisdom, SSL is designed to defeat
the so-called "Man in the Middle" attack, or
MITM for short.
Also known as Mallory, in crypto circles.
The question arises, why?  For what reason is
the MITM a core part of the SSL threat model?
And, why do all the implementations assume this?
(It is, in fact, possible to use SSL, or TLS
as it is now known, without regard to the MITM
protection that is part of the model - certs -
but I ignore that here, as do implementations!)
One has to go back to the original invention
of SSL, back in 1994 or so:  the web was storming
the barricades as the 2nd great killer application
for the net (email was the 1st).  Companies were
dipping their toes into the endless possibilities
of commerce.
Netscape was evolving as the master of the new
net, the challenge to Microsoft, the owner of
all things it surveyed.
And, as with all dot-com crazies to follow, it
had nothing spectacular in the way of a business
model.  Selling a few secured servers, was all.
This whole commerce thing was, at that time, a
great wonder, because it involved earning money,
and money that was honestly earnt was a precious
short commodity at Netscape in those days.
To cut a long story off at the knees, Netscape
put together a variant of the HTTP protocol
layered over crypto.  This was sold in addition
to its servers as the way to secure credit card
payments over the net.
The analysis of the designers of SSL indicated
that the threat model included the MITM.
On what did they found this?  It's hard to pin
it down, and it may very well be, being blessed
with nearly a decade's more experience, that
the inclusion of the MITM in the threat model
is simply best viewed as a mistake.
Consider this simple fact:  There has been no
MITM attack, in the lifetime of the Internet,
that has recorded or documented the acquisition
and fraudulent use of a credit card (CC).
(Over any Internet medium.)
Even worse, there's not been any known MITM of
any aggresive form.  The only cases known are
a bunch of demos, under laboratory conditions.
They don't count, and MITM remains a theoretical
attack, more the subject of learnings and design
exercises than the domain of business or crypto
How hard is this fact?  A bit softish, actually,
but given the amount of traffic we have seen
in the last decade, one would think that MITMs
would have made their appearance in aggressive
attacks by now, perhaps by scanning emails,
perhaps by listening to unprotected HTTP.
(In fact, there are now fertile grounds for the
attack, with the advent of 802.11b.  There are
even kits available for it.)
But so far, no cases have been found.  (In
fact, there isn't too much evidence, beyond
the circumstantial bemoanings of those that
can't, to indicate that aggressors are even
passively listening, let alone trying more
sophisticated MITM attacks.)
Within the world of credit cards, the people
who work directly within the ecommerce industry
admit privately that this is true [1].  All lost
credit card events are based on other attacks.
Which leads one to wonder what the threat is?
And if there is a threat?  That is, should the
MITM be in the threat model for SSL, or should
it be excluded?
Internet cryptography gives us one answer:
    If it can be protected against, it should
    be, as to do otherwise results in a false
    sense of security.
This is what I call "100% cryptography" for want
of a better term.  It's a sort of journeyman
phase of crypto-plumbing, at that time when as
beginners, we read from the big read book.  We
imagined how to deal with many dark and scary
threats and we all agreed, no question, the goal
was to cover more of them than the next guy.
We would swap conspiracy theories well into the
night, all the while, bemoaning the lack of usage
of real cryptography, the poverty of our opponent's
wit, and the fruitiness of our cheap red wine.
I miss those days, if not the product of those
mad times.  It was also a time where we rarely
saw the real life implications of our code,
deployed in a threatening environment.  In
short, we 100%-ers built systems based on
expectations, but we did not close the feedback
loop to push the real life results back into
the deployed systems.
Economics gives us another answer:  a standard
approach to deciding how to spend money.
  1.  estimate the average cost of each attack.
  2.  estimate the number of attacks
  3.  multiply the above two to get a total
      cost.
  4.  likewise, estimate the total cost of
      avoiding the attacks.
  5.a if you can avoid these attacks by
      spending less money, you profit.
  5.b if you spend more than you save, you lose.
It's just economics, and statistics, and the
validity here is simply that credit cards are
nothing if they are not economically- and
statistically-based models of commerce and
So, let's guess the cost of each CC lost to our
MITM as $1000.  (Pick your own number if you
don't like that one.)
Then, how many attacks?  None, from the above.
Multiplied together, and you get ... nothing.
What does that mean?  This theory predicts
that if you spend one cent protecting against
the MITM, you lose.  Because according to an
economics and statistics analysis, based on
there being no measurable risk to you of MITM,
there is no reason to spend any money to avoid
If one believes in economics - or, at least,
the above risks and costs model, then it
becomes very important indeed to quantify the
threat.  If there is no experience of MITMs
and there are no consequent costs, then the
model suggests no threat.
Is there a compromise?  Well, there is the other
side of the equation.  Let's look at that:  the
inclusion of MITM protection comes at a cost.
That cost should be estimated and compared to
the losses from any MITMs.
(If there were any.  Regardless, we can be more
ready for that day;  I feel in my bones it is
going to happen one day.  I mean, 10 million
unprotected sites, there's gotta be a time
coming soon!)
A "good" server cert costs about $700.  The
average cost of installing it - from start to
finish - at an average company seems to run to
many days elapsed, but let's estimate it at 6
hours time.
Why so long?  Because it is infrequent and
unautomated.  There are dozens of single
steps to go through.  Due diligence, documents,
and the like all of which befuddle the techie,
challenge the manager, and daunt the quality
Call the cost at your average western company
as $50 per hour, and we have an estimate of
$300 for time.  Forget other costs, but what
we can do is estimate the cost per certificate
as $1000 as a ballpark.
I think it is more, but call it $1000.  Multiply
by the number of  certificates in operation,
about 100,000, and we get a cost incurred of
$100 million to protect against the MITM.
Every year, or however often the certificates
expire, we, as a networked society, are spending
$100 million dollars to avert something that
doesn't exist.
Back to that compromise.
Imagine 10 MITMs successfully steal credit
cards and organise successful heists of $1000
each.  $10,000 of value is thus being protected.
Society - that's us, all of us - is losing big
time.  We would need to see 100,000 MITMs at
that size to justify the infrastructure.
Or, 100 sheiks with million dollar cards!
And that's just to break even.  We need more
to cut a profit.
This is totally ludicrous.  These numbers are
just unmalleable to achieving any sort of
aggregate benefit to society.
So, we can suggest that, not only is there no
measureable threat from the MITM (Mallory is
not to be found anywhere near any credit cards
known to the issuers of same) there is also
rampant waste going on in protecting servers
against some imagined bogey man with a silly
name like Mallory.
Now, for a particular server, any given server,
it *might* be prudent to protect against the MITM.
Maybe the server owner has a higher threat model
than the ordinary purveyor of goods.  Maybe the
users demand it.  Or maybe the owner has more
money and simply doesn't care about saving it.
All of which is well and good, *if* the owner
was solely capable of this decision.  That is,
spending own money.  But, in the web world of
today, the owner has no choice.  If encrypted
communications are required - useful to stop a
simple listening attack - then a certificate is
The software mandates it:  mostly the browsers,
but also the servers, are configured to kick up
a stink at the thought of talking to a site that
has no certificate.
As such, SSL, as implemented, shows itself to
include a gross failure of engineering.
The threat is not present, but the browsers
mandate the protection.  As they migrate from
unprotected to protected browsing, users are
coralled into certware, as if that made any
Clearly, the browsers should not discriminate
against cert-less browsing opportunities.
Indeed, I'd go further.  If a self-signed cert
was encountered, the browser should praise the
user's choice:
    Congratulations!
    You have selected a site that wisely protects
    our communications with a FREEDOM CERTIFICATE,
    designed to thwart scurillous and undesired
    spies and help win the war against the axis
    of evil listeners!
As the servers cannot communicate so readily
with the user, they would have to limit their
fight against waste and misallocated resources
by configuring with the best protection fastest
and up front.  Automatically generated self-
signed FREEDOM CERTIFICATES, as a convenient
temporary measure until widespread Anonymous-
Diffie-Hellman is deployed in the field, would
appear to strike the quickest and most cost-
effective blow for Browsing Liberty [2].
[1] See for example, Lynn Wheeler, 17th March,
    .... Now SSL protects credit card numbers
    while in flight. However, we never
    actually saw a reported exploit against
    credit card numbers in flight. All the
    reported instances of major credit card
    exploits have to do with harvesting
    of credit card merchant files ...  at
    rest at the merchant. So for the major
    exploit, SSL has no effect on.
    [2] AFAIR, Anonymous-Diffie-Hellman, or ADH, is
inside the SSL/TLS protocol, and would represent
a mighty fine encrypted browsing opportunity.
Write to your browser coder today and suggest
its immediate employment in the fight against
the terrorists with the flappy ears.

@_date: 2003-03-24 11:00:01
@_author: Ian Grigg 
@_subject: Keysigning @ CFP2003 
I must be out of touch - since when did
PGP key signing require a photo id?

@_date: 2003-03-24 15:26:59
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
Yes, that's definately an attack.  As
was pointed out, the use of the cert
seems to do two things:  stop the
MITM (via a secured key exchange so
the listener cannot see inside the
packets) and confirm the site as per
what is stated in the URL.
My post of last night addressed the
MITM only.  I completely ignored the
issue of spoofing, which would only
be possible if there is no complex
relationship between them - which is
a debateable point.
Does the cert stop spoofing?  That's
the question!  If it does, then there
might be value there.  In which case
we can measure it and construct a cost-
benefit analysis to decide whether to
protect against it.
No, there has been little evidence of MITMs
*outside* the system.  (I said none, Steve
Bellovin said some...)
The fact that there are none within the
system, yes, that would only show either
the attacks were defeated, or there weren't
going to be any, or that there are better
pickings elsewhere...  It doesn't allow
you to conclude anything about the need
for protection.
Check Lynn Wheeler's new post (thanks Lynn!)
which points to a lot of inside knowledge
about the absence of any aggressive MITM
activity inside the credit card world.
And, see Steve Bellovin's post for some
evidence of MITM outside the credit card
:-)  Terms are always debatable!  I'd say that
engineering *includes* the appropriateness
of the requirements.  Science does not.
Where I would agree:  the _protocol_ was engineered
very well to meet its requirements.  It's not a bad
protocol, by any logic.  However, no protocol
exists within a vacuum, this one exists within a
_system_ that is commonly also known as SSL.
(Therein lies a big problem here:  I know of no
separate term to distinguish SSL the protocol
from SSL, the secure browsing system that
you or I use to send our credit card numbers
No, but it would be an interesting exercise!
It's interesting that you say that ... why is it
then that people like Ben Laurie, Eric Young,
Eric Rescola and others spent years writing
and deploying software for free?  Why do the
people at Safari and Mozilla and Konqueror
also spend all that time getting SSL to work?
I don't claim to know the answer.  But, if
their answer is "to protect credit card numbers"
well, actually, I don't think so!
And that's the point of the rant:  to identify
some of these underlying assumptions like "SSL
protects your credit card numbers" and reveal
the truth or otherwise.
Hopefully, if we can strip out the myths,
we'll find the truth.
People don't think like us techies do.  They see
the messages, and they ask for explanations
from other people.  Who may or may not know
what it all means.  The end result is the lowest
common denominator - if there is a message,
then something is wrong.
And that's the point:  if there is nothing wrong,
the browser shouldn't say there is something
So, what is this "risk of MITM" that the browser
is protecting us against?

@_date: 2003-03-24 16:06:38
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
Thanks Steve, now we are getting closer.
802.11b is where I'd been expecting it to
happen, as the costs of the MITM come
right down there.
Would you characterise the attack as a
bunch of techies mucking around, or would
you characterise it as an aggressive attempt
to gain a commercial advantage?  I.e., did
the attackers steal anything?  Or did they
just annoy people by showing how cool
they were?
I would surmise that's a techie conference, and
is thus a demonstration, not a measurable
I'm can't see clearly whether this is
an MITM or a spoofing - did they stand
in the middle and listen and divert?
Or, did they just tell innocent servers
to start re-routing traffic?  It seems
like an announcement of routes, and
the listeners just believed...
(But, it is an aggressive attack, someone
tried to steal traffic for commercial gain.)
I think you may be right in that my
use of the term MITM is too broad.
The cert in SSL protects against a
cryptographic MITM in, for example,
an ADH session.
But, MITMs outside that are important
measurable risks so we can create our
threat model.  The fact that this attack
appears not to be analogous to the
SSL-style MITM may or may not be
He didn't do it to steal.  He did it to
highlight the business aspects.  Sadly
for him, he miscalculated (grossly, it
seems).  But, his case fits in the sense
of "not a criminal seeking to steal value,"
and therefore not a case of measurable
I certainly accept them as possible.
That's not disputed, and never has
been, as indeed, that was the whole
thrust of the discussion:  The SSL
designers put the protection in
because the threat was possible.
They quite rightly offered the choice
in the protocols.  Where I am concerned
is that they also wrongly forced the
certificate path on browsers and
servers.  To our detriment, and to
No, I'm afraid that does not hold.  The
reason we protect against attacks is
because when they happen, they incur
costs.  But, designing in protection also
incurs costs.  We must do a cost-benefit
analysis to decide if it is appropriate to
protect against it.
To say that attacks are "feasible" and
therefore must be defended against is
not how we work.  We can guaruntee
that you are immune to car accidents,
simply by asking you to stay at home.
You (probably) chose not to do so,
because you chose to enjoy the higher
benefit of travelling, as against the
smaller expected cost of a suffering
an accident.
Right.  But it's an empty argument if there
is no need.  We don't carry umbrellas when
the sun is shining, only when the sky is grey.
And, we don't build meteorite protection at
all, even though we could, and they happen!
We use information about real threats and
how they hurt us to decide whether to
worry about them.  And that's why the
question about MITMs is so key!
The question is, is there a need?  From
several economic points of view, the need
fails to show itself.  And, the cost is quite
high, both in cash, and lost security.
Taking your links above at face value, I'll
assume that the cost of stolen/hijacked IP
number there was about $10,000 in lost
business and customers being annoyed
at unexpected porn.
Say that happens once a metric month to
some random victim  ... or, $100,000 per
year.  That cost simply fails to justify any
level of signed-certificate infrastructure, so,
I'd conclude that the BGP protocol designers
have done exactly the right thing in not
deploying certs, and saved the users a bundle.
(And, Netscape has done exactly the wrong
thing by setting up CA-signed certs as
The fact that the MITM is possible, doesn't
make for a need.  Especially when we are
all paying O($100m) per year for that possibility.
Thanks for the MITM pointers.  I'm going
to have to look deeper into that BGP think
to see whether I'm wrong on the "none at all"
case.  MITMs are going to happen one day,
and then, we will be able to properly measure
the costs.  That's where we want to be!

@_date: 2003-03-24 19:03:50
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
That is indeed the question, sans personal
Actually, I think that if zero dollars had been
spent on MITM protection for SSL, then there
may well have been some MITM attacks.
That then would be a good position to be in,
because we could measure the costs of those
attacks, and decide from a monetary perspective
whether protection at the level of requiring
signed certificates is a good thing or just a
waste of money.
My own guess is that MITM activity is so low
across all domains of the net that we would
not be able to reliably measure it, and if we
could measure it, we'd find it not sufficient
to mandate certificates as is currently done.
Which - to repeat - is not to remove certs
from the servers or browser, but to change
the way in which we assume that "only
cert-protected browsing is good enough."
The certs are really good for high end sites
(because, economically, they return benefits
even if there was no MITM threat).
But why are they needed for smaller things?
Why do I need a certficate to run an SSL
server so that my family can share snapshots
for instance?  Just a hypothetical...
You provided your own answer :-)  You used
to get picked on, so you had a measure of
its cost.  You acted to defend against those
Did you ever get MITM'd?  Anywhere?  Any
time?  Anyone you know?

@_date: 2003-03-25 00:17:22
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
I love it!  Then, I'm wrong on that point, we
do in fact have some aggressive MITMs
occuring in some mediums over the net.
Steve Bellovin pointed one out, this is
Which gets us to the next stage of the
analysis (what did they cost!).
I'd say, SSL with the cert protection is the
strongest link in the chain.  In fact, it's
ludicrously strong.  It's like a Chubb vault
lock on a screen door.  If we were getting
physical here, the door wouldn't be strong
enough to hold up the lock.
So, cut to the chase:  if we "mandate" that
from now on, all commerce servers use
ADH, just hypothetically, for the sake of
argument, do you think that the connection
would then become anything other than the
strongest link in the chain?
(I think it would remain the strongest link,
by far.  In fact, even if it was unencrypted,
I think it would be one of the stronger links,
c.f., David Wagner's devilish advocacy.
But, nobody would suggest we throw away
the current cert infrastructure, just that we
back off a little and accept the intermediate
path of ADH / self-signed certs.)
Nobody's saying that we should.  I'm
saying that the server and browser
should offer the choice to deploy
and use more convenient levels of
security.  The message should
congratulate the user for moving up
to a more secure channel than HTTP,
not annoy them with imponderables
about how self-signed certs might be
insecure under a certain hard-to-measure
threat model... as is the case now.

@_date: 2003-03-25 00:36:20
@_author: Ian Grigg 
@_subject: Keysigning @ CFP2003 
Well, that's a surprise to me!  My understanding
of the PGPid  signature was that the semantics
were loose, deliberately undefined.  And, within
that limitation, it came down to "I met this guy,
he called himself Micky Mouse."
I've only been to one key signing event, and no
identity was flashed around that I recall.
So, do we have two completely disjoint communities
here?  One group that avoids "photo id" and another
that requires it?  Or is one group or the other so
small that nobody really noticed?
I'm curious, is all!
Um.  So, there are people out there that I am convinced
are who they say they are.  They happen to be nyms,
but I know that, and they are consistent nyms.  Can I
sign their key with the highest level?

@_date: 2003-03-25 12:42:38
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
I'm sorry to disagree, but I'm sticking to my
cost-benefit analysis:  monetary costs are totally
germane.  You see, we need some way in which
to measure the harm.  It's either subjective as
you describe above, which can't support an
infrastructure decision, or its objective, which
means, money.
But, luckily, there is a way to turn the above
subjective morass of harm into an objective
hard number:  civil suit.  Presumably, (you
mentioned America, right?) this injured party
filed a civil suit against the person and sought
Now, even if the case did not get filed, I imagine
that you would be able to find a few legal types
to provide an upper and lower bound on the sort
of damages that case would go for.
And there's your number!  From my ignorant
position, I'd scratch in a figure of about a
million dollars there, and wait for someone
to refine it.
I would agree.  Which is why we are having
this discussion - how can we get this poor
victim's traffic onto some form of crypto so
she doesn't get her life ripped apart by some
As far as SSL goes (switching from the
context of her mail to the system we are
discussing here), here's the answer:
    Make ADH / self-signed certs a respectable
    half-way house to CA-signed certs.
    Encourage all servers to accept them, by
    default.
    Encourage all browsers to switch up to
    ADH / self-signed secured traffic.  Don't
    discourage it, encourage it.
The problem is, it is just too darned hard &
expensive for sites to get into SSL.  That's
what we are looking at, here, lowering the
cost of entry into SSL.

@_date: 2003-03-25 16:15:22
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
I agree with that ... I was converting the
subjective harm into an objective cost.
I certainly wasn't intending to ignore it :-)
That being part and parcel of the problem.
It's a subjective harm, there is no solid way
to move subjective to objective, by definition.
We can only make estimates.
What is beneficial here is that - at least -
we have one way to do this.  And, it is a
way that has lots of disinterested observers,
lots of experience, and lots of interested
parties.  Much as I dislike courts, it is a
"fair and auditable" way of dollarising a
Bear says:
Nope.  But, all we want is an estimated
cost of the attack.  Ask some lawyers
for a quote.  Ignore the guy's family, we
are only after an estimate of the cost.
David says:
This of course is true especially with the
low level of MITM activity that we've found
to date.  If such a case were to happen
once a year, I'd not be really confident of the
accuracy of the numbers, especially if we
were estimating based on lawyer's opinions
rather than awarded damages.
(But that wouldn't so much matter if the
numbers came out as also too low to
consider, as I suspect they will.)
If however, we had such MITMs once per
month, then costs could be averaged over
the size of the activity.  Something like
  There are 500 million email users in the
  world today (guess!).  Cost of failures
  that could be rectified with proper crypto
  (amounts to 12 cases per year) is 12 million
  dollars.  Some judgements less than a
  million, some more.
  [ if you like, you could add in a fudge
  factor for unreported harms and other
  "judgement" calls. ]
  Now, the cost of prevention:  assume
  we pass a law to make every ISP sell
  every user a copy of OpenPGP to
  protect their privacy.  Bulk discount
  gives us $1 each copy, annually updated
  to cover for the inevitable new release.
  So, cost to protect:  500 million x $1.
  Saved costs in cases:  $12million.
That law won't get passed :-)

@_date: 2003-03-26 01:34:07
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
Yep.  I haven't counted them up yet, but
the full discussion includes at least 6
disparate threads.  The challenge is to
not arbitrarily switch from one thread to
another without losing the context of the
The way I got where (I think) I am is this:
  Fact:  The SSL cert that is required for
  the server is expensive.
  Question:  Why do we have to pay that
  expense, and what happens if we use a
  self-signed cert?
  Answer:  "the MITM!"   "Spoofing!"
OK, so now let's challenge the assumptions:
  Question: What is the MITM?
  And why should we care?
And, when we've answered that question,
let's plug that truth back into the 1st
question.  (And, the same for spoofing.)
Fantastic!  a 2 x 2:
              GOT                HTTP
              SSL+               ONLY
              cert
Want          Crypto        1
Want          (may have bugs)
Want          2                  3
Don't                            4
Totals:       1%                 99%
Hmm, it drew out as a 2 x 3 (only in fixed font).
So, I wonder what the totals on the right would
be?  How many people want crypto/MITM, how many
would be happy with crypto/no MITM protection,
and how many don't want any crypto?

@_date: 2003-03-26 12:50:28
@_author: Ian Grigg 
@_subject: Who's afraid of Mallory Wolf? 
I'm sorry, I won't be able to do more than
speculate on this, and I wasn't aware of
your legal background, so please take the
below as "not advice."  I.e., IANAL and
all that.
Courts are notoriously difficult to predict.
That's why they say "take legal advice" :-)
And, it may very well be that Netscape
took legal advice, and at that time, it did
seem that MITM protection at the level
of CA-certificates was a reasonable choice
(c.f., David Wagner's post) amongst other
reasonable choices, so I don't think there
is any doubt that what was done back in
'94 was reasonable in the circumstances.
But, on the face of it, you appear to be
saying that because the court saw warnings
then it ruled that the warnings were sufficient.
I don't read that at all.  I see that interpretatation
as a Chicken Little argument.  This opens the
way to Info-war style consultants saying that
because you were warned, you are liable.
That above snippet says "there are precautions
so imperative" which implies the court had already
reached its opinion on the merits of this protection,
which is precisely what this discussion has
aimed to address.  In fact, the court said very
clearly that it is the one to decide what the test
is - not the industry.
The court then went on to say that, as it found
the precautions imperitive, and as the industry
had warned, albeit contraversially, then, it
concluded, relying on the lack of industry custom
and agreement as a defence was insufficient.
So, with respect, I would say that the above
should be read as "do not rely on discordant
others, be they so-called experts or Chicken
Littles on either side, in applying your own
prudential measures," which is quite the
reverse of your reading.
Now, the above is speculation;  not having
the full ruling and the full training, one can't
do more.  But, to take mere warnings as
liabilities is to forgoe ones profession as an
engineer, and hand ones responsibilities
over on the one hand to the religious seers
of doom, and on the other, to the lawyers.
The ludicrousness of this approach is
perhaps more crystallised when we consider
that half of the world's web servers are
shipped for free (c.f Apache).  The crypto
components are still, AFAIK, dealt with
outside America for the most part.
And, a growing share of browsers are now
shipping for free or near-free.  We've seen
over the last year or so, Konqueror, Mozilla,
and Safari rise to take back the forgotten
gauntlet of "browser for the rest of us."
These are not sold products.  There are no
contracts that imply security.  The world
of open source is not necessarily going to
be treated in the courts the same as a
purchased product with implicit liabilities
of a consumer nature.
I grant that America may be moving towards
a world where Eric Y or Ben L will be norieged
and hailed before a california court in some
case for inadequate MITM protection, but,
I personally don't see that as a world that I
would accept on the face value of some
legal handwaving.
Is that really what we want for our Internet?

@_date: 2003-05-01 12:24:39
@_author: Ian Grigg 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
No, the point with print is that the cost of each
additional print is very high.  In fact, the cost
of each additional copied print may be higher than
the bribe ...
Whereas, once Kocher's high fixed cost first digital
stolen copy is made, subsequent clean digital copies
are (more or less) free (a number that is as low as
GB and bandwidth costs permit).
It seems that you need five players.  Then a whole
bunch of hacking or processing.  And that gets you
one movie.  Probably, the cost of the 5 players is
less than the cracking cost of the one movie, so
what we have achieved is a shift to per-movie fixed
costs, something that Hollywood did not have with
the DVD system.
(Digital copies still remain near-free ... for that
one movie.)
So, if there is a high cracking barrier to each movie,
one can guess that if any movies get cracked, it will
be the big budget ones;  the 10 or so 'big' movies of
the year.
As a postscript, it's almost certain that this level
of analysis has already been done.  There will be
some document sitting somewhere which states that
the fixed cost per movie cracked is X.  And Paul K
will have got the studios to sign off on that number.

@_date: 2003-05-02 09:46:26
@_author: Ian Grigg 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
Within the narrow domain of crypto, Boneh and
Shaw provide a framework for the fingerprinting
and collusion detection similar to that which
Paul Kocher claims.  (Whether he uses this method
was not clear to me.)
In practice, this paper raises - in my mind - more
questions than answers.  For example,
* the simplistic statement that an identified
  sharer is 'guilty' hides a wealth of detail.
* it seems way to complex to have any merit in
  court.  That is, I can't quite see how it would
  be possible to prove the results, given the
  math, to the satisfaction of a jury or judge.
  (E.g., Find an expert who will disagree, and
  battle it out.)
* it requires identity tracking.  Conceivably,
  that might work in a cooperative arrangement,
  such as commercial software, where companies
  hold still, but is unlikely to make much
  headway in retail movie sales.
* indeed, its complexity and its statistical
  approach raise weaknesses that could lead to
  ultimate downfall, by, for example, poisoning
  the market with false leads.
* Anonymous suggests (for his assumptions) that
  the system has practical limits in the order
  of 6 conspirers.  For a big budget movie, this
  won't be much of a barrier.  Indeed, one could
  imagine a future backup program (a la DeCSS)
  that would provide sharing facilities (a la
  Napster) that coordinates to improve the
  quality of the backup the more people join in
  the backup group.
I suppose coupled with DRM / Kocher's machine-
fingerprint, it could present a "plausible"
scenario for defendable sales of movies.  But,
I still can't see the final step assumed by
both these proposals as actually workable:
  identify the guilty machine and/or
  party, and then punish it/him.
What about theft?  What about borders?  What
about resale?  Cash?  Sharing and libraries?

@_date: 2003-05-04 11:02:07
@_author: Ian Grigg 
@_subject: The Pure Crypto Project's Hash Function 
Close.  Let's put it this way.  RSA has been
subject to more cryptanalysis than pretty much
any other algorithm, ('cepting maybe DES and
Enigma) in the last century.  Give it another
decade and AES/Rijndael might join that select
That's all based on the functions of encryption
and signing.  So when we say that RSA is good,
we say it on the basis of something like 25 (?)
years of aggressive analysis.  Not because we
can explain it.
(Indeed, I couldn't explain it to save my life.)
But, and here's the clanger:  there is relatively
little (possibly none) of that analysis directed
to RSA as digest algorithm.  So, no-one here is
going to say it is "secure" because there is no
analysis reporting how secure it is.
Now, in crypto, having no analysis is generally
a warning sign.  Having someone say "I know it
to be secure" is a red flag.  Someone saying
"it's better because I can explain it" makes no
sense to anyone, and when someone implies that it
is more secure because it is more explainable,
that's definately proof that someone has ignored
the last couple of centuries of cryptographic
And that, basically is the problem:  you are
ignoring the way things are done in the crypt
industry.  For that pecadillo, you'd better
have a really good reason.  And "easier to
explain" isn't it.  Lots of algorithms have
fallen with that sort of publicity shackled
around their necks.
As a postscript:  The guy who came up with RSA
also came up with the MDn series (MD1, MD2...
MD5).  SHA-x series is essentially based on the
MDn series, they are derivations of the same
process.  So, tuck that thought in the back of
your mind;  you are doing something that the
original guy didn't spend much time over.

@_date: 2003-05-04 11:25:27
@_author: Ian Grigg 
@_subject: The Pure Crypto Project's Hash Function 
Right!  I concur with this frustration.  But, it is
real life;  next time you go over a bridge, pause
for a moment and wonder whether it is going to fall
down.  Well, of course it isn't, because it was built
not to ... by engineers who assumed the responsibility
of handling the complexity.
If you are not convinced, try this:  stand on the
bridge in a 120km wind whistling down the canyon,
and decide how many 60 tonne trucks are allowed
over...  Bridges have limits, and it really takes
a specialist - an engineer - to understand the
In crypto, we call people who understand the limits
of the algorithms 'cryptographers'.  Then there are
the software engineers, who read what the cryptographers
write, and then apply it in software.  I call those
latter 'cryptoplumbers'.  These guys know what a
protocol means, understand what is a real attack and
what is a theoretical attack, and put their names on
the line for user's safety.
There's a really big gulf between them.  Great
cryptoplumbers are not great cryptographers, and
the reverse is as generally true.  One can count
on one hand the number of people who can claim to
seriously contribute in both fields.  And, the
best contributions came from people who knew their
limits and didn't invent things outside those
limits.  Failed cryptosystems generally have the
characteristic that the guys who designed them
went across that border too blithely.
I really do feel the frustration, there are many
among us that have been working to get more crypto
to more users.  But, assuming away the complexity
is not the answer.  And, unless you are going to
spend the next 10 years in academia attacking
the last 200 years of algorithms, inventing your
own algorithm is not the answer either.
I suspect that answer to that is that Modexp doesn't
have the characteristics to make a good hash.

@_date: 2003-05-08 10:34:33
@_author: Ian Grigg 
@_subject: Requim for DRM: Apple music sells one million in less than a week 
As a postscript to the troubled and tortured life
of DRM, Apple showed how to do it last week.  With
a combination of ease of use and easy licensing,
they have just sold a million songs in one week,
right into the market that DRM fans had hoped to
   "Copy protection is kept to a minimum. People
   can keep the songs indefinitely, play them on
   up to three Macs and on an unlimited number
   of Apple iPod music players."
PS:  this puts Apple in the same "Hall of Fame"
as the other Apple, which also achieved selling
a million records in one week.
-------- Original Message --------
BBC NEWS, 2003/05/06
Apple music service hits right note
Apple has sold more than a million songs since the launch of its online music service a week ago.
The figure is much higher than record industry executives had predicted, who had expected Apple to reach the million milestone in a "In less than one week we've broken every record and become the largest online music company in the world," said Apple boss Steve Jobs.
The success of Apple's music venture contrasts with other industry-backed, subscription-based music services which have been slow to take off.
Easy to use
Apple's iTunes Music Store is widely seen as one of the most consumer friendly methods of buying music online.
Copy protection is kept to a minimum. People can keep the songs indefinitely, play them on up to three Macs and on an unlimited  number of Apple iPod music players.
Customers can also can burn unlimited copies of the songs onto CDs.
"Apple has created the first complete solution for the digital music age," said Mr Jobs.
In contrast to other legal music services, there is no subscription fee.  Instead songs cost 99 cents (62 pence) per download.
Apple said more than half of the songs were purchased as albums, countering fears that selling music on a per-track basis would undermine album sales.
And more than half of the 200,000 songs offered by Apple were purchased at least once.
Forrester Research analyst Josh Bernoff said Apple's service marked "the beginning of a torrent of innovation" as others moved away from current subscription-based models.
He said that services like Apple's would attract people who currently use file-sharing programs as they would be "unwilling to put up with the questionable software installs and audio quality of free services".
Good week
"Our internal measure of success was having the iTunes Music Store sell one million songs in the first month. To do this in one week is an over-the-top success," said Doug Morris, Chief Executive Officer of record label Universal.
Apple is planning to add a further 3,200 tracks to the 200,000 already available, including new albums such as Andrea Bocelli's Tosca and Fleetwood Mac's Say You Will.
The online music service is currently only available in the US for Mac computer users. A Windows version is due by the end of the year.
To cap a good week for the computer maker, the company said it had also received 110,000 orders for its new iPods.

@_date: 2003-05-13 09:45:39
@_author: Ian Grigg 
@_subject: Payments as an answer to spam 
That's about the bottom line for prevention of
spam.  Add a payment to each email.  That doesn't
really prevent spam, it just makes it more
As a footnote from economics, it is considered
a bad thing to create a monetary system that
bases its scarcity on destruction of assets.
Such schemes are expected to be dominated by
schemes that achieve the same effect but
manage to conserve their assets.
This applies to hashcash (c.f. Adam B.) or
those various hash collision schemes of
tokenising money (c.f. Ron R?).  Pretty
much all of these schemes can be done more
practically just by doing plain-old-digital-
signatures (PODS?).
The real issue with propogating any such
mail payment scheme (whether destructive or
conservative of value) becomes one of client
ease.  Most all mail clients have trouble
understanding new conventions.
To look at it from an experience pov, if we
could adjust mail protocols and clients easily
enough to add a mail payment scheme, then we
could have done the same to add crypto for
privacy purposes.
Institutionally speaking, we (as an Internet)
have failed to deploy widespread crypto mail
in a lethargic and non-aggresive environment.
It's a bit hard to see how to deploy a mail
payment scheme when we are doing so against
the interests of an active, aggressive, funded
and smart enemy.
Ha!  I didn't know 1mdc had tokenish feature.
And yes, it does, I just picked it up :-)
Thanks Andrew!

@_date: 2003-05-14 12:21:44
@_author: Ian Grigg 
@_subject: Payments as an answer to spam 
I'd disagree with that.  PODS may
imply a centralised server architecture
(but, IMHO, so do payments).  But it
doesn't imply PKI.  Certs imply PKI.
One can do a perfectly good payment
system with PODS, and without PKI.
I'm not actually sure it is possible to
do a good payment system *with* PKI.
I'd suspect too much mass above the
waterline for ultimate stability.
Certainly hashcash has no centralised
component, which makes it better on
that point.  But, it doesn't raise
money.  Those points need to be taken
in balance.  Money makes a centralised
server workable, as its paid for.  More
money makes more centralised servers,
so scaleability is covered too (crudely
Sure, there are those issues.
So your point would be ... "let's strip
PKI out of email crypto and then it would
work?"  No arguments from this side :-)
Speaking any which way, a scheme which
relies on "Have Microsoft integrate it ..."
is dead in the water.  In practice, I think
we are both agreed that deployment of such
a protocol remains an unsolved problem!
Many people think spammers will find a way,
and the problem with Hashcash is that it is
hard to test in the small.  If you and I
follow the protocol, it means nothing to
the spammers.

@_date: 2003-05-14 12:24:29
@_author: Ian Grigg 
@_subject: Payments as an answer to spam 
I think the notion is that if the payment isn't
a good payment, the mail gets junked unread.
Ones Mail agent is supposed to do this for you.

@_date: 2003-05-14 12:36:03
@_author: Ian Grigg 
@_subject: Payments as an answer to spam 
Yes it's a challenge, certainly.  I think the
problem would have to be reduced to many many
issuers of such coins, to enable scaleability.
Great news for providers of such systems, but
still not compelling enough for any of us to
have done anything about it ;-)
Imagine a world dominated by hashcash.  I am
a spammer.  I can either run my servers incurring
a cost of - say - a penny each mail going out,
or I can offer to pay the recipient of my outgoing
mail that same penny to read my mail.
I think in the end the latter would win out.  That's
what economists mean by "more efficient use of
resources."  Those pennies are re-used, which
makes them much more powerful than the pennies
that were burnt up.

@_date: 2003-05-17 11:52:20
@_author: Ian Grigg 
@_subject: Fallacious use of 'bank' in net payment systems 
One of the more pervasive fallacies
in the digital payments world derives
from the use of the words 'bank' and
'financial institutions'.
The original inventor of both the
concept of digital cash, and the first
extant system (Dr David Chaum) made
the assumption that the centralised
entity that issued the value was a
traditional banking institution, a.k.a.,
a Bank.
Experience showed this was a bad choice.
Proper business analysis - the sort that
is taught in b-schools - also shows that
a Bank as the issuer is a bad choice.
(I beg indulgence on this point, one
would normally have to show this, but
space & time are not sufficiently elastic).
Sadly, the word 'bank' lives on.  And it
forcefully implies to everyone that the
center should be a Bank.  Unfortunately,
this implication has contributed to the
destruction of all competant cryptographic
payments systems, and cost us countless
VC millions.
Fast forward to the spam situation.  We
have several actors:  sender & recipient,
sender's ISP & recipient's ISP.  Spammer.
It takes only a moment's thought to
realise that if a payments solution is
to be used to address the spam situation,
then the ideal place for the issuer is
inside the ISP.
  * Each ISP already has a billing arrangement
    with the user,
  * Within the ISP, makes for short fast links
    between the mail payments client, the
    issuance server, the mail forwarding
    server, and the user's mail client,
  * Each ISP already has a relationship with
    each other ISP,
  * Each ISP having its own issuer of email
    postal stamps will more or less efficiently
    match to the scaleability issue of email,
    in that it adds O(1) complexity of links.
  * Each ISP already knows who is mailing whom,
    so there is no need to preserve the privacy
    of payments.
Etc, etc.
I personally use the word Issuer for the
central entity in a payments scheme.  In
token money (a.k.a. DBS / blinded coins)
the word Mint is often used.
The key thing here is that the words 'bank'
and 'financial institution' should be
expunged if ever one had intentions of
building a successful payment scheme.
Otherwise, one ends up with unfounded
assumptions such as, the issuer has to
be an external party!
PS:  from a quick look at the fallacies page
here, I'd say that the 'bank' fallacy is an Appeal
to Common Practice.  The other one I used
above was an Appeal to Authority ;)

@_date: 2003-05-17 12:08:17
@_author: Ian Grigg 
@_subject: Payments as an answer to spam 
So, what you are saying is that people really
didn't appreciate the notion of advertising
on their screens, whether it was being paid
for or not...  I'd guess that the result here
is that the service offering was not accepted
by the public?
People need to read their mail without being
bombarded with spam;  that seems to be a
counter-proposal to optionally asking them
to be paid to see spam.
Right.  Hence one definition of micropayments,
being those payments that can efficiently be made
below the cost of the old infrastructure.
Yep, this is a well known marketing phenomena.
However, it doesn't apply here, AFAICS, as the
user's mail client is expected to make all the
choices for the user without asking the user.
( Then, the user might have to top up the pot
for the client, occasionally, bringing up the
question of whether it is worthwhile...  and,
then, putting pressure on the ISP to deliver
an all-in-one package.  So that might be the
point where I contradict myself ;-)
See my other mail, I'd say that's an unmerited
Yep.  Which is why I think the only way a
payments solution would stand a chance of
succeeding is if it is closely integrated
w.r.t. the mail system.
The way I've always addressed this dilemma
is to do payments, not micropayments.  That
is, payments that can be used for very large
numbers and small numbers too.  Then, we've
worked to make them as efficient as possible,
stretching the payment down to as low a floor
of efficiency as possible.
In a business sense, it's often been observed
that it takes a lot of micropayments to make
a buck of revenue.  So, a pure micropayment
system is dead in the water.
Then you have a choice:  a micropayments
system that is also capable of doing some
larger payments, or a "macro"payments system
that can do some smaller ones.  The advantage
of the latter approach is that presumably
one *can* make revenues on large payments,
and as time goes on, Moores law and better
code gives you a lower floor.
I'd agree with all that.
My perhaps contrarian view on the
spam thing is that a payments solution
dominates the hashcash solution for the
basic economic reason that it better uses
But, I have trouble seeing how a payments
system would be a solution, too, there are
too many barriers (as many people point out).
This puts me in the unenviable position of
being the naysayer;  I can poke holes in
other's proposals, but cannot propose one
of my own.  Oh well.  Let's concentrate on
the issues, rather than the egos, it may
be that out of this discussion comes the
silver bullet...
(Hashcash is a fine invention, and does
credit to its author.  I find it much
more elegant than pretty much all of the
micropayment proposals, which all seem
to lack a sense of relationship to the
dirty world of the net.)

@_date: 2003-05-17 12:23:50
@_author: Ian Grigg 
@_subject: Payments as an answer to spam 
(See my other email on this!)
Right!  It's a bit chalk and cheese, at
that level.
And that precisely is the point:  We have
to live with what they choose.  And, they
choose not to do hashcash.  Not only do
they choose not to do hashcash, I think I
can fairly confidentally say they will
*never* choose to do hashcash (simply because
it makes no sense for Microsoft to do so...).
You literally have no say, nor I, nor any
one, on what Microsoft does.
So, to say that "Have Microsoft implement
it" is actually an impossibility.  None
of us can 'have' Microsoft do anything,
so it's futile to propose anything that
relies on that step.
OTOH, implementing the protocol - whichever
be it hashcash or payments or whitelists -
*can* be done in the free mailers out there.
We can all quit our day jobs and code for
the Mozilla mailer project or whichever,
and have this thing whipped out in a year
or so.  That is within our power.
Whether it is a sensible thing to do is
what people are discussing, as it is simply
not clear what the best way to solve the
spam problem is.
I actually don't have a complete proposal.
I'm just knocking holes in yours!  Sorry
for that.
I can say that a successful protocol
will deliver gains early on, and thus
reward the implementors in their efforts
to code it up into the 'open' mail
software products.
Later on, there is a possibility of
forcing Microsoft to adopt a successful
protocol, or risk being squeezed out of
the market.  It's slim pickings, but in
terms of Microsoft, that's all we, the
net people, have.
C.f., the web v. Blackbird, Java v.
C-sharp, etc etc.
Which is why, when we say that a spam
protection system only works when all
follow it, this actually means it is
unworkable.  Because there is no way
for us to implement a protocol that
everyone will adopt (or have adopted
for them).

@_date: 2003-05-18 10:31:43
@_author: Ian Grigg 
@_subject: Fallacious use of 'bank' in net payment systems 
One ISP can send packets to another (IP), and one
ISP can forward email to another (SMTP).  That's
the relationship.  As written up by Adam B, either
of these protocols can be extended or piggy backed
to carry non-core anti-spam tokens.
Compare this to banks:  one bank can send
a payment to another bank.  (At a pinch, a
bank can even manage to direct the payment
to the right user.)
But, there is no relationship between any bank
and any ISP.  There are only customer-provider
relationships between pairs of these agents.
In net terminology, relationship might be
rewritten as 'protocol'.  In which case ISPs
talk IP, SMTP, etc.  Banks talk SWIFT, FedWire,
BACS, etc.
As far as email is concerned, you are your
own ISP.

@_date: 2003-05-29 13:22:04
@_author: Ian Grigg 
@_subject: "PGP Encryption Proves Powerful" 
The following appears to be a bone fide case of a
threat model in action against the PGP program.
Leaving aside commentary on the pros and cons
within this example, there is a desparate lack of
real experience in how crypto systems are attacked.
IMHO, this leads to some rather poorly chosen
engineering decisions that have shown themselves
to stymie or halt the success of otherwise good
crypto systems.
Does anyone know of a repository for real life
attacks on crypto systems?  Or are we stuck with
theoretical and academic threats when building
new systems?
PS: for the archives:
PGP Encryption Proves Powerful If the police and FBI can't crack the code, is
the technology too strong? Philip Willan, IDG News Service Monday,
May 26, 2003 ROME -- Italian police have seized at least
two Psion personal digital assistants from
members of the Red Brigades terrorist
organization. But the major investigative
breakthrough they were hoping for as a result
of the information contained on the devices
has failed to materialize--thwarted by
encryption software used by the left-wing
revolutionaries. Failure to crack the code, despite the
reported assistance of U.S. Federal Bureau
of Investigation computer experts, puts a
spotlight on the controversy over the wide
availability of powerful encryption tools. The Psion devices were seized on March 2
after a shootout on a train traveling between
Rome and Florence, Italian media and
sources close to the investigation said. The
devices, believed to number two or three,
were seized from Nadia Desdemona Lioce
and her Red Brigades comrade Mario Galesi,
who was killed in the shootout. An Italian
police officer was also killed. At least one of
the devices contains information protected
by encryption software and has been sent for
analysis to the FBI facility in Quantico,
Virginia, news reports and sources said. The FBI declined to comment on ongoing
investigations, and Italian authorities would
not reveal details about the information or
equipment seized during the shootout. Pretty Good Privacy The software separating the investigators
from a potentially invaluable mine of
information about the shadowy terrorist
group, which destabilized Italy during the
1970s and 1980s and revived its practice of
political assassination four years ago after a
decade of quiescence, was PGP (Pretty
Good Privacy), the Rome daily La Repubblica
reported. So far the system has defied all
efforts to penetrate it, the paper said. Palm-top devices can only run PGP if they
use the Palm OS or Windows CE operating
systems, said Phil Zimmermann, who
developed the encryption software in the
early 1990s. Psion uses its own operating
system known as Epoc, but it might still be
possible to use PGP as a third party add-on,
a spokesperson for the British company said.
There is no way that the investigators will
succeed in breaking the code with the
collaboration of the current manufacturers of
PGP, the Palo Alto, California-based PGP,
Zimmermann said in a telephone interview. "Does PGP have a back door? The answer is
no, it does not," he said. "If the device is
running PGP it will not be possible to break it
with cryptanalysis alone." Investigators would need to employ
alternative techniques, such as looking at the
unused area of memory to see if it contained
remnants of plain text that existed before
encryption, Zimmermann said. Privacy vs. Security The investigators' failure to penetrate the
PDA's encryption provides a good example of
what is at stake in the
privacy-versus-security debate, which has
been given a whole new dimension by the
September 11 terrorist attacks in the U.S. Zimmermann remains convinced that the
advantages of PGP, which was originally
developed as a human rights project to
protect individuals against oppressive
governments, outweigh the disadvantages. "I'm sorry that cryptology is such a
problematic technology, but there is nothing
we can do that will give this technology to
everyone without also giving it to the
criminals," he said. "PGP is used by every
human rights organization in the world. It's
something that's used for good. It saves
lives." Nazi Germany and Stalin's Soviet Union are
examples of governments that had killed far
more people than all the world's criminals and
terrorists combined, Zimmermann said. It
was probably technically impossible,
Zimmermann said, to develop a system with
a back door without running the risk that the
key could fall into the hands of a Saddam
Hussein or a Slobodan Milosevic, the former
heads of Iraq and Yugoslavia, respectively. "A lot of cryptographers wracked their brains
in the 1990s trying to devise strategies that
would make everyone happy and we just
couldn't come up with a scheme for doing it,"
he said. "I recognize we are having more problems
with terrorists now than we did a decade ago.
Nonetheless the march of surveillance
technology is giving ever increasing power to
governments. We need to have some ability
for people to try to hide their private lives and
get out of the way of the video cameras," he
said. More Good Than Harm? Even in the wake of September 11,
Zimmermann retains the view that strong
cryptography does more good for a
democracy than harm. His personal website,
PhilZimmerman.com, contains letters of
appreciation from human rights organizations
that have been able to defy intrusion by
oppressive governments in Guatemala and
Eastern Europe thanks to PGP. One letter
describes how the software helped to protect
an Albanian Muslim woman who faced an
attack by Islamic extremists because she
had converted to Christianity. Zimmermann said he had received a letter
from a Kosovar man living in Scandinavia
describing how the software had helped the
Kosovo Liberation Army (KLA) in its
struggle against the Serbs. On one occasion,
he said, PGP-encrypted communications had
helped to coordinate the evacuation of 8,000
civilians trapped by the Serbs in a Kosovo
valley. "That could have turned into another
mass grave," Zimmermann said. Italian investigators have been particularly
frustrated by their failure to break into the
captured Psions because so little is known
about the new generation of Red Brigades.
Their predecessors left a swathe of blood
behind them, assassinating politicians,
businessmen, and security officials and
terrorizing the population by "knee-capping,"
or shooting in the legs, perceived opponents.
Since re-emerging from the shadows in 1999
they have shot dead two university
professors who advised the government on
labor law reform. Cracking the Code Zimmermann is not optimistic about the
investigators' chances of success. "The very
best encryption available today is out of
reach of the very best cryptanalytic methods
that are known in the academic world, and
it's likely to continue that way," he said. Sources close to the investigation have
suggested that they may even have to turn to
talented hackers for help in breaking into the
seized devices. One of the magistrates
coordinating the inquiry laughed at mention of
the idea. "I can't say anything about that," he
said. The technical difficulty in breaking PGP was
described by an expert witness at a trial in
the U.S. District Court in Tacoma,
Washington, in April 1999. Steven Russelle,
a detective with the Portland Police Bureau,
was asked to explain what he meant when he
said it was not "computationally feasible" to
crack the code. "It means that in terms of
today's technology and the speed of today's
computers, you can't put enough computers
together to crack a message of the kind that
we've discussed in any sort of reasonable
length of time," he told the court. Russelle was asked whether he was talking
about a couple of years or longer. "We're
talking about millions of years," he replied.

@_date: 2003-11-16 12:14:43
@_author: Ian Grigg 
@_subject: A-B-a-b encryption 
Perhaps you want to try looking at the various blinding
algorithms by Chaum (and Wagner, also Brands).  These use
schemes that are transitive (if that's the word).

@_date: 2003-11-25 14:19:48
@_author: Ian Grigg 
@_subject: Cryptophone locks out snoopers 
(link is very slow:)
Cryptophone locks out snoopers By electricnews.net
Posted: 20/11/2003 at 10:16 GMT
A German firm has launched a GSM mobile phone that
promises strong end-to-end encryption on calls,
preventing the possibility of anybody listening in. If you think that you'll soon be seeing this on the shelves
of your local mobile phone shop though, think again. For
a start, the Cryptophone sells for EUR1,799 per handset,
which puts it out of the reach of most buyers. Second,
the phone's maker, Berlin-based GSMK, say the phone
will not be sold off the shelf because of the measures
needed to ensure that the product received by the
customer is untampered with and secure. Buyers must
buy the phone direct from GSMK. According to GSMK, the new phone is designed to
counteract known measures used to intercept mobile
phone calls. While GSM networks are far more secure
than their analogue predecessors, there are ways and
means to circumvent security measures. The encryption in GSM is only used to protect the call
while it is in the air between the GSM base station and
the phone. During its entire route through the telephone
network, which may include other wireless links, the call
is not protected by encryption. Encryption on the GSM
network can also be broken. The equipment needed to do
this is extremely expensive and is said to be only
available to law enforcement agencies, but it has be
known to fall into the hands of criminal organisations. The Cryptophone is a very familiar-looking device, since
it is based around the same HTC smartphone that O2
used as its original XDA platform. The phone runs on a
heavily modified version of Microsoft Pocket PC 2002. GSMK says it is the only manufacturer of such devices
that has its source code publicly available for review. It
says this will prove that there are no back-doors in the
software, thus allaying the fears of the
security-conscious. Publication of the source code
doesn't compromise the phone's security, according to
GSMK. The Cryptophone is engineered in such a way
that the encryption key is only stored in the phone for the
duration of the call and securely erased immediately
afterwards. One drawback of the device is that it requires the
recipient of calls to also use a Cryptophone to ensure
security. GSMK does sell the device in pairs, but also
offers a free software download that allows any PC with
a modem to be used as a Cryptophone. GSMK says that the Cryptophone comples with German
and EU export law. This means the device can be sold
freely within the EU and a number of other states such
as the US, Japan and Australia. It cannot be sold to
customers within Afghanistan, Syria, Iraq, Iran, Libya
and North Korea. A number of other states are subject
to tight export controls and a special licence will have to
be obtained. ? ElectricNews.Net

@_date: 2003-11-28 16:31:12
@_author: Ian Grigg 
@_subject: Open Source Embedded SSL - (License and Memory) 
I think this applies more generally, but especially
for crypto software, because of the legal environment
and the complicated usage to which it is often put.
Placing any burdens of a non-technical nature on the
user is generally a downer.  Crypto-newbies are often
unsure and under rather intense pressure to get
something out.  If uncertainties of code licensing
issue are added, it can have a marked effect on the
The general result is a choice between no crypto and
poorly done crypto.  (Rarely is good crypto done in
the first instance.)  Opinions differ on this point,
but I generally err on the side of recommending less
than perfect crypto, which can be repaired later on
at a lower cost.  It's a lot easier to sell a manager
on "replacing poor crypto" when it becomes needed
than on "we need to add a crypto layer."
For that reason, we (Cryptix) have always placed all
our code under a BSD style licence, except a few cases
where it has been placed under public domain (AES).  Our
view has always been, "with crypto, the least barriers
the better."
In essence, "get it out there" is the mantra.

@_date: 2003-10-01 14:24:00
@_author: Ian Grigg 
@_subject: Monoculture 
Sadly, there is a shared culture amongst cryptography   professionals that presses a certain logical, scientific What is written in these posts (not just the present one)
does derive from that viewpoint and although one can   quibble about the details, it does look very much from
the outside that there is an informal "Cryptographers  Guild" in place [1].
I don't think the jury has reached an opinion on why
the cryptography group looks like a guild as yet,
and it may never do so.  A guild, of course, is either
a group of well-meaning skilled people serving the
community, or a cartel for raising prices, depending
on who is doing the answering.
But, even if a surprise to some, I think it is a fact
that the crypto community looks like and acts as if a
This is where maybe the guild and the outside world part
The guild would like the application builder to learn the
field.  They would like him to read up on all the literature,
the analysies.  To emulate the successes and avoid the
pitfalls of those protocols that went before them.  The  guild would like the builder to present his protocol and  hope it be taken seriously.  The guild would like the
builder of applications to reach "acceptable" standards.
And, the guild would like the builder to take the guild
seriously, in recognition of the large amounts of time
guildmembers invest in their knowledge.
None of that is likely to happen.  The barrier to entry
into serious cryptographic protocol design is too high
for the average builder of new applications [2].  He has,
after all, an application to build.
What *is* going to happen is this:  builders will continue
to ignore the guild.  They will build their application,
and throw any old shonk crypto in there.  Then, they will
deploy their application, in the marketplace, and they will
prove it, in the marketplace.
The builder will find users, again, in the marketplace.   At some point along this evolution, certain truths will   become evident:  the app is successful (or not).  The code
is good enough (or not).  People get benefit (or not).
Companies with value start depending on the app (or not).
Security is adequate (or is not).  Someone comes along and
finds some easy breaches (or not).  That embarrasses (or
And, maybe someone nasty comes along and starts doing
damage (or not).
What may not be clear is that the investment of the security
protocol does not earn its effort until well down the track.
And, as an unfortunate but inescapable corollary, if the app
never gets to travel the full distance of its evolutionary
path, then any effort spent up front on high-end security
is wasted.
Crypto is high up-front cost, and long term payoff.  In
such a scenario, standard finance theory would say that
if the project is risky, do not add expensive, heavy duty
crypto in up front.
This tradeoff is so strong that when we look about the
security field, we find very few applications that
succeeded when also built with security in mind from
the initial stages.
And, almost all successful apps had little or bad security
in them up front.  If they needed it later, they required
expensive add-ons.  Later on.
There are no successful systems that started with perfect
crypto, to my knowledge.  There are only perfect protocols
and successful systems.  A successful system can evolve
to enjoy a great crypto protocol, but it would seem that
a great protocol can only spoil the success of a system
in the first instance.
The best we can hope for, therefore, in the initial phase,
is a compromise: maybe the builder can be encouraged to
think about security as an add-on in the future?
Maybe some cheap and nasty crypto can be stuck in there
as a placemarker?  The equivalent of TEA or 40 bit RC4,
but in a protocol sense.
Or, maybe he can encourage a journeyman of the guild to
add the stuff in, on the side, as a fun project.
Maybe, just maybe, someone can create Bob's Simple Crypto
Library.  As a stopgap measure, as a simple little thing
that is so easy to use that even your average apps builder
wouldn't get turned away.  And just happens to be ammeniable
to replacement later on with Alice's Full & Complex Crypto
It's pretty much a given that projects will get out there
with inadequate security.  The market process for apps dictates that.
Although I haven't tested it, I believe it to be an
economically sound result.  The fact that this has little
to do with crypto is ... irrelevant.  That fact that the
cryptographers guild may find this uncomfortable is ... What remains uncertain is, when the time comes to upgrade
the security and add some real strength to an application
that cries out for help, will there be anyone there to    help add the strength?
"Why aren't you using SSL" is basically a warning sign
to all developers that the man asking the question is a
fully paid-up member of the guild.
"Why did you do that?" is also not the right question.  "What is your threat model?" is a far better start.
"What do you have in place right now?" is more or less
the question that Peter Guttman was asking, and that is
also a good start.
Even better would be a preamble that describes how we
need to get the threat model down on paper so that an
appropriate level of security can be put in place.
Well, the opposition to "the guild" is one of pro-market
people who get out there and build applications.  Some
people used to call them entrepreneurs, but these days   in the net, we think of them as just hackers, because
anyone with a code editor can be an entrepreneur.
They may sometimes say the wrong things about their own
crypto and their own security constructs - such as TINC's
"foolishly childish" attitude may have come across - but
that's just their eternel optimism and innate marketing
sense overriding any sense of absolute but irrelevant truth.
That's just them being entrepreneurs.  Don't however take
home the wrong message.  When they defend the crypto, that's
because they have a strong sense of priorities.  When they
need better crypto, then they will know it.  Because the
market will tell them.
It's not the first time that entrepreneurs have been
called kooks :-)  It generally happens before they are
successful, though.
[1] I'm ignoring the placement of the apostrophe for the
[2] We can argue the detail, but my point here is that the
barrier to entry is too high, so ignoring the guild is the
most likely result.  That said, here are some points.
It's nice that the literature is open and available.  What
is not nice is how much there is of it.  It's nice that
there is a lot of code available, and complete protocols
to boot.  What is not nice is that there is no easy way
to work out which code to use, and the protocols are not
so easy to understand.  It's nice that we have an open
community that discusses these things.  What is not nice
is that, in trying to determine the one path, the advice
of the community reduces to useless baubles like "use SSL"
or "why did you do that?"  It's great that the community
has standards, but those standards seem to be excessive
in the extreme.

@_date: 2003-10-01 14:34:23
@_author: Ian Grigg 
@_subject: Monoculture 
This is also security-minded thinking on the part
of the customer.
Including extra functionality means that they have
to understand it, they have to agree with its choices,
they have to follow the rules in using it, and have
to pay the costs.  If they can ditch the stuff they
don't want, that means they are generally much safer
in making simple statements about the security model
that they have left.
So, coming up with a tailor-made solution has the
security advantage of reducing complexity.  If one
is striving to develop the whole security model on
ones own, without the benefit of formal methods,
that approach is a big advantage.
(None of which goes to say that they won't ditch a
critical component, of course.  I'm just trying to
get into their heads here when they act like this.)

@_date: 2003-10-01 15:54:24
@_author: Ian Grigg 
@_subject: Monoculture 
What's your threat model?  Or, that's your threat
Applying the above threat model as written up in
"The Codebreakers" to, for example, SSL and its
original credit card nreeds would seem to be a
On the face of it, that is.  Correct me if I'm
wrong, but I don't recall anyone ever mentioning
that anyone was ever killed over a sniffed credit
And, I'm not sure it is wise to draw threat models
from military and national security history and
apply it to commercial and individual life.
There are scenarios where people may get killed
and there was crypto in the story.  But they are
far and few between [1].  And in general, those
parties gradually find themselves taking the crypto
seriously enough to match their own threat model
to an appropriate security model.
But, for the rest of us, that's not a good threat
model, IMHO.
OK, that's a statement.  What is clear is that,
regardless of the truth of the that statement,
developers time and time again look at the crypto
that is there and conclude that it is "too much."
The issue is that the gulf is there, not whether
it is a fair gulf.
BTW, just to clarify.  The intent of my post was not to
claim that there is a guild.  Just to claim that there
is an environment that is guild-like.
I find this mysterious.  When I send encrypted email
to my girlfriend with saucy chat in there, is that
what you mean by "critical" ?  Or perhaps, when I send
a credit card number that is limited to $50 losses, is
verified directly by the merchant, and has a home
delivery address, do you mean, that's "critical" ?  Or,
if I implement a VPN between my customers and suppliers,
do you mean that this is "critical" ?
I think not.  For most purposes, I'm looking to reduce
the statistical occurrences of breaches.  I'll take
elimination of breaches if it is free, but in the
absence of a perfect world, for most comms needs, near
enough is fine by me, and anyone that tells me that the
crypto is 100% secure is more than likely selling snake
For those applications that *are* critical, surely the
people best placed to understand and deal with that
criticality are the people who run the application
themselves?  Surely it's their call as to whether they
take their responsibilities fully, or not?
[1] the human rights activities of do in fact present a case where people can get killed, and their
safety may depend to a lesser or greater extent on crypto.

@_date: 2003-10-01 19:46:43
@_author: Ian Grigg 
@_subject: anonymous DH & MITM 
Ah, there's the rub.  ADH does not protect against
MITM, as far as I am aware.
View A:
MITM is extremely rare.  It's quite a valid threat
model to say that MITM is a possibility that won't
need to be defended against, 100%.
E.g.1, SSH which successfully defends most online
Unix servers, by assuming the first contact is a
good contact.  E.g.2, PGP, which bounces MITM
protection up to a higher layer.
Or, what's your threat model?  Why does it include
MITM and how much do you want to pay?
View B:
MITM is a real and valid threat, and should be
considered.  By this motive, ADH is not a recommended
mode in TLS, and is also deprecated.
Ergo, your threat model must include MITM, and you
will pay the cost.
(Presumably this logic is behind the decision by the
TLS RFC writers to deprecate ADH.  Hence, talking
about ADH in TLS is a waste of time, which is why I
have stopped suggesting that ADH be used to secure
browsing, and am concentrating on self-signed certs.
Anybody care to comment from the TLS team as to what
the posture is?)

@_date: 2003-10-01 19:53:49
@_author: Ian Grigg 
@_subject: VeriSign tapped to secure Internet voting 
It's actually quite cunning.  The reason that this
is going to work is because the voters are service
men & women, and if they attack the system, they'll
get their backsides tanned.  Basically, it should
be relatively easy to put together a secure voting
application under the limitations, control structures
and security infrastructure found within the US military.
It would be a mistake to apply the solution to wider
circumstances, and indeed another mistake to assume
that Verisign had anything to do with any purported
"success" in "solving" the voting problem.

@_date: 2003-10-01 23:50:01
@_author: Ian Grigg 
@_subject: anonymous DH & MITM 
I agree.  As a side note, I think it is probably
a good idea for TLS to deprecate ADH, simply
because self-signed certs are more or less
equivalent, and by unifying the protocol around
certificates, it reduces some amount of complexity
without major loss of functionality.
(AFAIK, self-signed certs in every way dominate
ADH in functional terms.)
I think we are in agreement on that!?
An appropriate security model for a security conference
might be to put a sign up at the door saying
    "All your assumptions are belong to us"
At least that way everyone would be in tune with the
nature of the conference.
Anything that happens at the Usenix Security Conference
is, in my book, ruled out of ones regular, commercially
relevant threat model.  Same goes for "demos in a Uni
student lab."
We all know it's possible.  The question is, should we
worry about it?  And, following on from Perry's method,
should we impose our own fears on others?
A threat must occur sufficiently in real use, and incur
sufficient costs in excess of protecting against it, in
order to be included in the threat model on its merits.
I know a couple of instances were posted maybe 6
months back.  What we need really is some sort of
repository of MITM attacks "in the wild."  Costs
would be very useful too.

@_date: 2003-10-02 16:53:25
@_author: Ian Grigg 
@_subject: crypto licence 
With crypto code, we have taken the view that it
should BSD 2 clause.  The reason for this is that
crypto code has enough other baggage, and corporates
are often the prime users.  These users are often
scared very easily by complex licences.
We'd tended to vacilate somewhat with applications,
between various Mozilla/Sun community models, but
with the underlying crypto, always as free as possible.
If you wanted to be in the GPL community, then LGPL.
GPL itself will infect any apps, so unless you have
a really great belief that you want those users and
no others, stick to LGPL.
Mind you, those have been our experiences.  It's
quite plausible that we'd have attracted a bigger
developer base simply by going GPL.

@_date: 2003-10-03 12:07:46
@_author: Ian Grigg 
@_subject: using SMS challenge/response to secure web sites 
Merchants who *really* rely on their web site being
secure are those that take instructions for the
delivery of value over them.  It's a given that they
have to work very hard to secure their websites, and
it is instructive to watch their efforts.
The cutting edge in making web sites secure is occuring
in gold community and presumably the PayPal community (I
don't really follow the latter).  AFAIK, this has been
the case since the late 90's, before that, some of the
European banks were doing heavy duty stuff with expensive
e-gold have a sort of graphical number that displays
and has to be entered in by hand [1].  This works against
bots, but of course, the bot writers have conquered
it somehow.  e-gold are of course the recurrent victim
of the spoofers, and it is not clear why they have not
taken serious steps to protect themselves against
attacks on their system.
eBullion sell an expensive hardware token that I have
heard stops attacks cold, but suffers from poor take
up because of its cost [2].
Goldmoney relies on client certs, which also seems
to be poor in takeup.  Probably more to do with the
clumsiness of them, due to the early uncertain support
in the browser and in the protocol.  Also, goldmoney
has structured themselves to be an unattractive target
for attackers, using governance and marketing techniques,
so I expect them to be the last to experience real tests
of their security.
Another small player called Pecunix allows you to integrate
your PGP key into your account, and confirm your nymity
using PGP signatures.  At least one other player had
decided to try smart cards.
Now a company called NetPay.TV - I have no idea about
them, really - have started a service that sends out
a 6 digit pin over the SMS messaging features of the
GSM network for the user to type in to the website [4].
It's highly innovative and great security to use a
completely different network to communicate with the
user and confirm their nymity.  On the face of it,
it would seem to pretty much knock a hole into the
incessant, boring and mind-bogglingly simple attacks
against the recommended SSL web site approach.
What remains to be seen is if users are prepared to
pay 15c each time for the SMS message.  In Europe,
SMS messaging is the rage, so there won't be much
of a problem there, I suspect.
What's interesing here is that we are seeing the
market for security evolve and bypass the rather
broken model that was invented by Netscape back in
'94 or so.  In the absence of structured, institutional,
or mandated approaches, we now have half a dozen distinct
approaches to web site application security [4].
As each of the programmes are voluntary, we have a
fair and honest market test of the security results [5].
[1]  here's one if it can be seen:
Hopefully that doesn't let you into my account!
It's curious, if you change the numbers in the above
URL, you get a similar drawing, but it is wrong...
[2] All companies are .com, unless otherwise noted.
[3] As well as the activity on the gold side, there
are the adventures of PayPal with its pairs of tiny
payments made to users' conventional bank accounts.
[4]  Below is their announcement, for the record.
[5]  I just thought of an attack against NetPay.TV,
but I'll keep quiet so as not to enjoy anyone else's
fun :-)
============================================================== N E T P A Y. T V N E W S L E T T E R October 3rd, 2003 Sent to NetPay members only, removal instructions at the
end of the message 1. SMS entry - Unique Patent pending entry system -
World first!  What is this new form of entry? Do you own a mobile phone? Can you receive SMS
messages? Would you like to have your own personal
NetPay security officer contact you when entry to your
account is required? Netpay would like to introduce a world
first in account security. This new feature is so simple, yet
so effective - we believe every member will utilize it. If you answered yes to the above, then your SMS capable
mobile is a powerful security device, which will stop any
unforced attempts of entry into your Netpay account. No
need to purchase expensive security token hardware, no
need to be utterly confused on how to use the security
device. If you know how to use your mobile, then you know
how to totally protect your Netpay account from any
possible unlawful entry. This new system sends you an automated 6 digit secure
random PIN direct to your phone whenever you try to
access your account. Without this PIN, it is impossible to
login. The PIN arrives direct to your mobile within seconds!
It is as good as having your own personal security officer
calling you whenever someone is trying to access your
account! SMS AUTHENTICATED SECURITY ENTRY It is simple. This new feature allows each member to set
his or her own mobile phone number within his or her
account. Now when you go to access your account again,
you choose to Login via SMS authentication (It is
impossible to access via standard login once you load a
mobile phone number within your account). You only need to now remember your 4 digit Trojan
bypassing PIN (unique to NetPay) and your Netpay
account number (this number is public knowledge). No
need to recall your password or any other numbers, as the
server instantly links your PIN and account number with
your SMS enabled mobile phone and sends you a random -
one time, 6 digit PIN (expires after 5 minutes) instantly to
your SMS capable phone! Once you receive the random security code, you then enter
it into the final entry page online and you now have access
to your account. Visit Flash promo here:  How do I set SMS entry for my account and what does it
cost? Very simple. Enter your account and go to the My Info
page in the secure members area - ADD your mobile phone
by clicking the link which states: Cell phone # for SMS
authentication. authorize this by entering your NetPay ID number (set
when you registered and is more than likely a passport or
drivers license number). You now have total SMS
protection. Each time you access, a small charge of 15c is removed
from your account to cover the fee charged for the SMS
secure message. This 15c is the best money spent when it
comes to securing your account online. *Remember, you
must have your SMS phone enabled and on when you try to
access or you will not be able to enter your account. Why is the new Patent pending NetPay SMS entry system
simpler than security encryption tokens or calculators? - Nearly everyone can use a GSM enabled mobile phone
(no confusion compared to expensive hardware tokens and
yet our online entry system is just as secure) - SMS
messages can be received in any country using GSM
(please check our list of countries before registering your
mobile as your phone may not be compatible). - It is totally portable. You can access your account from
any PC, anywhere SMS messages can be sent and as long
as you have your mobile switched on. Even if the PC was
infected with a virus, you can still enjoy secure access! - No need to purchase expensive and confusing hardware -
just simply read your text SMS message via your mobile
and you can access your account in total security. Total
account protection for only 15c per authorized entry. - You dont need to send any messages back for
verification, simply enter the 6 digits PIN you receive on
the online automated form. - SSL security ensures the PIN is totally encrypted,
random, and only able to be used once only. What makes this more secure than standard entry
systems? -Even if the user was using a PC with a virus or Trojan
keyboard logger, it would not be possible for the hacker to
obtain the 6 digit PIN being sent to your mobile - as it is a
one time, expiring PIN and is sent totally offline. Even if it
records the PIN when you enter it - it is useless, as it can
only be used once. This ensures at all times, even when
one is using someone elses PC, they are secure. The only
details the Trojan will pick up is the users account number,
which is common public knowledge (as the 4 digit Netpay
PIN number is entered not via the keyboard but using our
Patented keyless entry - which bypasses standard Trojans). - 6 digit SMS PIN sent is a one time only useable PIN,
totally random and is valid for a maximum of only 5
minutes. This ensures even if someone else reads the SMS
and tries to access, the time limit would have expired after
the 5-minute period. - SMS messages travel on a secure signaling network -
any possible interception of the PIN would take
considerable effort and time, thus the 5 minute limited life
of the random PIN ensure full security - Even if someone were to steal your mobile, they would
not know how to access your NetPay account as they
would need to know both your account number and 4 digit
security PIN. ENJOY THIS PATENTED FORM OF ENTRY NOW -
ACCESS YOUR ACCOUNT AND ADD A SMS MOBILE
NUMBER TODAY (COSTS ONLY 15c PER
AUTHORIZED ENTRY).

@_date: 2003-10-03 13:57:32
@_author: Ian Grigg 
@_subject: Simple SSL/TLS - Some Questions 
Don't worry about making enemies, they'll worry about
it for you :-)
C.  And write C++ wrappers or let someone else do it.
(IMHO.  I don't write much C++ but it seems to be
basically dangerous and difficult to get right.  Also,
you already have enough on your plate in securely
handling C, without having to worry about the built
in insecurities of C++ :-)
Just TLS, and only the compulsory parts.  Leave in
hooks for other parts right now.  Also, ditch the
Anon-DH mode, and stick to the cert model.
Again, IMHO.  Your market will be developers who want
a simple secure channel product.  Market in general
will not be people who already have to meet SSL as a spec,
those people will go with OpenSSL.  You want the green
field developers, and there is no reason to offer them
old stuff.
OpenSSL is thought to be complex, less well documented,
hard to use.  I've not "used" it, but I've hacked it a
couple of times, and that's how I remember it.  You sort
of have to be a C programmer so you can muck in and figure
out what it is doing.
Also, OpenSSL provides everything.  But you have to know
how to configure everything up.  So you have to understand
the choices placed in front of you, or blindly follow the
lead of various examples.
So, an alternate approach is to set up one way of doing
everything, to give you one rather good, but not perfect,
connection product.
It may very well have improved an awful lot since I've
looked, who knows?  But, reports keep coming in that it
is too hard to get into...
Go with your gut feel.  Don't listen to the experts,
you'll never get a consistent viewpoint, and even the
ones that disagree will be wrong :-)
My advice:  if the standard gets in your way, ignore it.
All standards are camels, and your job is to ride beasts
of burden, not the other way around.  Deliver product,
and don't let a bunch of horse designers push you around.
Create a single security product that talks just pure TLS.
The latest and greatest.  You will have enough to worry
about keeping track of future changes, let alone dealing
with ancient history.
Bear in mind that you are looking at a year-long project
here.  There is a reason why OpenSSL is the choice...
because it is already written.
I'll think more on those, or quietly slink away without
appearing more dumb than normal.  All the above is IMHO,
so please try hard to ignore it!

@_date: 2003-10-03 14:56:15
@_author: Ian Grigg 
@_subject: anonymous DH & MITM 
Conventionally, I think, Anonymity is when one
publishes a pamphlet of political criticism, and
there is no name on the pamphlet.
When the same person publishes a second pamphlet,
there is nothing to connect the two.  (Other than
style, of course.)
Psuedonymity would result if "Whielacronx" were
to appear on both pamphlets, so the readers could
establish a reputational link between the pamplets.
Anonymity doesn't support a connection.  Now, I
think there is value in trying to use these terms
as much as possible in alignment with their old
world roots.  But that might not always be possible....
So, in this sense, on the net, it is impossible
to open a connection anonymously.  The TCP/IP
connection system requires a source IP number,
and then allocates a port.  So a psuedonym of
IP/port gets allocated for the length of the
Bearer tokens normally achieve untraceability, in
the pure technical sense.  As most bearer systems
include a conventional identity based account of
some form, the notion of anonymity is confusing,
as certain actions can reverse the untraceability
and reveal the identities of the accounts.  E.g.,
double spending.
Of course, in the media and literature, anonymity
is widely used to refer to untraceable bearer
tokens.  As anonymity isn't so useful in its own
right, there appears to be few real problems with
this usage, until one starts bandying around more
than one concept.
Psuedonymous systems for transactions are normally
the reverse:  traceable, but one can only see the
chosen psuedonym, and that is not directly related
to any other info that might be useful.
But you knew all that :-)
My guess is that it is what Zooko's son says when he
is learning his name.

@_date: 2003-10-03 15:59:10
@_author: Ian Grigg 
@_subject: threat modelling strategies 
A very nice distinction.  The problem with this approach
is that it depends heavily on the notion of "reasonably
expect," which is highly obvious, after the fact.
In each of those cases, it was possible to trace the
development of the attack through history, again,
after the fact [1], [2], [3].
In each case, the history was mostly readable.  Just
like security today.  In each case, it was very difficult
to predict the future.  And, for those lucky few who
did, they were ignored.  And, for those lucky few
who did predict correctly, there were many score more
who predicted the wrong thing.
Military affairs are fairly typecast.  You are stuck
with the weapons of the past, chasing an infinite
number of possibilities in the future.  In all that,
you have to fight the current war.  Prepare for some
unlikely future at your peril.  If you pick the wrong
one, you'll be accused of being a dreamer, or of
fighting the last war.  Pick a future that actually
happens, and you'll be called a genius.
Crypto systems get pretty much deployed like that
as well.  Reasonable threat models are built up,
a point in the future is aimed for, and the system
gets deployed.  Then, you hope that attacks like
that of Adi Shamir's student don't happen until
the very end of life.  You watch, and you hope.
The alternate is to prepare for every possible
threat.  That's hard.  It may be that you can
justify this level of expenditure, but for most
ordinary missions, this is simply too expensive.
Mind you, I'm not sure of your first claim there,
can you explain why the security field has not
moved quickly to counter the threat of web site
spoofing?  It's been around for yonks, and it's
resulting in losses....
Anyone out there willing to send a chat message
that is protected by ROT13?
As we have defined our mission, we can set our
requirements, and build our threat model.
I don't see that the presence of huge costs in
some exotic industries means the rest of us have
to pay for heart surgery every time we want to
send a chat message.  Or face death threats every
time we pay for flowers with a credit card.
But, I grant you that FUD will play a part in
the ongoing evolution of the Cryptologists'
Guild, just as it has in the past.  It's too
powerful a card to ignore, just because it is
YMMV :-)
[1] Although Guderian's development of Blitzkreig was
kept a secret, as was all German war planning, it wasn't
totally unemulated by the Allies, just not up-played
as well as it might have been &.  C.f., Patton, who
famously "read Rommel's book," and de Gaulle, who
parlied a presidency out of his success at holding
back the Guderian advances, albeit briefly.
In fact, the French tanks outnumbered, outgunned, and
out armoured the Germans,  The Versaille Treaty
banned Germany from having *any* armoured vehicles.
That's preparation!
& _Panzer Leader_, General Heinz Guderian, 1952.
[2] box cutters v. skyscrapers - I have a collection of
films that predict the activities of 9/11 in the years
before *. In each case, note that Hollywood famously
predicts not only a diabolical attack with many
similarities, but also a cunningly devious deception
* _Die Hard_ (1,2,3), _Executive Decision_,
_Under Seige_, _The Seige_, and I gather one of
the Clancy books describes the precise form of
the attack.  Oh, and countless James Bond movies.
[3] Bombers v. battleships - yes, although bombers v.
submarines, no.  It took sonar, depth chargers and
lots of frigates to counter the U-boats.  Vacuum
tubes v. enigma machines, yes with 3 reels, not
immediately with 4, and useless when Hitler switched
back to motorcycle couriers in the battle of the
Bulge.  Etc).

@_date: 2003-10-04 15:13:43
@_author: Ian Grigg 
@_subject: Strong-Enough Pseudonymity as Functional Anonymity 
In a strictly theoretical sense, isn't this essentially
the job of the (perfect) TTP?  At least that's the way
many protocols seem to brush away the difficulty.

@_date: 2003-10-06 15:38:37
@_author: Ian Grigg 
@_subject: anonymous DH & MITM 
True, I think!  Is there a practical application for this?
( I can think of one trivial example: "a message system is
psuedonymous, but I want to send an anonymous message!" )
I'm asking myself whether "anonymous DH" is confusingly named.
Perhaps it should be called psuedonymous DH because it creates
psuedonyms for the life of the session?  Or, we need a name
that describes the creation of psuedonyms, de novo, from
an anonymous starting position?

@_date: 2003-10-06 17:34:30
@_author: Ian Grigg 
@_subject: Simple SSL/TLS - Some Questions 
This is the highest goal of all.  If it is not simple
to use, it misses out on a lot of opportunities.  And
missing out results in less crypto being deployed.
If you have to choose between simple-but-incomplete,
versus complex-but-complete, then choose the former
every time.  Later on, you can always upgrade - or
the programmer using the system can upgrade - to the
full needs if they have shown themselves the need
for the complete solution that you optimised away.
On these lines, I'd suggest something like:
1.  select one cipher suit only, and reject the
rest.  Select the strongest cipher suit, such as
large RSA keys / Rijndael / SHA-256 or somesuch,
so there are no discussions about security.
1.b,  means basically do TLS only.  Don't offer
any fallback.  If someone is using your protocol,
they can select it to talk to their own apps.
If someone has to talk to another app using TLS
or SSL, then they almost certainly have to talk
all suites, so they are more or less forced to
do OpenSSL already.  Hence, almost by definition,
you are forced into the market where poeple don't
want to negotiate cipher suites, they want the
channel product between their own apps up and
running with no fuss.
2.  Notwishtanding 1. above, leave the hooks in
to add another cipher suite.  You should really
only plan on one or two more.  One for embedded
purposes, for example, where you really push the
envelope of security for slower devices.  And
another because someone pays you to do it :-)
3.  Ditch Anon-DH as a separate suite.  Concentrate
on pure certificate comms.  Never deviate more than
briefly from the true flavour of the tools you are
working with.
4.  Ignore all complex certificate operations such
as CA work, etc.  If someone wants that order of
complexity, then they want OpenSSL, which includes
most of those tools.
5.  To meet the dilemma posed by 3, 4, generate
self-signed certificates on the fly.  Then, the
protocol should bootstrap up and get running
easily.  SSH model.  Anyone who wants more, can
replace the certs with alternately named and
signed certs, as created with more specialised
tools.  Or they can help you to write those
Good protocols divide into two parts, the second
part of which starts "trust this key totally."
Ignore the first part for now, being, how you
got the key.
6.  Pick a good X.509 / ASN1 tool.  Don't do
that part yourself.  See all the writings on how
hard this is to do.  If you want to join the
guild of people who've done an ASN1 tool and can
therefore call it "easy", do so, but tell your
family you won't be home for Christmas :-)
7.  Produce a complete working channel security
product before going for first release.  Nothing
slows down progress than a bunch of people trying
to help build something that they can't agree on.
Build it, then ask for help to round it out.
8.  What ever you do ... try and work on the code
that is most beneficial for other reasons.  Don't
plan on completing the project.  In the event
that you don't complete, make sure that what you
did do was worthwhile for other reasons!
9.  Take all expert advice, including the above,
with some skepticism.  You will have much more
intuition because you will be deep in the issues.
Go with it, then.  Being right means you win,
being wrong is an opportunity to learn :-)
Is that the case?  I wide variety of uses for
any protocol are application to same application.
The notion of client-to-server is an alternate,
but it's only an alternate.  It is not a given
that apps builders want to talk to other TLS libs.
TLS is there to be used, as is all other software
and standards.  It is at your option whether you
wish to join the group of people that can express
comms in *standard* TLS, talking heterogeneously.
Sounds like BSD-2 clause or one of the equivalents.
The only question I wasn't quite sure of
was whether, if I take your code, and modify it,
can I distribute a binary only version, and keep
the source changes proprietary?
If so, that's BSD.  If not, you need some sort
of restriction like Mozilla (heading towards GPL).
My own philosophy has always been that crypto has
enough barriers on it already, so it should not
add any more personality quirks than necessary,
hence preference for BSD two clause.  Mind you,
such a statement is a personality quirk, so you
be your own judge.
(Also, bear in mind that you - as owner of your
code - have the choice of issuing it under many
licences.  There is nothing technically wrong
with you issuing it under PD, BSD, Mozilla, GPL.
It might make things confusing for your user
base, but you can do it.  It's your code (as
long as it *was* your code :-)  The corollary to
this is that you don't need to decide up front,
it can change as time goes on, at the cost of
some confusion.)
I guess you disposed of SSL-easy.  J-TLS?
Jill-T might work if you are brave, and like
strong cola :)  "Jilt" has a ring to it.
Names are really hard.  I'd defer that one until
it pops out.
I think I recall the PGP Inc company having
trademarked "Pretty Good".
Q:  Does your employer have any say or comment
on this project?  Might be wise to clear up the
posture, and either get it in writing, or make
the repository public from the git-go.  Many an
open source project has foundered when the boss
discovered that it works...
I don't believe it is possible to multiply-sign
x.509 certs.  This is one of the reasons that
PKIs based on x.509 have a miserable record, as
the absence of any web of trust support and the
promoting of a hierarchical trust model goes
against most business and individual practices.
That said, having chosen TLs, you've chosen
x.509 certs (with some caveats).  This is the
reason why concentrating on self-signed certs
is such a clean idea, it neatly avoids the
issues of trust models, be they broken or not.
OT3H, I suppose you could do TLS-sans certs,
which would make for an interesting project,
not to mention a lot of time spent justifying
your choices :-)
But, what's the point to the question?  I'm
not quite sure how this relates to the essential
question of implementing TLS?

@_date: 2003-10-06 17:55:22
@_author: Ian Grigg 
@_subject: anonymity +- credentials 
Thanks for clearing up the record there - it was
also my understanding that Brands' work was the
current theoretical state of the art!
In terms of actual "practical" systems, ones
that implement to Brands' level don't exist,
as far as I know?  Also, the use of Brands work
would need to consider that he holds a swag of
patents over it all (as also applies to all of
the Chaum concepts).
There is an alternate approach, the E/capabilities
world.  Capabilities probably easily support the
development of psuedonyms and credentials, probably
more easily than any other system.   But, it would
seem that the E development is still a research
project, showing lots of promise, not yet breaking
out into the wider applications space.
A further alternate is what could be called the
hard-coded psuedonym approach as characterised
by SOX.  (That's the protocol that my company
wrote, so normal biases expected.)  This approach
builds psuedonyms from the ground up, which results
in a capabilities model like E, but every separate
use of the capability must be then re-coded in hard
lines by hardened coders.
Which means, for example, that whilst the E crowd
can knock up a new capability over lunchtime, it
takes us about a year of hard work to get a new
capability in place (we've done several - payments,
messaging, trading, projects, ...).  The plus side
is that these capabilities are far more suited to
purpose than something built over a high level
In summary, the state of the art would seem to be
just that, an art in a state.  There is no clear
view as to how this will pan out in the future,
to my mind.

@_date: 2003-10-07 13:57:18
@_author: Ian Grigg 
@_subject: Simple SSL/TLS - Some Questions 
You - or others - are talking about putting your TLS
thingie in an embedded device?  Like a toaster, for
sake of discussion, because we don't want the bad guy
to see us doing white bread in the mornings :-)
Are you envisaging a world where a toaster owners must
be permitted to inspect and rebuild the crypto code in
their toaster so as to be sure that there are no back
And, are you envisaging a world where you could do more
good by forcing this viewpoint on the manufacturers of
toasters, as opposed to a world where a manufacturer of
toasters decides that, regardless of the possible presence
of backdoors, they think it better than the user cannot
see the crypto in the toaster?
If so, then you need to craft some source code availability
clause into the licence.  That would mean more like Mozilla,
and definately not like BSD/MIT and the rest.  (And maybe
like GPL, as suggested by Jerry.)
Also, note that OpenSSL - your erstwhile competitor -
is under Apache licence and that has no such limit,
In practice, what you are suggesting doesn't work.  It
is pretty nigh impractical to take a set of open source,
and a finished deliverable crypto product, and show that
one was used to build the other.  This is because the
compilation process is not really deterministic and
duplicable, across a variety of times & machines &
In essence, a developer uses the open source if he wants
to be sure.  Anyone using a binary only product makes that
As a highly general comment, when we get to something
along the lines of "you must do it like I say" then you
have to apply the God test. Are we that omniscient?  Can
we really support the case that we know how this is best
For every successful god, there are a thousand who found
themselves forgotten and unmartyred.  RMS is one of the
few exceptions;  he crafted a prisoners' dilemma that
stretched broad and created a community of programmers.
I can't think of any similar successes in the field of
cryptography, although there are claims.
So, the question you have to ask yourself is, does that
arrangement he crafted - GPL or something similar - have
sufficient merit that it should be applied to crypto?
My call is "no" as I really don't want any user of my
crypto to actually have to think at all about my own
beliefs.  I want him to use it as fast and as furiously
as possible.  (There are many who disagree with this,
but that is orthogonal to the licensing issue.)  I
admire the game theory behind the GNU licence, but we
should also note the very large number of companies
that won't touch it because of the costs that it brings.
Now, there are a few GNU crypto products out there.
Also, please don't believe that I have much confidence
in the call!  What you might want to do is to check
how other GNU crypto products have faired, it would
be a useful exercise.
It's fairly well established in common law that
your employer owns what you do.  You would need
to (as Jerry says) check with the contract you
have with the employer, and check what the state
law says.  If you have a lawyer friend, ask them.
If you don't want to do that - and I can understand
the drudgery of reading law and contracts when you
should be writing crypto - then just go ahead and
write and publish under some licence.  At least if
you get told to stop, what is published will remain
But, it would be much better if you could get an
email from your boss saying it is ok for you to
work on an open source crypto product in your own
time...  Consider it a challenge.  Even an email
from you to the boss announcing your intentions
will be helpful.
PS: IANAL, University of Grisham.

@_date: 2003-10-07 20:38:34
@_author: Ian Grigg 
@_subject: Simple SSL/TLS - Some Questions 
Well, that is correct, all financial cryptography
protocols will have end-to-end replay, and in this
sense, the anti-reply of TLS is not needed / gets
in the way if one is doing financial stuff.
( I've recently discovered this wierdness in Java where
it automatically launches the entire POST again if
it sees a problem, thus resulting in two transaction
requests.  Of course, the protocols pick it up and
there is no danger, but I can't figure out how to
easily stop the client side telling the user that
the transaction had already been done.... )
You are not being fair, Lynn, you are hijacking
the name of TLS, in order to promote a protocol
to protect credit cards.
What you described was practically nothing to do
with TLS/SSL...
Such a protocol would be quite useful no doubt,
but it has little to do with TLS' design goal of
being a full service channel security product.

@_date: 2003-10-08 01:10:31
@_author: Ian Grigg 
@_subject: credit card threat model 
I totally agree that the business requirements
for protecting credit cards have scant
relationship to the security model of SSL/TLS!
I think the key here is that SSL/TLS is a channel
security protocol.  But, to harken back to its days
of origin, where Netscape asked for "something to
protect credit cards," is going to confuse the
issue for a lot of people.
In preference, if we want something to protect
credit cards, then the threat models should be
established, and the protocol should be created.
Yes, SSL/TLS protects credit cards a little bit
in one part of their flight, but SSL/TLS is much
bigger and grander than that small part.  It's
fair to say, I think, that it's whole security
model plays little attention to credit cards, it's
oriented to creating a good channel over which
any developer/implementor can pass *any* data.
Hence, for example, the emphasis on replay
prevention - which is at a higher layer in a
financial protocol, and was AFAIK in place in
credit cards since whenever.  But if one is
doing a channel security product, it has to
be there, as the overlaying application won't
consider it.
This history of how the business requirements
led to the SSL model are possibly closed to us
at this point...  I wasn't there, and I'm a
bit scared to ask :)
My own view - in conjecture - is that it comes
back to that old chestnut, what's your threat
model.  It would appear that this was one missing
phase in the early development of SSL.  Or, if
it was asked, it certainly wasn't validated, it
was predicted only.
But, in terms of useful posture today, 9 years
down the track, I personally think it is time to
give up the ghost and not ever mention credit
cards again.  Others will & do differ ...
but I don't think it is possible nor helpful to
mix and match the credit card mission and the
SSL result as if they are strongly related.
Yep.  This was obvious in 94.  In fact it was
obvious in 84 - the Internet has always been a
very safe place as far as eavesdroppers go, it
ranks up there with telcos and well above
physical mail as far as reliability and privacy
Yes, of course, eavesdropping is possible, and
of course there have been many incidents.  But,
in terms of the amount of traffic, the risk is
miniscule, and probably well below the credit
card companies' real threshholds.
And, even in the presence of widespread delivery
of credit card numbers in the clear, it's easy
to show that the prime threat is and was and will
always be the hacking into some easy Linux box
and scarfing up the millions from the database.
Why they didn't see that in '94 I don't know.
Absolutely right!  No amount of crypto was
going to help that.
I couldn't agree more.
Well, I don't think you can use the name for
such a drastic change.  Not unless you can get
people to agree, of course, that the name really
relates to the business requirements as much as
it does to the channel security mission.
Certainly, if we could get back to the basics
and identify what TLS is good for and intended
for, a lot of difficulties go away.  But we
can only do that by simplifying and challenging
the assumptions.  Hence, my claim that SSL/TLS
is no longer sensibly related to the credit
card business requirement, notwithstanding that
it is often used and enforced in that mission.
I agree - that's why I suggest self-signed certs
as being the starting point.  I used to think
there was a role for Anon-DH, but TLS has
deprecated it, and in truth, it does make TLS
a lot simpler to think about - it's a cert-
based channel security product, no ifs, no
Then, the whole nature of the CA architecture
should be separated out, and become some sort
of adjunct.  As I mentioned before, good
protocols come in two parts, and the second
part starts with "trust this key 100%."
How you get your signed cert is a completely
separate problem.  It needs to be untangled
out and placed into a separate problem box.
Right.  But, that looks more like a financial
protocol to protect financial messages such
as transactions.  Somewhat different to a TLS-
like channel security protocol.
Here's one test of a good financial protocol -
how well does it work without any encryption at all?

@_date: 2003-10-08 10:16:14
@_author: Ian Grigg 
@_subject: anonymity +- credentials 
CAFE now has a published report on it, so it
might actually be accessible.  I'm not sure
if any of the tech is available.
Is any of this published?  I'd assumed not,
ZKS were another company obscuring their
obvious projects with secrecy.
Reality caught up to them, I heard :)  As
Eric R recently commented, there are no
shortage of encrypted comms projects being
funded and .. collapsing when they discover
that selling secure comms is not a demand-
driven business model.
Back in '98 or so, I got involved with a project
to do bearer stuff.  I even went so far as to
commission a review of all the bearer protocols
(Cavendish, Chaum, Brands, Wagner, Mariott, etc
etc).  Brands came out as the best (please don't
ask me why), so Stefan and I spent many a pleasurable
negotiating session in Dutch bars trying to hammer
out a licence.  Unfortunately we didn't move fast
enough to lock up the terms, and he went off to
bigger and better things - ZKS.
Since then, we toyed around adding tokens to WebFunds.
We started out thinking about Wagner, but what
transpired was that it was just as easy to make
the whole lot available at once.  Now we have a
framework.  (It's an incomplete project, but we
recently picked it up again after a long period
of inactivity, as there is a group that has figured
out how to use it for a cool project.)  The protocol
only covers single phase withdrawals, not two
phase, so far.
Perhaps!  I don't understand the model for credentials,
but if they can all be put into a block-level protocol,
then sharing the code base is a mighty fine idea.
The capabilities guys hang around here:
SOX protocol is described here:

@_date: 2003-10-08 20:18:21
@_author: Ian Grigg 
@_subject: [dgc.chat] EU directive could spark patent war 
My guess is, nix, nada.  Patents are a red herring
in the blinding skirmishes, they became a convenient
excuse and a point to place the flag when rallying
the troops.  The battle was elsewhere, but it was
good to have something to keep the press distracted.
You can see this in, for example, the long available
Wagner variation, and the availability of a bunch of
other variations.  Even when people started doing
demo code of the various alternates (Magic Money,
Ben Laurie's Lucre, etc) there was little to no
amounts of interest.  (There is one guy working
to turn BLL into a system, and then there is our
WebFunds project, originally started from on an
old port of MM back in 1999 or so.  That's it as
far as I know, what is clear is that there is no
inundation of monetary offers for the tech.  I
know a couple of people who put or promised some
money, but it was all pocket change.)
Any one with any business experience realises that
the patents were a huge risk factor, so the obvious
thing was to de-risk it.  Hence, use Wagner first
and shop for another method later (we figured this
out in 2001 after the first coder's Chaum code was
replaced by the second's Wagner efforts...  Or was
it Brands....).
Hence, there are no business analysies being done,
and therefore, no business.
Here we remain within sight of the expiry of the
first of Chaum's patents, and still lukewarm
interest in blinding.  I predict the date will
pass and nothing will change.
The real barriers to token money systems are these:
   1. lack of a viable application
   2. tokens require downloaded clients
   3. bearer is a dirty word
   4. full implementation requires too many
      skills
(not authoritive)
As against approximations (DGCs, Paypals, nymous)
blinded token money systems don't attract enough
real business zing to make them attractive enough
to overcome the barriers.
(I personally am somewhat agnostic on blinding,
to the annoyance of many high priests of the
order.  I think the bank robbery problem is a
bit of a devil, but OTOH, I just spent today
working on getting token withdrawals going
again.  That's because I know of a group that
wants it for a very interesting application
to do vaguely with the 3rd world :-)
Another factor is that Europe has effectively
emasculated the entrepreneurial digital money
field with the E-money directive.  It's been
a while since I read it, but it basically forces
the small guy to be "just like a bank" or to be
so small as to not have a future.  Empirically,
I know two people - entrepreneurs - who've tried
to get into it, then read the directive, and said
"it can't be done" (both from different countries
that actually claim to promote the field).
(The USA, under the quiet guidance of certain
very smart people, went the other way and
deliberately held off from doing or saying
anything.  They realised that they could do
nothing but harm... so they "declined" to get
involved.  Also, in the US, there is very
much more of a spirit of doing something if
it is not explicitly banned.  In Europe, there
is much more of a spirit of getting permission
if it is not explicitly permitted, on the
assumption that the government knows what it
is talking about.)
The only ones who are interested in reducing
transaction costs (in the blinding fashion) are
new outsiders looking to set up new payment
systems.  Hence, the arisal of the digital
gold currencies was centered around the US, and
the smart card efforts of the Europeans were
centered around the national banking structures.
Smart card schemes cost O($100,000,000)
whereas these days a DGC costs O($100,000).
Go figure.

@_date: 2003-10-09 10:13:44
@_author: Ian Grigg 
@_subject: Easy VPNs? 
I'm curious - my understanding of a VPN was that
it set up a network that all applications could
transparently communicate over.
Port forwarding appears not to be that, in
practice each application has to be reconfigured
to talk to the appropriate port, or, each port
has to be forwarded.
Am I missing something here?  If there is an
easy SSH based strategy for VPNs, what is it?

@_date: 2003-10-11 16:22:26
@_author: Ian Grigg 
@_subject: NCipher Takes Hardware Security To Network Level 
(I am guessing you mean, in some sort of objective sense.)
Is there any reason to believe that people who
know nothing about security can actually evaluate
questions about security?
It's often been said that security is an inverted
product.  (I'm scratching to think of the proper
economic term here.)
That is, with security, you can measure easily when
it is letting the good stuff through, but you don't
know when and if and how well it is stopping the bad
stuff *.
The classical answer to "difficult to evaluate"
products is to concentrate on brand, or independant
assessors.  But, brands are based on revenues, not
on the underlying product.  Hence widespread confusion
as to whether Microsoft delivers secure product - the
brand gets in the way of any objective assessment.
And, independant assessors are generally subvertable
by special interests (mostly, the large incumbents
encourage independant assessors to raise barriers
to keep out low cost providers).  Hence, Peter's
points.  This is a very normal economic pattern, in
fact, it is the expected result.
So, right now, I'd say the answer to that question
is that there is no way for someone who knows nothing
about security to objectively evaluate a security
* In contrast, someone who knows little about cars,
can objectively evaluate a car.  They can take it
for a test drive and see if it feels right.  Using
it is proving it.

@_date: 2003-10-11 16:25:45
@_author: Ian Grigg 
@_subject: Easy VPNs? 
Thanks.  That's the key!  Then, the answer
might really be that a good system would
do the transport over UDP it if could, or
it would fall back to a connection in the
worst case.
You know, when placed in that context, the
discussion of whether the transport is done
over SSL, IPSec, or carrier pigeons is a
storm in a teacup.  If someone is concerned,
buy the upgrade that gets you the better

@_date: 2003-10-13 00:28:07
@_author: Ian Grigg 
@_subject: WYTM? 
As many have decried in recent threads, it all
comes down the WYTM - What's Your Threat Model.
It's hard to come up with anything more important
in crypto.  It's the starting point for ... every-
thing.  This seems increasingly evident because we
haven't successfully reverse-engineered the threat
model for the Quantum crypto stuff, for the Linux
VPN game, and for Tom's q&d channel security.
Which results in, at best, a sinking feeling, or
at worst, endless arguments as to whether we are
dealing with yet another a hype cycle, yet another
practically worthless crypto protocol, yet another
newbie leading users on to disaster through belief
in simple, hidden, insecure factors, or...
It's the first question, and I've thought it about
a lot in the context of SSL.  This rant is about
what I've found.  Please excuse the weak cross over!
For $40, you can pick up "SSL & TLS" by Eric
Rescorla [1].  It's is about as close as I could
get to finding serious commentary on the threat
model for SSL [2].
The threat model is in Section 1.2, and the reader
might like to run through that, in the flesh, here:
      perhaps for the benefit of at least one unbiased
reading.  Please, read it.  I typed it in by hand,
and my fingers want to know it was worth it [3].
The rest of this rant is about what the Threat
model says, in totally biased, opinionated terms
[4].  My commentary rails on the left, the book
composes centermost.
      1.2  The Internet Threat Model
      Designers of Internet security protocols
      typically share a more or less common
      threat model.  Eric doesn't say so explicitly, but this is pretty
much the SSL threat model.  Here comes the first
key point:
      First, it's assumed that the actual end
      systems that the protocol is being
      executed on are secure....
(And then some testing of that claim.  To round
this out, let's skip to the next paragraph:)
      ... we assume that the attacker has more or
      less complete control of the communications
      channel between any two machines. Ladies and Gentlemen, there you have it.  The
Internet Threat Model (ITM), in a nutshell, or,
two nutshells, if we are using those earlier two
sentance models.
It's a strong model:  the end nodes are secure and
the middle is not.  It's clean, it's simple, and
we just happen to have a solution for it.
Problem is, it's also wrong.  The end systems
are not secure, and the comms in the middle is
actually remarkably safe.
(Whoa!  Did he say that?)  Yep, I surely did: the
systems are insecure, and, the wire is safe.
Let's quantify that:  Windows.  Is most of the
end systems (and we don't need to belabour that
point).  Are infected with viruses, hacks, macros,
configuration tools, passwords, Norton recovery
tools, my kid sister...
And then there's Linux.  13,000 boxen hacked per
month... [5].  In fact, Linux beats Windows 4 to 1
and it hasn't even challenged the user's desktop
market yet!
It shows in the statistics, it shows in experience;
pretty much all of us have seen a cracked box at
close quarters at one point or another [6].
Windows systems are perverted in their millions by
worms, viruses, and other upgrades to the social
networking infrastructure.  Linux systems aren't
much more trust-inspiring, on the face of it.
Pretty much all of us present in this forum would
feel fairly confident about downloading some sort
of crack disc, walking into a public library and
taking over one of their machines.
Mind you... in that same library, could we walk
in and start listening to each other's comms?
Nope.  Probably not.
On the one hand, we'd have trouble on the cables,
without being spotted by that pesky librarian.
And those darn $100 switches, they so ruin the
party these days.
Admittedly, OTOH, we do have that wonderful 802.11b
stuff and there we can really listen in [7].
But, in practice, we can conclude, nobody much
listens to our traffic.  Really, so close to nobody
that nobody in reality worries about it [8].
But, every sumbitch is trying to hack into our
machine, everyone has a virus scanner, a firewall,
etc etc.  I'm sure we've all shared that wierd
feeling when we install a new firewall that
notifies when your machine is being port scanned?
A new machine can be put on a totally new IP, and
almost immediately, ports are being scanned....
How do they do that so fast?
Hence the point:  the comms is pretty darn safe.
And the node is in trouble.  We might have trouble
measuring it, but we can assert this fact:
    the node is way more insecure than the comms.
That's a good enough assumption for now;  which
takes us back to the so-called "Internet Threat
Model" and by extension and assumption, the SSL
threat model:
    "the actual end systems ... are secure.
     .... the attacker has more or less complete
     control of the communications channel between
     any two machines."
Quite the reverse pertains [5].  So where does that
leave us with SSL?
I am going to assume, for now,  that the Internet
Threat Model (ITM, (R)TM, YATLA) is the SSL threat
model.  And that both are described fairly in the
And, it's wrong.  There are, then, given these
stated assumptions, three questions:
   1.  why was it chosen?
   2.  what effect did it have on the protocol?
   3.  what's the deal with repairing it?
Let's go back to the book and see if we can't work
it out [9].
Here's a designed-in limitation in Part One (the
end systems are secure):
      Protecting against attacks where
      one of the end systems is under
      the control of the attacker is
      extraordinarily difficult, if not
      impossible.
Here's the acceptance of the all-powerful comms
channel attacker:
      Other than that, we assume that the
      attacker has more or less complete
      control of the communications
      channel between any two machines
with no limitations on the threat level of the
Now check this caveat in part two (the comms is
totally open):
      protocol designers don't worry about
      _denial-of-service_ attacks not
      because these attacks aren't
      important but because they're
      extraordinarily difficult to prevent.
What does all this say?  Well, in a nutshell,
we won't protect against the end system attack,
because its really difficult.  And we'll ignore
DOS because that's too difficult too.
But we'll cover the entire on-the-wire threats
... because, as the book goes on to show, we can!
And that's the clanger - the threat model is
about what we can protect.  It is not a
statement of what is needed for the application.
Rather, the whole SSL threat model is a statement,
lifted out of some book from some academic's
library, of what we know, in theory, about how
to create a channel protocol!
Whether the perfect channel protocol is useful or
relevant or applicable was never at issue.  This
means the threat model isn't the threat model it
should be.
A threat model looks at the application - at what
we are trying to protect.  In this case, we know
that the actual threat that SSL was built for was
the sniffer of credit card numbers.  But, he, the
sniffer, is not considered, what's replaced his
role is some theoretical bogey man.  The bogey
man can do anything that we know how to protect
against, and not the things we can't protect
This is pretty damning.  What it means is that,
in essence, the threat model analysis wasn't
carried out.  Properly, at least, or at all,
at the most.
SSL was put together as a "perfect" protocol to
solve a "convenient" threat model from the
(admittedly persuasive and pervasive) knowledge
of the times.  And, it took little or no account
of the needs of the application.
And now, it should be clear to us why SSL looks
so damn odd in the secure browsing application -
because it was, as a result of its unhappy
parentage, an unexpected child that wasn't
created to plan.
That's why, for example, the protocol finishes its
security job close to the borders of the comms.
That's why CA-signed certs were chosen, because
they solved something that could be solved, with
no particular analysis as to whether anyone would
bother to attack that weak link.  That's why, for
example, it's a channel security product, and not
a page (credit card number) protection product.
And, for example, the digsig creates a chain
instead of affirming an intent.
It was only assumed, guessed at, indeed, hoped for
that this protocol was the best way to secure the
credit card in a browsing application.  Here's the
assumption that confirms the failure:
      Designers of Internet security
      protocols typically share a more
      or less common threat model.
It's para three, section 1.2.  And, it is of course,
famously not true [10].
SSH is the most outstanding example of not sharing
that threat model [11].  In fact, it's fair to say
that most Internet security protocols do not share
that threat model, unless they happen to have
followed in SSL's footsteps and also forgotten to
do their threat model analysis.
Which is not to say that the threat model is
inappropriate in the circumstances.  And, there
is still some room to consider that fortune
might have favoured the brave.
But, it is murky enough that we can rip the
pretense aside:  SSL borrowed someone else's
threat model, and it happened to have at least
two highly challengeable assumptions in it,
both of which led to a strong design feature
we are now finding is detrimental.
These two assumptions - node is secure and comms
are insecure - led to the very strong emphasis on
MITM protection, which is the root cause of the
failure of availability of secure browsing to the
Internet public [12].
What do we do about it?  Firstly, we should
recognise that the threat model is wrong.  Broken,
in the crypto parlance.  No, not just broken, but
irrelevant.  We know that the active attack is
not a serious threat, and is in any event way
less important than the threat to the machine
Secondly, and thusly, this clears the way to de-
emphasise protection against active attacks.  We
can in most cases safely propose opportunistic
cryptography from self-signed certs, cached or
otherwise, from other methods of fingerprint
distribution, or even from anonymous Diffie-Hellman.
For example, for starters.
That doesn't mean ripping out the CA-signed certs,
but just making them honestly optional.  Any server
that wishes to use them can and should.
But servers and browsers that have no need, shouldn't
need to.
Thirdly, it remains that secure browsing, isn't [13].
We need to recognise that the pervasive myth that
SSL secures the browing process is holding back a
rethink on how to do it better.
And that has to come from the crypto community;
that's where the myth was created and that's where
the debunking has to come from.
Or, at least, it's better if the crypto community
repairs its own myths, rather than the Internet
community debunking it, and the credibility of the
crypto community along with it.
[1] is the link you want :-)  Rescorla's book is becoming
the must-have guide for SSL & TLS, a point I rely upon
overly much.
[2] The spec for TLS doesn't really mention threats.
The scattering of papers on the topic seem to gloss
over it as well.  So the book is both well needed
and somewhat belated, this late in the SSL cycle.
[3] It's also on the amazon link above.  I wish I'd
[4] Oh, yea of little faith!
[5] "During August, 67 per cent of all
    successful and verifiable digital
    attacks against on-line servers
    targeted Linux, followed by Microsoft
    Windows at 23.2 per cent. A total of
    12,892 Linux on-line servers running
    e-business and information sites were
    successfully breached in that month,
    followed by 4,626 Windows servers."
[6] I like BSD more and more.
[7] Note to self.  Must download a WEP crack kit!
I'm sure someone around here has a WEP network to
break into ...
Quick reality check:  yes, WEP is broken, but
no, it isn't useless:  who is going to go to the
library and crack the crypto?  Not me.  Fact of
the matter here is that for the vast majority
of uses, WEP may very well be "good enough" ...
not great or a protocol to be proud of, but it's
good enough for ordinary net use.
[8] Yeah, I know.  "At a conference, I saw..."
No, this rant is not about *us* people, it's about
security for *everyone*.  That includes, especially,
everyman aggressive attacker, however he does it.
[9] I'd love to hear the inside scoop, but all I
have is Eric's book.  Oh, and for the record,
Eric wasn't anywhere near this game when it was
all being cast out in concrete.  He's just the
historian on this one.  Or, that's the way I
understand it.
[10] There are others - PGP, Kerberos, Paypal,
and my own company's SOX spring to mind.
[11] In a recent presentation to Usenix, the author
admits that the "Internet Threat Model" is "not
really true."
 slide 5.
[12] argues that only 1% of servers make SSL available
to their customers.
[13]  Recall
here, onslaught of spoofing, etc.  Also, serious
high-level business types might like to look at this:
That is **mainstream** writings on how insecure
the browsing scenario is.  The point is - it's
about to become a major hot potatoe.  At some point
the mud will sling.

@_date: 2003-10-13 12:37:18
@_author: Ian Grigg 
@_subject: WYTM? 
Minor errata:
I found this link had moved to here;

@_date: 2003-10-13 18:49:30
@_author: Ian Grigg 
@_subject: WYTM? 
thanks for your reply!
My point is strictly limited to something
approximating "there was no threat model
for SSL / secure browsing."  And, as you
say, you don't really disagree with that
100% :-)
With that in mind, I think we agree on this:
Well, that's the sort of confirmation I'm looking
for.  From the documents and everything, it seems
as though the threat model wasn't analysed, it was
just picked out of a book somewhere.  Or, as you
say, even that is too kind, they simply didn't
think that way.
But, this is a very important point.  It means that
when we talk about secure browsing, it is wrong to
defend it on the basis of the threat model.  There
was no threat model.  What we have is an accident
of the past.
Which is great.  This means there is no real objection
to building a real threat model.  One more appropriate
to the times, the people, the applications, the needs.
And the today-threats.  Not the bogeyman threats.
Exactly.  Why do I care?  Why do you care?
It is mantra in the SSL community and in the
browsing world that we do care.  That's why
the software is arranged in a a double lock-
in, between the server and the browser, to
force use of a CA cert.
So, if we don't care, why do we care?  What
is the reason for doing this?  Why are we
paying to use free software?  What paycheck
does Ben draw from all our money being spent
on this "i don't care" thing called a cert?
Some people say "because of the threat model."
And that's what this thread is about:  we
agree that there is no threat model, in any
proper sense.  So this is a null and void
Other people say "to protect against MITM.
But, as we've discussed at length, there is
little or no real or measurable threat of MITM.
Yet others say "to be sure we are talking
to the merchant."  Sorry, that's not a good
answer either because in my email box today
there are about 10 different attacks on the
secure sites that I care about.  And mostly,
they don't care about ... certs.  But they
care enough to keep doing it.  Why is that?
Someone made a judgement call, 9 or so years
ago, and we're still paying for that person
caring on our behalf, erroneously.
Let's not care anymore.  Let's stop paying.
I don't care who it was, even.  I just want
to stop paying for his person, caring for me.
Let's start making our own security choices?
Let crypto run free!

@_date: 2003-10-13 20:24:35
@_author: Ian Grigg 
@_subject: WYTM? 
I'm sorry, but, yes, I do find great difficulty
in not dismissing it.  Indeed being other than
dismissive about it!
Cryptography is a special product, it may
appear to be working, but that isn't really
good enough.  Coincidence would lead us to
believe that clear text or ROT13 were good
enough, in the absence of any attackers.
For this reason, we have a process.  If the
process is not followed, then coincidence
doesn't help to save our bacon.
It has to follow, for it to be valuable.  If
it doesn't follow, to treat it as anything
other than a mere coincidence to be dismissed
out of hand is leading us on to make other
I think that Matt Blaze said it fairly well.
There are some security practices that in
the recent past are now considered appalling.
It's time to be a little bit appalled, and
to recognise SSL for what it is - a job that
survived not on its cryptographic merits, but
through market and structural conditions at
the time.
(No, it's a double-lock-in, or maybe more.  It's
a complex interrelated scenario.)
Here's specifically what the server does:  When
it is installed, it doesn't also install and
start up the SSL server.  You know that page
that has the feather on?  It should also start
up on the SSL side as well, perhaps with a
different colour.
Specifically, when you install the server, it
should create a self-signed certificate and use
it.  Straight away.  No questions asked.
Then, it becomes an administrator issue to
replace that with a custom signed one, if the
admin guy cares.
Right.  I'm looking to improve those numbers,
my guess would be 10-fold is not unreasonable.
There should be no dialogue at all.  Going from
HTTP to HTTPS/self signed is a mammoth increase
in security.  Why does the browser say it is
less/not secure?
Further, the popups are a bad way to tell the
user what the security level is.  The user can't
grok them and easily mucks up on any complex
qeustions.  There needs to be a security display
on the secured area that is more prominent and
also more graded (caching numbers) than the
current binary lock symbol.
There has been some research on this area, I
think it was Sean Smith (Dartmouth College)
that posted on this subject.  Yes, here it is:
  From: sws at cs.dartmouth.edu (Sean Smith)
  > Or, if we should bother to secure it, shouldn't
  > we mandate the security model as applying to the
  > browser as well?
  Exactly.
  That was the whole point of our Usenix paper last year
  E. Ye, S.W. Smith.
  ``Trusted Paths for Browsers.''
  11th Usenix Security Symposium. August 2002
  Oh, and:
  Advertisement: we also built this into Mozilla, for Linux and Windows.
  (Actually, I'm not sure what SSH pops up, it's
never popped up anything to me?  Are you talking
about a windows version?)

@_date: 2003-10-13 22:07:39
@_author: Ian Grigg 
@_subject: WYTM? 
(If you mean that the ITM is consenus, I grant
you that two less successful protocols follow
it - S/MIME and IPSec (partly) but I don't
think that makes it consensus.  I know there
are a lot of people who don't think in any other
terms than this model, and that is the issue!
There are also a lot of people who think in
terms completely opposed to ITM.
So to say that ITM is consensus is something
that is going to have to be established.
If that's not what you mean, can you please
I'm not sure I ever said that the protocol
doesn't match the threat model - did I?  What
I should have said and hoped to say was that
the protocol doesn't match the application.
I don't think I said "automatically," either.
I did hold out hope in that rant of mine that
the designers could have accidentally got it
right.  But, they didn't.
Now, SSL, by itself, within the bounds of the
ITM is actually probably pretty good.  By all
reports, if you want ITM, then SSL is your
best choice.
But, we have to be very careful to understand
that any protocol has a given set of characteristics,
and its applicability to an application is an
uncertain thing;  hence the process of the threat
model and the security model.  In SSL's case, one
needs to say "use SSL, but only if your threat
model is close to ITM."  Or similar.  Hence the
title of this rant.
The error of the past has been that too many
people have said something like "Use SSL, because
we already got it right."  Which, unfortunately,
skips the whole issue of what threat model one
is dealing with.  Just like happened with secure
In this case, the ITM was a) agreed upon after
the fact to fill in the hole, and b) not the right
one for the application.
My interpretation - which you won't like - is that
it is telling me that this certificate is bad, and
asking whether me if I am sure I want to do this.
A popup is symonymous with bad news.  It shouldn't be
used for good news.  As a general theme, that is,
although this is the reason I cited that paper:  others
have done work on this and they are a long way ahead
in their thinking, far beyond me.
I'm not sure I can make much of your point,
as I've never heard of nor seen a Firebird?

@_date: 2003-10-13 22:27:45
@_author: Ian Grigg 
@_subject: WYTM? 
The point is, any compromise of any system is more
likely to come from a node compromise than a wire
How much more likely?  We don't know for sure, but
I'd say it is in the many thousand times as much.  E.g.,
look at those statistics.  Basically, the wire threat
is unmeasurable - there are no stats that I've ever
seen, and the node compromise is subject of some
great scrutiny, not to mention 13,000 odd Linux
reinstalls every month.
Does it mean that we should ignore the wire threat?
No, but it does mean that we are foolish to let any
protection of the wire threat cause us any grief.
Protecting against any wire attack is fun, but no
more than that - if it costs us a dime, it needs to
be justified, and that is really hard given that we
are thousands of times more likely to see a compromise
on the node.
If we spend 10c protecting against the wire attack,
should we then spend $1,300 spending against the
node attack?
The situation is so ludicrously unbalanced, that if
one really wanted to be serious about this issue,
instead of dismissing certs out of hand (which would
be the engineering approach c.f., SSH), one would
run ADH across the net and wait to see what happened.
Or, spit credit cards in open HTTP, and check how
many were tried by credit card snafflers.  You might
be waiting a long time :-)  But, that would be a
serious way for credit card companies to measure
whether they care one iota about certs or even
crypto at all.
If the threat model is valid for individuals who
happen to understand what all this means, then  by all means they should use the resultant
security model.  I don't think that anyone is
saying that people can't use SSL in its current
recommended form.  JUst that more people would
use SSL if software didn't push them in the
direction of using overly fraught security
(See my first reply to Erik, where I quoted two
sections, earlier today.)
We protect against things which are cost-effective
to protect against.  That is, we use risk analysis
to work out the costs v. the benefits.
We know how to protect against an awful lot.  We
simply don't, unless the cost is less than the
benefit, in general.
And, this is the point:  SSL protected against
the MITM because it could.  Not because it was
present as a threat, and not because it was cost-
effective.  It was infamously and deplorably
weak security logic;  what it should do is
protect against things that are a threat, and
for a cost that matches the threat.
It's about relative risks - I'm not saying that
SSL should protect the node.  What I'm saying is
that it is ludicrous to worry overly much the
risk that SSL deals with - the ITM, supposedly -
in most practical environments, because that's
not where the trouble lies.
Another Analogy:  Soldiers don't carry umbrellas
into battle.   But it does rain!
The reasoning is simple - unless the umbrella
is *free* it's ludicrous to worry about water
when someone is shooting bullets at you.
We do a risk-analysis on the umbrella, and we
discover that it has a cost of making us too
conspicuous.  Cost exceed benefit, we ditch    the umbrella.

@_date: 2003-10-15 15:59:02
@_author: Ian Grigg 
@_subject: WYTM? 
Yes.  This is the attack that is going on.  This
is today's threat.  (In that it is a new threat.
The old threat still exists - hack the node.)
Nope.  It would seem that only the more sophisticated
users can be relied upon to correctly check that they
are at the correct secure site.  In practice almost
all of these attacks bypass any cert altogether and
do not use an SSL protected HTTPS site.
They use a variety of techniques to distract the
attention of the user, some highly imaginative.
For example, if you target the right browser, then it
is possible to popup a box that covers the appropriate
parts.  Or to put a display inside the window that
duplicates the browser display.  Or the URL is one
of those with strange features in there or funny
letters that look like something else.
In practice, these attacks are all statistical,
they look close enough, and the fool some of the
people some of the time.
Finally, just in the last month, they have also
started doing actual cert spoofs.  This was quite
exciting to me to see a spoof site using a cert,
so I went in and followed it.  Hey presto, it
showed me the cert, as it said it was wrong!  So
I clicked on the links and tried to see what was
Here's the interesting thing:  I couldn't easily
tell, and my first diagnosis was wrong.  So then
I realised that *even* if the spoof is using a
cert, the victim falls to a confusion attack (see
Tom Weinstein's comments on bad GUIs).
(But, for the most part, 95% or so ignore the cert,
and the user may or may not notice.)
Now, we have no statistics on how many of these
attacks work, other than the following:  they keep
happening, and with increasing frequency over time.
justify the cost of the attack at least.
I guess the best thing to say is that the raw
claim that the cert ensures that you are talking
to the merchant is not 100% true.  It will help
a sophisticated user.  An attack will bypass some
of the users a lot.  It might fool many of the
users only occasionally.
SSL isn't flawed, per se.  It's just not appropriately
being used in the secure browser application.  It's
fair to say that its use is misaligned to requirements,
and a lot of things could be done to improve matters.
But, one of the perceptions that exist in the browser
world is that SSL secures ecommerce.  Until that view
is rectified, we can't really build the consensus to
have efforts like Ye & Smith, and Close, and others,
be treated as serious and desirable.
(In practice, I don't think it matters how Verisign
and others check the cert.  This is shown by the
fact that almost all of these attacks have bypassed
the cert altogether.)

@_date: 2003-10-22 16:33:23
@_author: Ian Grigg 
@_subject: SSL, client certs, and MITM (was WYTM?) 
The frequency of MITM attacks is very low, in the sense
that there are few or no reported occurrences.  This
makes it a challenge to respond to in any measured way.
Nobody doubts that it can occur, and that it *can*
occur in practice.  It is whether it *does* occur
that is where the problem lies.
The question is one of costs and benefits - how much
should we spend to defend against this attack?  How
much do we save if we do defend?
[ Mind you, the issues that are raised by the paper
are to do with MITM attacks, when SSL/TLS is employed
in an anti-MITM role.  (I only skimmed it briefly I
could be wrong.)  We in the SSL/TLS/secure browsing
debate have always assumed that SSL/TLS when fully
employed covers that attack - although it's not the
first time I've seen evidence that the assumption
is unwarranted. ]
I think the failure of client certs has the same
root cause as the failure of SSL/TLS to branch
beyond its "mandated" role of "protecting e-
commerce."  Literally, the requirement that
the cert be supplied (signed) by a third party
killed it dead.  If there had been a button on
every browser that said "generate self-signed
client cert now" then the whole world would be
using them.
Mind you, the whole client cert thing was a bit
of an afterthought, wasn't it?  The orientation
that it was at server discretion also didn't help.
People often say that there are no successful MITM
attacks because of the presence of SSL/TLS !
The existance of the bugs in Microsoft browsers
puts the lie to this - literally, nobody has bothered
with MITM attacks, simply because they are way way
down on the average crook's list of sensible things
to do.
Hence, that rant was in part intended to separate
out 1994's view of threat models to today's view
of threat models.  MITM is simply not anywhere in
sight - but a whole heap of other stuff is!
So, why bother with something that isn't a threat?
Why can't we spend more time on something that *is*
a threat, one that occurs daily, even hourly, some
Because it's not necessary.  Nobody loses anything
much over the wire, that we know of.  There are
isolated cases of MITMs in other areas, and in
hacker conferences for example.  But, if 10 bit
crypto and ADH was used all the time, it would
still be the least of all risks.

@_date: 2003-10-22 19:38:03
@_author: Ian Grigg 
@_subject: SSL, client certs, and MITM (was WYTM?) 
In threat analysis, you base your assessment on
economics of what is reasonable to protect.  It
is perfectly valid to decline to protect against
a possible threat, if the cost thereof is too high,
as compared against the benefits.
This is the reason that we cannot simply accept
"the possible" as a basis for engineering of any
form, let alone cryptography.  And this is the
reason why, if we can't measure it, then we are
probably justified in assuming it's not a threat
we need to worry about.
(Of course, anecdotal evidence helps in that
respect, hence there is a lot of discussion
about MITMs in other forums.)
Here's Eric Rescorla's words on this:
The first thing that we need to do is define our threat model.
A threat model describes resources we expect the attacker to
have available and what attacks the attacker can be expected
to mount.  Nearly every security system is vulnerable to some
threat or another.  To see this, imagine that you keep your
papers in a completely unbreakable safe.  That's all well and
good, but if someone has planted a video camera in your office
they can see your confidential information whenever you take it
out to use it, so the safe hasn't bought you that much.
Therefore, when we define a threat model, we're concerned
not only with defining what attacks we are going to worry
about but also those we're not going to worry about.
Failure to take this important step typically leads to
complete deadlock as designers try to figure out how to
counter every possible threat.  What's important is to
figure out which threats are realistic and which ones we
can hope to counter with the tools available.

@_date: 2003-10-22 20:30:36
@_author: Ian Grigg 
@_subject: SSL, client certs, and MITM (was WYTM?) 
Not true!  The cost is from 10 million dollars to
100 million dollars per annum.  Those certs cost
money, Perry!  All that sysadmin time costs money,
too!  And all that managerial time trying to figure
out why the servers don't just "work".  All those
consultants that come in and look after all those
secure servers and secure key storage and all that.
In fact, it costs so much money that nobody bothers
to do it *unless* they are forced to do it by people
telling them that they are being irresponsibly
vulnerable to the MITM!  Whatever that means.
Literally, nobody - 1% of everyone - runs an SSL
server, and even only a quarter of those do it
"properly."  Which should be indisputable evidence
that there is huge resistance to spending money
on MITM.
I'm not sure how you come to that conclusion.  Simply
use TLS with self-signed certs.  Save the cost of the
cert, and save the cost of the re-evaluation.
If we could do that on a widespread basis, then it
would be worth going to the next step, which is caching
the self-signed certs, and we'd get our MITM protection
back!  Albeit with a bootstrap weakness, but at real
zero cost.
Any merchant who wants more, well, there *will* be
ten offers in his mailbox to upgrade the self-signed
cert to a better one.  Vendors of certs may not be
the smartest cookies in the jar, but they aren't so
dumb that they'll miss the financial benefit of self-
signed certs once it's been explained to them.
(If you mean, use TLS without certs - yes, I agree,
that's a no-won.)
This is a well known metric.  Half is a good rule of
thumb.  People will happily spend X to protect themselves
from X/2.  Not all the people all the time, but it's
enough to make a business model out of.  So if you
were able to show that certs protected us from 5-50
million dollars of damage every year, then you'd be
(Mind you, where you would be is, proposing that certs
would be good to make available.  Not compulsory for
So I should spend the money.  Sure.  My choice.
Can you take that to the specific case?

@_date: 2003-10-22 21:34:56
@_author: Ian Grigg 
@_subject: SSL, client certs, and MITM (was WYTM?) 
I agree with this.  Especially, I think we are
all coming to the view that TLS/SSL is in fact
a general purpose channel security protocol,
and should not be viewed as being designed to
protect credit cards or e-commerce especially.
Given this, it is unreasonable to talk about
threat models at all, when discussing just the
protocol.  I'm coming to the view that protocols
don't have threat models, they only have
characteristics.  They meet requirements, and
they get deployed according to the demands of
higher layers.
Applications have threat models, and in this is
seen the mistake that was made with the ITM.
Each application has to develop its own threat
model, and from there, its security model.
Once so developed, a set of requirements can
be passed on to the protocol.  Does SSL/TLS
meet the requirements passed on from on high?
That of course depends on the application and
what requirements are set.
So, yes, it is not really fair for a protocol
designer to have to undertake an economic
analysis, as much as they don't get involved
in threat models and security models.  It's
up to the application team to do that.
Where we get into trouble a lot in the crypto
world is that crypto has an exaggerated
importance, an almost magical property of
appearing to make everything safe.  Designers
expect a lot from cryptographers for these
reasons.  Too much, really.  Managers demand
some special sprinkling of crypto fairy dust
because it seems to make the brochure look
This will always be a problem.  Which is why
it's important for the crypto guy to ask the
question - what's *your* threat model?  Stick
to his scientific guys, as it were.
Right.  It is however the case that secure
browsing is facing a bit of a crisis in
security.  So, there may have to be some
changes, one way or another.

@_date: 2003-09-01 12:23:28
@_author: Ian Grigg 
@_subject: invoicing with PKI 
(Things seem quiet on the crypto front, here's a late reply.)
The dream of PKI seems to revolve around these major areas:
  1.  invoicing, contracting - no known instances
  2.  authentication and authorisation - SSL client
      side certs deployed within organisations.
  3.  payments
  4.  channel security (SSL)
  5.  email (OpenPGP, S/MIME)
In terms of actual deployed PKIs, the only significant
cases that I know of, deployed outside of organisations
and in widespread use are:
   HTTPS (141k, see below), and
   OpenPGP ("millions" says PGP Inc, so let's call it 100k or so).
I suspect the widest use of public key crypto in a
non-PKI context would be SSH, which opportunistically
generates keys rather than invite the user to fund
a PKI.  According to this page [1], there may or may
not be 2,400k SSH servers, but it's unclear whether
that is the sample size or the sites found.
(Right, tools, not applications.)
There are specific things like  and
 (costs money for what securityspace gives
for free).  Of these, start at [2].
(Which shows the penetration of SSL in websites has risen
from about 1% to 1.2% since the beginning of the year.
Although, there are now new figures on there that show
that only 31% of the 141k found are "valid" / self-signed
In terms of other uses of PKI, outside HTTPS, I don't
know any regular surveys.  I imagine it would be too
depressing to conduct more than once :)
Is SET still alive?  Available?  The crypto-based payments
field appears to be quiet at the moment (e.g., payments
that are not done over HTTPS).
About the only thing that I know of (other than own stuff)
is peppercoin which seems to be a DRM micropayments play.
Poking around on the website, it appears to be a crypto
download microtoken billing method, that is aggregated
onto credit cards or bank accounts [3].  IOW, a grab bag
of payments techniques that appears blithely ignorant of
the last decade in digital payments.
[1] [2] [3]

@_date: 2003-09-01 15:59:09
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
It depends on what the context is.  If we are talking
about "military security" then commsec is of some use, as
things like tactical security don't really "need" the
benefit of hard crypto, it's just a nice-to-have.  E.g.,
the presence of a radio signal is generally most of the
short term importance of tactical comms.  The rest of
it tends to be chit-chat which is hard to analyse in
real time anyway...  In this sense, commsec means radio
silence more than anything else.
If we are talking about "government security" then it
is of high use, because pretty much everything about
government is about talking and documents, and the time
aspect of tactical comms is not present.
Within the academic notion of infosec & commsec, it
would be fair to say that it's the most important, but
that's by absence, really.  There's isn't much else to
study if one is confined to academic research into the
security of data!
If we are talking about Internet security, then by far
the biggest problems are viruses, hacked hosts, identity
theft and DOS.
Snooping is next to non-existant but has a reputation
for being rampant.  Active attacks on comms - MITM, etc
- are basically a theoretical issue only, but are seen
by many theoreticians as "must-protects".
This discord is seen by the fact that a real snooping
event or, heaven forbid, an active MITM, is a newsworthy
event, whereas the real threats - hacked credit card
databases - are somewhere between boring and embarressing.
(I'm waiting with interest to see if there is much report
of WEP kits being used out in the world for aggressive
So, part of the problem is that cryptography people have
been concentrating on the wrong things (wrong threat model)
for so long that they have earnt a reputation of being
"mostly harmless."
That's a scary talk!  I see a lot of familiar
stuff, but it seems that whilst Eric courts the
dark side of real security, he holds back from
really letting go and getting stuck into SSL.
For example, he states that 28% of wireless
networks use WEP, and 1% of web servers use SSL,
but doesn't explain why SSL is a "success" and
WEP is a "failure" :-)
On the plus side, he balances the conventional
(SSL is the model) with the new view (SSH is the
model) quite well.  It's good news that the SSH
model is starting to receive some respect.  The
analysis of threat model failure is good.
One thing he doesn't stress is design by committee
v. design by small focused team.  Much of SSL and
SSH's strengths are that they were designed and
deployed quickly and cheaply (and insecurely!) so
as to tap into real needs real quickly.  I would
suggest that any security protocol designed by a
committee has a low survivability rating.
( Hmm, I wonder who designed WEP?  :-)
Right.  But, doing TLS over SMTP relays seems a
complete waste of time.  Basically doing node-to-
node encryption for an end-to-end protocol isn't
attractive, neither at the protocol level nor at
the administrator level.  [Ref: Eric's book.]
Yep.  It's just not fun to admit that being hidden
in the crowd is a valid form of security.  Or,
controlling the guest list is solves most of the
trouble at parties.
A large part of the problem, IMHO, is that cryptography
in the popular domain is treated as a discipline of science
and not of engineering.  This is mostly prevalent on the
Internet, where there is a sense of self-taught, non-
commercial application of cryptography.  My time in (or
close to) a telco taught me the difference, as there,
they have an engineering focus on cryptography, and really
understand what it means to calculate the cost of the
For them, leaving a weakness was just another risk
calculation, whereas so much stuff that happens on the
net starts from "we must protect against everything"
and then proceeds to design the set of "everything"
for ones convenience.

@_date: 2003-09-03 05:04:26
@_author: Ian Grigg 
@_subject: invoicing with PKI 
I read them twice the other say.  Recordings
would be nice.
:-) Was this an April 1st RFC?  Or a stealth DRM
It's worth looking at these figures from 1st Sep 2003:
      Description              Count
      Valid                    35709
      Self Signed              9769
      Unknown Signer           27507
      Cert-Host Mismatch       40276
      Expired                  54578
I used the total in my calculation to get 1.24% server
penetration, but the true story is way worse - only a
quarter are supposed PKI-valid.  The rest are deviant
in some form.
I've thought about this a lot, and I've come to the
conclusion that trying to bootstrap using ADH is not
worth the effort.  I think the best thing is if the
web servers were to automatically generate self-signed
certs on install, and present them by default.
Then, at least, we offer the opportunity for browsers
to do SSH-style time-trust analysis.
The forces of crypto-conservatism are so strong that
I suspect we only get one shot at saving the HTTPS
protocol.  Trying to get browsers and servers to agree
to like ADH seems too much a challenge.  Using self-
signed certs seems to promise more bang for buck.
For new applications, using ADH is definately a good
way to go.

@_date: 2003-09-03 05:24:36
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
A good point - commenting on slides is fraught
with danger, as others have pointed out.  Is
there to be a paper?
[further WEP comments at end...]
One thing that has been on my mind lately is how
to define success of a crypto protocol.  I.e.,
how to take your thoughts, and my thoughts, which
differ, and bring the two together.
There appear to be a number of metrics that have
been suggested:
   a.  nunber of design "wins"
   b.  penetration into equivalent unprotected
       market
   c.  number of actual attacks defeated
   d.  subjective good at the application level
   e.  worthless measures such as deployed copies,
       amount of traffic protected
All of these have their weaknesses, of course.
It may be that a composite measure is required
to define success.  I'm sure there are more
a. The only thing that seems to be clearly a win
for SSL is the number of design wins - quite
high.  That is, it would appear that when someone
is doing a new channel security application, the
starting point is to consider SSL.
b. we seem to be agreeing on 1% penetration of
the market, at least by server measurement (see
my other post where I upped that to 1.24% in the
most recent figures).
That's not going to support any notion of "big
success."  (Note here Michael Shields' comments
on HTTPS (in)convenience.)
c.  number of attacks defeated.  When correctly
deployed, SSL seems to defeat all known active and
passive attacks.
Problem is, at least for HTTPS, there are practically
no active attacks.  And passive attacks are not
very common.  We know this because credit cards get
stolen in their millions.  But in all cases, known,
they get stolen by hacking.  I still believe there
has never been a case of a credit card number being
eavesdropped off an open transmission (and delivering
CCs over open forms & email does go on!).
So, it's not clear that SSL achieves much with c.
either - for HTTPS.  For other applications, one
would need to look at those specific cases.
d.  subjective good.  For HTTPS, again, it's a
decidedly mixed score card.  When I go shopping
at Amazon, it makes little difference to me, because
the loss of info doesn't effect me as much as it
might - $50 limit on liability.  Same with the
merchant - what he's worried about is identity,
but SSL's only contribution to identity - client
certs - is a failure.  More on this another day,
the basic issue is that the threat model is wrong,
I think.
In sum, I think it highly arguable that SSL is a
huge success.  Highly arguable, and in terms of
any positive objective measure as to where one can
show SSL's success, I'm interested to see what that
is, from both the particular SSL case, and the
general crypto case.
Plenty of room for future discussion then :-)
(I sense your pain though - I see from the SHTTP
experiences, you've been through the mill.  And
written the book!  I also share the pain, as all
this fine work by many people has delivered what
amounts to very little.  Our comms, our browsing,
our net, remains fundamentally unprotected.)
PS:  in the interests of brevity, I stuck my
reponse on the WEP issue here, where it can be
skipped more readily...
I admit I was thinking it was an active attack,
but in fact a passive attack is sufficient.  This
makes the attack by far easier.
   "AirSnort requires approximately 5-10 million
   encrypted packets to be gathered. Once enough
   packets have been gathered, AirSnort can guess
   the encryption password in under a second."
   On a busy network, maybe many minutes of listening.
On a mostly idle network, many hours.  It seems
to reduce WEP to a complicated method of access
I'm almost convinced that WEP is a failure, but
I think it retains some residual value.
What matters is how much this slows down the
attackers.  Not the theoretical one (with his
copy of WepCrack or AirSnort) but the real
people out in the street.  For a start, just the
mere knowledge that you have to crack something
will reduce the potential for outsiders entering
into your network by about 100 fold.
Coz most people are honest.  What this achieves
is the equivalent of yellow police tape across
a prohibited area - more access control than
security, perhaps.
But, the interesting thing is that with WEP
penetration at 28%, then there should be some
experience out there that shows whether it is
good or useless.

@_date: 2003-09-05 19:14:38
@_author: Ian Grigg 
@_subject: SSL's threat model 
Does anyone have any pointers to the SSL threat model?
I have Eric Rescorla's book and slides talking about the
Internet threat model.
The TLS RFC ( says
nothing about threat models that I found.

@_date: 2003-09-07 03:01:30
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
We do have the problem that SSL can be considered
to be a channel security product, or it can be
considered to be a credit card protection product.
Part of the problem lies in the switching of arguments
from one purpose to another.  If one criticises the
credit card aspects, the answer is often that SSL is
a channel security product.  And v.v.  The result is
slippery, and it can only be done so many times before
one gains the impression that SSL is snake oil and
the real needs are elsewhere.
We need to decide.  If SSL is aimed at credit cards,
and HTTPS is only present for credit cards, then why
is it that OpenSSL spends so much time on it?  What's
their incentive?  I'm not sure I understand why Eric
and Tim and Ben and whoever does it these days spend
huge amounts of unpaid time developing code so that
other people could protect ... the credit card issuers'
And, isn't it about time we designed a new product
to protect against the threats that users face
today?  One that could be used by the rest of us?
I think it's fair to say that SSL was designed
because of credit cards.  But now, SSL/TLS is for
the purpose of a channel security requirement.  The
credit card thing is a forgotten issue, SSL can be
used to protect credit cards, but the protection
of credit cards no longer has anything to do with
the way SSL/TLS is designed, tested, measured or
To rest any current requirements on credit cards
means that SSL should be oriented towards the
threat to credit cards, and it plainly isn't.
That's easy to see, in that if SSL was oriented
to credit cards, why did they do SET?  (And,
SHTTP seems much closer to that mission, on a
quick reading, at least.)
Having said all that, maybe we need to add to the
list of "market success measures" a part on success
at original target, and applicability for other
(OK.  My guess there would be well above 50%, probably
around 90%, by servers, of those taking credit cards.
I spent a little time googling, and found about 10%
of credit card sites had open forms.  I've also seen
a fair amount of anecdotal evidence that suggests
that any merchant who's less than 100k of revenue
probably won't be worrying too much about secured
delivery of credit cards.  Probably in terms of
number of transactions and total value dealt with,
the coverage is right up there above 99%...)
Yup!  It is an extraordinarily interesting thing
from an economics pov.  It's a piece of market
interventionism that pleases few, except the
marketing folks that admire its beauty, or the
economists who admire its subsidy.
Right.  That might be a good argument if the issuers
of credit cards practiced good hygiene.  But, in
practice, they don't.  What they practice is risk
management.  That is, they figure out what the fraud
rate is, and charge percentages and penalties according
to the sector.
The issuers - the people that the browser and server
people are working so hard to protect - couldn't give
a flying f**k if the user has breached hygiene.  What
they are concerned about is that the costs are covered
by fees.  And, perversely, a little known finance
secret, the system works best if the fees are stabilised
at a high level.  See above, $50.
Reputedly, chargeback rates and fees in the fringe
industries - adult for example - can reach 50%.  But,
instead of denying those uses of the card - hygiene -
issuers have encouraged it (...until recently.  There is
now a movement, over the last year, to withdraw service
from the fringe industries, but, it is because of
additional risks being added, not the risks of fraud
or user loss.  Visa is doing it, Mastercard is "waiting
and seeing.")
It's all well and good that users are encouraged to
practice hygiene.  But, users should practice risk
management first.  Hygiene second.  In this case, the
merchant - a user - should be allowed to calculate his
risks.  With HTTPS, he is denied that opportunity.
(We all know where this is heading ...:-)
The other thing to be aware of is that ecommerce itself
is being stinted badly by the server and browser limits.
There's little doubt that because servers and browsers
made poorly contrived decisions on certificates, they
increased the overall risks to the net by reducing the
deployment, and probably reduced the revenue flow for
certificate providers by a factor of 2-5.
As if anyone cares about that ;)
Right.  They had this fallacious threat model.  And
what solved it - in their minds - was the application
of some crypto.  SSL as a placebo.  I grant, that may
well be the crowning reason for SSL's success.
(One can see this
when one considers the case for 40 bit crypto.  That
would have done perfectly for protecting credit cards.
But, because of the absence of a threat model, there
was also absence of requirements.  So 40 bits, instead
of being entirely adequate to protect credit cards,
became entirely inadequate.)
Fair enough, as long as we understand their motives
at the time.
That all was nearly 10 years ago.  (I'm not sure I as
an individual would have been able to see the full
story then.  It's important to realise that the net
was younger then, and the full impact of the commercial
steamroller was only imagined at by most of us.)
But, it's now a decade down the path, and its well
time to re-assess whether SSL/HTTPS, etc, is using
the right models to benefit us.  Or anybody, really.
hmm.  Are you saying that SHTTP didn't have a threat
model (one interpretation of your "Hickman" post) or
that SHTTP assumed the Internet Threat Model (ITM)
in the same way that SSL did?
That seems to be close to the truth of it.  About
the only thing that is stopping one from cracking
WEP other than laziness is the fact that one is
knowingly and deliberately breaking into a network
(no matter how weakly secured).  YMMV as to what
sort of an impediment that is, I don't think it is
much to write home about on a crypto scale.

@_date: 2003-09-07 15:14:06
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
Sure.  SSH.
It's about take up models.  HTTPS'
model of take-up is almost deliberately designed
to reduce take-up.  It uses a double interlocking
enforcement on purchase of a certificate.  Because
both the browser and server insist on the cert
being correct and CA-signed and present, it places
a barrier of size X in front of users.
Instead, if there were two barriers, each of half-X,
being the setup of the SSL server (a properly set
up browser would have no barrier to using crypto),
and the upgrade to a CA-signed cert, then many more
users would clear the hurdles, one after the other.
How high can you jump?  When I was young we used
to do this high jump thing, where we'd get up to
5 feet or so.
I could never do 6 feet.  I couldn't even do 4 feet
these days, but, I could do any number of 3 feet jumps.
I could probably even do a few 3 feet jumps these days.
(In that youth, we called them by feet.  These days,
a one metre jump looks more imposing...)
I'm curious.  You really think that in order to sell
certificates, the best thing is to make them hard to
use?  Is this a "quality" argument?

@_date: 2003-09-07 16:27:08
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
It's more than an anecdote.  If I quote from your
slides, SSH has achieved an almost total domination
of where it can be deployed.  Wherever there are Unix
servers, we suspect the domination of SSH.
(I haven't got a good figure on that.  Some stats
have been done Neils Provos and Peter Honeyman in
a paper, but I can't interpret the results sufficiently
to show SSH server distribution, nor penetration [1].
It's now a hot topic, so I believe the figures will
become available in time.)
There is a middle ground between data and the air,
which is analysis.  I've been meaning to write it
up, but I'm working on the SSL threat model right
I take the following approach here.  I think that
for Apache to promote the interests of the users,
it should configure automatically to run SSL, and
automatically generate a self-signed cert on install
(unless there is one there already).  I admit I
haven't looked to see whether that is reasonable
or possible, but I gather it does neither of those
things, and it certainly doesn't make doing self-
signed certs so easy.
Oh, and Apache does lead one astray by calling the
self-signed cert a "snake-oil" cert.  This misleads
the users into thinking there is something wrong
with a self-signed cert.  I'm not sure how easy
that is to correct.
The reason we have no idea how elastic the demand
for certs is, is because a) we've never tried it,
and b) we've not looked at the data that exists.
(Yes, those reasons are contradictory.  That's part
of the world that we want to change.)
It's nothing to do with whether the ivory tower
brigade does some econowhatsists on their models
and then speculates as to what this all means.
Have a look at the data that is available [2].  You
will see elasticity.  Have a look at the history
of a little company called Thawte.  There, you will
see how elasticity contributed to several hundred
millions of buyout money.
Mark S prays to the god of elasticity every night.
Check out the Utah digsig model.  If you can see
a better proof of cert elasticity, I'd like to know
about it.
[1]     [2]

@_date: 2003-09-07 16:27:52
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
I've left your entire email here, because it needs to
be re-read several times.  Understanding it is key to
developing protocols for security.
What you are talking about there is a misalignment
of interests.  That is, the car manufacturer has no
incentive to reduce the theft (by better locks, for
e.g.) if each theft results in a replacement sale.
Conventionally, this is dealt with by another interested
party, the insurer.  He arranges for the owner to have
more incentive to look after her car.  He also publishes
ratings and costs for different cars.  Eventually, the
car maker works out that there is a demand for a car
that doesn't incur so many follow-on costs for the owner.
This is what we call a "free market" solution to a
problem.  The alternative would be some form of
intervention into the marketplace, by some well-
meaning "authority."
The problem with the intervention is that it generally
fails to arise and align according to the underlying
problem.  That is, the authority is no such, and puts
in place some crock according to his own interests.
E.g., ordering all car manufacturers to fit NIST
standard locks (as lobbied for by NIST-standard
lock makers).  Or giving every car owner a free
steering lock.
And, that's more or less what we have with HTTPS.  A
security decision by the authority - the early designers
- that rides on a specious logical chain with no bearing
on the marketplace, and the result being a double block
against deployment.
(It's interesting to study these twin lock-ins, where
two parties are dependant on the other for their
mutual protocol.  For those interested, the longest
running commercial double cartel is about to come
crashing down:  DeBeers is now threatened by the the
advent of gem quality stones for throwaway prices,
its grip on the mines and retailers won't last out
the decade.  Understanding how DeBeers created its
twin interlocking cartels is perhaps the best single
path to understanding how cartels work.)
Correct!  You've revealed it.  IMHO, not understanding
that fact has been at the root cause of more crypto biz
failures than almost any other issue.  My seat of the
pants view is that over a billion was lost in the late
eighties on payments ventures alone (I worked for a
project that lost about 250 million before it gave up
and let itself be swallowed up...).
In reality, the finance industry cares little about
reducing fraud.  This is easy to show, as you've done.
It'e perverse, because as you say, the so-called
defrauded is profiting from this situation.  But,
nobody is looking at the real source of frauds,
comfortable in the knowledge that crypto has done
its job, and its impenetrability is a wonder to
behold.  Nobody can doubt the system is secure,
because the crypto is secure...
I couldn't agree more.  The essence of the secure
protocol is to build up trust.  For that, trust
has to be represented.  And it has to be encouraged
to flow.  As Lynn Wheeler has pointed, out trust is
not a huge cert with lots of data, it's a small thing
that is so insignificant, it can travel fast and light,
and it can be counted and catalogued.
When you walk through the jungle of trust, you watch
how the birds fly, not how the bamboo hides the tiger.
(I like Lynn Wheeler's posts.  Generally there's no
response possible, because he concentrates on the
point, and nails it.)
Pretty much.  "Trust" in the certificate world means that
a CA has authorised a web server to conduct crypto stuff.
That ain't trust, not as most of us know it.  And, any
trust, as we do know it, is actually removed from the system
by the overbearing nature of x.509 certs, in cohoots with
the rote browser acceptance of the CA's authorisation.
Trust could be added - in the browsers.  It would start
by ignoring or downgrading the presence of a signer, and
concentrating on the persistance.  For most browsing
purposes, a recognised face is probably the best trust
There have been many thoughts as to where to go beyond
simple SSH-style persistant self-signed certificate caching.
Can we mix in PGP web-of-trust?  Is it possible to infect
URLs with hashes?  etc etc.
But, I think the 1st step has to be to encourage the
designer to look at even the basic unit of trust, which
is establishing trust over a single certificate.  Until
the browsers start to cache and analyse, it's futile to
think of more sophisticated shared forms of trust, as
found in other trust-based protocols like PGP's WoT.

@_date: 2003-09-07 21:54:24
@_author: Ian Grigg 
@_subject: Is cryptography where security took the wrong branch? 
Sorry, Eric, I'm not quite with you on this...
You said:
You haven't established anything beyond some apparent
intention to consider inelasticity, as if it is some
superior magic property we have to do battle with.
Then, you said:
Firstly, most goods are elastic.  The natural
order of things is that if you want to establish
that your particular claimed good is otherwise,
then you might want to show it?  Please?
So, let's call this bluff:  please show why and
how the demand for certs is inelastic!
Secondly, I'm proposing self-signed certs.  What
does price inelasticity mean when the price is
Thirdly, after we have established the inelasticity
of the certs in question - a tough call - and we've
also established that it means something even when
the price is zero...
...we now want to establish how useful a tool is that
won't measure it, as you assert, without such a "major
econometrics project?"
I'm curious, why did you bring econometrics up if it
is too hard to use?
And, why should *I* use it?  The certs are elastic,
until I see something to the contrary.
Fourthly, econometrics delivers explanatory power.
I.e., the past.  It is weak on predictive power.
I.e., the future.
That's because the assumptions are arbitrary.  And,
that's why marketeers don't use econometrics:  they
prefer to change the assumptions, which takes us out
of the data set.
Which is where I am:  the assumptions.
First step, examine the assumptions of the past.
That's what we on this list have been doing for the
last 6 months.
Second step, change them.
PS: I'm curious, is there some URL that talks about
application of inelasticity and econometrics to crypto?
Somebody who's selling the notion?

@_date: 2003-09-08 14:34:25
@_author: Ian Grigg 
@_subject: Digital cash and campaign finance reform 
How would you audit such a system?  I'm not that up
on political cash, but I would have expected that there
would be a need to figure out where money was coming
from, by some interested third party
Also there would be a need to prove that the funds
were getting there, otherwise, I'd be the first to
jump in there and run the mix.  Or, the mint.

@_date: 2003-09-08 17:33:29
@_author: Ian Grigg 
@_subject: Code breakers crack GSM cellphone encryption 
Once upon a time, it used to be the favourite
sport of spy agencies to listen in on the
activities of other countries.  In that case,
access to the radio waves was much more juicy
than access to the POTS.
I've not heard anything explicitly on this,
but I'd expect satellites to be able to pick
up GSM calls.  (One of the things I have heard
is that the Chinese sold fibre networking to
Iraq, and the Russians sold special phones
with better crypto.  Don't know how true any
of that is.)
Also, the patent issue will work very well in
countries where there are laws against hacking
and cracking and so forth.  Rather than have
such laws subject to challenge in the supreme
court, a perp can be hit with both patent
infringement and illegal digital entry.  The
chances that anyone can defeat both of those
are slim.
(OTOH, I wonder if it is possible to patent or
licence something that depends on an illegal

@_date: 2003-09-09 13:44:46
@_author: Ian Grigg 
@_subject: x9.59 
Whatever happened to x9.59?
Also, is there a single short summary description of what
x9.59 does?  I don't mean a bucket full of links to plough
through, I mean some sort of technical overview that wasn't
approved by the marketing department.

@_date: 2003-09-09 14:39:37
@_author: Ian Grigg 
@_subject: Code breakers crack GSM cellphone encryption 
Within the context of their threat model, it is quite instructive
to consider how successful these algorithms are.
AFAIK, the phone threat model includes these two attackers:
  * johnny phone thief who steals billing identities and sells
    cheap spoofed phones, and
  * janie papparazzi that records the famous and foolish revealing
    themselves over the phone, and then publishes in the media
Empirically, the GSM system defeated these threats.  GSM first
hit the market about 10 years ago, and since then, the victims
of the above have enjoyed peace and prosperity, with no risk
of spoofed (GSM) phones and no risk of (GSM) eavesdroppers.
Yet, they did it with 17 bit crypto.
(Well, that's not quite the whole story.  We can probably guess
that they were encouraged to do is with very weak crypto.  In
fact, there is sufficient anecdotal evidence to conclude that
there were strange and unrelated people involved who diverted
the security equations from strength into weakness.)
By doing it with such superficial crypto, GSM was now faced
with a third threat:
  * the researcher who reveals the way to the other attackers.
To cover this threat, GSM instigated security-by-secrecy,
and wrapped it up in a marketing campaign that claimed the
crypto was unbreakable.  Basically, a lie.  I recall being
told by the salesman of my first phone that the crypto was
unbreakable, and I had to kick myself for buying it, when,
a year later, I realised that it could not be encrypted
beyond the basestation, and therefore, strong crypto was
And, it worked.  Eli Biham said    "I told him (Barkan) that it was impossible,"
Everyone in the community bought it.  Even post-Lucky Green,
there was no real thought that there was a bigger better
hack hiding in there.
   "The 450 participants, many of whom are leaders
   in encryption research, 'were shocked and astounded'
   by their revelation that most cellphones are
   susceptible to misuse."
The crack finally occurred a decade after deployment.  GSM
security even survived the infamous Lucky Green crack that
Dave Wagner and Ian Goldberg helped with;  there was no
practical fallout to that other than embarressment, that
I ever heard of, due to the difficulty of exploitation.
Lucky tells the story of how the one GSM security expert
brazenly said, "hey, it worked for 8 years!"  (Words from
my memory, perhaps Lucky can retell the story.)  It worked
for longer...
What's even better, or worse, depending on your pov, is
that the the timing couldn't be better:  there is still
time to beef up the G3 security, and its close enough to
rollout of that technology such that this crack will
*help* takeup.
Nothing more desirable could happen to the GSM group than
the first hand-built or grey-import GSM-2 phone crackers
start appearing, just as GSM3 is starting to roll out.
Perfect!  It's the huge win for GSM.  You simply can't
purchase help like that (not that I'm suggesting they
did, of course).
What can we learn from this?  I guess:
   * institutional crypto systems will always be perverted,
   * believe no claims of invulnerability,
   * large crypto systems need only a modicum of strength
     to do a sufficient job against their direct threats,
   * the independant researcher is part of the threat
     model, as an indirect threat, and
   * security-by-secrecy / obscurity can work, and can
     work exceedingly well.
What's not clear is whether the GSM group can pull this
trick off next time.  They may have to put in real security
into the G3, to counter the third threat.  Or, maybe not,
as now, there is the additional weapon of the law on their
side, which might be enough to keep the third threat at

@_date: 2003-09-14 12:31:09
@_author: Ian Grigg 
@_subject: quantum hype 
The only answer that I have come across - to which I
ascribe no view on accuracy - is "undersea fibre" [1].
According to the story, it is possible tap into an
undersea fibre without cutting into it, or the shield.
Something about a device that bends the fibre, and
listens to the energy that escapes...  It's accurate
enough to isolate individual fibres in a bundle.  Of
Which makes the attack simply a matter of getting there,
and for this purpose there are special assets available.
(I.e., submarines.  google USS Jimmy Carter.)
So, the analysis shifts to your threat model described
above.  How do you know when the enemy - a state that
has these subs and these beam benders - is listening
on our fibre?
Personally, it all sounds like too much like a bad
science fiction novel, where normal crypto practices
are forgotten for plot reasons.  But, that may still
be indistinguishable from the actions of your average
empire, from where we sit.  It remains an interesting
thought experiment, as long as we don't forget to
challenge the "because we said so" assumptions...
PS: I think there is one place where "QC" might
make more sense:  SOSUS.  With that network,
you don't so much care that the enemy is listening
in on your fibre (e.g., RTP commsec says that you
don't encrypt the enemy's location because he
already knows it.  Although there is more to it
than that.)
What you want is to find out where the enemy is
listening in, and when.  Then, it just becomes
another data point in the tracking game.
Still, it seems too elusive an advantage to worry
about, in a practical sense.  Once the enemy
figures it out, he'll stop doing it.  Or do it
to insert bad data.
[1]

@_date: 2003-09-16 11:41:43
@_author: Ian Grigg 
@_subject: PGP makes email encryption easier 
For the record, AFAIK, this approach was invented and
deployed by Dr. Ian Brown as his undergraduate thesis,
back in 1996 or so.  His Enigma used the now ancient
Cryptix 2.6 PGP code.  I used it for a long time, as
my personal proxy, until the newer PGP 4 formats started
to dominate.
It's a good approach.  It trades some sysadmin complexity
for the key admin complexity, but it also raises some
interesting challenges for deciding when to encrypt,
when not to encrypt, and also, when to block outgoing
mail that should be encrypted...
(I commend the PGP Inc company for being careful with
their marketing spiel!)

@_date: 2003-09-16 17:51:32
@_author: Ian Grigg 
@_subject: PGP makes email encryption easier 
Thanks for the correction!  Was this project ever released
or documented?  I never heard of it before.
:-)  Many inventions are obvious once well understood.
Although I would agree that such an invention should not
deserve to be patented.  Whether that's because it is too
obvious, or too useful, depends on ones pov...

@_date: 2003-09-17 21:06:49
@_author: Ian Grigg 
@_subject: Simple inner transposition steganography 
I'm not sure if this is novel, but it's new to me,
and a lot of fun to brighten up our otherwise dull
Some guys over on dgcchat have stumbled on a simple
steganography method.  What follows is their own
words, but in an edited single sequence:
Aoccdrnig to a rscheearch at an Elingsh uinervtisy, it deosn't
mttaer in waht oredr the ltteers in a wrod are, the olny
iprmoetnt tihng is taht frist and lsat ltteer is at the rghit
pclae. The rset can be a toatl mses and you can sitll raed it
wouthit porbelm. Tihs is bcuseae we do not raed ervey lteter by
it slef but the wrod as a wlohe.
Ragnar (2):
I suppose, for those that don't have encryption, a proggy can be
developed to change around words (keeping the last and first
letter the same) in an email before sending, based on this
research.  :)
Ken Griffith adds:
Taht wulod be an execlenlt way to sned emial msesgaes in palin txet taht
cnnaot be dteetced by ehceoln.  One culod tlak aoubt bmbos, trerroitss and
suftf lkie taht wiohtut trgigreing the fagls.
No work on the original research though.

@_date: 2003-09-18 00:42:06
@_author: Ian Grigg 
@_subject: Simple inner transposition steganography 
Indeed!  (The source remains elusive, some have commented
here  but
without tying it down.)
Either way, the point is well made.  One of the things
that the 911 guys apparently did was communicate from
public libraries.  If they were to use such a technique,
and also to use a set of pre-created email addresses on
hotmail or yahoo, one could imagine that they'd pretty
well stuff any massive scanning techniques.
And, after the fact, there's less of an issue.  So a
travelling terrirost could forego their copy of PGP and
instead carry around a list of email accounts and a
propensity for dyslexia.
Which makes an odd sort of sense;  one of the things that
was apparent in the payments world is that real bad guys
would not use supposed anonymous electronic payment systems
because they assumed out of hand that the anonymity was a
If such distrust was applicable across different systems,
then people who really care about their secret communications
might eschew crypto as well.
PS: the other aspect is the often claimed flood of stego
across the net.  Now, we can measure it easily, simply run
a spell checker over the emails :)

@_date: 2003-09-18 17:35:22
@_author: Ian Grigg 
@_subject: Simple inner transposition steganography 
One could declare such a simple trick to be "not stego."
Or, even, worthless, and beneath the contempt of the
serious student of cryptography.
That would be too harsh.  The elegance of the idea is
that it shows how little one needs to do to achieve some
security from observation.
How much is then the question - is it good enough?  Well,
that comes down to the threat.  And the costs you are
willing to bear.
There are those that say that unless you are using 128
bit blah blah with 1024 RSA acronymstandardwhatsits,
you haven't got a thing.  They are wrong, and, luckily,
we can now see that the market place ignores that as
much as its permitted.
They are wrong because they didn't ask what the threat
was, and didn't ask how much the user wanted to spend.

@_date: 2003-09-19 11:57:22
@_author: Ian Grigg 
@_subject: quantum hype 
"R. Hirschfeld" wrote on QKD:
If I understand this correctly, this is both
an eavesdropping scenario and an MITM scenario.
In the above, Eve is acting as Mallory, as she
is by definition intercepting the bits and re-
sending them on?
That is, the "Quantum Property" is that Eve can
be detected because she destroys photos in the
act of listening, and Mallory, who can resend
the photons, has only a 50% chance of reading
each bit correctly in advance, so he can be
detected after the fact as well, as 25% of his
bits are wrong.

@_date: 2003-09-20 12:52:21
@_author: Ian Grigg 
@_subject: Can Eve repeat? 
I'm curious on one point of terminology - my understanding
of the term "EVE" was that she could not be active, and
could not repeat.  Is this defined anywhere?
I had a look at the big red book, but Schneier doesn't
quite nail that issue.  He seems to err on the side of
Eve not being able to repeat, as the dividing line is
set as active v. passive.
As I say, just a point of terminology.

@_date: 2003-09-21 16:39:37
@_author: Ian Grigg 
@_subject: The Code Book - in CD form 
Has anyone reviewed Simon Singh's CD version
of "The Code Book" ?
After 12 months of intense development, the interactive
CD-ROM version of The Code Book is now available. I might
be biased, but I think that it is brilliant. Don't be
confused by the ridiculously low price, because this
CD-ROM contains tons of fascinating and dynamic material,
              1. Encryption tools,
              2. Code breaking tools,
              3. Dozens of video clips,
              4. Coded messages to crack,
              5. Material for teachers, e.g., worksheets,
              6. A realistic, virtual Enigma cipher machine,
              7. A beginner's cryptography tutorial,
              8. A history of codes from 1000BC to 2000AD,
              9. Material for junior codebreakers,
            10. Interviews with Whit Diffie and Clifford Cocks,             11. Sections on public key crypto & RSA,
            12. An animated section on quantum cryptography.
The CD-ROM is ideal for teenagers, parents who want to
encourage an interest in science and mathematics in their
children, grown-ups interested in the history of cryptography,
amateur codebreakers and anybody who wants to know about
encryption in the Information Age.

@_date: 2003-09-22 16:53:46
@_author: Ian Grigg 
@_subject: Who is this Mallory guy anyway? 
Mallory is the Man-in-the-Middle.  He is the one
that inserts himself into a connection, in an
active attack, and sends packets to both Alice
and Bob.  He can send one thing to Bob, and
send another thing to Bob.  In this way, he
can insert himself into a Diffie-Hellman key
exchange, and send completely separate numbers
to both both parties.
Eve is indeed the eavesdropper.  She can only
(As a further point, there are other personas,
being Trent, the trusted third party.  Also,
Victor, a verifier.  In financial cryptography
we use Ivan as an Issuer and sometimes Matilda
as a merchant.  Carol and Dave can assist
Alice and Bob in more complex protocols.)
Well, that's the question - is Eve allowed to
forward packets, in the act of listening, or
is that the Mallory's job?  I don't know.
Given the silence on the issue, and the differeng
usages, I'd say we've reached an uncertainty in
the definition.
The question revolves around whether Eve's name
derives from her eavesdropping, or whether she
is passive, and can only do stuff that can be
done by observation.  If she is allowed to resend
because she is eavesdropping then that's ok.  But,
if she must only passively listen - measure - and
cannot resend, then what this Quantum stuff does
is eliminate her from consideration because she
will always give herself away.  Hence, only
Mallory, the MITM, can do the job.  In effect,
it is very close to Anon-DH - in that Eve cannot
crack the crypto, but Mallory can.
It's a minor point, it doesn't really change the
crypto at all, but it can evoke different images
in different people if they don't agree on which
it is.  So one has to be careful, as the essence
of naming is, after all, efficient communication.

@_date: 2003-09-24 20:30:27
@_author: Ian Grigg 
@_subject: why are CAs charging so much for certs anyway? (Re: End of the line  
Excuse me?  Why are they being sold "per year" in the
first place?
It's not as if there are any root servers to run!

@_date: 2003-09-25 16:53:03
@_author: Ian Grigg 
@_subject: Reliance on Microsoft called risk to U.S. security 
On the face of it, this is being too kind and not
striking at the core of Microsoft's insecure OS.  For
example, viruses are almost totally a Microsoft game,
simply because most other systems aren't that vulnerable.
But, it is also possible to secure M$ OSs, so maybe there
is some merit to not putting "all the blame on Microsoft."
Either way, it can be tested.  There is one market where
M$ has not dominated, and that is the server platform.
I haven't looked for a while, but last I looked, the
 players were Linux, Microsoft, FreeBSD, and only
a percentage point or two separated them.  (I'm unsure
of the relative orders.  And this relates to testable
web server platforms, rather than all servers.)
So, in the market for server platform OSs, is there
any view as to which are more secure, and whether that
insecurity can be traced to the OS?  Or external factors
such as a culture of laziness in installing patches, or
derivative vulnerability from being part of the monoculture?
(I raise this as a research question, not expecting any

@_date: 2003-09-28 14:52:48
@_author: Ian Grigg 
@_subject: Tinc's response to "Linux's answer to MS-PPTP" 
It is curious, is it not, that there has been no well
written protocol that became successful on its first
attempt?  And, contrariwise, all successful systems
started out with crypto that slept shamefully with
This is the best thing written so far.  Even if Guus
and Ivo were not to distribute their designs for 2.0,
I would salute their efforts so far.
It is clear that they have users.  Hoorah! I say.  It
is clear that they have successfully enabled millions
of VPN connections.  There art we happy!  It is fair
to say that through their efforts, many hundreds or
thousands of Linux boxen have escaped becoming part
of the lamented and hacked 43,000.  A pack of blessings
light upon the backs of cryptographers!
The notion that Guus and Ivo have done anything in the
slightest sense, wrong, is mysterious to me.  It defies
explanation.  They built a product.  They protected users.
Now, later on, after *proving* the product meets the
needs of the market place, is the time to clean up the
stopgap home-brewed crypto.  It's not the most urgent
thing.  Only if the product is under sustained and
unavoidable attack by the bad guys - like HTTPS - is
it urgent to get in there and fix the security.
And from the absence of any commentary on actual attacks,
there seems all the time in Mantua to prepare a killer 2.0
crypto layer.
Or am I missing something?

@_date: 2004-04-03 15:19:34
@_author: Ian Grigg 
@_subject: Do Cryptographers burn? 
On a related note, this was one of the core premises
behind my paper on Financial Cryptography in 7 Layers.
The notion was that building systems involving the
two key words, finance and crypto, had almost always
failed due to great gaping holes, that amounted
to the designers ignoring one or more disciplines.
In that paper I attempt to map out all the core
areas that are "must dos".  I don't think it's
possible to cover *all* the fields to a professional
level, one would likely need 3 or 4 degrees to do
it.  E.g., within crypto and software, two of the
disciplines that are common on this group, there are
very few people who can crossover and seriously
contribute to the other discipline.  I know of a
handful (and wouldn't include me, as my crypto
knowledge is very basic).
Yet the challenge remains that all these things need
to be considered in an FC application.
No, it requires their contribution to be simple
and verifiable.  If the crypto goes beyond the
half dozen basics (Hashes, PK, SK, ...), then its
viability reduces rapidly, as the programmers
and others in higher layers will have trouble
dealing with it.
It's much much more likely that when a perfect
crypto algorithm is mated to a perfect protocol
and then mated to a perfect algorithm, the result
is swiss cheese.  That is, errors at the borders of
disciplines are a more likely error.
Security is a top-to-bottom
requirement, and integration is key.  That's why
a complex system is not a good idea, because you
can't mate it into any usable app without breaking
the complex and hidden assumptions.
PS: _Financial Cryptography in 7 Layers_,
Conference in Financial Cryptography, Feb 2000,
Proceedings are in

@_date: 2004-04-04 18:11:53
@_author: Ian Grigg 
@_subject: Do Cryptographers burn? 
Quite some story.  Most people seemed to assume
this demanded an answer from a *commercial*
 From an academic perspective, there is a very
different flavour.  It's strongly about networking
and published papers.  So, in a sense, the real
world protects itself from the academic world
in a variety of ways.  Perhaps what we are trying
to say is that *all* academics burn ;)  Famously,
someone said that last year and caused a furor
("Maybe it's all snake oil..." or somesuch.)
For example, if you look at any popular crypto
system out in the real world, that was successful
by some measure, it doesn't include any fancy
academic stuff.  If the contents are beyond the
inclusion in basic crypto tomes, it just doesn't
seem to survive.
Your use of the word "expertise" has me confused,
is this an actual english word, or is there a
better word?  It seems that opinion is the right
one, to my mind, but that's not quite right.
So, you have the University's answer - they will
just ride out the lawsuit, if you file?  It seems
that you have three options:  file a lawsuit and
prove you won't go away, because you want your
dissertation.  Second, publish your story, and
give up your dissertation.  Third, dump the lot,
and take up ditch digging.
I wouldn't necessarily agree that it makes sense
to fight it.  It sounds like you may have to fight
for more years than it is worthwhile, just to pick
up the dissertation.  And, by the time you get it,
that University's name might not be worthwhile.
OTOH, if you are going to stand up for academic
integrity, make sure that you are careful not to
publish anything that can be challenged.  E.g.,
this email does seem to have a lot of opinion in
it, and/or things that you claim but cannot prove.
In a real court case, your posts would be used
against you.
Maybe you should use the time wisely and do a
law degree instead of further academic work in
cryptography :)
 >
The fact that he is on the board of directors
for a club doesn't mean he knows anything about
the discipline.  The club may have elected him
for his accounting knowledge, or for any number
of unrelated reasons.  I suspect this is distinct
to the German situation, where directors might
be expected to be leaders in their field.  In
the Anglo world, directors are expected to be
appointed on their ability to help the organisation.
If we cared about their academic capabilities, then
things like tenure are a more important indicator.
Sounds like you are in a loser's position all
around.  The University has the power to award,
and that's "how it should be."  They've decided
not to, so you lose.  You can't ever gain any
money or value from fighting.  You will lose a
lot of time and money, and the more you fight,
the less likely it is you will ever work in this
field again.  They know this.
You're also branded as a troublemaker.  This will
make it harder to work in academia or in commerce
in countries like Germany.  But, not all countries
nor all peoples think that University education is
where it's at, and some are hostile in inverse
proportion to how long you were incarcerated.

@_date: 2004-04-07 15:42:47
@_author: Ian Grigg 
@_subject: Firm invites experts to punch holes in ballot software 
It seems to me that the requirement for after-the-vote
verification ("to prove your vote was counted") clashes
rather directly with the requirement to protect voters
from coercion ("I can't prove I voted in a particular
way.") or other incentives-based attacks.
You can have one, or the other, but not both, right?
It would seem that the former must give way to the latter,
at least in political voting.  I.e., no verification after
the vote.

@_date: 2004-04-08 09:58:05
@_author: Ian Grigg 
@_subject: Firm invites experts to punch holes in ballot software 
If I'm happy to pervert the electoral
process, then I'm quite happy to do it
in busloads.  In fact, this is a common
approach, busses are paid for by a party
candidate, the 1st stop is the polling
booth, the 2nd stop is the party booth.
In the west, this is done with old people's
homes, so I hear.
Now, one could say that we'd distribute
the verifiability over a random set of
pollees, but that would make the verification
impractically expensive.

@_date: 2004-04-19 09:58:35
@_author: Ian Grigg 
@_subject: Financial Cryptography Update: El Qaeda substitution ciphers 
((((( Financial Cryptography Update: El Qaeda substitution ciphers )))))
                              April 19, 2004

@_date: 2004-04-25 14:47:25
@_author: Ian Grigg 
@_subject: Bank transfer via quantum crypto 
You are looking at QC from a scientific perspective.
What is happening is not scientific, but business.
There are a few background issues that need to be
brought into focus.
1) The QC business is concentrated in the finance
industry, not national security.  Most of the
fiber runs are within range.  10 miles not 100.
2) Within the finance industry, the security
of links is done majorly by using private lines.
Put in a private line, and call it secure because
only the operator can listen in to it.
3) This model has broken down somewhat due to the
arisal of open market net carriers, open colos, etc.
So, even though the mindset of "private telco line
is secure" is still prevalent, the access to those
lines is much wider than thought.
4) there is eavesdropping going on.  This is clear,
although it is difficult to find confirmable
evidence on it or any stats:
   ?Security forces in the US discovered an illegally installed fiber
   eavesdropping device in Verizon?s optical network. It was placed at a
   mutual fund company?..shortly before the release of their quarterly
   numbers?   Wolf Report March, 2003
(some PDF that google knows about.)  These things
are known as vampire taps.  Anecdotal evidence
suggests that it is widespread, if not exactly
rampant.  That is, there are dozens or maybe hundreds
of people capable of setting up vampire taps.  And,
this would suggest maybe dozens or hundreds of taps
in place.  The vampires are not exactly cooperating
with hard information, of course.
5) What's in it for them?  That part is all too
The vampire taps are placed on funds managers to
see what they are up to.  When the vulnerabilities
are revealed over the fibre, the attacker can put
in trades that take advantage.  In such a case,
the profit from each single trade might be in the
order of a million (plus or minus a wide range).
6) I have not as yet seen any suggestion that an
*active* attack is taking place on the fibres,
so far, this is simply a listening attack.  The
use of the information happens elsewhere, some
batch of trades gets initiated over other means.
7) Finally, another thing to bear in mind is that
the mutual funds industry is going through what
is likely to be the biggest scandal ever.  Fines
to date are at 1.7bn, and it's only just started.
This is bigger than S&L, and LTCM, but as the
press does not understand it, they have not
presented it as such.  The suggested assumption
to draw from this is that the mutual funds are
*easy* to game, and are being gamed in very many
and various fashions.  A vampire tap is just one
way amongst many that are going on.
So, in the presence of quite open use of open
lines, and in the presence of quite frequent
attacking on mutual funds and the like in order
to game their systems (endemic), the question
has arisen how to secure the lines.
Hence, quantum cryptogtaphy.  Cryptographers and
engineers will recognise that this is a pure FUD
play.  But, QC is cool, and only cool sells.  The
business circumstances are ripe for a big cool
play that eases the fears of funds that their
info is being collected with impunity.  It shows
them doing something.
Where we are now is the start of a new hype
cycle.  This is to be expected, as the prior
hype cycle(s) have passed.  PKI has flopped and
is now known in the customer base (finance
industry and government) as a disaster.  But,
these same customers are desparate for solutions,
and as always are vulnerable to a sales pitch.
QC is a technology who's time has come.  Expect
it to get bigger and bigger for several years,
before companies work it out, and it becomes the
same disputed, angry white elephant that PKI is
If anyone is interested in a business idea, now
is the time to start building boxes that do "just
like QC but in software at half the price."  And
wait for the bubble to burst.
PS:  Points 1-7 are correct AFAIK.  Conclusions,
beyond those points, are just how I see it, IMHO.

@_date: 2004-08-03 22:18:27
@_author: Ian Grigg 
@_subject: Al Qaeda crypto reportedly fails the test 
Read about one of their coding systems here:
And, FWIW, my commentary:
[Moderator's Note: One wonders if the document on the "Smoking Gun"
website is even remotely real. It is amazingly amateurish -- the sort
of code practices that were obsolete before the Second World War. --Perry]

@_date: 2004-08-23 19:57:28
@_author: Ian Grigg 
@_subject: First quantum crypto bank transfer 
That logic amounts to "it's ok to sell a fraud that
will make people less secure if it shakes out some
money for my good works."  Which is fine, as it is
standard practice to pay off some scientist to sit
on the board and say what he's told.  As long as we
all understand that it's not science, and it's not
Still, it may be that the hype from selling expensive
boxes full of fibre snake oil will flow through to
more cheap boxes full of software crypto, which would
be a good thing.  So it's not clear that every QC box
sold will reduce security overall (through lost

@_date: 2004-08-27 00:22:26
@_author: Ian Grigg 
@_subject: How thorough are the hash breaks, anyway? 
Correct me if I'm wrong ... but once finding
a hash collision on a public key, you'd also
need to find a matching private key, right?

@_date: 2004-08-27 00:34:54
@_author: Ian Grigg 
@_subject: titles 
Right.  This approach - to which I also subscribe - lays
claim to the term "engineer."  So, technically, cryptoengineer
makes a lot of sense.
Where the emphasis is on programming up primitives, and also
participating on lower level software engineering issues, I've
also seen the term "cryptoplumber" used.
Where the emphasis is on applications, and slotting in the
crypto where it helps, the term "financial cryptographer"
has been used.  This was coined by Bob Hettinga, who has a
bottom-up view of it, meaning crypto heavy.  I prefer to
think of it as top-down, meaning application heavy.
 > "Security Engineer", according to Schneier...
I don't like that term for 3 reasons:  firstly, when we
build stuff, security should be top-to-bottom, integrated
in, and not seen as an add-on, an after-thought.  That
is, the overall engineer should build in the security as
required from the beginning, so it is a skill that all
need, and not something thrown over the wall to the guy
with "security" in his title.
Secondly, anything to do with security has a very strong
hype-to-value ratio, so much so that it's quite hard to
find a "security" company selling good security stuff.
Thirdly, good security engineering, as it should be done,
doesn't necessarily involve crypto.  The art is in using
as little crypto as possible - in precise and well placed
doses.  IMHO.  Oftentimes, however, security engineers
start from the pov that crypto is a hammer, and their
job is to go find a nail to encrypt.
(These reasons my be related ...)
All, IMHO!

@_date: 2004-12-04 11:49:05
@_author: Ian Grigg 
@_subject: 'Proving' the correctness of a network encryption system test  
(I thought this was an interesting problem, and was
waiting for a response...)
This means encrypting and decrypting network streams?
If you can inject chosen plaintext into the system in
1, then write another system that does the same thing,
run it with the same information, and spit out some
ciphertext.  Then, inject the plaintext into system 1,
and watch for the ciphertext.
Well.  Either write system 3 that does what system 2
does to system 1....  Or, use system 1 to test system 2?
I'd go for the former myself.
All three specs should be the same.
Ah.  OK.  How about this:  swap system 1 with 2 during
live usage and revert 1 into testing mode.
One good idea is to specify that the entirity of
the system(s) is open source, and Alice can construct
it.  This doesn't prove anything by itself, but it
raises the bar on any errors or weaknesses, because
once identified, they are much easier to find and
resolve.  Far less potential for childish arguments
like "is so, is not."

@_date: 2004-12-08 10:37:44
@_author: Ian Grigg 
@_subject: 3DES performance 
Try typing:
    openssl speed
on any Unix platform (until you find one with OpenSSL installed).

@_date: 2004-12-15 12:08:49
@_author: Ian Grigg 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
Let me get this right. ...
A blockbuster worth $100m gets cracked ... and
the crack gets watermarked with the Id of the
$100 machine that played it.
So the solution is to punish the $100 machine by
asking them to call Disney with a CC in hand?
As described this looks like snake oil.  Is this
for real?

@_date: 2004-12-23 20:23:21
@_author: Ian G 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
I think in comment to both Bill and John, the counter
argument seems to be the same:  is this likely to make
a difference in practice?  I can't see it.  Yet.
If Alice, notorious p2p pirate, has this particular DVD
player in front of her, she simply factors it in.  Instead
of releasing her copies in dribs and drabs, she releases
them in batch.  Once released, the player is determined
to be an "old material only" player.  But this is no barrier
as DVD players now retail for the price of 10 DVDs, so
upgrading every 6 months is really no drama.
Where this *does* has an effect, I think, is that when
the black-booted IP police come in through the front
door (and I mean, through it...) and seize all the guilty
tech equipment, what they also pick up is a player that
has been identified to be a source of pirated material.
So before the judge, they can state that they found
pirated material, the IP number was tracked, *and*
they found the tools, as identified by other pirated
material distributed on the net.  This wipes out the
defence of "using Kazaa for bona fide purposes".
Also, if they have a way of tracking the purchases
of players, then they can more easily get warrants
for their non-radial door penetration manouvers.
Imagine a world where all DVD players are barcoded
with serial numbers, and the sale is related to a
credit card.  Closed loop, easy to show sufficient to
the judge to get the warrant.
Which would be even nicer if we could enter a new
crime onto the books to the effect of "purchasing a
DVD player without a credit card."

@_date: 2004-12-25 10:50:55
@_author: Ian G 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
To add a postscript to that, yesterday's LAWgram
reported that $10 DVD *players* are now selling
in the US.  The economics of player-id-watermarking
are looking a little wobbly;  we can now buy
a throwaway player for the same price as a
throwaway disk.

@_date: 2004-12-29 17:09:44
@_author: Ian G 
@_subject: FC05 Preliminary Program Now Online 
The program and preliminary schedule can be found at:
      An official call for participation will be sent out as soon as
registration is open.  (We expect this to be early next week.)
   If you've yet to make travel arrangements, I would encourage you to stay
in Dominica on Thursday night (3/3) or longer to avoid a rush to the airport
after the morning program.  In the past, attendees who have stayed after the
conference have found that this is an excellent time to meet with others.
Keynote Speakers
Lynne Coventry (NCR)
Bezalel Gavish (Southern Methodist University)
Panel Sessions
Financial Technology in the Developing World
    Allan Friedman (Harvard) - Organizer
    Alessandro Acquisti (CMU)
    H William Burdett, Jr. (Foley & Lardner, LLP)
    Jon Peha (CMU)
    Steve Myers (Indiana University) - Organizer
    Drew Dean (SRI)
    Stuart Stubblebine (Stubblebine Research Labs)
    Richard Clayton (Cambridge, UK)
    Markus Jakobsson (Indiana University CACR)
Research Papers
Fraud within Asymmetric Multi-Hop Cellular Networks
    Gildas Avoine (EPFL, Lausanne, Switzerland)
Information-Theoretic Security Analysis of Physical Uncloneable Functions
    P. Tuyls
    B. Skoric
    S. Stallinga
    A.H. Akkermans
    W. Ophey (Philips Research Laboratories, The Netherlands)
Views, Reactions and Impact of Digitally-Signed Mail in e-Commerce.
    Simson L. Garfinkel
    Jeffrey I. Schiller
    Erik Nordlander (MIT)
    David Margrave (Amazon.com)
    Robert C. Miller (MIT)
Identity-based Partial Message Recovery Signatures
(or How to Shorten ID-based Signatures)
    Fangguo Zhang (Sun Yat Sen University, P.R.China)
    Yi Mu
    Willy Susilo (University of Wollongong, Australia)
How to Non-Interactively Update a Secret
    Eujin Goh (Stanford University)
    Philippe Golle (Palo Alto Research Center)
Interactive Diffie-Hellman Assumptions with Applications
to Password-Based Authentication
    Michel Abdalla
    David Pointcheval (Ecole Normale Superieure)
Achieving Fairness in Private Contract Negotiation
    Keith Frikken
    Mikhail Atallah (Purdue University)
Protecting Secret Data from Insider Attacks
    David Dagon
    Wenke Lee
    Richard Lipton (Georgia Tech)
RFID Traceability A Multilayer Problem
    Gildas Avoine
    Philippe Oechslin (EPFL Lausanne Switzerland)
A User-Friendly Approach to Human Authentication of Messages
    Jeff King
    Andre dos Santos (Georgia Tech)
Countering Identity Theft through Digital Uniqueness,
Location Cross-Checking, and Funneling
    P.C. van Oorschot (Carleton University)
    S. Stubblebine (Stubblebine Research Labs)
Policy-Based Cryptography and Applications
    Walid Bagga
    Refik Molva (Eurecom)
A Privacy Protecting Coupon System
    Liqun Chen (HP Laboratories)
    Matthias Enzmann (Fraunhofer SIT)
    Ahmad-Reza Sadeghi (University of Bochum)
    Markus Schneider (Fraunhofer SIT)
    Michael Steiner (IBM T.J. Watson)
Analysis of a Multi-Party Fair Exchange Protocol and Formal
Proof of Correctness in the Strand Space model
    Steve Kremer
    Aybek Mukhamedov
    Eike Ritter (University of Birmingham, UK)
Secure Biometric Authentication for Weak Computational Devices
    Mikhail J. Atallah
    Keith B. Frikken (Purdue)
    Michael T. Goodrich (UC Irvine)
    Roberto Tamassia (Brown)
Small Coalitions Cannot Manipulate Voting
    Edith Elkind (Princeton University)
    Helger Lipmaa (Helsinki University of Technology)
Efficient Privacy-Preserving Protocols for Multi-Unit Auctions
    Felix Brandt (Stanford)
    Tuomas Sandholm (Carnegie Mellon University)
Risk Assurance for Hedge Funds using Zero Knowledge Proofs
    Michael Szydlo (RSA Security/Independent)
Testing Disjointness of Private Datasets
    Aggelos Kiayias (University of Connecticut)
    Antonina Mitrofanova (Rutgers University)
Time Capsule Signature
    Yevgeniy Dodis (NYU)
    Dae Hyun Yum (POSTECH)
Probabilistic Escrow of Financial Transactions
with Cumulative Threshold Disclosure
    Stanislaw Jarecki (UC Irvine)
    Vitaly Shmatikov (UT Austin)
Approximation in Message Authentication
    Giovanni Di Crescenzo
    Richard Graveman (Telcordia)
    Gonzalo Arce
    Renwei Ge (U Delaware)
Systems & Applications Presentations
Securing Sensitive Data with the Ingrian DataSecure Platform
    Andrew Koyfman (Ingrian Networks)
Ciphire Mail Email Encryption
    Lars Eilebrecht (Ciphire Labs)
fc-announce mailing list
fc-announce at ifca.ai

@_date: 2004-01-01 21:03:34
@_author: Ian Grigg 
@_subject: digsig - when a MAC or MD is good enough? 
One view of digital signatures is that MACs and MDs may be
sufficient when:
   1.  the evidence is logged or otherwise kept by several
       parties, and
   2.  there exists sufficient legal clout to discourage
       tampering.
An example of 2. above would be the relatively new
Sarbanes-Oxley Act in the US.  Section 1102 of that act
adjusts the US Code to add this little gem:
    Whoever corruptly--
       "(1) alters, destroys, mutilates, or conceals a
       record, document, or other object, or attempts to
       do so, with the intent to impair the object's
       integrity or availability for use in an official
       proceeding; or
       "(2) otherwise obstructs, influences, or impedes
       any official proceeding, or attempts to do so,
    shall be fined under this title or imprisoned not
    more than 20 years, or both.".
Can we surmise that a digital record with an MD attached and
logged would fall within "object" ?
Having a full scale public key based signature implementation
would always be "better" in pure terms of systems closure, but
if a PKI costs too much, and a company was covered as above,
using cheaper solutions might work out.

@_date: 2004-01-03 13:01:58
@_author: Ian Grigg 
@_subject: digsig - when a MAC or MD is good enough? 
Battles like that will go on, although you raise an
interesting point - most docs have legal shelf life
The main observation here is that signatures, once
made, in whatever form, have a power well beyond the
bits that they consume or the paper they cover. This
law and others like it add more power, which in some
imprecise sense stacks up against the MD's recalculability.
Where it becomes interesting is if two parties in a
dispute both retain records.  If this is the case,
then it reduces the chance that someone might fiddle
with them or destroy them, as the other party has the
I suspect this makes more sense within corporates, or
for b2b scenarios.  For retail and other areas, there
are more complications.
The message digest and the record so digested can
travel different paths.  The MDs can be logged, and
the messages can be lost or disposed of.  Or some
such.  As long as the message digests are no longer
in control of a single party, they may be sufficient,
given the weight of the above, to strongly limit any
temptation to recording.
When it comes to auditing or validating of of any
records, searching on message digests is very easy.
If the message digest is with the record it covers,
it is a simple matter to quickly grep through mountains
of logs to find the entries.  It allows a positive
comparison to be done very quickly, which means those
that fail are the ones to pay attention to.
Another technique is to include a cookie in each
record which relates to the state of the log, being
a chained message digest.  If any attempt is made to
adjust a record, it throws out the following cookies.
Still, this is getting us further and further from
the original question - under what grounds could
an MD be considered a sufficient signature for
accuracy purposes?

@_date: 2004-01-04 20:24:07
@_author: Ian Grigg 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
Thanks for that!  As I'm not clear whether the status of
the paper is searching of (more, further) detailed criticisms,
I've not commented directly on Mr Bohm's remarks.  For the
most part, we are in agreement.
Rather, I'll just quickly mention where I find one large
difference of opinion:
It's pretty apparent that what passes for common sense and
knowledge of the meaning of words in the legal fraternity
doesn't necessarily translate to our world of techies.  I
found the key to this debate was in understanding the full
meaning of the word "repudiate" and that involved careful
scrutiny of several dictionaries.
The same goes for legal concepts such as presumptions,
application of law, and so forth - Mr Bohm nailed me on
my woeful understanding of rebuttals, and he'd have no
trouble nailing the average techie who asserts that private
key signatures prove this or that:  they do no such thing,
they provide evidence, yet, we still face a decade-old
obsession with constructing cryptographic systems that
purport to prove away all risks.
So, I personally don't accept the argument that common
sense can fill in the gaps.  If common sense and ordinary
knowledge had been available in such liberal doses, we
wouldn't have spent the last decade or so working with
But, it is only by going through these discussions that I
feel I now have a much firmer understanding of why non-
repudiation is a crock.  So thank you all!
Which leaves the issue of what we call the property that
differentiates a private key signature from a MAC or MD?
PS: to refresh:

@_date: 2004-01-08 10:08:18
@_author: Ian Grigg 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
The problem with this is that the squirms happen at
many levels.  It seems unlikely that we can provide
for conclusive processes when it comes to mixing
humans and tech and law.  If we try, we end up with
the Ross Anderson scenario - our work being trashed
in front of the courts.
Hence the need for a new framework.  Talk of non-
repudiation has gone to the extent of permitting
law makers to create new presumptions which - I
suggest - aren't going to help anyone.  For example,
the law that Pelle posted recently said one thing to
me:  no sane person wants to be caught dead using
these things:
   The real meat of the matter is handled in Article 31 (Page 10). "Guarantees    derived from the acceptance of a Certificate":
        "The subscriber, at the time of accepting a certificate, guarantees all the             people of good faith to be free of fault, and his information contained         within is correct, and that:         1. The authenticated electronic company/signature verified by means of this         certificate, was created under his exclusive control.
        2. No person has had access to the procedure of generation of the electronic         signature.
        3. The information contained in the certificate is true and corresponds to         the provided one by this one to the certification organization."
Is that for real?  Would you recommend that to
your mother?  I wouldn't be embarrassed to predict
that there will be no certificate systems in
Panama that rely upon that law.
I think aiming at conclusivity might be a noble
goal for protocol designers and others lower
down in the stack.  When humans are involved,
the emphasis should switch to reduction in costs:
strength of evidence, fast surfacing of problems,
sharing of information, crafting humans' part in
the protocol.
When I design financial systems, I generally think
in these terms:  what can I do to reduce the cost
and frequency of disputes?  I don't aim for any
sort of conclusivity at any costs, because that
can only be done by by setting up assumptions
that are later easily broken by real life.
Instead, I tend to examine the disputes that
might occur and examine their highest costs.
One of the easiest ways to deal with them is
to cause them to occur frequently, and thus
absorb them into the protocol.  For example,
a TCP connection breaks - did the packet get
there or not?  Conclusion: connections cannot
be relied upon.  Protocol response:  use a
datagram + request-reply + replay paradigm,
and lose a lot of connections, deliberately.
Conclusivity is achieved, at the cost of some
Another example - did the user sign the message?
We can't show what the user did with the key.
So, make the private key the agent, and give it
the legal standing.  Remove the human from the
loop.  Make lots of keys, and make the system
psuedonymous.  We can conclusively show that
the private key signed the message, and that
agent is to whom our contractual obligations
are directed.
Technical conclusivity is achieved, at the
expense of removing humans.  The dispute that
occurs then is when humans enter the loop
without fully understanding how they have
delegated their rights to their software
agent (a.k.a. private key).  We don't deny
his repudiating, we simply don't accept his
standing - only the key has standing.
Which brings us full circle to Panama :-)
Except, we've done it on our own contract
terms, not on the terms of the legislature,
so we can craft it with appropriate limits
rather than their irrebuttable presumptions.
is to presume one key and one irrebuttable
presumption.  It's a capabilities thing;
there should be a squillion keys, each with
tightly controlled and surfaced rights.

@_date: 2004-01-23 14:05:10
@_author: Ian Grigg 
@_subject: All Internet voting is insecure: report 
============================== START ==============================
All Internet voting is insecure: report
By electricnews.net
Posted: 23/01/2004 at 11:37 GMT
Get The Reg wherever you are, with The Mobile Register
Online voting is fundamentally insecure due to the architecture of the
Internet, according to leading cyber-security experts.
Using a voting system based upon the Internet poses a "serious and
unacceptable risk" for election fraud and is not secure enough for
something as serious as the election of government officials, according to
the four members of the Security Peer Review Group, an advisory group
formed by the US Department of Defense to evaluate a new on-line voting
The review group's members, and the authors of the damning report, include
David Wagner, Avi Rubin and David Jefferson from the University of
California, Berkeley, Johns Hopkins University and the Lawrence Livermore
National Laboratory, respectively, and Barbara Simons, a computer
scientist and technology policy consultant.
The federally-funded Secure Electronic Registration and Voting Experiment
(SERVE) system is currently slated for use in the US in this year's
primary and general elections. It will allow eligible voters to register
to vote at home and then to vote via the Internet from anywhere in the
world. The first tryout of SERVE is early in February for South Carolina's
presidential primary and its eventual goal is to provide voting services
to all eligible US citizens overseas and to US military personnel and
their dependents, a population estimated at six million.
After studying the prototype system the four researchers said that from
anywhere in the world a hacker could disrupt an election or influence its
outcome by employing any of several common types of cyber-attacks.
"Attacks could occur on a large scale and could be launched by anyone from
a disaffected lone individual to a well-financed enemy agency outside the
reach of US law," state the three computer science professors and a former
IBM researcher in the report.
A denial-of-service attack would delay or prevent a voter from casting a
ballot through a Web site. A "man in the middle" or "spoofing" attack
would involve the insertion of a phoney Web page between the voter and the
authentic server to prevent the vote from being counted or to alter the
voter's choice. What is particularly problematic, the authors say, is that
victims of "spoofing" may never know that their votes were not counted.
A third type of attack involves the use a virus or other malicious
software on the voter's computer to allow an outside party to monitor or
modify a voter's choices. The malicious software might then erase itself
and never be detected, according to the report.
While acknowledging the difficulties facing absentee voters, the authors
of the security analysis conclude that Internet voting presents far too
many opportunities fo

@_date: 2004-07-02 11:51:37
@_author: Ian Grigg 
@_subject: authentication and authorization 
Hi John,
thanks for your reply!
I think I'd echo Lynn's comments - it's the label
in use, so we might as well get used to it.  In
fact, the more I think of it, the more I realise
that a desire to get the right terms in place
might be part of the answer to the original question!
You are right that it's important to separate out
the two cases: the theft of the immediate account
(and money therein) which is more what phishing is,
from the acquisition of identity data in order to
open new places to steal from (credit ... see my
rant&comments on why this is an American issue and
hence may have escaped the rest of the world's attention:
Again, I see here an answer to why it is the
security industry is being ignored - all that
above is well and good in theory, but it doesn't
translate as easily to practice.  I mean, as a
hypothetical test - just how do you deliver some
form of privileges system that allows one person
to know my age, and another to know my sex, and
another to know my drinking problems?
That's not really a solved *cheap* problem, is it?
So the reality of it is, the predeliction with
identity being the root key to all power is the
way society is heading.  I don't like it, but
I'm not in a position to stop the world turning.
I think the security industry must at least
acknowledge their part in this.  For a decade
now we as a field have been telling everyone
that secure browsing with SSL and CA-signed
certs and all that stuff is ... secure.
What was that quote?  "The Netscape and Microsoft
Secure E-Commerce System" ??
In fact, we're still saying it, and mentally,
about half the field refuses to believe that
the "secure browsing" security model has been
breached.  The issue runs very deep, and a
lot of sacred cows have to be slaughtered
before this one will be resolved.
I mean, we could just go on ignoring it, but
that might explain why we are being ignored?
Well, it is true, in a sense, that as the problem
gets more expensive, there is more incentive to
fix it.  So far the banks have fiddled at the
edges with server based stuff.  But that can't
help them much.  About the only thing that can
help them directly is if they lock out other IP
numbers but that's a difficult one.
The issue is one for the client side to solve.
The user is the one who is being enticed with
the dodgy link.  So it's one of these three
agents:  user, mailer, browser.
I think if we re-characterise phishing as the
part of identity theft where accounts are stolen
directly, we might have more of an acceptable
compromise on the lingo.

@_date: 2004-07-04 08:46:44
@_author: Ian Grigg 
@_subject: Question on the state of the security industry 
Thanks Dan, and thanks Peter,
(just addressing Part one in this email)
OK.  It could well be that the community has an
inbuilt bias against protecting those that aren't
able to protect themselves.  If so, this would be
cognitive dissonance on a community scale:  in this
case, SSL, CAs, browsers are all set up to meet
the goal of "totally secure by default."
Yet, we know there aren't any secure systems, this
is Adi Shamir's 1st law.
Ignoring attacks on dimwits is one way to meet that
goal, comfortably.
But, let's go back to the goal.  Why has it been
set?  Because it's been widely recognised and assumed
that the user is not capable of dealing with their own
security.  In fact, in its lifetime over the last decade,
browsers have migrated from a "ternary security rating"
presented to the user, to whit, the old 40 bit crypto
security, to a "binary security rating," confirming
the basic principle that users don't know and don't
care, and thus the secure browsing model has to do
all the security for the user.  Further, they've been
protected from the infamous half-way house of self-
signed certs, presumably because they are too dim-
witted to recognise when they need less or more
security against the evil and pervasive MITM.
Who is thus a dimwit.  And, in order to bring it
together with Adi's 1st law, we ignore attacks
on dimwits (or in more technical terms, we assume
that those attacks are outside the security model).
(A further piece of evidence for this is a recent
policy debate conducted by Frank Hecker of Mozilla,
which confirmed that the default build and root
list for distribution of Mozilla is designed for
users who could not make security choices for
So, I think you're right.
 > Also, it is true, it was considered a
 > sub-set of SPAM.
And?  If we characterise phishing as a sub-set
of spam, does this mean we simply pass the buck
to anti-spam vendors?  Or is this just another
way of cataloging the problem in a convenient
box so we can ignore it?
(Not that I'm disagreeing with the observation,
just curious as to where it leads...)
This is actually much more serious, and I've
noticed that the media has picked up on this,
but the security community remains
characteristically silent.
What is happening now is that we are getting
much more complex attacks - and viruses are
being deployed for commercial theft rather
than spyware - information theft - or ego
proofs.  This feels like the nightmare
scenario, but I suppose it's ok because it
only happens to dimwits?
(On another note, as this is a cryptography
list, I'd encourage Peter and Dan to report
on the nature of the crypto used in the
I agree this is to be expected.  Once a
revenue stream is earnt, we can expect that
money to be invested back into areas that
are fruitful.  So we can expect much more
and more complex and difficult attacks.
I.e., it's only just starting.

@_date: 2004-07-06 00:49:09
@_author: Ian Grigg 
@_subject: authentication and authorization 
[identity theft v. phishing?]
Identity theft is a fairly well established
definition / crime.  Last I heard it was the
number one complaint at the US FTC.
Leaving that aside, the reason that phishing
is lumped in there is that it is *like* id
theft, rather than being id theft.  Just like
as many have pointed out that phishing is
*like* spam, and now we are dealing with the
fact that it is not spam.
 >
You are quoting a couple of "obscure Internet
systems" as evidence that society isn't moving
in the direction I indicated?
Yet, every day the papers are filled with the
progress the government is making on moving to
an identity-based system of control and commerce.
National drivers licences, foreigners being hit
with biometrics, etc etc.  Next time I cross the
borders, I probably have to be fingerprinted.
How many banks are introducing these obscure
features?  How many know what a capability is?
How to do a transactional security system, rather
than an identity system?
My claim seems unweakened as yet...
Curious - now that's a different phishing, but I
suppose it is close enough.  Need to think about
that one, I wouldn't call it phishing, just yet.
I'd call it invoice fraud, at first blush.
What I'd call phishing is this - mass mailings
to people about their bank accounts, collection
of the data, and then using the account details
to wire money out.
I guess we need some phishing experts to tell us
the real full definition.
It's not at issue whether you can or you can't -
what I was asserting is that no-one is asking you
(or me or anyone else).  Instead, cartels are being
formed, "solutions" being sold, congressmen lobbied,
etc, etc, and the real issues are being unaddressed.
I agree with that.
Yes, but it won't.  This is the question - why not?
Here's the question:
And here's *an* answer:
I'm afraid I agree.  The purpose seems to be to
create a cartel, suck in some fees, and ... do
some stuff.  As the fees base ensures that only
corporations join, only those with solutions to
sell have an incentive to join.  So in a while
you'll see that they have a list of preferred
solutions.  None of which will address the
problem, but they'll sure make you feel safe
from the size of the price tag.
That's a standard solution in mainland Europe
for accessing online accounts.
I'm not sure how it addresses phishing (of the
sort that I know) as the MITM just sits in the
middle and passes the query and response back
and forth, no?
Those tokens just prove that the token is on
the other end of the line.  So the password
and username wasn't stolen last week.  They
rely on the assumption that secure browsing
cannot be MITM'd, but phishing shows that
secure browsing can be MITM's.  Now, I've not
heard of anyone bothering to do a live, dynamic
MITM using phishing, but it's only a matter of
risk & reward.
(Perversely, the solution to this MITM is to
use the SSC - self-signed certs.)
Also, bear in mind that it needs both each
merchant and the consumer to adopt the system.
Pretty high barrier, really, I wouldn't hold
out too much hope.

@_date: 2004-07-07 16:39:27
@_author: Ian Grigg 
@_subject: The Ricardian Contract - using mundane cryptography to achieve powerful 
From-Mail: iang
From-Name: (((((((( Financial Cryptography Update: The Ricardian Contract ))))))))
                              July 07, 2004

@_date: 2004-07-08 16:09:11
@_author: Ian Grigg 
@_subject: EZ Pass and the fast lane .... 
From a business perspective, it makes no
sense to spend any money on crypto for this
application.  If it is free, sure use it,
but if not, then worry about the 0.01% of
users who fiddle the system later on.
It would be relatively easy to catch someone
doing this - just cross-correlate with other
information (address of home and work) and
then photograph the car at the on-ramp.
If the end result isn't as shown through
other means, then you have the evidence.
One high profile court case later, and the
chances of anyone copying this to escape
a toll fare shrink into the ignorable.

@_date: 2004-07-08 16:59:49
@_author: Ian Grigg 
@_subject: Mark Shuttleworth On Open Source 
Security Theatre:  From the man who made hundreds of
millions selling signatures on your keys:
It is your data, why do you have to pay a licence
fee for the application needed to access the data?

@_date: 2004-07-09 21:54:58
@_author: Ian Grigg 
@_subject: EZ Pass and the fast lane .... 
Well, I am presuming that ... the EZ Pass
does have an account number, right?  And
then, the car does have a licence place?
So, just correlate the account numbers
with the licence plates as they go through
the gates.
The thing about phones is that they have
no licence plates and no toll gates.  Oh,
and no cars.
What incentive does a miscreant have to
reprogram hundreds or thousands of other
Phones are great for spoofing because the
value can be high.  And, the risk of being
physically apprehended is low.  Cars and
toll ways are a different matter.

@_date: 2004-07-10 00:14:45
@_author: Ian Grigg 
@_subject: EZ Pass and the fast lane .... 
Sourceforge was doing that to me today!
No, that is to confuse the collecting of tolls
with the catching of defrauders.  Consider one
to be the automatic turnstile and the other to
be the ticket inspector.  One records the tolls,
the other looks for error conditions.
Yes, but so ineffective.  I can pass "through" the
toll gate - the cell site - and nobody can see
where I am.  I can make a call, and nobody can read
my location without doing complicated tracking stuff
with many cells.  The day that the cops get their
dream of cell phones being able to signal location,
that might change, but in the meantime, a cell phone
is for most purposes unlocatable.
Another factor is that the reward is very different,
one can save a lot more on a cellphone than a toll
way trip.
Sorry, yes:  if I catch a fraudster with a cell
phone, I can haul him down the station and seize
his phone.  BFD, it was probably stolen anyway.
If I catch a EZ Passter I can seize his car.
OK, so run this past me again.  I get to send a
virus or whatever that causes EZ Pass to go down
or mis-bill thousands of their customers, and I
also have to drive down the free way and drive
through their toll gates, in order to collect my
prize of ... a free ride on the toll way?
All of which is irrelevant.  The MP3s you are trading
do not generate a transaction request, being fraudulent
or otherwise, do not hit a server that has details on
who you are, and are probably encrypted so nobody can
tell what it is you are doing, thus forcing the cops
(IP terrorists being your  priority) to pull the car
to a halt and search for contraband music.
The only questions here are:  do the EZ Pass people have
your licence plate and your EZ Pass account number?  Do
they have the budget to employ some students with cameras?
Do they have the ability to target people who should be
travelling A -> D but keep getting billed from B -> C?
And, do the drivers who decide to defraud the EZ Pass
system have the ability to avoid 2 points, being any 2
of A, B, C, D?

@_date: 2004-07-11 00:26:06
@_author: Ian Grigg 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
This indeed is the crux of the weakness of the
SSL/secure browsing/CA system.  The concept
called for "all CAs are equal" which is an
assumption that is easily shown to be nonsense.
Until that assumption is reversed, the secure
browsing application is ... insecure.  (I of
course include "no CA" and "self-signed certs"
within the set of "all CAs.")
The essence of any fixes in the browsers should
be to address the (rather fruitful) diversity
amongst CAs, and help the user to make choices
amongst the brands of same.
Some CAs are more equal than others... and the
sooner a browser recognises this, the better.
I'm not sure I understand how logo certs would
work, as there is still the possibility of same
being issued by CA-Nigeria and having remarkable
similarity to those issued by USPTO.
Until the CA is surfaced and thrust at the face
of the user, each browser's 100 or so root CAs
will be a fundamental weakness.  Including of
course the absence of CA, which is something
that is nicely hidden from the user.

@_date: 2004-07-12 22:09:03
@_author: Ian Grigg 
@_subject: Jabber does Simple Crypto - Yoo Hoo! 
(( Financial Cryptography Update: Jabber does Simple Crypto - Yoo Hoo! ))
                              July 12, 2004

@_date: 2004-07-15 17:12:30
@_author: Ian Grigg 
@_subject: New Attack on Secure Browsing 
(((( Financial Cryptography Update: New Attack on Secure Browsing )))))
                              July 15, 2004

@_date: 2004-07-15 20:18:47
@_author: Ian Grigg 
@_subject: Humorous anti-SSL PR 
I guess the intention was to provide more end-to-end
security for transaction data.  After a reasonable start,
if a bit scattered, it breaks down with this:
     "What we can be certain of is that it is not possible
     to have a man-in-the-middle attack with FormsAssurity
     ? encryption ensures that the form has really come from
     the claimed web site, the form has not been altered,
     and the only person that can read the information
     filled in on the form is the authorized site."
Which is quite inconsistent - so much so that it seems
that the press release writer got confused over which
system he or she was talking about.

@_date: 2004-07-16 08:30:24
@_author: Ian Grigg 
@_subject: New Attack on Secure Browsing 
It's now pretty clear that PGP had no clue what this was
all about.  Apologies to all, that was my mistake.  Also,
to clarify, there was no SSL involved.
What we are looking at is a case of being able to put a
padlock on the browser in a place that *could* be confused
by a user.  This is an unintended consequence of the
favicon design by Microsoft.
Now, another thing becomes clearer, from your report and
others:  Microsoft implemented the display of the favicon
only as accepted / chosen by the user.  You have to add
this site as a favourite.
Other browsers - the competitors - went further and
displayed the favicon on arrival at the site.  I guess
they felt that it could be more useful than Microsoft
had intended.  But, in this case, it seems that they
may have stumbled on something that goes too far.
What will save them in this case is that the numbers of
users of such non-Microsoft browsers are relatively small.
If the tables were turned, and it was Microsoft that was
vulnerable, I'd confidentally predict that we would see
some attempted exploits of this in the next month's
phishing traffic.

@_date: 2004-07-16 11:06:37
@_author: Ian Grigg 
@_subject: Question on the state of the security industry 
One could postulate that the need to
notify customers, as pushed by California's
legislature, is an example of a good state
Securing those credit cards will now carry
with it the cost of carrying out all that
notification kerfuffle, and other incidental
liabilities.  This cost should easily outweigh
the cost of simple disk encryption systems.
Bob Evans is obviously trying to introduce
people gently to the gathering storm.  Is
he a softie?  Or are his editors nervous?
He missed the big one:  class action suits.
The big firms are mulling over this phishing
thing, and they don't quite smell the blood
yet, but they feel it should be there.
If I was (insert choice list of 4 companies),
I'd be having very rapid contingency meetings
on this.  But I'm not so I don't care.  Will
Kamishlian raised the spectre in this fine
contextual history essay:
I think you are too kind.  Something that wasn't
designed except as a placebo for worried execs
can't really be broken.

@_date: 2004-07-16 19:25:11
@_author: Ian Grigg 
@_subject: New Attack on Secure Browsing 
Just to clarify, there is no SSL cert involved - or
there shouldn't be?!  My original post was pointing
out that it is possible to fool users by putting a
favicon padlock in place.  This seems to work only
on non-IE browsers, as these are the ones that went
further and display the favicon without further
user intervention.
If users can be so fooled, then they can be encouraged
to enter their details as if they are logging into the
site (not PGP but say e*Trade).  Hey presto, stolen
authentication, and stolen money.
I didn't expect so much confusion on this point, but
if indeed that wasn't obvious so much the better:
that was the issue, that people could be easily

@_date: 2004-07-17 18:51:34
@_author: Ian Grigg 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
I estimated phishing losses about a month ago at about
a GigaBuck.
You'll also see two other numbers in that blog entry,
being $5 billion and $400 million (the latter taken
from Lynn's posted articles).
Of course these figures are very delicate, so we need
to wait a bit to get the real damage with any degree
of reliability.  Scientific skepticism should abound.
Notwithstanding that, I would suggest that the money
already lost is in excess of the amount paid out to
Certificate Authorities for secure ecommerce certificates
(somewhere around $100 million I guess) to date.  As
predicted, the CA-signed certificate missed the mark,
secure browsing is not secure, and the continued
resistance against revision of the browser's useless
padlock display is the barrier to addressing phishing.

@_date: 2004-07-18 02:44:49
@_author: Ian Grigg 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
By (2) I guess you mean a bypass MITM?
By (3) I guess you mean a protocol level MITM.
Then, there is:
(4) Active attacks against the client.  By this I mean
     hacking the client, installing a virus, malware,
     spyware or whathaveyou.  (This is now real, folks.)
(5) Active attacks against the server.  Basically,
     hacking the server and stealing all the good stuff.
     (This has always been real, ever since there have
     been servers.)
(6), (7) Insider attacks against client, server.
     Just read off the data and misuse it.  (This has
     been real since the dawn of time...)
Of course, SSL/SB doesn't protect against any of these,
and many people therefore assume the thinking stops
there.  Sadly, no.  Even though SSL doesn't protect
against these attacks, the frequency & cost of these
attacks directly impacts on the design choices of
secure browsing.
Sorry, I'm having trouble parsing "fairly adequate"
versus "lousy job" for threat (3)...  Both (a) and (b)
seem to deserve some examples?  I can connect directly
to expedia, and  is friendly
(Hmmm... I tell a lie, there is no as it redirects.)
(1) OK.  Now, granted, SSL protects against (1), "fairly
finely."  It does so in all its guises, although the
CA-signed variant in secure browsing does so at some
additional unneeded expense, as it eliminates certain
secure options, being SSCs and ADH.  OTOH, this is a
really rare attack - actual damage from sniffing HTTP
traffic doesn't seem to be recorded anywhere as a real
attack on people, so forgive me if I downgrade this one
as "almost not a threat."
(2) Then we come to (2), what i'd call a bypass MITM.  Or
a phish or a spoof.   (I'm not sure what "semi active"
and "infrastructure" have to do with it.)  This one is
certainly a threat.
When the browser is presented with a URL which happens
to purport only to be some secure site, without really
being that site, this is a spoof.  Your defence is to
be careful against this attack.  So, your defence is
nothing to do with SSL or secure browsing or anything really,
literally, (2) is unprotected against by SSL and secure
browsing in all their guises.  You yourself provide the
protection, because SSL / secure browsing does not.  Of
That is my point - secure browsing does not protect
against any real & present threat.
(3)  I don't understand at all.  But you suggest that
it's not your threat and it isn't protected well against.
In summary - we are left with one attack that is well
protected against, but isn't really seen that much,
and could be done with ADH.  Then, another attack that
you deal with yourself, so that's not really relevant
coz you're smart and experienced, and those using
browsers on the average are not, and they are hit by
the attack.  Then there is (3).
(And we haven't even begun on (4) thru (7).  What then,
is a threat model that only includes some threats?)
So in sum, I think my argument remains unchallenged:
secure browsing fails to secure.

@_date: 2004-07-18 12:09:39
@_author: Ian Grigg 
@_subject: On `SSL considered harmful`, correct use of condoms and SSL abuse 
(Amir, I replied to your other comments over on the
Mozilla security forum, which is presumably where they
will be more useful.  That just leaves this:)
Ha!  I wondered when someone would take me to task over
that title :-)
Here's the thing:  the title comes from a seminal paper
called "Gotos considered harmful [1]"  This was a highly
controversial paper in the 70s or so that in no small
part helped the development of structured programming.
What the author of that paper was trying to say was not
that the Goto was bad, but its use was substantially
related to poor programming practice.
And that's the point I'm making.  The Goto is just a
tool like any other.  But, the Goto became a tool over-
deployed and widely abused, as its early and liberal
use by a programmer took no account of later maintenance
costs that were incurred by the owner of the code.  So
the Goto became synonymous with bad programming and
excessive costs.
The same situation exists with SSL/TLS.  As a protocol,
it's a fine tool.  It's strong, it's well reviewed, and
it has corrected its deficiencies over time.
But, it also comes with a wider security model.  For
starters, the CA-signed regime.  As well as that, it
comes with a variety of other baggage, which basically
amounts to "use SSL/TLS as it is recommended and you
will be secure."
Unfortunately, this is wrong, and the result is bad
security practice.  Yet, we do have a generation of
people out there believing that because they have put
huge amounts of effort into implementing SSL with
its certs regime that they are secure.
We can see this ludicrous situation with the email
and chat variants of SSL / cert protected traffic.
In those cases the result is the same:  If one
suggests that the correct approach is for them to
use SSCs (self signed certs) or equivalent, people
go all weak and wobbly at the knees and start ranting
on about how those are insecure.
Yet these same systems are totally open to attacks
at the nodes and often to the intermediate hops,
which of course is where 99% of the attacks are [2].
These programmers truly believe that in order to
get security, they must deploy SSL.  As the manual
tells them to.  They are truly wrong.  In this,
SSL has harmed them, because it has blinded them
to the real risks that they are facing.
It's not the tool that has hurt them, but as you
suggest the abuse of the tool.  Edsgar Dijkstra
called for the abolition of Gotos as the way to
address the harm he saw being done.  That solution
may offend, as the tool itself cannot have harmed.
But, how else can we stop people deploying the tool
so abusively?
[1] Edsger W. Dijkstra, "Go To Statement Considered Harmful,"
[2] Jabber's use of SSL seems to mirror STARTTLS.
They both protect the traffic on the wire, but not
at rest on the hops.  The certificate system built
into mailers (name?) at least organises an end-to-end
packet protection, thus leaving the two end nodes
as the places at most risk, still by far the most
likely place to be attacked.

@_date: 2004-07-18 12:38:07
@_author: Ian Grigg 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
That's what I mean.  The normal security checks
ot the system have been bypassed, in this case,
by having the browser think that this is a
different, never seen before URL that doesn't
need checking.
Well, there are two things here:  firstly, as you
recognise, there are protocols that protect against
these things (some better than others, but that's
not a debate for the moment).
   "Of course, SSL/SB doesn't protect against any of these,
   and many people therefore assume the thinking stops
   there.  Sadly, no.  Even though SSL doesn't protect
   against these attacks, the frequency & cost of these
   attacks directly impacts on the design choices of
   secure browsing."
The assumption here that the SSL crowd made was
something like what you said - No COMSEC protocol
protects against these things.  But, that doesn't
mean that we now have an excuse to implement a
COMSEC protocol and ignore those things.  No, in
contrast, it means that if we are in the unfortunate
position of having to implement a COMSEC protocol
that only achieves fairly minor protection, we
should recognise the limitations of that, and not
try any harder to get it right than is dictated
by the other threats to the system.
E.g., if credit cards are being stolen in the
thousands by hackers, insiders and viruses (the
first two are true at least) then there may not
be much point in protecting them on the wire with
anything more serious than 10 bit crypto and ADH.
Right, now I understand.  Apologies for disagreeing
in advance, but that's now what we'd consider to be
an unlikely future for most normal practice.  Nobody
is going to take the customer's click URL feature
(Reference here to Amir's paper, and my comments on
branding box.)  I think we are stuck with the identities
we have now.  URLs aren't going to change, and not-
withstanding calls for the abolition of this or that,
the SSL/TLS/CA structure is here to stay.
Right...  It's easy to claim that "it went away"
because we protected against it.  Unfortunately,
that's just a claim - there is no evidence of
This is why I ask whether there has been any
evidence of MITMs, and listening attacks.  We
know for example that there were password
sniffing attacks back in the old days, by
hackers.  Hence SSH.  Costs -> Solution.
But, there is precious little to suggest that
credit cards would be sniffed - I've heard one
isolated and unconfirmable case.  And, there is
similar levels of MITM evidence - anecdotes and
some experiences in other fields, as reported
here on this list.
In absence of enough data, we have to construct
some sort of crook's equation to determine whether
it is a threat.  Every analysis of what a crook
does shows that sniffing credit cards is the
poorest way to acquire them.  And conducting an
MITM is just dumb - I suppose we could bother to
worry about MITM in order to catch the dumbest of
the dumb, but I'm not sure why anyone would want to
spend a dime doing that, as they almost certainly
are going to present other cheaper opportunities.
Further, there is a non-trivial proportion of
activity where credit cards are in fact sent
over the net - to merchants that simply don't
care about the rules, and can't afford the
high price set by the SSL system.  Likewise,
there is a huge amount of unprotected traffic
of other forms, like legal documents, but we
never hear of people being caught sniffing
The balance of evidence suggests that there is
no high or serious threat to credit card info
moving across the net in the clear.  Nothing
worth paying for, at the least.
No.  My argument is that one shouldn't wear
body armour because it costs money to buy,
it is heavy, causes chaffing of the skin,
and the point of it is lost as no-one is
going to shoot you as you walk around the
I don't see what is so ridiculous about that.
Neither does the rest of humanity.  It's only
the cryptography community that likes to create
the FUD, and then pretend to solve it, by
telling people that they are being ridiculous
for not wearing their body armour.  Or by
mandating it.
Perhaps we should ask the residents of the
greater Washington, DC area how many of them
decided to wear body armour when that sniper
was at large?  A few, I'll bet.  But 99% of
the people maintained their sense of reality
and went about their lives with only a nod to
the new situation - why?  Because they knew it
was ridiculous to consider changing their
patterns when the risk levels changed in such
an immeasurable fashion.
(Quick calculation - 4 million in the area?
More than 100 deaths per day due to other
causes.  What's one more?  The answer is:
don't panic.  Don't wear body armour.)
The threat model is really quite clear.  Yes, we
all agree on that.  What is not clear is that it
obtains.  Try and show that - it's quite hard to
 > Yes, it's
Ah, so a) it's a con job, so it's important to
recognise that we don't protect against it, because
the victim was conned and we don't protect the
stupid.  Curiously, most don't agree.  The whole
browser community has worked on the principle that
the system of secure browsing is designed to protect
those who aren't as adept as the power users.  So,
phishing is right there in the ambit.
Then, b) it's secure browsing that we are talking
about, not just the wire protocol - so we need
to include SSL/TLS, the CA-signed certificate
model that comes joined at the hip with it (check
the RFC for warnings) and also the security
presentation within the browser, which is strongly
influenced by the other two things.  That all is
what is failing.
Then, c) as a matter of fact, secure browsing,
and SSL/TLS *and* certs could do a mighty fine
job of protecting against phishing - because the
relationship exists, and the browser knows that.
The problem is, the browser does its best to hide
the relationship, when it should be doing its
best to surface the relationship a user has with
her bank.
Which leads to d) which is the assumption that
in secure browsing, the connection that needs to
be protected is one whereby there is no prior
relationship.  That's why we need the CA-signed
cert, right?  Because we don't have a prior
relationship with the server, we need someone,
a TTP, to stand in and provide that.
That assumption is wrong.  In general, and in
detail, but with phishing, it's completely the
reverse - there is a prior relationship, and that
relationship is strong.  It's called a bank account.
Finally e), all logical and connected because
security practice should be a science, not a belief
system, and we have the fact that any certificate
can protect the user's relationship with her bank,
because it measures the *persistence* of that
relationship.  So, SSCs have much to offer in
protecting against phishing, in fact, almost
as much as CA-signed certs (unless TCA/branding
comes into play).
Which leaves f) that if the SSL/TLS infrastructure
were to be used to protect against phishing, then
we would have to acknowledge that SSCs can do a
good job, in which case, why did we spend so much
time destroying their reputation in the last decade?
And, g) it is much much easier to simply state over
and over that phishing is a con job and the user
should protect themselves.  Elsewise, we have to
look into the abyss of what we really meant by
the assumption that there is no prior relationship.
Well, I apologise for writing all the above before
reading your last words.  You and me both:

@_date: 2004-07-18 17:45:19
@_author: Ian Grigg 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
In practice something like this:  Most of the
money is wired through to some stolen account,
and then moved out of the system to another system.
This might be a foreign account, or it might be a non-
bank such as a broker/dealer (E*Trade is being hit at
the moment, it seems) or it might be a digital gold
currency.  From there, it is moved once or twice more,
than back to the country where the phisher is.  This
might be the US or Russia, or anywhere else, but those
two countries seem to be quite big on this (maybe we
should blame Reagan :-) )
A couple of things:  it is very hard, but not impossible
to reverse a SWIFT style international wire.  I've seen
it done once, so I know it is not impossible.  If the
cash has gone, then reversing it doesn't make sense.
Also, phishing
isn't exactly a recognised and obvious criminal case.
Any particular instance might be, but getting to that
determination might take months.  Further, opening
accounts for anonymous purposes is still rather easy
in many countries, the chief perpertrator of this being
the USA.  Finally, every attempt to make money less like
money (by closing off easy accounts, for example) results
in what some call "unintended consequences" - the money
goes elsewhere.

@_date: 2004-07-20 07:07:49
@_author: Ian Grigg 
@_subject: Using crypto against Phishing, Spoofing and Spamming... 
thanks for addressing the issues with some actual
anecdotal evidence.  The conclusions still don't
hold, IMHO.
The trick is to show cause and effect.  We know the
effect and we know the cause(s).  The question is, how
are they related?  The reason it is important is that
we may misapply one cause if the effect results from
some other cause.
Which led to SSH, presumably, and was pre-credit card
days, so can only be used as a prediction of eavesdropping.
Question - are we facing a situation today whereby it is
easy to eavesdrop from the backbone of a major ISP and
capture a lot of traffic?  As far as I can see, that's
not likely to happen, but it could happen.
Secondly, who were the people doing those attacks?  Back
in 93-94, I'd postulate they weren't criminal types, but
hacker types.  That is, they were hackers looking for
machines.  Those people are still around - defeated by
SSH in large measure - and use other techniques now.
(Hackers had no liability in those days.  Criminals do
have liability, and are more concerned to cover their
tracks.  This makes active attacks less useful to them.
Criminals are getting braver though.)
Thirdly, why aren't we seeing more reports of this on
802.11b networks?  I've seen a few, but in each case,
the attack has been to hack into some machine.  I've
yet to see a case where listeners have scarfed up some
free email account passwords, although I suppose that
this must happen.
The point of all this is that we need to establish how
frequent and risky these things are.  Back in the pre-
commerce days, a certain amount of FUD was to be expected.
Now however, it's been a decade - whether that FUD was
warranted then is an issue for the historians, but now
we should be able to scientifically make a case that
the posture matches the threats.  Because it's been a
decade (almost).
As far as I can see, there *some* justification for
expecting eavesdropping attacks to credit cards.  There
is a lot more justification with unprotected non-commerce.
And in contrast, there is little justification for
expecting active attacks for purposes of theft.
What this leads to is not whether SSL should have been
deployed or changed in its current form (it is fruitless
to debate that, IMHO, except in order to lay down the
facts) but a discussion of certificates.
There seems some justification in suggesting that SSL be
(continued to be) deployed in any form.  Mostly, IMHO,
in areas outside commerce, and mostly, in the future,
not now.
There seems a lot of justfication for utilising certs as
they enable relationship-protection.  There seems quite a
bit of justification for utilising CA-signed certs because
they permit more advanced relationship protection such as
Amir's logo ideas and my branding ideas, and more so every
What there doesn't appear to be any justification for is
the effective or defacto mandating of CA-signed certs.
And there appears to be a quite serious cost involved in
that mandating - the loss of protection from the resultant
*very* low levels of SSL deployment.
This all hangs on the MITM - hence the question of frequency.
It seems to be very low, an extraordinarily desparate attack
for a criminal, especially in the light of experience.  He
does phishing and hacking with ease, but he doesn't like
leaving tracks in the infrastructure that point back to him.
If the MITM cannot be justified as an ever-present danger,
then there is no justification for the defacto mandating of
CA-signed certs.  Permitting and encouraging self-signed
certs would then make deployment of SSL much easier, and
thus increase use of SSL - in my view, dramatically -
which would lead to much better protection.  (Primarily
by relationship management on the client side, and also
by branding/logo management with the CAs, but that needs
to be enabled in code first at the browsers.)
(It has to be said that encouraging anon-diffie-hellman
SSL would also lead to dramatically improved levels of
SSL deployment and thus protection as well.  But since
RFC deprecates that, it simply becomes a higher burden
to show.)
Well, I think the thing here would be that credit cards
are sent in forms, so POSTs, and some sort of scanning
and interpretation would be required.  If it was an issue,
it would be a mildly interesting problem to design and
test scanning software.  But it's not an issue, so we
don't hear of it.  I believe the reason we don't hear of
it is that even over the open nets, the notion of scanning
for masses of credit cards would be too risky.  Poor ROI,
to use Lynn's term.  I think eavesdropping is a much bigger
risk for other purposes, and deploying more SSL would address
that.  But, for reasons outlined logic above, deployment of
SSL remains constrained.
(That's plausible, and has some good observations which
I'd love to debate but feel the need to focus on SSL and
now and low levels of deployment and thus protection.)
PS: in all the above I refer to SSL as the protocol,
sans key exchange.

@_date: 2004-07-22 19:09:22
@_author: Ian Grigg 
@_subject: Identity theft case could be largest so far 
From other reports, the indictment alleges that "Levine
gained access ... by misusing a legitimate password and user
name while working for a company doing business with Acxiom."
I.e., not even a hack, but an insider theft.

@_date: 2004-07-27 09:28:51
@_author: Ian Grigg 
@_subject: dual-use digital signature vulnerabilityastiglic@okiok.com 
It's also cost-effective.  The CA model as presented
is too expensive.  If a group makes the decision to
utilise the infrastructure for signing or encryption,
then it can significantly reduce costs by rolling out
from the centre.
I see this choice as smart.  They either don't do it
at all, or they do it cheaply.  This way they have a
(Then, there is still the option for upgrading to self-
created keys later on, if the project proves successful,
and the need can be shown.)
As a landmark, I received my first ever correctly
signed x.509 message the other day.  I've yet to find
the button on my mailer to generate a cert, so I could
not send a signed reply.  Another landmark for the
future, of course.

@_date: 2004-06-01 16:52:50
@_author: Ian Grigg 
@_subject: Yahoo releases internet standard draft for using DNS as public 
That's correct.  But, the goal is not to secure
email to the extent that there is no risk, that's
impossible, and arguing that the existence of a
weakness means you shouldn't do it just means that
we should never use crypto at all.
See those slides that Adi Shamir put up, I collected
the 3 useful ones in a recent blog:
I'd print these three out and post them on the wall,
if I had a printer!
The goal is to make it more difficult, within a
tight budget.  Using TLS for SMTP is free.  Why
not do it?
(Well, it's free if self-signed certs are used.
If CA-signed certs are used, I agree, that exceeds
the likely benefit.)
a) Once a bunch of people send mail via TLS/SMTP,
the ISP is incentivised to look at onward forwarding
it that way.
b) It may be that your local threat is the biggest,
if for example you are using 802.11b to send your
mail.  The threat of listening from the ISP onwards
is relatively small compared to what goes on closer
to the end nodes.
c) every node that starts protecting traffic this
way helps - because it boxes the attacker into
narrower and narrower attacks.  It may be that the
emails are totally open over the backbone, but who
cares if the attacker can't easily get there?

@_date: 2004-06-01 18:50:52
@_author: Ian Grigg 
@_subject: Yahoo releases internet standard draft for using DNS as public 
This I believe is a bad way to start looking
at cryptography.  There is no system that you
can put in place that you can *rely* upon to
protect your message.
(Adi Shamir again:  there are no secure systems,
ergo, it is not possible to rely on them, and
to think about relying will take one down false
In general terms, most ordinary users cannot
rely on their platform to be secure.  Even in
specific terms, those of us running BSD systems
on laptops that we have with us all the time
still have to sleep and shower...  There are
people out there who have the technology to
defeat my house alarm, install a custom
key logger designed for my model of laptop,
and get out before the hot water runs out.
For that reason, I and just about everyone
else do not *rely* on tech to keep my message
safe.  If I need to really rely on it, I do what
Adolf Hitler did in November of 1944 - deliver
all the orders for the great breakout by secure
courier, because he suspected the enigma codes
were being read.  (He was right.)
Otherwise, we adopt what military people call
"tactical security:"  strong enough to keep
the message secure enough so that most of the
time it does the job.
The principle which needs to be hammered time
and time again is that cryptography, like all
other security systems, should be about risk
and return - do what you can and put up with
the things you can't.
Applying the specifics to things like TLS and
mail delivery - yes, it looks very ropey.  Why
for example people think that they need CA-signed
certs for such a thing when (as you point out)
the mail is probably totally unprotected for half
the journey is just totally mysterious.

@_date: 2004-06-09 12:31:59
@_author: Ian Grigg 
@_subject: threat modelling tool by Microsoft? 
Has anyone tried out the threat modelling tool
mentioned in the link below, or reviewed the
book out this month:
"The Threat Modeling Tool allows users to create threat
model documents for applications. It organizes relevant
data points, such as entry points, assets, trust levels,
data flow diagrams, threats, threat trees, and vulnerabilities
into an easy-to-use tree-based view. The tool saves the
document as XML, and will export to HTML and MHT using
the included XSLTs, or a custom transform supplied by
the user."
"The Threat Modeling Tool was built by Microsoft Security
Software Engineer Frank Swiderski, the author of Threat
Modeling (Microsoft Press, June 2004)."

@_date: 2004-06-30 11:49:17
@_author: Ian Grigg 
@_subject: Question on the state of the security industry 
The phishing thing has now reached the mainstream,
epidemic proportions that were feared and predicted
in this list over the last year or two.  Many of
the "solution providers" are bailing in with ill-
thought out tools, presumably in the hope of cashing
in on a buying splurge, and hoping to turn the
result into lucrative cash flows.
In other news, Verisign just bailed in with a
service offering [1].  This is quite cunning,
as they have offered the service primarily as
a spam protection service, with a nod to phishing.
In this way they have something, a toe in the
water, but they avoid the embarrassing questions
about whatever happened to the last security
solution they sold.
Meanwhile, the security field has been deathly
silent.  (I recently had someone from the security
industry authoritively tell me phishing wasn't
a problem  ... because the local plod said he
couldn't find any!)
Here's my question - is anyone in the security
field of any sort of repute being asked about
phishing, consulted about solutions, contracted
to build?  Anything?
Or, are security professionals as a body being
totally ignored in the first major financial
attack that belongs totally to the Internet?
What I'm thinking of here is Scott's warning of
last year:
   Subject: Re: Maybe It's Snake Oil All the Way Down
   ...
   >When I drill down on the many pontifications made by computer
   >security and cryptography experts all I find is given wisdom.  Maybe
   >the reason that folks roll their own is because as far as they can see
   >that's what everyone does.  Roll your own then whip out your dick and
   >start swinging around just like the experts.
I think we have that situation.  For the first
time we are facing a real, difficult security
problem.  And the security experts have shot
their wad.
[1] Lynn Wheeler's links below if anyone is interested:
VeriSign Joins The Fight Against Online Fraud
[2] sorry, the original email I couldn't
find, but here's the snippet, routed at:
 at minder.net/msg01435.html

@_date: 2004-05-07 00:52:39
@_author: Ian Grigg 
@_subject: The future of security 
I would see these things, in no particular
order, and no huge thought process applied.
a.  a hype cycle in QC that will peak in a year
or two, then disappear as purchasers realise that
the boxes aren't any different to ones that are
half the price.
b.  much more use of opportunistic cryptography,
whereby crypto systems align their costs against
the risks being faced.  E.g., self-signed certs
and cert caching in SSL systems, caching and
application integration in other systems.
c.  much less emphasis on deductive no-risk
systems (PKIs like x.509 with SSL) due to the
poor security and market results of the CA
d.  more systems being built with basic, simple
home-grown techniques, including ones that are
only mildly secure.  These would be built by
programmers, not cryptoplumbers.  They would
require refits of proper crypto as/if they migrate
into successful user bases.  In project terms,
this is the same as b. above - more use of
opportunistic tactics to secure stuff basically
and quickly.
e.  greater and more costs to browser users
from phishing [1] will eventually result in
mods to security model to protect users.  In
the meantime, lots of snakeoil security solutions
will be sold to banks.  The day Microsoft decides
to fix the browser security model, phishing will
reduce to a "just another risk."
f.  arisal of mass crypto in the chat field,
and slow painful demise of email.  This is
because the chat protocols can be updated
within the power of small teams, including
adding simple crypto.  Email will continue to
defy the mass employment of crypto, although
if someone were to add a "create self-signed
cert now" button, things might improve.
g.  much interest in simple crypto in the p2p
field, especially file sharing, as the need
for protection and privacy increases due to
IP attacks.  All of the techniques will flow
across to other applications that need it less.
h.  almost all press will be in areas where
"crypto is sure to make a difference."  Voting,
QC, startups with sexy crypto algorithms, etc.
i.  Cryptographers will continue to be pressed
into service as security architects, because it
sounds like the same thing.  Security architects
will continue to do most of their work with
little or no crypto.
j.  a cryptographic solution for spam and
viruses won't be found.  Nor for DRM.
[1] one phisher took $75,000 from 400 victims:

@_date: 2004-05-11 17:02:28
@_author: Ian Grigg 
@_subject: The future of security 
Curious!  Do you mean, in the attempt to
stop aggressors doing traffic analysis,
such as was attempted by ZKS?
Or that the commercial world will engage
in mass scale traffic analysis?  If so, how?
Most traffic isn't accessible by the world,
so I can't see that coming to pass.

@_date: 2004-05-26 22:41:03
@_author: Ian Grigg 
@_subject: The future of security 
(Most of the people on this list are far too
professional and busy to fall for that.  If
the argument has merit, please summarise it.
If it really has merit, the summary might
tease people into reading the full paper.)
I for one don't see it.  I like hashcash as
an idea, but fundamentally, as Steve suggests,
we expect email from anyone, and it's free.
We have to change one of those basic features
to stop spam.  Either make it "non-free," or
make it "non-authorised."  Hashcash doesn't
achieve either of those, although a similar
system such as a payment based system might
achieve it.
Mind you, I would claim that if we change either
of the two fundamental characteristics of email,
then it is no longer email.  For this reason,
I predict that email will die out (ever so
slowly and painfully) to be replaced by better
and more appropriate forms of chat/IM.

@_date: 2004-11-01 16:41:33
@_author: Ian Grigg 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, 
On whether the goal of smart cards is to reduce
a)  Not with any systems I was familiar:  the major Dutch
systems were defensive, oriented to filling the space
that was potentially threatened by other parties.  The
trials were goaled to increase security, which they did
not by using smart cards, but by eliminating cash, which
had created an unacceptable risk of serious theft in
unattended petrol stations.  The same happened with UK
phone cards...  I'm unfamiliar with Mondex or the Belgium/
Proton based motives, but their structures indicate that
liability was not a question uppermost on their minds.
b)  Liability reduction cannot be a goal.  If it was, then
one could achieve the goal completely - eliminate liability -
by not doing the project.  Instead, liability and/or
reduction of same is a _limitation_ on the goal of the
c)  Whether liability reduction entered into any smart
card system as a limitation on their goals is a little
uncertain.  I would say no, as all the systems were
early stage in the institutional model;  in which case
there was little or no liability.  Instead, the only
drivers in that vague area would have been future
running costs reduction, which would have included well
considered security models, and partially considered
user support models, to reduce over all costs.  Including
all forms of risks, of course.
d)  Liability reduction generally comes into play when a
system is mature and/or regulatory issues come into play.
That is, liability reduction is something often seen when
the desire is to avoid surprises, and to avoid any costs
cropping up that weren't well built into the costs model.
I.e., the risk models used by credit card operators are
one example, and the customer agreement models (or whatever
they are called) used by CAs are another example of liability
e) Perversely, banks practice liability increase as well as
reduction.  In fact, a pure banking model is about the risk
of a loan, and they specialise in measuring and managing
the risk of that loan.  But, as we are talking about payment
systems, and loans are banking, and banking is not payment
systems, that would be a change in business, so out of
scope of the original topic.
f)  And, of course, all institutions will practice liability
increase if they can turn it into a barrier to entry, that
is, cartelise the industry so as to block new entrants.  See
the eMoney directive for the European barrier to entry, which
was effectively coordinated by the Bundesbank on behalf of
the banks, and resulted in the "like a bank, but not a bank,
and as costly as a bank" approach to digital cash.
All of which might or might not hit the target of liability
as you wrote it?

@_date: 2004-11-06 22:21:42
@_author: Ian Grigg 
@_subject: Your source code, for sale 
This is to mix up banking and payment systems.  Enzo's
description shows banks doing banking - lending money
on paper that eventually pays a rate of return.  In
contrast, in the DGC or digital gold currency world,
the issuers of gold like e-gold are payment systems and
not banks.  The distinction is that a payment system
does not issue credit.
So, in the e-gold scenario, there would need to be
similar third parties independent of the payment system
to provide the credit moving in the reverse direction to
the goods.  In the end it would be much like Enzo's
example, with a third party with the seller, a third
party with the buyer, and one or two third parties who
are dealing the physical goods.  There have been some
thoughts in the direction of credit creation in the
gold community, but nothing of any sustainability has
occurred as yet.

@_date: 2004-11-07 08:02:50
@_author: Ian Grigg 
@_subject: Your source code, for sale 
What is wrong with having a TTP, generally called a

@_date: 2004-11-30 12:07:14
@_author: Ian Grigg 
@_subject: SSL/TLS passive sniffing 
Hi Ben,
That's what most of us really are, to be fair.  Crypto is
such a small part of security that most all crypto people
move across to general security once they realise there
isn't much work around for a pure crypto person.  Which is
good, because only in the general security field can one
really make a difference, IMHO, because that's when starts
to understand what is needed, as opposed to what's cool.
OK.  A nice challenge to assumptions!
I note that disctinction well!  Certificate based systems
are totally vulnerable to a passive sniffing attack if the
attacker can get the key.  Whereas Diffie Hellman is not,
on the face of it.  Very curious...
( I had at first thought this would be related to the many
and various ways to acquire a "forged cert" but it is not.
You have to have access to the actual key in use at the
time, to conduct a passive attack.  Mind you, if you had
access to a forged cert then you could conduct an active
MITM.  But that then equates it to an active attack against
Diffie Hellman. )
Well.  I would say that "knowing" is rather difficult in
precise terms, even for the people who were involved in
the essential design of the first version of the PKI.  But,
for what it is worth, here are my impressions, derived from
a couple of years of (amateur) research into why SSL/PKI
is so ... obscure?  Well, pick your own term.
In the design of the SSL protocol and its associated PKI
using x.509 certs, there are a lot of decisions that are
questionable on security grounds.
There are a lot of ideas that simply don't make a lot of
sense if one is thinking from a pure security sense.  By
way of quick example, they were nominally trying to protect
credit cards from being sniffed, which would be a low value
attack.  But, they decided to downgrade things like Diffie
Hellman, which would easily have defeated a credit card
sniffing attack, by forcing in place an MITM, which would
have left easy tracks to follow.  And they downgraded the
self-signed cert, which properly employed defeats phishing
(they weren't to know that then, but clearly their view
of attacks was incomplete).  There is more and if you are
interested I'd invite you to spend an afternoon on the
rants at  .
So, eventually, in searching for the driving forces behind
the protocol, one is directed away from "defeating the
threat" that was nominally on paper to other areas.  And the
goal that seems to come up most is "using CA-signed certs."
If one thinks in terms of a protocol designed to use lots of
certs, then SSL would fit that bill;  it explains in some
sense why the user cannot just create their own cert and
use that to talk to their bank.  Instead, the bank must
demand a cert, and thus demand a particular cert, one
which presumably came with specific requirements as okayed
by the bank.  It explains why email using the certs is a
dead duck;  because there is no button on people's mailers
to create a user cert.
Now, I always assumed that what was happening here was that
the original advisers to the SSL team were trying to set up
a franchise to sell certs.  There is quite a lot of evidence
to this effect.  The whole pre-crash history of Verisign is
basically that;  RSADSI had a *lot* to do with it, and they
had a financial interest in Verisign.  In the commentary of
the times, you will see repeated reference to "the search for
the revenue model."  If you recall, Netscape was caught
between a wildly successful IPO and a big barrel of cash,
and no way to return any investment to the shareholders ...
(a bit like google is now ;)
So my personal theory is that the emphasis was placed on
"the cert secures all" because "the cert will make us lots
of money."  (History proved them wrong in one sense, in that
cert sales are only 100k per annum which is not enough to
justify all the fuss.  But, in another sense, the original
shareholders of Verisign, well, they would disagree and say
that it made them a bucket of money.  But that's all after
the fact, and irrelevant to why they did what they did.)
Either way, that's my theory.  If you view SSL / PKI as a
protocol designed to sell certs, a lot of it makes a lot
more sense.  It's just a theory!

@_date: 2004-11-30 12:38:12
@_author: Ian Grigg 
@_subject: SSL/TLS passive sniffing 
Ben raises an interesting thought:
We have the dichotomy that DH protects against all passive
attacks, and a signed cert protects against most active attacks,
and most passive attacks, but not passive attacks where the
key is leaked, and not active attacks where the key is
"forged" (as a cert).
But we do not use both DH and certificates at the same time,
we generally pick one or the other.
Could we however do both?
In the act of a public key protected key exchange, Alice
generally creates a random key and encrypts that to Bob's
public key.  That random then gets used for further traffic.
However could one do a Diffie Hellman key exchange and do this
under the protection of the public key?  In which case we are
now protected from Bob aggressively leaking the public key.
(Or, to put it more precisely, Bob would now have to record
and leak all his traffic as well, which is a substantially
more expensive thing to engage in.)
(This still leaves us with the active attack of a forged
key, but that is dealt with by public key (fingerprint)
Does that make sense?  The reason I ask is that I've just
written a new key exchange protocol element, and I thought
I was being clever by having both Bob and Alice provide
half the key each, so as to protect against either party
being non-robust with secret key generation.  (As a programmer
I'm more worried about the RNG clagging than the key leaking,
but let's leave that aside for now...)
Now I'm wondering whether the key exchange should do a DH
within the standard public key protected key exchange?
Hmmm, this sounds like I am trying to do PFS  (perfect
forward secrecy).  Any thoughts?

@_date: 2004-11-30 15:32:35
@_author: Ian Grigg 
@_subject: SSL/TLS passive sniffing 
(which I calculated as 98% DHE-xxx)
Great stats, guys!  Can either/both comment on what proportion
of connections you are seeing that use STARTTLS as opposed to
not using STARTTLS?

@_date: 2004-10-10 16:11:37
@_author: Ian Grigg 
@_subject: AES Modes 
Has anyone kept up to date with AES modes?
I'm looking for basic mode to encrypt blocks (using AES)
of about 1k in length, +/- an order of magnitude.  Looking
at the above table (2nd link) there are oodles of proposed
It would be nice to have a mode that didn't also require
a separate MAC operation - I get the impression that
this is behind some of the proposals?

@_date: 2004-10-11 13:08:13
@_author: Ian Grigg 
@_subject: AES Modes 
Zooko provided a bunch of useful comments in private mail,
which I've edited and forward for list consumption.

@_date: 2004-10-11 23:07:01
@_author: Ian Grigg 
@_subject: AES Modes 
Jack Lloyd also passed along lots of good comments I'd
like to forward (having gained permission) FTR.  I've
edited them for brevity and pertinence.
 > If it's small messages, CCM would probably work pretty well. Personally I think
 > CCM is really poorly designed (in terms of easy implementation/usage), but take
 > a look. There is also EAX, which is IMO significantly nicer. There are a ton of
 > others (most of the ones on the page you link to support encrypt+MAC), but it
 > seems like EAX and CCM are the only two that are going anywhere (many of the
 > others are patented and/or rather painful to implement).
 >
 > CCM and EAX are both going to be slower than AES+HMAC because they use AES in
 > some variant of CBC-MAC. Some of the others have faster MACs, mostly ones based
 > on universal hash functions, but the best of them (OCB in particular) have been
 > patented.
I'm obviously being naive here ... I had thought that
the combined mode would be faster, as it would run through
the data once only, and that AES seems to clip along
faster than SHA1.
Are you saying that as far as speed goes, I may as well
do EAS (using CBC) and add a HMAC on the end?
Or are you saying that only the patented ones manage to
deliver the savings we all expect?  Hmm, reading about
OCB on Phil Rogaway's site does clarify this somewhat.
============== To which jack replied:
 >>I'm obviously being naive here ... I had thought that the combined mode would
 >> be faster, as it would run through the data once only, and that AES seems to
 >> clip along faster than SHA1.
AFAIK all of the modes that use only one block cipher invocation per block of
input are patented. EAX+CCM both use two AES operations per block, and
byte-for-byte SHA-1 is 2-5x faster than AES (at least in the implementations
I've seen/used/written), so using AES+HMAC is probably going to be faster than
AES/EAX or AES/CCM. The obvious exception being boxes with hardware AES chips
and slow CPUs (eg, an ARM7 with an AES coprocessor), where AES will of course
be much faster than SHA-1.
 >> Are you saying that as far as speed goes, I may as well do EAS (using CBC)
 >> and add a HMAC on the end?
At least on general purpose CPUs, yes.
 >> Or are you saying that only the patented ones manage to deliver the savings
 >> we all expect?  Hmm, reading about OCB on Phil Rogaway's site does clarify
 >> this somewhat.  Pretty much. Though I just remembered that CWC has not been patented by it's
creators, but I wouldn't be at all surprised if it was covered by one of the
others. Even CWC is probably slower than AES+HMAC is software, though
apparently it's pretty fast in hardware.

@_date: 2004-10-19 21:14:50
@_author: Ian Grigg 
@_subject: Printers betray document secrets 
I don't think this is new - I'm pretty sure it was
published about 6 or 7 years back as a technique.

@_date: 2004-10-19 21:30:40
@_author: Ian Grigg 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, real 
Hi John,
We are way way past that point in security,
phishing is happening on an industrial scale, and
the virus, phish and spam people are united, or
at least working together.  Internet payment
systems are being DDOS/extorted on a regular
basis, and hack attempts are routine.
We literally already have that world.
Again, we're well past that point.  There have been
hundreds and hundreds of payment systems out there,
and maybe order of a thousand have failed by now,
mostly due to business reasons.  Some simply due
to hacks and attacks, but it is rare, because:
What happens is that beyond a certain threshold, the
payment system delivers valuable payments.  At that
point, it starts getting attacked.  If those attacks
are survived, then it moves on to the next phase.
Which would be more attacks of a different nature...
(In fact, one seems to have failed in the last few
days - EvoCash -  and another is on the watch list
for failure - DMT/Alta.  Both of them suffered from
business style attacks it seemed, rather than what
we would call security hacks.)
The notion that suddenly it's all over isn't what
happens.  It's a trickle, then it builds up to a
flood.  Some small hacks come in, and people either
look at them or they don't.  Those that are diligent
and keep an eye on these things respond.  Those that
don't go out of business.  There are more dead
payment systems than people on this list, I'd guess,
we do have plenty of experience in this.
In practice, we've also seen what happens when
money that gets stolen can't be traced or stopped.
Even though not "bearer", systems like e-gold are
plenty anon enough, and they don't easily reverse.
I doubt bearer systems would necessarily face a
problem because of users losing their bearer tokens
(but there are plenty of other problems out there
like the rather hard insider theft problem).
When we get to that point, we will have an answer
for him.  I can assert that with a fair degree of
confidence, because a) we can't ever get to that
point until we have an answer, and b) we already
have the answer, and have had it for a decade:
store it on a trusted machine.  Just say no to
Windows XP.  It's easy, especially when he's
storing a bearer bond worth a car.

@_date: 2004-10-21 17:20:24
@_author: Ian Grigg 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, real 
None.  But a machine that had one purpose in life:
to manage the bearer bond, that could be trusted
to a reasonable degree.  The trick is to stop
thinking of the machine as a general purpose
computer and think of it as a platform for one
single application.  Then secure that machine/OS/
stack/application combination.
Oh, and make it small enough to fit in the pocket,
put a display *and* a keypad on it, and tell the
user not to lose it.

@_date: 2004-10-22 11:45:59
@_author: Ian Grigg 
@_subject: Are new passports [an] identity-theft risk? 
Who was it that pointed out that radio waves don't
interfere, rather, receivers can't discriminate?

@_date: 2004-10-23 12:07:39
@_author: Ian Grigg 
@_subject: How to store the car-valued bearer bond?  (was Financial identity...) 
Theoretically, there may not be much difference, depending
on where the theory starts...
Practically there are a bunch of differences, which are
more or less issues, depending.
1.  The data store (a.k.a. the smart card) is separated
from the IO package.  Is this an advantage or a disadvantage?
For the most part it gives the user 2 tokens to worry about,
the expense of an additional interface, and more mass, as you
point out.  I can't quite see any offsetting advantage myself
in all that over one box that does the lot.  So that's a minus.
2.  The data store is in some sense secure.  If it's got
a car-valued bearer bond on it, that's probably not
secure enough.  It might give some security in the event
of loss, but so would a combined package with some other
password on it.  It is a marginal security improvement
over a single purpose non-smart package, and thus would
have a primary benefit in marketing (see Blue).  It's a
plus, but a small plus, as a single-purpose package could
just build in a smart card if it so desired.
3.  The smart card interface is not good.  It has to be
taken out of your trusted reader and put in someone else's
trusted reader.  Bad news.  So someone else's trusted
reader tells you it is paying you dividends on your bond,
when in fact it is replacing the bond with a mickey mouse
loyalty coupon.  Getting around that disadvantage costs
systems operators a bundle of money and restrictions.
This makes for a huge minus.
4.  The smart card interface, part 2.  In practice, smart
card readers are an example of historical detritus.  We
all said "next year is the year of the smartcard" in 1995,
and it still is.  In practice, the interfaces we want on our
bearer bond hardware token are these:  802.11x, ethernet,
bluetooth, IR, ... in that approximate order, all with IP
layered over and our real hot bearer transfer protocol, and
not some hokey old telco thing.  The smart card interface is
another huge minus, because it means that the infrastructure
is all specialised, the protocols are all closed, and the
system is all controlled at some level or other, which means
some big fella has to dig deep in the pockets to finance it.
Score card so far:  2 big minuses, one small minus, and
a small plus.
Next year is the year of the smart card!  In practice,
that advantage is just a rationalisation.  We can't use
any of those tokens to store your bearer bond.  If we
are going to ask someone to store a bearer bond, we
have to give that person the token.  Which means we can
start with a blank sheet of paper, we don't need to use
any smart card patriotism to justify your choices.

@_date: 2004-10-24 14:11:41
@_author: Ian Grigg 
@_subject: Are new passports [an] identity-theft risk? 
I have another question:  aren't RFIDs of this
nature all passive, and need to be excited in
some fashion?  It would seem that detecting the
powering signal may well be a way to defend
against unexpected reads.
OK, so your view would be that SSNs are just
indexes into databases and therefore are not
a threat to anyone.  Very silly of the US congress
to have put restrictions on their use, then...
Mind you, in Australia, they are even less
humours.  I hear, you go to jail if you are
caught using TSNs for anything but the tax
records for your employees.
Do you have anything to back up that "reasonable
surmise" ?  I'd say the DeCSS program, and any
analysis of the application, and any amount of
experience with similar apps like smart card money
would say the reverse is true.  Distributing data
that is private to a large quantity of readers
with pre-issued hardware and data is a hard problem,
especially if you can't use public key crypto (but
don't imagine public key crypto solves all the
Oodles of experience dictate that the information
on the RFIDs will be available to a huge number
of bureaucrats, for free, and anyone who wants
to purchase it, for a market price.  Going on
prior figures seen, I'd say the price would be
order of $10 - $100 (the price of doing business,
as the data cost would be marginal==0), and the
end result is that it would all be bundled up in
the standard package of identity for some given
Actually, I think the people you are dealing with
there are ingenious in a hive like fashion.  They
will come up with a good reason to have them
available for read without the user's knowledge.
The questions would then be, what frequency do these
things operate on, what power is required to power
them up, and what power is required to ... power them
down.  Any radio guys around?

@_date: 2004-10-25 11:23:25
@_author: Ian Grigg 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, real 
I think the recent decision by Microsoft to not upgrade
browsers indicates that they are plumbing for your choice
(1).  Backwards compatibility takes a back seat.  I wrote
more about it here:
I have two questions:  Does he have a board?  I
never heard of anyone but Bill Gates telling Ballmer
what to do.  Just curious!
Secondly, is a VM strategy likely to work?  Assuming
that Microsoft can make it work nicely, it also opens
the door for other OSs to be added into the mix, something
that Microsoft wouldn't be that keen to promote.
(I don't disagree with your comments, though!)

@_date: 2004-10-25 22:31:48
@_author: Ian Grigg 
@_subject: Financial identity is *dangerous*? (was re: Fake companies, real 
It should be obvious.  But it's not.  A few billions
of investment in smart cards says that it is anything
but obvious.
To be fair, the smart card investments I've been
familiar with have been at least very well aware of
the problem.  It didn't stop them proceeding with
papering over the symptoms, when they should have
gone for the underlying causes.

@_date: 2004-10-25 23:19:05
@_author: Ian Grigg 
@_subject: Printers betray document secrets 
It's actually quite an amusing problem.  When put
in those terms, it might be cheaper and more secure
to go find some druggie down back of central station,
and pay them a tenner to write out the ransom demand.
Or buy a newspaper and start cutting and pasting the
In more scientific terms, is there a way to efficiently
print an anonymous paper document?  (By anonymous,
I mean a document that leaves no easy clues back to
the author.)  When creating ones anonymous political
pamphlets revealing the latest government scandal,
one might need the help of RFC 666, "how to print
anonymous pamphlets with modern printers."
E.g., something like:  acquire a HP inkjet and a
Brother laser.  Disengage the ink drying fan in the
Brother.  Print the page through the Brother then
print the same page (wet!) through the HP within 5
seconds.  For paper, use fish&chip wrap, cleaned
with sarsons and dried for 30 mins under a tanning
lamp with the UV filter removed...

@_date: 2004-10-27 16:01:08
@_author: Ian Grigg 
@_subject: MCI set to offer secure two-way messaging with strong encryption 
MCI set to offer secure two-way messaging with strong encryption
By William Jackson,
GCN Staff
MCI Inc. will offer secure two-way messaging through its SkyTel
Communications subsidiary next month, encrypting wireless text
with the Advanced Encryption Algorithm.
?It was initially designed to meet the security needs of our
government customers,? SkyTel marketing director Michael Barnes
The company plans to get the device for its Secure 2Way service
certified for Federal Information Processing Standard 140-2,
which applies to cryptological devices used by the government.
The company also is promoting the service as compliant with the
Health Insurance Portability and Accountability Act and expects
the health care and financial service industries to be early users.
Text messaging and paging has emerged as a reliable?sometimes
the only?means of communication during emergencies that disrupt
other media, such as wired and cellular telephone systems and
the Internet.
The Secure 2Way service uses the handheld ST900 2Way messaging
device from Sun Telecom Inc. of Norcross, Ga. Messages are
encrypted between the device and an encryption server at SkyTel?s
secure network operations center.
Two levels of service are offered. Device-level security provides
device-to-device encryption when both users have the ST900. When
messages are received from nonsecure devices, traffic is encrypted
only between the operations center server and the ST900. With
end-to-end security, all traffic is blocked except that from
other secure ST900 devices so there is no unencrypted link on
any message.
The service uses 128-bit encryption keys with AES and the ANSI
X9.63 key management standard for symmetrical keys. The National
Security Agency has approved AES with 128-bit keys for use up to
secret classification. The key on each device is automatically
changed every 30 days or after 5,000 messages. The initial key
generation and exchange takes about eight minutes. Subsequent
key changes take two or three minutes.
Each device also is password protected with an eight-character
alphanumeric password.
?It was tough to build the AES encryption into the device,?
Barnes said. ?It is not done through add-on hardware.?
After buying the ST900, there?s no extra charge for the device-
level service. End-to-end service incurs an additional fee.
? 1996-2004 Post-Newsweek Media, Inc. All Rights Reserved.

@_date: 2004-09-15 16:30:54
@_author: Ian Grigg 
@_subject: pci hardware for secure crypto storage (OpenSSL/OpenBSD) 
There is a device that is similar to those characteristics:

@_date: 2004-09-16 00:12:54
@_author: Ian Grigg 
@_subject: potential new IETF WG on anonymous IPSec 
That seems to be the meaning of putting "Opportunistic"
and "Encryption" together.
Bill, you have a knack for putting this in context.
Historically, it's possible to see why Gilmore went
with the no-risk security, and reduced deployment of
FreeSWAN by an order of magnitude or more.
But, these days, it seems like a no-brainer:  there
is no such thing as an easily accessible trustworthy
PKI.  (I am recalled to mind the Hettingarian creed of
"only financial guaruntees are trustworthy guaruntees...")
And, the ones who have a government annoyed at them
probably know they need special care....  I've not met
a revolutionary that didn't know that the government
is shooting at them.
So the question is, how do we get FreeSWAN to use
opportunistic cryptography, sans DNS?

@_date: 2004-09-16 00:41:41
@_author: Ian Grigg 
@_subject: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from 
It occurs to me that a number of these ideas could
be written up over time ... a wiki, anyone?  I think
it is high past time to start documenting crypto
FWIW, on opportunistic cryptography (a la "SSH model")
I wrote up a opportunistic draft paper here:
(FTR, I've received substantial comments from TwanVDS
and DigbytT.)

@_date: 2004-09-16 18:12:48
@_author: Ian Grigg 
@_subject: public-key: the wrong model for email? 
I think the consensus from debate back last year on
this group when Voltage first surfaced was that it
didn't do anything that couldn't be done with PGP,
and added more risks to boot.  So, yet another biz
idea with some hand wavey crypto, which is great if
it works, but it's not necessarily security.
Yes, or worse;  it turns out that Verisign may very
well be the threat as well as the solution.  As I
wrote here:
Verisign are in the eavesdropping business, which
not only calls into doubt their own certs, but also
all other CAs, and the notion of a trusted third
party as a workable concept.

@_date: 2004-09-17 15:54:09
@_author: Ian Grigg 
@_subject: How to implement a self-destructing message. 
In terms of threats actually seen in the real world
leading to costs, etc, I would have thought that the
subpoena / civil / criminal case would be the largest.
In this case, the threat might be something like:
   - Recipient forwards plaintext to someone who
     forwards it to someone who is a threat, where
     the number of links between Recipient and Threat
     are from 0 to many.  Zero means, one year later,
     Recipient becomes threat.
         - Hard for the sender to detect and work around.
         - Could be mitigated by contract provisions,
           such as email clients that automatically
           attach "Confidential" tags on or otherwise
           arrange for emails to be excepted from civil
           proceedings *.
         - Could the email clients use digsigs to
           evidence entry into confidential comms?
As this threat is real, persistent and growing in
popularity, the obsession of perfectly covering more
crypto-savvy threats seems .. unbalanced?
Ahhhh, now if one could implement a message that self-
destructed on the recipient's machine, that would
start to improve security against the above outlined
threat.  I've toyed with the notion of integrating
contracts negotiation into clients, such that mailers
automatically delete messages agreed earlier to have
a TTL.
But, it seems that even in the chat world, there are
vast numbers of people that routinely save every chat
message / session.  So it needs to be an advisory
negotiation only.  Hence, my thought that if we could
add a contract / in-confidence / without prejudice
label on the message, even if the recipient kept a
copy (via override) then at least it could be locked
out of civil court proceedings *.
*  In some sense or other, if the term "WITHOUT
PREJUDICE" is put on correspondence, that makes it
confidential and protects it from being brought in
to civil proceedings.  Normal IANAL caveats apply.

@_date: 2004-09-17 19:35:09
@_author: Ian Grigg 
@_subject: public-key: the wrong model for email? 
Oh, that's really easy.  Each mailer (MUA) should (on
install) generate a self-signed cert.  Stick the fingerprint
in the headers of every mail going out.  An MUA that sees
the fingerpring in an incoming mail can send a request email
to acquire the full key.  Or stick the entire cert in there,
it's not as if anyone would care.
Then each MUA can start encrypting to that key opportunistically.
Lots of variations.  But the key thing is that the MUA
should simply generate the key, sign it, and send it out
on demand, or more freuqently.  There's really no reason
why this can't all be automated.  After all, the existing
email system is automated, and trusted well enough to
deliver email, so why can't it deliver self-signed certs?

@_date: 2004-09-19 07:15:05
@_author: Ian Grigg 
@_subject: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from 
It sounds like an RFC / BCP would be a good target.
I suspect given the controversy over a lot of these
ideas an intermediate phase would be needed where
the controversy could be aired in depth, before being
summarised into a BCP.
 From that pov, a wiki + discussion list leading to a
BCP would seem like a good idea.
Alternatively, let all these things be thrown into
the mixing pot and see what happens?

@_date: 2004-09-19 03:55:49
@_author: Ian Grigg 
@_subject: [anonsec] Re: potential new IETF WG on anonymous IPSec (fwd from 
It possibly requires both.  A mailing list by itself
tends to generate great thoughts that don't get finished
by being turned into summaries.  Also, those in charge
tend to slow the process, just through being too busy.
(I'm not talking about just this list, I've noticed
the effect on RFC lists where the editor wakes up after
a week and skips all the debate and starts again.)
A wiki working with a mailing list might address both
those issues.
(It's just a guess, I've never really worked with a
Wiki, just read some entries over at wikipedia.)

@_date: 2004-09-30 00:03:13
@_author: Ian Grigg 
@_subject: Customs and Excise Electronic Returns 
PKI, and the Customs & Excise's, mistake was to assume that a
key is only useful if it is signed by someone else.  From a
system point of view, this will require massive benefits to
make it work.  As there are few massive benefits if any over
a username / password combo, then that's what they'll use.
It might be worth pointing out to them that the US' Government
Accounting Office is downbeat on the use of PKI.

@_date: 2005-04-10 15:02:15
@_author: Ian G 
@_subject: Garfinkel analysis on Skype withdrawn? 
Has anyone got a copy of the Skype analysis done by Simson
Garfinkel?  It seems to have disappeared.
-------- Original Message --------
I am Italian, so forgive any possible error or whatever regards the
English language. I read your article on mail-archive.com
( at metzdowd.com/msg03305.html)
and I am so interested in reading what Simson Garfinkel has written
about skype.
Unfortunately the link you posted in the message is now broken
If you have this article saved on your hard disk could you please send it to me?
Best regards
Vito Catozzo

@_date: 2005-08-02 15:40:49
@_author: Ian Grigg 
@_subject: Ostiary 
I recently wrote this as a login program that was
hard coded to run the commands concerned.
The reason for doing this instead of the Ostiary
approach is that SSH had to be running anyway,
and SSH provides the key management regime.
Without that, I'd have to invent my own which
in Ostiary's case was the Hashing mechanisms.
So on this point it would come down to whether
we cared enough to replace SSH's authentication
regime, which I'd think would be rarer (perhaps
in the embedded market where Unix doesn't need
Also, efficiency of command sending was not
an issue - each send was about 10 seconds in
my tests.
I think it is smart to keep things simple regardless
of ones expertise :)  Also, I wouldn't overdo the
"hackability" argument.  If flaws are found, you'll
find time to fix them, and for the cost of a few
hacked boxes, you'll have the benefit of a lot
more secured boxes.

@_date: 2005-08-04 14:59:51
@_author: Ian Grigg 
@_subject: [Clips] Escaping Password Purgatory 
I think PwdHash also does this for browsers (probably Firefox):

@_date: 2005-08-13 12:17:34
@_author: Ian G 
@_subject: The summer of PKI love 
I have collected the criticism I've seen over many years
against PKI into this document:
It's long :)

@_date: 2005-08-14 20:14:54
@_author: Ian G 
@_subject: ID "theft" -- so what? 
Well, blaming a protocol which is an inanimate
invention of man is always unfair, but so is
avoiding the issues by quibbling on the meanings.
Blaming HTTP is totally unfair as it never ever
promised to protect against spoofs.
PKI+SSL promised to detect and cover spoofs.  In
fact, the original point of PKI was to close out
the MITM or spoof, and was then enlarged somewhat
confusingly to provide some sort of commerce
guarantee on the stated identity (c.f, Lynn's
amusing stories of CAs gone mad with dollarlust.)
Originally, Netscape's browser implemented the
complete anti-spoofing UI and included more info
on the screen.  This was then dropped in the
screen wars, against the advice of security
engineers at Netscape.  (Ref:  comments by Bob
So, to repeat:    It's not the certificate
level but the business and architecture level.
The *people* equation.  It's the people who
implement the PKI+SSL model and don't do it
properly that are the root cause of phishing.
Petnames, Trustbar, DSS are some of the solutions
that work *positively* and *constructively* to
close the loopholes in the browser's implementation
of PKI+SSL.

@_date: 2005-08-24 17:00:21
@_author: Ian G 
@_subject: Another entry in the internet security hall of shame.... 
In another routine event in the adventure known as
getting security to work in spite of the security,
I just received this ...
When creating a google talk compatible IM personality in Apple's iChat you
get the following warning on the Google Help pages:
12.	Check the boxes next to 'Connect using SSL' and 'Allow self-signed
certificates.' You don't need to check the box next to 'Warn before
password is sent insecurely' -- your password is always secure with Google
Congratulations! You are now ready to connect to the Google Talk service
using iChat.
Once you've configured iChat to connect to the Google Talk service, you may
receive a warning message that states your username and password will be
transferred insecurely. This error message is incorrect; your username and
password will be safely transferred.

@_date: 2005-08-25 21:51:46
@_author: Ian G 
@_subject: Another entry in the internet security hall of shame.... 
Perfectly acceptable over chat, no?  That is,
who else would you ask to confirm that your
chatting to your buddy?

@_date: 2005-08-25 21:55:06
@_author: Ian G 
@_subject: Another entry in the internet security hall of shame.... 
none of the above.  Using SSL is the wrong tool
for the job.  It's a chat message - it should be
encrypted end to end, using either OpenPGP or
something like OTR.  And even then, you've only
covered about 10% of the threat model - the
But, if people do use the wrong tool for the
job, they will strike these issues...

@_date: 2005-08-27 08:46:50
@_author: Ian G 
@_subject: e2e all the way (Re: Another entry in the internet security hall 
yes, this is the optimisation that makes Skype work,
it is (probably) vulnerable to an MITM at the center.
This is a tradeoff.  What it means is that the center
can do an active attack.  But it can't do a passive
attack (this is speculation but it seems reasonable
or at least achievable).
That's a good deal for users, when you consider their
alternative.  Fantastic value for money, really, it's
really very hard to criticise...
The evidence suggests that if you just slap an SSL
tunnel in place, you end up with an ongoing mess of
key management - ref - what this thread started with
from google.  If you do the thing properly, and
build it opportunistically, with the option of
upgrading to signed certs for those that really
want that, you can avoid all that.  But few do, for
some reason, or maybe those successful cases we just
never hear about because they work without fuss...
When SSL is your hammer, everything gets nailed as
a server.
On this I think we'd all agree.  Although I'd also
add that it should be economic - if it doesn't deploy
then it does not good.
It's fascinating that you see this and I wish you'd
share the threats you see.  I see only node threats,
you see only wire threats.  Why is this?
(I can quote reams and reams of news articles that
point to merchant data losses and PC malware and virus
attacks... but it would be boring.  Just ask Lynn for
his feed ...)
My view of the p2p threat model:
   other party - 70%
   own node    - 20%
   center      - 10%
To an accuracy of +/- X%.  Obviously, the wire
threats - that are protected by Jabber's SSL and the
like - are in the noise somewhere there (but I expect
them to get much more aggressive in the future).
Another way of looking at this is to ask what the damage
is.  If your chat traffic is breached by some random
threatening outsider, what does he gain?  Nothing, so
it doesn't take a PhD to realise nobody's interested.
But if your listener is a *related* other party and
has your messages, then that's a whole other story...
This is why for example the most popular IM security
system is the discarded nym.

@_date: 2005-08-27 09:17:11
@_author: Ian G 
@_subject: Another entry in the internet security hall of shame.... 
*That* makes much more sense, ignore my earlier email.
   "Secure web servers are the equivalent of heavy armored cars.
   The problem is, they are being used to transfer rolls of
   coins and checks written in crayon by people on park benches
   to merchants doing business in cardboard boxes from beneath
   highway bridges. Further, the roads are subject to random
   detours, anyone with a screwdriver can control the traffic
   lights, and there are no police."

@_date: 2005-08-29 20:06:04
@_author: Ian G 
@_subject: Another entry in the internet security hall of shame.... 
Right.  And do the primary authentication of the key
using some other mechanism that is outside the strict
(IOW, Dave, your plan will work, as long as it is
built from ground up with no prior baggage!  IMHO!)
This is such a no-brainer that when I first came
across the solution over a decade ago now, I never
gave a thought as to how it could be anything but
the one way to do things.  It just works, and very
little else works anywhere as well.
Yet, we are still grubbing around like cavemen in
the mud.  And then there is this:
$5M  Mobile ID for Credit Card Purchases
WHO: John Occhipinti, Woodside Fund, Redwood Shores, Calif.
WHO HE IS: A former executive at Oracle and Netscape, Occhipinti is a managing director and security specialist, leading investments in BorderWare and Tacit.
WHAT HE WANTS: Fraudproof credit card authorization via cell phones and PDAs.
WHY IT'S SMART: Credit card fraud is more rampant than ever, and consumers aren't the only ones feeling the pain. Last year banks and merchants lost more than $2 billion to fraud. Most of that could be eliminated if they offered two-part authentication with credit and debit purchases -- something akin to using a SecureID code as well as a password to access e-mail. Occhipinti thinks the cell phone, packaged with the right software, presents an ideal solution. Imagine getting a text message on your phone from a merchant, prompting you for a password or code to approve the $100 purchase you just made on your home PC or at the mall. It's an extra step, but one that most consumers would be happy to take to safeguard their privacy. More important, Occhipinti says, big banks would pay dearly to be able to offer the service. "It's a killer app no one's touched yet," Occhipinti says, "but the technology's within reach."
WHAT HE WANTS FROM YOU: A finished prototype application within eight months. "I'm looking for the best technologists in security and wireless, the top 2 percent in their industry," Occhipinti says. The team would need to be working with a handful of banks and merchants ready to start trials, in hopes of licensing the technology or selling the company.
SEND YOUR PLAN TO: jco at woodsidefund.com
The funniest part of all is that even though we
know how to do it in our sleep, Paypal actually
built one as their "original offering" and threw
it away...
Yup.  But this will only work if you go back to
basics and build the structure naturally around
the keys.  IOW, not using anything from PKI.
Watching security thinking advance is like watching
primates evolve from close distance.  Either we die
of old age before anything happens, or we get clubbed
to death...

@_date: 2005-08-31 13:44:25
@_author: Ian G 
@_subject: Another entry in the internet security hall of shame.... 
Having read this now [1] I wonder if it is too hopeful
to expect TLS-PKS to be "widely adopted" in browsing.
( I've guessing that you mean that the user's password
and username will be used to bootstrap the secure TLS
session - notwithstanding the comment in section 8 that
this is not the intention [2]. )
The issue I see here is that while the browser may have
access to this data, the server doesn't necessarily
have access to it.  In these days and times, major
websites are constructed with a plethora of methods
to do authentication, and they use a lot frameworks
to handle all that.  In any given framework, the
distance (in code and layers and backends) between
the TLS code and the password code can be quite
large.  One artifact of this is the use of straight
forms to deliver the password rather than use the
inbuilt underlying unix-style password mechanism;
it is far too popular to implement the password
authentication of a user over the top of any
framework as it is - in the application code - as
the framework never quite does what is needed.
Not only is there this distance, it is duplicated
across all languages and all the different auth
regimes and also for "homegrown" password auth,
over every application!  I'd wonder if given these
barriers it will ever be possible to get change to
Or have I misunderstood something here?
(Note that this shouldn't be interpreted as saying
anything about the general utility of TLS-PSK in
other environments as per [2]...)
[1] Pre-Shared Key Ciphersuites for Transport Layer Security (TLS)
     [2] "However, this draft is not intended for web password
     authentication, but rather for other uses of TLS."

@_date: 2005-12-02 18:40:09
@_author: Ian G 
@_subject: Session Key Negotiation 
One reason is that one side or the other might have
a screwed implementation.  For example, an RNG that
spits out zeroes.
Another reason is that one side or other might have
reasons for screwing the key deliberately;  a server
might for example fix its key so that it can be
listened to outside.  If a simple XOR is negotiated,
then the server could always choose its part to
XOR to a certain value.  This is plausible if a
server operator has "done a deal" to reveal to an
eavesdropper, but doesn't want to reveal its private
key.  (I suspect the newer ciphersuites in TLS may
have been motivated by this.)
Hence, slop in lots of random from both sides, and
hash the result, so you have at least the key space
of the one side that is behaving well.

@_date: 2005-12-04 14:51:29
@_author: Ian G 
@_subject: [Clips] Banks Seek Better Online-Security Tools 
I have not!  I declined the chance when my
bank told me that I had to download their
special client that only runs on windows...
However, I have used and/or written many
online DGC tools (which is for the sake of
this discussion, gold-denominated online
payments) which are honed through experience,
incentive and willingness to deal with the
( As an aside, e-gold was generally the first
to be hit by these problems as well as all the
other problems that have only effected banks
in passing.  Generally the DGC sector is much
more savvy about threats, through repetitive
losses, at least. )

@_date: 2005-12-05 09:24:04
@_author: Ian G 
@_subject: [Clips] Banks Seek Better Online-Security Tools 
That part I agree with, but this part:
George's story - watching my Ameritrade account get phished out in 3 minutes
Seems like a hopeful categorisation!

@_date: 2005-12-06 10:20:17
@_author: Ian G 
@_subject: [Clips] Banks Seek Better Online-Security Tools 
Not a bad summary.  I'd say that when one is
dealing with any such crime, there are always
unanswered questions, and issues of confusion
(probably as much for the attacker as the victim).
Well, even though phishing has been discussed
on this list for about 2 years, it is only in
the last 6 months or so that there has been a
wider acceptance in the subject.  I think your
specific question has been asked so many times
that people's eyes glaze over.
Only in the last few *weeks* did two of the browser
manufacturers acknowledge it publically.  So I
wouldn't expect too much from the banks, who have
to receive authoritive press, institution & regulatory
input before they will shift on matters of security.

@_date: 2005-12-23 18:48:59
@_author: Ian G 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
Indeed.... and even if that gets fixed we still have
to contend with:
   * the blog software can't handle the nature of a
     TLS site (internal problems like non-working
     trackbacks, internal links, posts, ...)
   * the cert has to be shared with 3 other sites
   * Firefox will still warn about it being a CAcert
     signed certificate
   * ...  I'm sure there's more.
Hopefully over the next year, the webserver (Apache)
will be capable of doing the TLS extension for sharing
certs so then it will be reasonable to upgrade.
PS:  SSL v2 must die!  Wot, you mean you haven't
turned it off in your browser yet?

@_date: 2005-12-24 12:59:07
@_author: Ian G 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
The best info I know of on the subject is here:
Philipp has a script which he claims automates
the best method(s) described within to create
the alt-names cert.
(The big problem of course is that you can use
one cert to describe many domains only if they
are the same administrative entity.)
What we really need is for the webservers to
implement the TLS extension which I think is
called "server name indication."
And we need SSL v2 to die so it doesn't interfere
with the above.

@_date: 2005-12-24 17:27:58
@_author: Ian G 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
Certainly they *can* share a cert.  But a cert
speaks to identity - at the human level the cert
is supposed to (by some readings) indicate who
the site is purporting to be and in some scenarios,
there are people who think the cert actually
proves that the site is who it claims to be.
So regardless of the technical details of the
underlying software (complex, I grant), websites
SHOULD NOT share a cert.
(by capitals I mean the RFC sense, not the shouting
If browsers don't know what is available on the
server, they send a Hello message that asks for
what protocol versions and ciphersuites to use.
This is the SSL v2 message, just in case.... so
to rectify this situation we need to get all
the browsers distro'd with SSL v2 turned off by
default.  The shorthand for this is "SSL v2 must
die..."  Thankfully, they did decide to do just
that at last month's browser pow-wow.

@_date: 2005-12-24 18:50:08
@_author: Ian G 
@_subject: browser vendors and CAs agreeing on high-assurance certificat 
1. Because the activity is being done "in the name
of" the site.  When a business "signs" or otherwise
represents a site as purporting to be in the name of
some business, we still want to do it in a way that
separates out that business from every other.
2. The system operator has access to the private
keys, yes, but he's just the agent, and this does
not mean that anyone else has access.  We have
systems in place to separate out the protection
of the keys from the rest of the business.
Most small businesses have some level of cooperation
where they share techies, systems, and other services,
so it is probably more seen and more useful in the
SOHO (small office home office) world.  Of course,
this is less interesting to the security world,
because there isn't the money to pay for consultants
All the more reason why the software should provide
the best it can for free!
Well, sure.  For many uses it will be a useful
stopgap measure, until SNI is deployed.  It's
only broken if you like a binary world, and you
happen to fall on the zero side of the question.

@_date: 2005-02-01 19:17:20
@_author: Ian G 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
That's correct.   is a group that
specialises in dealing with that market.  Curiously,
they have found evidence of MITMs from those
attackers, although I only have anecdotal accounts,
so nothing firm to report.
If one is actually going up against governments
("ours" or "theirs") then one needs to take more
care.  Downloading just any crypto tool from the
net and using it without thought is a death warrant
with some of these use cases.  (That's not an
exaggeration, or so I've been told.)
Note however, that these users know to take more
care; in that OSI funded the report (presumably)
for that very use case.  The report may very well
accurately be read as "not suitable if your life
depends on it."
But, that has only limited bearing on those users
without a clearly identified life-threatening enemy.
What Skype aught to do - and clearly don't - is
list the limitations of the product clearly on their
website.  The main concern that people have is
that because they say it is secure, nobody trusts
them.  If they said it was insecure in X,Y,Z ways,
then people would trust them more (after verifying
that X,Y,Z was true).
But getting to a world where people will list the
security weaknesses honestly is a challenge that
we all face, on both sides of the crypto debate.

@_date: 2005-02-02 23:38:46
@_author: Ian G 
@_subject: Dell to Add Security Chip to PCs 
So .. the way this works is that Dell & Microsoft
ship you a computer with lots of nice multimedia
stuff on it.  You take control of your chip by erasing
it and regenerating keys, and then the multimedia
software that you paid for no longer works?
I'm just curious on this point.  I haven't seen much
to indicate that Microsoft and others are ready
for a nymous, tradeable software assets world.

@_date: 2005-02-02 23:25:44
@_author: Ian G 
@_subject: VeriSign and Conflicts of Interest 
((( Financial Cryptography Update: VeriSign and Conflicts of Interest )))
                           February 02, 2005

@_date: 2005-02-03 03:57:06
@_author: Ian G 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
The user doesn't select the trust path, the
browser manufacturer does.  It is a bug to
think that the user trusts the CA.  She
doesn't even know their names, let alone
whether she would trust them, in the current
The existing method is that the root list is
chosen by methods arcane and obscure,
which may have to do with user benefit,
or may not.  Either way, the user is given
a root list that is long and chosen and hidden.
How do you suggest the user deals with
this list?  Given that the average list has
100+ entries...
What Amir and Ahmad are looking at is
showing the CA as part of the trust equation
when the user hits a site.  Some CAs will
enter the user's consciousness via normal
branding methods, and new ones will
trigger care & caution.  Which is what
we want - if something strange pops up,
the user should take more care.

@_date: 2005-02-03 15:38:42
@_author: Ian G 
@_subject: Can you help develop crypto anti-spoofing/phishing tool ? 
Michael H. Warfield wrote
I'd suggest you have a quick browse through
their paper, skip the words and look for the
graphics.  It will show it faster than these 1000
In one word, it is 'branding.'  In many words,
it goes like this:  TrustBar allows the user to
'sign off' on her favourite banking sites, which
means when that cert is seen it shows a logo
that the user is familiar with.  It also shows
the logo of the CA, which is something that
the user is familiar with.
Note that this is not a popup with techie
messages in it, but an 'advert' that appears
on the chrome.  On the basis of the recognition
of the cert, which belongs to that site, the
browser shows the bright coloured advert
for the bank and for the CA.
Now, a phisher, to attack that, would have to
acquire a cert from the same CA, and get the
user to also sign off on that cert as being her
bank.  Which is hard to do because she already
has signed off on her bank.
So what happens under attack is that the brand
adverts change, and the user should notice that.
This is in effect what branding is, it is a message
to you to notice when you are not drinking your
favourite cola brand, and to make you feel guilty
or something.
So, to use a little handwaving, we do know how
to make the user notice that she is in a different
place - by using the brand concepts that marketing
as a science and art has used for many a century.

@_date: 2005-02-04 19:46:39
@_author: Ian G 
@_subject: Is 3DES Broken? 
It seems that the block size of an algorithm then
is a severe limiting factor.  Is there anyway to
expand the effective block size of an (old 8byte)
algorithm, in a manner akin to the TDES trick,
and get an updated 16byte composite that neuters
the birthday trick?
Hypothetically, by say having 2 keys and running
2 machines in parallel to generate a 2x blocksize.
(I'm just thinking of this as a sort of mental challenge,
although over on the OpenPGP group we were toying
with the idea of adding GOST, but faced the difficulty
of its apparent age/weakness.)

@_date: 2005-02-09 19:22:05
@_author: Ian G 
@_subject: A cool demo of how to spoof sites (also shows how TrustBar preventsthis...) 
Yes, this was asked over on the cap-talk list.
Below is what I posted there.  I'm somewhat
sympathetic as doing a real field trial which
involves testing real responses to a browser
attack raises all sorts of heisenberg uncertainty /
experimental method issues.  Off the top of
my head, I think this is a really tricky problem,
and if anyone knows how to test security
breaches on ordinary users, shout!
I agree it wasn't much.  But it was a bit more than
just a multiple choice:
  "The second goal of the third question was to evaluate whether the use of TrustBar is likely to improve the ability of users to discern between unprotected sites, protected sites and spoofed (fake) sites. For this purpose, we gave users a very brief explanation on the TrustBar security indicators, and then presented three additional screen shots, this time using a browser equipped with TrustBar. Again, the screen shots are presented in Appendix B, and each was presented for 10 to 15 seconds, taken using Mozilla in the Amazon web site. We leave it as a simple exercise to the reader to identify the protected, unprotected and spoofed (fake) among these three screen shots.
  "The results provide positive indication supporting out belief that the use of TrustBar improves the ability of (na?ve) web users to discern between protected, unprotected and fake sites. Specifically, the number of user that correctly identified each of the three sites essentially doubled (to 21, 22 and 29).
That would rate as a simulation rather than
a field trial, I guess.

@_date: 2005-02-09 21:08:45
@_author: Ian G 
@_subject: A cool demo of how to spoof sites (also shows how TrustBar preventsthis...) 
I understand this is a theoretical question, but
here is an answer:
The plugin is downloadable from a MozDev site,
and presumably if enough attention warrants it,
Amir can go to the extent of signing it with a
cert in Mozilla's code signing regime.
Also, as Amir is a relatively well known name in
the world of crypto I suppose you could consider
his incentives to be more aligned with delivering
good code than code that would do you damage.

@_date: 2005-02-14 15:02:04
@_author: Ian G 
@_subject: critical bits in certs 
Has anyone got any experience or tips on critical
bits in certificates?  These are bits that can be
set in optional records that a certificate creator
puts in there to do a particular job.  The critical
bit says "don't interpret this entire certificate
if you don't understand this record."
x.509 certs have them, they are mentioned in RFCs
Also, OpenPGP may have them (I recall arguing against
them a while back, never checked where it all ended).
The reason I ask is that a CA has started issuing
certs with an optional critical section.  It has a
good reason to do this ... but the results aren't
pretty, and the CA is now asking browser manufacturers
to accept its certs and/or "comply" with the crit.
Many issues are swirling around, so it seems useful
to ask around.

@_date: 2005-02-16 22:33:06
@_author: Ian G 
@_subject: SHA-1 cracked 
Stefan Brands just posted on my blog (and I saw
reference to this in other blogs, posted anon)
saying that "it seems that Schneier forgot to
mention that the paper has a footnote which
says that the attack on full SHA-1 only works
if some padding (which SHA-1 requires) is not
I think this might be an opportune time to introduce a
new way of looking at algorithms.  I've written it up
in draft (excuse the postit notes) :
In short, what I do is apply the concepts of the econ
theory of "Pareto efficiency" to the metric of security.
This allows a definition of what we mean by "secure"
which is quite close to colloquial usage;  in the
language so introduced, I'd suggest that SHA-1 used
to be Pareto-complete, and is now Pareto-secure for
certain applications.  I have a little table down
the end that now needs to be updated!
Comments welcome, it is not a long nor mathematical
paper!  Some small consolation for those not at the
RSA conference.

@_date: 2005-02-17 15:28:29
@_author: Ian G 
@_subject: SHA-1 cracked 
The *words* part I typed in here:
I skipped the examples.  It is very brief.
This is the relevant para:
"Table 2: A collision of SHA1 reduced to 58 steps. The two messages that collide are M0 and M'0. Note that padding rules were not applied to the

@_date: 2005-02-18 18:23:51
@_author: Ian G 
@_subject: Many Wireless Security Breaches Reported At (RSA) Security Conference 
(As I've said many times, security breaches reported at
conferences full of security people don't count as a
predictor of what's out in the real world as a threat.
But, it makes for interesting reading and establishes
some metric on the ease of the attack.  iang)
February 18, 2005
 Many Wireless Security Breaches Reported At Security Conference By Mobile Pipeline Staff
There were 32 "Evil Twin" attacks and many other types of security breaches aimed at Wi-Fi users of the recently-concluded RSA security conference, wireless security vendor AirDefense claimed Thursday.
In an Evil Twin attack, hackers set up bogus access points and try to get nearby wireless users to log on either. Then, they can steal information that the user transmits The use of this method of attack marks a significant shift in how eavesdroppers and hackers are trying to steal information from wireless LAN users, according to the company.
"Rather than simply scanning for and identifying access points, people are now imitating access points," Richard Rushing, AirDefense's chief security officer, said in a statement. "The same holds true for identity theft -- hackers have realized the value is in trying to become the access point or station, not merely finding one."
AirDefense regularly monitors the airwaves at industry conferences and reports the results afterwards. The company noted that the conference organizers made extraordinary efforts to provide secure wireless access, including as issuing digital credentials for accessing the wireless network used at the conference.
AirDefense acknowledged that the efforts made the conference's wireless network secure, but that didn't mean individual users were secure. That's because hackers were probing individual users' wireless profiles on their laptops, which list previously-used wireless networks. The hackers could then use the names of those networks to launch Evil Twin "We cannot stress how important it is for wireless users to clear their profile of access points on a regular basis," Rushing said. "Wireless, by design, will always connect with the strongest signal, even if that means abandoning a secure connection."
The Evil Twin attacks mimicked networks such as T-Mobile's and Wayport's networks of public Wi-Fi hotspots. That meant that some users who previously had accessed those networks were automatically logged on to the bogus versions of those networks.
In addition, AirDefense noted that it detected other types of attacks at the conference. Specifically, it sand it found 116 attempts to spoof MAC addresses and 45 denial-of-service attacks against access points. It also found 28 unauthorized access points connected to the conference's wireless LAN. The unauthorized access points drew a lot of traffic, the company said.

@_date: 2005-01-04 20:44:11
@_author: Ian G 
@_subject: AOL Help : About =?ISO-8859-1?Q?AOL=AE_PassCode?= 
OK.  So all I have to do is craft a good reason to
get people to reset their PassCode, craft it into
a phishing mail and send it out?

@_date: 2005-01-05 15:53:22
@_author: Ian G 
@_subject: Cryptography Research wants piracy speed bump on HD DVDs 
If the p2p apps could collude, they could create
a pre-threshold image and share it amongst
themselves only, gradually combining it until
no more differences were detected.  When a
post threshold watermark was reached, the
final image could be released.  You would need
some way to know that the watermark had
been reached, according to the testing against
a sufficient sized pool or somesuch metric.
Add some reputation nyms to sign and that
should avoid the poisoning attacks as well.

@_date: 2005-01-05 16:57:45
@_author: Ian G 
@_subject: FreeBSD's urandom versus random 
While we're on the subject of /dev/[u]random, has anyone
looked at the new FreeBSD 5.3 version?  I recently installed
5.3, and much to my surprise they have got rid of the random
component, and are now only using the urandom part (they
have a symlink in place).
localhost$ ls -l /dev/*random
crw-rw-rw-  1 root  wheel  249,   0 Dec 26 11:41 /dev/random
lrwxr-xr-x  1 root  wheel         6 Dec 26 11:41 /dev/urandom -> random
localhost$ time dd if=/dev/random bs=1k count=100000 of=/dev/null
100000+0 records in
100000+0 records out
102400000 bytes transferred in 7.526354 secs (13605525 bytes/sec)
real    0m7.532s
user    0m0.125s
sys     0m6.979s
(Also, there is a startup script that asks for initial type-in
entropy on install - mine broke and didn't clearly indicate
its state...).
Personally, I quite liked the random v. urandom separation.
It gave you choice.  Now, I feel tempted to go back to doing
the entropy in my own code and not relying on FreeBSD,
because I lack a feel-good factor represented by the block
that occurs when entropy runs out.
But, these are ignorant, external speculations...

@_date: 2005-01-06 13:10:31
@_author: Ian G 
@_subject: AOL Help : About =?ISO-8859-1?Q?AOL=AE_PassCode?= 
By this you mean a dynamic, immediate MITM where
the attacker proxies through to the website in real
Just as a point of terms clarification, I would say that
if the attacker collects all the information by using
a copy of the site, and then logs in later at leisure
to the real site, that's an MITM.
(If he were to use that information elsewhere, so for
example creating a new credit arrangement at another
bank, then that technically wouldn't be an MITM.)
Perhaps we need a name for this:  real time MITM
versus delayed time MITM?  Batch time MITM?
The user+client has to authenticate the server.  Everything
that I've seen over the last two years seems to fall into
that one bucket.
Maybe.  But that only addresses the MITM, not the
theft of user information.

@_date: 2005-01-07 22:24:35
@_author: Ian G 
@_subject: AOL Help : About =?ISO-8859-1?Q?AOL=AE_PassCode?= 
An interesting data point!
afaik, the active or dynamic MITM attack has not
been seen out in the wild.  Lots of other thing,
which lead one to suspect that it will turn up just
as soon as it is worth the trouble, but not precisely
that.  A summary here:
I agree, these advantages made me think that
we'd see the full cycle attacks sooner.  But, they
went for the simpler attack first.  I guess there's
a lesson in there for us.
I would say that this is the old saw of "you
signed for this legally" versus "what works."
In practice, depending on the institution
concerned, the model that gets put in place
is "what works" where working can be
measured by as tight or loose as the spirit
of the moment determines.
Ah so!  Bill and his browser authenticate the
website directly.  Where have we seen that
before ;-)

@_date: 2005-01-09 00:23:48
@_author: Ian G 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
Voice Over Internet Protocol and Skype Security
Simson L. Garfinkel
January 7, 2005
With the increased deployment of high-speed ("broadband") Internet connectivity, a growing number of businesses and individuals are using the Internet for voice telephony, a technique known as Voice over Internet Protocol (VoIP). With a VoIP system, two people can speak with each other by using headsets and microphones connected directly to their Skype is a proprietary VoIP system developed by Skype Technologies S.A. Like the popular KaZaA file-trading system, Skype is based on peer-to-peer technology: instead of transmitting all voice calls through a central server, as some VoIP services do (Vonage, for example), Skype clients seek out and find other Skype clients, then build from these connections a network that can be used to search for other users and send them messages.
Is Skype secure? How does its security compare with that of conventional telephone calls, or of other VoIP-based systems? In this article commissioned by OSI's Information Program, Simson Garfinkel, an expert on Internet security and networking issues, looks at the security properties of key importance for civil society organizations relying on Skype for voice communications.

@_date: 2005-01-09 11:32:19
@_author: Ian G 
@_subject: entropy depletion (was: SSL/TLS passive sniffing) 
This is a great post.  One can't stress enough
that programmers need programming guidance,
not arcane information theoretic concepts.
By this, do you mean that /dev/*random should deliver
unpredictability, and /dev/entropy should deliver ...
pure entropy?
Right, but in the big picture, this is one of those
frequently omitted steps.  Why?  Coders don't have
time to acquire the knowledge or to incorporate
all the theory of RNG in, and as much of today's
software is based on open source, it is becoming the
baseline that no theoretical foundation is required
in order to do that work.  Whereas before, companies
c/would make a pretence at such a foundation, today,
it is acceptable to say that you've read the Yarrow
paper and are therefore qualified.
I don't think this is a bad thing, I'd rather have a
crappy /dev/random than none at all.  But if we
are to improve the auditing, etc, what we would
need is information on just _what that means_.
E.g., a sort of "webtrust-CA" list of steps to take
in checking that the implementation meets the
If that's the definition that we like then we should
create that definition, get it written in stone, and
start clubbing people with it (*).
As long as this doesn't effect definition (2) then it
matters not.  At the level of the definition, that is,
and this note belongs in the "implementation notes"
as do (2B), (2C).
( LOL... Being a proponent of 40-bit myself, I wouldn't
be so distrusting.  I'd hope he was just pointing out
that 40-bits is way stronger than the vast majority
of traffic out there;  that which we talked about here
is buried in the noise level when it comes to real effects
on security simply because it's so rare. )
I certainly agree that overloading the term 'random'
has caused a lot of confusion.  And, I think it's an
excellent idea to abandon hope in that area, and
concentrate on terms that are useful.
If we can define an entropy device and present
that definition, then there is a chance that the
implementors of devices in Unixen will follow that
lead.  But entropy needs to be strongly defined in
practical programming terms, along with random
and potentially urandom, with care to eliminate
such crypto academic notions as information
theoretic arguments and entropy reduction.
I'm confused by this aggresive containment of the
entropy/random device.  I'm assuming here that
as /dev/entropy) and Urandom is the real good PRNG
which doesn't block post-good-state.
If I take out 1000 bits from the *entropy* device, what
difference does it make to the state?  It has no state,
other than a collection of unused entropy bits, which
aren't really state, because there is no relationship
from one bit to any other bit.  By definition.  They get
depleted, and more gets collected, which by definition
are unrelated.
Why then restrict it to non-communications usages?
What does it matter if an SSH daemon leaks bits used
in its *own* key generation if those bits can never be
used for any other purpose?
(Other than to SSH that is...) (**)
Great post!
(*) last night I discovered that the new /tmp cleanup
setting for FreeBSD also means X won't run after boot
up because it expects its special dir to be safe in /tmp
... this means that after 30 years, the definition of /tmp
is still being fought over in the patch wars.
(**) Ideally, I'd like secure machines to generate server
primary install-time keys from the entropy device and
all other uses can go to the Urandom device.

@_date: 2005-01-11 16:22:43
@_author: Ian G 
@_subject: entropy depletion 
Right.  So what we are looking at here is a requirement
such that we don't leak any internal state.  Traditionally,
the Unix people would do this by a) limiting access to the
resource to root and b) auditing the user of the resource
carefully.  But that's a bit OTT, IMHO.
Another way of doing this is to put in a requirement that
each read is separate and unlinked.  That is, if you do a
read of 1024 bits for some key operation, the contract with
the random device is that entropy might be mixed between
those bits, *but* it isn't mixed with any other read that
might be done.
Trivially, this could be met by throwing away all existing
entropy, locking the entropy for syncronised read, and
waiting until enough fresh stuff was built up.  That might
take a while, but hey, that's the contract that the coder
asked for.  And there are plenty of variants imaginable.
Either way, it seems that restriction is not the only way
to deal with the leakage problem, once we understand that
avoiding the leakage is the requirement.

@_date: 2005-01-11 17:39:08
@_author: Ian G 
@_subject: entropy depletion 
Right.  I'd suggest the original statement of the
issue at hand might be better rephrased as:
The *requirement* is that the generator not leak
This requirement applies equally well to an entropy
collector as to a PRNG.
For an entropy collector there are a number of ways
of meeting the requirement.
1.  Constrain access to the device and audit all
users of the device.
2.  set the contract in the read() call such that
the bits returned may be internally entangled, but
must not be entangled with any other read().  This
can trivially be met by locking the device for
single read access, and resetting the pool after
every read.  Slow, but it's what the caller wanted!
Better variants can be experimented on...
We are still left with the notion as Bill suggested
that no entropy collector is truly clean, in that
the bits collected will have some small element of
leakage across the bits.  But I suggest we just
cop that one on the chin, and stick in the random(5)
page the description of how reliable the device
meets the requirement.
(This might be a resend, my net was dropping all
sorts of stuff today and I lost the original.)

@_date: 2005-01-14 10:18:20
@_author: Ian G 
@_subject: entropy depletion 
I'm sorry, I don't see the relationship
between the requirement to not leak,
and the requirement to deliver a quantifiable
That seems to address the "trivial implementation"
rather than the requirement?
(In practice, I'd be inclined to not so much reset the
pool after every read, but flush or discard a certain
amount that is calculated to reduce any cross read
leakage.  But yes, the requirement may prove to
present some interesting challenges to the implementor.)
Zooko and I struck this issue in our recent SDP1.
As a datagram secret key layout, it uses a lot of
secret keying material.  The way we resolved it was
to set a requirement for quality in the key exchange
phase, which derived from a requirement to reduce
the complexities of the datagram encryption phase
(for programming reasons, not crypto reasons);  in
effect we punted the problem upstairs and put all
the load on the key exchange phase.

@_date: 2005-01-21 12:50:23
@_author: Ian G 
@_subject: Internet 'Phishing' Scams Getting More Devious 
Financial Cryptography Update: Internet 'Phishing' Scams Getting More                             January 21, 2005

@_date: 2005-01-27 11:02:56
@_author: Ian G 
@_subject: Simson Garfinkel analyses Skype - Open Society Institute 
[Good analysis!  Snipped...]
I would say that the threat of a bunch of teenagers
cracking Skype would not be read as a much of a threat
in my book.  What are they going to do, listen to each
other's teenagerish calls?  If they break it, well and good,
as it's one community you can expect to share the break
quickly, which will give Skype the requisite kick up the

@_date: 2005-01-30 16:51:58
@_author: Ian G 
@_subject: Weaknesses in RFID-based transponders 
This is good research!
I don't think the designers have done the wrong thing.  A
cursory scanning of the HTML above indicates that:
     + the device reduced auto theft by "as much as 90%";
     + In 2003, auto loss was 1.3 million vehicles and $8.6
         billion dollars.
     + the device was fitted to 150,000 vehicles.
If we say (pick a number any number) 1% of vehicles are
stolen every year, then 1500 of those vehicles were to have
been stolen, and only a tenth of them were.  That's a saving
of 1350 vehicles, or at a cost o $6600, $8.9 million saved.
Saved.  Nothing can change that (except picking better
fudge factors, of course ;).
To criticise the actions of the designers would be to say
that they will lose inordinately more in the future, but even
that's not the case.  It is unlikely that the system will result
in all vehicles now being at 'ordinary' risk of theft, it is more
likely that the risk benefit will shift from 10% to 20% over
time.  So even in the future, they still save, just not as much
as last year.
Which is to say, the designers are still in profit.
Also, one has to wonder when the algorithm/size was chosen.
If 150,000 of these are out there, it wouldn't surprise me if the
basic design paramaters of the system are 10 years old, which
takes us back to the good old days when RC4 was considered
good, 40 bits was not easy to crack, and you were lucky to get
an RFID that could do crypto ...  (Indeed, the paper states that
40 bits was beyond them unless they built the 16-way FPGA

@_date: 2005-07-09 19:56:36
@_author: Ian Grigg 
@_subject: the limits of crypto and authentication 
FTR, e-gold were aware of the general makeup of this
threat since 1998 and asked someone to look at it.  The
long and the short was that it was more difficult to solve
than at first claimed, so the project was scrapped.  This
was a good risk-based decision.  The first trojans that I
know of for e-gold weren't spotted until 12-18 months
ago, so it was also a profitable decision.  What they are
doing now I don't know.
In the payments world we've known how to solve all
this for some time, since the early 90s to my knowledge.
The only question really is, have you got a business
model that will pay for it, because any form of token is
very expensive, and the form of token that is needed -
a trusted device to put the application, display, keypad
and net connection on - is even more expensive than
the stop-gap two-factor authentication units commonly

@_date: 2005-07-10 12:22:46
@_author: Ian Grigg 
@_subject: the limits of crypto and authentication 
There was no market failure - Amex Blue was
an outstanding success that sent waves of
astonishment through the credit card industry.
Everyone was talking about how stunningly
successful it was - how it had broken the laws
of account creation by actually acquiring new
accounts in the millions instead of cannibalising
existing accounts.  (I recall a number of 4 million?)
You may be thinking that the usage of the smart
card being a total and complete flop was in some
way correlated with the market success, but it
was quite the reverse - the smart card usage was
a complete and utter failure for the obvious reasons,
but the program itself was fantastically successful.

@_date: 2005-07-14 03:57:30
@_author: Ian Grigg 
@_subject: ID "theft" -- so what? 
He's not proposing PKI, but nymous accounts.  The
account is the asset, the key is the owner;  at the
simplest conceptual level it is the difference between
Paypal and e-gold.
But, thank the heavens that we now have reached
the point where people can honestly say that PKI
is the root cause of the problem.  Can you now tell
the browser people?
Right, alongside nyms on a spectrum is big random
number-sized tokens.  If you want to get sexy, go
for the blinded ones.  It's all the same infrastructure,
we call it FC.
True, but also easy to copy and can be stolen.  For
some value, you don't want to go there.

@_date: 2005-07-14 15:37:05
@_author: Ian Grigg 
@_subject: ID "theft" -- so what? 
(Dan, in answer to your question on certs, below.)
Well, that's helpful.  Having built one or two of
these things (and I know of 3 others on the list
that have done the same thing) it helps to know
we aren't starting from scratch.
OK, so maybe this part is the new realisation:
The browser security model includes PKI for two
purposes - MITM protection and spoofing protection.
Ignoring MITM (today), the spoofing protection is
supposed to alert the user that the cert and the
site don't match.
Phishing is a spoof - the wrong site is used.  So
SSL+PKI should pick that up.  It isn't.  Why?
Simply put because the browser too easily lets
SSL's anti-spoofing protection not be seen.  It's
not being done properly.
Why is that?  Because the browser people are
under severe constraints - your words - and
nobody is correcting their missunderstandings.
No security folk, no security companies, no CAs,
just a few researchers (some lurking here...).
Too many words?  OK, here's the short version
of why phising occurs:
"Browsers implement SSL+PKI and SSL+PKI is
secure so we don't need to worry about it."
PKI+SSL *is* the root cause of the problem.  It's
just not the certificate level but the business and
architecture level.  The *people* equation.
I've been over at Mozilla trying to tell them the PKI
isn't doing it's job.  Peter Gutmann and Amir Herzberg
have been there supporting this push.  They're not
visionaries either but at least they put their money
where their mouths are - trying to get Mozo people
to touch up the PKI + SSL code to deal with spoofing.
(Demos and code available on request !!!!)
We recently set up a new
group for all anti-phishing researchers so they could
congregate and cross-fertilise ideas in a scientific
fashion.  I'm proud to say that in less than one month
our understanding of phishing and the browser
security model has significantly advanced.
We've talked to dozens of programmers over on the
Mozilla camp, sadly without success and I think that's
because the crypto community has been relatively
silent on this issue.  Most over in the browser
community remain simply unaware and uneducated
on the reasoning behind the security model, and how
out of date it is.
So, where have you been, Perry?  If you wish to
patronize me (on a public list, with no right of reply!)
do so from a position of strength.
Perry, for the last few months or so the game
you have been playing is "disagree with Ian,
rag him in public, drop his posts."
I don't mind .. but as I showed above, you are
100% diametrically wrong about what it is I am
saying or likely to say.  Just so you're aware
that you're inventing the rest of the discussion
and disagreeing with your own invention...

@_date: 2005-07-14 23:21:57
@_author: Ian Grigg 
@_subject: ID "theft" -- so what? 
Because it's the major example of what most would
agree is PKI, I'd guess.  When we talked to people
in the certs and CAs world, they call it PKI.  They
refer to lots of documents, which call it the PKI.  The
business model of PKI vendors used to at least be
partly based on selling certs.  It's an assumption
they make or made.
(John Kelsey answered this very well.)
There is a sort of doublethink here - when people
look down their nose at PKI from the PGP side,
the PKI side is sometimes at pains to say that PGP's
WoT is a PKI.  Yet when the converse happens
and PGP pundits suggest using WoT with (e.g.,)
x.509 certs, the PKI people say "WoT is not PKI."
Personally, I call "what PGP does" a Web of Trust.
And I call what browsers do a PKI.  The fact that
there is "trust" in PKI and there is "infrastructure"
in WoT is an issue, yes, but we have to have some
sense of differentiation;  and those terms are what
the people in those fields tend to be comfortable

@_date: 2005-06-01 12:16:09
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
Right, so we are agreed that listening to credit cards
is not an economic attack - regardless of the presence
of SSL.
Now, the point of this is somewhat subtle.  It is not
that you should turn off SSL.
The point is this:  you *could*
turn off SSL and it wouldn't make much difference
to actual security in the short term at least, and maybe
not even in the long term depending on the economic
OK, so, are we agreed on that:  we *could* turn off
SSL, but that isn't the same thing as "should* ?
If we've got that far we can go to the next step.
If we *could* turn off SSL then we have some breathing
space, some room to manouvre.  Some wiggle room.
Which means we could modify the model.  Which
means we could change the model, we could tune
the crypto or the PKI.  And in the short term, that
would not be a problem for security because there
isn't an economic attack anyway.  Right now, at
OK so far?
This means that we could improve or decrease
its strength ... as our objectives suggest ... or we
could *re-purpose* SSL if this were so desired.
So we could for example use SSL and PKI to
protect from something else.  If that were an issue.
Let's assume phishing is an issue (1.2 billion
dollars of american money is the favourite number).
If we could figure out a way to change the usage
of SSL and PKI to protect against phishing, would
that be a good idea?
It wouldn't be a bad idea, would it?  How could it
be a bad idea when the infrastructure is in place,
and is not currently being used to defeat any
So, even in a stupidly aggressive worst case
scenario, if were to "turn off SSL/PKI" in the process
and turn its benefit over to phishing, and discover
that it no longer protects against listening attacks
at all - remember I'm being ridiculously hypothetical
here - then as long as it did *some* benefit in
stopping phishing, that would still be a net good.
That is, there would be some phishing victims
who would thank you for saving them, and there
would *not* be any Visa merchants who would
necessarily damn your grandmother for losing
credit cards.  Not in the short term at least.
And if listening were to erupt in a frenzy in the
future it would likely be possible to turn off the
anti-phishing tasking and turn SSL/PKI back to
protecting against eavesdropping.  Perhaps as
a tradeoff between the credit card victim and
the phishing victim.
But that's just stupidly hypothetical.  The main
thing is that we can fiddle with SSL/PKI if we want
to and we can even afford to make some mistakes.
So the question then results in - could it be used
to benefit phishing?  I can point at some stuff that
says it will be.
But every time this good stuff is suggested, the
developers, cryptographers, security experts and
what have you suck air between their teeth in and
say you can't change SSL or PKI because of this
crypto blah blah reason.
My point is you can change it.  Of course you
can change it - and here's why:  it's not being
economically used over here (listening), and
right over there (phishing), there is an economic
loss waiting attention.
All that is absolutely true, in that we can conjecture
that if we close everything else off, then sniffing will
become economic.  That's a fair statement.
But, go and work in one of these places for a while,
or see what Perry said yesterday:
I'm sure that you'll agree that the likelihood of them
closing of all the other attacks is next to nil.  Even
if some top flight security experts manages to find
a client that really cares about security and they
together manage to actually lock everything down
(a rather low probability, I'd suggest) then there will
still be 1000 other places for the attacker to steal
the data.
The day to day reality of financial institutions is that
they do not have good protections in place, they
have *adequate* protections for what they *know*
about.  Which means that there is plenty of pickings
out there.
So I would suggest that listening for credit cards will
never ever be an economic attack.  Sniffing for random
credit cards at the doorsteps of amazon will never ever
be an economic attack, not because it isn't possible,
but because there always likely to be easier pickings
But don't get me wrong - I am not saying that we should
carry out a world wide pogrom on SSL/PKI.  What I am
saying is that once we accept that listening right now
is not an issue - not a threat that is being actively
dedended against - this allows us the wiggle room to
deploy that infrastructure against phishing.
Does that make sense?
PS: nor does it matter whether I'm right or I'm wrong
about my prediction that sniffing will be an economic
attack or not - it's just a prediction about the future,
just a hypothetical estimate.
What matters is now:  what attacks are happening
now.  Does phishing exist, and does it take a lot of
money?  What can we do about it?

@_date: 2005-06-01 10:37:20
@_author: Ian G 
@_subject: Digital signatures have a big problem with meaning 
Yes, this is directly what we found with the signed
contracts for digital instruments (aka ecash).  We did
all the normal digital signature infrastructure (using PGP
WoT and even x.509 PKI for a while) but the digsig
never actually made or delivered any meaningful biz
results.  In contrast, it was all the other steps that
we considered from the biz environment that made
the difference:  a readable contract, a guaruntee
that it wouldn't change, a solid linkage to every
transaction, and so forth and so on.
In the end, the digital signature was just crypto
candy.  We preserve it still because we want to
experiment with WoT between issuers and governance
roles, and because we need a signing process of
some form.  In any small scenario (<1000 users)
that sort of linkage is better done outside the tech
and for large scenarios it is simply unproven whether
it can deliver.
PS: must look up the exec summary of aads one day!

@_date: 2005-06-01 15:23:21
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
Hi Birger,
Nice debate!
Right.  That's the point.  It is not a universal
and inescapable bad to fiddle with SSL/PKI.
OK, and that's where we get into poor use of
data.  Yes, sniffing of passwords existed back
then.  So we know that sniffing is quite possible
and on reasonable scale, plausible technically.
But the motive of sniffing back then was different.
It was for attacking boxes.  Access attack.  Not
for the purpose of theft of commercial data.  It
was a postulation that those that attacked boxes
for access would also sniff for credit cards.  But,
we think that to have been a stretch (hence the
outrageous title of this post) at least up until
Before 2004, these forces and
attackers were disconnected.  In 2004 they joined
forces.  In which case, you do now have quite a
good case that the installation of sniffers could be
used if there was nothing else worth picking up.
So at least we now have the motive cleared up,
if not the economic attack.
(Darn ... I seem to have argued your case for you ;-) )
Indeed.  It also doesn't mean that they will come
and attack.  Maybe it is a choice between the
attack that is happening right now and the attack
that will come back.  Or maybe the choice is
not really there, maybe we can cover both if
we put our thinking caps on?
Nice analogy!  Like all analogies it should be taken
for descriptive power not presecription.
The point being that one should not slavishly stick
to an argument, one needs to establish principles.
One principle is that we protect where money is being
lost, over and above somewhere where someone
says it was once lost in the past.  And at least then
we'll learn the appropriate balance when we get it
wrong, which can't be much worse than now, coz
we are getting it really wrong at the moment.
(On the monetary economics analogy, if you said your
principle was to eliminate inflation, I'd say fine!  There
is an easy way to do just that, just use gold as money,
which has maintained its value throughout recorded
history, not just the last century!  The "targets" debate
has been echoing on for decades, and there is no
real end in sight.)
That's one way.  Another way is to put the fixes into
browsers and trial it out there.  To some extent this
has been done - you can see for example Trustbar
and Petname as both similar attempts, and a very
different approach taken by Netcraft.  The results
are all positive.
What is needed is a series of experiments to try out
how different mechanisms work - there is no way that
you or I could predict the perfect way.  But we can't
push those experiments so far forward unless we can
convince the detractors of change to back off and
let some experimentation be done.
That is the problem with all these suggestions - they
do some "change" to the model of SSL/PKI.  And that
change is sometimes good sometimes bad and sometimes
completely dismissive - but they all get the same response
being  you can't change SSL! Consider petname.  *All* it does is allow someone to put
a name on a cert.  That's all.  But because it changes the
security model about how a cert is supposed to protect
users - in what might be thought of as a positive way -
this has received resistance.
That's what we are fighting against.  That's why I have
to go to such extraordinary lengths to show that it is
possible to change the model.
Well, I'm establishing a point here - there is wiggle room,
room to manouvre.
This might mean SSL/PKI is reduced in strength.  *OR*
it might mean it is improved in strength.  I really don't
care, I'm interested in the results, not the way it was
*OR* it might mean that SSL/PKI is bypassed totally.
For an example of the latter, look at Netcraft.  This is
quite serious - they are putting out a tool that totally
bypasses PKI/SSL in securing browsing.  Is it insecure?
Yes of course, and "it leaks my data like a seive" as
one PKI guy said.
Is it securing browsing?  Yes!  As recently reported,
60,000 downloads "within hours" for Firefox after it
got slashdotted.  The contradictions need to be
addressed, and more seriously than just rejecting
any change to SSL/PKI;  by rejecting the minor
changes by Trustbar and Petname, the way is
open for the really dramatic and major changes
like Netcraft.  And it's insecure to boot!
Not really.  My point is that there is room to move.  You
can fiddle with the model.  Nobody's talking about removing
SSL/PKI, and I don't know why people keep assuming that
it is on the agenda.  There is simply too much code out
there to remove it.
What people are talking about is trying different things.  The
SSL/PKI people are pretty universally blocking those things
at the moment, for whatever reasons, but sadly, because
they are not taking the bigger picture seriously, they are
somewhat in danger of being bypassed totally by things like
Netcraft's plugin.  Or worse (yes, there is worse than Netcraft's
toolbar in the wings.)
So where is the structural problem in phishing?  On the
left is the phisher, on the right is the user.  Between them
is a wire, a browser, a PC, and a hook delivery mechanism.
Over in the distance is the other victim, the online bank.
So where's this structural problem and let's get in there
and fix it?

@_date: 2005-06-01 16:24:07
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
Just on the narrow issue of data - I hope I've
addressed the other substantial points in the
other posts.
On the issue of sharing data by victims, I'd strongly
recommend the paper by Schechter and Smith, FC03.
" How Much Security is Enough to Stop a Thief?"
I've also got a draft paper that argues the same thing
and speaks directly and contrarily to your statement:
Sharing data is part of the way towards better security.
(But I argue it from a different perspective to S&S.)
The world for security in the USA changed dramatically
when Choicepoint hit.  Check out the data at:
Also, check out Adam's blog at
He has a whole category entitled Choicepoint for
background reading:
Finally we have our data in the internal governance
and hacking breaches.  As someone said today, Amen
to that.  No more arguments, just say "Choicepoint."
We all know that the attacker is active and can
change tactics.  But locksmiths still recommend
that you put a lock on your door that is a) a bit
stronger than the door and b) a bit better than your
neighbours.  Just because there are interesting
quirks and edge cases in these sciences doesn't
mean we should wipe out other aspects of our
knowledge of scientific method.
Which is true regardless of whether you are
slightly drunk or not at all or whether a few
pills had been taken or tiredness hits.
Literally, like driving when not 100% fit, the
decision maker makes a quick decision based
on what they know.  The more they know, the
better off they are.  The more data they have,
the better informed their decision.
Sure.  Life's a bitch.  One can only do ones
best and hope it doesn't hit.  But have a read
of S&S' paper, and if you still have the appetite,
try my draft:
No, that's way beyond what I was saying.
I was simply asserting one thing:  without data, we do
not know if an issue exists.  Without even a vaguely
measured sense of seeing it in enough cases to know
it is not an anomoly, we simply can't differentiate it
from all the other conspiracy theories, FUD sales,
government agendas, regulatory hobby horses,
history lessons written by victors, or what-have-you.
Ask any manager.  Go to him or her with a new
threat.  He or she will ask "who has this happened
If the answer is "it used to happen all the time in
1994 ..." then a manager could be forgiven for
deciding the data was stale.  If the answer is
no-one, then no matter how risky, the likely
answer is "get out!"  If the answer is "these X
companies in the last month" then you've got
some mileage.
Data is everything.

@_date: 2005-06-01 16:24:47
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
Well, I'm not arguing it is technically hard.  It's just
un-economic.  In the same sense that it is not technically
difficult for us to get in a car and go run someone
over;  but we still don't do it.  And we don't ban the
roads nor insist on our butlers walking with a red
flag in front of the car, either.  Well, not any more.
So I stand by my statement - correlation is not causality.
Exactly my point.  Sniffing isn't noticeable.  Neither
in the cases we know it could happen, nor in the
areas.  The one place where it has been noticed is
with passwords and what we know from that experience
is that even the slightest security works to overcome
that threat.  SSH is overkill, compared to the passwords
mailouts that successfully protect online password sites.
SSH just works - and it worked directly against the
threat you listed above (password sniffing).  But it
has no "PKI" to speak of, and this discussion is about
whether PKI protects people, because it is PKI that is
supposed to protect against spoofing - a.k.a. phishing.
And it is PKI that makes SSL "just doesn't set up."
Anyone who's ever had to set up an Apache web
server for SSL has to have asked themselves the
question ... "why doesn't this just work" ?
Simply, evidence that people are listening.  Sniffing
by means of the wire.
Evidence that people abuse to gain unprotected
access is nothing to do with sniffing traffic to steal
information.  That's theft of access, which is a fairly
minor issue, especially as it doesn't have any
economic damages worth speaking of.  In fact,
many cases seem to be more accidental access
where neighbours end up using each other's access
points because the software doesn't know where the
property lines are.
OK, so maybe I am incorrectly reading this - are you
saying that spyware is being delivered that incorporates
wire sniffers?  Sniffers that listen to the ethernet traffic?
If that's the case, that is the first I've heard of it.  What
is it that these sniffers are listening for?

@_date: 2005-06-01 18:50:04
@_author: Ian G 
@_subject: Digital signatures have a big problem with meaning 
Yes, indeed!  The thing about a signature is that
*it* itself - the mark on paper or the digital result
of some formula - isn't the essence of signing.
The essence of the process is something that
lawyers call "intent" (I'm definately not clear on
these words so if there are any real lawyers in
the house...).  And, when the dispute comes to
court, the process is not one of "proving the
signature" but of showing intent.
And as the transaction gets bigger, the process
of making and showing intent gets more involved,
more complex.  So it is naturally ramped up to the
transaction, in a way that digsigs just totally miss
out on.
Which means that the digital signature school
got it completely wrong.  A digital signature is
only "just one more" element in a process that
is quite complex, involved, and goes back into
history more years than we can count.  It is
therefore completely unlikely that a digsig will
ever replace all that;  however it is quite possible
that a digsig could comfortably add a new element
to that process.
(Speaking here of common law, which is not
universally applicable...)
And this is where we found for example the OpenPGP
cleartext digital signature to be the only one that
has any merit.  Because it can be printed on paper,
and that piece of paper can be presented to the
jury of an O.J.Simpson style case, or even a Homer
Simpson style case, this carries weight.
An OpenPGP clear text signature carries weight
because it is there, in black and white, and no
side would dare to deny that because they know
it would be a simple matter to go to the next level.
But any other form of non-printable digital signature
is not "presentable" to a jury.  What are you going
to do? Throw a number in front of a jury and say its a
signature on another number?  It's a mental leap of
orders of magnitude more effort, and there are many
ways the "other side" could sidestep that.
PS: To get this in x.509, we coded up cleartext
sigs into the x.509 format.

@_date: 2005-06-01 18:59:30
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
Ahh-oops!  That particular reply was scrappily written
late at night and wasn't meant to be sent!  Apologies
belatedly, I'd since actually come to the conclusion
that Steve's statement was strictly correct, in that
we won't ever *see* sniffing because SSL is in place,
whereas I interpreted this incorrectly perhaps as
SSL *stopped* sniffing.  Subtle distinctions can
sometimes matter.
So please ignore the previous email, unless a cruel
and unusual punishment is demanded...

@_date: 2005-06-02 13:58:40
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
That's the point.  There is no link to SSL or PKI.
The only thing in common is the objective - to
protect the user when browsing.  Secure browsing
is now being offered by centralised database sans
Sure, I think it is a piece of junk, myself.  But I
am not important, I'm not an "average user."
The only thing that is important is what the user
thinks and does.
When Netcraft announced their plugin had been
ported from IE to Firefox last week, they also
revealed that they had "60,000 downloads in
hours."  That tells us a few things.
Firstly, users want protection from phishing.
Secondly, Netcraft have succeeded enough
in the IE world in creating a user base for their
solution that it easily jumped across to the
Firefox userbase and scored impressive numbers
straight away.  Which tells us that it actually
delivers something useful (which may or may
not be security).  So we cannot discount that
the centralised database concept works "well
enough" by some measure or other.
So now we wait to see which model wins in
protecting the user from spoofing.

@_date: 2005-06-02 18:36:56
@_author: Ian G 
@_subject: Citibank discloses private information to improve security 
Firefox have added a cert domain into the status bar
on the bottom of the browser.  This is part way to what
you suggest and a very welcome improvement to
browser security.
It falls short for (IMHO) 3 reasons:  1. the domain that
is shown isn't the certificate domain, but is something
amalgamated from the URL and the cert;  which then
breaks the independent check you are hoping for above.
2., the CA should be listed so as to complete the
security statement.  Something like "ThisCA signed the
This.Domain.Com cert".  This is done in the Mouseover,
but not displayed all the time, and it is possible to get a
Mouseover that shows a statement that is strictly false
because of 1. above.  (Bugs filed and all the rest...)
3. Another issue is that it is not big enough nor loud enough
in the Trustbar sense to break through the current user
teachings that they can ignore everything as its all safe.
Yes, this would be a much better way forward.  Now,
bear in mind that the people writing the plugins would
give their left legs to get the attention and respect of
the browser manufacturers so as to create this
integrated solution.  See various other rants...
This will change,.  I predict that the banks will end up
with the liability for phishing, for good or for bad, and
they will then find it in their hearts to finance the add-ons,
which will battle it out, thus leading to the 'best practices'
which will be incorporated into the browsers.
(Seeing as this is prediction time, I'll stick my neck
out another several kms and say it will be in about 6
months that the banks are asked to take on the liability.)

@_date: 2005-06-03 00:48:28
@_author: Ian G 
@_subject: [Clips] Storm Brews Over Encryption 'Safe Harbor' in Data Breach Bills 
Just to make it more interesting, the AG of New York, Elliot Spitzer
has introduced a  package of legislation intended to "rein in identity theft"
  Facilitating prosecutions against computer hackers by creating
  specific criminal penalties for the use of encryption to conceal
  a crime, to conceal the identity of another person who commits
  a crime, or to disrupt the normal operation of a computer;
Full PR is here:
I'm hoping this was a trial balloon.

@_date: 2005-06-03 00:56:08
@_author: Ian G 
@_subject: Cell phone crypto aims to baffle eavesdroppers 
Cell phone crypto aims to baffle eavesdroppers
By Munir Kotadia, ZDNet Australia
Published on ZDNet News: May 31, 2005, 4:10 PM PT
An Australian company last week launched a security tool for GSM mobile
 phones that encrypts transmissions to avoid eavesdroppers.
GSM, or Global System for Mobile Communications, is one of the most popular
 mobile phone standards and is built to provide a basic level of security.
 However, for more than five years the security has been "cracked," and
 commercial scanners that can emulate GSM base stations are becoming more
 common. That prompted Melbourne-based SecureGSM to launch its encryption
 tool at the CeBit exhibition in Sydney last week.
Roman Korolik, managing director of SecureGSM, said that because GSM security
 was cracked so long ago, there was a lot of information and equipment
 available that could be used for intercepting GSM calls.
"There are devices available for interception and decoding (GSM calls) in
 real time...Although they are, strictly speaking, illegal in most countries,
 you can buy them," said Korolik, who believes that these scanners are
 already being used to intercept sensitive calls. "You can imagine that in
 places like the stock exchange, where the traders are on their mobile
 phones...there could be a few scanners there."
As far back as 1999, the security used by GSM has been questioned. In a paper
 published by Lauri Pesonen from the Department of Computer Science and
 Engineering at Helsinki University of Technology, the GSM model was said to
 have been "broken on many levels."
"The GSM security model is broken on many levels and is thus vulnerable to
 numerous attacks targeted at different parts of an operator's network...If
 somebody wants to intercept a GSM call, he can do so. It cannot be assumed
 that the GSM security model provides any kind of security against a
 dedicated attacker," Pesonen wrote in the paper.
However, additional GSM security is unlikely to be used by the masses,
 according to Neil Campbell, national security manager of IT services company
 Dimension Data, who said companies are likely to have higher priorities.
"This is a security control like any other control--like a firewall or a
 policy. An organization needs to believe it is appropriate for their risks
 to implement this control. Obviously the military is one that you would
 expect to have a need for secure communications, but I wouldn't expect there
 to be too many organizations in this country that would think it necessary
 to encrypt their mobile phone conversations," said Campbell.
SecureGSM requires Windows Mobile Phone Edition
 with an ARM or
 compatible processor running at 200MHz or better. It also requires 6Mb of
 RAM (random access memory) and 2MB of storage space.
The SecureGSM application uses 256-bit, triple cipher, layered encryption
 based on AES, Twofish and Serpent ciphers. According to SecureGSM, all of
 these algorithms are considered "unbreakable" and the triple layer ensures
 that "encrypted data is future proof." The product costs $188 (AU$249) for a
 single-user license, and each "secure" device requires a license.
Dimension Data's Campbell said that companies thinking about implementing
 such a solution will need to calculate how much they could lose if their
 communications were intercepted.
"Share traders may need it, but this is for an organization that communicates
 by mobile telephone and understands that the risk of interception is
 generally extremely low, but that risk is completely unacceptable," Campbell
 said.
Munir Kotadia of ZDNet Australia reported from Sydney
Copyright ?2005 CNET Networks, Inc. All Rights Reserved.

@_date: 2005-06-03 16:51:36
@_author: Ian G 
@_subject: [Clips] Storm Brews Over Encryption 'Safe Harbor' in Data Breach Bills 
They might have a problem with meeting the legal requirements
for disclosure if the alleged criminals were not as yet behind bars... I wonder if bin Laden would have an action against the Justice
Department if his file was stolen?
FBI Probes Theft of Justice Dept. Data
The FBI is investigating the theft of a laptop computer containing travel account information for as many as 80,000 Justice Department employees, but it is unclear how much personal data are at risk of falling into the wrong Authorities think the computer was stolen between May 7 and May 9 from Omega World Travel of Fairfax, which is one of the largest travel companies in the Washington area and does extensive business with government agencies.
  Justice Department spokeswoman Gina Talamona said the data included names and account numbers from travel account credit cards issued to government employees by J.P Morgan Chase & Co. and its subsidiary Bank One Corp.
She said the information did not include Social Security numbers or home addresses that often are used by identity thieves to establish credit or to purchase goods in other people's names.
In addition, she said the account information was protected by passwords, although sophisticated hackers often can break into stored databases.
Omega World Travel officials declined to comment on how the laptop was stolen or other elements of the case, as did the FBI, which is investigating.
The theft is one of a spate of incidents over the past several months that have resulted in sensitive data on millions of U.S. consumers being stolen or In December, Bank of America Corp. lost computer tapes containing records on 1.2 million federal workers, including several U.S. senators.
Talamona said that no Justice Department worker has reported suspicious activity on his or her financial accounts since the incident.
The banks issuing the travel cards have placed alerts on the workers' accounts, Talamona said.
She added that Omega World Travel has agreed to several changes to its security practices, including beefing up physical security at its offices, conducting a computer security review and ensuring that the stolen computer cannot be reconnected to the firm's network.
The travel cards have not been canceled, Talamona said.

@_date: 2005-06-04 11:43:22
@_author: Ian G 
@_subject: Papers about "Algorithm hiding" ? 
The number of people who are involved is actually quite
small if you think it through.  It's more a shift in attitude that
is the barrier, not a large number of people who have to
be sold.
GPG is an application that could be delivered by default
in all free OSs.  BSD is more or less installed automatically
with SSH installed.  Linux machines that are set up are
also generally set up with SSH.
From there it isn't a large step conceptually to install GPG
in the base installs.  Start with the BSDs (because they
understand security) and Linux (because they understand
It's also not a large step to add a special hook into SSH
and browsers to add a simple file encryption utility.  Just
like OpenPGP's secret key mode.  It doesn't have to be
good, it just has to be there.  A lot of machines have OpenSSL
in them (this is how we get easy access to SHA1).  Can we
add a simple file encrypt to that?
Once all the Unixen have these, the next step is to encourage
a little usage...  All you need to do is have one person that
you communicate with like your brother or sister for the fun
of doing some crypto chat, and it now becomes a regular
*non-relevant* issue.  All we need to do is to encrypt and
protect one file and encryption becomes easy.
Right.  If they find any evidence of "information hiding"
other than a boring OpenPGP install that is as common
as crazy frog mp3s then that's what I'd call "highly relevent"
evidence.  That would make matters worse for the particular
case at hand.
Information hiding is real sexy.  I wouldn't recommend it
for anyone who isn't really sure of their situation, and is
willing to understand that if he gets caught with it, he's
Certainly using another app is fine.  What would be more
relevant to the direct issue is that it becomes routine to
encrypt and to have encryption installed.  See the recent
threads on where all the data is being lost - user data is
being lost simply because the companies don't protect
it.  Why aren't they protecting it?  Because there are no
easy tools that are built in to automatically and easily
protect it.
The picture here is becoming overwhelmingly clear - in order
to protect users we should be employing as much crypto as
we can openly, opportunistically, and easily.  Anything that
holds back from users protecting their data is a bad, and
anything that moves them forward in protecting their data
is a good.

@_date: 2005-06-07 17:41:12
@_author: Ian G 
@_subject: Papers about "Algorithm hiding" ? 
OK, yes, you are right, we are talking about two
different things.
The difficulty here is that there is what we might call
the Choicepoint syndrome and then there is the
specific facts about the actual Choicepoint heist.
When I say Choicepoint I mean the former, and the
great long list of similar failures as posted last week.
I.e., it is a syndrome that might be characterised as
"companies are not protecting data" or in other words
"the threat is on the node not the wire."
Whereas in the specific Choicepoint heist, there is
the precise issue that they are selling their data to
someone.  That's much more complex, and crypto
won't change that issue, easily.
No it's not rocket science - it's economic science.
It makes no difference in whether the business is
small or large - it is simply a question of costs.  If
it costs money to do it then it has to deliver a
In the case of the backup tapes there was no reward
to be enjoyed.  So they could never justify encrypting
them if it were to cost any money.  Now, in an unusual
exception to the rule that laws cause costs without
delivering useful rewards, the California law SBxxxx
changed all that by adding a new cost:  disclosure.
(Considering that banks probably lose a set of backups
each every year and have been doing so since whenever,
it's not the cost of the tapes or the potential for ID theft
that we care about...)
Now consider what happens when we change the
cost structure of crypto such that it is easier to do it
than not.  This is a *hypothetical* discussion of course.
Take tar(1) and change it such that every archive is
created as an encrypted archive to many public keys.
Remove the mode where it puts the data in the clear.
Then encrypt to a big set of public keys such that
anyone who can remotely want the data can decrypt
it (this covers the biggest headache which is when
you want the data it is no longer readable).
So, now it becomes trivial to make an encrypted
backup.  In fact, it is harder to make an unencrypted
backup.  What are companies going to do?  Encrypt,
of course.  Because it costs to do anything else.
Yes, you are right, I was thinking "Choicepoint syndrome"
here.  In order to address "Choicepoint-actual" with crypto
we'd have to look at Rights systems: nyms, caps and Brands,
or address it at the business level.
Yes, that's why I'm saying that the tools should actually
make it easier to use the crypto than to do the alternate.
If we need to explain the reason, we've already lost.
The presence of some tools doesn't effect the argument
that other tools should be easier.  All this says is that
they - the bigger businesses - are capable of fielding
and paying for some tools.
Yes, I'd agree with that.

@_date: 2005-06-08 21:33:40
@_author: Ian G 
@_subject: The "encrypt everything" problem 
Yes, this is a perfect example of where we need tools
that can make this use of crypto more transparent.
Of course, anyone who's worked on big database
projects must have realised that they've drifted somewhat
away from the idealistic vision of the relational story
(as told by Coase? Date?  some other guys no doubt)
and adding encryption and key handling to that is just
like throwing sand into the machine.
I'd suspect most of us here could have a fair stab at
the "encrypted tapes" problem.  But we'd not get nearly
as far with the "encrypted database" problem.
I think this is one area where databases are going to
continue to create more noise than value, and things
like capabilities are more likely to advance, simply as
they are looking more clearly at the underlying data
and the connections and authorisations that need to
be protected.

@_date: 2005-06-12 16:27:36
@_author: Ian G 
@_subject: expanding a password into many keys 
I'd like to take a password and expand it into
several keys.  It seems like a fairly simple operation
of hashing the concatonatonation of the password
with each key name in turn to get each key.
Are there any 'gotchas' with that?
PS: some psuedo code if the above is not clear.
for k in {set of keys needed}
    key[k] = sha1( pass | k );

@_date: 2005-06-22 01:28:24
@_author: Ian Grigg 
@_subject: AES cache timing attack 
I agree with your comments about the injection, but I
don't see why the attack doesn't work on the session
passively.  Are you assuming that because it is a
session, it's in some way not plausible to match the
inbound packets with outbound packets?  I would
have thought that was possible with things like keep
alives and so forth.  The only drawback I can see is
that there might not be enough data (hence desire to
tickle things along with an injection).
When I was thinking about "use a mode" I was more
thinking about how a mode could be the cover needed
to hide the decrypt time.  A straight CBC mode would
probably make matters worse because it is a known
length and the key doesn't change, so plausibly the
longer the total packet, the better the time estimate.
But if the key were to change for each block in a
decrypt-dependent fashion, this would presumably
render the total time as an average over many decrypts
of many block keys.  The longer the packets, the more
the cover, and no key gets used more than once anyway.
So, hypothetically a mode that XOR'd the previous output
with the key before encryption (heaven knows whether
that would be cryptographically sound, but something
along those lines, anyway).
Alternatively, if one is in the unfortunate position of being
an oracle for a single block encryption then the packet
could be augmented with a cleartext random block to be
xor'd with the key each request.

@_date: 2005-06-22 13:54:34
@_author: Ian Grigg 
@_subject: WYTM - "but what if it was true?" 
A highly aspirated but otherwise normal watcher of black helicopters asked:
Beats me.  But what it if it was true.  What's your advice to

@_date: 2005-06-24 11:06:16
@_author: Ian Grigg 
@_subject: AES timing attacks, why not "whiten" the implementation? 
The rearrangement cost should be fairly low compared to
the cost of doing the decrypt in the first place?  And rekeying
involves network interchange which is expensive and
You don't need entropy, do you?  All you need to do is generate
an unrelatable time signature for a particular decryption, and for
that you just need a stream that is unrelated in its timing effects.
What I'm not sure about is if the stream needs to be secret.  If
the listener knows how you permute, can that be then factored
into the timing statistics?  If not, then simply use the last decrypt
in the mode as a seed to create the next table.  If it has to be
kept secret then generate a new xor-chain that is keyed from
original secret key (including an enlarged key).  Either way,
it seems as if the "PRNG" of past decrypts would solve the table
keying need.
Further as there is no coordination required in the table keying,
the decryptor has wide flexibility in strategies, such as using
the secret key to hash the last ciphertext to hash the table.
There are two distinct classes of problems here - fixes that
would work on AES, and fixes that would work on any block
cipher.  Your neat idea falls into the former.

@_date: 2005-03-04 11:40:01
@_author: Ian G 
@_subject: NSA names ECC as the exclusive technology for key agreement and digital 
NSA names ECC as the exclusive technology for key agreement and digital
signature standards for the U.S. government
Certicom's ECC-based solutions enable government contractors to add security
that meets NSA guidelines
    MISSISSAUGA, ON, March 2 /CNW/ - Elliptic Curve Cryptography (ECC), a
strong, efficient public key cryptosystem, will soon become the standard to
protect U.S. government communications. On February 16, 2005 at the RSA
conference, the National Security Agency (NSA) presented its strategy and
recommendations for securing U.S. government sensitive and unclassified
communications. The strategy included a recommended set of advanced
cryptography algorithms known as Suite B for securing sensitive and
unclassified data.
    The only public key protocols included in Suite B are Elliptic Curve
Menezes-Qu-Vanstone (ECMQV) and Elliptic Curve Diffie-Hellman (ECDH) for key
agreement and Elliptic Curve Digital Signature Algorithm (ECDSA) for
authentication. The Advanced Encryption Standard (AES) for data encryption
and SHA for hashing are also included. All of the Suite B algorithms are
consistent with the National Institute of Standards and Technology (NIST)
    Interoperability and information sharing are two key principles in the
NSA strategy. In his remarks, Daniel Wolf, the NSA's information assurance
director discussed the importance of sharing information between departments
and using consistent and strong standards to protect that information. The
NSA recommends that the same level of security that is used to protect
mission critical information - ECC-based protocols - now be extended to
protect sensitive and unclassified data.
    "The NSA strategy is major news for the security industry and all
government agencies or suppliers because it sets the security standards for
at least the next few decades. The NSA has stated that there are more than
1.3 million cryptographic devices in the U.S. inventory, over 75 percent of
which will be replaced during the next decade under the U.S. Crypto
Modernization Program," said Scott Vanstone, Certicom's founder & executive
vice-president strategic technology. "A system is only as strong as its
weakest link. By using the same high level of protection for all
communications, especially security that is standards-based and
interoperable, agencies and all organizations can establish a trusted system
that is much harder to compromise."
    ECC is a publicly-available algorithm and Certicom is known as the ECC
pioneer and expert, having researched and developed ECC-based
implementations and security for the past 20 years. In 1997, Certicom
developed the industry's first toolkit to include ECC which has since been
adopted by over 300 organizations. Today, its Certicom Security
Architecture, a modular set of security services, software cryptographic
providers (including a FIPS 140-2 Validated cryptographic module), and board
support packages, enables device
manufacturers and other government suppliers to easily add strong, efficient
cryptography that meets the NSA recommendations and NIST publications.
About Certicom
Certicom Corp. (TSX: CIC) is the authority for strong, efficient
cryptography required by software vendors and device manufacturers to embed
security in their products. Adopted by the US government's National Security
Agency (NSA), Certicom technologies for Elliptic Curve Cryptography (ECC)
provide the most security per bit of any known public key scheme, making it
ideal for constrained environments. Certicom products and services are
currently licensed to more than 300 customers including Motorola, Oracle,
Research In Motion, Terayon, Texas Instruments and Unisys. Founded in 1985,
Certicom is headquartered in Mississauga, ON, Canada, with offices in
Ottawa, ON; Reston, VA; San Mateo, CA; and London, England. Visit
Certicom, Certicom Security Architecture, Certicom CodeSign, Security
Builder, Security Builder Middleware, Security Builder API, Security Builder
Crypto, Security Builder SSL, Security Builder PKI, Security Builder NSE and
Security Builder GSE are trademarks or registered trademarks of Certicom
Corp. All other companies and products listed herein are trademarks or
registered trademarks of their respective holders.
Except for historical information contained herein, this news release
contains forward-looking statements that involve risks and uncertainties.
Actual results may differ materially. Factors that might cause a difference
include, but are not limited to, those relating to the acceptance of mobile
and wireless devices and the continued growth of e-commerce and m-commerce,
the increase of the demand for mutual authentication in m-commerce
transactions, the acceptance of Elliptic Curve Cryptography (ECC) technology
as an industry standard, the market acceptance of our principal products and
sales of our customer's products, the impact of competitive products and
technologies, the possibility of our products infringing patents and other
intellectual property of fourth parties, and costs of product development.
Certicom will not update these forward-looking statements to reflect events
or circumstances after the date hereof. More detailed information about
potential factors that could affect Certicom's financial results is included
in the documents Certicom files from time to time with the Canadian
securities regulatory authorities.
For further information: please contact: Tim Cox, Zing Public Relations,
(650) 369-7784, tim at zingpr.com; Brendan Ziolo, Certicom Corp.,
(613) 254-9267, bziolo at certicom.com
? 2005 CNW Group

@_date: 2005-03-07 19:18:02
@_author: Ian G 
@_subject: datamining the NSA 
The recent story on datamining the NSA is getting a lot of
airplay here in Austria.  I gather it was slashdotted, and
can be found on "datamining the NSA."  We just listened to
a half hour long interview in prime time (7pm) on national radio
on the subject.  In german.  (Accidentally, I happen to be
jacked in at the offices of the organisation that did the
datamining, or some such.)  It's also been in major newspapers,
so I'm told.
The background of the story appears to have a lot of relevence
to the wider identity debate.  In brief, it looks like there
was a several-year tussle between the NSA (voice recognition),
the FBI (fingerprints) and the British (iris scanning) over
what the standard for biometrics should be.  No mention yet
as to whether any of this data should be encrypted and thus
protected from aggressive threats, such as those mooted in
the passport-rfid debate.
There are also eyebrow-raising comments on how important
components of the biometric technology was developed by
American firms as standard approaches for american systems,
and is now owned by the French government...

@_date: 2005-03-11 21:24:02
@_author: Ian G 
@_subject: $90 for high assurance _versus_ $349 for low assurance 
In the below, John posted a handy dandy table of cert prices, and
Nelson postulated that we need to separate high assurance from low
assurance.  Leaving aside the technical question of how the user
gets to see that for now, note how godaddy charges $90 for their
high assurance and Verisign charges $349 for their low assurance.
Does anyone have a view on what "low" and "high" means in this
context?  Indeed, what does "assurance" mean?
 >  shows that this CA runs
 > two classes, high assurance and low assurance.
 >
 > Do they have two roots that correspond to these two classes?
 > If not, how can users choose to trust high assurance separately
 > from (perhaps instead of) low assurance certs?
 >
 > I think mozilla's policy should require separate roots for separate
 > classes of assurance.  Alternatively, we could require separate
 > intermediate CAs for each class, issued from a common root, but
 > then the intermediates would have to be shipped with mozilla so
 > that they can be marked with explicit trust.

@_date: 2005-03-15 16:00:46
@_author: Ian G 
@_subject: Encryption plugins for gaim 
Just a quick note of clarification, there is a collision
in the name Ian G.  4 letters does not a message digest
Gaim-otr as I understand it is authored by Nikita Borisov
and Ian Goldberg .  It can be acquired
   and here are some other links:
   Just to confuse the issue I also am working on a private
instant messaging service which is markedly different, in
that I am taking a payment system and reworking it into an
IM system:
   But I haven't got around to a download yet.  And it's not
AIM compatible, as it works through its host payment system.
Over to Ian G.

@_date: 2005-03-15 19:50:54
@_author: Ian G 
@_subject: Encryption plugins for gaim 
My thoughts are similar.  When I first got into the
design, I thought that the privacy aspects of the
protocol would be integral with the messaging system,
but that proved to be not the case.
For several reasons, I think the privacy layer is
going to end up being totally divorced from the messaging
layer.  As a stab at these:
    *  there are many messaging systems, and there are
       efforts at integrating these, so any decent
       privacy layer has to think about hops,
    *  we desperately want to preserve many messaging
       systems in violent competition,
    *  any privacy layer that involves a "decrypt at
       server and then re-encrypt" is not a privacy
       layer, as the threat is 99.9% at the node
       (all three - alice, bob, server) and not on
       the wire,
    *  involving the server in any identity and privacy
       concerns brings up conflicts such as asking the
       server to know who the user is, excrow, liability,...,
    *  messaging systems move at different paces and
       incorporating crypto into them may result in
       yoyo behaviour for safe chat - there today,
       gone tomorrow on the new alpha,
    *  the final authentication - alice of bob and v.v.
       - is something that is best done divorced from the
       lowtech as much as possible, so that means some
       sort of plugin and leveraging off pgp-style WoT.
       Integrating that step into the messaging system
       gives you "S/MIME authentication" which doesn't
       scale.
That was scratched off without pause...
Hence, my own efforts will probably go in these two
parallel directions:
     *  opportunistic key exchange followed by chat
        in SDP1 over SOX.  (Note that SOX is also
        encrypted client-to-server so for much of
        the journey packets will be doubly encrypted,
        but end-to-end is the target).  This method
        will be integrated and fast but lack user
        authentication.  This is uninteresting to
        anyone outside the SOX world.
     *  OpenPGP packets without any interference,
        and a sort of plugin ability to bootstrap
        a fast key exchange, with fingerprint display.
        Key signing to follow later...  Now this is
        much more interesting as conceivably the same
        protocol would (once designed!) work over
        email, Jabber, AIM, etc.  At least, that would
        be the intention.
I'm not sure what this obsession with digital signatures
over messages is.  That probably wants to be unwound.  If
people are "signing a contract" over chat or indeed email,
then they probably need a lot more support in the tech and
a lot more warning, training, and legal support as to the
ramifications.  C.f.,
I agree that encrypting a chat message straight GPG/OpenPGP-
over-IM would probably be clunky.  I was more envisaging
using OpenPGP to handle the clunky key exchange and then
go fast from there.
(Note however that my efforts are towards integrating
two separate disparate systems - payments and IM - and
I am less concerned with the privacy aspects as Ian
Goldberg is.  This is one area where I'm adopting a
wait and see attitude because I'm not convinced that
this is an entirely tech issue.  But whichever, when
we get to that stage there is nothing wrong with doing
several possibilities.)
iang (the other other one)

@_date: 2005-03-21 00:42:02
@_author: Ian G 
@_subject: how to phase in new hash algorithms? 
The wider question is how to get moving on new hash
algorithms.  That's a bit tricky.
Normally we'd look to see NIST or the NESSIE guys
lead a competition.  But NESSIE just finished a
comp, and may not have the appetite for another.
NIST likewise just came out with SHA256 et al, and
they seem to have a full work load as it is trying
to get DSS-2 out.
How about the IACR?  Would they be up to leading
a competition?  I don't know them at all myself,
but if the Shandong results are heard at IACR
conferences, then maybe it's time to take on a
larger role.
Most of the effort could be volunteer, and it would
also be easy enough to schedule everything aligned
with the conference circuit.
Just a thought.  Anyone know anyone at the IACR?

@_date: 2005-03-23 10:48:29
@_author: Ian G 
@_subject: What is to be said about pre-image resistance? 
Collision resistance of message digests is effected by the birthday
paradox, but that does not effect pre-image resistance.  (correct?)
So can we suggest that for pre-image resistance, the strength of
the SHA-1 algorithm may have been reduced from 160 to 149?  Or can
we make some statement like "reduced by some number of bits that may
be related to 11?"
Or is there no statement we can make?
PS: There is a nice description (with a bad title) here for the
amateurs like myself:

@_date: 2005-03-29 15:17:59
@_author: Ian G 
@_subject: how email encryption should work 
Hi James,
I read that last night, and was still musing on it...
For clarity reasons, I think you mean that the
default should be to not invoke the 'extra stuff'
on automagic creation, rather than "do nothing"
which is in fact what users get today - nothing.
Concur.  The notion that a user needs a cert
from anyone else for the purpose of email is
wrong;  this doesn't mean denying them so they
can take part in corporate nets for example,
but that ordinary users in ordinary email will
not get much benefit from certs signed by other
I would put this in the "extra stuff" category,
and not in the default category.
The reason is that it creates a dependency on a
server that might not exist and even if a good
idea, will take a while to prove itself.
The problem I've discovered with this is that the
signing of mail is (I suggest) not a good idea
unless you have a good idea what the signature
means.  I've not seen anywhere where it sets out
what a signature means for S/MIME.  For OpenPGP
the signature is strictly undefined by the code,
so that's a better situation - it means whatever
you want it to mean.
Which means that most people under most circumstances
should not send most emails out signed.  Which sort
of makes "signed emails" a poor carrier pigeon for a
key exchange.
(I don't have a solution to this - just pointing
out what I see as a difficulty.  The workaround is
that the user turns off signing and has to send
an explicit blank signed email as a key exchange
message.  Clumsy.)
(One possibility is to put the cert in headers.)
Yes this would help a lot.  Any petname set should
be displayed distinctly from the default name.
(Oh, as a nitpick, a default address is not a petname,
it's just a default name.  A petname has to be set by
the user to exist.)
Right, the UI could do a lot to show what is possible
by shading the various email addresses or adding little
icons to indicate their encryptability state.
I had an entertaining read of the paper on Naive
Sign & Encrypt last night.  There are a lot of
issues in how signatures are combined with encryption,
I don't think this is a solved issue by any means
when it comes to email.
See caveat about signing above.  I certainly agree
that any message that can be encrypted should be
If you thought that the threat was changing emails
etc etc then it might be possible to create a cert
that could just be used for message integrity and
stick some tag in there that says "message integrity
only purposes."  Then, as long as the client at the
other end said "this message was delivered without
change" instead of "this message was signed" then
this might work.
When cryptograpers say a message is signed, users
think that a message has been signed....  getting
around that confusion is quite hard.
Yes, the model for an unobtrusive message is the
red cross alert bars added in Firefox these days.
They solve the popup nightmare quite nicely.

@_date: 2005-03-29 15:49:46
@_author: Ian G 
@_subject: aid worker stego 
This is the area that cryptorights.org has been
looking at.  They were looking at creation of
tools to support aid workers and the like.
(I'm not sure if they are still active though.)
It's a very tricky problem because the details of
Alice's environment and the attacker's methodology
matter a lot.  For example, some countries run SSL
proxies and some other countries do not.
If I was the aid worker (by way of example) I would
prepare pgp emails on my laptop, transfer them to
a public machine and email them out that way.  And
then scrub the laptop of the email itself.  Also,
I'd be inclined to use a one-time-password situation
as seizure of keys and compelled revelation would be
a threat.
But whether that works for every aid worker I doubt,
because it assumes things like ... laptops!
Stego itself is the sort of thing that can make
matters much worse if it is discovered, because it
indicates that you have something to be scared of,
and makes you a target.  Better to just routinely
encrypt everything and put in lots of chit chat.

@_date: 2005-03-29 16:06:05
@_author: Ian G 
@_subject: Secure Science issues preview of their upcoming block cipher 
Aha!  I seem to recall on this very list about
2 years back, Tom got crucified for trying to
invent his own simple connection protocol.  He
withdrew from doing useful work in creating a
new crypto protocol because of criticism here,
and the world is a poorer place for it.
I'd be interested to hear why he wants to
"improve" on AES.  The issue with doing that
is that any marginal improvements he makes
will have trouble overcoming the costs
involved with others analysing his work.
Using AES is just efficient, it allows us all
to say, "right, ok, next question" in 2 seconds
and then easily recommend his product.
Still, even if he hasn't got any good reasons,
I'd still support his right to try.

@_date: 2005-05-02 10:45:11
@_author: Ian G 
@_subject: calling all French-reading cryptologers - Kerckhoff's 6 principles needs a translation 
It's been a year or so since this was raised, perhaps there are
some French reading cryptologers around now?
----------  Forwarded Message  ----------
 Financial Cryptography Update: HCI/security - start with Kerckhoff's 6
 principles
                              May 01, 2005

@_date: 2005-05-12 16:21:11
@_author: Ian G 
@_subject: [Fwd] Advances in Financial Cryptography - "First Issue" 
Advances in Financial Cryptography - "First Issue"
                              May 11, 2005

@_date: 2005-05-20 23:24:45
@_author: Ian G 
@_subject: Malaysia car thieves steal finger 
Photo of an advert that ran in Germany.  You need
German for the words but that's not necessary.

@_date: 2005-05-31 14:45:56
@_author: Ian G 
@_subject: Citibank discloses private information to improve security 
I've heard an anecdote on 2 out of 3 of those criteria:
In a bank that makes heavy use of SSH, the users have
to phone the help desk to get the key reset when the
warning pops up.  The users of course blame the tool.
I suspect in time the addition of certificate based
checking into SSH or the centralised management
of keys will overcome this.

@_date: 2005-05-31 17:33:40
@_author: Ian G 
@_subject: Papers about "Algorithm hiding" ? 
I don't agree with your conclusion that hiding algorithms
is a requirement.  I think there is a much better direction:
spread more algorithms.  If everyone is using crypto then
how can that be "relevant" to the case?
I would suggest that the best way to overcome this
flawed view of cryptography by the judges is to have
the operating systems install with GPG installed by
default.  Some of the better ones already install SSH
by default.
(In fact the thrust of the argument was flawed as the
user's PC almost certainly had a browser with SSL
installed.  As HTTPS can be used to access webmail
privately and as we have seen this was an El Qaeda
means of secret communication, the presence of one
more crypto tool as "relevent" is a stretch.)

@_date: 2005-05-31 18:31:04
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
I think you meant to write that James' reasoning is
circular, but strangely, your reasoning is at least as
unfounded - correlation not causality.  And I think
the evidence is pretty much against any causality,
although this will be something that is hard to show,
in the absence.
 * AFAICS, a non-trivial proportion of credit
card traffic occurs over totally unprotected
traffic, and that has never been sniffed as far as
anyone has ever reported.  (By this I mean lots of
small merchants with MOTO accounts that don't
bother to set up "proper" SSL servers.)
 * We know that from our experiences
of the wireless 802.11 crypto - even though we've
got repeated breaks and the FBI even demonstrating
how to break it, and the majority of people don't even
bother to turn on the crypto, there remains practically
zero evidence that anyone is listening.
  FBI tells you how to do it:
  As an alternate hypothesis, credit cards are not
sniffed and never will be sniffed simply because
that is not economic.  If you can hack a database
and lift 10,000++ credit card numbers, or simply
buy the info from some insider, why would an
attacker ever bother to try and sniff the wire to
pick up one credit card number at a time?
And if they did, why would we care?  Better to
let a stupid thief find a way to remove himself from
a life of crime than to channel him into a really
dangerous and expensive crime like phishing,
box cracking, and purchasing identity info from
But this is totally incorrect!  The spyware installs on the
users' machines, and thus does not need to sniff the
wire.  The assumption of SSL is (as written up in Eric's
fine book) that the wire is insecure and the node is
secure, and if the node is insecure then we are sunk.
  Eric's book and "1.2 The Internet Threat Model"
  Presence of keyboard sniffing does not give us any
evidence at all towards wire sniffing and only serves
to further embarrass the SSL threat model.
Yes, that's being tried now too.  This is I suspect the
one area where the SSL model correctly predicted
a minor threat.  But from what I can tell, server-based
DNS hijacking isn't that successful for the obvious
reasons (attacking the ISP to get to the user is a
higher risk strategy than makes sense in phishing).
User node-based hijacking might be more successful.
Again, that's on the node, so it can totally bypass any
PKI based protections anyway.
I say "minor threat" because you have to look at the big
picture:  attackers have figured out a way to breach the
secure browsing model so well and so economically
that they now have lots and lots of investment money,
and are gradually working their way through the various
lesser ways of attacking secure browsing.
As perhaps further evidence of the black mark against
so-called secure browsing, phishers still have not
bothered to acquire control-of-domain certs for $30
and use them to spoof websites over SSL.
Now, that's either evidence that $30 is too much to
pay, or that users just ignore the certs and padlocks
so it is no big deal anyway.  Either way, a model
that is bypassed so disparagingly without even a
direct attack on the PKI is not exactly recommending

@_date: 2005-05-31 22:42:38
@_author: Ian G 
@_subject: "SSL stops credit card sniffing" is a correlation/causality myth 
This leads to a big issue.  If there are no reliable reports,
what are we to believe in?  Are we to believe that the
problem doesn't exist because there is no scientific data,
or are we to believe those that say "I assure you it is a
big problem?"
It can't be the latter;  not because I don't believe you in
particular, but because the industry as a whole has not
the credibility to make such a statement.  Everyone who
makes such a statement is likely to be selling some
service designed to benefit from that statement, which
makes it very difficult to simply believe on the face of it.
The only way we can overcome this issue is data.  If
you have seen such situations, document them and
report them - on forums like these.  Anonymise them
suitably if you have to.
Another way of looking at this is to look at Choicepoint.
For years, we all suspected that the real problem was
the insider / node problem.  The company was where
the leaks occurred, traditionally.
But nobody had any data.  Until Choicepoint.  Now we
have data.  We know how big a problem the node is.
We now know that the problem inside the company is
So we need to see a "Choicepoint" for listening and
sniffing and so forth.  And we need that before we can
consider the listening threat to be economically validated.
And I'd expect to see massive email scanning by
now of say lawyer's email at ISPs.  But, no, very
little has occurred.
In this we agree.  Indeed, my thrust all along in
"attacking PKI" has been to get people to realise
that the PKI doesn't do nearly as much as people
think, and therefore it is OK to consider improving
it.  Especially, where it is weak and where attackers
are attacking.
Unfortunately, PKI and SSL are considered to be
sacrosanct and perfect by the community.  As these
two things working together are what protects people
from phishing (site spoofing) fixing them requires
people to recognise that the PKI isn't doing the job.
The cryptography community especially should get
out there and tell developers and browser implementors
that the reason phishing is taking place is that the
browser security model is being bypassed, and that
some tweaks are needed.
Can you distinguish between break-ins and sniffing
and listening attacks?  Break-ins, sure, I've seen a
few cases of that.  In each case the hackers tried to
break into an unprotected site that was accessible
over an unprotected 802.11.
My point though is that this attack is not listening.
It's an access attack.  So one must be careful not
to use this as evidence that we need to protect
data from being listened to.
I've seen a few reports of DNS hijacking for phsishing over
the last year.  In each case that I saw, the eventual conclusion
was that it wasn't a sensible attack, it was under control,
and the attacker did himself mischief by potentially leading
the ISPs back to him.
However I was specifically interested in phishing - attacks
of direct economic theft - rather than nuisance attacks or
attacks related to indirect economic effects such as access
or DOS, etc.  I know a lot of that goes on.
It if is anything other than that, let us know.  We need
more data.  Without the data it's just more FUD.  Schechter
and Smith's FC03 paper went further and suggests that lack
of data is part of the problem of security.

@_date: 2005-11-02 14:02:54
@_author: Ian G 
@_subject: Some thoughts on high-assurance certificates 
In your long and interesting email you outlined
some issues with the tool known as PKI.  What I'm
curious about is why, given these issues and maybe
100 more documented elsewhere **, you propose that:
    "Getting PKI baked into the every day representations
    people routinely manage seems desirable and necessary to me."
We have this tool.  It has many and huge issues.
What I don't understand is why the desire is so
strong to put this tool into play, when it has
singularly failed to prove itself?
Where does the bottom-up drive come from?  Why is
it that what "people do routinely" isn't driven
top-down, so that the tools they need are application
driven, but is instead subjugated to the tools-first
approach, even against such negative experience and
** some here:

@_date: 2005-11-18 20:48:14
@_author: Ian G 
@_subject: "ISAKMP" flaws? 
Actually, if one variable-length integer
encoding is used instead of 5 other formats
in all sorts of strange places, I'd say this
is a good sign.  Although I didn't originally
like the variable-length integer I've seen
used, I've come to appreciate how much simpler
and thus much more secure it makes the code.
Containers for things are inevitable.  I've
found they should be encapsulated in their
own protected container, so that bugs do not
cross boundaries.  Yes, this makes for redundancy
and possibly conflict, but wasn't it said that
in security programming, we should be precise
in what we write out and precise in what we
accept?  Any conflict - reject it.
PS: I think it was Dan Bernstein who said that,
in opposition to the aphorism "be gentle in what
you accept?"

@_date: 2005-11-19 13:24:00
@_author: Ian G 
@_subject: Haskell crypto 
Someone mailed me with this question, anyone know
anything about Haskell?
-------- Original Message --------
I just recently stepped into open source cryptography directly, rather
than just as a user.  I'm writing a SHA-2 library completely in
Haskell, which I recently got a thing for in a bad way.  Seems to me
that nearly all of the message digest implementations out there are
written in C/C++, or maybe Java or in hw as an ASIC, but I can't find
any in a purely functional programming language, let alone in one that
can have properties of programs proved.  Haskell can, and also has a
very good optimizing compiler.  I'm not sure where to submit for
publication when I'm done and have it all written up, though!

@_date: 2005-10-14 16:22:31
@_author: Ian G 
@_subject: NSA Suite B Cryptography 
I didn't read it that way at all.  AFAICS,
the NSA has acquired the licences it needs
to deliver (have delivered) software to its
government customers.  As all the government
customers will need to use approved software
anyway, it will be acquired on some approved
list, and the licences will be automatically
Anyone outside the "national security" market
will need to negotiate separately with Certicom
if they need to use it.  This represents a big
subsidy to Certicom, but as they are a Canadian
company it is harder to argue against on purely
statist grounds.
Which is to say, NSA solved its problem and it
is nothing to do with FOSS.
The big question (to me perhaps) is where and
how far the Certicom patents are granted.  If
they are widely granted across the world then
the software standards won't spread as there
won't be enough of an initial free market to
make it bloom (like happened to RSA).  But if
for example they are not granted in Europe
then Europeans will get the free ride on NSA
DD and this will cause the package to become
widespread, which will create the market in
the US.  Of course predicting the future is

@_date: 2005-10-22 13:43:50
@_author: Ian G 
@_subject: [fc-discuss] Financial Cryptography Update: On Digital Cash-like 
Huh - first I've heard of that, would be
encouraging if that worked.  How does it
handle an intermediary fall guy?   Say
Bad Guy Bob extorts Alice, and organises
the payoff to Freddy Fall Guy.  This would
mean that Alice can strip her blinding
factors and reveal that she paid to Freddy,
but as Freddy is not to be found, he can't
be encouraged to reveal his blinding factors
so as to reveal that Bob bolted with the

@_date: 2005-09-02 22:35:21
@_author: Ian G 
@_subject: AES implementation in C - any recommendations? 
I'm after an AES implementation in C, preferably with
something approximating BSD/open licence.  Does anyone
have a view on which would be a current favourite?
(I'm writing a protocol that needs it, and would like
to deliver the code totally complete, but with switches
to turn on ones other favourite libraries as appropriate.)

@_date: 2005-09-07 19:08:58
@_author: Ian G 
@_subject: [Anti-fraud] Re: Another entry in the internet security hall 
This statement is only plausible if you consider
the paper cryptography domain.  When applied to
the business / user world, the statement fails
due to the way that real life breaks the assumptions.
Generally, for most apps, there is already a way
to share stuff.  Just to look at one particular
application such as online banking, the bank and
the user generally communicate through post and
other means such as email at a minimum so as to
set up a relationship.  These methods may not be
secure (according to paper crypto metrics) but
they are multi-factor and are uncorrelated with
the threats.  So they work;  so the keys can be
shared securely, according to some risk measure.
Shared keys validate in that they are shared; the
keys themselves aren't sent over the wire in the
setup, and if the other party doesn't have the key,
then the setup fails.  This amounts to validation
of identity being measured by "has a copy of my key
Now, you'll probably think this is woefully insecure
because it falls to MITM.  True, but so does most every
other system.  online browsing fails to MITM by means
of phishing in such an evidently purile fashion that
heads should be hung with shame .. if not lopped off.
Even if the browser were to do more here, the MITM is
still possible within the ivory tower of the CA,
which have't exactly inspired of late given that
they now sell lots and lots of domain-certs for
bargain basement prices.  ($7 was the latest I saw!)
So, in a business context, PSK does identity validation
more or less as well as anything else, at least on
paper (coz it hasn't been tried yet!).
Not really.  With SSL in the browser you have
approximately zero assurance that anyone verified
it.  If you look at the browser, and find a padlock
that gives you maybe 5% of what you need.  If you
go searching into the cert then you might be able
to establish the CA which would perhaps give you
5-20% of what you need, but to actually work out
whether a website is really the right one, you
are going to have to go elsewhere for assurance.
Sure.  When it matters, expect phishers to set up
SSL sites, to steal domains, to steal email confirmations,
to do all sorts of things.  Right now, they are dealing
with low hanging fruit.
No, I'd challenge your underlying assumption here that
the intention is to deliver trust.  Trust cannot be
delivered, it can't be sent, it can't "lie" anywhere.
Trust is something that only each individual can find
for themselves on their own checks.  Trust never leaves
a person.
What the system can do is make statements and present
evidence.  It's up to the user to decide whether to
trust those statements and whether to seek further
evidence or risk it with what she has.
The difference in these two approaches is immense.  In
your view you have to get it right;  except you have no
way to establish "trust" that actually makes sense and
hence you're trapped into an ever increasing quality
cycle, while the businesses selling that "trust" are
trapped in an ever decreasing quality cycle!
In the alternate view, the system simply has to present
a set of evidence.  PSKs presents a good statement - both
parties have the keys.  But others might too!  PKI presents
some other statements, with their own set of flaws.  It's
up to the user to assess her own risk profiles and decide
beyond that.
You don't necessarily know these things but the question
you are asking is about someone else's threat model, it
isn't about the threat model that the average user faces
when dealing with the average web site.

@_date: 2005-09-14 12:42:45
@_author: Ian G 
@_subject: [Anti-fraud] simple (&secure??) PW-based web login (was Re: Another 
(Minor point - if relying on incrementing
Iterations, this may impact password sharing
scenarios.  Whether that's a good thing or a
bad thing depends...)
I suspect this would not work so well in the
(common enough?) cases where a site uses a farm
of SSL boxes and certs;  a couple of sites I've
come across provide different certs every time
(although admittedly I saw this with IMAP TLS not
with browsing).
What is the reason for hashing twice?  Instead of
the more obvious H(0)=h(PK, r(PK), PW) ?
(Also, you are missing a closing parenthesis there
so maybe your intent was other.)
(Somewhat challenging your assumptions here) your
design does not seem to cope with MITM.  But, it
may do if you are assuming there is an extension
that is handling the client side, and the exchange
is the setup for later transactions, not the
transaction itself:  the server can send back its
token X which needs to be further hashed in order
to gain the useful token for later:
    Y = h(..., X, PW);
where Y could be the session identifier or cookie
or something.  In this way both sides have proven
their possession of the password and hopefully
eliminated other parties from further comms.
(But I may have misunderstood something...)

@_date: 2005-09-21 10:43:06
@_author: Ian G 
@_subject: [dave@farber.net: [IP] more on  ARMSTRONG LECTURE on Quantum 
Eugen Leitl forwarded:
There's two problems with this - I can't think of
a valid use case where anyone would need to keep a
secret for 50 years.  That *is* the
time that states set ... and when you read about
the stuff that does get released after that time,
nothing stands out as having to have been protected
for that long.
Secondly, if we are talking about protecting fiber,
just add some additional packets.  Fill it up.  You
can put enough stuff over the fiber to fill up
disk drives within seconds, so what sort of adversary
are we talking about that wants to keep fiber-loads
of data for 50 years?
Quantum Key Exchange has all the hallmarks of a cute
toy for boys in big companies, and none of the hallmarks
of a useful application.  It's not even like the laser,
stuck with a cool physics waiting for someone to work
out what to do with it.

@_date: 2006-02-24 18:09:55
@_author: Ian G 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Usability is the issue.  If I look over onto
my skype window, it says there are 5 million
or so users right now.  It did that without
any of the hullabaloo of the other systems,
and still manages to encrypt my comms.  By
some measures it is the most successful crypto
system ever.
Over on Ping's site there is this little essay
about something or other:
Which starts out:
   "So, right up front, here is the key property of this proposal:
    _using it is more convenient than not using it_. "
Which relates back to Kerchoffs' 6th principle.
To add to that:
To get people to do something they will say "no"
to, we have to give them a freebie, and tie it
to the unpleasantry.  E.g., in SSH, we get a better
telnet, and there is only the encrypted version.
In skype we get a cheaper phone call, and there
is only the encrypted version.
The problem with PGP is that there is no loss
leader in it, and it is possible to turn it off.
Same with SSL.  So that's what people do - they
say no.

@_date: 2006-02-25 19:33:38
@_author: Ian G 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Well!  Within the context of any given application,
we can learn lessons.  Just because SSH is only used
by geeks is meaningless, really, we need to ground
that criticism in something that relates it to other
areas.  The fact is that SSH came in with a solution
and beat the other guy - Telnet secured over SSL.  It
wasn't the crypto that did this, it was the key management,
plain and simple.
Telnet was in widespread use - but was incapable of
making the jump to secure.  Just like email.  So if
the SSH example were illuminating, we would predict
that some completely different *non-compatible* app
would replace email.
Hence, IM/chat, Skype, TLS experiments at Jabber, as
well as the OpenPGP attempts.
There are important lessons to be learnt in the rise of
IM over email.  Email is held back by its standardisation,
chat seems to overcome spam quite nicely.  Email is hard
to get encrypted, but it didn't stop Skype from doing
encryped IMs "easily."  Phishing is possible over chat,
but has also been relatively easy to address - because
the system owners have incentives and can adjust.
The competition between the IM systems is what is driving
the security forward.  As there is no competition in the
email world, at least at the level of the basic protocol
and standard, there is no way for the security to move

@_date: 2006-01-10 16:10:44
@_author: Ian G 
@_subject: long-term GPG signing key 
1. Signing keys face a different set of
non-crypto threats than to encryption
keys.  In practice, the attack envelope
is much smaller, less likely.  Unless you
have particular circumstances, it's not
as important to have massive strength in
signing keys as it is in encryption keys.
2. DSA has a problem, it relies on a 160
bit hash, which is for most purposes the
SHA-1 hash.  Upgrading the crypto to cope
with current hash circumstances is not
worthwhile;  we currently are waiting on
NIST to lead review in hashes so as to
craft a new generation.  Only after that
is it possible to start on a new "DSA".
So any replacement / fix for DSA is years
away, IMO.  The OpenPGP group has wrestled
with this and more or less decided to defer
3. The RSA patent expired, which means that
RSA no longer has everyone over a barrel.
For various reasons, many projects are
drifting back to RSA for signing and for
If you want something stronger, then I'd
suggest you just use a big RSA key for

@_date: 2006-01-11 08:04:19
@_author: Ian G 
@_subject: long-term GPG signing key 
Well, yes, depends on the application of course!
With this particular application - signing
people's keys for WoT - that's generally true.
If I was to crack your signing key for example,
then wander around impersonating you, this is
unlikely to do anything useful except confuse
people a lot until you all figure it out.
If we limit our discussion to actual extent
and popular protocols, it is easier to see.
Take for example this *extreme* case of the CA
application.  If I was to publish Verisign's
private key on usenet, what difference would
that make?
Other than a lot
of red faces, not as much as one would think;
they would simply roll another key, then re-sign
everyone's certs and post them out with a free
year for the nuisance factor.  Then a CERT
advisory would tell every "merchant" to roll
over their certs, and browsers would ship new
(Actually it's probably worse than that.  We
stand at the cusp of SSL attacks, 450 seen
last year, so this would spur a bunch of forged
cert attacks.  Compare this to a couple of years
back when someone noticed that IE had a cert
bug in it, and nobody noticed.  And nobody ever
bothered to attack it.)
But that's the *extreme* case, more or less like
Microsoft faces every month.
For the regular case of say Amazon's private key,
well, Amazon would have a lot of nuisance to
deal with, but in practice it would just be in
up-tick in normal phishing against them for a
few months.
Various random posts:
Netcraft - 450 phishing cases using SSL / HTTPS certs
RSA comes clean: MITM on the rise, Hardware Tokens don't cut it, Certificate Model to be Replaced!
GP4.3 - Growth and Fraud - Case  - Phishing
I don't think EC is available for OpenPGP although
GPG may have some experimental product in it?
On the whole - another complete generalisation -
open projects tend to shy away from EC as there
is no clear patent situation, and putting all
the work in only to discover some claim later on
is not effective use of time.  Our Cryptix project
to do EC in Java (Paulo in Brazil) stalled when he
discovered that the so-called "unencumbered" set
was actually quite slow...

@_date: 2006-01-11 15:00:02
@_author: Ian G 
@_subject: long-term GPG signing key 
Well, it's a pragmatic situation:
   * all SHA algorithms are under a cloud
   * anything 160 bits or less is under a dark-ish cloud
   * the bigger ones won't break, but maybe
     the engineering will all change anyway
   * DSA has to be upgraded anyway
   * what's wrong with RSA in this role?
   * where's the threat to the DSA algorithm given that
     the attack is the birthday attack?
   * where's the threat to any extent usage of DSA
     (within its application profile)?
Pragmatically, wait and see is a good choice here,
IMO, but others disagree.
DSA is fixed to a 160 bit hash (or is it DSS?).
So, it's possible to do RIPEM or a chopped off
version of SHA-256.  The question is, what does
that gain you?  Not that much, and probably not
as much as the pain of rolling out a new digsig
Well, using two different MDs to cover one
failing is a plausible idea - but at a logical
and cryptographic level, all you are doing is
inventing your own hash algorithm, constructed
from some prior work.
So, we can look at for example cipher chaining
like triple-DES.  There are strange artifacts
such as groups where non-obvious things come
in and trip you up.  Even though triple-DES
is still considered to have avoided that trap,
its relatively small block size means you can
now put the entire decrypt table on a dvd (or
somesuch, I forget the maths).
So in general, it's not a good idea to just
invent your own algorithms;  if you could do
better so easily, so could the professional
cryptographers, and they would have by now.

@_date: 2006-01-11 15:16:06
@_author: Ian G 
@_subject: long-term GPG signing key 
You seem to have missed the next sentance:
    ".... Unless you have
    particular circumstances, it's not
    as important to have massive strength in
    signing keys as it is in encryption keys."
As he asked "what the current recommendation
is" it seems reasonable to assume the general
case, not the particular, and invite him to
elaborate if so needed.  Etc etc.
Errata - if you (Travis) are using 4096-bit D/H
as your encryption keys, you might want something
a bit beefier for signing keys.  Check out
the key length calculator:
and click on "NIST 2005 Recommendations" and
also "ECRYPT 2005 Report" for comparison.

@_date: 2006-01-13 15:32:52
@_author: Ian G 
@_subject: long-term GPG signing key 
Thanks for the correction, yes, so obviously I
muffed that one.  I saw it mentioned on this list
about a year ago, but didn't pay enough attention
to recall the precise difficulty that the small
block size of 8 bytes now has.
A few calculations here - even blueray disks won't
help me out.

@_date: 2007-04-19 23:09:36
@_author: Ian G 
@_subject: crypto component services - is there a market? 
Hmmm... last I heard, qualified certificates can only be issued to individuals, and invoicing (of the e-form that the regulations speak) can only be done by VAT-registered companies.
Is that not the case?  How is Germany resolving the Easier to invoice with paper!

@_date: 2007-12-06 16:26:48
@_author: Ian G 
@_subject: PlayStation 3 predicts next US president 
What is "correct" depends on requirements and semantics, and neither is well addressed in that paper nor in standards, w.r.t. email and signing.
Ditto, in terms of your question.
As an example (Ricardian Contract [1]), we might say that a signed contract is done as
    hash-digsig-hash
[2] With this procedure, the first hash-digsig is a human signing (classical cleartext openpgp signature) and the last hash is a signature that causes sharing of the exact document [3].
[1] To complete the picture, even this evidence is distributed by means of transactions over the document;  to be extreme, the end result is this:
     hash-digsig(hash-digsig(hash-digsig-hash))
[2] a public key signature is normally done hash-digsig, right?  So your sign-hash-sign might really be:
     hash-digsig-hash-hash-digsig
but that's a guess.
[3]

@_date: 2007-02-13 12:37:54
@_author: Ian G 
@_subject: Failure of PKI in messaging 
Actually, there are many problems.  If you ask the low-level crypto guys, they say that the HI is the problem.  If you ask the HI guys, they say that the PKI concept is the problem.  If you ask the PKI people, they say the users are not playing the game, and if you ask the users they say the deployment is broken ...  Everyone has got someone else to They are all right, in some sense.  The PKI concepts need loosening up, emails should be digsig'd for authentication (**), and the HI should start to look at what those digsigs could be used for.
But, until someone breaks the deadly embrace, nothing is going to happen.  That's what James is alluding to:  what part can we fix, and will it help the others to move?
** I didn't say digital signing ... that's another problem that needs fixing before it is safe to use, from the "ask the lawyers" basket.

@_date: 2007-07-01 18:30:27
@_author: Ian G 
@_subject: The bank fraud blame game 
Banks are the larger and more informed party.  They need to provide systems that are reasonable given the situation (anglo courts generally take this line, when pushed, I'm unsure what continental courts would do with that logic). Customers aren't in any position to dictate security requirements to banks.
Unfortunately for the banks, there is a vast body of evidence that we knew and they knew or should have known that the PC was insecure [1].  So, by fielding a system -- online commerce -- with a known weakness, they took responsibility for the fraud (from all places).
Now they are in the dilemma.  The customer can't provide evidence of the fraud, because the system fielded doesn't support it (it's login authentication not transaction authorisation).  The NZ response above is simply not facing up to the facts, it is trying to create an easy way out that (again) shifts the liability to the customer.
They now face the question of whether to roll-back online access or to upgrade with some form of dual-channel authorisation [2].
[1] To my knowledge, continental banks knew of the risks and acted in the 90s, then scaled it down because the risks proved overstated.  Brit banks knew of the risks and didn't care.  American banks didn't care.
[2] Again, continental banks are shifting to SMS authorisation (dual-channel) ... Brit banks are unsure what to do ... American banks apparently don't care.

@_date: 2007-07-19 18:10:38
@_author: Ian G 
@_subject: New article on root certificate problems with Windows 
I agree with all the above, including deleted.
To jump up and down ... if that was the solution, it would have been done by now :)
I would instead state that the solution was whatever Skype and SSH did.  And the opposite of whatever IPSec, SSL, Clipper, S/MIME, DRM, and all the other failures did.
HCI was one of the things, but others were as important: lack of open critique, service-before-security, crypto-for-free, total solution, narrow problem, etc.

@_date: 2007-06-21 15:47:21
@_author: Ian G 
@_subject: Blackberries insecure? 
(quick reply) they specifically mentioned the servers:
"The ban has been prompted by SGDN concerns that the BlackBerry system is based on servers located in the US and the UK,..."

@_date: 2007-03-30 22:55:24
@_author: Ian G 
@_subject: Governance of anonymous financial services 
The servers are not so relevant, as long as you have created legally firm transactions.  Although, in the event of collapse, the data trail suddenly becomes of critical importance, so there are limits to that.
The reserve assets' location(s) is fairly important from a customer trust perspective.  People look at the overall safety and make their own judgements.  One person might decide that New York is safe and another will find that a horrible thought (for those who follow this arcane field, there was a big bust of a dodgy operator in NY some months back).  Having said that, once a system is up and running, and is robust, it seems that moving the assets from one continent to another has not been a source of concern to many users.
The issuer himself is pretty important.  His physical location isn't so important -- everyone flies around these days -- but nobody has ever been able to gain trust in a system to date without reference to a real meatspace hook. And for good reason ... how do you take him to court?  (And if you are thinking of extra-jurisdictional transactions, how do you beat him to a pulp with a baseball bat?)
It does ... but only after the full governance story is put into place.  Then, we can look at ways to solve certain governance problems with crypto.
E.g., Ricardian contracts (my stuff) take the user agreement as a document and bind it into each transaction by means of the hash of the contract;  they also ensure various other benefits such as the contract being available and readable to all at all times, and the acceptability of same, by the simple expedient of coding the decimalisation into the contract.  Ensuring that the contract is readable, applicable and is available to all is a huge win in any court case.
Other governance tricks:  the usage of signed receipts can be used to construct a full audit of the digital system. Also, signed receipts are strong evidence of a transaction, which leads by some logic to a new regime which we call triple entry accounting.  This dramatically changes the practice of accounting (which feeds into governance).
With DB side, one trick is to use psuedonym accounts for the basis, and this allows no-loss protocols to be created. Again, this is useful for governance, because if you have a lossy protocol, you have a potential for fraud.
The essence is the contract.  In a classical online financial offering, this contract defaults to the user agreement.  This contract offers things to the user, and it offers it in the name of the Issuer.
If the contract offers nothing, you don't care who the Issuers is.  (Some contracts do offer you nothing...)
An Independent Auditor (of a valuable contract) would need to know the pedigree of the Issuer.  In evaluating the contract that is extended between the issuer and the holders of value, there needs to be some "meatspace mass" that says that the various clauses in the contract can be met.  E.g., If the issuer is totally anonymous and the contract says that the issuer will be good for a million of personal assets backing then this is a difficult clause to believe in.
Well, one of the things that is normally done is that the assets that reserve the contractual promises can be audited in some fashion.  For the gold people it was commonly suggested that cameras be used;  another possibility was to conduct an audit of reserves from time to time with a person of known integrity and independence, a different one each time, under the cameras.
It's well explored in Ricardo (my stuff).  The digital side is capable of being fully and completely audited (not that it is, but the signed receipt structure allows it).  5PM and the balance sheet approach tie the numbers to the contract and then across to the physical assets.  5PM can also be used to control the physical assets to a lesser extent, but there we find more need for physical auditing.  It's hard to go totally digital and cryptographic with a pallet of gold, unless we're in one of those Neal Stephenson novels.
The Independent Auditor is likely to demand the whole list and then to sample and test.  If not, he has to audit your formulas, and Auditors don't place much faith in crypto blah blah as a matter of principle.
With something like physical assets, it is hard to gain long term trust if you do not identify the location of the assets to some extent, at least in the early days.  Short term trust can be gained, this has been shown empirically, so if you are operating a transient payment system then that has more of a chance of getting away with missing elements of governance.  The smaller transactions cycle is completed so quickly that people know when things aren't working more Bear also in mind that the classical audit approach is designed for a static, snap-shot, long-distance approach. This is all topsy turvy these days.  You need to look more for open governance, rather than employing auditors, as otherwise you're wasting your money.
PS: disclosure, I write these things, and am also a auditing a non-FC system at the moment.

@_date: 2007-05-03 00:52:34
@_author: Ian G 
@_subject: Was a mistake made in the design of AACS? 
This seems to assume that when a crack is announced, all revenue stops.  This would appear to be false.  When cracks are announced in such systems, normally revenues aren't strongly effected.  C.f. DVDs.

@_date: 2007-05-11 16:42:47
@_author: Ian G 
@_subject: no surprise - Sun fails to open source the crypto part of Java 
Does anyone know what Sun failed to opensource in the crypto part of Java?
They also involve some elements of sound and cryptography, said Tom Marble, Sun's OpenJDK ambassador. "We have already contacted the copyright holders. We were unable to negotiate release under an open-source license," Marble said.
To sidestep the issue, Sun for now includes the proprietary software as prebuilt "binary" modules that programmers can attach to the versions of Java built from source code.

@_date: 2007-05-14 14:08:20
@_author: Ian G 
@_subject: no surprise - Sun fails to open source the crypto part of Java 
Third option:  the architecture of Sun's Java crypto framework is based on motives that should have been avoided, and have come back to bite (again).
The crypto framework in Java as designed by Sun was built on   motives (nefarious, warped or just plain stupid, I don't know) such as
* the need or desire to separate out encryption from authentication, and deliver two compatible but varying implementations in one variable body of code.  With a switch.  Somewhere.
* some notion that crypto code should be ("must be") a competitive market, one that is created by Sun, and is controlled by Sun.
* circular dependency where we have to install a signed provider which means we need signing which means we need crypto ...
* Being dependent on PKI style certificates for signing, so for example, if your machine doesn't have a properly configured domain name, touching the crypto caused DNS timeouts ... (1.5 from memory, might be fixed).
Hence, the framework is clumsy in practice, and trying to change it (in any way) was likely to run into roadblocks at the legal, policy and other areas like rights ...
As an aside, security is the baby that got thrown out with the bathwater.
The real interest was whether there was any difficulty in modifying the source code to add in the parts needed.  As Florian points out (thanks!), it is Sun's Provider that has not been delivered.
This is good, that is the part that is intended to be replaceable, so any of the Cryptix or Bouncy Castle or IAIK providers can be easy alternatives.
My worry was that they hadn't open sourced the architecture component, the part that wasn't meant to be replaceable. However even if open sourced, Sun may still wield a stick over the providers by insisting that they manage the signing process for the providers.
(This is in effect what open source organisations like Mozilla do with their source.  There is a tiny hook in there that stops people from changing the root list.)

@_date: 2007-05-15 11:37:56
@_author: Ian G 
@_subject: no surprise - Sun fails to open source the crypto part of Java 
But that's what they've got.  If the theory was that they needed to provide crypto without a hole, then they shouldn't have provided the crypto.  *The framework is the hole*, and pretending to stop other holes from being added is a fool's Which isn't to say that this is the end of the story.  The story was no doubt very complex.
Some topic drift (as Lynn would say):  I did a fair bit of investigation on the SSL v1 -> v2 transition and discovered 10 different forces working at the time.  As a historical comparison, we can suggest that Sun's Java group faced the same messy cauldron of forces at the time of the JCA being designed.  In that historical investigation I concluded that Netscape could not avoid the forces, and quite possibly ("no surprise") the Sun group cannot avoid the forces either.
OK, but can we agree that this is a motive outside normal engineering practices?  And it is definately nothing to do with security as understood at the language and application The point is that once we agree that this is an "outside" requirement, then we can see that as it starts to impact the security architecture, it can only worsen the security.
 From what the guys in Cryptix have told me, this is true. Getting the certificate is simply a bureaucratic hurdle, at the current time.  This part is good.  But, in the big picture:
J1.0:  no crypto
J1.1:  crypto with no barriers
J1.2:  JCA with no encryption, but replaceable
J1.4:  JCA with low encryption, stuck, but providers are easy
J1.5:  JCA, low encryption, signed providers, easy to get a key for your provider
J1.6:  ??
(The java version numbers are descriptive, not accurate.)
The really lucky part here is that (due to circumstances outside control) the entire language or implementation has gone open source.
No more games are possible ==>  outside requirements are neutered.  This may save crypto security in Java.
Sure.  There are two issues here, one backwards-looking and one forwards-looking.
1.  What is the way this should be done?  the Java story is a good case study of how the software engineering department put in place a heavyweight structure that drifted away from security.  We can learn from that.
2.  What is needed now?  Florian says the provider is missing and the "root list" is empty.  What to do?  Is it time to reinvigorate the open source Java crypto scene?

@_date: 2007-05-28 15:18:24
@_author: Ian G 
@_subject: A crazy thought? 
What you are suggesting is called Web of Trust (WoT). That's what the PGP world does, more or less, and I gather that the SPKI concept includes it, too.
However, x.509 does not support it.  There is no easy way to add multiple signatures to an x.509 certificate without running into support problems (that is, of course you can hack it in, but browsers won't understand it, and developers won't support you).
(Anecdote 1:  I pushed all of the Ricardo financial transaction stuff over to x.509 for a time in 1998, but when I discovered the lack of multiple sigs, and a few other things, I was forced to go back to PGP.  Unfortunately, finance is fundamentally web of trust, and hierarchical PKI concepts such as coded into x.509, etc, will not work in that environment.)
(Anecdote 2: over at CAcert they attempt to graft a web of trust on to the PKI, and they sort of succeed.  But the result is not truly WoT, it is a hybrid, in that there is still only one sig on the cert, and we are back to the scenario that you suggest.  Disclosure:  I have something to do with CAcert...)
So as a practical matter, that which is known as x.509 PKI cannot do this.  For this reason, some critics have relabeled the CAs as Centralised Vulnerability Parties (CVPs) instead of the more familiar Trusted Third Parties As a side note, outside the cryptography layer, there are legal, contractual, customary defences against the attacks that you outline.

@_date: 2007-11-02 18:23:30
@_author: Ian G 
@_subject: Hushmail in U.S. v. Tyler Stumbo 
It certainly was not a scam when I was involved (cryptix guys did some part of the original java crypto) many years ago.  The private key is encrypted by your passphrase, so the private key is not available to Hushmail.
The basic concept is of course somewhat limited by what it tries to do, but it is sound.  Hushmail published the applet that did all this, and it was possible to read the code and attack it.  At least one flaw was found, from deep dim memory.
There is for example a danger that hushmail could simply change the applet, and then acquire someone's key.  A victim would not notice so easily because there isn't much in the browser that stops the applet from changing code.  That's a threat, and they were aware of it, but it's also a bit of a high risk one, as, if it were spotted, their credibility would be shot.
In practice, the larger danger with email is that the high-profile threats to email security are on the client side.  Either you, your own machine, the other guy's machine, or the other guy.  I was involved in one case where super-secret stuff was shared through hushmail, and was also dual encrypted with non-hushmail-PGP for added security.  In the end, the lawyers came in and scarfed up the lot with subpoenas ... all the secrets were revealed to everyone they should never have been revealed to.  We don't have a crypto tool for embarrassing secrets to fade away.

@_date: 2007-11-07 16:05:07
@_author: Ian G 
@_subject: forward-secrecy for email? (Re: Hushmail in U.S. v. Tyler Stumbo) 
Hi Adam,
many people have suggested that.  On paper, it looks like a solution to the problem, at least to us.
I think however it is going to require quite significant support from the user tools to do this.  That is, the user application is going to have to manage the sense of lifetime over the message.
One tool that does approach this issue at least superficially is Skype.  It can be configured to save chat messages for different periods of time, I have mine set to around 2 weeks currently.
But, then we run slap-bang into the problem that the *other* client also keeps messages.  How long are they kept for? I'm not told, and of course even if I was told, we can all imagine the limitations of that.
I hypothesise that it might be possible to use contracts to address this issue, at least for a civil-not-criminal scope.   That is, client software could arrange a contractual exchange between Alice and Bob where they both agree to keep messages for X weeks, and if not, then commitments and penalties might apply.  Judges will look at contracts like that and might rule the evidence out of court, in a civil OK, so we need a lawyer to work that out, and I'm definately whiteboarding here, I'm not sure if the solution is worth the effort.
Which is why I am skeptical of schemes like "delete the private key periodically."  Unless we solve or address the counterparty problem, it just isn't worth the effort to be totally secure on our own node.
We know how to do invisible ink in cryptography.  How do we do its converse, fading ink?

@_date: 2007-10-08 22:55:33
@_author: Ian G 
@_subject: Full Disk Encryption solutions selected for US Government use 
A slightly off-topic question:  if we accept that current processes (FIPS-140, CC, etc) are inadequate indicators of quality for OSS products, is there something that can be done about it?  Is there a reasonable criteria / process that can be built that is more suitable?

@_date: 2007-10-22 21:55:39
@_author: Ian G 
@_subject: Commercial CAPTCHA-breakers for sale 
The other approach that I've heard of against such turing/human tests is that attackers have experimented with mechanical turk style systems.

@_date: 2007-09-13 14:14:13
@_author: Ian G 
@_subject: Another Snake Oil Candidate 
So, is snake oil:
    * a crap product?
    * a fine product with weaknesses?
    * a marketing campaign that goes OTT?
    * a term used to slander the opposing security model?
    * an adjective that applies to any of the above?
OTT == over-the-top, excessive and dangerous.  Derives from WW1 trench warfare.

@_date: 2007-09-23 13:32:51
@_author: Ian G 
@_subject: open source digital cash packages 
I can think of a few ways to answer this question.
1.  blinded money demo programs:  there is magic money, in C and in Java.  Also I think Ben Laurie wrote another one demo'd at EFCE.  These demos are generally around 1-4kloc.
2.  hard money systems:  These allow you to actually issue money and survive aggressive communities.  epointsystem is GPL I think, Ricardo is something or other but I haven't the energy to support the server side as an open source project.   Ricardo is 100-150kloc, epointsystem is much smaller (and lighter in features and scope).
3.  soft community money systems:  cyclos and similar (one from south africa, another from NZ from memory).  These products are designed for small communities where trust is implicit, they have no internal governance capabilities and only limited external security exposures.  But you can use them to issue money.
4.  then there are other variants like barcode money.  A lot of interest is being put into mobile phone money atm.

@_date: 2007-09-23 13:48:17
@_author: Ian G 
@_subject: Scare tactic? 
I agree that this is minutia, but there is a difference.  If the peer can arrange the key to be some predictable secret, it can do so without revealing itself.  Eve is happy.  If however it has to leak the key some other way, it needs some covert channel.  This channel is the sort of thing that security reviews might more easily stumble over.  E.g., IDS guy asking why these strange packets emanate from the crypto Which is to say, it's worth closing off this particular form of attack if it can be done without undue cost.  When I did a key exchange last in a protocol design, I attempted to address it by inserting some hashing steps.

@_date: 2008-04-24 10:26:15
@_author: Ian G 
@_subject: Cruising the stacks and finding stuff 
Wander over to  and poke at their models.  They have 6 or so to choose from, and they have it coded up in the webapplication so you can get the appropriate comparisons.
Each model is reasonably well-founded (some work was put in by somebody who knows something) and they've been doing it for a few years now.

@_date: 2008-02-01 16:26:04
@_author: Ian G 
@_subject: Gutmann Soundwave Therapy 
This is what Guus was getting at:
- We needed to tunnel data over UDP, with UDP semantics.
   SSL requires a reliable stream. Therefore, we had to
   use something other that SSL to tunnel data.
To put it in more fundamental terms, TLS assumes that what you want is a stream.  If you want packets, then TLS is a millstone around your neck.  It's not that it can't deliver packets, but that it forces all your application to think in stream-mode, which results in messes up and down the stack (including the human).
The vast majority of applications are not pure stream.  The vast majority are not pure packet, either ... so they are all somewhere in between.
The selection of where your app is on the spectrum and what tools you need is the job of the protocol architect; unfortunately, the prevailing wisdom is that as we only have a widely deployed stream protocol (TLS) then that should be used for everything.  This has resulted in some "easy wins" and some "intractable messes" as well the current thread (repeated into the past and will be repeated into the future).
Advising TLS for a packet delivery requirement is simply "wrong."  You might be "wise" to give that advice, if you can show some other factors, but that requires ... more subtlety than simply repeating that TLS has to be used for I'm interested.  FTR, zooko and I worked on part of the problem, documented briefly here: I've successfully got that going in 3 UDP transport scenarios, with different key exchange scenarios and languages.  (I was never able to deploy it tho, for business reasons.)  For the most part, the requirements include no relationship between packets, but an expectation of a return path  ... a.k.a. connections, but without the streaming assumption ... which means having to relearn how to do "context" over UDP.
One can compare that approach to the DTLS, which has the benefit of leveraging SSL technology and history.  My impression was that it assumed too much of the nature of SSL at the core, so it didn't cover enough of the territory to satisfy me.  But if it becomes widely deployed, that may be the better bet than designing another one or a home-brew. Deployment counts over elegance, most times.

@_date: 2008-02-01 21:34:09
@_author: Ian G 
@_subject: TLS-SRP & TLS-PSK support in browsers (Re: Dutch Transport Card 
I spent a few years trying to interest (at least) one browser vendor with looking at new security problems (phishing) and using the knowledge that we had to solve this (opportunistic cryptography).  No luck whatsoever.  My view of why it is impractical / impossible to interest the browser vendors in new ideas and new security might be summed as this:
* Browser vendors operate a closed security shop.  I think this is because of a combination of things.  Mostly, all security shops are closed, and there aren't any good examples of open security shops (at least that I can think of).  We see some outreach in the last few years (blogs or lists by some) but they are very ... protected, the moat is still there.
* Browser vendors are influenced heavily by companies, which have strong agendas.  Security programmers at the open browsers are often employed by big companies who want their security in.  They are not interested in user security. Security programmers need jobs, they don't do this stuff for fun.  So it is not as if you can blame them.
* Browser vendors don't employ security people as we know them on this mailgroup, they employ cryptoplumbers. Completely different layer.  These people are mostly good (and often very good) at fixing security bugs.  We thank them for that!  But they are completely at sea when it comes to systemic security failings or designing new systems.
* Which also means it is rather difficult to have a conversation with them.  For example, programmers don't know what governance is, so they don't know how to deal with PKI (which is governance with some certificate sugar), and they can't readily map a multi-party failure.  OTOH, they know what code is, so if you code it up you can have a conversation.  But if your conversation needs non-code elements ... glug glug...
* Browser vendors work to a limited subset of the old PKI book.  Unfortunately, the book itself isn't written, with consequent problems.  So certain myths (like "all CAs must be the same") have arisen which are out of sync with the original PKI thinking ... and out of sync with reality ... but there is no easy way to deal with this because of the previous points.
* Browser vendors may be on the hook for phishing.  When you start to talk in terms like that, legal considerations make people go gooey and vague.  Nobody in a browser vendor can have that conversation.
Which is all to say ... it's not the people!  It's the assumptions and history and finance and all other structural issues.  That won't change until they are ready to change, and there are only limited things that outsiders can do.
Just a personal opinion.

@_date: 2008-02-10 15:16:55
@_author: Ian G 
@_subject: TLS-SRP & TLS-PSK support in browsers (Re: Dutch Transport Card 
The situation with CAcert and Mozo is fairly simple.
Mozo ran a long and open design exercise for a CA policy, which specifies that each CA requires an audit [1].  CAcert hasn't got an audit [2].
Mozo did indeed work quite hard to give CAcert and others some more open access to the process.  One could debate the wisdom of having an audit at all, or ascribe the motives to politics, or whatever [3] ... in the end, Mozo moved a considerable distance by opening up the process to non-financial-audit firms and to criteria from non-consortium authors [4].
CAcert also now conducts an open process [5], so it is much easier to talk about the audit.  It is well advanced on the policy side, only lacking one or two critical policies which are works-in-progress.  Audits generally deliver reports that say things like "management has put in place procedures and policies..." so CAcert is in good shape here.
Where the audit has stalled is on the systems side (and the missing policies are all on that side as well).  CAcert will either solve their systems problems or die in the attempt. My current estimate is that if CAcert moves seriously to solve the systems problems, then it may have the audit by early 2009.  If not, not.
You can read more about it [6] or ask me or them or join their many mail lists, etc etc.
[1]  The process was led by Frank Hecker on the open mozo security maillist.  I was part of that process, as was Duane (founder of CAcert), because it was an open process.
IMO, the Mozo CA policy project was a great case study in open security, and should be copied by others, including other Mozo security processes.
[2] By way of disclosure, I am the auditor.  Minutes of most recent published audit report:
[3] FTR I argued against the requirements for audits.
[4] The case for audits was significantly weakened when rumours spread of audited CAs conducting MITMs on their own customers, and the logical claim that this was permitted under audit as long as it was disclosed, sort of, somewhere, maybe.  This was crucial in shifting consensus to allow competition in audit criteria and auditors.
[5] Due to direction from Greg Rose (retiring President) and a funding deal with NLnet that imposes frequent public reports.

@_date: 2008-02-10 15:27:53
@_author: Ian G 
@_subject: TLS-SRP & TLS-PSK support in browsers (Re: Dutch Transport Card 
I would say that this would not hold the FF developers back, as they were definately capable of implementing TLS/SNI extension a year or two back, without any support from stable libraries in Apache httpd, Microsoft IIS, etc (still I'd also suggest that the TLS/SNI (which will apparently turn up one day in Apache) will have a much more dramatic effect on phishing than TLS-PSK/SRP ... because of the economics of course.  Lowering the barriers on all TLS use is far more important than making existing TLS use easier.
Of course, this is not a competition, as the effect adds, not competes.  The good thing is that we may actually get to see the effects of both fixes to TLS rollout at similar times.  In economics, it is a truism that we can't run the experiment, we have to watch real life, Heisenberg style, and this may give us a chance to do that.
Also, we can observe another significant factor in the mix:   the rollout of virtual machine platforms (xen and the like) is dramatically changed the economics of IP these now becoming more the limiting factor than they were, which might also put more pressure on Apache ... to release earlier and more often.

@_date: 2008-01-26 22:27:41
@_author: Ian G 
@_subject: Lack of fraud reporting paths considered harmful. 
There is an interesting analogue in the area of SAR (suspicious activity report) filings through financial services.  This has been in place with various providers for maybe a decade or so.  I'm not aware of any serious economic analysis that would suggest copying the lessons, though.
There is a philosophical problem with suggesting an automated protocol method for reporting fraud, in that one might be better off ... fixing the underlying fraud.

@_date: 2008-01-29 20:52:16
@_author: Ian G 
@_subject: two-person login? 
OK, putting on the devil's advocate hat & cape here...
Consider the latest case with SocGen where a trader goes rogue (so the news has it at least).  One might argue that the system you are talking about provides a control over that.
There is the possibility of compliance controls.  In audits and sarbanes-oxley and other things there is frequent talk of dual control and 4 eyes principle.  Now, it could be that these points can be "easily" covered by employing a system that "enforces" this.  Often, auditors will be convinced if they can see something in place, and not feel the need to audit the system itself.  The auditor's job is done when he can safely say "management has put in place procedures..." and the system you mention meets that protocol in words at It might be useful for management to decree that all juniors must work with a senior watching over.  Also e.g.,  critical systems where two systems administrators work together.  In linux there is a program called screen(1) that allows two sysadms to share the same screen and type together.  This has a lot of value when "two minds are better than one." But, yes, this is not quite what you are describing.
Also, it might be a control to enforce other procedures.  If the sysadm is given the controls to some departmental system, then instead of just waltzing in and playing with it, he has to ask the non-techie boss, who then asks what the story is.  This way she can know that the appropriate procedures are in place, such as notification to users.
It's far easier to figure out what the sysadm is up to if he is forced to have a conversation every time he wants to log in...  this addresses your point b above, in that it now clearly labels any disaster as something the sysadm should have told the boss about before, instead of leaving it in the murky area of "of course I intended to scrub the disks, that's my job!"
I'd expect a proper physical token to be the manager's login mechanism.  If it was a password he typed in there would be too much incentive to share the password.

@_date: 2008-02-01 01:55:11
@_author: Ian G 
@_subject: Fixing SSL (was Re: Dutch Transport Card Broken) 
In a CA I have something to do with, I'm observing a site that just started experimenting with client certs (100 users, will reach 1000, maybe more).
When we discovered that the certificate includes PII (personally identifying information) and the website stores additional PII, the service was directed to drop all additional PII, and some thought was put into the in-cert PII.
Current view is that the service must engage the user in a contract to accept the storing of that in-cert PII, otherwise it must not store the info in the cert (which means no identity, no persistence, and no point to the client certs).
Writing contracts and securing agreement of course is a barrier, a burden.  If this were a general requirement, then this would be enough (imho) to not recommend client certs, because contracts need lawyers, they cost real money, they don't solve the problem, and not recommending them is likewise unacceptable.
(Then, as you say, there are convenience issues.)
This is an experiment to force client certs to be used, so they are plugging on.  It's a CA so it is trying to prove that there is value in these things.
So... there are two slight variations that could be employed.  Firstly, all data placed in the cert could be declared public in advance, and then no contract is required to use it in a context that is compatible with public data.   That is, the question of the contract is pushed to the CA/CPS.
(You mentioned that the premise is that it is all public Another variation is to switch to username + password, of course, in which case the username is freely given and expected to be stored (certs being more or less invisible to users, so we can presume no such).
(definately open to other ideas...)
The PII equation is particularly daunting, echoing Lynn's early '90s experiences.  I am told (but haven't really verified) that the certificate serial number is PII and therefore falls under the full weight of privacy law & regs ... this may sound ludicrous, but privacy and security are different fields with different logics.  If that is true, the liability is far too high for something that should be private, but is already public by dint of its exposure in certificates.  Privacy liabilities are sky-high in some places, and not only that, they are incalculable, unknowable, and vary with the person you are talking to.
So a superficial conclusion would be "don't use client certificates because of the privacy issues" although the issues are somewhat more complex than "PII revealed in SSL key exchange."
As I say, they'll plug on, as they need to prove that the cert is worth issuing.  It's a data point, no more, and it doesn't exactly answer your spec above.  But I'm having fun observing them trying to prove that client certs are worth any amount of effort.
PS: normal disclosures of interest + conflicts, included.

@_date: 2008-06-03 13:33:26
@_author: IanG 
@_subject: Can we copy trust? 
It is useful and efficient to get trust from third parties, but not essential, imho.  If you find yourself meeting someone for the first time in random circumstances, you can get to know them over time, and trust them, fully 2nd Trust comes from events of risk and reward, not from channels.  It just so happens that the best expressions of risk and reward are over independent therefore 3rd party Trust is an expression of something you may rely on.  It has risks, liabilities, obligations, etc.  Information does not Well.  Actions speak louder than words.  The *act* of a third party is to put their own reputation at risk if they say "trust this 2nd person."  This works if the two people are independent, but not if the two people are dependent (or the same).  If they are independent, the costs incur to one party and the benefits incur to another party.
So the independent cost of placing the reputation at risk is a significant event.  You can rely on someone who will incur cost on your behalf.  Saying "trust me" carries no risks because the benefits cancel out the risks.
Attempting to cast trust as a aspect of channels is a technological approach, and will lead one astray, just as PKI did;  trust is built on acts, of humans, and involves parties and events, risks and rewards.  The channels are You can see this better in the study of negotiation.  It is possible using this theory&practice to build trust, or to prove that no trust can be achieved.  Negotiation is primarily a paradigm of two parties.
(Economists will recognise it as game theory, prisoner's dilemma, perhaps agent-principal theory, etc.)
Your comment that someone who says "trust me" is in fact signalling that they cannot be trusted ... is more clearly explained in negotiation.  Often, someone will state up front that they want to find the win-win;  which is a signal that they are in the win-lose, because real win-win is about actions not words, and words in this case would lead to a false sense of security.

@_date: 2008-05-04 22:28:32
@_author: Ian G 
@_subject: User interface, security, and "simplicity" 
Kerckhoffs' 6th, providing great entertainment for the security world, since 1883.
6. Finally, it is necessary, given the circumstances that command its application, that the system be easy to use, requiring neither mental strain nor the knowledge of a long series of rules to observe.
PS:  Although his 6th is arguably the most important, his others are well worth considering:

@_date: 2008-05-06 23:08:04
@_author: Ian G 
@_subject: User interface, security, and "simplicity" 
I think we are all coming around to the view that any choices are practically messy and dangerous, no matter how nice they look on paper.
The way I put it, there is only one mode, and it is secure.   From there on, it only gets better.  Obligatory rant:

@_date: 2008-05-25 18:06:16
@_author: IanG 
@_subject: The perils of security tools 
Yes, but with different semantics:
      /dev/urandom is a compatibility nod
      to Linux. On Linux, /dev/urandom will
      produce lower quality output if the
      entropy pool drains, while
      /dev/random will prefer to block and
      wait for additional entropy to be
      collected.  With Yarrow, this choice
      and distinction is not necessary,
      and the two devices behave
      identically. You may use either.
(random(4) from Mac OSX.)
Depending on where you are in the security paranoia equation, the differences matter little or a lot.  If doing medium level security, it's fine to outsource the critical components to the OS, and accept any failings.  If doing paranoid-level stuff, then best to implement ones own mix and just stir in the OS level offering.  That way we reduce the surface area for lower-layer config attacks like the Debian adventure.

@_date: 2008-10-25 13:40:16
@_author: IanG 
@_subject: combining entropy 
I can see that my description was a bit weak, yes.  Here's a better
view, incorporating the feedback:
   If I have N people, each with a single pool of entropy,
   and I pool each of their contributions together with XOR,
   is that as good as it gets?
My assumptions are:
 * I trust no single person and their source of entropy.
 * I trust at least one person + pool.
 * Entropy by its definition is independent and is private
   (but it is worth stating these, as any leaks will kill us!)
 * Efficiency is not a concern, we just expand the pool size
   (each pool is size X, and the result is size X).
 * The people have ordinary skill.
now to respond to the questions:
1.  I am assuming that at least one pool is good entropy.  This is
partly an assumption of desperation or simplicity.
In practice, no individual (source or person) is trusted at an
isolated level.  But this leads to a sort of circular argument that
says, nobody is trusted.  We can solve this two ways:
    I join the circle.  I trust myself, *but* I don't trust
    my source of entropy.  So this is still hopeful.
    We ensure that there are at least two cartels in the
    circle that don't trust each other!  Then, add a dash
    of game theory, and the two cartel pools should at
    least be independent of each other, and therefore the
    result should be good entropy.
I suspect others could more logically arrive at a better assumption,
but for now, the assumption of one trusted person/pool seems to
cover it.
2.  Having thought about Stephan's comment a bit more (because it
arrived first), and a bit more about John D's entropy comments
(because they were precise), it is clear that I need to stress the
privacy / independence criteria, even if strictly covered by the
definition of entropy.  Too much of the practical aspects will
depend on ensuring independence of the pools to just lean blithely
on the definitions.  I had missed that dependency.
3.  The proposals on concatenation and cleanup are tempting.  In
Jon's words, it can solve obvious problems.  However, they introduce
a complexity of understanding the cleanup function, and potential
for failures.  Jack's tradeoffs.  This has made me realise the last
assumption, now added:
   The people have ordinary skill.
Which means they are unable to determine whether a cryptographically
complex cleanup function is indeed cleaning, or not.
Here, then, we reach an obvious limit, in that the people have to be
able to determine that the XOR is doing its job, and they need to be
able to do a bit of research to decide what is their best guess at
their private entropy source.
Thanks to all.

@_date: 2008-09-06 14:58:22
@_author: IanG 
@_subject: Quiet in the list... 
There are approximately these combos to achieve private email for the masses.
1.  GPG command line.  Will never achieve adoption.
2.  GPG + Engimail + Thunderbird.  Will never be totally robust because there is too much dependency.  Same as any similar combo.  So no widespread adoption.
3.  S/MIME.  Is so badly architectured that it can only be got going by neutering a lot of the assumptions, and that will take years of work.  Assuming everyone agrees, which they don't.  No widespread adoption possible.
4.  Skype.  Doesn't do email, but aside from that minor character flaw, it cracked everything else.  It's the best example of what it should look like.
5.  Browser solutions are cumbersome, but:  Hushmail versus gmail.  If google were to adopt hushmail, then this might work.  But, they won't.  They've passed the magic point where nose-thumbing at the state is part of the b-plan, and anyway, they *like* your data.  They are nothing without your data.
6.  Start from scratch.  Will take a long time, and only the larger projects have the resources to do this.  But, the larger projects are typically those that copy others architectures without thought.  See above.
All, strictly IMHO.  YMMV.  Private email for the masses isn't really in the foreseeable future.  The future is really moving towards newer architetures, email is old and tired.  Think mobile, chat and p2p directions.  But those guys are quite incremental in their improvements, so it will take a while.
(Yes, I know that's not the question you asked :)

@_date: 2008-09-06 16:06:33
@_author: IanG 
@_subject: Quiet in the list... 
No, interaction between different software packages has costs.  When you spend time to load up Thunderbird, then load up enigmail, then load up gpg ... this is more work than just loading up Tbird and sticking with it.
Then, when a new Thunderbird comes out, you load that up and the other packages cease to work.  What do you do?  Wait a few months until the others come back into line?  Or stop using encrypted email.  The masses do the latter, the geeks might do the former.
Most people download one thing and stick to it.  They follow the automated upgrades, or don't upgrade at all.  Most people have a life other than package management.  These are the masses.  For them, the softare has to work first time, every time, all the time.  And upgrade itself.
These are the target.  Aiming to do security for geeks alone is pointless, it just marks us out for special treatment. Using gpg is evidence of your guilt.  Using skype is normal, it's just the easiest way to chat and phone.

@_date: 2008-09-20 09:09:54
@_author: IanG 
@_subject: Lava lamp random number generator made useful? 
Does anyone know of a cheap USB random number source?
As a meandering comment, it would be extremely good for us if we had
cheap pocket random number sources of arguable quality [1].
I've often thought that if we had an open source hardware design of
a USB random number generator ... that cost a few pennies to add
onto any other USB toy ... then we could ask the manufacturers to
throw it in for laughs.  Something like a small mountable disk that
returns randoms on every block read, so the interface is trivial.
Then, when it comes time to generate those special keys, we could
simply plug it in, run it, clean up the output in software and use
it.  Hey presto, all those nasty software and theoretical
difficulties evaporate.
[1] the competitive process and a software clean-up would sort out
any quality issues.

@_date: 2008-09-29 14:13:39
@_author: IanG 
@_subject: combining entropy 
If I have N pools of entropy (all same size X) and I pool them
together with XOR, is that as good as it gets?
My assumptions are:
 * I trust no single source of Random Numbers.
 * I trust at least one source of all the sources.
 * no particular difficulty with lossy combination.

@_date: 2009-02-21 13:19:01
@_author: Ian G 
@_subject: The password-reset paradox 
The two numbers are not comparable.  One is the business cost to a company including all the internal, absorbed costs (see Steve's email), while the other is the pricelist of the supplier, without internal user-company costs.
If we compared each method using the other's methodology, passwords would "list" at $0 per reset, and tokens recoveries would "estimate" at $105 to $205, plus shipping.
It is a typical claim of the smart card & tokens industry that that the bulk unit cost of their product is an "important number".  This is possibly because the sellers of such product cannot offer the real project work because they are too product oriented and/or too small.  So they have to sell on somthing, and push "the number."  It is for this reason that IBM once ruled the world, they bypassed the whole listprice/commodity issue.
As a humourous aside, here's another deceptive sales approach available to the token world, the end of "something we know" security, as we know it :)

@_date: 2009-02-23 21:44:31
@_author: Ian G 
@_subject: SHA-3 Round 1: Buffer Overflows 
No controversy there.
It is certainly appreciated that work is put in to improve the implementations during the competition (my group did something similar for the Java parts of AES, so I know how much work it can be).
However I think it is not really efficient at this stage to insist on secure programming for submission implementations.  For the simple reason that there are 42 submissions, and 41 of those will be thrown away, more or less.  There isn't much point in making the 41 secure; better off to save the energy until "the one" is found.  Then concentrate the energy, no?

@_date: 2009-10-05 17:04:38
@_author: Ian G 
@_subject: Trusted timestamping 
My view is that there is no demand for this as a service.  The apparent need for it is more a paper requirement that came out of PKI world's search for a perfect product than any business need.
E.g., if you think you want it, you might be better rewarded by re-examining your assumptions as to why it is needed, than building it...

@_date: 2009-09-18 14:36:32
@_author: Ian G 
@_subject: Detecting attempts to decrypt with incorrect secret key in OWASP 
I have to add vote+1 on this selection.  For various reasons, today's safe choice seems to be:
   * CBC
   * AES-128
   * HMAC-SHA-1 on the outside of the ciphertext
What is left is padding so that the message is clearly deliminated.  I suggest you treat this as a software engineering thing, not a crypto thing, and make sure that you have a length in your packet layout so that it is totally clear what is the packet and what is not.
If you want to see such a design exercise, following Dave's prescription, have a look at SDP1 which Zooko and I did a few years back.
It's a straight forward secret-key encrypted packet layout.  It has one novelty in it, which is how it solves the padding / IV issues.  Other than that it should be boring.
PS: you are on the right track in trying to avoid any sensitivity to JCE.  As long as you can design your layout without any dependency on JCE it should work.  JCE is basically a slock design that was put in place for market- and crypto-control reasons, it has no place in software engineering.  I speak from experience, I managed the Cryptix project, which was the first Java crypto engine.
PPS: you haven't said enough about the application (or I missed it) to be able to comment on keys.  Generally, try to separate the protocol around the key:  every good protocol divides into two parts, the first of which says to the second, "trust this key completely".  Software engineering ...

@_date: 2009-09-24 01:57:56
@_author: Ian G 
@_subject: FileVault on other than home directories on MacOS? 
An extremely minor point, that looks like Jacob and Ralf-Philipp perhaps "aka nsa.org", rather than the NSA.gov.
Still useful.

@_date: 2010-08-02 21:32:12
@_author: Ian G 
@_subject: Five Theses on Security Protocols 
Although distasteful, this is more or less essential.  The problem is best seen like this:  take all the potential relying parties for a large site / large CA, and multiply that by the damages in (hypothetically) fat-ass class action suit.  Think phishing, or an MD5 crunch, or a random debian code downsizing.
What results is a Very Large Number (tm).
By fairly standard business processes one ends up at the sad but inevitable principle:
    the CA sets expected liabilities to zero
And must do so.  Note that there is a difference between "expected liabilities" and "liabilities stated in some document".  I use the term "expected" in the finance sense (c.f. Net Present Value calculations).
In practice, this is what could be called best practices, to the extent that I've seen it.
 says the same thing in many many pages, and shows how CAcert does it.
I've never heard of anyone collecting, but I wish I had (heard).
In theory, yes.  This is "expected".  In some sense, the certificate's name might be interpreted as suggesting that because the name is validated, then you can sue that person.
However, I'd stress that's a theory.  See above paper for my trashing of that, "What's in a Name?" at an individual level.  I'd speculate that the problem will be some class action suit because of the enourmous costs involved.
If the cause of loss is listed in the documentation . . .
We are facing Dan Geer's disambiguation problem:
 > The design goal for any security system is that the
 > number of failures is small but non-zero, i.e., N>0.
 > If the number of failures is zero, there is no way
 > to disambiguate good luck from spending too much.
 > Calibration requires differing outcomes.
Maybe money can buy luck ;)

@_date: 2010-08-27 00:17:44
@_author: Ian G 
@_subject: towards https everywhere and strict transport security (was: 
Yes, it is inherent in the design assumptions of the early 1990s.  At the time, the idea was to secure HTTP, which was (is) a request-response protocol layered over TCP.  Now, some of the design features that the designers settled on were:
     + ignore HTTP and secure TCP
     + make SSL look just like TCP
     + third-party authority authentication
     + no client-side caching of certs
And those features they delivered reasonably well.
However, if they had dug a bit deeper at the time (unlikely, really unlikely) they would have discovered that the core HTTP protocol is request-response, which means it is two packets, one for request and one for response.
Layering HTTP over TCP was a simplification, because just about everyone does that, and still does it for whatever reason.  However it was a simplification that ultimately caused a lot more cost than they realised, because it led to further layering, and further unreliability.
The original assumptions can be challenged.  If one goes to pure request-respose, then the whole lot can be done over datagrams (UDP). Once that is done properly, the protocol can move to 4 packets startup, then cached 2 packets mode.  The improvement in reliability is a gift.
This is possible, but you have to think outside the box, discard the obsession of layering and the mindtrap of reliable TCP.  I've done it, so I know it's possible.  Fast, and reliable, too.  Lynn as well, it seems.  James points out the architectural secret, that security has to be baked into the app, any security below the app is unreliable.
SPDY only takes the low-hanging fruit, IIRC.  Very cautious, very conservative, hardly seems worth the effort to me.
If you're content with slow, stick with TLS :)  Fast starts with a clean sheet of paper.  It is of course a complete rewrite, but IMHO the work effort is less than working with layered mistakes of the past.

@_date: 2010-07-29 16:18:25
@_author: Ian G 
@_subject: Fwd: Introduction, plus: Open Transactions -- digital cash library 
Hi Bob,
Sorry to get your hopes up ... Just reading the words below not the code:  it is basically modelled on the SOX/Ricardo concepts, AFAICS.
As you know, the SOX concept used (PGP) keys to make an account with the server/issuer Ivan, or a long term persistent relationship, call them Alice and Bob.  DigiCash also had something like this too, it's essential for application robustness.
The simplest payments metaphor then is a signed instruction to transfer from Alice to Bob, which Ivan follows by issuing a signed receipt.  What you'd call double entry, but in Ricardo is distinct enough to deserve the monika triple-entry (not triple-signed, that is something different, another possible innovation).
Then, the blinding formula/transaction is simply a replacement for the standard payments tranaction above:  Alice withdraws a coin from Ivan, sends it to Bob, who deposits it with Ivan.
(Ricardo had Wagner too from around 2001, and like this author, had a path to add Chaum, with future extension to Brands.  The code for Chaum was mostly written, but wasn't factored correctly...)
Another possible clue:  the author has obviously taken on board the lessons of the Ricardian Contract form, and put that in there (albeit in XML).  I find that very encouraging, even the guys from DigiCash never understood that one!  So I'm guessing that they have studied their stuff.
BTW, FTR, I do not know who this is.
Lucre was 1-2k lines.  Ones heart beats blood into thin air until there is another 1-2 orders of body parts built on...  This is looking much more like that 1-2 orders of magnitude down the track.

@_date: 2010-09-14 10:24:08
@_author: Ian G 
@_subject: Hashing algorithm needed 
I don't recall the full discussion, but what you described is generally handled by public key cryptography, and it is built into HTTPS.
Here's my suggestion:
1.  In your initial account creation / login, trigger a creation of a client certificate in the browser.
1.b.  record the client cert as the authenticator in the database.
2.  when someone connects, the application examines the cert used, and confirms the account indicated.  If an unknown cert, transfer to a landing page.
2.b  note that there is no login per se, each request can as easily check the client cert listed by Apache.
3.  you just need some way to roll-over keys from time to time.  Left for later.
3.b  There are some other bugs, but if the approximate scheme works...

@_date: 2010-09-14 21:29:36
@_author: Ian G 
@_subject: Hashing algorithm needed 
Just to be frank here, I'm also not sure what the implementation details are here.  I somewhat avoided implementation until it becomes useful.
Marsh's notes +1 from me.
