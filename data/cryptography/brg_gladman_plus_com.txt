
@_date: 2004-10-11 09:16:54
@_author: Brian Gladman 
@_subject: AES Modes 
I provide some code and some speed comparison data for some of the AES modes here:
   I focus mainly on the combined encryption/authentication modes but I only cover those that I believe are free of licensing costs.
     Brian Gladman

@_date: 2004-10-12 18:17:41
@_author: Brian Gladman 
@_subject: AES Modes 
Maybe my C implementation of SHA1 is hopeless but I get SHA1 on an x86 at about 17 cycles per byte (over 100,000 bytes) and AES in C at 21 cycles per byte.
So I would put these two algorihms at about the same speed in C. In consequence I rather suspect that the 'two encryptions per block' cost might also apply to combined modes when AES is used with HMAC-SHA1.
Rich Schroeppel's CS mode has been added to the NIST modes list earlier this year and is not patented. It seems to have a cost that is close to 'one encryption per block' but it has the 'interesting' property of using the internal 'mid-point' state of the cipher algorithm that is in use.
    Brian Gladman

@_date: 2004-10-14 07:32:03
@_author: Brian Gladman 
@_subject: AES Modes 
The SHA1 figure is for a P3 using VC++ set to generate code that will run on all Pentium family machines.  I have not optimised the C code for any particular machine. 17/12 for C/ASM is a bit worse than I would have hoped for but is not that bad.
I would not be surprised to see an average AES/SHA1 speed comparison in the 1.5:2.5 range but I was a bit surprised to see Jack's 2.0:5.0 range.
I will have to see if VC++ can be coaxed down from 17 cycles per byte for SHA1 without giving up on code that runs on all Pentium compatible machines :-)
    Brian Gladman

@_date: 2004-09-14 09:35:59
@_author: Brian Gladman 
@_subject: Looking for Source of AES code 
I don't know whose code it is but it has bugs in it.
The line above should be:
  *(state+3)= *(in+12)<<24 | *(in+13)<< 16  | *(in+14)<<8 | *(in+15);
I doubt that this is the only problem in this code either.
     Brian Gladman

@_date: 2005-06-17 21:03:05
@_author: Brian Gladman 
@_subject: AES cache timing attack 
This is a very nice piece of work by Bernstein but I am not convinced
about the practical significance of the attack.
And I certainly don't see any reason to abandon some of the design
approaches (e.g table lookup) as he has been suggesting just because
they are exploitable in some situations. In many situations they are not
exploitable at all and in those situations where they might cause
problems it is up to system designers to decide whether or not they need
to deploy countermeasures.
Nevertheless Bernstein has shown up one issue that I had not been
conscious of and this is that on modern (Intel) x86 systems there is no
longer a significant speed penalty for unaligned memory accesses to
32-bit words, a feature that allows AES to be implemented with very much
less table space than is normally the case.
There is almost no speed penalty in terms of best speed and the typical
speed is likely to be a lot better in most practical situations because
the load on the cache is greatly reduced.  And the timing variability of
this code is greatly reduced so its an all round win on the x86.
The downside is that, although unaligned accesses on x86 are ok, on many
other architectures these cause exceptions and this makes it tedious to
build compressed table operation into portable C code. In fact it is so
tedious that I am not going to offer this and have instead simply
published x86 assembler code which I report on here:
   For those who can live with x86 only, and with an assembler
implementation, this code matches the maximum speed of my large table
assembler version on the P3 and P4.
Another issue that this raises is that of the crypto API since in those
situations where the timing attack matters it is necessary to control
the position of the expanded AES key on the stack and this requires that
key expansion and encryption is done as one integrated API call, aka:
   encrypt(key[], in[], out[], no_of_blocks)
I hope this helps but if not I will try and answer any other questions.
   Brian Gladman

@_date: 2006-12-04 13:32:22
@_author: Brian Gladman 
@_subject: [-SPAM-] Re: Can you keep a secret? This encrypted drive can... 
AES-256 does not encrypt any more data per round than AES-128.
My guess is that you are thinking about Rijndael with a 256 bit block
and a 256 bit key.
  Brian Gladman

@_date: 2006-12-05 23:22:13
@_author: Brian Gladman 
@_subject: [-SPAM-] Re: Can you keep a secret? This encrypted drive can... 
For AES the round function and key scheduling cost per round are
basically the same for both AES-128 and AES-256.  In consequence I would
expect the speed ratio to be close to the ratio of the number of rounds,
which is 14 / 10 or 40%.
My own figures on AMD64 are 1.35 for encryption and 1.39 for decryption.
And on a P4 they are 1.36 and 1.38 respectively. These are hence close
to the expected 40% figure.
This suggests to me that a figure around 20% would apply in applications
in which about half the time is spent in encryption and half in other
higher level activities.
Can I hence assume that your benchmark is being run at application level
rather than algorithm level?  If not why is the ratio only 22% on the
   Brian Gladman

@_date: 2006-11-21 00:26:01
@_author: Brian Gladman 
@_subject: EAX and CCM code 
You might like to look at my mode implementations here:
  where there are some test vectors.
I have more test vectors now, including coverage of some hard to find
error cases.
I will be updating this code within the next few days to include these
new vectors and some improvements in the modes implementations.
    Brian Gladman

@_date: 2006-10-10 12:56:07
@_author: Brian Gladman 
@_subject: TPM & disk crypto 
I haven't been keeping up to date with this trusted computing stuff over
the last two years but when I was last involved it was accepted that it
was vital that the owner of a machine (not necessarily the user) should
be able to do the sort of things you suggest and also be able to exert
ultimate control over how a computing system presents itself to the
outside world.
Only in this way can we undermine the treacherous computing model of
"trusted machines with untrusted owners" and replace it with a model in
which "trust in this machine requires trust in its owner" on which real
information security ultimately depends (I might add that even this
model has serious potential problems when most machine owners do not
understand security).
Does anyone know the current state of affairs on this issue within the
Trusted Computing Group (and the marketed products of its members)?
   Brian Gladman

@_date: 2007-01-16 17:10:09
@_author: Brian Gladman 
@_subject: It's a Presidential Mandate, Feds use it. How come you are  not 
The situation here in the UK is that Parliament has passed a law (RIPA)
that allows the UK government to introduce key disclosure powers if it
wishes to do so.
So far these powers have not been bought into operation but the UK
government initiated a consultation last year on whether it should take
this step.  We are still awaiting a decision on this.
   Brian Gladman

@_date: 2008-08-25 10:52:22
@_author: Brian Gladman 
@_subject: 5x speedup for AES using SSE5? 
The best figure I obtain on an AMD64 system is 11 cycles/byte, which
matches your results (you had me worried for a while with 9 cycles/byte!)
To go 5 times faster than this would mean close to 2 cycles/byte, a
speed that I find hard to believe without hardware acceleration
But a fully byte oriented implementation runs at about 140 cycles/byte
and here the S-Box substitution step is a significant bottleneck.  I too
think the PPERM instruction could be used for this and it seems possible
that this would produce large savings.  So 30 cycles/byte might well be
achievable in this case.
I hence wonder whether this is the comparison that AMD are making.
It is also possible that the PPERM instruction could be used to speed up
the Galois field calculations to produce the S-Box mathematically rather
than by table lookup. I have tried this in the past but it has not
proved competitive.  But PPERM looks interesting here as well.
   Brian Gladman

@_date: 2009-02-02 22:07:51
@_author: Brian Gladman 
@_subject: full-disk subversion standards released 
; ; Sent: Monday, February 02, 2009 3:53 AM
I well understand the difficulties of mounting attacks but the fact remains that if someone else is able to take over _control_ of your machine you won't obtain any security irrespective of whether your interest is in network or storage encryption.
And _if_ Intel were to be interested in being able to take over your machine whenever it wished to do so -- which I don't believe it is -- subverting its processor designs to make this possible will be many, many orders of magnitude more effective than subverting the design of a TPM that 99.999...% of machines won't have.
I am personally happy to trust Intel and I am also happy to trust the design of the TPM I happen to use.  And it is completey useless for DRM provided only that Intel and the TPM supplier have not been subverted.
I simply don't believe that TPM's will ever achieve (or could ever have achieved) the widespread adoption that effective DRM demands and I don't personally believe that such applications ever played much part in the design.   But _provided_ the hardware suppplier can be trusted, hardware based security is able to achieve a much higher level of assurance than pure software ever can.    TPMs are hence useful in custom security applications and I am personally much more confident in my security using my TPM based solution than if I would be if I were relying on a pure software approach.
I am _not_ advocating TPM technology since I doubt its general utility for widespread adoption but I reject the idea that TPMs are part of an evil plot to infect the world with DRM.
    Brian Gladman

@_date: 2013-12-28 18:17:51
@_author: Brian Gladman 
@_subject: [Cryptography] What do we know? (Was 'We cannot trust' ...) 
In my experience they do care in the information security business
since, as Donald has said, even if the R&D costs can be sustained, the
ongoing support costs soon kill any prospect of ongoing deployment.
In the 1970s and 80s Honeywell further developed the Multics Operating
System and obtained B2 certification for defence use. The resulting
system was deployed and was considered a success but once the commercial
market had moved on, the ongoing cost of maintenance and support for the
defence variant became prohibitive.
Following on from this, Honeywell was funded by Dod to produce a new
secure computer system (called SCOMP) and this again received
certification.  But it was never deployed (AFAIK) because it was
realised that the costs of ongoing support would fall entirely on DoD.
In the UK we also developed secure versions of commercial OS but they
were never deployed for exactly the same reason.  We considered the use
of a version of Mach (T-Mach) as well as the possible development of a
version of Windows NT for which I remember obtaining a large but
actually very reasonable cost estimate from Microsoft for the additional
work that would be needed to obtain UK defence certification.
But we soon realised that, even though we could afford to fund the
development of either of these, we would simply be repeating the same
cycle again -- we would just find ourselves several years behind the
commercial market with an obsolesent product whose support costs would
be astronomic and which would fall entirely on the defence budget.  So
we ditched the idea before it saw the light of day and switched to the
funding of security enhancements in commercial systems without creating
defence variants.
   Brian Gladman

@_date: 2013-10-02 22:13:47
@_author: Brian Gladman 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
As someone who was heavily involved in writing the AES specification as
eventually used by NIST, I can confirm what John is saying.
The NIST specification only eliminated Rijndael options - none of the
Rijndael options included in AES were changed in any way by NIST.
   Brian Gladman

@_date: 2013-10-03 15:09:32
@_author: Brian Gladman 
@_subject: [Cryptography] AES-256- More NIST-y? paranoia 
I may be wrong about this, but if you are talking about the theoretical
strength of AES-256, then I am not aware of any attacks against it that
come even remotely close to reducing its effective key length to 128
bits.  So my answer would be 'no'.
But, having said that, I consider the use of AES-256 in place of AES-128
to be driven more by marketing hype than by reality.  The theoreticaal
strength of modern cryptographic algorithms is the least of our worries
in producing practical secure systems.
   Brian Gladman

@_date: 2013-09-07 09:33:28
@_author: Brian Gladman 
@_subject: [Cryptography] Bruce Schneier has gotten seriously spooked 
Because NSA and GCHQ are much more interested in attacking communictions
in transit rather than attacking endpoints.
Endpoint attacks cost more to undertake, only give access to a limited
amount of data and involve much greater risks that their attack will
either be discovered or their means of attack will leave evidence of
what they have done and how they have done it.  The internal bueaucratic
costs of gaining approval for (adverarial) endpoint attacks also makes
it a more costly process than the use of network based interception.
There is significant use of open source encryption software in end to
end encryption solutions, in file archivers, in wifi and network
routers, and in protecing the communications used to manage and control
such components when at remote locations.  The open source software is
provided in source code form and is compiled from source in a huge
number of applications and this means that the ability to covertly
substitute broken source code could provide access to a huge amount of
traffic without the risks involved in endpoint attacks.
I stress that I am NOT suggesting that this has happened (or is
happening), simply that it has attractions from an NSA/GCHQ viewpoint.
Fortunately, I think it is a difficult attack to mount covertly (that
is, without the acqiecience of the author(s) of the software in question).
On the more general debate here, in my view, 'security for the masses'
through the deployment of encryption is a 'pipe dream' that isn't going
to happen.  Functionality (and the complexity that comes with it) is the
enemy of security and it is very clear that the public places a much
higher value on functionality than it does on security (or privacy).
Every time a new device comes onto the market, it starts with limited
functionality and some hope of decent security but rapidly evolves to be
a high functionality product in which the prospect of decent security
declines rapidly to zero.  Raspberry Pis look interesting _now_ but I
would be willing to bet that they won't buck the trend of increasing
funtionality and declining security simply because this is what the
majority in even this limited user community will want.
To buck this trend we need an effort like the Raspberry Pi effort but
one driven by our community with a strong commitment to simplicty and
deliberately limited functionality in both hardware and software.
   Brian Gladman

@_date: 2013-09-08 00:32:50
@_author: Brian Gladman 
@_subject: [Cryptography] Bruce Schneier has gotten seriously spooked 
I don't have experience of how the FBI operates so my comments were
directed specifcally at NSA/GCHQ interests.  I am doubtful that very
large organisations change their direction of travel very quickly so I
see the huge investments being made in data centres, in the tapping of
key commmunications cables and core network routers and 'above our
heads', as evidence that this approach still works well for NSA and
GCHQ.  And I certainly don't think that volume is a problem yet since
they have been able to invest heavily to develop the techniques that
they use to see through lightweight protection and to pull out 'needles
from haystacks'.
Of course, you might well be right about the future direction they will
have to travel because increasing volume in combination with better end
to end protection must be a nightmare scenario for them.  But I don't
see this move happening all that soon because a surprisingly large
amount of the data in which they have an interest crosses our networks
with very little protection.  And it seems even that which is protected
has been kept open to their eyes by one means or another.
  Brian

@_date: 2014-07-11 18:34:03
@_author: Brian Gladman 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
It's a nice story, Jerry, but I very much doubt that it was a true
reflection of the situation (I assume this is a US anecdote).
Prior to retiring from the UK Ministry of Defence (in the mid 1990s), I
was the Chief Scientist for the Ordnance Board, the UK body that manages
the safety of weapons systems deployed by the UK Armed Forces (it has a
history going back to the 1400s).
We took the safety and integrity of computing and software in weapons
fusing, arming and release very, very seriously and my contacts with my
US counterparts suggest to me that they were no less dligent in such
In fact, one of the biggest issues we faced was that of ensuring that
advances in the formal analysis of security critical systems would be
available for use in the analysis of safety critical systems in a
situation where some of the 'players' would have much preferred to keep
these techniques under wraps.
   Brian

@_date: 2014-06-29 10:29:40
@_author: Brian Gladman 
@_subject: [Cryptography] Good GPG email solution for Windows (NOT 
I haven't had a problem with Thunderbird but I have also tried Gpg4win
as well and it worked fine for me on Windows 7 and 8:
But it has its own email client (Claws mail) which may not be to your
    Brian

@_date: 2015-07-05 09:11:33
@_author: Brian Gladman 
@_subject: [Cryptography] Best AES candidate brokenby the way that 
But it is important to distinguish between algorithm failures and
implementation failures.
The fact that _some_ AES (or Rijndael) _implementations_ can be broken
in _some_ usage scenarios does not mean that the algorithm itself is
All cryptographic algorithms are susceptible to failures that might be
introduced by the way that they are implemented (although it is true
that algorithm design can have a significant influence on the nature and
impact of implementation weaknesses).

@_date: 2015-07-05 22:45:00
@_author: Brian Gladman 
@_subject: [Cryptography] Best AES candidate broken 
In my view the practical impact of cache timing attacks is less than
might be expected given the publicity that they have received.  Moving
such attacks from the laboratory to the real world is quite difficult
and even in situations where such attacks are feasible, simple measures
are generally available to prevent timing information being gained
without having to forego the speed of table driven implementations.
And, before someone mentions it, I am aware that such attacks have been
demonstrated outside the laboratory. But I am also aware of just how
many systems involving cryptographic components have been deployed
without any proper threat analysis and/or systems security reviews.

@_date: 2015-11-03 13:10:03
@_author: Brian Gladman 
@_subject: [Cryptography] How programming language design can help us 
I have been lurking on this thread for some time but have been
reluctant to join in as I know from previous experience that any debate
on the extent to which programming languages can impact on the integrity
of the systems will very often raise a great deal more heat than light.
In my work in defence in the period from 1961 through to 1998, I was
lucky enough to be at the centre of systems engineering research and
development in two fields, both of which should have needed high
integrity systems engineering approaches.
The first of these was safety critical systems in areas such as aircraft
and missile flight control, and in weapon systems fusing, arming and
release.  The second was in secure information systems.
What struck me during this time was that while we saw a strong degree of
commonality in what these two domains needed in terms of systems
engineering, in reality the approaches adopted within the two domains
could hardly have been more different.
In the safety critical systems domain there was an enormous focus on
systems engineering at every stage in development and a great deal of
rigour was involved in developing requirements and in translating these
into forms from which designs could be developed and then measured for
conformance.  There was an equally rigorous approach to the
documentation of the design itself, and to the development and
management of the interfaces and the design specifications for the
component parts and in the development, documentation and operation of
the systems tests and systems monitoring both during development and in
eventual operation.
And then within the Ministry of Defence there were very senior managment
Boards that had the responsibility for giving the final approval for the
operational deployment of all aircraft and weapons systems (the Ordnance
Board has a history going back to the Tudors
In the security critical systems domain, however, there was none of this
rigour at a systems engineering level with the result that systems were
'put together' rather than being designed. And there was no senior
management Board that had resonsibility for approving the operational
deployment of security critical systems.  Here each bit of the defence
estate did its own thing.
When it came to their attitude to research and development (R&D), the
reaction of the companies and organisations in these two domains was as
different as their approach to systems engineering.
The Royal Radar Establishment (where I worked) had some success in the
70s with a programming language callled Coral
( but rather than continuing with
this in the 1980s we decided to participate in the US DoD effort to
develop Ada.  And here once more the reaction of the UK companies and
organisations involved in safety and security could hardly have been
more different. Those involved in safety volunteered to get invoved in
the language design and development process with very little
encouragement from us. But those involved in information security showed
an almost complete disinterest despite a considerable effort on our part
to get them involved.
The reaction was also the same in another initiative we took in
developing a processor architecture
( for use in high
integrity systems.  Once more we got a lot of support from the companies
and organisations involved in safety critical systems but at best
bemused disterest from those involved in secure systems (GCHQ and NSA
were notable exceptions here). [1]
I am sorry for all this background, but I think it may help in making my
main point - that there is a big paradox in the reactions of the safety
and security critical communities to the role of programming language
choice in building high integrity systems.
Although within the safety critical systems community there has been a
significant adoption of Ada and SPARK -- see
 -- the
paradox here is that, when set within the context of an overall systems
engineering approach, the language choice is really not critical.
Although it would certainly have taken longer and cost more, these
systems could have been programmed in hand coded binary and still
achieved the high integrity required of them.
But those involved in building security critical systems in the 1980s
and 90s were mostly involved in putting systems togther from the bottom
up using any C compiler that came with the hardware (that they had often
chosen before knowing what the system had to do). And, as I know from my
efforts at the time, any suggestion that they needed a more systematic
approach to system design - or that C might not be the most appropriate
language choice - was very often met with derision.
The point is that within the safety critical domain the focus on systems
engineering and the understanding of the role of language choice within
this process are mutually reinforcing in the drive towards higher
In contast, although we talk about information systems engineering, my
feeling is that most information systems are still 'put togther' rather
than being engineered. And here the choice of C as the programming
language with its focus on being near the 'metal' acts to encourage a
bottom-up approach to systems construction and acts to weaken any
pressures that might encourage the evolution of a more systematic
approach to systems design.
Here the relative weakness in systems engineering and the almost
universal acceptance of C as the language to use (or the only language
on offer) are mutually reinforcing in a downward direction when it comes
to any drive towards a more systematic apppproach to higher integrity.
As other have said the combination of C with a belief that building
secure information systems is all about writing code is a toxic combination.
My conclusion is that while it is systems engineering that really makes
the difference in achieving high integrity in computer based systems,
the choice of language can have a significant influence on the rate at
which a community evolves an understanding of the need for an organised
approach to the engineering of systems.
And here, while other factors have had a larger impact, the almost
universal adoption of C as the language to be used in building
information systems has in my view played a small but nevertheless
significant part in delaying the evolution of a systems engineering
approach in their overall design.
   Brian Gladman
[1] VIPER ulimately failed in a botched MoD attempt to commercialise it.

@_date: 2015-11-04 09:00:55
@_author: Brian Gladman 
@_subject: [Cryptography] Why Rijndael ? 
I think this is the content of your original mail:
If this is not asking about NIST's rationale, I no longer understand the
language you appear to be using :-)
   Brian

@_date: 2015-11-06 09:20:40
@_author: Brian Gladman 
@_subject: [Cryptography] Systems Engineering for Safety and Security 
At risk of annoying Tamzen, I would like to comment on your observations
(but I have changed the subject accordingly).
At a microscopic level I agree with you that there is enormous
fragmentation in what the information systems community means by
security.  But I dispute that this is true at a macroscopic level where
it is only too obvious to everyone that pretty well all deployed
information systems exhibit routine ongoing security failures that
create serious risks for end users.
For me, the contrast between the situation when seen at microscopic and
macroscopic levels is just another symptom that of the fact that this
community has failed to evolve a systems engineering approach to what it
does (the safety community had evolved a view of safety long before
computers came along so it is not surprising that it is so far ahead in
systems engineering terms).
At a microscopic level it is of course true that secure systems can be
written any language including C. But when we consider this at a
macroscopic level it is clear that the vast majority of information
systems (at least until now) have been written in C and the vast
majority of them have been insecure. Of course, this correlation doesn't
prove causality but given that we continue to use 1960s technology to
build 21st century systems I am not in the least surprised by this.
Where security really matters - in NSA and GCHQ - they are looking at
alternative approaches pioneered by the safety community (see, for
example, But because I didn't want to write a thesis, I only focussed my comments
on whether or not a programming language has an impact on achieving
security.  But I was careful to say that this was not by any means the
largest factor, other more important distinctions between the safety and
security communities being:
1. Engineering for safety predates computers by hundreds of years - they
have had much longer to develop a concensus;
2. If an industry is to survive, it has to supply what its customers
want (or think they want) and information systems customers consistently
prefer functionality over security (will this change?);
3. In safety both government and industry want improvements in safety;
in security the governments that matter are ambivalent - they want
security for themselves and insecurity for others.
4. The enormous rate of growth of the information systems industry and
the fact that much early computer science teaching was really only about
programming left many companies with few if any staff with an
understanding of the need for a systems engineering approach.
5. And no doubt many more!
    Brian

@_date: 2015-10-02 08:49:42
@_author: Brian Gladman 
@_subject: [Cryptography] Paper check security 
Here in the UK some financial organisations still require a cheque for
some types of transaction (e.g. high value ones on 'free' bank accounts
where the account holder wishes to avoid a transaction fee).  Some
financial organisations also seem to use the ability to write a cheque
as a form of authentication/identification.
   Brian

@_date: 2016-04-07 19:42:51
@_author: Brian Gladman 
@_subject: [Cryptography] At what point should people not use TLS? 
I also looked for it as I wanted to find out well it was implemented.
But like yourself, I could not find anything.
   Brian

@_date: 2016-04-08 08:34:21
@_author: Brian Gladman 
@_subject: [Cryptography] At what point should people not use TLS? 
Thanks for the reference, which is a more detailed exposition of the
implementation than the one I found on the Whatsapp site.  But, as you
say, this is a textual description of the structure of the code, not the
code itself.
What struck me about the protocol was that it looked quite neat on a
cursory insspection but it was rather more complex than I expected it to
be in how it did key management.  I hence felt that its actual security
would depend critically on how well it was implemented and on how
effective the implementation would be at defending against endpoint
attacks on key management in particular.  And since I am guessing that
largely common code is used in a quite a few different target
environments, I was interested in how the code was being protected by
the code security features of the different target architectures.
With millions of users, it seems to me that independent code review of
open source code would be a powerful confidence builder and one that I
would have thought that Whatsapp would appreciate and encourage.
But beyond this, the problem for phone security is exactly that we have
seen for computers in that in that functionality and security are
features that are very hard (I would say impossible) to achieve at the
same time and the 'market' always drives functionality at the expense of
Blackberry, for example, was moving towards QNX for its phones, which in
security terms was a very positive development, but the evolution of
'app stores' undermined their market position even though they had a
better chance than other phone suppliers of providing a platform with
effective security.
So, while I see what Whatasapp is doing as a worthwhile step forward,
end to end security will inevitably place the focus on endpoint security
and until we have a way of gaining confidence in the code being run and
the environments in which it is running, 'security for the masses' will
remain an elusive dream.
   Brian

@_date: 2016-07-13 16:03:26
@_author: Brian Gladman 
@_subject: [Cryptography] The Laws (was the principles) of secure 
I don't think it qualifies as a law but I have always liked the phrase
"two's company, three's a crowd" given how may countries seem to want
three people in all conversations when two will do very nicely for those
who seek privacy and security.
I also believe you can have at most two of security, functionality and
scale in any system but not all three.
   Brian

@_date: 2016-06-28 08:48:31
@_author: Brian Gladman 
@_subject: [Cryptography] 40 years of "Diffie-Hellman" 
I give Ellis, Cocks and Williamson as individuals more credit than that
since they were prevented from publication by their employer GCCHQ who
was more interested in maintaining the insecurity of others rather than
in giving them a tool that was capable of moving in the other direction.
I am very confident that they would have given the world the benefit of
their work had they been allowed to.
   Brian Gladman

@_date: 2016-03-24 23:55:13
@_author: Brian Gladman 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
I certainly agree the standardisation committees involved in
cryptographic and related protocols often produce overly complex and
convoluted designs that turn out to be fragile as a result.
But our major problems are not really with the low level primitives we
have available, quite a few of which have proved robust in  prcatice,
but rather in the fact that we are designing to accommodate too many
options for each primtive at each level in our protocols and too many
protocols that do essentially the same job in different ways.
I can hence see why the IETF would embark on a round of protocol
rationalisation, including an effort to reduce the number of primitives
involved.  But it seems to me very odd to discard a range of primitives
that have proved to be robust and reliable in real use (AES, DH, ...).
We certainly want to reduce the 'size' of our multi-culture but moving
to the other end of the spectrum is surely not the answer, especially so
if this means throwing out primitives that have proved to be effective.
    Brian Gladman

@_date: 2016-03-25 08:51:21
@_author: Brian Gladman 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
[snip details]
Much of what cryptographic protocols are about is the protection of
communications so when individual platform suppliers make their own
individual monocultures we are stuck either with no ability to
communicate securely between those using products from different
suppliers or with applications from other parties who are prepared to
support multiple supplier choices in what soon becomes a messy and
brittle multiculture.
So the issue is not that the Apple (or any other platform supplier) has
individually made good or bad choices but rather that they have made
different choices.  In overall terms this produces an unmanaged
multi-culture that has little or no chance of producing a good
information security result.
Moreover when it dawns on these companies that they have to meet wider
needs for communications with users on other platforms, the role of
their participants in standardisation processes is, at least in part,
one of protecting their existing investment in their security choices.
This inevitably results in extra complexity that introduces exploitable
security vulnerabilities and contibutes to the evolution of a brittle
mullti-culture and one that is much more difficult and costly to
implement for everyone involved.  So I can understand the IETF
motivation for wanting to 'start again and do it better this time'. But
I don't see their argument for throwing out primitives such as AES that
are now very widely supported and have proved to be effective in real
I do not like monocultures and I argued during the AES standardisation
process for the selection of three AES winners rather than one. The
immediate response from industry was that this would be a disaster
because they would have to implement all three at enormous cost.  But at
least this would have been a 'managed' multiculture rather than the
chaotic multiculture that emerges from a combination of the individual
choices made by the many individual players in the market.
So planned multicultures don't work and we are left with unplanned ones
driven by market interests in which better security plays very little
part (at least until now).
   Brian Gladman

@_date: 2016-03-26 17:08:27
@_author: Brian Gladman 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
[snip details]
My thanks to Stephen and yourself for clarifying what the IETF is doing.
Cutting out the 'rubbish', removing options of little value and
providing a main and a back up algorithm for each primitive seems to me
to be a very worthwhile approach (I still wish we had established a main
and a backup when we adopted AES and I am not convinced that we needed
more than a single key length of 128 bits)
I have a long term love of the IETF as it is not (at least not
obviously) under the control of governments in the way some other
standardisation committees with cryptographic interests have been.
   Brian Gladman
