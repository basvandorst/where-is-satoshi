
@_date: 2010-08-04 12:36:54
@_author: Chris Palmer 
@_subject: EFF/iSEC's SSL Observatory slides available 
"We have downloaded a dataset of all of the publicly-visible SSL
certificates, and will be making that data available to the research
community in the near future."
So, keep an eye on that page. The data is very useful. Many more interesting
conclusions remain to be drawn from the data; once it's out (I'm told Real
Soon Now), you can chew on it yourself and find things out that Eckersley
and Burns haven't gotten to yet.
Highlights from the slide deck are troubling:
* In addition to the implausibly large and diverse group of CAs you trust,
you also completely trust all the intermediary signers they they've signed.
Including, of course, DHS (slide 42). (See also: Soghoian and Stamm,
"Certified Lies".)  Windows and Firefox trust 1,482 CA certs (651
* Of 16 M IPs listening on 443, 10.8 M started SSL handshake; of those, 4.3
M used CA-signed cert chains (slide 14). Thus, the majority of servers use
"invalid"/invalid/self-signed certs.
* The invalid certs contain all kinds of bad stuff (see slide 16).
* The valid certs contain all kinds of bad stuff (see the rest of the slide
* CAs re-use keypairs in new certs to prolong the effective life (slide 28).
* Many CAs sign reserved/private names. Several CAs have signed e.g.
192.168.1.2. That host is certified to live in many countries by many CAs.
One CA thinks its identity is the same as a public/routable IP.
* The single most often signed name is "localhost" (6K distinct certs for
that subject name). Many CAs have signed that name many times; a few CAs
only signed it once. This suggests many CAs don't even track the names
they've signed to make sure they don't get tricked into signing a name
twice. Never mind the fact that they shouldn't be signing private names in
the first place... A colleague of mine got a CA-signed cert for "mail".
Could that be a problem? :)
* Your browser trusts two signing certs that use a 512-bit RSA key (slide 32).
* The bad Debian keys are not dead, and 530 are CA-signed. 73 of the 530
are revoked.
I am, as you know, predisposed to interpret Eckersley's and Burns's findings
as damning for the entire trusted third party with no accountability idea
--- "Trent Considered Harmful". But even CA/TTP proponents must admit that
our current system has failed hard: in principle, and empirically. Any new
system must include a substantial answer to the numerous fatal problems
Eckersley, Burns, and Ristic have observed.

@_date: 2010-08-04 13:30:01
@_author: Chris Palmer 
@_subject: EFF/iSEC's SSL Observatory slides available 
They tell me they will be releasing the data both raw and as a MySQL
database, so you can learn interesting things just by writing SQL queries.

@_date: 2010-08-05 10:33:17
@_author: Chris Palmer 
@_subject: phpwn: PHP cookie PRNG flawed (Netscape redux) 
He doesn't mention the php.ini variables session.entropy_length and
session.entropy_file. Last I checked, their default settings were unsafe,
but setting them to 16 and /dev/urandom should solve the problem he
describes in the paper.
Unless not.

@_date: 2010-08-13 12:16:01
@_author: Chris Palmer 
@_subject: Has there been a change in US banking regulations recently? 
When was this *ever* true? Seriously.
N-tier applications are I/O-bound in at least N ways. Common development
practice leads to excessive message sizes in most of the N tiers. Rare is
the web site with a low N and small message sizes. Fire up your HTTP proxy
and/or packet sniffer and compare Google to, well, just about everyone else.
Now that your packet sniffer and/or proxy is started up, it is also a fun
drinking game to play Spot The SSL Server Misconfiguration. When you see
HTTP persistence turned off, take a shot. When you see TLS session
resumption turned off or sessions destroyed after a short period of time,
take a shot. When you see a new TLS session for each new HTTP request, take
three shots (in addition to the shot you probably took when noticing HTTP
persistence turned off, because let's face it probably was). If the
misconfiguration is at a CDN, take three shots and smash your face with a
frying pan.
The game is usually played with Jaegermeister, but if you are testing a bank
site, you can afford good scotch. The adjudicating committee rarely
disqualifies a contestant on the basis of liquor, although I'd still advise
against the use of vodka.

@_date: 2010-08-26 13:01:11
@_author: Chris Palmer 
@_subject: towards https everywhere and strict transport security (was: Has there been a change in US banking regulations recently?) 
Cryptographic operations are measured in cycles (i.e. nanoseconds now);
network operations are measured in milliseconds. That should not be a
stunning surprise.
What is neither stunning nor surprising, but continually sad, is that web
developers don't measure anything. Predictably, web app performance is
unnecessarily terrible.
I once asked some developers why they couldn't use HTTPS. "Performance!" was
the cry.
"Ok," I said. "What is your performance target, and by how much does HTTPS
make you miss it? Maybe we can optimize something so you can afford HTTPS
"As fast as possible!!!" was the response.
When I pointed out that their app sent AJAX requests and responses that were
tens or even hundreds of KB every couple seconds, and that as a result their
app was barely usable outside their LAN, I was met with blank stares.
Did they use HTTP persistent connections, TLS session resumption, text
content compression, maximize HTTP caching, ...? I think you can guess. :)
Efforts like SPDY are the natural progression of organizations like Google
*WHO HAVE ALREADY OPTIMIZED EVERYTHING ELSE*. Until you've optimized the
content and application layers, worrying about the transport layers makes no
sense. A bloated app will still be slow when transported over SPDY.
Developers are already under the dangerous misapprehension that "TLS is too
expensive". When they hear security experts and cryptographers mistakenly
agree, the idea will stick in their minds forever; we will have failed.
The problem comes from insufficiently broad understanding: the sysadmins
fiddle their kernel tuning knobs, the security people don't understand how
applications work, and the developers call malloc 5,000 times and perform
2,500 I/O ops just to print "Hello, World". The resulting software is
unsafe, slow, and too expensive.

@_date: 2010-07-10 12:00:05
@_author: Chris Palmer 
@_subject: Question w.r.t. AES-CBC IV 
I'd rather have a known-safe design than to save 12 bytes.
Seriously: what the hell.
Say you have 1-byte messages, and that the cryptography will expand them to
128 bytes (...you use a MAC, right?). If this overhead factor is really bad
for you, for example because you expect to send thousands of messages per
second, your problem is a bad protocol design. Don't break the safety
mechanism to "support" an inefficient protocol.
Alternately, if you send messages only rarely, the overhead doesn't matter.
My point is, since you have tiny messages, throughput must not be your goal.
And yet, even with 128-byte messages, your messages are so small that
latency and bloat are not problems. You get confidential and MAC'd
communications for less than the cost of a tweet or SMS.

@_date: 2010-07-23 13:19:18
@_author: Chris Palmer 
@_subject: Encryption and authentication modes 
I wrote such a thing for web developers, and other people have too.
I can't see a reason to do anything other than
      ciphertext = aes_cbc(enc-key, random-iv, plaintext)
      sealed-blob = hmac(mac-key, ciphertext + timestamp) +
                    ciphertext + timestamp
You wrap this magic up in a trivial interface:
      byte [] seal(byte [] macKey, byte [] encKey, byte [] plaintext)
            throws GeneralSecurityException
      { ... }
      byte [] unseal(byte [] macKey, byte [] encKey, byte [] ciphertext,
                     long expirationInterval)
            throws UnexplodedCowException
      { ... }
You can find my Java code with a google search, but it's not special.  You
can write it yourself in your favorite language with small effort.
This gives you expiration, integrity, and confidentiality. You can make the
keys implicit by getting them from the server config variables or something.
In case of mangled or expired ciphertext, the checking function can fail
fast (no need to decrypt, since you do the hmac check first).

@_date: 2010-07-25 15:30:21
@_author: Chris Palmer 
@_subject: MITM attack against WPA2-Enterprise? 
Luckily, that describes nobody, right?
I used to think that non-end-to-end security mechanisms were wastefully
pointless, but adorably harmless. However, in my experience people keep
using link-layer garbage (and network-layer trash, and support protocol
junk) as a way to put off the hard work of real (i.e. E2E) security.
Non-E2E stuff hurts usability, availability, and security (by creating a
false sense).
Of course, we E2E fans have to get our usable security ducks in a row first.

@_date: 2010-07-26 21:22:39
@_author: Chris Palmer 
@_subject: A mighty fortress is our PKI 
...and trust any of those CAs on any (TCP) connection in the (web app)
session. Even if your first connection was authenticated by the right CA,
the second one may not be. Zusmann and Sotirov suggested "SSL pinning" (like
DNS pinning, in which the browser caches the DNS response for the rest of
the browser process' lifetime), but as far as I know browsers haven't
implemented the feature.
A presentation I've given at a few security gatherings may be of interest. I
cover some specific security, UI/UX, and policy problems, as well as some
general observations about incentives and barriers to improvement. Our
overall recommendation is to emulate the success of SSH, but in a browser-y,
gentle-compliance-with-the-status-quo-where-safe way.
Eckersley's and Burns' presentation at Defcon (coming right up) will present
their findings from a global survey of certs presented by hosts listening on
port 443. Their results are disturbing.
Ivan Ristic is also presenting his results of a survey at Black Hat on the
29th. I don't know anything about his findings.

@_date: 2010-07-26 23:43:50
@_author: Chris Palmer 
@_subject: A mighty fortress is our PKI 
If an attacker can steal the cert by any means, perhaps by means particular
to one of the hosted sites, he can now forge the identities of the 100+
sites. It gives the attack a multiplier. (It appears 100+ is not even the
largest number of entities in a single cert.) Potential attacks:
* Attack the server (e.g. buffer overflow in FooHTTPD or some other bug in
a web app the CDN runs (I know not all CDNs run cloud app hosting services,
but some do)). Note that even though all sites are served by the same
server, all sites suffer the risk profile of the highest-profile site. If a
CDN server is serving tiny.unknown.com and also mega.often-attacked.net,
tiny.unknown effectively endures attacks on mega.often-attacked.
* Questionable reseller. Although reselling a CDN might normally give you
access only to a subset of the CDN's subscriber's sites, you can get many in
one go because these certs have so many subjects. See below.
Let's just say I'd rather get the advantages of a CDN by other means, but
that I recognize that using a CDN can be a reasoanble economic trade-off in
many situations.
I wouldn't say "abhorrent", but the increased size of the cert could be a
I just wiresharked an HTTPS connection to  The
cert is 7,044 bytes. Admirably small given how many names are in it, but
still 6 KB larger than another cert I observed containing only one subject.
I have a hard enough time convincing people that HTTPS is not the root of
their web app performance problems and that therefore they CAN afford to use
it; the last thing we need is a certificate that big increasing latency at a
critical time in the page load. TLS sessions to that server don't seem to
last very long either, increasing the frequency of cert delivery; but maybe
that is necessary due to the high traffic such a server handles. (Gotta have
a limit on the size of the session store.)
I know it's a small thing, especially relative to the general content layer
heft of most sites, but still. When trying to convince developers to use
HTTPS, I need every rhetorical advantage I can get. :)
As Peter noted,

@_date: 2010-07-27 10:59:29
@_author: Chris Palmer 
@_subject: A mighty fortress is our PKI 
Defcon is the publishing event; and Black Hat for Ristic's material. It's in
a few days (Friday evening for Ekersley and Burns). Also keep an eye on the
eff.org site, I bet they'll say something there too. Possibly also at

@_date: 2010-07-27 11:34:25
@_author: Chris Palmer 
@_subject: A mighty fortress is our PKI 
I can recommend a good single-malt scotch or tawny port if you like. Have
you tried the Macallan 18?
False metrics are rampant in the security industry. We really need to do
something about them. I propose that we make fun of them.

@_date: 2010-07-27 23:38:58
@_author: Chris Palmer 
@_subject: A mighty fortress is our PKI 
That's along the lines of what EFF and I propose, yes. As I state in the
slides, a key problem is how to smooth over the adaptation problem by
various heuristics. We don't necessarily think that our mechanism is best,
just that it's one of a family of likely approaches.
Even if anyone other than spammers had adopted SPF, we should still be
seeking to reduce cruft, not increase it.
Sure, or simply put the cert in the DNS itelf. But, DNS is not secure, so in
doing so we would not actually be solving the secure introduction problem.
Some people think that DNSSEC can fill in here, but it hasn't yet.
As you can see, I am a firm advocate that we should emulate and improve on
SSH's success. On one of my computers I use the HTTPS Everywhere and
Perspectives plugins for Firefox; the latter renders CAs pretty much moot
and the former gets me "HTTPS by default" at least some of the time. It's a
fine thing.
Remember when we all dropped telnet like a hot potato and migrated to SSH
pretty much overnight? Let's do that again. Browsers should use secure
transport by default in a way that is meaningful to humans and cheap to
I believe it would be a vast improvement in such a scenario. It would be
hard to do worse than the status quo.
Thank you.

@_date: 2010-07-31 12:35:23
@_author: Chris Palmer 
@_subject: Five Theses on Security Protocols 
Usability engineering requires empathy. Isn't it interesting that nerds
built themselves a system, SSH, that mostly adheres to Perry's theses? We
nerds have empathy for ourselves. But when it comes to a system for other
people, we suddenly lose all empathy and design a system that ignores
Perry's theses.
(In an alternative scenario, given the history of X.509, we can imagine that
PKI's woes are due not to nerd un-empathy, but to
government/military/hierarchy-lover un-empathy. Even in that scenario, nerd
cooperation is necessary.)
The irony is, normal people and nerds need systems with the same properties,
for the same reasons.

@_date: 2010-09-30 22:23:59
@_author: Chris Palmer 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org #2354] [PATCH] Increase Default RSA Key Size to 2048-bits] 
In my quantitative, non-hand-waving, repeated experience with many clients in
many business sectors using a wide array of web application technology
stacks, almost all web apps suffer a network and disk I/O bloat factor of 5,
10, 20, ...
There are these sites where page-loads incur the creation of 30 TCP
connections. Pages have 20 tiny PNGs for navigation elements, all served
over non-persistent HTTP connections with the Cache-Control: header set to
no-cache. Each page view incurs a re-load of these static images. Take a
look at those images: why are they 35KB each? Oh, they have unoptimized
color palettes and 500 bytes of worthless comments and header junk and
actually they are twice as large as they appear on screen (the developer
shrinks them on the page with height= and width= attributes). To speed up
page loads, they serve the images from 10 distinct hostnames (to trick the
browser into parallelizing the downloads more). "What's spriting?"
How long does it take the browser to compile your 500KB of JavaScript? To
run it?
Compression is not turned on. The database is choked. The web is a front-end
for an oversubscribed and badly-implemented SOAP service. (I've seen backend
messaging services where the smallest message type was 200KB.) The 80KB
JavaScript file contains 40KB of redundant whitespace and is
dynamically-generated and so uncacheable. (I usually find a few XSS bugs
while I'm at it --- good luck properly escaping user data in the context of
arbitrary JavaScript code, but never mind that...)
The .NET ViewState field and/or the cookies are huge, like 20KB (I've seen
100KB) of serialized object state. It seems fine in the office, but from
home on my asymmetric cable line, performance blows --- it takes too long to
get the huge requests to the server! And yeah, your 20 PNGs are in the same
domain as your cookie, so that huge cookie goes up on every request. Oops...
I'm sure Steven's friend is competent. A competent web developer, or a
competent network architect? I have indeed seen this 12x cost factor before.
Every single time, it was a case where nobody knew the whole story of how
the app works. (Layering and encapsulation are good for software designs,
but bad for people.) Every single time, there were obvious and blatant ways
to improve page-load latency and/or transaction throughput by a factor of 9
or 12 or more. It translates directly into dollars: lower infrastructure
costs, higher conversion rates. Suddenly SSL is free.
I'm still fully with you; it's just that of all the 9x pessimalities, the
I/O ones matter way more.
Recommended reading:
"""...a popular network news site's home page required about a 180 requests
to fully load... [but for Gmail] it now takes as few as four requests from
the click of the "Sign in" button to the display of your inbox"""
Performance is a security concern, not just for DoS reasons but because you
have to be able to walk the walk to convince people that your security
mechanism will work.
The concern about the impact of 2048-bit RSA on low-power devices is
well-placed. But there too, content-layer concerns dominate overall, perhaps
even moreso.
Again, I'm not waving hands: I've measured. You can measure too, the tools
are free.

@_date: 2010-09-08 20:34:02
@_author: Chris Palmer 
@_subject: Hashing algorithm needed 
Why not?
Using HTTPS is easier than making up some half-baked scheme that won't work

@_date: 2010-09-29 21:22:38
@_author: Chris Palmer 
@_subject: 2048 bits, damn the electrons! [rt@openssl.org: [openssl.org #2354] [PATCH] Increase Default RSA Key Size to 2048-bits] 
That would only happen if we (as security experts) allowed web developers to
believe that the speed of RSA is the limiting factor for web application
That would only happen if we did not understand how web applications work.
Thankfully, we do understand how web applications work, and we therefore
advise our colleagues and clients in a way that takes the whole problem
space of web application security/performance/availability into account.
Sure, 2048 is overkill. But our most pressing problems are much bigger and
very different. The biggest security problem, usability, rarely involves any
math beyond rudimentary statistics...
