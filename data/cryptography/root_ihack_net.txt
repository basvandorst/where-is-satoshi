
@_date: 2005-04-03 09:43:27
@_author: Charles M. Hannum 
@_subject: New cipher used by iTunes 
============================== START ==============================
I took a look at the new cipher used in iTunes 4.7, and spent some time reducing it.  The algorithm appears to have a similar structure to a 10-round Twofish variant with fixed S-boxes, optimized via precomputed tables.  I have not fully analyzed what the permutation matrix and polynomial are, though.
There are a couple of strange changes.  E.g., they had put the IV mixing between the pre-whitening and post-whitening, but this turned out to effectively cancel out and be equivalent to an altered version with a more traditional CBC structure.
I'm including the current working implementation, along with some test vectors, if anyone else wants to take a look at it.

@_date: 2005-07-01 17:08:50
@_author: Charles M. Hannum 
@_subject: /dev/random is probably not 
Most implementations of /dev/random (or so-called "entropy gathering daemons") rely on disk I/O timings as a primary source of randomness.  This is based on a CRYPTO '94 paper[1] that analyzed randomness from air turbulence inside the drive case.
I was recently introduced to Don Davis and, being the sort of person who rethinks everything, I began to question the correctness of this methodology.  While I have found no fault with the original analysis (and have not actually considered it much), I have found three major problems with the way it is implemented in current systems.  I have not written exploits for these problems, but I believe it is readily apparent that such exploits could be a) Most modern IDE drives, at least, ship with write-behind caching enabled.  This means that a typical write returns a successful status after the data is written into the drive's buffer, before the drive even begins the process of writing the data to the medium.  Therefore, if we do not overflow the buffer and get stuck waiting for previous data to be flushed, the timing will not include any air turbulence whatsoever, and should have nearly constant time.
b) At least one implementation uses *all* "disk" type devices -- including flash devices, which we expect to have nearly constant time -- for timing.  This is obviously a bogus source of entropy.
c) Even if we turned off write-behind caching, and so our timings did include air turbulence, consider how a typical application is written.  It waits for, say, a read() to complete and then immediately does something else.  By timing how long this higher-level operation (read(), or possibly even a remote request via HTTP, SMTP, etc.) takes, we can apply an adjustment factor and determine with a reasonable probability how long the actual disk I/O Using any of these strategies, it is possible for us to know the input data to the RNG -- either by measurement or by stuffing -- and, therefore, quite possibly determine the future output of the RNG.
Have a nice holiday weekend.
[1] D. Davis, R. Ihaka, P.R. Fenstermacher, "Cryptographic Randomness from Air Turbulence in Disk Drives", in  Advances in Cryptology -- CRYPTO '94 Conference Proceedings, edited by Yvo G. Desmedt, pp.114--120. Lecture Notes in Computer Science  Heidelberg: Springer-Verlag, 1994.

@_date: 2005-07-03 11:42:21
@_author: Charles M. Hannum 
@_subject: /dev/random is probably not 
Remember that I specifically stated that I'm talking about problems with real-world implementations, not your original analysis.  Unfortunately, a few implementations (FreeBSD's implementation of "Yarrow" and NetBSD's "rnd" come to mind immediately) do not appear to implement the behavior you describe -- they simply always count disk I/O as contributing some entropy (using the minimum of the first-, second- and third-order differentials, which is likely to be non-0, but small and predictable, due to other timing variance).
Again, this problem exists in real-world implementations.
No, you just need to be able to estimate it with a high probability.  I don't see any reason this is not possible, given that response times are directly proportional to the interrupt timing.  This may be especially bad in implementations such as OpenBSD and NetBSD which limit the precision of the time samples to 1 microsecond.
Also, I don't buy for a picosecond that you have to gather "all" timings in order to predict the output.  As we know from countless other attacks, anything that gives you some bits will reduce the search space and therefore weaken the system, even if it does not directly give you the result.

@_date: 2005-07-14 02:34:24
@_author: Charles M. Hannum 
@_subject: mother's maiden names... 
FYI, that's a feature of Costco, not AmEx.  Costco requires a picture because the card is used in place of a normal Costco card to get admitted into the store.  They are somewhat ruthless about sharing cards for personal

@_date: 2005-06-09 01:59:12
@_author: Charles M. Hannum 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
I can name at least one obvious case where "sensitive" data -- namely credit card numbers -- is in fact something you want to search on: credit card billing companies like CCbill and iBill.  Without the ability to search by CC customers are pretty screwed.
That said, I will never buy the "only encrypt sensitive data" argument.  In my experience, you *always* end up leaking something that way.

@_date: 2005-06-09 17:37:22
@_author: Charles M. Hannum 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
Are you joking?
If we assume that the last 4 digits have been exposed somewhere -- and they usually are -- then this gives you at most 38 bits -- i.e. 2^38 hashes to test -- to search (even a couple less if you know a priori which *brand* of card it is).  How long do you suppose this would take?
(Admittedly, it's pretty sketchy even if you have to search the whole CC# space -- but this is why you need to prevent the data being accessed in any

@_date: 2005-06-09 17:44:49
@_author: Charles M. Hannum 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
On reconsideration, given the presence of the check digit, I think you have at most 2^34 tests (or 2^32 if you know the brand of card).  And this assumes there aren't additional limitations on the card numbering scheme, which there always are.
I guess you could use a keyed hash.  Remember, though, you can't use random padding if this is going to be searchable with a database index, so the amount of entropy you're putting in is pretty limited.
