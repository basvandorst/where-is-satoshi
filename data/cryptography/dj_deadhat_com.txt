
@_date: 2004-04-02 22:06:21
@_author: dj@deadhat.com 
@_subject: Privacy Concerns for UWB technology? 
The answer to the question "Is UWB secure" depends entirely on the
assumptions you make. UWB is after all a physical layer radio/modulation
technology that is only peripherally related to the layers above that may
provide security in a number of forms, say link security, VPNs, SSL etc.
Here are a couple of assumptions I am throwing in:
1) You are referring to one of the two significant adopters of UWB in a
standardized context, namely IEEE 802.15.3a and Wireless USB.
2) The actual UWB technology being considered is the MBOA (Multi Band OFDM
Alliance) flavor.
3) The UWB technology provides and underlying bit rate of 1 Gbps. This is
As an 802.15.3a standard (when or if they finish it) there are two places
that security specific to the technology will come from. The first is
802.1X/ae/af (EAP conduit, link ciphers and authenticated key exchange).
These standards are works in progress (.1X is currently withdrawn awaiting
necessary undercarriage work). The second is from native 802.15 link
security that the 802.15 working group may choose to supply. Above the
link layer, it is not the IEEE's problem. 802.15 is an IEEE link layer
spec and upper layer methods that might apply are more generic E.G. those
from the IETF.
If we are talking about its application in Wireless USB then it is the job
of the writers of the Wireless USB standard to provide the appropriate
security mechanisms. They have a bigger problem to solve, since they are
having to secure a set of higher layer functions that are outside the
scope of IEEE 802. Beyond observing thats its not written yet, there's not
much to go on other than low expectations set by a long and sorry history
of wireless security mechanisms.
Getting down to specifics, if its 1 Gbps, then the currently in vogue IEEE
802 link cipher (AES-CCM) will do fine in terms of implementation
constraints. However 802.1ae has chosen GCM as its link cipher and this
holds the promise of linearly scalable implementability up to much higher
The short range of UWB technology may provide security from some attack
models (the guy next door), but don't count on it and you can be sure that
by the nature of networks, remote attacks will be possible unless
anticipated and secured against. I remember reading recently about a
succesful bluetooth fishing expedition at a trade show. The short range
didn't help there.
The pessimistic view of IEEE 802 link security mechanisms is that first
time publications of standards will always come with broken security
because they never get the attention they deserve from the crypto
community until the flaws are identified and need fixing. In support of
that allegation, I offer you 802.11 WEP, 802.16 PKM, The 802.16 link
cipher, and 802.1X. All are broken and currently being overhauled.
The optimistic view of IEEE 802 link security is that 802.1X/ae/af is the
second go around, it is getting the attention it deserves and may deliver
an appropriate result in terms of security. However its meeting the needs
of wireline standards (802.3, 802.17) and is too high in the stack to
support the needs of mobile wireless links.
Just to complicate matters and to lower your expectations further, the use
of UWB in media centric home networks leads to new usage models and
unsophisticated users, tied in with the horrors of DRM on the media side.
This presents problems that arguably are not solved in a practical,
deployable sense today. Your example of the neighbour watching your TV is
a fine example.
DJ

@_date: 2004-10-13 10:59:53
@_author: dj@deadhat.com 
@_subject: AES Modes 
On the IEEE 802 standards track, CCM and GCM have traction. CCM has been
in 802.11 for a while and the 802.16-2004 was published last week,
supplanting the broken DES-CBC mode with AES-CCM. For wireless systems, we
know and like CCM and it's going to take a lot to oust it.
I'm aware of a handful of other wireless protocols in development that
appear to be all headed the CCM way.
GCM proposed for 802.1ae linksec. This is the 'fast' mode for wired ethernet.
For packetised traffic below 1Gbps, CCM works just great. The CTR and CBC
parts of CCM run in parallel in hardware, making the basic latency = 1 AES
(which is 11 clocks in a simple implementation). With a bit of HW loop
unrolling and pipelining, I can do CCM upto several gigabits.
CCM nicely matches the structure of packets. Namely
1) Get header -> Process additional authenticated data
2) Get payload -> Process MAC and encryption in parallel.
So it is not a bear to implement in a typical communications datapath IC,
where things are presented to you in this order.
GCM allows block level parallelism up to a point. Hence enabling me to put
lots of parallel AES blocks on a chip and go at multi gigabits without
breaking a sweat. It does however have all that Galois arithmetic to do
per block, which increases the path depth a bit.
There is however a fundamental speed problem with packet oriented AES
based modes. The parallelism you can achieve on things like GCM requires
that you have multiple blocks to run in parallel. If I get a large number
of small packets, each < 128 bits long, then there's nothing to do in
parallel at the block level and so my speed limit is determined by how
fast I can run 11 rounds of AES.
This may come to bite us in the future and when we start having to protect
data pushing the terabits, we either need larger packets or something
different in the crypto. One way is to protect over large packet
aggregates, but no 802 level protocol is set up to support it. Stream
ciphers look attractive here, we can make them go *really* fast in
Another frustrating aspect of the current crop of modes is frame
expansion. Throwing in an 8 byte nonce and an 8 byte ICV per packet is a
heavy overhead. Deriving nonces from the protocol state is generally not
wise since the frame counts are either non existant (802.3, 802.11) or not
secured (802.16).
In the coming years, I would like to see link protocols (I.E. 802.*) move
link security down to the PHY, to protect management and data traffic
equally, to secure the protocol state as well as data and so to reduce
packet overhead. I would also like to see the standardized crypto and
protocols be structured to work over larger aggregates of packets,
protecting the structure of transmission as well as the content and
allowing much greater levels of parallelism in the HW implementations.
Obviously, none of this is very relevant above layer 2.

@_date: 2006-12-03 11:03:18
@_author: David Johnston 
@_subject: Can you keep a secret? This encrypted drive can... 
Compared to AES-128, AES-256 is 140% of the rounds to encrypt 200% as much data. So when implemented in hardware, AES-256 is substantially faster.
AES-256 - 18.26 bits per round
AES-128 - 12.8 bits per round
I imagine that this would matter when the implementation is in a hard disk or disk interface.

@_date: 2006-12-04 12:48:17
@_author: David Johnston 
@_subject: Can you keep a secret? This encrypted drive can... 
I stand corrected.. The source of my error was reading the rijndael spec, not the nist spec.

@_date: 2006-03-15 16:40:18
@_author: David Johnston 
@_subject: ISO rejects WAPI (for now) 
WAPI is no longer secret sausce. It has been published in Chinese, including the formerly unpublished SMS4 block cipher.

@_date: 2009-09-04 20:57:30
@_author: David Johnston 
@_subject: RNG using AES CTR as encryption algorithm 
NIST doesn't provide specific KAT vectors for AES-CTR because the results depend on your specific counter construction.
When you interact with a FIPS test lab, you will provide them with your counter construction, they will provide you with the KATs and you will then test to those KATs.
This is probably why you are not finding AES-CTR vectors in the same places you might find AES-CBC vectors. NIST explains this somewhere in their publications.
Convincing yourself that you have implemented AES-CTR correctly usually involves first checking that your AES-ECB is correct, then putting the output of you counter construction into some other known good AES-CTR implementation and comparing the results with your implementation.

@_date: 2010-08-26 09:48:00
@_author: dj@deadhat.com 
@_subject: questions about RNGs and FIPS 140 
The Nevada rules don't convincingly demand non determinism. They do say
things that probably unintentionally exclude non determinism.
"4. The random number generator and random selection process must be
impervious to influences from outside the device, including, but not
limited to, electro-magnetic interference, electro-static interference,
and radio frequency interference. A gaming device must use
appropriate communication protocols to protect the random number generator
and random selection process from influence by associated equipment which
is conducting data communications with the gaming device.
(Adopted: 9/89. Amended: 11/05; 11/17/05.)
An impossible requirement for a TRNG based on physical processes. This
requirement pretty much demands determinism and in practice is untestable.
Some definitions..
"23. ?Randomness? is the observed unpredictability and absence of pattern
in a set of elements or events that have definite probabilities of
 and
"20. ?Random Number Generator? is a hardware, software, or combination
hardware and software device for generating number values that exhibit
characteristics of randomness."
Definitions that both a TRNG and a PRNG can meet. They don't get down to
the nitty gritty of what the observer might know, like the internal state
of a PRNG, that would impact whether the data has 'observed
"14.040 Minimum standards for gaming devices..
2. Must use a random selection process to determine the game outcome of
each play of a game. The random selection process must meet 95 percent
confidence limits using a standard chi-squared test for goodness of fit.
(a) Each possible permutation or combination of game elements which
produce winning or losing game outcomes must be available for random
selection at the initiation of each play.
(b) For gaming devices that are representative of live gambling games, the
mathematical probability of a symbol or other element appearing in a game
outcome must be equal to the mathematical probability of that symbol or
element occurring in the live gambling game. For other gaming devices, the
mathematical probability of a symbol appearing in a position in any game
outcome must be constant. (c) The selection process must not produce
detectable patterns of game elements or detectable dependency upon any
previous game outcome, the amount wagered, or upon the style or method of
Again, a PRNG would meet these requirements. The only specific test
proposed is the Chi-square GOF.

@_date: 2010-07-14 10:22:49
@_author: dj@deadhat.com 
@_subject: Encryption and authentication modes 
CCM is widely implemented. It's a matter of where you look.
Down at the MAC layer, AES-CCM has proved popular in wireless packet
communication because it is well adapted for separating the treatment of
the header as plaintext AAD from the packet body as ciphertext. Also it is
relatively efficient to implement in hardware since it relies only on a
single AES encrypt block cipher and the birthday resistance of the
ciphertext MAC reduces on-air per packet overhead. This is the why for
example that you see AES-CCM in wireles USB, 802.11, 802.16 and WiMAX
management protocols.
A couple of years after 802 went for AES-CCM, AES-GCM became the
802.3/ethernet choice since it is more parallelizable and so can be
implemented for 10Gbps+ links where CCM becomes trickier. The per packet
overhead is higher, but bandwidth on wires is cheap.
I don't think you can really implement CCM except in the context of a more
detailed specification for a protocol. CCM is a flexible specification and
protocols that use it must nail down a number of parameters and field
sizes in order to be interoperable. It's not so easy to just plug it in
which makes is less convenient for the more pluggable software based
protocols higher up the stack.
Some technically good candidates for standards adoption, E.G. OCB met
resistance due to licensing issues.

@_date: 2013-12-25 11:27:42
@_author: dj@deadhat.com 
@_subject: [Cryptography] [IP] 'We cannot trust' Intel and Via's 
It would be normal to add characterization circuits to chips to measure
the properties of the silicon during and after manufacture. But if you
were doing your own layout, leaving space for these would be part of the
design rules.
Is there anything substantive to suggest it was something else?

@_date: 2013-11-08 13:01:58
@_author: David Johnston 
@_subject: [Cryptography] randomness +- entropy 
I disagree about the 'sealed-off' thing.
I have championed the use of the radio as an entropy source in several wireless product developments. Most wireless modems have a symbol error vector register that is trivial to read. I have never found it to be a technical problem. The problem lays in having the right engineers aware of the need.

@_date: 2013-11-08 14:10:25
@_author: David Johnston 
@_subject: [Cryptography] SP800-90A B & C 
For those with insomnia issues, I have submitted public comments to NIST against SP800-90A, B and C.
The current comments are here:
Earlier comments on earlier drafts are here:
and here:

@_date: 2013-11-10 13:22:31
@_author: David Johnston 
@_subject: [Cryptography] SP800-90A B & C 
A good standards process is iterative and convergent. So comments can be discussed and resolved before the standard is updated.
Unfortunately the NIST process looks nothing like a normal standard process.
Some of my comments were about the way the spec and FIPS make it hard to add multiple sources. I would like to enable users to add their own trusted sources so they can ensure randomness is robust.

@_date: 2013-11-11 21:06:14
@_author: dj@deadhat.com 
@_subject: [Cryptography] SP800-90A B & C 
A normal standard process puts out a draft, invites comments and has a
standards body vote on the proposals in the comments.
The the process iterates. But in each iteration, the parts of the document
open for technical comment is limited to unresolved comments, things that
changed and obviously broken things.
This iterates until there are no comments left.
Then the whole document is opened up for comment again and the process
repeated 1 or 2 more times.
This is a very effective way of getting consensus in a specification and
making the process work on a cadence with which people can align their
product developments.
NIST seems to operate by
1) Writing a document
2) Inviting comments
3) Updating the document and stamping it as done.
So we all end up with an inadequately reviewed document which no one is
happy with. Check out the RNG discussions on this forum. What educated
people want looks nothing like what SP800-90 offers. Those of us
implementing to SP800-90 and FIPS run into issues that have to be argued
out with the certification lab and have to be designed around in ways that
do not enhance security.
The no-inputs to meet FIPS140-2 + SP800-90 issue is real. We were not told
"You can't do that". We asked for guidance because the problem was evident
and in discussion with the certification house the options, everyone
agreed that the "no inputs" solution would meet both specs. No one was
misinterpreting a spec. The conflict is there for everyone to see. I can
name names in private, but I'm not pointing a finger at the certification
lab. The problem is in the spec.
How the 4.9.2 issue come into being is beyond my comprehension. Why is it
there? What were they thinking?
During the ongoing development of SP800-90, new math and new algorithms
have comes out that change the playing field. Compliant solutions are
locked out from benefiting and users are therefore locked out from the
security benefits, which with RNGs lay mostly in greater deployment, since
the biggest problem with good TRNGs is their absence on computers which in
part is related to the heavy resource load of SP800-90 solutions.
I haven't seen a lot of comments in SP800-90B&C. I didn't see my previous
sets of comments get published in the last two rounds. Am I missing other
people's comments? It's necessary to be able to see all the comments in
order to converge on consensus and learn from each other.
We've got chips going out the door and the spec is still subject to
change. As I stated in a recent presentation "This is not a standards
process one can love".

@_date: 2013-11-11 21:18:37
@_author: dj@deadhat.com 
@_subject: [Cryptography] SP800-90A B & C 
Part of my argument was that we can have both. The design must ensure that
if designed to the spec without manipulation, it will offer secure random
numbers. The spec can allow that users can mix in their own sources to
mitigate the issues that the former model raises.
In reality, pool constructions are really just the merging of conditioning
with reseeding. SP800-90 thinks in terms of having a good seed, condensed
from a larger volume of partially entropic data and so for example (in CTR
DRBG) just XORs in the seed to the DRBG state. A pool construction stirs
in the new entropy to the state of the PRNG in what amounts to an entropy
extraction process.
I don't see that as a big difference. It's just how you label your
algorithms. But SP800-90 and FIPS140-2 is hostile to user sourced entropy.

@_date: 2013-11-11 21:22:09
@_author: dj@deadhat.com 
@_subject: [Cryptography] SP800-90A B & C 
My comments point to the source of the problem being in the spec and ask
for resolution with specificity.

@_date: 2013-10-03 21:27:47
@_author: David Johnston 
@_subject: [Cryptography] Sha3 
What makes you think Keccak is faster than the alternatives that were not selected? My implementations suggest otherwise.
I thought the main motivation for selecting Keccak was "Sponge good".

@_date: 2013-10-04 19:22:08
@_author: David Johnston 
@_subject: [Cryptography] Sha3 
It may be, but that wasn't really my argument. My argument was that algorithm speed cannot have been the primary selection criteria because Keccak is not the fastest and faster alternatives had a similar or better security margin in the claims made.
I don't disagree, but it's a tad orthogonal to my argument.

@_date: 2013-10-07 17:28:54
@_author: David Johnston 
@_subject: [Cryptography] P=NP on TV 
That gets to the heart of why I think P != NP.
We are led to believe that if it is shown that P = NP, we suddenly have a break for all sorts of algorithms.
So if P really does = NP, we can just assume P = NP and the breaks will make themselves evident. They do not. Hence P != NP.
Wheres my Field's Medal?

@_date: 2013-10-17 20:36:11
@_author: dj@deadhat.com 
@_subject: [Cryptography] /dev/random is not robust 
Don't confuse the SP800-90 requirements for continuous testing and rest of
the necessary chip testing that all chips need. FIPS 140-2 imposes limits
on what passes through a FIPS boundary. This means that it is way easier
to do your chip testing using logic BIST contained within the FIPS
boundary than it is to try and pull full chip test through FIPS. You also
need the SP800-90 continuous tests implemented within the FIPS boundary.
You need to test your chips. FIPS forces you to use BIST.
You need to monitor your sources. SP800-90 forces you to use a particular
type of continuous monitoring.
These are quite separate things in type and style, but you have to do both.
In a FIPS compliant system with a software SP800-90 DRBG, I assume that
the FIPS boundary almost never coincides with the boundary of the SP800-90
implementation, so the rules and driving issues are quite different.
This has almost nothing to do with the Linux kernel random service which
does it's own thing very differently to NISTy RNGs.

@_date: 2013-10-17 20:56:31
@_author: dj@deadhat.com 
@_subject: [Cryptography] /dev/random is not robust 
Yes. So they don't release low entropy numbers.
But hardware should provide a firehose throughput entropy source from
power on so the RNG is always initialized to full entropy and won't block
regardless of the load on the random number service.

@_date: 2013-10-28 21:28:11
@_author: dj@deadhat.com 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
But the specifications (SP800-90x & FIPS 140-2) make it spectacularly hard
to mix in multiple sources in a compliant way. SP800-90 gives a way to mix
in "additional entropy" and "personalization strings", but FIPS 140-2
states that all sources must be authenticated. All configuring entities
must be authenticated. Try authenticating hardware on one end of chip
against hardware at the other end of the chip. It is the mother of all
chicken and egg problems.
The 'per instance' nature of the additional entropy data provided during
the SP800-90A instantiation algorithm is entirely incompatible with
hardware implementations that have fixed state.
The solution is to not use or permit any of these extraneous inputs and
don't permit instantiation other than at reset time, then you don't need
to go about 'authenticating' the caller, whatever that means in the
context of one circuit on a chip talking to another circuit on a chip.
Just so you can take it in for FIPS certification.
SP800-90A defines a reseed as a callable function and defines the caller
as being the thing that provides the fresh entropy. FIPS 40-2 requires
that the caller be authenticated. That's what I call a mess.
If NIST want people to make compliant paranoia RNGs, that accept all
sources, authenticated or not, and mix them, then that is what the specs
should say, but they do not. All sources have to have been designed in
ahead of time. All entities it interacts with have to have been
provisioned with authentication credentials.
So the 'mix everything in' RNGs give one model of security (Only 1 of N
sources need be good). The NIST RNGs give another (We designed the one
true source to be good). But there is no overlap. One cannot be both a
NISTesque RNG and a 'mix everything in' RNG.
I have heard this complaint from multiple people on multiple projects -
paraphrasing: "We had to make our RNG less secure to get it through FIPS -
We had to take out the additional mix in sources".

@_date: 2013-10-29 21:09:57
@_author: David Johnston 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
But FIPS requires that the inputting entity be authenticated. In a chip scenario, that is silly. Especially when 'authenticated' means a FIPS authentication scheme where each on-chip bus attached entity has to be provisioned a cert by a third party or undergo some ephemeral key exchange with bignum arithmetic.

@_date: 2013-10-31 20:01:24
@_author: dj@deadhat.com 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
It's not a misunderstanding. It's right there in section 4 of FIPS 140-2.
FIPS 140-2 and SP800-90 are fundamentally incompatible.  It leads directly
to demands from the certification houses that cannot be met. Similarly I
can't get CTR_DRBG vectors out of a test lab that don't include additional
entropy. They read it as mandatory due to ambiguous text in the spec. The
situation for certifying a coincident boundary FIPS 140-2, SP800-90
circuit is thoroughly messed up in ways that compromise security.
I've detailed the details in my comments against SP800-90A, which I'll be
sending off in the next couple of days.
I'm not persuaded by the angst about malicious input. The text in SP800-90
around defining the 'consuming application' is just fine. The consuming
application can give the nonces, personalization strings and additional
entropy necessary to secure its own instance. If you trust the other guy
to supply your personalization string for you, good luck with that.
But it's FIPS 140-2 that prevents an unauthenticated consuming application
putting in the material necessary to get the random numbers necessary to
perform a secure authentication. A chicken and egg problem if ever there
was one.

@_date: 2013-10-31 21:43:54
@_author: dj@deadhat.com 
@_subject: [Cryptography] [RNG] /dev/random initialisation 
And what are we supposed to do with 4.9.2 of FIPS 140-2? That reduces the
output entropy to considerably less than (1-epsilon) | epsilon < 1/2^64,
as required by SP800-90A.
If every there was a clause that I though was inserted to weaken an RNG
spec, that would be it.
Please re-open FIPS 140-2 for comment.
"If each call to a RNG produces blocks of n bits (where n > 15), the first
n-bit block generated after power-up, initialization, or reset shall not
be used, but shall be saved for comparison with the next n-bit block to be
generated. Each subsequent generation of an n-bit block shall be compared
with the previously generated block. The test shall fail if any two
compared n-bit blocks are equal.

@_date: 2013-09-08 21:40:34
@_author: David Johnston 
@_subject: [Cryptography] [cryptography] Random number generation 
So that that state remains secret from things trying to discern that state for purposes of predicting past or future outputs of the DRBG.
 So that one thread cannot undermine a second thread by putting the DRNG into a broken mode. There is only one DRNG, not one per core or one per thread. Having one DRNG per thread would be one of the many preconditions necessary before this could be contemplated.
 Any method of access is going have to be documented and supported and maintained as a constant interface across many generations of chip. We don't throw that sort of thing into the PC architecture without a good    Obviously there are debug modes to access raw entropy source output. The privilege required to access those modes is the same debug access necessary to undermine the security of the system. This only happens in very controlled circumstances.
That's what BIST is for. It's a FIPS and SP800-90 requirement.
Yes they have. I've answered this same question multiple times.
Access to the raw output would have been a massive can of worms. The statistical properties of the entropy source are easy to model and easy to test online in hardware. They are described in the CRI paper if you want to read them. That's a necessary part of a good entropy source. If you can't build an effective online test in hardware then the entropy source is not fit for purpose.

@_date: 2014-04-08 00:19:28
@_author: dj@deadhat.com 
@_subject: [Cryptography] [cryptography] Announcing Mozilla::PKIX, 
Staring at code is lots of fun, but I didn't see a design document there.
Are we supposed to just guess what all those variable names and pointers
are for?
I wouldn't dream of asking people to review my code without providing
written documentation of the design structure and intent, so I wasn't
wasting their time.

@_date: 2014-04-15 20:07:08
@_author: dj@deadhat.com 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
It is one of my long term, implausible goals to replace TLS with a
collection of independent app to app function-targeted security protocols
that are individually simple enough to understand and implement cleanly. I
will certainly fail.
For paying with a credit card.. A secure credit card payment protocol
For authenticating a web site and producing keys to bind .. A web page
authentication protocol.
For remotely logging into a shell producing keys to bind .. A secure shell
login protocol.
There are many more possibilities.
Today, SSL and TLS with all that entails (ASN.1, X.509, PKCS, TCP/IP etc.)
is the hammer and any securable thing is the nail. But it's really a
client-server session privacy and integrity protocol with issues. It isn't
designed to protect my banking transactions, just the traffic over which I
communicate my transaction intent. If I had a secure bank transaction
protocol independent of TLS, heartbleed wouldn't matter.
A classic mismatch between TLS and its primary use securing web traffic is
the failure of a virtual server to be able to produce the right cert for
the right virtual web site. The cert is really identifying the TLS
termination point which may be a web server, rather than a web site, of
which the server may be serving many. That's one reason why a web-site
security protocol would be more effective than TLS plumbed under HTTP.
TLS does need nuking so we can replace it with simpler things. The
sentiment isn't wrong, it's just hard to pull off.

@_date: 2014-04-21 21:49:10
@_author: dj@deadhat.com 
@_subject: [Cryptography] SP800-90A Again and again and again. 
A new draft for SP800-90A has been published:
With the rationale here:
I haven't been through it yet.
David Johnston,
Chief complainer about SP800-90.

@_date: 2014-12-09 17:10:08
@_author: dj@deadhat.com 
@_subject: [Cryptography] A TRNG review per day (week?): ATSHA204A has low 
Clock drift between the Pi and the part's internal clock could be the
source of entropy. It is entirely possible to have the timing of reads
inject entropy into a PRNG. In fact SP800-90A pretty much works this way
if you implement it with a straight face. The DRBG will arrive at a
reseed/generate decision and the timing of your reads will drive the
It could be that the part has a ring oscillator for it's own clock and
there is no entropy source, the variation is all read timing.
If that's what is going on, then looking at the distribution of the first
value over many boots should eliminate more of the external entropy and
get you closer to the internal entropy.

@_date: 2014-12-18 23:29:06
@_author: dj@deadhat.com 
@_subject: [Cryptography] Fwd: 78716A 
'Overly broad' is not the limiting factor.
I made an FOIA request that they tell me where I've traveled (I don't have
records, but I know the government does and they want me to fill in where
I've been on a form).
It couldn't be less broad.
I'm still waiting for a response 6 months later.

@_date: 2014-02-01 15:40:25
@_author: David Johnston 
@_subject: [Cryptography] cheap sources of entropy 
This is one of the reasons we made RdRand an instruction that will inject entropy into the state of a VM when it is executed, providing the hypervisor doesn't trap it.
A VM quantizing interrupt timing on a machine with an SSD, no network traffic (yet - it's early boot) and no keyboard or mouse leaves the VM potentially without any entropy sources, but the poor OS may not even know it since it is assuming the interrupts and disk timing are entropic.
It is incumbent on a provider of a hypervisor to explain and show how entropy gets into the VM.

@_date: 2014-02-14 02:57:23
@_author: dj@deadhat.com 
@_subject: [Cryptography] The ultimate random source 
I'm currently learning how to write Android programs so I can write an
entropy extractor and random number generator using the camera and all the
other physical sensors.
Of course a phone is no place to hold a secure random number. But it's a
fun project to learn android programming.

@_date: 2014-01-03 15:50:27
@_author: dj@deadhat.com 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Is NIST seriously considering this?
Has anyone proposed this?
I notice that the comment period on SP800-90A/B/C has long been closed,
but the comments have not been published. So it's hard to tell what is
going on.
A little more transparency would go a long way.

@_date: 2014-01-03 15:57:51
@_author: dj@deadhat.com 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
I apologize. I was wrong. The comments weren't in the first place I looked
on the csrc website, but they were in the second.

@_date: 2014-01-14 14:42:27
@_author: dj@deadhat.com 
@_subject: [Cryptography] Registration for Real World Cryptography 2014 
For a second year running, I found out about it just as the conference was

@_date: 2014-01-20 16:46:23
@_author: dj@deadhat.com 
@_subject: [Cryptography] cheap sources of entropy 
This is the generic form of the "where do I get entropy discussion":
Question) How do I get entropy into my computer?
Answer) Sample noise from the environment using method X.
Response) But something in the chain might be subverting it.
Chips contain semiconductor junctions which are quite entropic when
stimulated with volts. Perhaps we should shame chip vendors into including
the quite small and power efficient circuits that can gather entropy as
bits and pass them to a nearby CPU.
As always, something in the chain might be subverting it, but unless you
build your own circuits, you're going to have to deal with the cognitive
dissonance if you don't want to fall into the "Paranoid Entropy Trap".
Paranoid Entropy Trap:
  The tendency to get no entropy because you turned off all the sources of
entropy, because you don't trust any of them.
See various versions of the Linux kernel for examples.

@_date: 2014-01-20 16:49:07
@_author: dj@deadhat.com 
@_subject: [Cryptography] HSM's 
Not normally thought of as a HSM, but it's an RNG+non volatile
storage+algorithms. So it looks like one to me.

@_date: 2014-01-21 18:51:08
@_author: dj@deadhat.com 
@_subject: [Cryptography] cheap sources of entropy 
Things (well most things) don't need a 3Gbps RNG. You build a 3Gbps RNG so
that thread A can't perform an RNG DoS on thread B. It's an availability
thing and relates to the saturation properties of on-chip busses. The
performance is neither an argument for nor against the multiple sources
Your position falls down when the only source of entropy on the platform
is the single provided RNG and you don't credit it any entropy because you
choose not to trust it on behalf of the user, so their OS mediated random
number supply blocks when if fact there's copious entropy to be had.

@_date: 2014-01-21 18:56:21
@_author: dj@deadhat.com 
@_subject: [Cryptography] Auditing rngs 
It would be more straightforward to fix FIPS 140-2 and SP800-90 so that
there was conformant behavior defined for a raw access interface in
SP800-90 that didn't fall foul of the role based authentication required
by 140-2.
Seriously? The last time someone tried asymmetric keys in a DRBG, it
didn't go so well. I'm certainly not building bignum hardware into any RNG
I design.
I can imagine this being a nightmare. How much code need validating to do
this, compared with validating the original RNG?

@_date: 2014-03-07 17:56:44
@_author: dj@deadhat.com 
@_subject: [Cryptography] OMB Circular A-119 
Do we think this is a bad thing? I can see why ANSI are upset, but in
general I like the idea of the agencies avoiding consortia where there's
less scrutiny.

@_date: 2014-03-12 18:48:20
@_author: dj@deadhat.com 
@_subject: [Cryptography] recommending ChaCha20 instead of RC4 (RC4 again) 
Every CCM implementation I've seen or designed myself in commercial
products has been in hardware.
I like CCM because I can see how it works without needing a degree in
mathematics and because the authors paid attention to how packets are
encoded. I'm one of those that voted OCB off the 802.11i island in favor
of CCM.
If you need many bytes/clock, GCM is the right choice. Hence 802.11i used
CCM whereas 802.1AE (really for 802.3) used GCM since it has to work on
wired protocols.

@_date: 2014-03-17 20:48:37
@_author: dj@deadhat.com 
@_subject: [Cryptography] Apple's Early Random PRNG 
RdRand works from the first instruction executed. This is a pretty basic
requirement for a system RNG. You should expect your device vendors to
meet that requirement.

@_date: 2014-03-17 22:50:44
@_author: dj@deadhat.com 
@_subject: [Cryptography] Apple's Early Random PRNG 
Well RdRand is what Intel's provides in its CPUs. I'm not trying to have
yet another a RdRand discussion. Just a discussion about the sense behind
whatever RNG you put in the hardware, you make it available as soon as
instructions start executing.
I find it bizarre but entirely consistent with reality, that a PoS might
be created and deployed with no effective RNG.
While Target could and should require their PoS suppliers to make them
secure, I don't know that Target are the best people to reference for for
a well reasoned set of PoS requirements.
Sadly, PCI-DSS is the right place, but they are the ones who's specs
require us to keep plaintext credentials on our credit cards. PCI-DSS
specs do nothing to enhance security.

@_date: 2014-03-23 06:16:07
@_author: dj@deadhat.com 
@_subject: [Cryptography] BLAKE2: "Harder, Better, Faster, 
Count me as one of the engineers who have to pick a hash function to
implement. I have that problem right now.
 I do not care about speed because I implement my stuff in hardware and
regardless of the efficiency of the algorithm I can throw the necessary
number of gates at the problem to make it run at whatever speed is
 I do care about security. Shipping insecure crypto is not an option.
 I do care about being able to publicly and honestly justify the
reasons for whatever selection is made.
 is easy.
 is something of a judgement call for all algorithms not currently known
to be broken, so I lean on the advice of good cryptographers.
 is the killer. The justification of a couple of years ago "We use NIST
SP800-xx because that's the standard" doesn't really fly today because
NIST lacks the trustworthy angle. But there is no obvious consensus body
that we can point to for sound algorithm selection. If I select Blake2,
put it in a substantial fraction of the CPUs out there and it's
subsequently broken, that's my fault. If I select SHA3 for the same chips
and it's subsequently broken, that's NIST's fault but still by problem.
ANSI X9 isn't much help, it's written by the same people that write NIST
I don't know what the right justification might be given what we learn in
the next few years, but for now, it would be great if a proper consensus
driven standards body will sufficient attention from cryptographers could
provide the kind of algorithm certification that is trustworthy and we can
use as justification for selecting any particular crypto algorithm. I
don't have high hopes for this happening.
I would personally be happy with the 'any DJB algorithm is fine' approach,
but I don't think I could use that as the kind of justification I could
put in chip documentation. SHA3 may win by default unless it's made easy
to justify better alternatives for well defined meanings of the word

@_date: 2014-03-30 19:18:31
@_author: dj@deadhat.com 
@_subject: [Cryptography] A possibility for random device drivers 
What is your goal? Are you suggesting replacing the pool mixing function?
Or the PRNG? Or the entropy extraction? Linux muddies the boundaries, do
you want to sharpen them?
We have many good algorithms and proofs for entropy extraction. The
difficulty is usually in correctly characterizing the entropy source in OS
software that doesn't know what hardware it will be run on. So you can't
know if you are meeting the input assumptions of the extractor proof that
you rely on for correctness.
Pool mixing doesn't seem to have received as much attention from the
mathematicians, but I think it devolves down to a 2 input extractor
problem new_pool_state=f(the pool state, the input entropy).
On PCs, any AES mode gets a boost because of the AES instructions, but
that's not fundamental to the algorithms. Hardware will change over time.
Your ARM based phone is not like your PC when it comes to entropy sources
and hardware crypto support.
PRNGs are PRNGs. Take your pick. The random driver could have PRNGs tuned
for software and PRNGs tunes for instruction level crypto support. Take
your pick.
My hardware oriented approach has been to share an AES for PRNG and
extraction, because the AES is relatively expensive in hardware but it
works well for both uses. HMACs are mess in hardware and hashes have
inconvenient word lengths. What drives software choices is different and
to my mind it's a Hobson's choice.
-CBC-MAC is a good extractor. So are hashes and
HMACs.  Simple GF arithmetic makes a good extractor when you have independent
inputs. These algorithms are certainly faster than any block cipher, given
the key setup requirements, even with AES instructions to hand:
So sure, write a block cipher based random driver replacement, but the
bigger problems lay elsewhere. I think maybe we need a checklist for
random(4) replacement proposals:
1) Does it allow the OS to know the min-entropy of sources?
2) Does it cause /dev/random to not block?
3) Does it cause /dev/urandom to have > 256 bits of initial entropy in its
4) Does it correctly and conservatively account for entropy?
5) Does it prevent load on /dev/random starving /dev/urandom and visa-versa.
6) Can mortals understand why it is good?

@_date: 2014-05-19 13:48:39
@_author: dj@deadhat.com 
@_subject: [Cryptography] Another crypto paper in the wrong place 
This paper by a group of physicists:
Claims that they have a perfect QRNG made out of a phone camera
essentially "because it's quantum".
The physics may be ok (I don't know enough to be able to tell) but they
make some flat out wrong cryptographic claims.
The claim a single input 4:1 ratio extractor can turn their imperfect
output into perfect entropy.
They claim there's a different between quantum noise and electrical noise,
but they describe the measured values as the mean of quantum events, which
is exactly what electrical noise is.
Instead of running a min-entropy estimate algorithm over their data, they
run it through PRNG tests from Dieharder and claim it "passed all tests",
but full entropy bits passed through dieharder will hit a marginal result
on some of the tests most of the time and since they don't describe the
extractor algorithm we have no way of knowing. If they ran it through a
hash, it's going to pass dieharder regardless of the entropy.
They tested 625MBytes of raw data compressed to 156 MBytes. When I have
'close to perfect' raw data, I typically need 100GBytes or so to get a
reasonable error margin on my min-entropy estimates.
I'm doubt physics reviewers are going to be able to judge these
cryptographic claims.
David Johnston

@_date: 2014-05-19 18:22:44
@_author: dj@deadhat.com 
@_subject: [Cryptography] updating a counter 
Hardware AES can include an inline key schedule, so you don't need to
re-key. You just present the new key and the key schedule is computed
inline as you go through each round.
If you run AES with the same key many times, you leave yourself open to
side channel attacks and fault injection attacks. A system or mode which
updates the key on every AES invocation mitigates these attacks.
The SP800-90B AES-CTR-DRBG uses the same key up to 3 times before updating
it, which isn't too shabby, but a simple transformation of the key every
invocation of AES would improve side channel attack resistance.

@_date: 2014-05-19 18:29:11
@_author: dj@deadhat.com 
@_subject: [Cryptography] updating a counter 
I personally like *3 in the GF(2**n) field of your choice (a shift + xor +
xor poly on overflow). Guaranteed maximal length. Simple to work out for
any n. Is less boring than CRCx and doesn't involve the silly bit
reordering and inversion of the CCITT CRCs.

@_date: 2014-05-19 19:13:25
@_author: dj@deadhat.com 
@_subject: [Cryptography] updating a counter 
There is one relation for a block cipher E:
  E(key, x)  != E(key, x+1)
This is not true for a hash. There may be collisions, but they are hard to
find with a secure hash.

@_date: 2014-05-19 19:20:45
@_author: dj@deadhat.com 
@_subject: [Cryptography] Another crypto paper in the wrong place 
For raw data, first I take the basic statistical properties using ent
(bias, SCC, compression, Chi-square TOR etc).
Then I use the min entropy algorithms from SP800-90B. The markov-renye min
entropy test is particularly useful because it's sensitive to bias,
correlation and non stationarity. So comparing the min entropy output
against data generated with the same bias as the measured bias should
closely match if there isn't correlation and deviate a lot if there is.
Using these tests we can judge if the model of the source run in
simulation matches reality. This gives us (or doesn't) confidence in the
model. From the model we must show model_min-entropy >
Once we have that, we can add some conservative margin to the min-entropy
estimate and then know how much conditioning to do.
The smaller the deviation from pure random, the more data you need to
detect it or support the null hypothesis. So crappy data is easier to
characterize. Good data deserves more scrutiny to check for claims of no
correlation or stationarity (which never ever occur in real circuits).

@_date: 2014-05-23 16:12:54
@_author: David Johnston 
@_subject: [Cryptography] New attacks on discrete logs? 
However I have to think about what crypto to adopt to put in mass market chips and they take a long time to put into those chips and those chips persist in the market for a long time. So I have to imagine the worst case that might occur in the next 6 or so years.
a) Was presented two weeks ago. I was there, it was the first presentation of the week and we were all jetlagged and the response was rather muted compared to the breathless press articles. But it's one in a series of findings. They're digging rich seam and I expect the bounds to be expanded.
What I see is a chipping away at the some curves and the DLP in some classes of finite fields in some contexts. GF(2^n) is very popular due to its engineering simplicity. Extend that a few years and there is a risk that the attacks against the DLP might become more general and the small characteristic limit might become less small. I don't consider (b) to be so very unlikely, or at least it's not something to bet billions of dollars of business on.
So that tells me
1) If RSA is fine, use it
2) If you do things in fields (EC etc) use prime fields.
3) Be skeptical of new schemes. The world of small things is throwing up lots of these 'resource constrained' solutions.
 From a 'crypto is fun' perspective, ECC and EDH are great. From a laying down silicon perspective, they're rather scary.

@_date: 2014-05-30 18:07:22
@_author: dj@deadhat.com 
@_subject: [Cryptography] Fork of TrueCrypt 
This is the right place to discuss the deeper crypto aspects.
People might be interested in the Honey Encryption paper presented at
EuroCrypt earlier this month. It claims to provide a way of encrypting the
password such that a brute force attack will hit many plausible but wrong
plaintexts before succeeding.

@_date: 2014-11-07 23:55:37
@_author: dj@deadhat.com 
@_subject: [Cryptography] Vulnerability of RSA vs. DLP to single-bit faults 
CCITT CRC32 inverts the initial state so that C(0) != 0

@_date: 2014-11-09 19:04:17
@_author: dj@deadhat.com 
@_subject: [Cryptography] A TRNG review per day (week?): ATSH204A 
scary level of ignorance for guys implementing such critical security
code.  If the MiB have a copy of every ATSHA204A's seed, they might be
able to PWN a huge number of routers.
hardware random number generator and an internal seed value, which| is
not externally accessible. The internal seed is stored in the EEPROM,
and is normally updated once after every power-up or sleep|wake cycle."
do they need the seed at all?  Why bother to "update" it?  Here's what I
think: they only implemented a short counter to save a few gates.  When
it overflows, the device will start spitting out repeated "random"
The description is correct. It doesn't indicate whether they are doing
that, or something else, but the math of extractor theory says there is no
such thing as a perfect single source extractor without a 'seed' or
'advice', which is itself a full entropy bit string. This describes a
chicken-and-egg situation. How do you get a full entropy seed for your
full entropy extractor if no full entropy extractor exists until the full
entropy seed exists?
But extractor theory has moved on. Dodis et al showed that you can't have
a perfect single source extactor, but you can show computational bounds on
the attacker and that's good enough for crypto.
Barack, Impagliazzo and Wigdersen '06 showed that you can have a multi
input extractor provided the inputs are independent.
There is a constant stream of new findings and I can't keep up with it.
Dodis gave an excellent extractor paper at EuroCrypt this year, which was
a refreshing change from all the MPC and HE stuff, but the content leaked
out of my ears a while back.
That's what I did. SP800-90A says reseed or block at 2^40 generate()s. But
if you're going to reseed long before 2^40, why waste gates on a long
If they've done a single source extractor with advice, then I don't know
what magic wand they used to create that advice.

@_date: 2014-11-19 04:16:23
@_author: dj@deadhat.com 
@_subject: [Cryptography] Where should I start with cryptography? 
I started out as a hardware engineer with a degree in computer science.
Then I moved to ASIC chip design. Then I found myself having to implement
some standard crypto algorithms in an 802.11 chip. Then I ended up having
to attend 802.11i because they were updating the security specifications
and we needed the chip to be up to date.
So I found myself in the middle of crypto people arguing about crypto
security, but my input as an implementer was important because the crypto
people didn't know much about hardware implementation of those algorithms.
Once that was over, I was judged by non crypto people to be a crypto
person because I knew a little bit more than other non crypto people. This
is a dangerous place to be because you will get sucked in and can never
escape. So after several years of implementing crypto and interacting with
real cryptographers, I've learned at lot and cut myself a niche as an
implementer who knows how to talk to crypto people.
So my advice is
1) Study set theory, number theory, group theory, any discrete math,
especially statistics and anything similar. It will prove useful in the
2) Implement crypto algorithms. Find the specs, learn how to read them,
write code for the algorithms and get the vectors to match.
3) Study books on secure implementation. There's more to making algorithms
secure that just getting the algorithm right. There are all sorts of other
issues with state, side channels, attack vectors, out of bounds input,
compilers undermining your code etc. etc.
That's probably enough to start with. Once you're on the other side of
college, all this experience will have put you ahead of the game as an
implementer or started you on the right academic track.
If you want to implement a hash function, you might want to start reading
up on Davies-Meyer construction and sponge constructions. You will get it
very wrong, but as long as you can find someone to tell you how to messed
up, or identify it yourself, that's the learning process.
As a crypto implementer, always assume your are wrong until a lot of
cleverer people that yourself think you are right. You'll still be wrong,
but it'll take longer to be found out. This is how crypto works.

@_date: 2014-11-23 11:37:11
@_author: David Johnston 
@_subject: [Cryptography] Multi Level Perceptons! 
I got this in my email from a spammer of academic-ish papers calling itself Academic.edu.
I think we've already touched on non crypto journals publishing crypto papers with no ability to review them cogently.

@_date: 2014-10-08 18:17:57
@_author: David Johnston 
@_subject: [Cryptography] The world's most secure TRNG 
The USB serial profile isn't a bad one. The drivers will be present in any OS and you can communicate the necessary protocol on top of the serial device. It certainly beats writing a device driver for every OS.
Since the device would be external to the computer (i.e. on the other end of a usb connection) it would be good if the owner of the device could provision the device with a secret key or a keypair which then sends the random data in signed lumps with some monotonic counter. So if something evil got in between the device and the consumer (application or OS kernel or VM or whatever) the consumer could check the data is what came from the device and isn't a replay or spoofed data. It's not perfect, but it addresses a number of attack scenarios.
I think the primary problem with writing software that uses random data is establishing that you have it. Most environments are indistinguishable in that sense. A low entropy platform with lots of interrupts (E.G. a synchronously clocked embedded controller with no IO until after it booted) will still provide data from /dev/random. It's easy to build a platform that has an entropy supply. It's hard to know how to tell that you're on such a platform if you're writing software to run on many platforms.
An external USB source is a good solution if you have an application that can securely identify data sourced from the device, regardless of what the platform in between is. Stick the device in the usb port, run the software and you've bypassed the risks of a low entropy platform that isn't otherwise acting against your best interests. If it's just a noise source, it'll still work, but I wouldn't call it the most defensive design you could create.
FWIW, I've analyzed the raw entropy from hardware entropy sources on several products from several manufacturers and an alarming proportion of them either don't meet their min-entropy criteria or never defined them in the first place. Get your ducks in a row on the min-entropy you guarantee, the design margin, the online testing to ensure it's working and the extraction process and you will be in the upper quartile of RNG design quality.

@_date: 2014-10-21 22:16:13
@_author: dj@deadhat.com 
@_subject: [Cryptography] Simon, Speck and ISO 
Today the NSA proposed that Simon and Speck be added the the ISO JTC1/SC27
approved ciphers spec.
A study period was approved.
But no other non NSA lightweight algorithms have been proposed to ISO,
other than chaskey from Hitachi.
If you have opinions on alternatives for lightweight block vipers, macs,
hashes etc, please let me know so I can try and set the ball rolling with
Simon and speck look OK. But the source is not a little bit tainted.

@_date: 2014-10-22 20:17:59
@_author: dj@deadhat.com 
@_subject: [Cryptography] Simon, Speck and ISO 
The entirely non cryptographic issue goes like this:
If country X doesn't like county Y's backdoored RNG standard, they can
write their own backdoored RNG spec and refuse import of devices not
complying with the local national standard. The WTO will be fine with
However if there's an international standard (E.G. an ISO standard),
approved by the national bodies, then when they try to ban imports, the
WTO will not be fine with it.
It so happens that one well known backdoored RNG spec is copy-and-pasted
into ISO/IEC 18031. So this spec is being opened up again.
So if you're in the business of selling chips around the world that
contain hopefully non-backdoored parts of said specs, you want to be able
to keep selling your products, so you're interested in fixing the steaming
pile that is currently in ISO/IEC 18031.
That's why I'm here in Mexico City.
In passing, the NSA turned up and proposed added Simon and Speck as the
only lightweight block ciphers in ISO.
It not ideal that the only internationally standardized lightweight block
ciphers come directly from the organization that gave us the dual-ec-drbg.
Since I expect to be at the next meeting, I'd be happy to propose some
alternatives with better provenance and I don't know a better place to
find a pithy put down of dodgy standards than right here on this list.

@_date: 2014-10-23 14:44:49
@_author: dj@deadhat.com 
@_subject: [Cryptography] Simon, Speck and ISO 
Alas, AEAD schemes and stream ciphers would be a different document at a
different time.
I would be very happy for this to be in the specs. But since it's my first
meeting in ISO I haven't earned my black belt in ISO standards setting
yet. I have however verified that all the mind-control techniques that
work in the IEEE work just fine in ISO.

@_date: 2014-10-23 17:43:49
@_author: dj@deadhat.com 
@_subject: [Cryptography] Best internet crypto clock 
Remote sources require external inputs.
Transistors have plenty of thermal noise in their gates. It's local, well
understood and can be modeled for min-entropy analysis over all
environmental conditions and attack scenarios.
You just need know how to get at it in a robust way. We published two
different circuits that do this. The one for RdRand that's been repeated
in numerous papers (like this:
 and
the other is this one:
There are many gigabits/s of data you can get out of a transistor with a
high entropy distribution. You can be reasonably confident that the noise
in the transistor gate is the aggregate signal from many quantum events in
the particles out of which it is constructed.
On silicon, the circuits are small. You may be able to do something
similar with discrete components, albeit at lower speed.

@_date: 2014-10-25 20:46:39
@_author: dj@deadhat.com 
@_subject: [Cryptography] Simon, Speck and ISO 
All the published analysis makes them look pretty good on the
security/compute complexity tradeoff. I have nothing to show there is
anything wrong with the algorithms.
Someone at JTC1/SC7 corrected me. The spec to which it is proposed these
be added already has Clefia and PRESENT which are slower and bigger.
I'm concerned with how it looks and concerned that no-one else is jumping
up to offer alternatives with a more transparent background. We certainly
can't criticize anyone for submitting a proposal when no one else is.

@_date: 2014-10-25 20:58:29
@_author: dj@deadhat.com 
@_subject: [Cryptography] Uncorrelated sequence length, 
Since I'm sitting in an airport I've got plenty of time to cogitate.
We are criticizing Linux for not hashing on the way into the pool.
You can hash into the pool, out of the pool or both, or do something
completely different.
Linux hashes on the way out. Why is this better or worse than hashing on
the way in? I don't know.
In terms of Linux architecture it makes sense. The data on the way in
comes in as a function of what the machine provides. Putting a compute
heavy task on this path leads to an uncontrolled amount of effort being
spent, regardless of whether or not /dev/[u]random is used.
Hashing on the way out makes the effort spent a function of the rate which
things call /dev/[u]random, which is an unspent effort if you aren't. This
is sensible, only invoke the cost if you invoke the feature.
Other things are certainly better, like in the BSDs.
I thought everyone who cared had their own version of random.c. I do, but
I'm too lazy to patch the kernel each time I bring up a machine.

@_date: 2014-10-31 19:09:00
@_author: dj@deadhat.com 
@_subject: [Cryptography] Uncorrelated sequence length, 
I'm confused. Wouldn't any CAZAC code meet this definition without being
remotely useful for cryptography.
Presumably the term 'uncorrelated' isn't sufficiently precisely defined. A
optimal CS-PRNG should produce both correlated and uncorrelated outputs
for any definition of which output strings are correlated and which are

@_date: 2015-04-21 06:28:37
@_author: dj@deadhat.com 
@_subject: [Cryptography] Entropy is forever ... 
I don't like that definition much, not because it's wrong, but because
it's far from the worst case.
Wigdersen got it right when he said entropy is a function of the observer.
It is. But entropy is also a function of the generation process.
The adversary may see the output of a seeded PRNG as effectively full
entropy is she is ignorant of the generation process of the seed and the
algorithm of the PRNG. Without the compute power to do the brute force
thing, she's out of luck and can predict no better than chance. The
'distribution' looks uniform to her, but may look far from uniform to a
better informed observer.
min-entropy of the source process, because it's the best informed view and
therefore the worst case metric.
The whole notion of measuring entropy (and min-entropy) from distributions
doesn't sit well with me because you can't actually do it. With a full
entropy source, all distributions from all samplings are equally possible.
With a non stationary source (and all real sources are non stationary),
the distribution is not a simple thing to analyze because it's a function
of when you look. The short term distribution is completely different to
the distribution over the lifetime of the hardware. So which mode of
observation are you going to use? The 'over all time' one, or the one that
matters when the hardware is used?
With a source which has gaussian behavior, those tails go out to infinity.
If the feedback variable (and all real sources have a feedback variable)
follows a gaussian distribution, you can always show that there will be a
window in time when the entropy asymptotically approaches zero, even
though the majority of the time it's 99.99999%
One aspect of RNG design that I've concluded is good practice is to have a
pool structure entropy extractor where the pool size is (1) The same size
as the reseed needs of the PRNG and no bigger and (2) at least big enough
that it's simple to show the 'asymptotically approaches zero' case won't
happen in the lifetime of this universe. 256 is a good size, but it
depends on your entropy source and extraction algorithm. It's not
necessarily the best way, but it's a way that permits simple analysis by
anyone with a basic grasp of statistics.
My views may have been skewed by having to design RNGs for high volume
products for the past few years and also having to evaluate and test other
people's RNGs. There are more ways to get it wrong than a jaded skeptic
like me can thing up of an evening.

@_date: 2015-04-22 04:54:33
@_author: dj@deadhat.com 
@_subject: [Cryptography] Entropy is forever ... 
It's also used widely in cryptographic mathematics, because it has
properties that make it convenient for making proofs.
I've never seen the term surprisal used in any crypto math paper. I don't
remember seeing it all before this email chain, but I have far from
perfect memory.
All the entropy extractors I have implemented have been based on proofs
showing the algorithm converts one or more sources of some min-entropy to
a slower source of data with higher min-entropy. All the CSPRNGs I've
implemented have been based on proofs that a seed with defined min entropy
fed into a given algorithm will yield an output with computation bounds on
the work required by the adversary to determine the state of the PRNG.
I googled the term and it seem to be used in thermodynamics circles. The
math definition doesn't seem to provide any cause to abandon Renye Entropy
as the normal metric.
Sadly NIST is far from consistent in its use of the term entropy and it
has it's own rather creative definition of 'full entropy'. Better called
'Good enough for NIST'. I've submitted a lot of comments to NIST about
this. You can download them and read them if you need a cure for insomnia.

@_date: 2015-04-24 19:06:22
@_author: dj@deadhat.com 
@_subject: [Cryptography] Entropy is forever ... 
Also analog circuit experts. A physicist can do a good job of the min-entropy
math, but a poor job of understanding how to make a production circuit
robust over a production PVT envelope, and manufacturable and testable.
I've been through this with physicists, cryptographers and analog experts
with the entropy source designs for my RNGs. Entropy source design is
necessarily a team effort.
The proofs for most extractors define a required min-entropy (as in Hinf(X))
at their inputs. 0.5 is a common barrier below which you cannot go,
except that multiple input extractors can get you from below 0.5 to
above 0.5. So min-entropy lower bound testing is my primary concern
with data analysis from entropy sources. There also stationarity
testing which is a whole different problem.
Yes. You can't do min-entropy testing online. Online testing must be
testing for a known failure mode. Success looks like any and all
random strings. You can't test for that. You can test for a high
confidence of the data looking like what you would get from a broken
And in practice too. The testing you do against the output of the entropy
source is not the same as the testing against the output of the
conditioner which in turn isn't the same as the testing at the output of a
CSPRNG. These things sit in a chain and you test them all online and
offline in a decent RNG. It's a pretty complex logic design challenge, at
least it seemed that way before I did it, which was a lot of work. I know
how to do it now, so it's not so hard.

@_date: 2015-08-04 19:06:37
@_author: dj@deadhat.com 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
I'm currently in the process of developing a security protocol spec in a
standards group, that will be deployed everywhere.
The reverse seems to be true. There is a desire to do some things new
(specifically to avoid X.509 and NIST curves and make things as brutally
simple as possible), but there is a NISE (Not invented somewhere else)
crowd that calls for external specs we can point to for all crypto things.
This leads down the slippery path to NIST, DSA and X.509.

@_date: 2015-08-05 22:01:59
@_author: David Johnston 
@_subject: [Cryptography] asymmetric attacks on crypto-protocols - the 
In the case of the UPB (unnamed peripheral bus) that I'm referring to, It's not so much creating new crypto algorithms as composing normal algorithms in a system that is simpler than the complex specs like X.509 that lead to complex software that lead to bugs. Or like DSA that is very fragile in the face of biased random numbers. Also avoiding NIST curves with the unexplained constants and trying to use algorithms that aren't thought to be subject to government interference which could cause export problems in international markets.
Many standards (e.g. from IETF, IEEE 802 (before they learned to stop asking the government for help), SP800-90, X.509 etc. ) have proven toxic, either cryptographically, structurally or in terms of implementation complexity. Doing a good job of interoperability standards writing these days involves taking this into account and being very circumspect about what parts of what standards can be considered safe and what parts should be composed in a new fashion that achieves something simpler, or more scalable or more efficient or all three.
Standards are a minefield. We need to learn to tread carefully.

@_date: 2015-08-12 23:55:55
@_author: dj@deadhat.com 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
However in my experience so far, the shakes are the preferred primitives.
When you're getting a room of people in a standards group to first agree
on a minimum security strength (say O(2^128)) then to agree on a hash,
taking recent history into account and looking to the future deployments,
the shakes are the obvious choice and shake128 has already been adopted in
one standards body I'm involved in.

@_date: 2015-08-28 20:57:38
@_author: dj@deadhat.com 
@_subject: [Cryptography] Augmented Reality Encrypted Displays 
It's getting harder &
Roughly 100% of the times I click on a banner ad, it is not because the ad
is hiding. It is because the ad renders and reformats the screen while I'm
aiming for something else. So everything shifts down and my click hits the
I would not be at all surprised to find people doing this deliberately
(delaying the rendering with scripts to resize late) to get more clicks.

@_date: 2015-12-07 06:55:46
@_author: dj@deadhat.com 
@_subject: [Cryptography] Montgomery multiplication bug in OpenSSL? 
We are implementing algorithms of this sort in hardware. The consequences
of bugs of this sort would be much worse than in software. Things can't be
patched. Having concluded that randomized testing wasn't going to get at
the corner cases and that our brains were not big enough to get it right,
we took the track of formal equivalence testing of the logic gates to high
level algorithm descriptions. The intent is to have validation for all
input values.
We have the advantage of knowing what a gate is, much more than we can
know what a compiler does (as per the recent discussions on gcc) so FEV is
good and powerful stuff.
It's maybe not relevant to the problem at hand, but high-level -> Gates
FEV has worked extraordinarily well for us. I can't help thinking it
should be able to work well against assembler for this class of algorithm.

@_date: 2015-12-14 05:05:33
@_author: dj@deadhat.com 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
The last time I paid attention to the Nevada Gaming Commission random
number generation rules, the one thing they did not require was random
numbers. They required statistical distributions that were uniform over
long time spans. I.E. It was within the rules to skew the results one way,
then the other, providing the odds remained the same in the long run.

@_date: 2015-12-14 19:40:13
@_author: dj@deadhat.com 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
I apologize. I'll try to put it back in the bag.
The 'quantum' RNG thing is interesting in that people are actually doing it.
I got to see a quantum RNG product at a conference a few weeks ago. It was
using a module maybe 2cm x 4cm soldered to the board, where magic
happened, but this was on a very large PCIe card, full of other stuff. I
think (a lot of) power conditioning, some CPU to do the post processing,
support logic for the CPU and a PCIe interface chip.
On questioning, it appears the sampled events are not idealized individual
unbiased quantum events, they are 'a few' events. So you get the expected
binomial distribution of summing binary events and then it gets put
through an extractor in the traditional style to get 'more random'
So I failed to see the difference between this and any other noise source
beyond the number of quantum events being summed over to give a typical
noise distribution.
The vendor was happy to tell me it was the world's fastest RNG at (if
remember correctly) 400Mbit/s. Not withstanding all the others that are a
lot faster than that including the ones in every modern desktop PCs.
More interestingly there is a lot of work going on on quantum secure
entropy extractors some even with side-channel goodness. Just google it.
There are lots of papers. This is worthwhile work and I hope we can deploy
it in real products the future.
If you saw my talk at ICMC2015, you'd see I'm happy to lose the DRBGs and
make entropy source and entropy extractors fast enough. They're faster per
watt and faster per unit area of silicon than DRBGs. OHTs are comparably
small. DRBGs are a band aid for slow sources and poor extractors. Today we
have fast sources and good, small, efficient extractors. It's the DRBGs
that still take 10s of thousands of gates.

@_date: 2015-12-14 19:53:30
@_author: dj@deadhat.com 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
While I tend to concur with the assessment that certification is more a
marketing tick box item, that doesn't mean it doesn't add value. I'm not
seeing any other 3rd party auditing capability that is doing a better job.
So certifications are about the only thing purchasing businesses have to
go on.
We paid for CRI to do an audit of ours and it proved invaluable, but
that's hardly a scalable solution or affordable and doesn't yield a
certification. We have a couple of NIST certs for a couple of our
products, but those are CAVS. The entropy assessment side is still ad-hoc,
not a part of CAVS and FIPS 140 requirements run counter to security
When I know I can sign up to the "Good guys who really know what they are
doing looked at this" ceritfication, I'll be happier.
For now, NIST are slowly getting around to establishing reasonable tests.
I have high hopes and low expectations for the predictor tests that I
understand will be in the next draft of SP800-90B. The Markov test got
messed up in the last SP800-90B draft relative to the original algorithm
in the paper. Trying to make it more plug-and-play made it useless as a
analytic tool. You need to shmoo it across different sequence lengths and
group sizes to find the worst case. That parameterizability went away in
the SP800-90B draft.

@_date: 2015-12-16 23:48:28
@_author: dj@deadhat.com 
@_subject: [Cryptography] Photon beam splitters for "true" random number 
Your behaviour depends on your place in the universe.
If RNGs are not your concern. E.G. you work for the DMV and don't
subscribe to crypto mailing lists, you will use the RNG in your PC. It
will serve you well.
If you designed the RNG in your PC (that's me), you will use the RNG in
your PC. You designed it to provide useful random numbers for users,
regardless of their experience. It will serve me well.
If you are in the business of writing operating systems or crypto
libraries, you might consider it your job to defend your users from evil
untrustable RNGs and so find a way to combine multiple sources, in the
hope that at least one of them is ok. This will work ok providing your
software gets deployed in places where there is at least one good RNG. See
the Mining Your Ps and Qs paper for a classic example of where this
assumption didn't hold.
If you are involved in crypto in some way and know why you need a good
source of entropy and know how to post process the output into
cryptographically useful data and know how to wield a soldering iron, you
might consider it your interests to have an RNG in front of you with an
open design so you can verify for yourself that it does what it is meant
to do. You might check on your preferred mailing list to see what is
available and what is what.
So what's wrong with the noise multiplier thing? Nothing much for feeding
your own desire to have your own physical RNG instance you control and
understand. But it isn't a general solution that will end up in everyone's
PC and phones.
What's wrong with the RNG in people's PCs and phones? The people who know
crypto and didn't design it aren't in a position to convince themselves
it's ok. So they seek out alternatives that make them happy.
So pick the solution that makes you happy.
I built one myself a few weeks ago based on a paper I read about an
entropy source design that flipped a ring oscillator between local
oscillation of each cell with a global loop. It didn't work. At least I
showed the claims of the paper were untrue in the general case. I won't be
selling it online.

@_date: 2015-12-25 08:01:52
@_author: dj@deadhat.com 
@_subject: [Cryptography] Juniper & Dual_EC_DRBG 
I know nothing about the Juniper system, but I know a thing or two about
how RNGs are used in real systems. An RNG in an offload accelerator
generating per packet random numbers is typically going to be fast.
Therefore someone implementing the accelerator will have considered the
options and dismissed the Dual-EC-DRBG on performance grounds, without
ever having to visit the other issues. For hardware performance, the
CTR-DRBG is one for which you can make very fast implementations.
But there's usually a layer of software somewhere doing session
establishment with public key stuff and this would use whatever RNG is
available from the system it is running on. Typically the OS or RdRand or
an RNG hanging off a bus in an ASIC or some combination thereof.
Subverting either the accelerator RNG or the system RNG would do the job I
presume, albeit with a different attack methodology required for each.

@_date: 2015-02-17 22:53:47
@_author: David Johnston 
@_subject: [Cryptography] trojans in the firmware 
I've designed many embedded computers using directly memory mapped flash devices. It was how it was always done before people started trying to make them look like hard disks at the interface.
There is a direct analogue with hard disks of old, that the OS directly controlled. The OS knew where the tracks and sectors were and where the head was and could therefore do things like head trajectory optimization. When disks got smart and started trying to look like a perfect array of sectors while hiding the physical layout, the opportunity for the OS to do the right thing was removed. So we ended up with ludicrous things like cache memory in the disk drive, rather than close to the CPU, because the CPU doesn't have the information necessary to manage the hardware.
So the management software in flash disks is a lot more complex than the incremental write and leveling algorithms that were commonplace with directly mapped flash. Not because it needs to be, but because it is trying to present a model at the interface that is very different to the underlying medium.
You can (or you certainly could in the past) get PCI cards with memory mapped flash on them, if you aren't in a situation to make you own circuit boards.
Flash files systems were commonplace. They are still there in the Linux source code and I assume they are used in many products.

@_date: 2015-02-22 23:41:34
@_author: dj@deadhat.com 
@_subject: [Cryptography] A better random number generator... 
Be careful. LCGs and PCGs and algorithms like xorshift are not
cryptographically secure. The goal is to have good statistical properties
at a minimum algorithmic cost. You can see in the talk that speed is one
property they aim for. However for more speed, parallelizable crypto modes
are better because you can throw optimized instructions like AES-NI at
them, or arbitrary amounts of silicon. But for crypto, you need secure
entropy extraction, optionally followed by a secure PRNG.

@_date: 2015-01-26 19:18:53
@_author: dj@deadhat.com 
@_subject: [Cryptography] random numbers on virtual machines? 
I posted my reply there.
Executive summary, have the hardware and an up to date RNGd.

@_date: 2015-01-26 19:23:42
@_author: dj@deadhat.com 
@_subject: [Cryptography] 2008 revision of Bitcoin whitepaper 
I'm up for it. I'm not anonymous, neither is my web site. Send 'em to me
by the split, slow, anonymous path and I'll assemble and host.

@_date: 2015-01-26 23:07:58
@_author: David Johnston 
@_subject: [Cryptography] The Crypto Pi 
While 'pure, virgin, bit for bit entropy, right from a mathematically fine entropy extrator hooked up to a fast and highly entropic entropy source' might sound nice, it is almost always better to run it through a computationally secure PRNG, since the pure source is necessarily speed limited to be less than the physical entropy source entropy rate.
The PRNG converts your min-entropy assurance into assurances of computational bounds on the adversary, which is news you can use and the PRNG can generally be designed to address side channel issues in ways the entropy source cannot.
That's why RdSeed is called RdSeed and not MoBetterEntropy.

@_date: 2015-01-28 22:09:34
@_author: dj@deadhat.com 
@_subject: [Cryptography] traffic analysis 
With a suitable shared secret at both ends arranged by the key agreement
of your choice, a pseudorandom sequence can be xored on top of the wire
and de-xored at the other end (assuming this is compatible with the phy).
If you have to meet framing on the wire, then you need filler packets and
you can do something similar within the data fields, with the filler
packets decoding to "This isn't a real packet" with packet sizes following
the usual distribution.
It can be done, it just won't be done by your vendor.

@_date: 2015-01-28 17:00:43
@_author: David Johnston 
@_subject: [Cryptography] traffic analysis 
Indistinguishable by anyone not in possession of the shared secret.

@_date: 2015-01-29 21:51:16
@_author: David Johnston 
@_subject: [Cryptography] 2008 revision of Bitcoin whitepaper 
I finally have the first issued version of the bitcoin paper from For your amusement and to enable you to download it from a website with an SSL certificate signed by one of those SSL companies (that sadly used SHA1), I've made it a page on our yarn store web site.
 >sha256sum bitcoin.pdf
 >427c63b364c6db914cf23072a09ffd53ee078397b7c6ab2d604e12865a982faa

@_date: 2015-07-06 10:23:39
@_author: David Johnston 
@_subject: [Cryptography] Best AES candidate brokenby the way that 
It seems that what is optimal for software speed of ECC is not optimal for side-channel mitigation in hardware ECC implementations. This is at the core of 'which curve to use' arguments I've been in recently.

@_date: 2015-07-23 14:07:48
@_author: dj@deadhat.com 
@_subject: [Cryptography] Whitening Algorithm 
Yes. There is.
You are posting in a cryptography oriented mailing list. So I assume you
want the output not just to be spectrally white, but to meet cryptographic
needs and so be indistinguishable from random and unpredictable.
You need to understand that any process that meets your needs will
necessarily slow the output significantly. You have partially entropic
data. You are trying to make it more entropic. You would like to make it
fully entropic but you cannot. However you can get close enough to get
random numbers useful for cryptography.
What you want it an entropy extractor. Extractor theory is well developed.
You have choices. You have a single source, so multiple input extractors
are not an option. Seeded extractors are not an option (chicken and egg
problem - where does the seed come from?). So you want a single input
extractor. You could use for example the algorithms (called conditioners)
in SP800-90B. They involve hashing or MACing X data bits down to Y data
bits, where Y < X and the min-entropy in X > 2Y. So the reduction in data
volume is greater than a factor of 2, since you are starting from a lower
than perfect min-entropy.
A Von Neumann whitener is not an entropy extractor, or at least not one
that is useful given the data that you get from physical sources. If you
put significantly serially correlated data into a Von Neumann whitener,
you will get unbiased data with lower min-entropy than the data you put
in. It is an unbiaser and only an unbiaser.
Don't feel bad - you are in good company. The notion that Von Neumann
whiteners are entropy extractors persists, even amongst RNG designing
academics who should know better. This:
was presented on Tuesday at the NIST Lightweight Crypto Workshop and it
recommends applying a Von Neumann whitener over SRAM start up values. This
is wrong. It will not achieve the intended effect.
This algorithm does not compress data. Therefore it is not an entropy
extractor. You are muddling the data, but in a deterministic process that
is reversible. It looks more random on the output, but has no additional
So you go ahead an implement say the CBC conditioner from SP800-90B :
AES-CBC-MAC(k=aes(1,0), bytes[0:48]) -> output_bytes[0:16] and find that
you have fully entropic data (this assume > 50% entropy in the source
data. Put more data in if you have less source entropy per bit.
Now you have a slower source. You want it faster. So you use the
close-to-full-entropy value out of the conditioner to seed a
cryptographically secure pseudo random number generator, which will
generate useable random number as fast as your software can run. There are
many to choose from. Try the SP800-90A CTR-DRBG, or cha-cha-cha or one of
many others out there.

@_date: 2015-07-27 05:46:49
@_author: dj@deadhat.com 
@_subject: [Cryptography] Whitening Algorithm 
These are different things. AES-CBC-MAC (when it isn't being used as a
MAC) is an entropy extractor. This was proved in this paper:
 .
Cha Cha Cha is a CSPRNG. Not an extractor.
Blake is a hash function. A hash function can be used for entropy
extraction, but its properties as an extractor (as far as I know) haven't
been proven in the same way a MAC has.
Encryption is not extraction. An extractor always gives less out than went
Rather than 'option' think all-of-the-above. The standard chain of events
1) Sense bits of partially random data from the environment.
2) Pass them through an entropy extractor to get 'full entropy' random
data (for your preferred definition of full entropy.
3) Use this data as seeds to a CSPRNG, for performance reasons.
In terms of what you get at each stage..
1) Raw data:  Bad quality - Typically Slow (but metastable sources run at
2) Extractor Output: Best quality - even slower.
3) Output of CSPRNG seeded by extractor - Good enough for crypto - much
The third step is optional, and possibly a bad idea if you expect it's
output to be used to generate seeds for a downstream CSPRNG.
Entropy source design is interesting in that most the mechanisms that
people think make good entropy sources are in fact completely useless.
Avalanche noise is an example. You want an entropy source to:
1) Be unconditionally stable over the lifetime of the device
2) Be close enough to statistically stationary that it doesn't matter much
3) Be able to be mathematically modeled in a way that worst case
min-entropy bounds can be derived.
4) Be manufacturable (presuming you're making more than one of them.
5) Not need user intervention to make it work, since that yields an attack
6) Not have pathological modes (like ring oscillators do).
In my corner of the universe, we like the noise driven resolution of
metastable circuits. Metastability always resolves and with proper design,
always happens.
If you can identify the failure modes of your circuit with another
circuit, then you are in business, see below..
Online health testing is the means to salvation. You cannot look at data,
test it and know whether or not it is random. In random data, all
sequences are equally likely. So the answer of such a test is always 'yes,
it looks like random data could look'.
However you should know what the output of a physical source should look
like since it will not be perfectly random. It will have some property,
like a gaussian distribution, or serial correlation, or suppression of
certain sequences relative to others. When it is not working in some way
you can expect it to do something else that is clearly distinguishable. So
online testing is useful in that it can be used to identify that data is
statistically likely to be due to a failure. If you're doing a circuit,
SPOF and DPOF analysis is useful to find out what the non trivial modes
might be.
So you can understand your circuit and have mathematical and simulation
models that tell you that if you construct the circuit right, it will be
entropic. You can have online tests that tell you whether or not you
constructed it right.
Your test, whatever it is, is going to be spitting the universe of bit
strings from your device into 'good' strings and 'bad' strings. Assuming
the output to be random, the bad strings are not in fact bad, just more
likely in a broken instantiation.
So you should create your tests to be pessimistic, with a high false
positive rate, tagging data that looks bad as being bad - to suppress the
false-negative rate. But use a long term count of the bad/good ratio to
tell you if the source is broken and use the bad/good tagging to tell you
whether or not to trust the current blob of data that has been tested. By
'trust' I mean count its entropy in the input to the extractor. If it's
bad, still throw it in, but have the extractor not complete a seed until
it has had enough good samples in. This way you don't throw away the
entropy or diminish the entropy into the extractor, you modulate your
level of extraction conservatism based on the short term data and use the
long term data to decide if it's broke. This yields the right behavior in
a system - no numbers out when it's broke, numbers out when it's working,
an instantaneous response to an instantaneous failure and no user
detectable response to transient false positives when it is working.
This is the fashion in which the design of entropy source, entropy
extractor and online health test all go together to make a working system.
They are not independent things. They need to do the right thing when
bolted together. Other people may have better ideas for composing the
parts of an RNG, but that's how I do it.
Yes, see above.
Nooooo!! A Von Neumann unbiaser is not fit for use as an entropy extractor.
Algorithmically cheap extractors require multiple independent sources :
Von Neumann Extractors. Just kill them already. Yuval Peres whiteners too.
Single input extractors require cryptographic functions (hashes, MACs
etc.) which tend to require more computation resources.
Would a simple monobit test be a good (read: cheap) litmus
No. There's no reason to think a monobit test can detect all the failure
modes of the source. I've been arguing for model-equivalence checking as
being the sane way to do online testing for a long time. This recent paper
is saying something similar but goes on to describe offline tests, rather
than online:  and
based on discussions at the NIST lightweight crypto workshop last week, I
fully expect this to become the basis for the entropy estimation
algorithms in SP800-90B, replacing or adding to the non-IID set that is in
the current draft.
Something less sophisticated is needed for online testing. I'm giving a
talk that will cover this at ICMC 2015. In the RNG circuits I design, we
count the frequency of patterns in a sliding window and compare to
expected statistical bounds. This is unreasonably effective, and cheap in
hardware. E.G.
That's not how I would do it. Test and tag the data going into the
extractor so you know whether or not to count its entropy.
Your diagram on the right is closer to correct. However what are you doing
with the result of the test? If you are dropping data on the floor because
it failed a test, then you have a data changing process and it will reduce
the entropy. This is bad. The behavior of the extractor should change
based on the result of the test in the way I described above, by mixing in
more data into the next seed when the data isn't good.
Good luck! Easy to read is good. I see it that library has SHA256 in HMAC
mode. That would make a useable extractor if coupled to a quantified
source and a fit-for-use online health test.

@_date: 2015-06-02 20:02:08
@_author: dj@deadhat.com 
@_subject: [Cryptography] open questions in secure protocol design? 
I may or may not have been a 'reasonably good engineer' in that window of
time, but back then I was actively opposing the use of ECC while my crypto
colleagues were trying to deploy it (see PKMv2 for an example of this
written down - 2048 bit RSA is what the spec requires) . My gut feeling
was that while someone may know something, there certainly wasn't a broad
understanding of the risks of ECC. "It's smaller, more efficient and
better" didn't cut it. The sharp edges have to be there somewhere.
Things are different today. Many sharp edges have been exposed. We can
express several good reasons for judging one curve against another and ECC
against RSA. I don't feed bad about deploying ECC today. There are always
risks, but they are no worse for the right ECC curves than for RSA. The
implementation risks and costs weight heavily against RSA today.

@_date: 2015-06-04 21:26:01
@_author: dj@deadhat.com 
@_subject: [Cryptography] Introducing Chicha stream cipher 
Thing What makes you think Chi-Square T.O.R. is the right algorithm to test for
randomness? I don't see the logic. It's not even easy to judge the output
of that test because what you are looking for is the uniformity of the
metric over many tests. No individual test tells you anything, nor is it
sensitive to correlation between the data.
In addition to base statistics like you get out of ent, I would look at
things like the Markov-Renye min entropy test and run it over a few
megabytes of data with group lengths > 7. It's described in draft
SP800-90B. Also distinguishability tests (dieharder, SP800-22 etc), if you
can get enough data.
Thing It reminds of meta-AES which is a lunatic cipher I dreamt up that has
similar goals. Implement AES, but replace the key expansion algorithm with
a reversible mode of AES that over 10 rounds is strongly indistinguishable
from random, specifically AES-CTR. Inefficient as heck, but it works. If
you want 256 bit keys, just make key[128:255] = the CTR IV rather than
using the nasty 256 bit key schedule of normal-AES. For this I probably
deserve shooting.

@_date: 2015-05-12 01:13:02
@_author: dj@deadhat.com 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
The DJB curves are finding traction elsewhere and will be adopted by other
standards bodies that I am involved in (because in part, I'm pushing for
them). The efficiency, simplicity of implementation and acceptance by the
crypto community of these algorithms make for strong arguments in
standards contexts.
NIST's primary problem with ECC are the NIST curves. If they can bring
themselves to move on to curves with better provenance, then progress can
be made with NIST. Otherwise the NIST curves will become obsolete and
superseded by other standards bodies.
There is also the Lightweight Crypto Workshop at NIST. This heavily
overlaps with the ECC thing, because the right options for ECC curves are
also the right options for lightweight crypto.
I'm attending the lightweight Crypto Workshop, but not the ECC Workshop. I
don't have bandwidth for both.
I spoke with Lily Chen of NIST last week (at SC27) about the
Lighweight/ECC overlap and the need for them to move to better curves.
They know what I think.

@_date: 2015-05-12 08:19:16
@_author: dj@deadhat.com 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
Simon and speck have had quite a few cryptanalyses published and time has
passed. Simon is a lovely thing to implement in hardware. It goes up to
256,128 key and data size as is more efficient than AES in that
configuration by about a factor of 3 in hardware for the same performance.
If you don't read ISO specs for amusement (I can't blame you, they charge
money) PRESENT and CLEFIA are approved lightweight ciphers in ISO. But
they aren't as lightweight as Simon.
So all other things being equal, it seems to have something over PRESENT,
CLEFIA and AES. But all other things are not equal. The parentage is
unfortunate, because as an implementor, I really want Simon to make it
into the standards space, enabling us to deploy it in products where
standards compliance is mandatory.
My request to Doug Shors (who was at SC27 last week promoting Simon and
Speck for WG2) was - Add the missing 256 bit block size. It's the same
Achilles heel that AES has. The maximum block size is too small. The idea
that there is a need for lightweight crypto has poisoned the design of
lightweight ciphers. They are efficient ciphers, whether with small or big
key sizes or small or big block sizes. The more tasteful ones are smoothly
scalable in terms of width, unrolling and pipelining. But when they stop
at 64 bit block sizes or 128 bit key sizes, they limit the deployability
and performance limits.

@_date: 2015-05-13 00:00:24
@_author: dj@deadhat.com 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
To paraphrase Bowman: "Oh my God. It's full of integer adders!"
Integer adders don't pass the sniff test for lightweight hardware.
Alas, the world isn't just the internet and smart cards. We are throwing
crypto on silicon as fast as we can to address the many threats to
computer hardware. No one block size is correct.

@_date: 2015-05-13 20:46:07
@_author: dj@deadhat.com 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
I'm coming from a place where many data sizes are fixed. I.E. Data fields
on chip. But they might be 256 or 512 bits instead of 64 or 128. So my
world view may be different.
The published lightweight ciphers are interesting in that their real
design feature is they disperse and non-linearize more per gate than say
AES. In hardware this lets you do more with fewer gates. It doesn't
dictate the key or block size of the cipher at all. Yes under the
lightweight title, they self limit themselves to small block and key
I would like such a thing to exist. Do you have an algorithm handy? The
closest thing I can think of is format preserving encryption, like
Rogaway's Sometimes Recurse Shuffle. That can work on arbitrary string
That would be nice, but hardware implementation parameters are usually
tied to the size required for the application. So probably more of a
software thing, which isn't my gig.
Orthogonally I have been thinking of ciphers taking the number of rounds
as a parameter. Then use that in protocol negotiation. Algorithm gets
weak, increase the rounds. It beats undeleteable cipher options.
I spoke with a block cipher designer about this and his argument against
was that if you can run the same data and key through with a different
number of rounds, it's trivial to break. However I see this as just
another constraint, like 'never use the same key and IV twice'. Never use
the same key and iteration count twice.

@_date: 2015-05-14 21:54:35
@_author: dj@deadhat.com 
@_subject: [Cryptography] [cryptography] NIST Workshop on Elliptic Curve 
I found the DJB arguments to be solid and believable. That the NIST
Weierstrass curves are so difficult to implement correctly while being so
easy to implement in a way that meets the test vectors, it would lead to
many weak implementations. So if the NSA were good with Montgomery or
Edwards or  curve crypto and left the Weierstrass curves
for the great unwashed, it would make complete sense.

@_date: 2015-05-28 18:31:29
@_author: dj@deadhat.com 
@_subject: [Cryptography] open questions in secure protocol design? 
That's a bit close to home right now. I'm working on defining just such a
protocol in a standrd. I'm pushing for (It's a multi company thing)
'algorithm migration' in place of algorithm agility or 1TCS.
Yes it is because algorithm agility is a black hole into which complexity
goes in and from which Hawking-bugs comes back out. I don't want that in
the spec. I don't want the complexities and cost of TLS and X.509 and all
that goes with it.
1TCS is a problem for reasons that have already been discussed.
The context is hardware devices, usually low-cost, with an undefined but
certainly finite lifetime.
Algorithm migration in this context means:
1) We start with algorithm version 1. It is a suite of algorithms chosen
wisely to be considered good for several years. Where 'several' is long
compared with the typical lifespan of the devices.
2) New algorithms are adopted by the standards body in sequence (1,2,3..)
when there is a reason to (example sha1 looked shaky years before it
failed). The algorithm list is in time order. There's no branching or a
menu to choose from or negotiate.
3) New devices implement the current algorithm version and the next
algorithm version if it exists.
4) When the new algorithm version is widely deployed, policy is updated to
deprecate the old algorithm version.
So algorithm migration is a slow migration from one cipher-suite to the
next, when supported by deployed hardware. No a run time negotiation
between many cipher-suites. The cadence may be 1 decade.
If this turns out to be bad idea, people will know who to blame. But I'm
pretty sure algorithm agility and 1TCS is a bad idea.

@_date: 2015-05-28 19:16:57
@_author: dj@deadhat.com 
@_subject: [Cryptography] open questions in secure protocol design? 
As per my other post, this is exactly what I'm pushing for in a real
standard coming your way soon.

@_date: 2015-05-31 10:40:26
@_author: David Johnston 
@_subject: [Cryptography] open questions in secure protocol design? 
The problem here is that the crypto implementation will be baked into hardware devices. Responding to sudden crypto failure in this context seems like a hard problem and I don't claim to have a solution. However we don't seem to see that happen much with mainstream algorithms. It's more like SHA-1 or the AES 256 key schedule. We have time to respond.
I do actually claim to have a solution, but I don't think it would fly in terms of getting buy in by manufacturers. Namely parameterizing algorithms with parameters we can negotiate on the fly. Like a minimum_extra_rounds parameter. It's easy to bake that into an implementation and it's easy to negotiate. It would address a certain class of crypto failures, but not all. It also gives you an opportunity to shoot yourself in the foot. Never use the same key with different numbers of rounds.
Yes you can rollback - but this is the thing I'm least happy with. I don't want to run into TLS's downgrade nightmare.
Yes. There will be no experts present.
This presupposes we can define a cluster of algorithms of which not all will fail. Algorithms fail all the time. There's usually a few new attacks at each IACR conference. But the suite of well burnished algorithms is small and many are tainted by coming from governments, which impedes their acceptance in international markets.
'Break' in this case would primarily means fraudulently authenticating as something that is authorized. My sensibilities lead me to think the risk from the implementation complexity of these schemes is worse than the risk of the chosen algorithm not outliving the device.
Yes. I expect specs, TA and source code will be out in the public fairly soon, so we can beat it to a pulp in the grand tradition of this forum.

@_date: 2015-11-02 18:54:33
@_author: dj@deadhat.com 
@_subject: [Cryptography] How programming language design can help us 
This is why I'm very partial to python's ability to easily handle
arithmetic over rationals. For the probabilistic sums I do a lot of, that
have asymptotes all over the place, it's nice not to have to look over
your shoulder for FP problems.

@_date: 2015-10-28 17:42:53
@_author: dj@deadhat.com 
@_subject: [Cryptography] [FORGED] The Energy Budget for Wireless Security 
When we are designing protocols people actually use in products, we are
very much aware of the cost of the individual parts in compute time,
memory, storage, silicon area and user experience. It's good to have the
implementers involved.
A good goal is to minimize the number of underlaying algorithms, so the
pain of implementing them in hardware is reduced. E.G. Try and use AES for
ciphers, MACs, KDFs etc. Try and use one curve for the signing and key
agreement. Try and align the cryptographic strength so you are
implementing O(2^512) strength key agreement against an O(2^128) link
If XML, ASN.1 or X.509 is involved, then that's outside any spec I'm
willing to write these days.

@_date: 2015-09-26 06:59:06
@_author: dj@deadhat.com 
@_subject: [Cryptography] VW/EPA tests as crypto protocols ? 
Indeed. I seem to deliver a talk on this topic about once a week. You have
to look at the entropy source process to have a hope of determine if it's
actually entropic and determine if it meets the input requirements of the
On this I think world is a little further along, than you suggest. In
hardware, formal equivalence verification and formal assertion
verification is routine. If you push the process a little the tools exist
to support verifying a high level language implementation that is proven
formally equivalent to a high level algorithm model and which is then
synthesized down to gates and formal equivalence checking is done over
that step. So you can show your hardware does only what the model
describes and nothing more. The limits are really that this tends to be
done over algorithms, rather than bigger system compositions. SPIN has
been around for decades for making certain assertions over protocols but
I've yet to see systems that can make formal correctness assertions about
say a general purpose CPU or a bus network outside of university research.
I've seen this go too far, where randomized testing is all that is done
and so the corner cases you can reach with directed testing are never
handled. Randomized testing is good, especially in complex systems, but
it's no substitute for thinking about how you can establish the system
behavior over larger sets than can be tested in a brute-force manner. You
have to build a randomized test environment, but then every directed test
is more design work. It's easy to stop adding directed tests because it's
expensive and you might thing the randomized testing is enough.
Like I said above, we've moved on in hardware. Tools are available to test
over the entire input space that don't require the logic designer to have
the specialist knowledge that used to be typical of formally proving
implementation properties. It would be good if they were more widely
deployed, but they typically are deployed over security sensitive things
like I/O and crypto. It's no big surprise to me that the nature of system
security failures in chips has moved to places where the composition of
disparate functions can lead to vulnerabilities. Subsystems which are
individually secure become insecure when brought together. I'm aware of a
lot of work directed at addressing those problems. I predict many PhDs
will be written on this topic.
None of this stops an organization cheating though. It can make life
difficult for a cheater in an honest organization, but a dishonest
organization is free to cheat.
I'm keeping my skeptic's hat on. Until someone shows us the code, I have
seen nothing that can reconcile the journalists claims with what I know
about how the testing works. It's possible that the necessary testing
simply was not done, otherwise it would have found what the researchers
found. But I would be surprised if that's what happened. What did the
researchers do different to the government tests? Nothing I suspect. The
car doesn't know when there's a sensor inserted in the exhaust pipe. I
seems like the government outsourced this testing to the researchers, so
maybe it wasn't being done beforehand, or palms were being greased or
incompetence led to the tests being invalid. An enterprising journalist
might want to take a look as the normal government testing procedures and
ask what differs from what the researchers published recently.
Given my past experience in vehicle technology, I would argue that this is
how car manufacturers have to meet the testing requirements. Cars are
designed to work over a set of continuous variables and in general the
test requirement are point tests. It's simply a fact that the car has to
do something at all the points between the tested points. They have to do
something different to what it done at the tested points and so the
difference between 'cheating' and 'normal behavior' is qualitative, not
quantitative. You might be able to formulate testing schemes to make
cheating harder, but that is not at all an aspect of the current testing

@_date: 2016-04-25 22:29:23
@_author: David Johnston 
@_subject: [Cryptography] Current state of WPA2 security for IoT access ? 
>
 > > >A sysadmin told me within the last week that WPA2 was easily broken >via Aircrack.  > >I wasn't aware of this; is this really true?
PSK is vulnerable to offline dictionary attack. This was known, discussed and well understood by the participants at the 802.11i meeting when PSK was adopted.
 >The overall question I'm interested in has to do with IoT wifi access. If I try to hide a WPA2 access password in an IoT device, someone can easily steal the (outdoor) IoT device & "waterboard" it until it gives up the WPA2 password.
Yes. Physical access matters. I know what to do about it, but it requires silicon area, relatively new technology and an EAP method instead of PSK.
Per device revocable credentials would be a practical and deployable approach, albeit ineffective if your adversary is stealthy or knows when you're out at work.
If you use PSK, use a secure password, or the perp won't even need physical access.
 >So what is the current recommendation w.r.t. IoT devices accessing WPA2 wireless routers?
Try not to have it matter when it's stolen. Limit the blast zone of the loss and use per device passwords that are resistant to dictionary attack.

@_date: 2016-04-26 20:09:25
@_author: David Johnston 
@_subject: [Cryptography] Current state of WPA2 security for IoT access ? 
802.1X is a component of WPA2. WPA2 includes the 802.11 transport for EaPOL, EAP, the key agreement protocol and the CCM link cipher.
PSK is an option for those not wanting to deploy EAP, RADIUS and all that malarky (I.E. everybody except us). 802.1X is included by reference. It exists independently of WPA[n] or 802.11.
If I remember right (since I am one of the many authors), it would be called an RSN (Robust Secure Network) to distinguish it from WPA that was part of a TSN (A transitional secure network) because the TSN had TKIP and indeed its security was transitional.
WPA and WPA2 are marketing names from the WiFi alliance, rather than terms in the 802.11 specification.
PEAP on its own is not an 802.11 security protocol. It doesn't have the link cipher, key agreement or bindings to layer 2.

@_date: 2016-04-29 18:21:37
@_author: dj@deadhat.com 
@_subject: [Cryptography] USB 3.0 authentication 
It's X.509 certs and a 1 way authentication exchange. No link cipher, no
key agreement. All crypto is aimed at 128 bit security (so 256 bit curves
and hashes). There's P-256 and SHA-3 (or SHA-256 it's gone back and
This may look odd. There are reasons.
This spec is the first part. It's addressing the authenticity of PD (power
delivery) devices by checking that they have been provisioned with certs
under a root controlled by the certification body. These devices may not
have USB data capability. The PD wires carry a low speed protocol to
negotiate volts and amps. The 'problem' is counterfeit chargers and
defective cables that can and do damage expensive computers and phones.
What it is not doing is protecting any data. That is part 2 and it hasn't
been written yet. Part 2 will nominally have a mutual authentication, key
agreement, link cipher and is intended to protect against a variety of USB
threats we are familiar with - MITM key loggers, driver abuse, car park
flash attacks etc.
The use of X.509 and NIST curves was not my idea, but you can't always get
what you want.

@_date: 2016-04-30 02:19:45
@_author: dj@deadhat.com 
@_subject: [Cryptography] USB 3.0 authentication 
It seems that many of the silicon vendors who supply USB interface chips
are also in the business of smart card, identity card and payment card
businesses. So it's possible that they are competent at hiding secrets in
a chip.
I think the major defense is having the cost of the attack be more
expensive than buying the authentic goods. The PD authentication protocol
is aimed at cheap devices, so that may be the case.
There is also the possibility of revoking intermediate certs, also I'm a
big sceptic on cert revocation.

@_date: 2016-08-06 18:13:17
@_author: dj@deadhat.com 
@_subject: [Cryptography] Generating random values in a particular range 
My reading of the patent is that it says - Instead of pulling a random
number until we find one where H(rn)<q, we pull one random number, seed a
PRNG with it and pull from the PRNG until we find one where  H(rn)<q.
The PRNG they propose is CTR mode, but using a hash instead of a block
This is no different to pulling from a RNG that presents a PRNG seeded
from an entropy source until you get one where H(rn) < q. It's just moving
the boundary.

@_date: 2016-08-07 19:37:48
@_author: dj@deadhat.com 
@_subject: [Cryptography] Generating random values in a particular range 
I note for no particular reason that re-hashing the last hash until you
get a number less than q, rather than adding 1 to the original value and
hashing is cheaper by the cost of one addition.

@_date: 2016-08-10 16:56:09
@_author: dj@deadhat.com 
@_subject: [Cryptography] BBC to deploy detection vans to snoop on 
I have been on the pointy end of BBC letters telling me I haven't been
paying the license fee when I should have.  I didn't have a television and
they made the determination on no more information than the fact I had not
been paying for what I didn't have. No vans were involved.
I checked and in fact it was not my problem to pay up or deny. I could
ignore the letters and it was up to them to show I had a TV. There was a
period of about 4 years where I got a letter every 6 weeks of so. Then I
left the county.

@_date: 2016-08-29 17:33:35
@_author: dj@deadhat.com 
@_subject: [Cryptography] ORWL - The First Open Source, 
People build CPUs in FPGAs all the time.
The practicality of building a secure CPU in an FPGA is of course
dependent on first understanding what is meant by secure.

@_date: 2016-02-24 04:52:24
@_author: dj@deadhat.com 
@_subject: [Cryptography] "On-chip random key generation done using carbon 
It's a PUF. It walks like a PUF, quacks like a PUF and the paper even
calls it a PUF. It's a PUF.
Maybe vulnerable to probing attacks, which matters or doesn't matter
depending on the threats you are choosing to defend against. But those
pads look kind of big. You could drill down to them without disturbing the
nanotubes between the pads.

@_date: 2016-01-01 20:20:33
@_author: dj@deadhat.com 
@_subject: [Cryptography] Formal definition of lightweight crypto 
I've designed circuits using algorithms claiming to be lightweight crypto
and there seems to be two common properties of lightweight crypto
algorithms (1) The smallest instantiations are less secure, using shorter
keys and/or shorter block sizes. and (2) they are more scalable, since the
inner round functions are very small, so there is a lot more unrolling
flexibility, so you can build small slow ones and big fast ones and many
points in between those extremes.
The consensus at the NIST lightweight crypto conference last year was that
we shouldn't compromise on security. So the real important feature of
algorithms is efficiency and scalability and lightweight algorithms
generally meet those criteria. Simon for instance turns out to be 3X more
efficient than AES at the same strength and performance so it is a much
better algorithm overall than AES.

@_date: 2016-01-02 22:20:21
@_author: dj@deadhat.com 
@_subject: [Cryptography] Any Electrical Engineers here who know about 
The noise spectrum of any bitstream XORed with a fully random (for your
preferred definition of fully random) bitstream will be uniform as the
quantity of data tends to infinity.
Compared to classical EE filtering theory, it isn't a case of
filter(signal+noise). It's signal convoluted with noise. So a frequency
selective filter would not be able to recover the signal from the noise. A
deconvolution process would require that you can regenerate the
pre-convolution random noise, which is where the need for the uniqueness
and unpredictability properties of the random sequence become important.

@_date: 2016-01-05 19:08:39
@_author: dj@deadhat.com 
@_subject: [Cryptography] Lighweight goes Heavyweight: Simon 256, 256 
It's the new year, so 'tis the season to roll-your-own crypto.
We wanted a light(er)weight block cipher with a 256 bit block size and 256
bit key for a thing. Simon seemed like a good candidate.
The Simon papers give the following parameters for the different key and
block sizes:
Block Size 2n,	Key Size mn,	Word size n,	Key words m,	Const Seq,	Rounds T
32	64	16	4	Z0	32
48	72	24	3	Z0	36
48	96	24	4	Z1	36
64	96	32	3	Z2	42
64	128	32	4	Z3	44
96	96	48	2	Z2	52
96	144	48	3	Z3	54
128	128	64	2	Z2	68
128	192	64	3	Z3	69
128	256	64	4	Z4	72
256,256 is missing:
256     256     ??      ?       ??      ??
After some fiddling and meddling, we came up with:
256     256     128     2       Z5      126
Where Z5 = w =
This was the only unused combination of patterns in the sequences (Z0-Z1
and the same XORed with 10101010... for Z3-Z5) . It slots right in.
124 rounds was the minimum we were comfortable with. 126 rounds gives the
most unrolling factors 2, 3, 6, 7, 9, 12, 14, 21, 42 and 63. So we picked
126 rounds. 128 rounds was the second place contender, but running for a
couple of clocks less than an integer multiple of the number of bits in
the input word is a fine thing in hardware, giving a spare couple of
clocks for strobes, so 126 wins. The other parameters just drop out from
the spec.
Having done this, we thought it a good idea to publish the parameters and
suggest that these are the right parameters for this, so people could use
them or shoot holes in them. So here they are.

@_date: 2016-01-11 04:55:03
@_author: dj@deadhat.com 
@_subject: [Cryptography] Australia commits encryption suicide 
The US rules are not much different. Which is why after discussions twixt
lawyers and the state department, I put a variety of pre-standardization
algorithms on my public web site. So I could then take proposals to adopt
those algorithms to standards meetings in China an return without being
slapped in irons for "exporting arms".

@_date: 2016-01-12 20:45:24
@_author: dj@deadhat.com 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
As users, you should vote with your wallets and not buy products with CPUs
that don't come with a proper entropy source built in.

@_date: 2016-01-13 19:09:34
@_author: dj@deadhat.com 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
There probably isn't. There certainly are datacenter applications for
multi gigabit random number generators and we make those. But sources for
resource constrained chips are in the 10s of megabits/s and are switched
off most of the time. The speed is usually a function of the clock speed
of the device. E.G. 100MHz, 8 clocks per bit => 12.5Mbits/s.
Where the need for random bits is low, there is no longer a need for a
DRBG/PRNG. A source and extractor will be sufficient.
In either case, an entropy source is the thing you can't do without, be it
fast, slow, fully or partially entropic, you need something. The rest you
can fix up with algorithms.
Storing state is often a problem in resource constrained chips. A few
hundred fresh random bits per power cycle is more practical.
I know why the term IoT is popular. I get weary of typing "resource
constrained" all the time.

@_date: 2016-07-07 01:17:18
@_author: dj@deadhat.com 
@_subject: [Cryptography] What to put in a new cryptography course 
I know that when I make that assertion, I mean that the argument you can
make for the security of the system is simple enough to be comprehended
and analyzed.
It may be in the context of a complex thing, but it's good to be able to
point to a simple set of states and interactions that are orthogonal to
the state and operation of the rest of the system and show how those
engender some security property.
It can't be taken too literally. For example side channel and fault
injection mitigation mechanisms often involve layering multiple defenses
so that all have to be broken simultaneously. This is rarely completely
simple, but then it's rarely super complicated either.

@_date: 2016-07-23 14:48:12
@_author: David Johnston 
@_subject: [Cryptography] Entropy of a diode 
The entropy in a diode is a very open question indeed. In practice the entropy in the diode is vast relative to anyone's cryptographic needs.
The problems are
A) Getting it out and turning it to useful random bits.
B) Knowing how much entropy is intrinsic to the diode and how much is externally sourced that you don't want to count.
C) Knowing what kind of extractor to use
D) Knowing how to conduct online health tests, which means knowing the failure modes.

@_date: 2016-07-24 12:45:16
@_author: David Johnston 
@_subject: [Cryptography] Entropy of a diode 
This isn't just a property of transistor noise. It's a property of all digital samplings of physical processes in this universe.
If you are building a circuit to convert sampled noise into bits, you need to do entropy distillation/conditioning/extraction or whatever it's called this week. I prefer distillation since it gets at the essence of what's going on.
Pinkas proved with a single source, no deterministic algorithm will get you to 100% entropy, as in Hinf(X)=1, or stated more directly, max_i(P(xi)=1) = 0.5.
Dodis proves you can get close enough for crypto, but the guarantee is a computational bound, not full entropy.
You can get there with multiple independent sources, but where do you find independent sources in this universe?
For the practicing engineer, you can take advantage of multiple sources and accept that while you can't prove they are independent, you can assume they are independent enough that it's going to work. The benefits being that the multiple input extractors can be simpler with more clear proofs. We built one this way and as far as I know it's the world's most efficient RNG (in bits/s/W and bit/s/m^2). I think it is an issue for crypto systems that people built physical RNG noise sources without paying attention to extractor theory. The two are tightly coupled disciplines.

@_date: 2016-03-03 07:56:04
@_author: David Johnston 
@_subject: [Cryptography] A question on Shannon's entropy 
You can't measure randomness by looking at the data from a source. You can infer bounds for the randomness (using loose terms here) from analyzing the source generation process, but all you can get from looking at the data are statistics. Entropy estimation algorithms are just that - an estimation.
If you have a theoretical model of your source giving an entropy prediction, a simulation of your circuit giving an entropy prediction and a lower bound estimate for entropy from data from the physical source then you want them all to agree, so you can be confident that the model is right.
Sorting is a deterministic process that is applied to the data. A bad one in the sense that it is shrinking the space of values and reducing the entropy rate at the same time. This points to the problem of estimating entropy based on symbol frequency. It is not sensitive to correlation, non stationarity or algorithmic associations between symbols. Sorting introduces both algorithmic invariants X[i] >= X[i+1] and serial correlation (the next value is likely to be close to the last). Never use the entropy estimate number from 'ent'. It would not detect the difference.

@_date: 2016-03-22 05:49:20
@_author: dj@deadhat.com 
@_subject: [Cryptography] Formal Verification (was Re: Trust & randomness 
Formal verification of software is something I gave up hope on in 1991,
but formal verification of digital logic, in particular any moderately
complicated crypto implemented in digital logic is pretty much the only
hope and salvation. Digital logic is roughly equivalent to functional
programming, except the syntax is friendlier and people actually use it in
real products. However the conventional mix of directed and randomized
testing that is used to validate and verify digital logic designs is next
to useless checking moderately complex crypto algorithms with many input
states. However formal equivalence verification of hardware against a high
level model of a crypto algorithm (be it C, cryptol, or whatever,
preferably human readable) can do the job over all states in minutes.
FEV is traditionally used in digital logic to check the synthesizer did
its job in turning RTL into gates. This is exactly the same as having a
tool that would verify that compiled machine code is formally equivalent
to the C that the compiler translated it from. This is because in digital
logic, no one trusts the synthesizer, whereas in software, only the
security people seem to mistrust the C compiler.
If we wrote software with the same restrictions we write RTL (fixed state
with purely functional transitions), the tools for formally verifying
model equivalence would already exist.
My might want to make our network protocols a little less baroque to make
that feasible.

@_date: 2016-03-29 05:42:21
@_author: dj@deadhat.com 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Indeed look at Skein and Simon. Two prime examples of HW friendly crypto.
Simon scales from serial to highly parallel designs and is highly
Skein unrolls both width wise and depth wise so
area/speed/performance/power tradeoffs can be easily matched to the
AES not so much. It has a large round function, a horrible number of
rounds for unrolling and has asymmetry in the key schedule and mix column
between encryption and decryption.
I've implemented all these things in RTL in various configurations. Skein
and Simon were the ones where the HW friendliness stood out.

@_date: 2016-03-29 06:29:00
@_author: dj@deadhat.com 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
The authors of Simon may have had small crypto implementations in mind,
but the underlaying property of these 'lightweight' algorithms (compared
to say AES) is that they are more efficient with their use of gates - they
get more done per gate. It's perfectly possible to throw lots of gates at
Simon and produce a high performance, high security strength cipher
The smallest AES implementations are proportionately slower than larger
implementations. The work done per unit are doesn't change much compared
to a native 10 clock, 1 round per clock implementation.
S-boxes dominate the AES area. The optimum sbox implementation (table,
synthesized table, Boyar Peralta, Canwright etc.) varies depending on what
you are optimizing for (clock speed, power, area etc.)  Memory accesses in
an sbox implementation would be not a good idea. They can done in gates
with much better side-channel properties and we have the solutions
available in published papers. Those same algorithms can be implemented in
software with state small enough to remain in registers, particularly the
tower field stuff that have long skinny dependency graphs.
A while back I published my proposed parameters for Simon with a 256 bit
block size on this fine email list. Amongst the various things was a
number of rounds that had the most integer divisors (above the minimum
acceptable number of rounds), for optimum unrolling flexibility. These
things matter a lot when it comes to hardware implementations.
Maybe back to the original point, we want ciphers with huge, and
preferably arbitary size blocks. A guy from Cisco made essentially the
same point in his talk at the NIST lightweight crypto workshop so it's not
just me and the people I work with. NIST/NSA public consumption ciphers
seem to deliberately avoid large block sizes. The absence of 256 bit block
sizes from AES and Simon/Speck is clearly deliberate. We want all the
powers of 2 block sizes up to at least the size of the largest jumbo
packets and the largest unit of disk block storage.
Take a look at the lightweight ciphers which have various block sizes. You
can plot the number of rounds against block size and see that the rounds
per block-bit goes down as the block size increases. These algorithms get
more efficient as the block size increases. There's no good reason not
have larger block sizes.
I found Rogaway's Sometimes Recursive Shuffle, to be an interesting
direction in block size independent ciphers. I don't know where that's
going to go. I have good uses for it though.
Friends don't let friends do crypto in software.

@_date: 2016-03-30 17:12:58
@_author: dj@deadhat.com 
@_subject: [Cryptography] Gates are cheap. Should cipher design change? 
Bigger or arbitrary block sizes.
Fine grained parallel scalabilty.
Intrinsic Side channel resistance.
To be fair those were not such hot topics at the time. But that's in part
because HW implementers didn't have much of a voice in the process.
Similarly for SHA3, the data for HW properties seemed to come from studies
on FPGAs that came up with the wrong answer for synthesis to cells and all
the performance and implementability went out of the window in the final
selection. Packing the room with European researchers determined the
"Abstract. We propose a new cryptographic primitive, the tweakable
block cipher. Such a cipher has not only the usual inputsmessage and
cryptographic keybut also a third input, the tweak. The tweak serves
much the same purpose that an initialization vector does for CBC mode
or that a nonce does for OCB mode. Our proposal thus brings this feature
down to the primitive block-cipher level, instead of incorporating it only
at the higher modes-of-operation levels. We suggest that (1) tweakable
block ciphers are easy to design, (2) the extra cost of making a block
cipher tweakable is small, and (3) it is easier to design and prove
modes of operation based on tweakable block ciphers.
Which I take to mean "more data input bits" because it's inconvenient to
steal from the existing power-of-2 number of data bits or add a whole
extra block.

@_date: 2016-05-01 10:56:47
@_author: David Johnston 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
The basic mechanisms are already deployed in proprietary ways. The USB PD authentication spec is just a standardization of existing practice - which I'm told works just fine at limiting counterfeit chargers.
The spec is not a copy and paste of any existing protocol though. It's a clean sheet design by members of the USB-IF.
The PD auth spec is not fit for purpose for preventing the delivering of malware, except in specific cases that an enterprising malware distributor would just work around by using the USB data wires instead of the PD wires.
The malware threat is principally on the USB data wires, both by exploiting vulnerabilities in known drivers ("Hi I'm an xyz-corp mouse, load my Swiss cheese driver") and exploiting overly trusting operating systems. That is for the other, as yet unwritten, spec which would do the auth before a driver is loaded and would enable different certification models (think corporate CA provisioning devices received through a secure supply chain).
There are plenty of motives for a USB security spec without inventing hypothetical ones. Car park flash attacks, BadUSB, MITM loggers and other USB vectors all provide the motivation for a security spec on the data wires, but that simply isn't done yet.
On PD it is entirely possible to make a device that lies and cause more volts or amps to be presented or pulled respectively than it compatible with the continues functioning of the device. This happens today with resistors on Type-C connectors, but with the PD protocol that negotiation is done with a protocol.
The other thing the PD auth spec does is provide a means to see that specific electrical certifications (UL, EC etc) have been attested to and who is doing the attesting. Also to see that specific USB certifications have been granted.
So the 'hidden' motive you suggest is not a motive for this spec, but it is a motive for the second part. As with any standards development, this can change until the final draft is approved.

@_date: 2016-05-02 17:34:01
@_author: dj@deadhat.com 
@_subject: [Cryptography] [FORGED] Re: USB 3.0 authentication: market 
No. This is the wrong. The PD spec is adjacent point to adjacent point.
The USB data spec is end-end through hubs. This is one of the many reasons
for treating the PD part separately from the USB data part.
Measurement and electrical defense is orthogonal to establishing if
quality and compliance have been attested to cryptographically. You can
still do measurement if you want to.

@_date: 2016-05-02 17:35:06
@_author: dj@deadhat.com 
@_subject: [Cryptography] Craig Wright is Satoshi 
Or did he? The skepticism seems well founded for now..

@_date: 2016-05-02 18:18:27
@_author: dj@deadhat.com 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
The CA that needs to exist would the the USB-IF. That's a consequent of
the spec that says the mandatory cert is one signed under the USB-IF root
In the USB data security spec (that is not yet released) leaves other
slots open to organizational certs, some organization (a household, a
corp, a government etc.) could provision devices with an org cert they
want to work with their internal devices that enforce a policy of not
working without the organizational cert.
This was principally my idea. It has been present in a draft that I wrong
a few years ago that sat moribund under the USB manufacturers felt
pressure to have some sort of security spec. It doesn't include a model
for using the normal CAs used for web certification since they have proven
themselves ineffective so many times.
My experience with normal CAs when trying to get them to support device
certificates is that they expect too much money. They want $100 per year,
per cert, rather than a couple of cents per device one time. That's why
for WiMAX we had to initially deploy our own CA in a corporate CA, before
the task to could passed on to an external CA that would accept the
business. I see no difference here. The USB-IF is going to have to set up
a CA somehow because that's what the spec implies.
The USB-IF already does that for the (relatively small) population of USB
silicon vendors.
If you thing something is stupid in the spec, please email specifics in
response to the release of the spec, to the email addresses at the bottom
of the page with the spec on it.
Your PC or phone authenticating a charger certainly can do revocation
using the usual mechanisms, but it has been my assertion that these things
tend to be done by policy download from OS vendors and browser vendors.
Why would this be any different? Browsers and OSes contain whitelists and
blacklists as policy to be enforced because revocation is rarely fit for

@_date: 2016-05-03 20:49:53
@_author: dj@deadhat.com 
@_subject: [Cryptography] WhatsApp, Curve25519 workspace etc. 
I rather like 128 bits for a key size. Especially when the data size is
also 128 bits. This may be mostly because I implement crypto in hardware.
An O(2**128) problem being attacked by 100,000,000 custom circuits in an
NSA data center, each circuit trying 10,000,000,000 combinations a second
(parameters we might hypothetically reach in 20 years if we're
optimistic), it would take 10,790,283,070,806 years to complete. So my
estimate of attack strength could be off by a factor of 1,000,000 and we
still would be ok. I'd really like to get the contract for supplying those
The odds of a cryptographic attack seem higher that of a brute force attack.
I am unafraid of quantum computers. They cannot and will not happen in the
way imagined by the media. I'm having some of my time occupied in
'preparing' for quantum computers. I think it'll be a backwards step,
moving us away from simple public key schemes to really complex ones that
will be more vulnerable to cryptographic failures that would not happen to
say Curve25519.
If you're increasing the key strength to 256 or 512 bits to increase
security, you are failing to achieve your goals. Your weakest link lies
elsewhere and by focusing on key size beyond 128 bits, you are missing the
opportunity to address the weakest link, or spending extra time lobbying
NIST for 256 bit block sizes in their block ciphers (am I the only one
doing this?).

@_date: 2016-05-22 19:35:13
@_author: dj@deadhat.com 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
Indeed. Up to 8 cert chain slots are addressable - The slot number
occupies 3 bits in the protocol.
The USB-IF slot 0 is mandatory for USB certified devices and gets filled
with a cert chain rooted in the USB-IF. That is what I meant - It's
mandatory for the USB-IF to set up  CA to support that. Not mandatory on
manufacturers to get certified (unless you want to use the trademarks and
not mandatory for you to use or honor the USB rooted credentials.
The USB rooted credential chain tells you that the USB-IF tested and
certified the device design, subject to the usual ways that might be
circumvented, which has already been discussed on this mailing list (the
metzdowd crypto list at least). This is a mechanism that has proprietary
solutions today, but it's not based on those solutions as far as I'm
Other slots can be used for whatever your purpose.
What you need to be concerned about is control over policy. If the policy
is hardwired to 'must have a USB-IF certified device cert' then you will
be limited to devices certified by the USB-IF. If you want to roll your
own USB device, you might want to wrestle control of the policy setting.
The other caveat is that if you want to certify your devices under your
own PKI, then you need 'provisionable' devices. Meaning they support the
provisioning protocol that's in the spec, have enough non volatile storage
and a means to securely hold keys.
I can envisage companies charging extra for 'provisionable devices', even
though the silicon might support it by default. That's the way of things.
The other wrinkle is it might be the silicon that's certified, not the box
it is in. Think about stand alone USB-RS232 chips. They get certified by
the USB and integrator doesn't meddle with the insides.
I do.
I have no argument with that. Again, in the USB context, this is a matter
of devices being provisionable to support your desire to enforce your own
policy. The analogy to the secure boot situation is finding that a USB
vendor had pre provisioned slots above zero and had hard coded a policy
relating to the pre-provisioned slot. Don't buy those devices. The spec
has words to say on not using the auth protocol for vendor lock in, and so
enforcement could happen with the certification process. Whether or not it
will is not something I know. Of course a non certified device could do as
it pleases.

@_date: 2016-05-22 21:13:20
@_author: David Johnston 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
While I'm gainfully employed as an RNG designer and general crypto security person, I hold the opinion that ignorance beats entropy.
In one sense, ignorance of the state of a system can be equated to that system having entropy relative to the thing that is ignorant of the state of the system.
However we tend to think of entropy as being an intrinsic thing, arising from underlying quantum uncertainty, rather than a relative thing.
However we know we don't have a complete understanding of quantum physics or quantum uncertainty, whereas we know all about ignorance. You can rely on ignorance. If someone is ignorant of your key, the key works just fine in a crypto system that is intended to prevent that person undermining security in some way.
Deterministic processes are just fine at taking samples from complex system and turning into a state that is hard to predict. While having 'full entropy' numbers that therefore have no algorithmic connection between them is a fine thing for random numbers, the whole concept of full entropy comes from the assumption that the randomness of quantum uncertainty is a real thing. If not. If the rules of the universe are actually deterministic then we have to fall back on ignorance of the state of complex systems in order to create unpredictable numbers.
So in that sense, ignorance beats quantum uncertainty. You can rely on ignorance, but have to trust the assumption that quantum uncertainty is If you make your crypto system such that it's secure providing either one of ignorance of a complex system state or quantum uncertainty is true, then the assumptions on which the security of the system are based will be more robust.

@_date: 2016-05-25 14:56:28
@_author: dj@deadhat.com 
@_subject: [Cryptography] Hacking spread spectrum clocking of HW ? 
It depends on the application.
Cryptographic spreading codes and wide bandwidths are seen in military
CDMA Spread spectrum mobile phones used Walsh codes. I haven't kept up
with WCDMA and LTE since I got sucked into crypto but the principle is the
Those little clock oscillators typically use an LFSR since the goal is
simply to smear out the clock peak to keep within emissions limits. But
any long random looking sequence into a VCO would do.
Bluetooth's frequency hopping spread spectrum was not designed to resist
predicting the sequence. Quite the opposite.
This is wireless 101. Any modern wireless comms textbook should cover it.
If you're talking side channel mitigation or FI tolerance, then it's
currently open season on clever ideas. But that sounds too much like my
day job.
CAZAC codes for stealth canaries anyone?

@_date: 2016-11-22 21:19:48
@_author: dj@deadhat.com 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
It's a mixed bag.
I am one of a handful of people who know for sure that RdRand is secure
because we designed, built and tested it. So I'm happy to use it and I do.
Other people have to trust it and their basis of trust should be the
independent audit, researcher reviews and SP800-90 certifications which to
a greater or lesser extent mean people have looked at it and declared it
to be exactly what it claims to be.
If you instead choose to get your random numbers from your OS, that gets
those random numbers from RdRand, all the OS is doing is adding attack
surface and reducing performance.
The OS can provide a useful service by combining multiple sources, but
they have to (a) have those multiple sources (b) know how to get at them
and (c) do it right.
A library or the language compile could combine sources if they know how,
but subject to the same constraints.
If you are sufficiently paranoid, you could choose to combine multiple
sources yourself in the hope that at least one of them is secure. I have
developed software that does exactly that for applications that requested
it. To do that yourself you should study extractor theory to understand
how to combine sources. Don't believe anyone who says "just XOR them
together". For imperfect sources or malicious sources, that probably will
not work as intended. XORing certainly can make things worse, especially
when sources are not independent, as you typically find with RO sources
subject to the same supply noise. "Just hashing it" will probably do, but
there is zero mathematical evidence that it will do and extractor theory
says otherwise.
However one technical problem surfaced early on for practically combining
RdRand output with other sources in software. Most physical sources are
very slow compared to the multi gigabit/s source used by the circuit that
feeds RdRand and they can be slower still because the post processing is
done in software and lower quality because the extractor is often simply
wrong, like using a twisted LFSR or a Von Neumann whitener or a Yuval
Perez  whitener, all of which don't work as intended when fed data from
serially correlated sources (which is practically all of them) . You might
want to check for that.
So when you choose to combine 1 second worth of data from RdRand with 1
second worth of data from your undersampled ring oscillator, you end up
with 800MBytes of RdRand data and a couple of kilobits of data from the
other source. You can choose to lower the performance to the slowest
source you have and if that's ok and you're using a good extractor, you'll
be ok.
I'd point you to a good book on extractor theory, but I haven't finished
writing it yet, sorry and I can't make any guarantees as to whether or not
it is good.

@_date: 2016-11-26 04:15:30
@_author: dj@deadhat.com 
@_subject: [Cryptography] Is Ron right on randomness 
No, the above  is not accurate. It does matter how good your entropy
source is. The leftover hash lemma gives you the expression for the amount
of entropy you can extract from entropy sources - but doesn't tell you how
and for the real constructions the answer is worse. Subsequent papers
given bounds for certain specific extractors. This can be be summarized as
the 0.5 limit. If your input data has less that 0.5 bits of entropy per
bit of data, your extractor is swimming upstream slower than the stream is
moving downstream. This is true for seeded single input extractors. It is
more complicated with multiple input extractors, but in general, multiple
input extractors are the way to get above the 0.5 limit, whereupon you can
feed a single input cryptographic extractor.
It also doesn't address the issue of malicious sources as inputs to
multiple input extractors. There may or may not be extractors that can
defend against this, but there certainly are extractors for which a
malicious input can reduce the entropy at the output. The simplest example
being the XOR gate - one input full entropy, the other input malicious.
The malicious input just needs to be correlated with the other input in
order to reduce the entropy at the output. The Linux kernel entropy pool
mixing was is a more complicated example that has been well documented in
this respect.
The other thing that is probably relevant, but not mentioned above is that
frequent reseeding, or other schemes for injecting fresh entropy into the
system is an effective defense against a class of attacks that seek to
learn the internal state of the CSPRNG. It's hard to do this well in
software because you don't usually get to use every spare CPU cycle to do
more extraction and reseeding. It would upset people coping with their
unnecessarily slow computer. So how in software do you know you are
putting fresh state in faster than an attack is learning it?  How do you
know how much effort to apply? A hardware extractor gets to sit there all
day extracting seed data from an entropy source that similarly can sit
there all day turning noise into bits. This is one of the basic design
principles of the RNG's we design - reseed times should be measured in
microseconds, not seconds.
A software CSPRNG is simple enough. Seeding it is the hard part. How do
you extract from a source when you don't know the nature of the source? A
source is a hardware component. The extractor design needs to be strongly
coupled to the nature of the entropy source. If it's a weak source in
terms of min entropy (Hinf(X) < 0.5n) then maybe some other more
descriptive property of the source lets you use design a pre extractor to
get above 0.5. So designing an entropy source pretty much obligates you to
design the extractor to go along with it. A general purpose software
crypto library like openssl is probably the wrong place to do extraction,
unless you are writing a comprehensive extraction algorithm that is aware
of the nature of many sources out there and is able to identify them
unambiguously and then do the right thing with them from an extractor
algorithm point of view.

@_date: 2016-11-26 12:43:11
@_author: David Johnston 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
RdSeed is an implementation of the XOR Construction ENRBG as defined in SP800-90C. This draft spec did not exist when RdRand was first developed.

@_date: 2016-11-26 12:52:41
@_author: David Johnston 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
This has been addressed multiple times on this list so once again:
The raw access mode is disabled on production parts. This is because it would be an attack vector. Log into a VM in a cloud machine and put the RNG into raw mode and other processes in other VMs would then be getting unwhitened data while thinking they were getting the output of a CSPRNG (RdRand) or an ENRBG (RdSeed).
If we enabled ring 0 processes to put the shared RNG in raw mode to satisfy the curiosity of people on this list we would be enabling an attack on most users. Also we would be violating the FIPS140-2 and SP800-90 requirements. The quality of the raw data is tested internally to the DRNG with the algorithms we have published.
Feel free to talk to NIST about adding things to their specs to enable raw access in while simultaneously in a FIPS context and a cloud context.

@_date: 2016-11-26 15:42:04
@_author: dj@deadhat.com 
@_subject: [Cryptography] Is Ron right on randomness 
This is extractor theory which I'm by no means an expert in, but I did
just come back from the COST-IACR School on Randomness in Cryptography
which was a week long set of lectures mostly on the current state of
extractor theory from the professors who developed it so I'm feeling
pretty clued in right now - that will pass.
I've posted these before on this list, but the previous discussions
suggest they are not widely read. In particular the
Start at the start, you can't encrypt one bit from in the presence of a
weak source.
So you're going to have to do something else.
[b] But you can still do crypto:
The utility of your extracted random numbers depend on what you plan to do
with them:
Bounds for CBC-MAC and HMAC. AKA the 0.5 Limit.
But those bounds are in terms of computational bounds on the adversary,
not min entropy:
Last week's randomness school had a strong focus on developing chain rules
for conditional entropy under various entropy models (Min, Shannon,
Guessing, Collision, HILL, metric, metric*) but since the authors of these
paper were lecturing, I think I actually understood it. There are a lot of
papers to read, but this survey paper is a good place to start:
Multiple input extractors help you get past the 0.5 limit:
Extractors have different bounds when you consider the entropy against
computationally bounded adversaries (that's us!) the general case being
HILL entropy.
Is there a chain rule from which you can derive the entropy? It's an open
question, but this paper has counterexamples under some simple
assumptions, which is a no vote:
There were other cryptographers present who were not so sure.
There is a lot lot more, but these are good starting points.

@_date: 2016-11-26 16:32:47
@_author: David Johnston 
@_subject: [Cryptography] Is Ron right on randomness 
1: Add the ability to identify the sources available.
     RdRand and RdSeed in Intel and AMD are easy. Use CPUID.
     The old i810 RNG might be available or emulated in a VM (but then you need to know what the VM is doing).
     I understand that there are other on chip sources available on other architectures but I am not an expert in using those.
2: Properly characterize the source:
     Min Entropy Sources
     Hill Entropy/Seeded Pseudo random sources
     Squish Sources
     Gaussian sources (like RF receive chains or the demod error vector popular on phone chips)
     FIPS compliant sources
     DRBGs or ENRBGs
     etc.
3: Apply proper extractor theory to correctly identify the right extractor algorithms to apply to these sources
4: Combine them with some additional extractor theory.
5: Use that to seed your own CSPRNG as frequently as required - this is a slightly hard problem which I wrote about yesterday.
6: Provide a simple and comprehensible configuration interface and reporting options so code through an API and users through the command line can interrogte what's going on regarding available entropy and set policy as required (E.G. require only FIPS compliant or SP800-90 compliant or tin foil hat compliant sources).

@_date: 2016-10-09 06:15:12
@_author: dj@deadhat.com 
@_subject: [Cryptography] Security Fatigue 
It seems being a paying member of the IEEE computer society doesn't grant
access to the paper either. It's not quite as bad as ISO, but it's getting

@_date: 2016-10-12 23:18:38
@_author: David Johnston 
@_subject: [Cryptography] Defending against weak/trapdoored keys 
Rather that running two DHs, why not give both ends a hand in choosing the prime randomly? Alice and bob swap random numbers and hash them. Alice and Bob both know their fresh random number went into the hash. Use the hash output to seed a CSPRNG that services a prime search algorithm. Use the prime that is found. You will want to do the usual stuff to prevent MITMs.
Then it becomes a race between what's more efficient, the extra round trip of DH or the extra cost of the prime search. This will depend on compute vs network capability.

@_date: 2016-09-06 01:14:45
@_author: dj@deadhat.com 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
I've worked on several security protocols, both wired and wireless that
are widely used and each time they have been an add on to existing
protocols, so there were lots of compromises. It seems each time that if
they had decided to do security at the physical layer, it would have been
more secure and easier since it would be easy to obfuscate the carried
packet sizes, header contents and timing.
So if you are contemplating making a new mass market protocol with a
physical layer, please include the security protocol people in the
physical layer design.

@_date: 2016-09-06 18:17:24
@_author: dj@deadhat.com 
@_subject: [Cryptography] "Flip Feng Shui: Hammering a Needle in the 
Nope. I can't point to such articles. I know of none.
However I have plenty of ideas of my own.
In particular, framed protocols - many generations of cell phone
protocols, 802.16, WirelessUSB etc, where time is split into frames and
transmitters are granted slots within the frame. The downlink from the
base station dictates frame timing. You can consider 802.11 to be a bit
like this, but it's looser.
Go down one level and you find time split into big old OFDM symbols
containing a bunch-o-bits. Say 256.
The protocol over these frames is plaintext. The headers of packets within
slots within these frames is plaintext. The pack contents of the layer
above are encrypted and integrity protected. So there's plenty of scope
for meddling with the physical layer.
Simply by pushing the security protocol down a level and protecting
symbols instead of packets, all that identifying header stuff goes away.
The overhead of IVs and ICVs is not proportional to the inverse of the
packet size which is a problem with mac layer fragments in wireless
protocols - lots of little PDUs each with a 64 bit IV and 128 bit ICV?.
For 802.16 I proposed a scheme where you add IV_PDUs and ICV_PDUs and only
issue them at SDU boundaries. So fragmentation costs are avoided and all
the PDU contents, headers and all, are passed through the AEAD mode. Not
quite physical layer crypto, but it would be easier to do it at the
physical layer.
In other places, DSSS is an obvious model where you can make the spreading
code a secure random sequence. I understand the military like that, but I
don't design weapons and I haven't come across in things I do.
The additional robustness here is the MAC layer protocol is mostly
protected. There is no other magic that I'm aware of.

@_date: 2016-09-17 18:21:46
@_author: dj@deadhat.com 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
of instances of it are out there - perhaps right in your pocket.
I think there are North of 200 Million RNGs of the type I'm partially to
blame for, many of them in people's pockets.
I'm perplexed by the notion that you 'just' need 256 bits of entropy and
then you can deal with it with PRNGs, storage and secure compute elements
and things, as if the entropy source was the difficult bit. That post
handling is of the order of 100X more complicated, power hungry and
silicon area consuming than an entropy source, extractor and online health
test that can produce 256 bits of entropy every microsecond, continuously.
Non volatile storage in particular is a pig to render secure since you
face the chicken and egg problem of where to store the key to protect the
NVM? You could use PUFs, but some OTC PUF solutions require off chip flash
for the helper data.
If a usable supply of cryptographically secure random numbers is required,
just put a good RNG in your chip or board.
10 years ago, a CSPRNG (complete with AES block) was faster per unit area
and faster per Watt than entropy sources. These days, due to entropy
sources getting smaller, faster than silicon process, the reverse is true.
The same silicon area filled with entropy sources and extractors will be
faster (or lower power or whatever variable you want to optimize) than the
same area filled with a CSPRNG.
Shrinking entropy sources comes from (a) better circuits and (b) better
extractor theory.
I haven't noticed similar improvements in CSPRNGS. Much of the
'lightweight' PRNG work seems to have been aimed at trading off area for
security level.

@_date: 2016-09-18 16:25:15
@_author: David Johnston 
@_subject: [Cryptography] True RNG: elementary particle noise sensed with 
This paper
      is one we built, based on a circuit we developed and a paper I found.
It was and may still be the smallest, most efficient secure RNG in terms of joules per bit and bits/s/W. It started with this youtube video:
      (skip to minutes 41-42)
leading to the paper:
     which itself has lots of references worth following.
The circuit was described here: Since then there have been lots more papers on extractor theory, some of which express efficient structures, some don't. Most are unreadable to non mathematicians and this has limited the rate of adoption. Here's one - I defy you to find the extractor in there in under 5 minutes and then describe it in actual operations on bits and bytes: This pair of papers made the news in 2015, but I never could work out what the actual algorithm was. It reads like they proved a function exists, but don't define any specifics. It's on my list of papers to decode. We've had good (by my definition of good) 2 source extractors since 2006 and I don't know what makes these better:
I'm not asserting that the improved extractors are recently described, I'm saying it took a good decade for anyone to notice, decrypt the terse descriptions and start building them in silicon.
I suspect the largely useless Von Neumann whitener and the related Yuval Perez whitener were so often used and used wrongly (E.G. with serially correlated sources) because the algorithms in the papers were comprehensible to engineers, but the preconditions were not.

@_date: 2016-09-23 22:14:04
@_author: dj@deadhat.com 
@_subject: [Cryptography] Spooky quantum radar at a distance 
Not a change, there's no transmission at that point. The measurement of
one particle will be correlated with the measurement of the other
particle. This is the basis of QKD - you can yield the same random value
at both ends. It's not FTL because you had to send the particle on it's
way. It's not transmission of information yielded from the measurement
because the information didn't exist at either end until measured.
That's exactly how normal radar works. Measurement and entanglement are
the same thing. It's not that effective though.
I don't see how that could work. There's a rather limited amount of
information and the entanglement of the remote particles can't be used to
transmit any information. You would need to send lots of photons and keep
track of them all in order to get one back. Keeping track of one photon
right now required special handling.
What's to prevent the bomb including a device that implements a hermitian
matrix that's the inverse of the hermitian matrix that represents the path
of the photon to the bomb so making the photon sharp (in quantum
terminology) at the bomb?

@_date: 2017-02-05 18:10:54
@_author: dj@deadhat.com 
@_subject: [Cryptography] Pure Randomness Extracted from Two Poor Sources 
This is another paper with the HECS (Hidden Explicit Construction
Syndrome). If I have managed to understand it correctly, it is showing a
lower bound on the distance from uniform of the 2Ext algorithm. There's
another paper (  ) that points out that 2Ext
is safe against quantum entangled adversaries, which is nice. Lets all use
My problem is with 2Ext (  -
another paper with HECS).
It is defines a blender(X,Y) and invokes a Trevisan seeded extractor
trevisan(S,Y). (Trevisan extractors are based on error correcting codes).
You use the blender to make the seed for the Trevisan seeded extractor.
I.E. 2Ext(X,Y) = trevisan(blender(X,Y),Y).
The blender algorithm defines a series of square, full rank matrices, one
matrix for each output bit. The matrix elements are bits. The sides are
length |X| and |X|=|Y|.
The algorithm is output_bit_x = matrix_x * X (as a column matrix) inner
product multiply with Y.
The matrix construction is in the paper and leads to a fairly sparse
matrix, which is nice for implementation.
The problem I have is when you try it, the blender output is substantially
more biased than the inputs, even with uniform inputs and when I try it
with randomly generated full rank matrices, I get a much better result. So
the min entropy of the seed is reduced relative to just trevisan(X,Y) and
the random matrices (which the paper affords worse bounds) gets a better
result in practice. This doesn't seem right to me.  I'd like to use 2Ext,
but until these loose ends are understood, it's a no go.
Note the title of the journalist article you link is misleading. The
paper's claims are properly worded and they don't claim to produce 'Pure

@_date: 2017-01-01 20:49:06
@_author: dj@deadhat.com 
@_subject: [Cryptography] Smart electricity meters can be dangerously 
The smart meters I am familiar with cannot control the electricity.
The payoff of smart meters is saving costs by reading meters remotely.
Adding expensive control components wouldn't be cost effective.

@_date: 2017-05-21 17:33:30
@_author: David Johnston 
@_subject: [Cryptography] NIST SP800-22 Rev1a Test Suite 
For those that take an interest in the SP800-22 randomness test suite (and all its many flaws), I've released an implementation of the tests that has the benefit of not crashing as much as the NIST STS-2.1.2 software.
