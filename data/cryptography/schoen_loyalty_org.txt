
@_date: 2001-08-30 09:50:06
@_author: Seth David Schoen 
@_subject: Outreach Volunteers Needed - Content Control is a Dead End 
The Association of American Publishers said something like this in reply
to DMCA critics.
Amy Gwiazdowski said:
But the argument was completely bogus.
As we know, there are many kinds of things that get called "privacy".
One is the ability to prevent information from becoming known
(communications privacy, or confidentiality; and then also privacy
relating to what you do, or where you are, which has nothing to do
with communications or cryptography).  Then there's the ability to
prevent people from publishing, sharing, or using that information in
certain ways.  This includes the legal rights of privacy and
publicity, as well as things like consumer protection legislation (the
Fair Credit Reporting Act, more recent U.S. credit legislation, and
EU laws on publishing data about individuals).
All of these things are different, and treated differently by law.
Just as patents are not the same as copyrights, which are not the same
as trademarks, so copyrights are not the same as consumer privacy,
which is not the same as communications privacy.  These things are not
treated in the same way by the law.  (There are lots of examples: for
instance, there's fair use doctrine in copyright law, but I don't
think there's a fair use doctrine in EU consumer privacy law.)
Some kinds of control of information -- like the confidentiality of a
communication between two consenting parties, whose interests are
aligned -- can be protected by engineering and technology.  Some
kind of control -- like preventing copying of digital data _by a
customer who buys a copy of that data_ -- cannot be protected by
engineering.  When different kinds of control are protected by
legislation (or, per Lessig, by technology backed by legislation), the
legislation is drafted differently for each particular case, and is
not equivalent.
Meanwhile, PGP is not insecure just because DRM systems are insecure.
(U.S. law and regulations used to make it illegal to export PGP, which
may have been secure, but legal to export DRM and copy protection, which
was not.)
We should not say that all kinds of "information control" are
equivalent.  They have different motives, different threat models,
different social and legislative traditions, and probably even
different moral presuppositions.

@_date: 2001-09-01 08:55:35
@_author: Seth David Schoen 
@_subject: Outreach Volunteers Needed - Content Control is a Dead End 
I disagree about both of those things, but doesn't what you wrote last
time, about having to choose both or neither of privacy and copyright,
implicate mechanism?
Were you saying that people who believe in privacy are obliged to
believed in copyright, in order to be logically consistent?
Or were you saying that it isn't possible to enforce privacy without
mechanisms which are also suitable for enforcing copyright, and it
isn't possible to enforce copyright without mechanisms which are also
suitable for enforcing privacy?
I still think that "owning information as property" -- as a general
principle -- is mistaken not only philosophically but also in blurring
important details in enforcement mechanisms.  Lawyers are always
complaining that people confuse copyrights, patents, trademarks, and
trade secrets (although, as Richard Stallman says, this confusion may
come from the fact that all of these things are often referred to as
"intellectual property").  And then there are rights of privacy and
publicity, in civil law, and consumer privacy rights, and privacy rights
against the government in searches and seizures, and marital privacy
rights, and legal privileges against compelled disclosure...
Now, all of these things are _somewhat like_ what it would be like to
own information, and yet they're very different.  A copyright is not
the same as an attorney-client privilege, which is not the same as a
trade secret, which is not the same as your legal rights when dealing
with a credit card company.
Later in your message you mentioned the need for
in order to enforce control of information.
On the technological side, privacy-protecting technologies include
remailers, proxies, e-mail encryption, file encryption, stream/session
encryption, steganography, and so on.  Copyright-protecting
technologies so far mainly include watermarks and DRM (and you might
want to include spiders which try to find illict copies).  But it's
silly to think that PGP is like Adobe eBook Reader, even though both
of them try to use technological measures to enforce somebody's policy
about how certain information ought to be used.  Now, some publishers
may think that _every single attempt to enforce any policy is
equivalent to any other attempt to enforce any other policy_, but
philosophically, legally, and technically, a claim like that seems
bizarre to me.
Even if all "containers" worked properly, they wouldn't be enough to
enforce certain kinds of rights.  For example, PGP-encrypted e-mail
with your lawyer is not enough to enforce your attorney-client
privilege.  A secure intranet is not enough to enforce your trade
secrets, and cryptography not enough for consumer privacy (although it
can help by reducing the amount of information that has to be given
out in the first place in order to complete a transaction).
The general encapsulation of information into boxes that do only what
their owner wants is a pipe dream.  These boxes are useful in certain
contexts, but (per Schneier's _Secrets and Lies_, for example), we have
to remember the contexts.  Some of the biggest information-related
offenses in history (espionage, misappropriations of trade secrets,
infringements of copyright, invasions of privacy) were accomplished with
a piece of paper and a pencil, or with a mechnical printing press.  The
intermediate step was a human mind, in the role of mens rea.

@_date: 2001-09-01 08:55:35
@_author: Seth David Schoen 
@_subject: Outreach Volunteers Needed - Content Control is a Dead End 
I disagree about both of those things, but doesn't what you wrote last
time, about having to choose both or neither of privacy and copyright,
implicate mechanism?
Were you saying that people who believe in privacy are obliged to
believed in copyright, in order to be logically consistent?
Or were you saying that it isn't possible to enforce privacy without
mechanisms which are also suitable for enforcing copyright, and it
isn't possible to enforce copyright without mechanisms which are also
suitable for enforcing privacy?
I still think that "owning information as property" -- as a general
principle -- is mistaken not only philosophically but also in blurring
important details in enforcement mechanisms.  Lawyers are always
complaining that people confuse copyrights, patents, trademarks, and
trade secrets (although, as Richard Stallman says, this confusion may
come from the fact that all of these things are often referred to as
"intellectual property").  And then there are rights of privacy and
publicity, in civil law, and consumer privacy rights, and privacy rights
against the government in searches and seizures, and marital privacy
rights, and legal privileges against compelled disclosure...
Now, all of these things are _somewhat like_ what it would be like to
own information, and yet they're very different.  A copyright is not
the same as an attorney-client privilege, which is not the same as a
trade secret, which is not the same as your legal rights when dealing
with a credit card company.
Later in your message you mentioned the need for
in order to enforce control of information.
On the technological side, privacy-protecting technologies include
remailers, proxies, e-mail encryption, file encryption, stream/session
encryption, steganography, and so on.  Copyright-protecting
technologies so far mainly include watermarks and DRM (and you might
want to include spiders which try to find illict copies).  But it's
silly to think that PGP is like Adobe eBook Reader, even though both
of them try to use technological measures to enforce somebody's policy
about how certain information ought to be used.  Now, some publishers
may think that _every single attempt to enforce any policy is
equivalent to any other attempt to enforce any other policy_, but
philosophically, legally, and technically, a claim like that seems
bizarre to me.
Even if all "containers" worked properly, they wouldn't be enough to
enforce certain kinds of rights.  For example, PGP-encrypted e-mail
with your lawyer is not enough to enforce your attorney-client
privilege.  A secure intranet is not enough to enforce your trade
secrets, and cryptography not enough for consumer privacy (although it
can help by reducing the amount of information that has to be given
out in the first place in order to complete a transaction).
The general encapsulation of information into boxes that do only what
their owner wants is a pipe dream.  These boxes are useful in certain
contexts, but (per Schneier's _Secrets and Lies_, for example), we have
to remember the contexts.  Some of the biggest information-related
offenses in history (espionage, misappropriations of trade secrets,
infringements of copyright, invasions of privacy) were accomplished with
a piece of paper and a pencil, or with a mechnical printing press.  The
intermediate step was a human mind, in the role of mens rea.

@_date: 2002-08-06 12:11:39
@_author: Seth David Schoen 
@_subject: Privacy-enhancing uses for TCPA 
I would just like to point out that the view that "the protection of
privacy [is] the same technical problem as the protection of
copyright" is Microsoft's and not mine.  I don't agree that these
problems are the same.
An old WinHEC presentation by Microsoft's Peter Biddle says that
computer security, copyright enforcement, and privacy are the same
problem.  I've argued with Peter about that claim before, and I'm
going to keep arguing about it.
For one thing, facts are not copyrightable -- copyright law in the
U.S. has an "idea/expression dichotomy", which, while it might be
ultimately incoherent, suggests that copyright is not violated when
factual information is reproduced or retransmitted without permission.
So, for example, giving a detailed summary of the plot of a novel or
a movie -- even revealing what happens in the ending! -- is not an
infringement of copyright.  It's also not something a DRM system can
But privacy is frequently violated when "mere" facts are redistributed.
It often doesn't matter that no bits, bytes, words, or sentences were
copied verbatim.  In some cases (sexual orientation, medical history,
criminal history, religious or political belief, substance abuse), the
actual informational content of a "privacy-sensitive" assertion is
extremely tiny, and would probably not be enough to be "copyrightable
subject matter".  Sentences like "X is gay", "Y has had an abortion",
"Z has AIDS", etc., are not even copyrightable, but their dissemination
in certain contexts will have tremendous privacy implications.
"Technical enforcement of policies for the use of a file within a
computer system" is a pretty poor proxy for privacy.
This is not to say that trusted computing systems don't have interesting
advantages (and disadvantages) for privacy.

@_date: 2002-08-08 21:15:33
@_author: Seth David Schoen 
@_subject: dangers of TCPA/palladium 
I heard a suggestion that Microsoft could develop (for this purpose)
a provably-correct minimal compiler which always produced identical
output for any given input.  If you believe the proof of correctness,
then you can trust the compiler; the compiler, in turn, should produce
precisely the same nub when you run it on Microsoft's source code as
it did when Microsoft ran it on Microsoft's source code (and you can
check the nub's hash, just as the SCP can).
I don't know for sure whether Microsoft is going to do this, or is
even capable of doing this.  It would be a cool idea.  It also isn't
sufficient to address all questions about deliberate malfeasance.  Back
in the Clipper days, one question about Clipper's security was "how do
we know the Clipper spec is secure?" (and the answer actually turned
out to be "it's not").  But a different question was "how do we know
that this tamper-resistant chip produced by Mykotronix even implements
the Clipper spec correctly?".
The corresponding questions in Palladium are "how do we know that the
Palladium specs (and Microsoft's nub implementation) are secure?" and
"how do we know that this tamper-resistant chip produced by a
Microsoft contractor even implements the Palladium specs correctly?".
In that sense, TCPA or Palladium can _reduce_ the size of the hardware
trust problem (you only have to trust a small number of components,
such as the SCP), and nearly eliminate the software trust problem, but
you still don't have an independent means of verifying that the logic
in the tamper-resistant chip performs according to its specifications.
(In fact, publishing the plans for the chip would hardly help there.)
This is a sobering thought, and it's consistent with ordinary security
practice, where security engineers try to _reduce_ the number of
trusted system components.  They do not assume that they can eliminate
trusted components entirely.  In fact, any demonstration of the
effectiveness of a security system must make some assumptions,
explicit or implicit.  As in other reasoning, when the assumptions are
undermined, the demonstration may go astray.
The chip fabricator can still -- for example -- find a covert channel
within a protocol supported by the chip, and use that covert channel
to leak your keys, or to leak your serial number, or to accept secret,
undocumented commands.
This problem is actually not any _worse_ in Palladium than it is in
existing hardware.  I am typing this in an ssh window on a Mac laptop.
I can read the MacSSH source code (my client) and the OpenSSH source
code (the server listening at the other end), and I can read specs for
most of the software and most of the parts which make up this laptop,
but I can't independently verify that they actually implement the
specs, the whole specs, and nothing but the specs.
As Ken Thompson pointed out in "Reflections on Trusting Trust", the
opportunities for introducing backdoors in hardware or software run
deep, and can conceivably survive multiple generations, as though they
were viruses capable of causing Lamarckian mutations which cause the
cells of future generations to produce fresh virus copies.  Even if I
have a Motorola databook for the CPU in this iBook, I won't know
whether the microcode inside that CPU is compliant with the spec, or
whether it might contain back doors which can be used against me
somehow.  It's technically conceivable that the CPU microcode on this
machine understands MacOS, ssh, vt100, and vi, and is programmed to
detect BWA HA HA! arguments about trusted computing and invisibly
insert errors into them.  I would never know.
This problem exists with or without Palladium.  Palladium would
provide a new place where a particular vendor could put
security-critical (trusted) logic without direct end-user
accountability.  But there are already several such places in the
PC.  I don't think that trust-bootstrapping problem can ever be
overcome, although maybe it's possible to chip away at it.  There is
a much larger conversation about trusted computing in general, which
we ought to be having:
What would make you want to enter sensitive information into a
complicated device, built by people you don't know, which you can't
take apart under a microscope?
That device doesn't have to be a computer.

@_date: 2002-01-26 23:22:33
@_author: Seth David Schoen 
@_subject: Limitations of limitations on RE/tampering (was: Re: biometrics) 
I'm curious about the theoretical limits of tamper-resistance and
reverse-engineering resistance.  Clearly, at any given moment, it's
an arms race.  But who is destined to win it in the long run?
I was very interested in a result which Prof. Steven Rudich of CMU
told me about -- the non-existence of obfuscators.  There is a
research paper on this:
"[A]n obfuscator O [...] takes as input a program (or circuit) P and
produces a new program O(P) that has the same functionality as P yet is
'unintelligible' in some sense. [...]  Our main result is that, even
under very weak formalizations of the above intuition, obfuscation is
Rudich said that his collaborators' impossibility proof hadn't stopped
commercial software vendors from continuing to develop obfuscation
techniques, but that's not surprising.  (I do enjoy mentioning this
impossibility proof whenever I hear about obfuscation, though.)
The result applies both to software obfuscation and to circuit
obfuscation.  (I need to think a bit more about its scope.  As I
understand it, there _do_ exist obfuscated programs -- which perform
a function but which can't be "understood" -- but there are just no
reliable algorithmic techniques for obfuscating an arbitrary piece of
Now, programs can attempt to tell whether they're being run under
debuggers, but, at least in open-source operating systems, there's no
ultimately reliable way to decide.  When you ask the operating system
"am I traced?", it can just say "no".  Simulators and debuggers are
becoming a lot more sophisticated, and there's no indication that
"software protection" is any more effective now than it was in the
1980s.  (The DMCA has made it more "effective" in a certain sense,
by creating, as Judge Kaplan said, "a moat filled with litigators
rather than alligators".)  There are also really cool things like
But this obviously doesn't say anything about tamper-resistance at a
physical level, in hardware, because of devices which can destroy
themselves, whether with thermite or with some active tamper-detection
circuit, when they "believe" that some probing activity has exceeded a
particular threshold.  Software simply can't do that unless it can
communicate with some tamper-proof authority (a hardware dongle or a
revocation entity).
On the other side, probing and imaging techniques have been getting
more sophisticated all the time.  Medical technology has produced all
kinds of non-invasive scanners (CT, MRI, SPECT, PET, etc.) and
researchers have been using microscopes to look inside of many
"tamper-proof" smart cards.  A device which carries its own power
supply can _try_ to detect that it's been scanned (the equivalent of
software detecting that it's being traced or running on a virtual
machine), and certainly many of the medical imaging techniques use
some sort of active irradiation or otherwise provide a lot of energy
which a device could detect (assuming there's no way to disable the
device's power supply or otherwise destroy the tamper-detection logic).
So maybe devices could be made I understand that the state of the art in hardware favors the reverse
engineers in most cases, but a lot of people still have confidence in
the ability of hardware engineers to create genuinely tamper-resistent
devices.  And some people believe in particular contemporary designs
and products.
A couple of years ago, I heard about a technique called
interaction-free measurement, which uses quantum physics to measure or
photograph/image an object _without touching it or interacting with it
in any way_ (from the point of view of classical physics); this was
colloquially called "seeing in the dark" because no light or other
electromagnetic radiation need end up being incident on the target
Does IFM justify the conclusion that tamper-resistance in hardware
will never be achieved?  (There could still be an arms race over
costs and benefits.)

@_date: 2003-04-02 18:16:18
@_author: Seth David Schoen 
@_subject: Logging of Web Usage 
I'm skeptical that it will even take "a few hours"; on a 1.5 GHz
desktop machine, using "openssl speed", I see about a million hash
operations per second.  (It depends slightly on which hash you choose.)
This is without compiling OpenSSL with processor-specific optimizations.
That would imply a mean time to reverse the hash of about 2100 seconds,
which we could probably improve with processor-specific optimizations
or by buying a more recent machine.  What's more, we can exclude from our
search parts of the IP address space which haven't been allocated, and
optimize the search by beginning with IP networks which are more
likely to be the source of hits based on prior statistical evidence.  Even
without _any_ of these improvements, it's just about 35 minutes on average.
I used to advocate one-way hashing for logs, but a 35-minute search on
an ordinary desktop PC is not much obstacle.  It might still be
helpful if you used a keyed hash and then threw away the key after a
short time period (perhaps every 6 hours).  Then you can't identify or
link visitors across 6-hour periods.  If the key is very long,
reversing the hash could become very hard.
The logging problem will depend on what server operators are trying to
accomplish.  Some people just want to try to count unique visitors;
strangely enough, they might get more privacy-protective (and comparably
precise) results by issuing short-lived cookies.

@_date: 2003-04-19 13:35:44
@_author: Seth David Schoen 
@_subject: the futility of DRM (Re: DMCA Crypto Software) 
By the way, there are entertainment companies currently trying to prevent
the latter (previously called the "analog hole", now called the
"analog reconversion problem").

@_date: 2003-12-18 11:22:41
@_author: Seth David Schoen 
@_subject: Difference between TCPA-Hardware and other forms of trust 
There's the rub.
The prevalence of systems that can be trusted by third parties who do
not trust their owners affects what applications are possible, and it
affects the balance of power between computer owners and others.
If very few such systems are deployed, it would be absurd to say that
"the music company would refuse to do business with you" -- because the
music company has to do business to keep its doors open.  Businesses
that refuse to serve customers will not last very long.
In the entertainment case, the antagonism is already clearly expressed.
("Open war is upon you", as I recall Theoden being told in that movie
last night.)  The more so-called "legacy" systems do not do DRM or do
not do it very well or easily, the more difficult it is for publishers
to apply DRM systems and succeed in the market.  The more systems do
DRM natively, or easily or cheaply, the easier it is to be successful
publishing things restricted with DRM.  In either case, the publishers
still have to publish; before the creation of DVD-A and SACD,
publishers of audio CDs couldn't very well say "CD-DA, we hates it!
Nasty, tricksy format!" (sorry, um) "We are going to stop publishing
in the CD-DA format because it isn't encrypted."  Even today, they
would be hard-pressed to do so, because DVD-A and SACD players are
extraordinarily rare compared to audio CD players.
The question of whether the supposed added profit that comes with being
able to enforce DRM terms provides an important "creative incentive"
comparable to that provided by copyright law goes back to the era
immediately before the adoption of the DMCA, where the Bruce Lehman
White Paper argue that it did (that copyright law's incentive was
becoming inadequate and an additional control-of-the-public and
control-of-technology incentive would be required).  Indeed, the group
that pushed for the DMCA was called the Creative Incentives Coalition,
and it said that thus restricting customers was really all a matter of
preserving and expanding creative incentives.
I think Bruce Lehman was wrong then and is wrong now.  On the other
hand, the _structure_ of the argument that the prospect of restricting
customers provides an incentive to do something that one would not
otherwise do is not incoherent on its face.  The interesting question
about remote attestation is whether there are (as some people have
suggested) interesting and important new applications that customers
would really value that are infeasible today.
For example, it has been argued by Unlimited Freedom that there would
be incentives to invest in useful things we don't have now (and things
we would benefit from) only if attestation could be used to control
what software we used to interact with those things.
In the entertainment case, though, there is already a large
entertainment industry that has to sell into a base of actually
deployed platforms (unless it wants to bundle players with
entertainment works) -- and its ability to refuse to do business with
you is constrained by what it can learn about you as a basis for
making that decision.  It's also constrained if its rationale for
refusing to sell to you would also imply that it needs to refuse to
sell to millions of other people.  Only if enormous numbers of people
in the future can preserve the benefit of creating uncertainty about
their software environment's identity will entertainment publishers
and others lack the ability to discriminate against people who use
disfavored software.
I find this puzzling because I don't see how the leveraging happens.
I'm puzzled as a pure matter of cryptography, because if my computer
doesn't come with any tokens that a third party can use to establish
trust in it, I don't see how it can prevent me from doing a
man-in-the-middle attack whenever someone tries to exchange a key with
In the TCG model, there are already keys shipped with the PC, inside
the TPM.  These keys make signatures that have a particular
significance.  People who are concerned about the effects of attestation
have proposed changes in which the meaning of these signatures is changed
slightly, or in which the keys are disclosed to the computer owner, or
in which the keys are not preloaded at all.  These changes are
proposed specifically in order to preserve an aspect of the status
quo: that other people can't trust your computer without also trusting
its owner.
So if these proposals don't have that effect, I'd be glad to hear why

@_date: 2003-12-21 15:03:23
@_author: Seth David Schoen 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
The only difficulty here is the economic effect if an attestation
capability is really ubiquitous, since people you interact with can
tell whether you chose to offer them attestations or not.  (Imagine
if there was a way to tell whether someone had had an abortion in
the past by looking at her.  That would have a major effect on the
decision to have an abortion, without directly affecting the
availability of abortion services at all.  It would all be a matter
of secondary effects.)
My two favorite examples currently are the polygraph machine and
genetic screening.  Of course, both of these are opt-in technologies;
nobody will come up to you on the street and force you to take a
polygraph, and nobody will come up to you and stab you to collect
blood for genetic screening.  (There are supposedly a few cases of
genetic testing being done surreptitiously, and that might become
more common in the future.)  On the other hand, people can conceivably
condition certain interactions or benefits on the results of a
polygraph test or a genetic test for some condition.  The most obvious
example is employment: someone can refuse to hire you unless you
submit to one or the other of these tests.  As a result of various
concerns about this, Congress has now regulated the use of both of
these technologies by employers in the U.S.  Whether or not people
agree with that decision by Congress, they should be able to see that
the very _existence_ of these opt-in technologies, and their potential
availability to employers, would make many prospective employees worse
off than they would have been if the technologies had not been
(Oh, and then there are the ways insurers would like to use genetic
tests.  I'm sure some insurers wouldn't have minded subjecting their
insureds to lie detector tests, either, when asking them whether they
ever smoke.)
On-line interactions are becoming terribly frequent, and it's common
for on-line service providers to wish to know about you (or know what
software you use or try to get you to use particular other software
instead).  In the current environment you can use the personal
firewalls you mention, and a host of other techniques, to prevent
on-line services from learning much more about you than you would like
them to -- and in principle they can't determine whether or not you're
using the programs they would prefer.
John Gilmore recently quoted in this thread the TCPA white paper
"Building a Foundation of Trust in the PC", which says
If that happens, publishers and service providers can use their
leverage over software choices to gain a lot more power over computer
owners than they have right now.
"Building a Foundation of Trust in the PC" suggests this:
Some people may have read things like this and mistakenly thought that
this would not be an opt-in process.  (There is some language about
how the user's platform takes various actions and then "responds" to
challenges, and perhaps people reasoned that it was responding
autonomously, rather than under its user's direction.)
But it's clear from the context why the computer user is opting in:
because it's a condition of access to the service.  If you don't
attest at all, that's treated as giving an unacceptable answer.
There might seem to be a certain circularity here (you can only get
people to give attestations if you can deny them access to the service
if they refuse, and you can only deny them access to the service for
refusing if people are generally willing to give attestations).  But I
think it's mainly a question of network effects.
Your desire to "attest about the state of the software to myself, the
machine owner" could be met in various ways without increasing other
people's potential leverage over what software you use.  I have
suggest that you could have a TPM that allows you deliberately to attest
to a software environment that is different from your real software
environment.  There are other possibilities.  Ka-Ping Yee suggested
that, when you buy a device with a TPM, you should get a copy of all the
secret keys that were preloaded in your TPM; another alternative would be
not pre-loading any keys at all.  In these models, not only can you
get an attestation through some local UI, as you suggest, but you can
also give an attestation to a machine or service that you operate --
or that someone else operates -- whenever you have reason to believe
that the attestation will be used in a way that will benefit you.

@_date: 2003-12-23 13:42:22
@_author: Seth David Schoen 
@_subject: example: secure computing kernel needed 
I'm a little confused about why you consider these similar.  They seem
very different to me, particularly in the context of mass-market
transactions, where a service provider is likely to want to deal with
"the general public".
While it's true that service providers could try to use some demand
some sort of PKI credential as a way of getting the true name of those
they deal with, the particular things they can do with a true name are
much more limited than the things they could do with proof of
someone's software configuration.  Also, in the future, the cost of
demanding a true name could be much higher than the cost of demanding
a proof of software identity.
To give a trivial example, I've signed this paragraph using a PGP
clear signature made by my key 0167ca38.  You'll note that the Version
header claims to be "PGP 17.0", but in fact I don't have a copy of PGP
17.0.  I simply modified that header with my text editor.  You can tell
that this paragraph was written by me, but not what software I used to
write it.
As a result, you can't usefully expect to take any action based on my
choice of software -- but you can take some action based on whether
you trust me (or the key 0167ca38).  You can adopt a policy that you
will only read signed mail -- or only mail signed by a key that Phil
Zimmermann has signed, or a key that Bruce Lehman has signed -- but
you can't adopt a policy that you will only read mail written by mutt
users.  In the present environment, it's somewhat difficult to use
technical means to increase or diminish others' incentive to use
particular software (at least if there are programmers actively
working to preserve interoperability).
Sure, attestation for platform identity and integrity has some things
in common with authentication of human identity.  (They both use
public-key cryptography, they can both use a PKI, they both attempt to
prove things to a challenger based on establishing that some entity
has access to a relevant secret key.)  But it also has important
differences.  One of those differences has to do with whether trust is
reposed in people or in devices!  I think your suggestion is tantamount
to saying that an electrocardiogram and a seismograph have the same
medical utility because they are both devices for measuring and
recording waveforms.
This application is described in some detail at
I haven't seen a more detailed analysis of how attestation would
benefit particular designs for anonymous communication networks
against particular attacks.  But it's definitely true that there are
some applications of attestation to third parties that many computer
owners might want.  (The two that first come to mind are distributed
computing projects like SETI at Home and network games like Quake,
although I have a certain caution about the latter which I will
describe when the video game software interoperability litigation I'm
working on is over.)
It's interesting to note that in this case you benefit because you
received an attestation, not because you gave one (although the
network is so structured that giving an attestation is arranged to be
the price of receiving one: "Give me your name, horse-master, and I
shall give you mine!").
The other thing that end-users might like is if _non-peer-to-peer_
services they interacted with could prove properties about themselves

@_date: 2003-12-23 13:53:39
@_author: Seth David Schoen 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
The main answer is that the TPM will let you disable attestation, so
you don't even have to use a firewall (although if you have a LAN, you
could have a border firewall that prevented anybody on the LAN from
using attestation within protocols that the firewall was sufficiently
familiar with).
When attestation is used, it likely will be passed in a service like
HTTP, but in a documented way (for example, using a protocol based on
XML-RPC).  There isn't really any security benefit obtained by hiding
the content of the attestation _from the party providing it_!
Certainly attestation can be used as part of a key exchange so that
subsequent communications between local software and a third party are
hidden from the computer owner, but because the attestation must
happen before that key exchange is concluded, you can still examine
and destroy the attestation fields.
One problem is that a client could use HTTPS to establish a session
key for a session within which an attestation would be presented.
That might disable your ability to use the border firewall to block
the attestation, but you can still disable it in the TPM on that
machine if you control the machine.
The steganographic thing is implausible because the TPM is a passive
device which can't control other components in order to get them to
signal information.

@_date: 2003-12-31 01:20:12
@_author: Seth David Schoen 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
Thanks for the kind words.
Nikita Borisov has proposed an alternative to Owner Override which
Ka-Ping Yee has called "Owner Gets Key", and which is probably the
same as what you're discussing.
Most TC vendors have entered into this with some awareness of the
risks.  For example, the TCPA whitepaper that John Gilmore mentioned
here earlier specifically contemplates punishing people for using
disapproved software, without considering exactly why it is that
people would want to put themselves into a position where they could
be punished for doing that (given that they can't now!).  (In
deference to Unlimited Freedom's observations, it is not logically
impossible that people would ever want to put themselves into that
position; the TCPA whitepaper just didn't consider why they would.)
As a result, I have not had any TC vendor express much interest in
Owner Override or Owner Gets Key.  Some of them correctly pointed out
that there are interesting user interface problems associated with
making this usable yet resistant to social engineering attacks.  There
might be "paternalistic" reasons for not wanting to give end-users the
attestation keys, if you simply don't trust that they will use them
safely.  (But there's probably no technical way to have our cake and
eat it too: if you want to do paternalistic security, you can probably
then abuse it; if you want to give the owner total control, you can't
prevent the owner from falling victim to social engineering.)  Still,
the lack of a totally obvious secure UI hasn't stopped research from
taking place in related areas.  For example, Microsoft is reportedly
still trying to figure out how to make clear to people whether the
source of a particular UI element is the program they think it is, and
how to handle the installation of NGSCB trusted computing agents.
Secure UI is full of thorny problems.
I've recently been concerned about one problem with the Owner Override
or Owner Gets Key approaches.  This is the question of whether they
are particularly vulnerable to a man-in-the-middle attack.
Suppose that I own a computer with the TCG TPM "FOO" and you are a
server operator, and you and I trust each other and believe that we
have aligned interests.  (One example is the case where you are a bank
and we both want to be sure that I am using a pristine, unaltered
computing environment in order to access my account.  Neither of us
will benefit if I can be tricked into making bogus transactions.)
An attacker Mallory owns a computer with the TCG TPM "BAR".  We assume
that Mallory has already compromised my computer (because our ability
to detect when Mallory does that is the whole reason we're using
attestation in the first place).  Mallory replaces my web browser (or
financial software) with a web browser that he has modified to send
queries to him instead of to you, and to contain a root CA certificate
that makes it trust a root CA that Mallory controls.  (Alternatively,
he's just made the new web browser ignore the results of SSL
certificate validation entirely, though that might be easier to detect.)
Now when I go to your web site, my connection is redirected to Mallory's
computer, which proxies it and initiates a connection to you.  You ask
for an attestation as a condition of accessing your service.  Since I
have no particular reason to lie to you (I believe that your reason
for requesting the attestation is aligned with my interest), I direct
my computer to give you an attestation reflecting the actual state of
FOO's PCR values.  This attestation is generated and reflects a
signature by foo on a set of PCR values that show the results of
Mallory's tampering.  But Mallory does _not_ pass this attestation
along to you.  Instead, Mallory uses Owner Override or Owner Gets Key
to generate a new attestation reflecting the original set of PCR
values that FOO had _before Mallory tampered with my software
environment_.  He then generates an attestation by BAR falsely
reflecting that BAR presently has those PCR values.  (Instead, BAR's
PCR values actually reflect that Mallory is running some custom-built
MITM attack software.  But the attestation Mallory generates conceals
that fact.)
Mallory then passes his false attestation along to you in place of my
real attestation.  On the basis of his attestation, you believe that
my computer has not been tampered with and you exchange a session key
with Mallory (believing that you are exchanging it with me).  Mallory
now exchanges a session key with me, and proxies the remainder of the
encrypted connection, which he can then observe or alter.  You falsely
believe that the session key is held by my original, unmodified
software environment, where it is really held by Mallory.
I think the lesson of this example is that believing attestations
requires some out-of-band mechanism to establish trust in TPM signing
keys.  That mechanism _could be_ vendor signatures in the present TCG
scheme, or it could be some completely different mechanism in an Owner
Override or Owner Gets Key system.  (For instance, in a corporate
environment, an IT manager can generate all the keys directly, and
then knows their values.  The IT manager does not need to rely on TPM
manufacturers to establish the legitimacy of TPM signing keys.)  The
MITM attack works because the TPM manufacturer's signature is no
longer a sufficient basis to establish trust in a TPM, if the TPM
might have an Owner Override feature.
So in my example, I need to find an out-of-band way to tell you FOO's
key so that you know it and can trust it (and distinguish it from
BAR's key).  If Owner Override exists and I've never told you precisely
which TPM I'm using, all you can tell is that you got an attestation
from _some_ TPM, but that might be an attacker's TPM and not my TPM!
In the corporate environment, the IT manager knows which TPM is in
each machine and can therefore easily tell who really generated a
particular attestation.  (I'm oversimplifying in various ways, but I
think this point is right.)
This extra work is not necessarily a bad thing -- it can be seen as a
legitimate cost of making TPMs that can never be used against their
owners -- but I'm not sure it can be avoided, and it might make
attestation less useful for some applications.  In all the cases where
the application is "technically sophisticated people want to receive
attestations from computers they own and configure", it's probably
still fine, but there may be some challenges for other purposes.
Here's an ASCII art diagram of that attack (eliding many details of
key exchange protocols that don't seem relevant):
     me           Mallory          you
  ________       _________       ________
 EvilOS | --- | MITM-OS | --- | BankOS |
  --------       ---------       --------
  TPM "FOO"      TPM "BAR"
Me (to Mallory): Hi, Bank, it's me!
Mallory (to you): Hi, Bank, it's me!
You (to Mallory): Hi, sign your PCRs with nonce "ahroowah" to prevent replay.
Mallory (to me): Hi, sign your PCRs with nonce "ahroowah" to prevent replay.
Me (to Mallory): Ok.  sign(FOO, "EvilOS/ahroowah")
Mallory (to you): Ok.  sign(BAR, "NormalOS/ahroowah")
You (to Mallory): Great, you're NormalOS.  session key: encrypt(BAR, sugheequ)
Mallory (to me): Great, you're NormalOS.  session key: encrypt(FOO, heepheig)
(Again, I can't exchange a session key with you that Mallory can't
intercept because I'm running EvilOS, which deliberately fails to
notice when Mallory's SSL certificate gets substituted for the bank's.
and the bank can't benefit from this attestation because it doesn't
know whether my TPM is FOO or BAR.  If I had previously had an
out-of-band way to tell the bank that my TPM was FOO, Mallory would not
be able to carry out this attack.  Mallory still can't sign things as
FOO, and without a sufficiently clever social engineering attack,
can't get me to sign things as FOO for him.)

@_date: 2003-03-16 11:11:55
@_author: Seth David Schoen 
@_subject: Microsoft: Palladium will not limit what you can run 
The Xbox is definitely not based on NGSCB; Microsoft told EFF very
clearly last year that Palladium was still being designed and hadn't
gone into manufacturing.  The Xbox was certainly being sold then.
The Xbox was analyzed by Andrew "bunnie" Huang, who found that it was
using a sui generis security system.

@_date: 2004-01-04 02:31:49
@_author: Seth David Schoen 
@_subject: Difference between TCPA-Hardware and a smart card (was: example: secure computing kernel needed) 
You could conceivably have a PC where the developers don't trust
Linus, but instead trust the PC manufacturer.  The PC manufacturer
could have made it extremely expensive for Linus to tamper with the PC
in order to "violate [the developers'] security model".  (It isn't
logically impossible, it's just extremely expensive.  Perhaps it costs
millions of dollars, or something.)
There are computers like that today.  At least, there are devices that can
run software, that are highly tamper-resistant, and that can do attestations.
(Now there is an important question about what the cost to do a hardware
attack against those devices would be.)  It seems to me to be a good thing
that the ordinary PC is not such a device.  (Ryan Lackey, in a talk
about security for colocated machines, described using devices like
these for colocation where it's not appropriate or desirable to rely on
the physical security of the colocated machine.  Of course, strictly
speaking, all security always relies on physical security.)
I don't know how the key management works in these devices.  If the
keys used to sign attestations are loaded by (or known to) the device
owner, it wouldn't help with the case where the device owner is
untrusted.  If the keys are loaded by the manufacturer, it might
support a model where the owner is untrusted and the manufacturer is
