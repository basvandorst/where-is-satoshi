
@_date: 2014-04-16 11:32:42
@_author: Judson Lester 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
So much poorly supported hate for ASN.1 in there. Especially
surprising that they imply that JSON would be better (or two of the
headline articles in the referenced at
I'll grant that TLS is complicated, and there's some truth to there
being a misnomer, but I'm a little leery of comparisons between a
broadly distributed implementation and hypothetical or experimental

@_date: 2014-04-16 13:18:02
@_author: Judson Lester 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
So much poorly supported hate for ASN.1 in there. Especially
surprising that they imply that JSON would be better (or two of the
headline articles in the referenced at
I'll grant that TLS is complicated, and there's some truth to there
being a misnomer, but I'm a little leery of comparisons between a
broadly distributed implementation and hypothetical or experimental

@_date: 2014-04-16 13:19:09
@_author: Judson Lester 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
To be honest, I just did. I'd read a couple of the other papers
referenced and was frustrated by not finding the proof of the parser
strength required. I did run across another paper that enumerated the
same attacks without getting to math.
I think the language formalism approach has a lot of merit.
A couple of points:
First, the attacks they enumerate don't make use of the fact that DER
is context-sensitive. The example they present against X.509
specifically has to do with the fact that DER allows for embedded
nulls - and that C-related implementations don't properly handle that
case. It'd be as possible to produce a regular language with the same
Second, my intuition is that CER would be context-free - as much as
s-expressions would be. And that DER is O(n) isomorphic to CER. So if
anything, there's an direction for how to implement an X.509 parser.
But it still wouldn't solve null embeddings - but that's solvable.
(And: I'm not familiar enough with Rust to answer this off the top of
my head: you'd have to implement a check for nulls there, anyway,
The other part of my issue with the original essay is this: TLS is the
result of decades of work. I'm not saying that that's inherently
valuable, but I'm concerned that an effort to replace it will resemble
e.g. Mongo's reimplementation of SQL in JSON. I'm not impressed with
"let's switch to CurveCP, which would work in some other world where
TCP wasn't prevalent."

@_date: 2014-04-16 15:01:25
@_author: Judson Lester 
@_subject: [Cryptography] Is it time for a revolution to replace TLS? 
On Wed, Apr 16, 2014 at 2:28 PM, Viktor Dukhovni
Agreed - you'd need to use the length against the char[] - but that's
a very different usage than normal in C and C-like languages.
If only! Good to note that no matter how obvious something seems,
someone somewhere will miss it.

@_date: 2014-04-17 14:29:29
@_author: Judson Lester 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
I have to say, I'm really taking by the langsec formal approach to
input recognition. But regarding TLS in particular, I'm unclear on
their specifics. My understanding is that TLS mandates DER for ASN.1,
which is unambiguous. Further it seems to me that DER is isomorphic
with CER, which should be context-free, right? (This is leaving aside
the issue of x.509 CNs)

@_date: 2014-04-17 14:50:38
@_author: Judson Lester 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
Related (from the IETF discussion):
The point being: the actual TLS wire protocol uses an ad hoc
definition rather than e.g. ASN.1.

@_date: 2014-12-15 23:56:01
@_author: Judson Lester 
@_subject: [Cryptography] Any opinions on keybase.io? 
I wrote something of a polemic against it a few months ago. I think the
fundamental point they're making (crypto should be easier to use, and keys
easier to exchange) is a good one, but the conclusions (centralize with us,
send your private key over the network, use our encryption algorithm, etc
etc) they draw are non sequitors and distressing.
There was an open project I did a little bit of work on called keygraph - I
think it stalled out a bit, though.

@_date: 2014-12-17 01:24:20
@_author: Judson Lester 
@_subject: [Cryptography] Any opinions on keybase.io? 
alpha" explains. As a system, it is designed with remote generation and
network transmission of a private key. These features are optional,
granted, but they're still incredible misfeatures. I would love to be able
to interpret their inclusion in the design as anything but incompetence or
But, without those features, there's no reason to centralize the GPG
process, and without centralization there's no startup.
Anyway, the polemic I mentioned:

@_date: 2014-12-17 18:15:01
@_author: Judson Lester 
@_subject: [Cryptography] Any opinions on keybase.io? 
Everyone deserves trustworthy communications online.
I don't have a design that makes that possible for everyone. I don't know a
design that does. Keybase isn't it. My deepest objection to Keybase is that
it promises that and manifestly fails to deliver.
(I was working with the keybase group on a decentralized standard - the
popularity of Keybase with technical folks discouraged me. But, yeah, it'd
be command line stuff, most likely.)
One significant wrinkle is right there in your challenge: "willing to run a
trusted binary" and "only web browsers."
But more significantly, I don't know *how* to design a system that can
protect participants without putting a significant responsibility on them -
to understand the protocols, to review the programs they're using, to do
*something* more than tick the 'secure' box.
And that's by no means a condemnation of computer users everywhere. It's
the voice of despair, because I think we deserve security, and to be able
to do something other than maintain our security all the time.

@_date: 2014-06-04 14:16:39
@_author: Judson Lester 
@_subject: [Cryptography] Is it mathematically provably impossible to 
There was a really interesting point you don't address though: if the
notarization system became untrustworthy, what do you do then? Actors
in this currency system have a significant fraction of their material
wealth pinned to that trust - how do you recover if 16 notaries report
one ledger and the others report a different one? (Or 12/20 for that
matter.) Worse, what if the split ledger is revealed as a falling out
of a colluders, so even the last agreed ledger is in question?

@_date: 2014-06-04 14:20:15
@_author: Judson Lester 
@_subject: [Cryptography] Is it mathematically provably impossible to 
Sincerely curious: How do you demonstrate that the proof and the code
are both accurate representations of the specification? I'm thinking
back to a subverted GCC that was able to protect itself by modifying
programs as they were built. In other words, how do we know that the
proof of the specification applies to the proof of the code?

@_date: 2014-05-26 11:23:34
@_author: Judson Lester 
@_subject: [Cryptography] Langsec & authentication 
I've been fascinated to discover and read about the langsec movement
in the wake of heartbleed. The fundamental ideas seem sound, but
there's at least one question I'm have but haven't seen addressed
As I understand it, the langsec position is that specifying your
protocol language to be as easy to parse as possible, in Chomsky
hierarchy terms, has direct security implications - if the uppermost
surface of your networked application doesn't have to include a Turing
machine, that severely limits an avenue of attack on that application.
What confuses me is trying to align this with a principle of
cryptography that you should only authenticate what you mean, as
opposed to authenticating a particular series of bytes, especially in
the face of langsec sites that recommend the use of JSON after having
argued convincingly against ASN.1 DER.
Here's what I mean: moving on immediately from JSON, it seems to me
that any language that includes key-value pairs, to be safe to
authenticate, has to guarantee that the keys in any mapping form a
set. Otherwise I can produce two documents that *mean* the same thing
even though they have different bytes - because in foo=bar,foo=baz,
our interpretation has to choose a meaning - does foo == bar, baz or
maybe [bar,baz]?
But I think that requiring that the keys belong to a set pushes the
language into context sensitivity i.e. as bad as ASN.1 DER.
Conversely, I can't think of a system I use regularly that doesn't
define a language that doesn't either use set-of-keys, should use
set-of-keys or repeat-implies-array, all of which imply
context-sensitivity, I think. On the other hand, removing key/value
from a protocol would make it comparatively easy to reduce to a
regular language.
Am I onto something here? Well addressed elsewhere?

@_date: 2014-05-27 11:20:52
@_author: Judson Lester 
@_subject: [Cryptography] Langsec & authentication 
I was asking. A shell based config file would serve as a good example
I think:
So I write a simple parser - it matches key=value pairs and assigns
the pairs into e.g. a Ruby Hash as they come in. And the full protocol
looks like: client produces the list of pairs, client MACs the list,
server checks the MAC, server parses the list and takes action.
But this document:
has the same outcome in the exchange as
But they authenticate differently. Which implies that there's a list
of pairs that mean the same thing as
but authenticate as if
because there's a chain of pointless assignments of FOO. The solution
to that would be (on way or another) to constrain the lists language
so that keys have to be part of a set, i.e.
FOO=value2 XXX error!
because FOO has been repeated.
So I redefine the language: it's a list of key=value, but no key can
be repeated. I *believe* that language is context sensitive, (where
before it was regular) and the langsec OWS people are going to camp
out on my lawn. But if I don't make it context sensitive, there's a
message authentication problem, because multiple valid documents have
the same authentication code.
Also granted: in this example, it would be easy enough to check for
the presence of the key in the host language data structure on each
match - but now you're pulling some of the parsing out into the
application code, which might be acceptable in some circumstances?
For instance, a language like DER that uses byte counts can be
transformed into CER with field-end markers trivially, and therefore
reduced on the server side from context-sensitive to context-free, no?
Is that okay? If not, why not if checking for key uniqueness is okay
outside the parser?

@_date: 2014-05-27 12:30:06
@_author: Judson Lester 
@_subject: [Cryptography] Langsec & authentication 
On Tue, May 27, 2014 at 12:08 PM, Stephan Neuhaus
But, and this is the other half of my dilemma, authenticating
ambiguous blobs of data opens a giant hole in your MAC system: here's
two documents that mean "re-order coffee" and "nuke North Korea" with
the same MAC. Oops. And while that's theoretically possible
regardless, it becomes much easier to do if there's many many ways to
say "nuke North Korea."

@_date: 2014-05-27 12:59:06
@_author: Judson Lester 
@_subject: [Cryptography] Langsec & authentication 
On Tue, May 27, 2014 at 12:53 PM, Stephan Neuhaus
So, in that case you need your protocol language that describes
messages unambiguously - so that there's only one blob that
legitimately means "nuke North Korea."

@_date: 2014-05-29 10:33:12
@_author: Judson Lester 
@_subject: [Cryptography] Langsec & authentication 
Myself, I'm a big fan of ASN.1, and especially of the canonical
encodings, but the langsec objection to PER and DER is that
length-prefix encodings are context-sensitive. My point in all this
has been: a full ASN.1 implementation includes sets, and attribute
lists, so that even without length-prefix (e.g. CER) I think full
ASN.1 would be context-sensitive - although some protocol definitions
might not be, and some set of protocol design constraints (e.g. "don't
use sets"), including using CER, might reduce the resulting protocol
language to regular.

@_date: 2014-09-13 16:54:31
@_author: Judson Lester 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
No QR scanner I've ever used, even the most janky overseas mobile app, has
ever automatically opened URLs for me. They always present the URL and
offer to open them - sometimes to my slight dismay, although the "oh right,
not taking me to an unknown website automatically is a *good thing*" gets
me past. :) I agree though, a scanner that opens a helper application
without user input should be considered broken-to-malware.

@_date: 2016-08-25 23:36:40
@_author: Judson Lester 
@_subject: [Cryptography] Hashing with CTR mode? 
This is likely a dumb question, but it came to mind as I was reading the
paper describing SWEET32. There, the authors assert that because block
ciphers are random permutations as opposed to random functions, their
collision properties under CTR mode are the same as under CBC.
Parenthetically, they relate that the collision time increases to 2^n if
you used a random function.
Based on that, I wonder: what's wrong with using CTR where the "block
cipher" is SHA256(IV | key)? The intent is to use the hash as a keyed
random function - if there's a problem with the naive approach here, can
the construction be done?

@_date: 2016-07-01 17:42:26
@_author: Judson Lester 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
I always imagine that it may be useful to have this kind of conversation in
a public forum, not because an arguer's mind will be changed, but because a
member of the audience may learn something.
In this case, I'm very glad to have read this piece, and to learn of its
existence, for which I thank you.

@_date: 2016-06-30 23:14:23
@_author: Judson Lester 
@_subject: [Cryptography] Proposal of a fair contract signing protocol 
It appears that your "fairness" is either vacuous or paradoxical.
In the execution of your protocol, after step 1, Alice is committed to sign
the executed contract when Bob completes step 2. Either this is equivalent
to Alice being committed when Bob is not, or it is not equivalent.
If the commitment to sign is equivalent to signing, the protocol isn't fair
precisely because the scenario without this protocol isn't fair: Alice is
committed where Bob isn't.
After step 2, Bob is committed to the contract. If Alice's commitment to
sign is not equivalent to signing the contract, then Bob is committed and
Alice is not, which is a violation of the fairness you've defined, with
Alice and Bob's roles reversed.
I conclude: either "commitment to sign" is equivalent to signing, in which
case your protocol is unfair to Alice, or it is not and it is unfair to
Bob. Any other interpretation is of a property of the transaction so vague
as to be meaningless.
This isn't a novel formulation - I quite liked the general impossibility
result, which this is merely a specialization of to your protocol.

@_date: 2017-11-13 21:56:40
@_author: Judson Lester 
@_subject: [Cryptography] Is ASN.1 still the thing? 
There's the langsec argument to be made that DER is fairly complicated to
parse, and as a result many critical errors have been found in popular ASN
parser generators. Specifically, the theory is that simpler Chomsky
categories of languages should be used as protocol encodings, and that DER
is context free. The largest proponents of this position, however, suggest
using JSON on the grounds that it is context-sensitive - a claim which I've
always been idly sure collapses if you insist on a normalized form, or even
that fields not repeat.
That said, using a regular language for a secure protocol seems sensible,
and the parsers have been with us for a long time.

@_date: 2018-08-09 10:30:47
@_author: Judson Lester 
@_subject: [Cryptography] PGP -- Can someone help me understand something? 
Hey Matt,
might search for that and AES (which is the most commonly used block cipher
in PGP.) A well designed cipher should be as resistant to known-plaintext
as brute force. In other words, it should be as efficient to solve for x as
to simply try keys until you get a result.

@_date: 2018-08-22 17:02:25
@_author: Judson Lester 
@_subject: [Cryptography] Throwing dice for "random" numbers 
A little Googling about von Neumann and dice led me to this
 which describes a
very cute algorithm for getting fair resutls from biased dice. Since a lot
of the discussion here has been "assuming perfect dice" and you describe a
cheap pack of 12, which are likely far from perfect.
Essentially, partition the dice as suggested by John Denker - an egg carton
seems ideal. Roll a few times, recording the results for each die. Result
the lists of results to a list of *result ranks* - e.g. 3 2 6 -> 2 1 3 and
4 4 5 -> 1 1 2. Each such list is indexed into a canonical order of rank
lists (e.g. 123, 132, [213], 231, 312, 321 -> 3) and the index is the
result for that die for that set of rolls. Consume dice based on the "size"
of the fair die (i.e. 326 is a result of a 6 sided fair die, but 445 is on
a 3 sided die) until the sum of the sides is at least your required range.
The paper also describes how to convert the results into fair bitstrings,
which might let you truncate at the point where you have enough bits, where
the above summing solution would demand that you discard the whole result
if it were greater than your intended range.
Alternatively (and on increasingly shaky ground), you might discard final
"fair dice" until you found one of the right size to get you to your needed
range. I think if the algorithm for choosing which dice to include were
deterministic for a particular result, you might still get good random
values, but I'd be shocked if no one here differed on that point.
