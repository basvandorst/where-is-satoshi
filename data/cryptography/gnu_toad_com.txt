
@_date: 2001-12-10 21:04:31
@_author: John Gilmore 
@_subject: FreeSWAN & US export controls 
Anonymous said:
(From the pulpit:)
Once we kick John Asscroft's unconstitutional ash outta town, bush
George Bust along with more than a thousand other innocents, and
eliminate the spectre of Judd Gregg and other retrograde stalinists
're-regulating' US crypto, then we'll think about polluting the
precious bodily fluids of worldwide freeware privacy protection with
the stench of US crypto policy.  It probably won't happen for a few months.
Or hadn't you noticed that the US government is not in much of a mood
to follow the constitution or to tolerate dissent or privacy among the
sleepy sheeplike citizens?  They're doing their best to stamp that
radical stuff out right here in the USSA, let alone let it cross the
border into parts of the world that they don't have firmly under their
thumb.  Less than 100% support for every paranoid and senseless twitch
of the current Administration is a demonstration not not only of
treason but of active support for terrorism, which everyone knows is a
terrible thing except when the US or Israel or Great Britain does it.
Anybody reading this mailing list is already gonna be first up against
the wall once the joy of arresting immigrant movers as 'terrorists'
fades, and spying on 'domestic political groups' become fair game.
Your packets are already in the lint screen on that big, big vacuum
cleaner.  And our new policy of maximum sentences for trivial
'crimes', like forgetting to file some form, reduces the expense and
bother of actually trying suspects for the crimes that the agencies
suspect them of.  Of course you can confront your accusers!  Did you or
did you not jaywalk across Route 1 last July, Mr. May?
The reason I started the IPSEC-for-Linux project those many years ago
was because Linux kernel releases used to be built in free countries,
unlike the releases of most other operating systems.  Now they aren't.
Perhaps mr. or ms. 'anonymous' and the primary kernel developers
didn't spend seven years making a principled tilt at the windmill of
NSA's export controls.  We overturned them by a pretty thin margin.
The government managed to maneuver such that no binding precedents
were set: if they unilaterally change the regulations tomorrow to
block the export of public domain crypto, they wouldn't be violating
any court orders or any judicial decisions.  I.e. they are not BOUND
by the policy change.  They changed it "voluntarily", in order to
sneak out of the court cases by the back door.  Even today it is
sometimes said that once Dan Bernstein ends his court case (which
still continues today), the NSA is ready, willing, and able to slap
the controls right back on.  And it would take months or years in
court -- and lots more volunteer citizen money spent for freedom,
while the bastards spend tax money to lock us up -- to get the
controls removed again.  If the judges haven't changed their minds in
the meantime.
(You may have noticed that last month, the Second Circuit Court of
Appeals accpted Judge Kaplan's half-lies-half-truth judgment 3-0 in
the 2600 case appeal: Yes, absolutely, software is First Amendment
protected speech.  But no, somehow the First Amendment really doesn't
mean what it means elsewhere; of *course* they can regulate the
publication of software on flimsy grounds.  Like that sometime later,
somebody somewhere might potentially be somewhat hurt by something
somebody else does with the software, if we don't eliminate that
option by restricting the publication of that software now.  Suppose
the next crypto export court case happens in NY rather than CA?  EFF
would be proud to defend John Young and Perry Metzger, but all its
lawyers might be in prison, charged by John Asscroft with "aiding
terrorists by eroding our national unity and diminishing our
Make my day.
PS: Of course, the only software worth wasting your time on comes from
those macho dudes of the U.S. of A.  Those furriners don't even know
how to speek the lingua proper, let alone write solid buggy code like
Microsoft.  High crypto math is all Greek to them.  It's just lucky
for Linus that he moved to the US, otherwise we'd all know his furrin
software was crap too, even tho he tricked us by cloning it from Bell Labs.

@_date: 2001-12-17 17:20:23
@_author: John Gilmore 
@_subject: [DailyRotten] FBI requests worm-built password log  
Steve, the FBI doesn't care about what the law says.  Their big
concern is what they can get away with.  If having two million
peoples' passwords, stolen by a worm, lets them get away with more
stuff, then they'll go for it, laws or no laws.
Who's going to prosecute them if they violate the law before getting
around to changing it?  John Asscroft?  He's sworn to uphold the
Constitution, and of course none of his proposals would ever violate
anyone's civil rights.  But since his view seems to be that we never
had any rights anyway, he can't very well diminish them, though he is
desparately looking for the boundaries just in case there are any left.

@_date: 2001-12-21 11:54:04
@_author: John Gilmore 
@_subject: Stegdetect 0.4 released and results from USENET search available  
Niels & Peter, congratulations on finding no secret messages.  This is
why computers are getting faster -- so we can spend more and more time
searching out the lack of any information being communicated.
An obvious step is to extend your detector to handle other formats
besides JPEG.  That would involve more 'research' than merely running
it on other collections of images (e.g. JPEGs pulled from the Web in
the Internet Archive collection, or from your own crawler).
[Other people can also do the work of running your publicly released
software against other collections.  It would take more talent to
write something that processes other formats.]
By the way, I'm interested in what "steganographic" messages you are
finding in the plaintext tags in JPEG files.  I've heard that some
cameras mark each photo with the serial number of the camera, date,
etc.  You can probably also detect what model of camera produced the
image (based on exactly what tags it puts in the image, whether
there's a thumbnail, what the filename is, etc).  ("Jpegdump" provides
an easy way to see these tags.)  Remember how Microsoft Word documents
encode the Ethernet address of the PC on which they were created, and
how this has been used in several high-profile cases to track
documents to individuals?  I am a lot more concerned about popular
cameras that spy on their own users, than I am about the occasional
subliminal message sent through the Usenet.  It would be useful to
have a tool that removes all the nonessential tags from a jpeg file, a
'stegremover' to delete any spyware that your camera has left behind,
as well as a detector, and a "hall of shame" page for manufacturers
who are building that spyware.
PS: Cypherpunks, where *are* you putting your secret messages?  Give
us a hint!  Surely *somebody* in this crew must be leaving some
bread-crumbs around for Niels and NSA to find... :-)

@_date: 2001-12-30 14:59:10
@_author: John Gilmore 
@_subject: Steganography & covert communications - Between Silk and Cyanide 
Along these lines I can't help but recommend reading one of the best
crypto books of the last few years:
This wonderful, funny, serious, and readable book was written by the
chief cryptographer for the 'nefarious organization' in England which
ran covert agents all over Europe during WW2 -- the Special Operations
Executive.  He found upon arriving (as a teenager) that agents were
constantly dying in the field because of poor codes and poor
encryption and radio transmission practices.  Their bad systems had
been penetrated for years, and in some countries such as the
Netherlands, all of their agents had been killed or captured by the
Germans.  He shored up their poor systems until he could work around
the bureacracy to get them replaced.  He taught the receiving code
clerks in England how to decode even garbled messages, rather than
asking agents to re-send them.  (Re-sends of the same text gave the
enemy even more trivial ways to crack the codes.)  He trained each
outgoing agent in good coding practices, then watched heartbroken as
many were captured.  He independently reinvented one-time pads, and
had them printed on silk.  They could be sewn into the linings of
clothing for non-detection even during searches by the enemy, and so
that as each part was used, it could be cut off and burned to keep
previous messages secret (providing forward secrecy).
Leo Marks died almost a year ago, but fortunately he wrote down much
of the practical knowledge that came from making and breaking codes
for a covert organization working in a very hostile environment.  Here
is his AP obituary:

@_date: 2001-07-28 13:34:00
@_author: John Gilmore 
@_subject: Criminalizing crypto criticism  
The anonymous poster's legal analysis was not particularly novel.  It
states that the "exemptions" in the DMCA actually cover the things
that they were supposedly intended to cover.  That would be a
refreshing change if it were true, but the law is full of weasel words
and exemptions to the exemptions.  Only accredited researchers, not
cypherpunks, can do research, for example.  And you're only exempt if
you tell the company first, so they know to sue you before you do the
research, rather than after the results are leaking out to the public.
Neither my opinion nor the poster's opinion controls, though.  What
matters is what the judges will say, and how expensive it is to
ordinary researchers to find out.  In the 2600 case, what the judge
said is that even if Jon Johansen might have been able to reverse-
engineer DVD players under an exemption (an issue that he didn't
decide), 2600 Magazine was unable, under the statute, to publish even
*A LINK* to Jon's results.
The judge swept aside all the clauses like:
The other side argued in the 2600 appeal that this was a standard
"savings clause" inserted in the legislation and was not intended to
mean anything.  It goes like this: either the law is constitutional or
it isn't.  If it is constitutional, this clause is inoperative, since
clearly those Constitutional rights weren't diminished.  If the law
violates the Constitution, then the Constitution, not the statute,
controls what rights the public has; again this clause doesn't.  The
judge agreed with the government and Hollywood that it was clearly put
in there to "buy off" some opponents of the DMCA and didn't have any
legal effect.  The only minor issue is that THOSE SUCKERS ACTUALLY
BELIEVED IT, dropped their opposition, and let the DMCA become law.
But that wasn't the judge's problem -- only the defendant's.
Because the Felten case so clearly shows what's wrong with the DMCA,
RIAA is desparately trying to convince the court that it need not,
indeed cannot, make any decision in the Felten case.  Therefore
SDMI/RIAA is lying to the public and the court by saying that it
never, *ever*, intended to sue or threaten.  It was merely informing
people about their rights, you see.  They have moved to dismiss the
case on the grounds that "we agree with the other side's legal
analysis, so there's no issue for a court to decide."  They only agree
long enough to get out of that courtroom, then they'll find some way
to be disagreeable again.  The judge will decide whether to believe
them or not; the papers are still being filed about that.
Their mistakenness didn't prevent the RIAA from sending legal threats
to every author of the Felten paper, every member of the conference
committee that had decided to publish it, AND ALL OF THEIR BOSSES (one
of whom, a US Navy commander, shamefully abandoned the soldier-under-
fire who was reporting to him).  It didn't prevent Adobe from getting
its competitor Elcomsoft kicked off of four different spineless ISPs,
by sending lawyer letters alleging copyright infringement TO THE ISP,
when there was no copyright infringement going on.  Mistakes in
analysis, reconsidered a week later by Adobe, didn't prevent a US
Attorney's office from bringing charges against Dmitry.  Attorney
General Ashcroft just announced that they're setting up a dozen more
similar computer-and-copyright-prosecution task forces around the
country -- none of which will have any practical experience with the
DMCA yet.  Their mistakes are your problem, not their problem, until
YOU sue THEM.
Will everyone in the infrastructure on whom you depend be as strong as
you are in protecting your rights?  After you lose your job, your
Internet access, and your freedom of motion, because your scientific
work threatened some lawyer-infested company's business model, if you
have lots of spare money or raise lots of money somehow, you can have
your day in court, "as the above analysis makes clear".
And then maybe your judge will agree with the 2600 judge, or maybe
he'll agree with the anonymous poster.  Maybe the anonymous poster IS
Judge Kaplan and he's changed his mind.  I'll see you in court.
PS: EFF won't be able to take every case that comes along.  The
community's donations to EFF have been gratifying, useful, indeed
essential.  But there is far more money going into rabid company
lawyers than is going into EFF or anywhere else for DMCA legal
defense.  It's classic public choice economics -- the benefit of the
DMCA is concentrated in big profits to small numbers of companies,
while the harm of the DMCA is spread widely through society.  The
companies will spend a lot to get those profits, while relatively few
people will want to spend much to defend against them.  EFF will have
to pick which cases to focus on: ones where we can set precedents and
get good leverage that will ultimately help the most people.  But some
people -- I predict many people -- are going to twist in the wind or
in prison for years, before the courts or Congress are pushed into
fixing the havoc caused by rabid copyright maximalists.  So what if it
decimates our profession?  We're a tiny minority of society, and we
don't bribe any legislators.  They'll only notice that we matter after
we're gone, when their security infrastructures fall to bits.

@_date: 2001-11-03 21:15:13
@_author: John Gilmore 
@_subject: California appeals court holds that DeCSS code is protected speech 
Actually, the fact that the issue in question is a "prior" restraint

@_date: 2001-11-27 02:04:44
@_author: John Gilmore 
@_subject: cypherpunks@toad.com is going away 
The cypherpunks list degenerated a long time ago to the point where I
have no idea why more than 500 people are still receiving it every
As part of cleaning up the email system on toad.com, I plan to shut
down the cypherpunks-unedited list, which receives all the traffic
sent to cypherpunks at toad.com, within the next week or two.
I suggest that anyone who wants to talk or listen about encryption
should send mail to:
with a one-line plain text message saying "subscribe".  This will
begin the process of subscribing them to the Cryptography mailing
list, which is edited to remove irrelevant postings and to keep the
volume down and the discussion focused.  (I tried to do this with the
cypherpunks list some years ago, but was shouted down by people who
complained of "censorship".  So I just left it unedited, with the
expectable result that serious discussions deserted it.)
If you were subscribed to the cypherpunks-unedited at toad.com list because
you like to collect spam, talk with me personally and I'll see if I can
help you.  I have a large collection :-).
The old "cypherpunks-announce" list was superseded many months ago by
"meetingpunks at cryptorights.org".  Cypherpunks-announce is no longer in
There remains a single encryption-related mailing list on toad.com,
"coderpunks" which is for people who write code.

@_date: 2001-10-01 14:48:47
@_author: John Gilmore 
@_subject: New encryption technology closes WLAN security loopholes  
The Linux FreeS/WAN ipsec implementation has been working on this
issue.  The fundamental problem is that as specified, IPSEC
requires manual setup of each tunnel at both ends.  If N parties all
want to communicate securely, this takes (N-1)**2 tunnels and
(N-1)**2 effort.
In the existing 1.91 release at  we added another
layer above the standard IPSEC (which continues to work and
interoperate fine).  This new "opportunistic encryption" layer delays
outgoing plaintext packets, and lets the IPSEC daemon look up their
destination in the DNS.  If it finds an authentication key there, the
daemon sets up an IPSEC tunnel to the destination, and redirects
the plaintext down that tunnel.  If it finds no authentication key, it
just forwards on the plaintext packet in the clear.  Thus it takes
advantage of 'opportunities to encrypt' while not impeding
communication with unencrypted endpoints.
This reduces the work of securing N sites to a factor of N (each site
needs to publish its authentication key and install the opportunistic
software), rather than (N-1)**2.  The software sets up whatever subset
is active of the (N-1)**2 tunnels, automatically.  This not only makes
IPSEC administration scale to the size of the Internet, but also
introduces the 'fax effect' which encourages newcomers to adopt the
same method because it means they can talk securely and effortlessly
with an existing and growing population.
This software is working well in testbeds, and the support will
continue to be improved in upcoming software releases.  The protocol is
documented in an Internet-Draft.  Both the software and the I-D are
available at  .
This method protects us today against passive attacks.  As Secure DNS
is more widely deployed and operated, it will also protect us against
active attacks, by making it impossible to spoof the keys provided in the
There's almost certainly an Ethernet hop or a wireless repeater
somewhere between the two ends of the IPSEC tunnel (or the TCP
connections from your mobile node), where a mere wiring change or
firmware patch will insert a black box in the middle.  And there are
also the techniques of listening to most of the packet, and then
garbling its epilogue by jumping a conflicting transmission into the
ether, making the real recipient decide that they didn't get a good

@_date: 2001-10-12 22:28:52
@_author: John Gilmore 
@_subject: Computer Security Division Activities  
What is this lunacy about registering?  Does someone seriously think
that terrorists will attack the National Institute of Standards?
Or that if they were serious about attacking, they wouldn't just
pre-register their real or fake ID's so they'd be allowed in?
This sort of idiocy just puts a barrier between the government and
the public it is supposedly serving.  And of course does nothing to
actually improve the actual "security".  It's particularly galling since the meeting is supposedly among experts in computer security.
Wouldn't it be much better to all go home and hide under the covers?
That would make us feel much safer.  Just like children, which is the
way our government is treating us.
I will not participate in activities that require me to "identify myself"
to the government, or to be pre-vetted for attendance.

@_date: 2001-09-15 00:32:12
@_author: John Gilmore 
@_subject: Please make stable NON-US homes for strong crypto projects 
It's clear that the US administration is putting out feelers to
again ban publication of strong encryption.  See:
  The evil gnomes who keep advancing unconstitutional US anti-crypto
policies know that the current hysteria in Congress and the
Administration will not last forever.  So they will probably move very
quickly -- within a week is my guess -- to re-control encryption,
either by a unilateral action of the Administration (by amending
the Export Administration Regulations), or by stuffing a rider onto
some so-called "emergency" bill in Congress.
They maneuvered very carefully in the Bernstein case such that there
is no outstanding injunction against violating the Constitution this
way -- and even no binding 9th-Circuit precedent that tells them it's
unconstitutional to do so.  They know in their hearts that numerous
judges have found it unconstitutional, but they have proven throughout
the seven-year history of the case that they don't give a damn about
the Constitution.  Which means it may take weeks, months or years for
civil liberties workers to get a judge to roll back any such action.
Not just days.  We won the case, but they squirmed out of any
permanent restrictions -- so far.
The US government has a new mania for wiretapping everyone in case
they might be a terrorist.  There's already two bills in Congress to
make it trivial for them to wiretap anybody on flimsy excuses, and to
retroactively justify their precipitous act of rolling Carnivore boxes
into major ISPs this week and demanding, without legal authority, that
they be put at the heart of the networks.  See:
  Even more than before, we will need good encryption tools, merely to
maintain privacy for law-abiding citizens, political activists, and
human rights workers.  (In the current hysteria, mere messages
advocating peace or Constitutional rights might best be encrypted.)
The European Parliament also recently recommended that European
communications be routinely encrypted to protect them from pervasive
US Echelon wiretaps.
Some US developers, who thought such a reversal would never happen,
have built or maintained a number of good open source encryption tools
in the United States, and may not have lined up solid foreign
maintainers or home sites.
LET'S FIX THAT!  We need volunteers in many countries to mirror
current distributions, CVS trees, etc.  We need volunteers to also
act as maintainers, accepting patches and integrating them into
solid releases.
(Note that too many countries have pledged to stand toe-to-toe with the
US while they march off to make war on somebody they can't figure out
who it is yet.  If you live in one of those countries, you may
suddenly find that your own crypto regs have been sneakily altered.
Take care that each useful package has maintainers and distribution
points in diverse countries.)
I haven't kept close track of which packages are in danger.  I
suggest that people nominate packages on this mailing list, and that
others immediately grab mirror copies of them as they are nominated.
And that some of those who mirror them keep quiet, in case hysterical
governments make a concerted effort to stamp out all copies and/or all
major distribution sites.  If you aren't the quiet type, then *AFTER*
IMMEDIATELY PULLING A COPY OF THE CODE OUTSIDE US JURISDICTION,
announce your mirror on this mailing list.
We freedom-loving US citizens have had to rely on the freedom-loving
citizens of saner countries, to do the work of making strong
encryption, for many years.  We had a brief respite, which we will
eventually resume for good.  In the meantime, please let me apologize
for my countrymen and for my government, for asking you to shoulder
most of the burden again.  Thank you so much.
PS: Companies with proprietary encryption packages might consider
immediately open-sourcing and exporting their encryption add-ins, so
their customers can still get them from overseas archives.  Or taking
other actions to safeguard the privacy and integrity of their
customers' data and their society's infrastructure.  I also advise
that they lobby like hell to keep privacy and integrity legal in the US.

@_date: 2001-09-19 13:50:53
@_author: John Gilmore 
@_subject: chip-level randomness?  
The real-RNG in the Intel chip generates something like 75 kbits/sec
of processed random bits.  These are merely wasted if nobody reads them
before it generates 75kbits more in the next second.
I suggest that if application programs don't read all of these bits
out of /dev/intel-rng (or whatever it's called), and the kernel
driver should feed some of the excess random bits into the /dev/random
pool periodically.  When and how it siphons off bits from the RNG is a
separate issue; but can we agree that feeding otherwise-wasted bits
into a depleted /dev/random would be a good idea?
A better way to structure this might be for /dev/intel-rng to register
with /dev/random as a source of entropy that /dev/random can call upon
if it depletes its pool.  /dev/random would then be making decisions
about when to stir more entropy into the pool (either in response to a
read on /dev/random, or to "read ahead" to increase the available pool
in between such reads).  Thus, when demand on /dev/random is high, it
would become one of the "application programs" that would compete to
read from /dev/intel-rng.  Since /dev/random is the defined interface
for arbitrary applications to get unpredictable bits out of the
kernel, I would expect that in general, /dev/random is likely to be
the MAJOR consumer of /dev/intel-rng bits.
(Linux IPSEC uses /dev/random or /dev/urandom for keying material.  It
can easily consume many thousands of random bits per second in doing
IKE's Diffie-Hellman to set up dozens of tunnels.  Today this "surge
demand" occurs at boot time when setting up preconfigured tunnels -- a
particularly bad time since the system hasn't been collecting entropy
for very long.  /dev/intel-rng's high-spead stream can significantly
improve the quality of this keying material, by replenishing the entropy
pool almost as fast IPSEC consumes it.  Over time, IPSEC's
long-term demand for random bits will increase, since opportunistic
encryption allows many more tunnels to be created, with much less
effort per tunnel by the system administrator.)
Also, the PRNG in /dev/random and /dev/urandom may someday be broken
by analytical techniques.  The more diverse sources of true or
apparent randomness that we can feed into it, the less likely it is
that a successful theoretical attack on the PRNG will be practically
successful.  If even a single entropy source of sufficiently high
speed is feeding it, even a compromised PRNG may well be unbreakable.

@_date: 2002-08-02 00:47:16
@_author: John Gilmore 
@_subject: Canadian CSE wiretaps used against US citizens in court 
[CSE = Canada's NSA.  Supposedly "legal" under Patriot Act?   --gnu]
Canadians Listen in on NSA's Behalf
A high-level U.S. intelligence source has revealed exclusively to
Intelligence Online that some of the communications surveillance
evidence used by the U.S. government to try two Lebanese-born
U.S. citizens of running a cigarette smuggling ring and sending cash
to Hezbollah was collected by Canada's Communications Security
Establishment (CSE), the Canadian counter-part of the National
Security Agency and a long-standing partner of Echelon.  The Canadian
intercept data supplemented FBI wiretap evidence that a federal judge
in Charlotte, North Carolina allowed to be entered into evidence in
the trial of the two Shi'ite brothers, Mohammed and Chawi Hammoud.  On
June 21, the Hammoud brothers were convicted on a wide range of
charges, with Mohammed specifically being found guilty of aiding a
terrorist group.  In the past, NSA has denied that it uses its Echelon
partners to eavesdrop on U.S. citizens.  In the instance, however,
judge Graham Mullen allowed Canadian intercept information to be
used. The case illustrated changes in electronic surveillance policy
that were enacted by Congress following the Sept. 11 terror attacks.
Still, charges that Echelon partners help one another out in covert
operations aren't entirely new.  In the 1980's, Britain's GCHQ was
accused of asking its partners abroad to listen in on journalists who
were investigating the business affairs of prime minister Margaret
Thatcher's son, Mark.
Excerpted from Intelligence Online newsletter, No 434, 25 July - 28
August 2002, Cryptome offers the USA v. Mohammed Hammoud, et al case docket (no
filings are available online):

@_date: 2002-08-10 04:02:36
@_author: John Gilmore 
@_subject: responding to claims about TCPA 
Many of the people who "know something about TCPA" are constrained
by NDA's with Intel.  Perhaps that is Eric's problem -- I don't know.
(I have advised Intel about its security and privacy initiatives,
under a modified NDA, for a few years now.  Ross Anderson has also.
Dave Farber has also.  It was a win-win: I could hear about things
early enough to have a shot at convincing Intel to do the right things
according to my principles; they could get criticized privately rather
than publicly, if they actually corrected the criticized problems
before publicly announcing.  They consult me less than they used to,
probably because I told them too many things they didn't want to
One of the things I told them years ago was that they should draw
clean lines between things that are designed to protect YOU, the
computer owner, from third parties; versus things that are designed to
protect THIRD PARTIES from you, the computer owner.  This is so
consumers can accept the first category and reject the second, which,
if well-informed, they will do.  If it's all a mishmash, then
consumers will have to reject all of it, and Intel can't even improve
the security of their machines FOR THE OWNER, because of their history
of "security" projects that work against the buyer's interest, such as
the Pentium serial number and HDCP.
TCPA began in that "protect third parties from the owner" category,
and is apparently still there today.  You won't find that out by
reading Intel's modern public literature on TCPA, though; it doesn't
admit to being designed for, or even useful for, DRM.  My guess is
that they took my suggestion as marketing advice rather than as a
design separation issue.  "Pitch all your protect-third-party products
as if they are protect-the-owner products" was the opposite of what I
suggested, but it's the course they (and the rest of the DRM industry)
are on.  E.g. see the July 2002 TCPA faq at:
    3. Is the real "goal" of TCPA to design a TPM to act as a DRM or
     Content Protection device?   No.  The TCPA wants to increase the trust ... [blah blah blah]
I believe that "No" is a direct lie.  Intel has removed the first
public version 0.90 of the TCPA spec from their web site, but I have
copies, and many of the examples in the mention DRM, e.g.:
    (still there)
This TCPA white paper says that the goal is "ubiquity".  Another way to
say that is monopoly.  The idea is to force any other choices out of
the market, except the ones that the movie & record companies want.
The first "scenario" (PDF page 7) states: "For example, before making
content available to a subscriber, it is likely that a service
provider will need to know that the remote platform is trustworthy."
       (gone now)
Even this 200-page TCPA-0.90 specification, which is carefully written
to be obfuscatory and misleading, leaks such gems as: "These features
encourage third parties to grant access to by the platform to
information that would otherwise be denied to the platform" (page 14).
"The 'protected store' feature...can hold and manipulate confidential
data, and will allow the release or use of that data only in the
presence of a particular combination of access rghts and software
environment.  ... Applications that might benefit include ... delivery
of digital content (such as movies and songs)."  (page 15).
Of course, they can't help writing in the DRM mindset regardless of
their intent to confuse us.  In that July 2002 FAQ again:
  9. Does TCPA certify applications and OS's that utilize TPMs?   No.  The TCPA has no plans to create a "certifying authority" to
  certify OS's or applications as "trusted".  The trust model the TCPA
  promotes for the PC is: 1) the owner runs whatever OS or
  applications they want; 2) The TPM assures reliable reporting of the
  state of the platform; and 3) the two parties engaged in the
  transaction determine if the other platform is trusted for the
  intended transaction.
"The transaction"?  What transaction?  They were talking about the
owner getting reliable reporting on the security of their applications
and OS's and -- uh -- oh yeah, buying music or video over the Internet.
Part of their misleading technique has apparently been to present no
clear layman's explanations of the actual workings of the technology.
There's a huge gap between the appealing marketing sound bites -- or
FAQ lies -- and the deliberately dry and uneducational 400-page
technical specs.  My own judgement is that this is probably
deliberate, since if the public had an accurate 20-page document that
explained how this stuff works and what it is good for, they would
reject the tech instantly.
Perhaps we in the community should write such a document.  Lucky and
Adam Back seem to be working towards it.  The similar document about
key-escrow (that CDT published after assembling a panel of experts
including me, Whit, and Matt Blaze) was quite useful in explaining to
lay people and Congressmen what was wrong with it.  NSA/DoJ had
trouble countering it, since it was based on the published facts, and
they couldn't impugn the credentials of the authors, nor the
document's internal reasoning.
Intel and Microsoft and anonymous chauvanists can and should criticize
such a document if we write one.  That will strengthen it by
eliminating any faulty reasoning or errors of public facts.  But they
had better bring forth new exculpating facts if they expect the
authors to change their conclusions.  They're free to allege that "No
current Microsoft products have Document Revocation Lists", but that
doesn't undermine the conclusion that their architectures make it easy
to secretly implement that feature, anytime they want to.

@_date: 2002-08-11 11:15:28
@_author: John Gilmore 
@_subject: Seth on TCPA at Defcon/Usenix  
Isn't this how Windows XP and Office XP work?  They let you set up the
system and fill it with your data for a while -- then lock up and
won't let you access your locally stored data, until you put the
computer on the Internet and "register" it with Microsoft.  They
charge less than a million dollars to unhand your data, but otherwise
it looks to me like a very similar scheme.
There's a first-person report about how Office XP made the computers
donated for the 9/11 missing persons database useless after several
days of data entry -- so the data was abandoned, and re-entered into a
previous (non-DRM) Microsoft word processor.  The report came through
this very mailing list.  See:
   at wasabisystems.com/msg02134.html
This scenario of word processor vendors denying people access to their
own documents until they do something to benefit the vendor is not
just "plausible" -- it's happening here and now.

@_date: 2002-08-13 16:57:21
@_author: John Gilmore 
@_subject: Paul Wouters: Update Tapping in the Netherlands 
[Paul has been tracking Dutch government requirements that ISPs implement covert wiretaps against their customers -- and the technical
standards of the equipment that does it -- for a few years.  See   --gnu]
Update tapping in the Netherlands, August 12, 2002
(also available at: Here is a small update on matters in the Netherlands. Mostly the updates
focus around the Dutch organisation for ISP's, NLIP's conference talk at
Megabit ( now apparently already defunct) but some other information that surfaced in the last weeks has been included as well.
Some of the internet media has also been mentioning little bits, I assume
as a result of asking what NLIP was going to say at megabit, eg:
In short, the new organisation NBIP has seen the light. Webwereld
mentions the ISP's that are in the co-operation: ZonNet, Inter NL Net, IntroWeb, PSInet, Internet Access Facilities en Netland
Another 7 committed to joining the organisation when it would see the
light, according to Van Stam. This means around 14 ISP's will bundle their tapping equipment, in an attempt to make it affordable.
A new central organisation to co-ordinate all tapping, the LIO ("Landelijk
Interceptie Orgaan") which was planned to take over tapping matters in a
few years, has been rushed into existence as a result of "September 11",
and is expected to be fully operational before the end of the year. I
believe it will handle the tapping warrants, and infrastructure (though
the latter might be outsourced, but not to ITO) of the government side
of lawful interception (eg T1's and prob. some T2's). All tapping requests, wether from regular police (KLPD), a special department (eg taxoffice "FIOD") or our security service ("AIVD") or the military ("MIVD") should
go through the LIO. (I think this means the LIO will operate the T1's,
the machines to accept the traffic from the ISP's, and perhaps the T2's,
the machines that collect/decrypt the suspects data, for some agencies,
eg KLPD, FIOD, but I'd guess not the AIVD/MIVD.
DGTP, the "Directoraat Generaal Telecommunicatie en Post" (who now have
their own website,  has been moved to a different
department as of jul 22nd. formerly part of the "Ministerie van Verkeer en
Waterstaat" ('traffic and waterways') it now falls under the "Ministerie van
Economische Zaken" ('Economic Affairs')
In june 2002, the new version of the WIV law ("Wet op de inlichtingen- en veiligheidsdiensten") came into effect. For some discussion and a link to
the lawtext, see: In june, the results of the "bake off 1" got formulated in a new version
of the tapping specification, TIIT v 0.9.9. This document has not surfaced
into the public domain yet. However, a "final" version of the document,
version 1.0.0 is expected in september (expected not meaning released). At
that point, a third bake off will start, which focusses on the paperwork
side of things, including the electronisc paperwork (eg: HI1 in FuncSpec
Only three Vendors were part of the current testing/bake off:
- - Pine / ENAI
- - Accuris (Group 2000)
- - SS8 (Formerly ADC)
Currently, the following vendor's are also in testing phases:
- - IDD (Innovative Design Delft)
- - Heynen (with GTEN)
- - Aqsacom (with Riser)
- - Digivox
- - Verint Systems (formerly Comverse Infosys)
A new Directive ("Algemene Maatregel van Bestuur") named "Beveiliging gegevens Aftappen") is being written. It will contain the requirement
for ISP's to have a "secure FAX" to which the LEA can fax the tap order,
along with the NAW (name,address,city) to the LIO and DGTP.
Ironically, current law dictates warrants should arrive on CDrom in XML
format, but as can be seen from bake off 3, this isn't reality yet.
Another interesting item in the Directive is that all ISP's should at
least appoint one person as liason to the government regarding tapping.
This person will be checked by the BVD (AIVD or whatever you want to call
them these days), a so-called "antecedenten onderzoek".
Another requirement is to sent the LIO an "Provider ID" neccessary for
for the TIIT spec (so the government can see which ISP sent the information).
You cannot request a number, you're not assigned a number. You need to make
one up, and hope it's not taken, or otherwise come up with a new one. It's
inclear to me why they don't just assign ISP's a number. NLIP advices to
use your IANA Enterprise Number, but most ISP's probably don't even have
Buma-Stemra , our local RIAA/MPAA, apparently lost their special rights,
and can no longer "order a tap" (I'm not entirely sure how they could order
this in the past)
Where Telco's have to have a tap operational in 12 hours, there has not been
a set time for ISP's yet. It has been defined as "without delay", in article
25 of the new "WIV" law. This applies to "special cases" ("Bijzondere Last"),
which needs the permission of the "Minister van Binnelandse Zaken" (National
Misc. items of unconfirmed information and/or rumors
There are currently three T1's operational. They are located in Den Haag (The Hague), Bilthoven en Zoetermeer It's still unclear wether Internet Exchanges, and large "non public" (in
the legal sense) need to be tappable. Surfnet was on the list of "ISP's"
that were notified in a letter from the government reminding them to
implement the tapping infrastructure
(See: The matter of wether ISP's/webhosters/colocation facilities need to register with OPTA (central register for Telco's) is still unclear. It
seems that law dictates you have to register, but OPTA will refuse to
register you. (So "you must, but you can't"). Since being registered with
OPTA is still an official requirement to obtain the tapping specification,
this matter is important. Also, if OPTA would need to register all ISP's
and webhosters, it would currently have less then 5% or so in its register.
The NAO has been effectively shut down. The main reason being that it was no
longer a "secure" party, after documents appeared on Opentap. It could no
longer participate in closed-doors negotiations/discussion.
(personal note: I believe those should never have happened closed-doors, esp
 since NAO suggested to represent all those who were affected by the laws,
 while in practice it only represented telco's/access providers, and not
 small ISP's without access networks, or webhosters/resellers)
Another reason was that NLIP couldn't justify the time/money spent on NAO (eg maintaining its website). Deloitte & Touche are investigating a financial model for a) internet and
b) mobile phone tapping (costs). The government wants one model (personal
note: I think they're right, these will become effective one within the
next one-two years, see GPRS, UMTS, Imode)
The statues of the NBIP are public and can be requested (contact them or
It seems the ciphers that were allowed in TIIT 0.1.2 have been limited to
only RC4 and AES (Rijndael) in version 0.9.9. But that's not a great suprise,
as this was clearly the intend of TIIT, but the AES candidate wasn't known at the time of writing). Apparently, the biggest hole in the specificatin, the "email tap" has been resolved.
Comments, corrections, information and suggestions are always welcome,
Paul Wouters

@_date: 2002-01-04 17:52:07
@_author: John Gilmore 
@_subject: Baltimore Sun: MD police seek easier wiretaps 
Md. police seek law for easier wiretaps
Use of technology by criminals outruns current authority

@_date: 2002-01-07 13:46:48
@_author: John Gilmore 
@_subject: On ISPs Not Filtering Viruses  
John Young, why are your web servers running virus-prone operating systems?
Haven't you installed the Linux security patches on 'em and turned
off all nonessential services?
I thought ISPs were supposed to be bit-pipes.  End-to-end unrestricted
connectivity is the basic feature of the Internet.  This feature is
what made the Internet superior to every preceding network.  If my ISP
was filtering my mail or my packets, I'd complain.  (In fact, when
they started to, I did complain until they changed it, and my web site
still complains about it.)

@_date: 2002-01-10 00:52:29
@_author: John Gilmore 
@_subject: FreeSWAN & US export controls  
Absolutely.  It's already pretty secure.  We should just make it
trivial to install, automatic, transparent, self-configuring,
painless to administer, and free of serious bugs.  Then they'll have
every reason to drop it in.

@_date: 2002-01-20 18:30:47
@_author: John Gilmore 
@_subject: PGP & GPG compatibility  
These days, PGP is effectively useless for interoperable email.  If
you have not prearranged with the recipient, you can't exchange
encrypted mail.  And even if you have, one or the other of you will
probably have to change your software, which will produce other ripple
effects if you are trying to talk to TWO different people or groups
using encrypted email.
PGP compatibility problems started with Phil Zimmermann's deliberate
decision to eliminate compatibility with RSA keys.  Once that problem
existed, disabling communication with anyone who used PGP before late
1997, nobody else seemed to mind introducing all sorts of lesser
incompatibilities, including many mere bugs.
Having wrestled with these problems for years, my guess is that we
need to abandon PGP and spec something else, probably in the IETF.
(Perhaps we might be able to shortcut that process if the OpenPGP
standards effort actually produces many compatible implementations
including NAI's, and/or if NAI falls apart and every other
implementation meets the IETF specs.)
Note, however, that there are many things that OpenPGP doesn't do,
making encrypted email still a pretty sophisticated thing to do.
Brad Templeton has been kicking around some ideas on how to make
zero-UI encryption work (with some small UI available for us experts
who care more about our privacy than the average joe).

@_date: 2002-01-24 15:30:24
@_author: John Gilmore 
@_subject: James Bamford talk: Intelligence Failures that led to Sep 11th attacks 
The Goldman School of Public Policy is pleased to announce the following
public lecture.
All are cordially invited to attend.
Time:      6:30 p.m.
Place:     Sibley Auditorium, Bechtel Center, U.C. Berkeley
Speaker: James Bamford (Visiting Professor, GSPP)
Topic:      INTELLIGENCE FAILURES THAT LED TO THE SEPTEMBER 11TH ATTACKS
James Bamford is the author of The Puzzle Palace and Body of Secrets, two
national bestsellers and the only books written about the National Security
Agency, the country's most secret intelligence agency.  Bamford's expertise
is in information technology, national security and public policy.  He was
until recently Washington Investigative Producer for ABC's World News
Tonight with Peter Jennings and has written investigative cover stories for
the New York Times Magazine, the Washington Post Magazine, and the Los
Angeles Times Magazine.  He lives in Washington, D.C.
For more information, please call the Goldman School of Public Policy at
CHECK OUT OUR WEBSITE AT FOR A COMPLETE LISTING OF GSPP EVENTS: GSPP EVENTS LISTSERV INSTRUCTIONS:
TO SUBSCRIBE: Send a message to: majordomo at listlink.berkeley.edu with
subscribe gspp_events in the body of the message.
[Bring your copies of his books for him to sign...and ask him some questions
that deserve answers!   --gnu]

@_date: 2002-01-28 02:27:43
@_author: John Gilmore 
@_subject: A risk with using MD5 for software package fingerprinting  
When I was working at Cygnus, I was FSF's official maintainer of GDB.
Whenever I cut a GDB release, I would diff it against the previous
release, and read the diff by eye.  I encouraged others at Cygnus, who
maintained the GNU assembler, linker, compilers, etc, to do the same.
While it's hard to bring a fresh eye to a piece of code that you've
spent years working on, it's a lot easier to look over small scattered
differences, remember why each was made, and notice whether they have
any security implications.
This quality-assurance technique frequently caught minor problems,
but mostly I was doing it because the GNU development tools would have
been a great place to slip in a Trojan Horse (recall the Turing Award
paper by Ken Thompson).  Think about how many times you've run GDB
as root, or at all.  And about how many systems have these tools installed.
This is also part of the reason that we did three-stage builds (build
the tools with other tools; use the resulting tools to build
themselves; use the resulting tools to build themselves again).  The
last two versions will be identical, down to the bit -- modulo bugs,
Trojan horses, embedded timestamps, etc.  Our regression testing would
make sure they matched, and we'd investigate every difference.  And if
a Thompson-style binary-only Trojan had been introduced, compiling the
compiler with somebody else's compiler first would defeat that attack
(unless both compilers had been not only compromised, but also taught
to compromise the other compiler).
Cygnus even had the ability to check that cross-compilers built and
running on a variety of different machines (Suns, DECs, PCs, HPs, etc)
would produce exactly the same output files, down to the bit, from the
same input files.  We had all the different machines sitting there for
testing, and we wrote and used the infrastructure to automate that
testing.  Not only did it find lots of obscure little bugs for us,
as well as some floating-point representation problems.  It also discouraged platform-specific security breaches.
I'm probably not your average code jockey, but there are at least
some people in positions of trust (e.g. at the top of big distribution pipelines) who *do* care enough to read the code, look for holes, and
to work to automate the finding of 'em.
PS: I didn't start my release-comparing practice with GDB.  I don't
know who taught it to me; perhaps Bill Shannon of Sun.  I taught it to
Hugh Daniel, and we used it to build product-quality PostScript-based
window system releases.
PPS: I never did audit the "diff" program though...  :-) As Thompson
so eloquently points out, complex systems involve a lot of reliance on
every piece of code that contributes to the system.  And if after
years of inspections, all your software is clean, how do you know
nobody paid Tim May to slip a security hole into the 8086 circuit
design?  I have done enough years of chip testing AND architectural
validation to know how few of the infinitely many combinations of
instructions or bus cycles are actually tested to make sure that
somebody didn't intentionally make *one* combination do something
interesting.  Even if you trust your processor, didn't the NSA pay the
Taiwanese designer of your RAM chips to replace particular stored code
sequences with other interesting ones, one time out of a hundred, when

@_date: 2002-01-28 02:49:34
@_author: John Gilmore 
@_subject: A risk with using MD5 for software package fingerprinting  
A small PS to my last message.
In 1978 I was lent an Apple II running the ABBS software (Apple
Bulletin Board System), and it ran in a corner of my bedroom for some
years as the PCnet ABBS in San Francisco.  This was a machine with an
8-bit 1 MHz processor, 48K of RAM, and a custom floppy that held maybe
100 or 200K bytes; no hard drive.  It did email for a regular
community of dozens of users, and hundreds of assorted visitors, on a
single 300-baud phone line.
While getting the PCnet (uucp-like packet-switching and email
transfer) software running on this beast, I also improved the ABBS
software, which was written in Applesoft (Microsoft) BASIC and thus
came with its own source code.  One day I found a very interesting
line in that code.  It went something like this:
18520	if (%K.eq.%U5) goto 3700
You needed a lot of context to understand that this was a backdoor in
the ABBS software.  It compared "K", the message number that the caller
had just asked the BBS to delete, with the machine address of an I/O
port "U5" that the BBS used to talk to the modem or something.  If the
message number and the I/O address matched, it would jump into another
bit of BASIC code at line 3700, which was where it handled commands
for the local Apple operator of the BBS, including what is now called
"shell" access.  So asking the ABBS to delete message number 32547 or so would give you
operator privileges.
This obscure line among thousands, placed just so, could do that.
This is why only someone who actually understands the code at a deep
level is likely to find back doors like this.
I deleted that line, and put out an alert to other ABBS users that the
author of the ABBS software had inserted a back-door in it.
I think that was the only deliberately build backdoor I've ever found
in a piece of software or hardware.  (Well, not counting NSA's designs
for cellular phone encryption algorithms, key exchange protocols, and
the Clipper chip.  Or the weakening of DES in the first place.)
(All the variable names and line numbers in this story have been
changed to protect the innocent -- and to avoid me having to try to
dig out probably nonexistent printouts of that software.  But if you
have the ABBS BASIC source, look in the 'K' (kill message) command

@_date: 2002-11-03 13:47:56
@_author: John Gilmore 
@_subject: NSA director Hayden's testimony on NSA and 9/11 
Hayden's testimony deserves to go into the cryptome archives, and
should be read by everyone on this list.
He spends ten pages explaining how NSA worked on terrorism pre- and
post-9/11, and then tells Congress that they can best help him by going
back to their constituents and understanding where the public wants to
draw the line between liberty and safety, i.e. between not being
wiretapped domestically and being wiretapped domestically.
We should help to give him a loud and clear answer to that.
The head of the National Security Agency said last week that Congress might want to aim the most powerful surveillance system in the world at American Lt. Gen. Michael Hayden, in a rare public appearance before the Senate Intelligence committee, said the ongoing terrorist threat means America needs to debate where to draw the line between foreign and domestic surveillance. Currently the NSA is prohibited from spying domestically.
Here's an excerpt:
"Where do we draw the line between the government's need for (counter-terrorism) information about people in the United States and the privacy interests of people located in the United States? This line-drawing affects the focus of NSA's activities, foreign versus domestic... the type of data NSA is permitted to collect and how, and the rules under which NSA retains and disseminates information about U.S. persons."
Until the 1970s, when the Senate's Church Committee revealed what had been going on in secret, the CIA and the NSA conducted illegal surveillance on American citizens. In response, Congress enacted a series of reforms, notably the Foreign Intelligence Surveillance Act.
"These are serious issues that the country addressed, and resolved to
its satisfaction, once before in the mid-1970's," Hayden said. "In light of the events of September 11th, it is appropriate that we, as a country, readdress them. We need to get it right."

@_date: 2002-11-17 23:16:12
@_author: John Gilmore 
@_subject: AIR TRAVELER ID REQUIREMENT CHALLENGED  
The regulations I'm challenging purport to require air and train
travelers to show a "government issued ID".  Every traveler has been
subjected to these "requirements", but it turns out that they aren't
really required by any published law or regulation.  And if you refuse
to meet the supposed requirements, you find out that there are
alternative requirements, that they weren't telling you about.
The government has responded, as have the airlines.  Their response is
to ask the court to dismiss the case, as expected.  See the web site
   for copies of their motions.
The Federal one has the most interesting arguments.  In summary, they
argue that I can't challenge the no-fly list or anything other than
the ID demand because, having not shown ID, the no-fly list was not
applied to me; that I can't sue in a District Court anyway because the
Court of Appeals is supposed to have original jurisdiction; that the
government can make any rule it wants which relates to air security,
and penalize the public over violations, without ever telling the
public what the rule is; that being refused passage unless I present
an ID does not infringe my constitutional right to travel anyway; that
being prevented from traveling anoymously does not implicate any First
Amendment interests; that every possible form of airport security is a
fully constitutional 4th-Amendment search; and that since my right
to travel is not being infringed, these searches give me equal
protection just like all members of the public, because any 'rational'
reason for singling out anonymous travelers will suffice.
If everyone shows ID to fly, and they can get away with preventing
anonymous travel, it becomes easy for the government to single out
e.g. members of the Green Party.  (If no ID was required, any
persecuted minority would soon learn to book their tickets under
assumed names.)  The Nixon Administration had its "enemies list", who
it subjected to IRS audits and other harassment.  But even that evil
President didn't prevent his "enemies" from moving around the country
to associate with anyone they liked.  The Bush Administration's list
interferes with freedom of association and with the constitutional
right to travel.
As my experience on July 4th, 2002, in the San Francisco airport
demonstrated, citizens are free to not show ID to fly, if they spend
half an hour arguing with security personnel over what the secret
rules actually say.  But then, catch-22, the citizen can board the
plane only if they'll submit to a physical search like the ones that
Green Party members and other "on the list" people are subjected to.
So, you can identify yourself to them and be harassed for your
political beliefs, unconstitutionally.  Or you can stand up for your
right to travel anonymously, and be searched unconstitutionally.  Or
you can just not travel.  That's why I'm suing Mr. Ashcroft and his
totalitarian buddies.
The government motion to dismiss my case is filed at:
  The index to all the related documents is at:
  No.  We will file a response to this motion by approx Dec 1.  Then
they will file their reply in mid December or so.  Both of those will
go on the web site.  (If anybody wants to OCR the PDFs of the gov't
documents, please go for it and email me the text.)  Then the court
will read all this stuff, and we'll have a hearing, which is
tentatively scheduled for mid-January.

@_date: 2002-11-17 23:29:59
@_author: John Gilmore 
@_subject: Why we spent a decade+ building strong crypto & security 
The US government's moves to impose totalitarian control in the last
year (secret trials, enemies lists, massive domestic surveillance) are
what some of the more paranoid among us have been expecting for years.
I was particularly amused by last week's comments from the
Administration that it'll be too hard to retrain the moral FBI agents
who are so careful of our civil rights -- so we'll need a new
domestic-spying agency that will have no compunctions about violating
our civil rights and wasting our money by spying on innocent people.
While there's plenty of fodder for argument among the details, the
overall thrust of the effort seems pretty clear.
Now's a great time to deploy good working encryption, everywhere you
can.  Next month or next year may be too late.  And even honest ISPs,
banks, airlines (hah), etc, may be forced by law or by secret pressure
to act as government spies.  Make your security work end-to-end.
Got STARTTLS?
Got IPSEC?
Got SSH?
Use it or lose it.

@_date: 2002-09-13 22:40:59
@_author: John Gilmore 
@_subject: [Discuss-gnuradio] SDR Forum Report on Security 
Thanks to Dave Emery for pointing us at this report.
This 121-page report on "How to secure Software Defined Radios",
written to help the FCC decide how to handle software radios, is very
slanted toward monopoly-industry viewpoints.  The whole focus is on
giving the system operator lots of flexibility to do whatever they
want, while giving customers, experimenters, competitors, and citizens
zero flexibility or opportunity.  They managed to suppress the few
pages of actual information about the paucity of any actual threat to
public safety (see last paragraph of this review).
The document reminds me of those eco-terror reports about how the
world is falling to shit (whether it really is or not).  On the page
numbered 14 at the bottom (page 21 in the PDF -- I wish Adobe could
get this straight), it says:
  The impending widespread use of software changes, whether to add or
  improve user services or to reconfigure RF parameters of a wireless
  device, presents substantial new challenges to manufacturers and
  operators, particularly in the face of a youthful "digital generation"...
Having a whole generation of young people grow up full of knowledge
about engineering, computers, and networking is a wonderful advance in
the state of humanity.  The authors of the paper apparently see it as
a drawback, since they depend on their customers being ignorant.
The report pays little attention to the huge opportunities offered by
separating the development of hardware from the development of
software.  If you look at the history of computers, the people who
built great hardware tended to suck at software.  The people who built
great communications hardware built the worst networking software.
IBM mainframes are the most obvious example in computers.  Not only
the PC, but the 1970's minicomputer and the 1980's engineering
workstation were great examples of how open hardware could allow
software innovation to flourish.
Welded-shut systems tend to be terrible, compared to the innovation
and cost reduction available in systems where one vendor supplies
hardware, another supplies more plug-in hardware, and four or five
more supply various bits of software (the OS, the applications, custom
scripting, etc).  The Bell System telephone is the obvious example of
a welded-shut system that it took a 1950's FCC decision to open (the
Carterfone decision), unleashing vendors to supply *modems* which
permitted *data* traffic which permitted *computer-mediated
communication* and eventual *networking*, eventually creating the
public *internet* about forty years later.  The US got a head start in
creating the Internet because of the Carterfone decision and
subsequent decisions on access to leased lines, while almost every
other country's telephones were still run by a monopoly PTT whose
interest was in preventing competing forms of communication.
Today's cellphones and Cable TV systems are similarly welded shut,
resulting in endless lock-in that profits the vendors while beggaring
the society; they provide only the minimal amount of innovation that
keeps the vendors alive.  (As a small example, there's still no decent
mobile data networking; the one company that temporarily provided it,
Metricom, grew out of the ham radio fraternity rather than the
cellphone companies.)
The report's focus on "Wireless Threats" is obsessive about threats to
their revenue models or to their "systems" or their control.  For
example, a software modification that would permit cellphones to talk
directly to each other when in range of each other, without the use of
a base station or its network, would be considered a threat to the
integrity of the system ("unauthorized access to services").  However,
it would be considered a positive feature by end users, who would be
happy to pay third parties to write such software; it would serve more
total users by reusing the spectrum locally; etc.
A really useful SDR communication device would have a jack that would
take either an Ethernet or a phone line; if neither was plugged in, it
would look for 802.11 local connectivity, or Metricom wide-area
connectivity, or its own networking protocol if any similar station
was in range, or failing that, would be able to use a terrible and
expensive cellular data or voice network.  All of this would be
transparent to the user.  None of the vendors who authored this report
would ever build such a device, because the majority of the time it
would not force users to pay them by the minute.  And user or
competitor attempts to reprogram existing devices to do this, or any
part of it, would be warded off as "attacks" by "malicious hackers".
The report is followed by the individual company reports that were
submitted to it.  They make interesting if duplicative reading, since
they reveal that the whole SDR Forum report just seems to be a
stitching-together of the most self-serving parts of the documents
submitted by various companies.
Wow!  There's even an Appendix H (PDF page 100) that is a verbatim
copy of a Digital Restrictions Management report from the Copy
Protection Technical Working Group, which talks about how "Securing
adequate protection for copyrighted works in the digital environment
will allow development of viable business models.  Viable business
models will in turn help drive adoption of broadband ... and expanded
consumer choices ..."  Of course, it's full of horseshit; DRM is all
about preventing UN-VIABLE monopoly business models from going extinct
when they have been obsoleted by technology.  This is even the report
that talks about how the "broadcast flag" will save digital broadcasts
that happen "in the clear".  It's particularly incongruous in a report
full of glowing public-key crypto recommendations.
PDF page 102 is a great overview of some companies that deserve
cypherpunk scrutiny.  E.g. "Signum Technology" provides iPak that lets
you print packaging labels with "sophisticated invisible watermarking
that allows incorporation of hidden identifying data" which can be
revealed "in a matter of seconds" with "an inexpensive scanning
The report in general also follows the "academic paper" model of
security: See, we're using public key algorithms.  Therefore our
systems will be secure.  The impact of bugs, protocol design failures,
social engineering, breach of trust, undersized keys, revelation or
government appropriation of private keys, etc, are all ignored.
On page 8 (PDF 15) it mischaracterizes the problems with 802.11 and
WEP.  First, it leads off with Adam Stubblefield's break of WEP --
failing to start with the 30-year history of trivial-to-crack wireless
network protocols that were built under the guidance of the FCC and
with active help and legal threats by the National Security Agency.
WEP uses 40-bit keys because they were the largest available in
exportable products until EFF's lawsuit cracked the unconstitutional
export controls.  Securing wireless networks has always been a second
priority to making it trivial for the government to illegally wiretap
them.  This tension isn't going to go away.
Second, its 802.11 discussion directly lies that the popular practice
of recreational listening for open wireless networks results from the
ability to crack WEP-secured networks -- rather than the public's
tendency to leave 802.11 networks unsecured because the industry and
the vendors only provided painful hand-configured ways to secure them.
I know of no "wardriving" that seeks and cracks WEP-secured nets; it's
all merely probes for networks that people have left open, either by
default or by intent.  (The idea that someone would intentionally
PERMIT the nearby public to freely use their wireless network
infrastructure is apparently heresy to the authors of the report.)
The report also looks approvingly on digital-restrictions-management
systems (DRM) as the solution.  E.g. no SDR will be able to run code
unless it has been digitally certified by the vendor.  Like every
other restrictions-management system, this will be deliberately used
to cement established monopolies and prevent innovation.  It even rates
part of the "threat model" consequences as "Digital Rights Violation" --
defined not as a violation of the user's rights or privacy, but as "unauthorized access to, or theft of, digital content and software".
The report's focus is also inappropriately largely focused on
"cellphones".  It reminds me of a Motorola emp who told me in the
early '90s "car phone" era that offering wireless data networking
would be irresponsible because "you should keep your eyes on the
road", implying that (1) only people in cars would want wireless data
networking, and (2) they would use it while driving.  This tunnel
vision mindset doesn't enable the authors to notice that everything
from car alarms to shipping crates to pets to ballpoint pens to
automotive light bulbs already today, or soon will, come with data
networking built in.  Software-defined radios will be broadly useful
throughout society, not just for "cellphones" but for
****everything****.  So if the "SDR Forum" or the FCC denies society
the benefits of rapid innovation, they won't just be denying it to
cellphone users, but to auto owners, package shippers, pet owners,
doctors, writers, lightbulb users, and everyone else.
Page 23 (PDF 30) jocularly reports that "Eavesdropping on user data
(Breach of security, public safety has some experience in this
scenario)".  I think they meant that they have some experience being
intercepted, but of course "public safety" agencies systematically
intercept private citizen communications.  The report goes on to
suggest that:
  Sophisticated encryption techniques should be made available to public
  safety users of SDR technology.  AES voice encryption and 128-bit data
  encryption should be considered minimum standards for public safety
  SDR devices.  Devices can be lost or stolen and, therefore, must be
  capable of remote revocation of service.
One would hate for the *citizens* to get access to AES voice
encryption or 128-bit data encryption, therefore we had better only
give those devices to cops, and "remotely revoke" them if they leak
outside the Government Trust Barrier to ordinary untrustworthy voters
or citizens.
Page 25 (PDF 32) lauds the "Trusted Computing Platform Alliance",
Intel's fuck-the-customers-for-Hollywood initiative, as being a model
for SDR companies to follow.  As usual, they completely obscure the
critical question, which is "Who trusts who?".  Their "Trusted"
systems are trusted by monopolists to not be susceptible to
unauthorized competition.  This word game deceives people who
foolishly think they're trying to build "systems the consumer can trust".
Page 29 (PDF 36) even pushes the "NTRUEncrypt" snake oil encryption system.
Page 30 and 31 (PDF 37 and 38) discuss GSM security, without bothering
to mention that they kept their snake oil algorithm secret for years
so that consumers would not find out how insecure it was.  It bleats
that the "Security Group has realized two important initiatives over
the past 12 months": introducing AS/3 to replace the faulty algorithms,
and a protocol for "authenticated key agreement", AKA.  Until I hear
differently, I'll assume that these are both more proprietary snake oil.
On page 34 (PDF 41) their prime example of how "public safety" users
need priority and reliability is "best illustrated by the SWAT team
commander notifying the sniper with the 'shoot' or 'don't shoot'
command".  Given the number of SWAT teams deployed against innocent
civilian drug users, such as the raid on terminal cancer patients made
in Santa Cruz last week by just such a machine-gun-toting SWAT team,
this is an insulting example.  Cops need good communication to know
whether they've been ordered by a corrupt higher official to shoot a
citizen from concealment.  I see.
Page 40 (47) goes so far astray as to say:
  "As a multitude of products and services still uses proprietary
   solutions there is no advantage to using secure standards which only
   give extra security if everyone else offers them."
The next page points up the monopoly control problem in wireless cellular
networks as a positive feature:
    5.2.2 Asymmetry of Information
  Unlike the general IT situation where there is varying levels of
  control over client devices attached to a network; from closely
  controlled private networks, to no control of devices connected to
  the internet; commercial handsets must be qualified to operate on an
  operators network before they are allowed to connect. Therefore
  operators have complete control over the capabilities of devices
  they allow to connect to their network. As such there is no
  asymmetry of information in the case of handsets deployed on an
  operators network.
The report then concludes without saying much more than these terrible
But then come a bunch of interesting submissions from various
companies.  Intel's reveals the source of the TCPA paragraphs (copied
directly from Intel propaganda).  It again skips over the REASON why
802.11 is insecure, and fails to mention the real cure (standardized
mass-market end-to-end encryption, which is politically disfavored
since it discourages wiretapping).
The paper from the "Mobile Virtual Center of Excellence" in the UK at
least makes explicit the authoritarian model that's implicit
throughout the rest of the document (PDF page 64):
  Domains and regulatory bodies.  We suppose that the world is divided
  into an number of administrative domains, which may correspond to
  single nations (e.g.  the USA), or groups of nations (e.g. the EU).
  In some cases, it may also be the case that nations are sub-divided
  into separate domains.  Each domain is assumed to have a single
  regulatory body, responsible for deciding which software is permitted
  to be downloaded and executed in SDR platforms.
Which software citizens will be permitted to download or execute.
What a fascist concept.
The MVCE proceeds on page PDF 71 to say, "Also, issues relating to Digital
Rights Management (DRM) may arise, i.e.  where the SV restricts use of
code modules to enforce payment for these modules."
Motorola's submission (PDF pg 74) seemingly ignores Kevin Mitnick's
penetration of North Carolina cell site software, well documented
in several books, when it says:
  The data links which connect the OMC to the cell sites are private
  data networks controlled by the network operators, and offer no entry
  point for remote hackers.  Furthermore, there is no internet or other
  publicly accessible external data connection into the OMC.  The
  inherent security of base station equipment is demonstrated by the
  fact that second generation (2G) commercial base stations are remotely
  programmable, and have been operating in high volume for over ten
  years without any significant security issues.  The focus of this
  report, therefore, will be on security issues surrounding commercial
  handsets.
Sorry, Kev, you were insignificant.
The Moto document also has a 5-page slideshow tutorial on encryption,
for the braindead who have nevertheless managed to read that far.  (PDF 77)
The Moto document is the source of the "Security Threat Model" that
included that "Digital Rights Violation" example above, and "Example
Threat Scenarios" that are almost uniformly "hacker", "black market",
"unethical", or "disreputable" parties modifying the system.  (In one
example, an "inadvertent" bug does not bring the repute or ethics of
the creator into question, presumably since Motorola would be the
originator of the bug.)  There's no example of beneficial improvements
made by third parties; of experimentation by scientists or university
students; of innovation by competitors.  The system is designed to block
all those things.
Motorola's document surprisingly honestly says that security systems
should be designed to work even when the entire design is known to
opponents (page PDF 83).  But then on page PDF 89 it argues for snake
oil, which has served the wireless industry so well in the past:
  Finally, it should be stressed that the very nature of the security
  challenge is that the "threat" is ever changing.  Malicious hackers
  make it their business to try and decode the security systems designed
  to thwart their unscrupulous efforts.  Therefore, regulatory mandate
  of specific security methods would be counterproductive.  To do so
  would provide a blue print for the malicious hacker, and would impede
  the industry's responsiveness to an ever-changing security landscape.
Clearly, anyone who might want to put software of their own choice
onto the equipment they have purchased is "malicious" and
"unscrupulous" rather than "an honest competitor" or "a customer".
And we can't have the laws merely say what is required, or that would
provide a "blue print" for people who want to do something else.
The only really sane part of the Motorola document is the page and a
half at PDF 86, which says how tiny the "SDR security threat" really
is.  Basically it says that people who design mobile radio gear are
operating in a very tight design space and aren't going to put in a
whole lot of expensive flexibility that would allow operation in
multiple frequency bands, massive increases in output power, etc.
I.e. the whole inquiry is basically a sham.  This sensible part of the
language never made it into the final report of the SDR Forum, though.

@_date: 2002-09-26 14:45:12
@_author: John Gilmore 
@_subject: RSA's RC5-64 Secret Key Challenge has been solved.  
Congratulations to the team!
I think it's worth starting.  Computers are getting exponentially
faster, so d.net's current rate will continue to increase.  Humans
have little experience with long-drawn-out exponential processes, and
tend to underestimate their impact.  They are pretty powerful
speeder-uppers at the tail end.
After getting that getting started, though, I suggest beginning a
brute-force attack on the GSM cellphone encryption algorithm.  That's
in use in hundreds of millions of devices worldwide, protecting (or
failing to protect) the privacy of billions of phone calls a day.

@_date: 2003-08-23 00:38:21
@_author: John Gilmore 
@_subject: UPnP Security specs available for review  
What's the design lifetime of this security system?
1024 bit RSA is too short.  If you're going to all the trouble to
build a supposedly secure system, use a length that won't be broken.
My suggestion these days is significantly north of 2048 bits.  Don't
use a power of two, and, ideally, use key lengths that vary among
devices, so that there's no "sweet spot" for someone to build a
key-cracking machine for.
E.g. One device that implements your spec might have a key length of
2432 bits; the next one a length of 2200 bits; a third 2648 bits.
It's clear that the crypto implemented in these devices in the near
future is going to be in iterative software, rather than wide
hardware, so there's no reason to limit the keys to 1024 bits except for
performance and the tiny cost of memory space.  And we all know that:
   the number of public-key operations required is low
   the latency of public-key operations is usually negligible at system level
   the performance of hardware always increases rapidly
   available memory always increases rapidly
So don't fall into the NSA trap that's plagued cellphones and every
other consumer device.  Don't build a security system for every
household device that is "secure ENOUGH" but has a weak link designed
And don't forget that Intel wants to sell that increased-performance
hardware too.  So the crypto system should be right at the "sluggishly
slow" point when first released.  Because two or three years out, new
devices will be "plenty fast" and then a few years later any crypto
overhead will be "invisible" in new devices.
Also, once you have established session keys between two devices, why
would you EVER send plaintext between them (page 6, paragraph 9)?  The
spec should say that plaintext messages will not be accepted, and the
implementations should definitely ignore any that arrive.  This is the
failure mode of US cellphones: the TDMA and CDMA standards define a
(poor) encryption scheme, but even though it's in every phone, the
cellphone service vendors have all been pressured to disable it on
every call.  (In fact it isn't even built into the base stations.)  IF
IT'S POSSIBLE TO DISABLE THE CRYPTO, THE GOVERNMENT WILL CAUSE IT TO
HAPPEN IN PRACTICE.  So spec it so each device *must* have the secure
crypto, on every message, or it won't interoperate.
Also, what's this business about manufacturers generating the
long-term keys and putting them in the devices and not letting users
change them (pg 6, first sentence)?  Have you gone over to the Dark
How many seconds would it take for a rogue Security Console to try all
possible 6-uppercase-letters passwords after you plug in a device
(e.g. to charge) and before you try to control it from your own
Security Console?  2 milliseconds per try (500 tries a second) seems
like a high estimate, for an operation that only has to check a hash,
especially ten years from now.  I didn't work out the message lengths,
but on a 10 mbit ethernet, at wire speed, wouldn't the attacker be
able to try passwords faster than this today?  And what about on the
10 gbit ethernet that'll be default in ten years, with the hash done
in the invisibly small 40 GHz embedded processor that'll be default in
ten years?
PS: It's nice that on page 7 you tell manufacturers in paragraph 14
how to build back doors into devices.  As is obvious in current
telecomm systems (including "anonymity" software), if these are
buildable, then the government will pass a law mandating their use to
subvert the user's control over their own life and privacy.  E.g. this
is where you'd put the "Execute DRM" interface, where Senator Hatch
will send a message that destroys the device if hollywood thinks you
aren't a subservient godfearin' amurrican.  And the CALEA interface.
PPS: I stopped at page 7; these comments were getting long and I was
losing interest.  Carl, You know better than to design in each of
these flaws, so presumably if you'd had the power to fix them, I
wouldn't have to send in these comments.  Thus, none of them are going
to get fixed.  Right?  Why did you bother publishing this spec?  Might
as well have all the mfrs just agree on it in secret and ram it down
our throats.

@_date: 2003-12-14 14:32:31
@_author: John Gilmore 
@_subject: Sign up now for the Bush antiterror board on civil liberties! 
[Oops, I mean the Bush anti-civil-liberties board on terror.  But
seriously, folks, there seem to be some honest politicians blowing the
whistle here.  Check out the report on Monday.  PS: I am no relation
to Jim Gilmore.  -- John]
Saturday, Dec. 13, 2003
Bush Gets a 'Can Do Better' From Terror Panel
Federal advisory body complains of lack of strategy guiding domestic security efforts
By TIMOTHY J. BURGER/WASHINGTON
President Bush's anti-terrorism policies are about to come under fire
from a somewhat unlikely source: A federal advisory panel headed by a
former Republican Party chairman is set to rap the President's
knuckles this week when it issues a report criticizing the
administration for failing to develop a comprehensive, pro-active
anti-terror strategy more than two years after the 9/11 attacks.
Headed by former Virginia governor Jim Gilmore, the Advisory Panel to
Assess Domestic Response Capabilities for Terrorism Involving Weapons
of Mass Destruction voices concern, in its draft report, that civil
liberties are getting too little attention as new security measures
are proposed.   ... the source added. ?The Department of
Homeland Security is focusing on today and the crisis of the
moment. But who's looking at the broader issues of economic security
and societal stability??   ...
The advisory panel will also recommend that the President name a
bipartisan civil liberties oversight board ? drawing members from
across the political spectrum, academia and the private sector ? to
assess the impact on civil liberties of anti-terror measures such as
the Patriot Act and proposals to strengthen it. The report suggests
that greater oversight is required for any use of U.S. spy satellites
on targets inside the United States, and that legislation may be
required to set the rules. Since September 11, the National Geospatial
Imagery Agency's spy satellites have been increasingly pointed inside
U.S. borders in support of the Pentagon's new Northern Command,
charged with protecting the homeland. The agency's officials insist
they don't target American citizens. The question, said the source
familiar with the panel's work, is: ?How do we use the technology of
today to monitor the activities of citizens in a manner that protects
civil freedoms??
The panel will also recommend that the Terrorist Threat Integration
Center be established as an independent agency, and urges that the
ongoing spending of billions of dollars on anti-terror and homeland
security measures at all levels of government and in the private
sector needs to be guided by an overall strategy.
Copyright 2003 Time Inc. All rights reserved.
Reproduction in whole or in part without permission is prohibited.
[Excerpted for fair use in political discussion.]
WASHINGTON -  President Bush should appoint an  advisory board to assess how new anti-terrorism measures such as the  Patriot Act have affected Americans' civil liberties, a new report by a  federal terrorism commission says.
The commission, led by former Virginia Gov. James Gilmore, a  Republican, also expresses concern that the Bush administration has  failed to develop a comprehensive, forward-looking strategy to combat  terrorism more than two years after the 2001 terrorist attacks.
  Details of the report, to be released Monday, were first reported by  Time magazine on its Web site Saturday.
  The terrorism commission, composed of federal, state and local  officials, was created after the 1998 bombings of U.S. embassies in  Kenya and Tanzania.

@_date: 2003-12-16 13:53:24
@_author: John Gilmore 
@_subject: Difference between TCPA-Hardware and other forms of trust 
I used to run a commercial time-sharing mainframe in the 1970's.
Jerrold's wrong.  The owner of the machine has desires (what he calls
"demands") different than those of the users.
The users, for example, want to be charged fairly; the owner may not.
We charged every user for their CPU time, but only for the fraction that
they actually used.  In a given second, we might charge eight users
for different parts of that fraction.
Suppose we charged those eight users amounts that added up to 1.3
seconds?  How would they know?  We'd increase our prices by 30%, in
effect, by charging for 1.3 seconds of CPU for every one second that
was really expended.  Each user would just assume that they'd gotten a
larger fraction of the CPU than they expected.  If we were tricky
enough, we'd do this in a way that never charged a single user for
more than one second per second.  Two users would then have to collude
to notice that they together had been charged for more than a second
per second.
(Our CPU pricing was actually hard to manage as we shifted the load
among different mainframes that ran different applications at
different multiples of the speed of the previous mainframe.  E.g. our
Amdahl 470/V6 price for a CPU second might be 1.78x the price on an
IBM 370/158.  A user's bill might go up or down from running the same
calculation on the same data, based on whether their instruction
sequences ran more efficiently or less efficiently than average on the
new CPU.  And of course if our changed "average" price was slightly
different than the actual CPU performance, this provided a way to
cheat on our prices.
Our CPU accounting also changed when we improved the OS's timer
management, so it could record finer fractions of seconds.  On average,
this made the system fairer.  But your application might suffer, if its
pattern of context switches had been undercharged by the old algorithm.)
The users had to trust us to keep our accounting and pricing fair.
System security mechanisms that kept one user's files from access by
another could not do this.  It required actual trust, since the users
didn't have access to the data required to check up on us (our entire
billing logs, and our accounting software).
TCPA is being built specifically at the behest of Hollywood.  It is
built around protecting "content" from "subscribers" for the benefit
of a "service provider".  I know this because I read, and kept, all
the early public design documents, such as the white paper
  (This is no longer available from the web site, but I have a copy.)
It says, on page 7-8:
  The following usage scenarios briefly illustrate the benefits of TCPA
  compliance.
  Scenario I: Remote Attestation
  TCPA remote attestation allows an application (the "challenger") to
  trust a remote platform. This trust is built by obtaining integrity
  metrics for the remote platform, securely storing these metrics and
  then ensuring that the reporting of the metrics is secure.
  For example, before making content available to a subscriber, it is
  likely that a service provider will need to know that the remote
  platform is trustworthy. The service provider's platform (the
  "challenger") queries the remote platform. During system boot, the
  challenged platform creates a cryptographic hash of the system BIOS,
  using an algorithm to create a statistically unique identifier for the
  platform. The integrity metrics are then stored.
  When it receives the query from the challenger, the remote platform
  responds by digitally signing and then sending the integrity
  metrics. The digital signature prevents tampering and allows the
  challenger to verify the signature. If the signature is verified, the
  challenger can then determine whether the identity metrics are
  trustworthy. If so, the challenger, in this case the service provider,
  can then deliver the content. It is important to note that the TCPA
  process does not make judgments regarding the integrity metrics. It
  merely reports the metrics and lets the challenger make the final
  decision regarding the trustworthiness of the remote platform.
They eventually censored out all the sample application scenarios like
DRM'd online music, and ramped up the level of jargon significantly,
so that nobody reading it can tell what it's for any more.  Now all
the documents available at that site go on for pages and pages saying
things like "FIA_UAU.1 Timing of authentication. Hierarchical to: No
other components. FIA_UAU.1.1 The TSF shall allow access to data and
keys where entity owner has given the 'world' access based on the
value of TCPA_AUTH_DATA_USAGE; access to the following commands:
TPM_SelfTestFull, TPM_ContinueSelfTest, TPM_GetTestResult,
TPM_PcrRead, TPM_DirRead, and TPM_EvictKey on behalf of the user to be
performed before the user is authenticated."
But the historical record is clear that DRM was "Usage Scenario for TCPA.
Now, back to Hollywood.  If you have not read "This Business of Music"
(a thick book on how musicians can arm themselves with knowledge to
get slightly less screwed by the record industry -- including sample
contracts and explanations of the impact and history of each
provision), you won't know the long history of why Hollywood can be
trusted only to cheat everyone they deal with.
A music-industry contract equivalent to charging for 30% more seconds
than you deliver, is the provision for "breakage".  No artist gets
paid for more than 90% of the albums that the record company sells,
because in the days of shellac records, about 10% of them would break
in shipping.  That problem largely went away with vinyl records, and
went even further away with CDs.  Today's actual breakage is way under
1%.  But record companies won't sign a contract that pays the artist
for more than 90% of the albums shipped on CD.  That 10% underpayment
of musicians goes straight back to the record company's profits.
Their DRM software will cheat users the same way -- or a different way -- or a hundred different ways.  And TCPA will make it un-auditable
by us.

@_date: 2003-12-17 15:56:40
@_author: John Gilmore 
@_subject: The RIAA Succeeds Where the CypherPunks Failed 
Sent: Wednesday, December 17, 2003 12:29 PM
NEC @ Shirky.com, a mailing list about Networks, Economics, and Culture
            Published periodically /  / December 17, 2003
                Subscribe at            Social Software weblog at In this issue:
  - Introduction
  - Essay: The RIAA Succeeds Where the Cypherpunks Failed
      Also at   - Worth Reading:
     - GrokLaw: MVP of the SCO Wars
     - Tom Coates Talks With A Slashdot Troller
* Introduction =======================================================
The end of another year. Thank you all for reading. See you in January.
* Essay ==============================================================
The RIAA Succeeds Where the Cypherpunks Failed
   For years, the US Government has been terrified of losing surveillance
powers over digital communications generally, and one of their biggest
fears has been broad public adoption of encryption. If the average user
were to routinely encrypt their email, files, and instant messages,
whole swaths of public communication currently available to law
enforcement with a simple subpoena (at most) would become either
unreadable, or readable only at huge expense.
The first broad attempt by the Government to deflect general adoption of
encryption came 10 years ago, in the form of the Clipper Chip
[ The Clipper Chip was part of a
proposal for a secure digital phone that would only work if the
encryption keys were held in such a way that the Government could get to
them. With a pair of Clipper phones, users could make phone calls secure
from everyone except the Government.
Though opposition to Clipper by civil liberties groups was swift and
extreme [1] the thing that killed it was work by Matt Blaze, a Bell Labs
security researcher, showing that the phone's wiretap capabilities could
be easily defeated [2], allowing Clipper users to make calls that even
the Government couldn't decrypt. (Ironically, ATT had designed the
phones originally, and had a contract to sell them before Blaze sunk the
The Government's failure to get the Clipper implemented came at a heady
time for advocates of digital privacy -- the NSA was losing control of
cryptographic products, Phil Zimmerman had launched his Pretty Good
Privacy (PGP) email program, and the Cypherpunks, a merry band of
crypto-loving civil libertarians, were on the cover of
[ the second
issue of Wired. The floodgates were opening, leading to...
...pretty much nothing. Even after the death of Clipper and the launch
of PGP, the Government discovered that for the most part, users didn't
_want_ to encrypt their communications. The single biggest barrier to
the spread of encryption has turned out to be not control but apathy.
Though business users encrypt sensitive data to hide it from one
another, the use of encryption to hide private communications from the
Government has been limited mainly to techno-libertarians and a small
criminal class.
The reason for this is the obvious one: the average user has little to
hide, and so hides little. As a result, 10 years on, e-mail is still
sent as plain text, files are almost universally unsecured, and so on.
The Cypherpunk fantasy of a culture that routinely hides both legal and
illegal activities from the state has been defeated by a giant
distributed veto. Until now.
It may be time to dust off that old issue of Wired, because the RIAA is
succeeding where 10 years of hectoring by the Cypherpunks failed. When
shutting down Napster turned out to have all the containing effects of
stomping on a tube of toothpaste, the RIAA switched to suing users
directly. This strategy has worked much better than shutting down
Napster did, convincing many users to stop using public file sharing
systems, and to delete MP3s from their hard drives. However, to sue
users, they had to serve a subpoena, and to do that, they had to get
their identities from the user's internet service providers.
Identifying those users has had a second effect, and that's to create a
real-world version of the scenario that drove the invention of
user-controlled encryption in the first place. Whitfield Diffie,
inventor of public key encryption
[ the
strategy that underlies most of today's cryptographic products, saw the
problem as a version of "Who will guard the guardians?"
In any system where a user's identity is in the hands of a third party,
that third party cannot be trusted. No matter who the third party is,
there will be at least hypothetical situations where the user does not
want his or her identity revealed, but the third party chooses or is
forced to disclose it anyway. (The first large scale example of this
happening was the compromise of anon.penet.fi, the anonymous email
service, in 1995
[ Seeing that this problem
was endemic to all systems where third parties had access to a user's
identity, Diffie set out to design a system that put control of
anonymity directly in the hands of the user.
Diffie published theoretical work on public key encryption in 1975, and
by the early 90s, practical implementations were being offered to the
users. However, the scenario Diffie envisioned had little obvious
relevance to users, who were fairly anonymous on the internet already.
Instead of worrying now about possible future dangers, most users'
privacy concerns centered on issues local to the PC, like hiding
downloaded pornography, rather than on encrypting network traffic.
However, Diffie's scenario, where legal intervention destroys the users'
de facto privacy wherever it is in the hands of commercial entities, is
now real. The RIAA's successful extraction of user identity from
internet service providers makes it vividly clear that the veil of
privacy enjoyed by the average internet user is diaphanous at best, and
that the obstacles to piercing that veil are much much lower than for,
say, allowing the police to search your home or read your (physical)
mail. Diffie's hypothetical problem is today's reality. As a result,
after years of apathy, his proposed solution is being adopted as well.
In response to the RIAA's suits, users who want to share music files are
adopting tools like WINW (WINW Is Not WASTE) [ and
BadBlue [ that allow them to create encrypted
spaces where they can share files and converse with one another. As a
result, all their communications in these spaces, even messages with no
more commercial content than "BRITN3Y SUX!!!1!" are hidden from prying
eyes. This is not because such messages are sensitive, but rather
because once a user starts encrypting messages and files, it's often
easier to encrypt everything than to pick and choose. Note that the
broadening adoption of encryption is not because users have become
libertarians, but because they have become criminals; to a first
approximation, every PC owner under the age of 35 is now a felon.
The obvious parallel here is with Prohibition. By making it
unconstitutional for an adult to have a drink in their own home,
Prohibition created a cat and mouse game between law enforcement and
millions of citizens engaged in an activity that was illegal but
popular. As with file sharing, the essence of the game was hidden
transactions -- you needed to be able to get into a speakeasy or buy
bootleg without being seen.
This requirement in turn created several long-term effects in American
society, everything from greatly increased skepticism of Government-
mandated morality to broad support for anyone who could arrange for
hidden transactions, including organized crime. Reversing the cause did
not reverse the effects; both the heightened skepticism and the
increased power of organized crime lasted decades after Prohibition
itself was reversed.
As with Prohibition, so with file sharing -- the direct effects from the
current conflict are going to be minor and over quickly, compared to the
shifts in society as a whole. New entertainment technology goes from
revolutionary to normal quite rapidly. There were dire predictions made
by the silent movie orchestras' union trying to kill talkies, or film
executives trying to kill television, or television executives trying to
kill the VCR. Once those technologies were in place, however, it was
hard to remember what all the fuss was about. Though most of the writing
about file sharing concentrates on the effects on the music industry,
whatever new bargain is struck between musicians and listeners will
almost certainly be unremarkable five years from now. The long-term
effects of file sharing are elsewhere.
The music industry's attempts to force digital data to behave like
physical objects has had two profound effects, neither of them about
music. The first is the progressive development of decentralized network
models [], loosely bundled together under the rubric of peer-to-peer.
Though there were several version of such architectures as early as the
mid-90s such as ICQ and SETI at Home, it took Napster to ignite general
interest in this class of solutions.
And the second effect, of course, is the long-predicted and oft-delayed
spread of encryption. The RIAA is succeeding where the Cypherpunks
failed, convincing users to trade a broad but penetrable privacy for
unbreakable anonymity under their personal control. In contrast to the
Cypherpunks "eat your peas" approach, touting encryption as a
first-order service users should work to embrace, encryption is now
becoming a background feature of  collaborative workspaces. Because
encryption is becoming something that must run in the background, there
is now an incentive to make it's adoption as easy and transparent to the
user as possible. It's too early to say how widely casual encryption use
will spread, but it isn't too early to see that the shift is both
profound and irreversible.
People will differ on the value of this change, depending on their
feelings about privacy and their trust of the Government, but the
effects of the increased use of encryption, and the subsequent
difficulties for law enforcement in decrypting messages and files, will
last far longer than the current transition to digital music delivery,
and may in fact be the most important legacy of the current legal
* Worth Reading =======================================================
- GrokLaw: MVP of the SCO Wars
My colleague Elizabeth Lawley of RIT has convinced me that one of the
most profound effects of weblogs is the communal workings of those who
publish them, and that they contribute significant new value to
collaboration across disciplines and boundaries.
And now that she's convinced me, I see the pattern everywhere. The Dean
campaign piece I posted earlier today exhibits much of that pattern, and
so does today's Groklaw piece on SCO. By way of background, SCO, once a
technology company, has become a company devoted to a single legal
1. Assert rights to the Unix operating system
2. Assert infirnging contributions of Unix source code to Linux 3. Sue
firms that sell or use Linux, especially deep-pocketed IBM 4.
Profit!!!1! (or at least buyout by IBM, to save them the expense of the
Much of the matter is in dispute, and IANAL, but what is clear is
this: a) many SCO employees contributed to the Linux kernel, back when
SCO was a tech company ("oldSCO"), with the approval of their bosses,
and b) the Groklaw is doing an astonishing, world-changing job of
finding, documenting and publicizing these occurrences (alongside much
other work on the case.)
A recent GrokLaw entry reads:
   Groklaw has reported before on contributions made to the Linux
   kernel by Christoph Hellwig while he was a Caldera employee.  We
   have also offered some evidence of contributions by oldSCO employees
   as well.  Alex Rosten decided to do some more digging about the
   contributions of one kernel coder, Tigran Aivazian.
   [...]
   This paper is a group effort.  Alex's research was shared with
   others in the Groklaw community, who honed, edited, and added
   further research.  Then the final draft was sent to Tigran himself,
   so he could correct and/or amplify, which he has done.
   Look at that second graf: "This paper is a group effort." Everyone
always says that about complex work, but this is different. This is the
end of two-party law, where plaintiff and defendant duke it out in an
arms race of $350/hr laywers and "Take that" counter-motions.
Instead, we have a third party, Groklaw, acting as a proxy for millions
of Linux users, affecting the public perception of the case (and the
outcome SCO wants has to do with its stock price, not redress in the
courts.) Groklaw may also be affecting the case in the courts, by
helping IBM with a distributed discovery effort that they, IBM, could
never accomplish on their own, no matter how may lawyers they throw at
There are two ways to change the amount of leverage you have. The
obvious one is to put more force on the lever, and this is what SCO
thought they were doing -- engaging IBM in a teeter-totter battle that
would make it cheaper for IBM to simply buy SCO than to fight it out in
the courts.
The other way to get more leverage is to move the fulcrum. Groklaw has
moved the fulcrum of this battle considerably closer to SCO, making it
easier for IBM to exert leverage, and harder for SCO to. I can't predict
how the current conflict will end, but the pattern Groklaw has
established, of acting on behalf of the people who will be adversely
affected by a two-party legal battle, has already been vindicated, even
if SCO avoids bankruptcy.
- Tom Coates talks with a Slashdot troller:
Tom Coates, who has been talking on EverythingInModeration.org about his
travails with a persistent troll on the Barbelith community and his
subsequent attempts to ban that user, has elicited a response, which has
now become a conversation, with a slashdot troller. This troller,
posting as 20721, is arguing that any hidden moderation system helps
stimulate an arms race:
   i believe that it takes a certain amount of hubris to assume that
   the people you want to exclude are, by their nature, not as smart as
   you. you may be right about the people you're trying to exclude; i
   defer to your judgement, i'm not a member of the communities you
   are; but where i come from, the best & the brightest are the ones
   being cast out. they're cast out from communities by the following
   chain of events:
   1) secretive backhanded moderation tactic by the admins is discovered
   2) someone alerts the community
   3) the most technically apt in the community are able to reproduce
   the backhanded moderation tactic and verify its existence
   4) these people call foul and are labelled "trolls" for doing so,
   leading to the institution of more of 1) (repeat).
   this is how i started down the road i'm on. i was one of the many
   people who discovered that the people at slashdot were secretly
   moderating the users' comments, and one day they moderated the same
   comment 800 times - and then they lied about it, and said anyone who
   told the truth about it was a "troll". hence i became what they
   called me.
More, much more, at
* End
This work is licensed under the Creative Commons Attribution License.
The licensor permits others to copy, distribute, display, and perform
the work.  In return, licensees must give the original author credit.
To view a copy of this license, visit
or send a letter to
Creative Commons, 559 Nathan Abbott Way, Stanford, California 94305,
2003, Clay Shirky _______________________________________________
NEC - Clay Shirky's distribution list on Networks, Economics & Culture
NEC at shirky.com

@_date: 2003-12-30 22:16:38
@_author: John Gilmore 
@_subject: hiding attestation from the consumer 
This statement reveals confusion between the "parties".  There are at least
three parties involved in an attestation:
  *  The DRM'd product vendor (somewhere on the net)
  *  The consumer (sitting at their PC)
  *  The PC hardware and software vendors (building attestation in)
There are strong reasons to hide the content of the attestation -- or
even its mere existence -- from the consumer party.  If consumers knew
their PCs were spying on them and letting vendors say, "Sorry, our
server is down today" not because the server is down, but because the
consumer's PC is blacklisted, then consumers would be upset.  It's a
much simpler "customer relations" problem if it just doesn't happen to
work, without the consumer ever finding out that they live in a
redlined neighborhood and it will NEVER work for them.
It's really easy to infer that DRM problems are going to be
deliberately inscrutable.  You don't see DRM vendors advertising the
restrictions on their products.  These restrictions aren't in boldface
in the table of contents.  They're hidden deep in the guts of the
manual, if they appear at all.  (In the list of error messages is
where you usually find 'em, with a very brief mention.)  It's the
consumer's fault, or their ISP's fault, or somebody else's, if the
site doesn't work for you.  If your DAT recorder won't record, you
must have cabled it up wrong.  If your HDTV won't work, you ran it
through your VCR by mistake.  And if your music site won't download to
you, you must have installed your software wrong, or there's a
firewall problem, or your codecs are incompatible, or something.  When
the entire goal is to covertly change consumer behavior, by making
things that are utterly legal simply NOT WORK, plain language about
the restrictions has no place.  Consumer problems caused by DRM are
seldom advertised, documented, or reported as the DRM's fault.
You can get a similar effect merely by turning off cookies and
JavaScript today.  (You *do* use a browser that has simple switches to
turn these off, right?  Mozilla is your friend, and it runs on your
platform.)  Web sites will start to fail at random, in inscrutable
ways.  Only about 1% of them will tell you "This site requires
JavaScript" -- and of those that do, only about a quarter of them
actually do require it.

@_date: 2003-02-08 17:49:10
@_author: John Gilmore 
@_subject: NASA/NSA searching for Shuttle encryption system 
AP reported on Feb 7 that NASA is looking for a secret device that
encrypts communication between the shuttle and ground controllers.
If someone else finds it they could "study the technology", says the
AP.  Sounds like fun for cypherpunks.  Anybody seen it on eBay?  :-)
Alternatively, c'punks could plant fake mil-spec DES or CPRM
encryption boxes around Louisiana and Texas in the hope that foreign
spies will find them and 'crack' them.

@_date: 2003-01-08 02:17:40
@_author: John Gilmore 
@_subject: DeCSS, crypto, law, and economics  
Actually, the scheme was invented in Japan, and the
"predecessor-in-interest to the DVD-CCA", Matsushita, designed it to
be weak because Japanese export laws prevented the export of more than
40-bit encryption.
The US had pressured Japan to impose 40-bit crypto export controls.
The Japanese laws didn't change, even after EFF's Bernstein lawsuit
and commercial firms' political pressure forced US policy to become
sensible.  Last I heard, crypto export is still a morass in Japan.
Correct, but.  One of the basic prongs of the entire DVDCCA "trade
secret" series of cases was that the reverse-engineering had been
illegal in Norway.  If it wasn't illegal to do it, it wasn't illegal
to reproduce the results of it.  Since Norwegian courts have
determined that it wasn't illegal to reverse-engineer it, there is no
case against any of the defendants.  Like Matt Pavlovich, Andrew
Bunner, and many dozens of other people who DVDCCA have been trying to
drag into California courts.  You may not have noticed, but EFF and its
pro-bono partners have been spending major time on winning these cases.
The Norwegian decision will make it much easier.
This is false.  Market segmentation by country is deliberately
outlawed by "free trade" laws and treaties, which exist to benefit
consumers by letting them import whatever products they want from
other countries.
For example, in New Zealand, the DVD region-code system was
found to violate their free-trade laws, and therefore New Zealand
never permitted one-region players to be sold there.
The Coors brewery tried to limit distribution of their beer to certain
Western states.  They failed.  My local liquor store in Washington, DC
made a ton of money bringing in semi-loads of Coors, in violation of
Coors company policy, and selling them to thirsty expatriate Rocky
Similarly, the US Supreme Court recently struck down laws in many US
states that prohibited the interstate purchase of wine and other
products.  These laws were all designed to benefit local producers, at
the expense of local consumers.  Most of these laws were wrapped up in
a cloak of "consumer protection against shoddy products" or
"protection of minors" but it was easy to pierce that veil to see the
monopoly interest.
(This is not to say that market segmentation is dead in the US!  Many
continue.  The federally supported "Milk Compact" deliberately
segments the New England market and costs consumers of milk many
billions of dollars per year.  The federal DMCA has nothing to do with
protecting copyrights and everything to do with protecting monopolies,
as the judge agreed in the 2600 case.  Many state and local laws
continue to restrict entry into fields such as lawyering, surveying,
haircutting, and even carpentry ("union shop" laws).  Producers are
always looking for political opportunities to outlaw their
competition, and there are always corrupt people inside governments,
who are happy to oblige.)
Unfortunately you set the wrong tone by starting as apologist for it.
The kind of segmentation your graphs rely on can easily be created
by *time* segmentation.  Producers start off charging high prices for
their goods, and then gradually reduce the prices as they ramp up
volumes, pay off their startup costs, learn the desires of their market
better, etc.  This gets the social benefit you desire, without propping
up any artificial forms of segmentation.
Of course, there are always people who will claim that people aren't
free to change their prices up or down over time.  (After the
earthquake, according to those folks, bottled water should sell for
the same price as before, even if at that price the entire supply has
sold in two hours, to the people who value the water least.)
That's not actually true.  Several years before the DMCA passed, the
legal control structure was in place.
The studios got a couple of manufacturers (including Matsushita) to
design an encryption system (CSS).  The companies & studios set up a
licensing entity that would issue CSS licenses to manufacturers of DVD
players, makers and operators of DVD pressing equipment, and copyright
holders.  These licenses were relatively cheap to buy, but imposed
most of a hundred pages of restrictions on what the licensees could
do.  One thing that they could NOT do is to build multi-region
Manufacturers were free to build DVD players that would play
UNencrypted disks, without signing any license with Matsushita or the
DVDCCA.  But since Hollywood was only releasing encrypted disks, any
manufacturer who didn't play along would have useless products (at
least until DVD Recorders came along a few years later, allowing
consumers to record things of their own making on unencrypted disks).
Thus, the control was via a contract that manufacturers, studios,
and pressing plants had to sign in order to get access to the trade
secrets required to interoperate.  That contract contained many specific
provisions that prohibited unencrypted digital outputs, required
the no-fast-fowarding crap and region codes, and had many other
anti-consumer features.
The licensing entity was a subsidiary of Matsushita.  The licensing
authority was only transferred to the DVDCCA a few weeks before the
lawsuits started.  Apparently Matsushita didn't want to be known as
the heavy who was suing everybody (they succeeded in keeping their
name out of EVERY ONE of the cases and out of the press).  And it
probably looked better for the licensing organization to be a
"California nonprofit" rather than a "Japanese megacorp", particularly
when trying to sue competitors under California law, claiming damage
in California.
Such a license would only survive until the trade secret was reverse
engineered, which is legal to do in almost all jurisdictions.  But
most of the parties likely to benefit from that reverse engineering
had already agreed to the license.  And anyone with no money, but who
had not yet published the secret, could be tied in knots for years by
overpaid studio lawyers.  It took the free spirit of honest
technologists who refused to sign restrictive licenses, and who
believe in open publication of scientific and technological ideas, to
do the hard work of reverse engineering that benefited all DVD
consumers.  And on them has fallen the punishment of years of harassment and uncertainty by the studio mafia.
(You'll note that even after CSS was broken, the industry didn't stop
pressing DVDs, and continues to make increasing billions of dollars
from making and selling them.  While the breaking of CSS benefited
consumers, it did not harm producers.  As usual, the producers' fears
were overblown ghosts, just as in the Betamax VCR situation.)
Just in case somebody came along to cater to the market of consumers
who wanted restrictionless players, the studios and their buddies
in other monopolies paid off politicians to pass the DMCA.  But they
were releasing DVDs and players long before it passed.  It was a
belt-and-suspenders strategy.
This is probably also false.  The reason is that today you can't get
consumer DVD recorders that will let you record encrypted DVDs.  Their
firmware refuses to write in the key area of the disk, and the blank
disks are shipped with the key area obliterated.  (And, DVD readers
will only let you read out the keys from a disk after you've reverse
engineered some simple bits that the industry wouldn't reveal.)  So
you can't do any bit-for-bit copying of DVDs unless you have a very
expensive (and restrictively licensed) DVD mastering press.
How this lack of bit-copying ability came to be, we haven't unearthed
yet.  No other major computer storage technology has lacked it (though
every medium invented since then has tried to shoehorn it in -- even
hard disk drives!).  The patents on DVD recording technologies are
owned by another consortium, and they were probably pressured by
Hollywood into putting this condition into their licenses.  Apple, the
first major computer company to release a DVD recorder, was
notoriously silent on the whole subject, while their monster
"Rip. Mix. Burn" billboards tried to create the opposite impression
among consumers.
Since the keys would get lost in the transfer, consumers wouldn't be
able to make copies of DVDs for backup or for their friends, the way
they can back up CDs, mix songs from different CDs, transfer them into
their preferred formats like MP3 on hard drives, etc.  The cracking of CSS has made all of those applications possible.  For
this we must thank the always innocent Mr. Jon Johansen, and also
particularly Frank Andrew Stevenson, who cryptanalyzed CSS and made
player keys unnecessary, and the LiViD project, which turned their
early prototypes into point-and-click free software for Linux.

@_date: 2003-01-14 05:12:06
@_author: John Gilmore 
@_subject: Air ID: Gilmore v. Ashcroft: Friday AM hearing in SF 
[Moderator's note: This isn't exactly about cryptography, but I think
it is pretty important so I'm forwarding it. --Perry]
My case against John Ashcroft, TSA, and various other agencies will
have its first hearing at 9AM on January 17, 2003 in San Francisco.
You-all are encouraged to attend if you're nearby.  We'll be arguing
about whether the case should be thrown out as invalid.
I'm asking for a declaration from the court that would overturn the
unconstitutional requirement that US persons must show ID to travel
throughout the US.  Not only airplanes, but trains, buses, cruise
ships, and major hotel chains are now enforcing ID requirements,
largely at the behest of the Federal Government.  Many skyscrapers
also demanded ID for a time after 9/11; I refused, and eventually most
of them have relented.  I have not flown in the US since 9/11/01, and
I've recently been denied lodging as well as travel, for my refusal to
present ID on demand.  (Note that this is a *separate* issue from the
government's recent demand for more information from citizens who
enter or exit the US border.  That's a bad idea too, but raises
different issues.)
We free citizens not only have a constitutional right to travel
throughout the US without government-imposed restrictions, but also a
constitutional right to refuse to identify ourselves to government
agents unless there is probable cause to suspect us of a crime.  These
aren't made-up issues.  There are many legal cases that uphold them in
the last few decades, as well as more than a hundred years ago.  Read
our reply brief for a guide to these cases:
We citizens also have a right to know what the laws are that affect
the general public.  There is no such law requiring IDs of travelers,
and TSA won't publish their secret regulation that purports to require
ID.  So nobody actually knows whether ID is required, in what
circumstances, what kinds of ID are OK or not, what options people
without ID have, etc.  (By nobody, I really mean nobody -- not even
the people "enforcing" this "rule" know what the "rules" are.  Try
refusing to show ID on your next flight, and when they tell you that
you can't board, ask them what regulation requires you to show ID to
board a plane.  I did this on July 4, 2002.  The resulting confusion
of different answers from each person in authority would be very
amusing if it wasn't an unconstitutionally vague infringement of our
right to travel.)
The government and airlines responded to my lawsuit with a motion to
dismiss the case.  Here are their arguments: I can't challenge
anything but the demand for ID, not what they do with it after getting
it (thus I can't challenge or inquire into the "no-fly list" and other
database lookups that *motivate* the demand for ID).  I can't
challenge anything at all in this court, because the Courts of Appeals
have exclusive jurisdiction over TSA orders.  The government need not
publish a rule like this, because (1) TSA security directives are
exempt from FOIA by statute, and (2) no other reason requires them to
publish the law.  The airlines' "request" for ID does not infringe my
right to travel, even if they don't let me travel when I "decline".  I
can't challenge the ID demand on trains, buses, or cruise ships
because I didn't actually go down and get rejected by a train, bus, or
cruise line, even though their web sites told me I'd be rejected.  The
First Amendment is not involved, even though I can't assemble with
others, speak at conferences, or petition the government for redress,
without traveling.  Anything they do to anyone in an airport is exempt
from the Fourth Amendment if they claim it relates to "security".
That's the short version; you can read the long version here:
 and their later reply here:
The government gave away their real motivation on the last page of
their brief, though:
  "If a passenger refuses to provide or verify his or her identity,
   airline security officers cannot determine whether the passenger is
   among those individuals 'known to pose, or suspected of posing, a risk
   of air piracy or terrorism or a threat to airline or passenger
   safety.'"
In other words, the demand for ID is integral to building a dragnet.  They
have a little enemies list of "suspected" "threats", and they need our
ID to check us against the list.  (How you get on and off this list is
dubious and secret, but many documented cases exist of innocents being
harassed, searched, and denied boarding, due to errors in it.)  The
constitutional catch is that the government can't set up a dragnet and
demand that every passerby identify themselves -- not in airports, and
not anywhere else unless there's an exigent emergency (e.g. a bank was
just robbed down the block by someone matching our description).  A
demand for ID is a search under the Fourth Amendment, and they have no
probable cause to search us.
We will argue that the case should not be dismissed, in the courtroom
of Judge Susan Illston, on the 19th floor of the San Francisco Federal
Building, 450 Golden Gate Avenue at Polk St, at 9AM on January 17, 2003.
If you think airport security is out of hand, show up.  If you think
Total Information Awareness is a terrible idea, show up.  (CAPPS 2 is
the version of TIA they'll roll out in airports in 2003, and it all
hangs on the demand for your ID.)  If you think John Ashcroft is a
traitor to the Constitution he swore to uphold, show up.  If you think
every "free" citizen should not be routinely treated like a suspected
terrorist, show up.  Wear good clothes and be polite.  Impress the
judge with the seriousness of your interest in these issues.  Oh yes,
you'll have to show ID to get into the Federal Building.  That's unconstitutional too, but not the subject of this particular case.
You can read all the case documents at:
  Thank you.

@_date: 2003-01-15 00:53:53
@_author: John Gilmore 
@_subject: RIAA turns against Hollings bill  
The detailed RIAA statement tries to leave exactly this impression,
but it's the usual smokescreen.  Check the sentence in their "7 policy
principles" joint statement, principle 6:
  "...  The role of government, if needed at all, should be limited to
   enforcing compliance with voluntarily developed functional
   specifications reflecting consensus among affected interests."
I.e. it's the same old game.  TCPA is such a voluntarily developed
functional spec.  So is the "broadcast flag", and the HDCP copy
protection of your video cable, and IBM's copy-protection for hard
disk drives.  Everything is all voluntary, until some competitor
reverse engineers one of these, and builds a product that lets the
information get out of the little "consensus" boxes.  Consumers want
that, but it can't be allowed to happen.  THEN the role of government
is to eliminate that competitor by outlawing them and their product.

@_date: 2003-01-22 22:35:19
@_author: John Gilmore 
@_subject: making anonymity illegal 
Will's straight answer points up the magnitude of the problem we face.
It doesn't matter what the Constitution says; the government has figured
a way around the Constitution.
The courts have already ruled that the government can't require someone
to produce ID unless they have 'probable cause' to search that person.
(Kolender v. Lawson, 9th Circuit; upheld by Supreme Court on other grounds).
There's no probable cause to search every person using the Internet (or
every person entering a Federal building, or airport, or ...).
The government's problem is: to circumvent the Fourth Amendment, which
prohibits the government being able to search any person on flimsy
excuses.  The answer is: The government merely mandates that THIRD
PARTIES collect all the information that the government desires to
search on flimsy excuses.  The government gets a court rulings that
say people have no Fourth Amendment interest in records about them
which are held by third parties (These rulings have existed for
years).  Then the government searches those third party records at
This is the Constitutional end-run that's behind CAPPS 2, TIA, and
many other government programs (catching deadbeat moms, etc).
Our trick will be to teach the courts about how this end-run works.

@_date: 2003-01-23 02:30:57
@_author: John Gilmore 
@_subject: Peacefire & VOA need tech help to defeat China Firewall 
Sent: Monday, January 20, 2003 6:26 AM
[You are receiving this after signing up for membership in Peacefire at
  To unsubscribe yourself from this list and
cancel your Peacefire membership, see unsubscription instructions at the
end of this message.]
Happy New Year to everybody -- sorry it's been an unusually long time since
the last Peacefire newsletter, but the good news is that there are big
things coming in 2003.  Peacefire has joined forces with Voice of America
, a federal agency that used to do pro-democracy radio
broadcasts into communist Eastern Europe and Asia, and is currently still
broadcasting into China while branching out into finding ways to defeat
Internet censorship.  They've contracted with us to help defeat the "Great
Firewall of China", the firewalls put in place by the Chinese censors to
block people in China from reading foreign Web sites that criticize the
Chinese government.  The technology could be extended to help people in
other regions such as the Middle East where the Internet is heavily
This is the kind of project that I hope many tech-savvy members will be
able to help with, one way or another.  Personally I think this may be one
of the most important things I ever get to work on, if not *the* most
important.  To a lifelong puzzle-hobbyist, it's like a dream: working on a
problem that's like a giant, open-ended puzzle that's never been completely
solved, where the answer could help millions of people around the
world.  As for working on the problem itself, it requires some technical
knowledge, but not a lot; I coach a high school math team and I've worked
on some of these problems with the students in the math club.  I could do
that since there's nothing classified about the solutions to the problem
that VOA has asked us to find, because our strategy is to assume the
Chinese censors will be able to "take apart" the software and figure out
how it works anyway, so we should publish all the details of how it will
work, and encourage people to try and find ways to defeat the system.  Only
if the complete design is published and nobody can find any flaws that
would enable the censors to attack it, then we go ahead with building it
according to that design.
One of the first papers I put out as part of the project, was about the
common pitfalls and problems with many existing "anti-censorship" systems:
If you can follow most of the discussion on that page, you'd probably be
able to help.  It's less about technical knowledge, and more about looking
at a given problem through new angles, so it's an ideal technical project
for young people to contribute.
There are several existing anti-censorship projects out there, made by
companies including SafeWeb, DynaWeb, and a self-described hacker coalition
called Hacktivismo, all of which have contributed some valuable insights,
but many of their designs fall prey to the attacks listed at the URL
above.  Also, none of the other groups working on this problem have
published the details of how their proposed solutions work, so there may be
other problems that haven't come to light yet.  (If any of their programs
ever came to represent a serious threat to the Chinese censors, the Chinese
government would almost certainly "take it apart" to find out how it works
and find any exploitable weaknesses, so keeping the design secret is really
just delaying the inevitable.  This is why our strategy is to publish the
design in advance, and only proceed with it if no one is able to find a
weakness in the design, even knowing all the details of how it works.)
One good question that nobody has asked me, but some people probably will,
is why I would be asking people to contribute ideas for free, if VOA is
paying me.  I would say that even if you subtract all the hours per week
that VOA has paid for at a normal programmer's salary, that still leaves a
lot of hours every week that I'm working on the project, which could be
considered "donated" time (not to mention all those years with Peacefire,
which is how VOA heard about us in the first place :) ).  In any case, it's
up to each individual person whether they want to help.  Besides, the most
important part of the process is to have many reviewers look at the
software design and try to find flaws that the censors could exploit, and
that doesn't take any minimum time commitment.
As part of this project, Peacefire is probably going to move towards fewer
consumer-reports-style pages about what blocking software really blocks,
and more towards work on anti-censorship technology.  We will still help to
publicize the problems with blocking software, especially when the Supreme
Court decides this year whether the "Children's Internet Protection Act" is
constitutional, which requires blocking software to be installed on all
computers used by children *or* adults, in any library that receives
federal funding.  But for the most part, most people who are paying any
attention at all, have gotten the message that blocking software is sloppy
and often politically motivated.  Plus, many other research groups are now
also doing studies on the problems with blocking software.  On the other
hand, developing secure anti-censorship technology is still something that
no group has ever pulled off completely, and I think we're in a position to
do it.
If you'd be interested in working on the design for an anti-censorship
program, you might want to check out the URL above.  Some other recommended
reading on how the design has evolved so far, most of which is about
pitfalls in existing systems, pitfalls that our design should avoid:
Problems with using a "distributed cloud" of circumvention points to defeat
Internet censorship:
An attack that can be used to map out a peer-to-peer network of machines
being used as circumventors:
An attack that can be used against Anonymizer-type Web sites even if they
encrypt page contents using HTTPS:
As you'll notice if you read those, all the stuff so far has my name on
it.  Let's do something about that :)  If you'd be interested in
contributing in any way, email me at bennett at peacefire.org with some
information about your background if you want (even though no background is
necessary).  We'll be setting up a separate mailing list to discuss the
strategies for anti-censorship software, and anybody can contribute ideas
for possible attacks against the anti-censorship that the censors might use

@_date: 2003-07-20 00:55:52
@_author: John Gilmore 
@_subject: Cnet: location wiretapping on hold; T-Mobile to pay up for E911 delay 
The FCC is certainly turning Orwellian these days.  Now the firms
that it regulates are making "voluntary" contributions to the government
at the whim of the FCC.  Remember, these are the regulators who
sided totally with the FBI when it demanded that everything be designed
for wiretapping, even the stuff that was specifically not covered
in the statute passed by Congress.
But the good news is that none of the nationwide cellphone carriers
has yet deployed working technology for tracking the locations of
cellphones.  We may have a few months of freedom from permanent
location tracking left.  Time to go back to 1-way beepers, and only turn
on your cellphone when you get beeped.
*T-Mobile to pay up for E911 delay*
By Ben Charny Staff Writer, CNET News.com
July 18, 2003, 12:26 PM PT
*T-Mobile USA will pay a $1.1 million "voluntary contribution" to federal regulators for missing deadlines to make it possible for emergency call center operators to locate cell phones dialing 911.*
The carrier, a subsidiary of Deutsche Telekom, also has agreed to a new, 22-month deadline to meet the Federal Communications Commission's so-called E911 rules, according to the FCC.
All U.S. cell phone carriers have missed deadlines for a federally mandated schedule to make it possible for police to know the location of cell phones that call 911, something they can't do now. The FCC has asked AT&T Wireless  and Cingular Wireless  to make similar "contributions" to the agency's coffers.
Traditional landline phone providers complied to the E911 mandate  several years ago. Emergency call centers credit their ability to locate a landline phone with saving the lives of those unaware of where they are or too injured or panicked to provide details. With more than half of all 911 calls now coming from cell phones, emergency call centers say they need the E911 service more than ever.
T-Mobile representatives did not return phone calls Friday seeking comment.
The company has 22 months to begin clearing the backlog of requests for E911 service from emergency call centers. If not, it could be on the hook for more "voluntary" contributions, the FCC wrote in its order released Thursday.
T-Mobile switched to new E911 technology in March. According to the FCC report, the carrier is still determining if the technology is accurate enough  to meet the agency's mandate. T-Mobile told the FCC it expects the technology will pass muster.

@_date: 2003-06-10 06:21:17
@_author: John Gilmore 
@_subject: "C.Wiebes": Bosnia SIGINT & Intelligence '92-'95 
[Dr. Wiebes ran the excellent conference on Cold War SIGINT, held
in the Netherlands a few years ago.  -- John]
It is my pleasure to inform you that book dealing with the Intelligence and the War in Bosnia 1992 - 1995 has been published this week. In this book are two chapters dealing with Sigint during this war.
You can find details about my book here.
If interested I can mail the contents of the various chapters.
Best regards
Dr. Cees Wiebes

@_date: 2003-06-12 22:35:12
@_author: John Gilmore 
@_subject: An attack on paypal  
Yes; this is why (I think) VeriSign bought Network Solutions.  Then no
matter who wins the tussle over which infrastructure certifies
peoples' keys, VeriSign will own it.  Of course, canny spooks at SAIC
extracted billions of dollars from VeriSign for this privilege...after
extracting hundreds of millions from gullible public investors, and
extracting more from domain users via their official government
approved monopoly...

@_date: 2003-03-03 17:47:06
@_author: John Gilmore 
@_subject: NSA being used to influence UN votes on Iraq  
JI questioned:
If the US found a similar memo from the French government, you can be
sure it would be published immediately as newsworthy.  At least in the
lapdog US press.
NSA's instructions to find tidbits usable to sway Security Council
members were newsworthy in the UK, because the UK government is
warmongering to suck up to the US, while the UK populace is opposed to
the war.  So "dirty tricks" being played by the US and UK governments
to impose their will on the world are interesting to the UK populace.
Most people regard wiretapping their opponents as an evil act,
violative of privacy norms.  Some people condone it in international
relations on self-defense grounds; if your own life is threatened,
then you gouge the other guy's eyes out, or chop off his hand, despite
being revolted by doing that in normal life.  But when wiretapping is
used to overturn a legitimate sovereign government, which poses no
obvious threat, then wiretapping is not justifiable on self-defense
grounds.  Civilized morality, rather than brute survival, becomes the
defining standard.  And the US is violating the standards of civilized
morality by wiretapping its opponents (and its allies and neutrals) in
an attempt to start a war of aggression.
Tell me, how well have the cypherpunks done, after a decade, at
protecting their own communications?  We're still mostly talking in
the clear, as far as I can tell.  And no cypherpunk, to my knowledge,
is well defended against the kinds of miniature bug that would
routinely be planted in every suit jacket laundered anywhere near the
UN Building.
What was most interesting for me about that NSA message was that it
said they needed to add "surge capacity" on some countries on the
Security Council.  Notably absent from the list was Mexico, which is
on the Security Council.  I guess NSA is already monitoring Mexican
diplomatic communications so well that they didn't need to add any
PS: I spent a few weeks in Mexico last month.  The majority of
Mexicans want peace, as does their populist leader.  Spain tried to
sway Mexican president Vicente Fox from the peace position, and got
nowhere.  People who have recently experienced war first-hand tend to
view it as more of a last resort, compared to people who have only
experienced war via TV, videogames, and economic downturns.

@_date: 2003-03-06 00:19:34
@_author: John Gilmore 
@_subject: Delta CAPPS-2 watch: decrypt boarding passes! 
Delta Air Lines is the guinea pig for the CAPPS-2 intrusive database
search on every passenger.  They'll be doing this in three cities,
starting THIS MONTH.
First, if you were thinking of flying, be sure not to fly on Delta.
See Second, if you're stuck on Delta, or want to watch their system, then
please report back (to me, gnu at delta.toad.com, or to the cryptography
list) about how the airport checkin and screening process has changed.
We should be able to rapidly figure out which cities they are doing
this in, based on the airline's behavior changes.
For example, some stories say that the system will require more info
from you, like your home address and date of birth.  Other stories say
that no new info is collected.  One has pointed out that Delta's
frequent flyer program has collected birthdate info for years.  I
suggest flying WITHOUT tying your flight into the frequent flyer
Also, most news stories claim that your boarding pass will have
"encrypted" on it a "red/yellow/green" flag that tells the security
screeners whether to:
    *  Block you from getting on the flight
    *  Search the hell out of you
    *  Let you walk through with minimal hassle
The stories report that the security screeners at the checkpoint might
have new machines to run your boarding pass through (to "decrypt" this
info).  This could all be disinformation.  If true, it should be easy
to spot, particularly if you've flown through these airports before.
And, besides identifying what cities they're doing this in, we should
also start examining a collection of these boarding passes, looking
for the encrypted "let me through without searching me" information.
Or the "Don't let me fly" information.  Then we can evaluate how easy
it would be to turn one into another.  (Don't mistake a system that
claims to provide security for one that actually does.)
I'll restate just for the record that I oppose this entire program,
as well as the unconstitutional demand for ID before traveling in the US.
I'm suing Ashcroft, TSA, and Homeland Security over it.  We're currently
awaiting Judge Illston's decision on the government's motion to dismiss
the case as frivolous.  (How many of you who thought it was frivolous
eight months ago, still think it is?)

@_date: 2003-03-31 10:44:35
@_author: John Gilmore 
@_subject: Russia Intercepts US Military Communications?  
1.  "Look for plaintext."  This was rule  stated by Robert Morris
Sr.  in his lecture to the annual Crypto conference after retiring as
NSA's chief scientist.  You'd be amazed how much of it is floating
around out there, even in military communications.
2.  Wars are great opportunities to learn what other folks are doing
for communications security.  Whether or not you are a belligerant in
the war, you clearly want to be focusing your interception
capabilities on that battlefield and its supply and command trails.
Besides operational errors made under stress, which can compromise
whole systems, you just learn what works and what doesn't work among
the fielded systems.  And what works or not in your own interception
facilities.  Wars are much better than sending probe jets a few miles
into an opponent's territory, to show you how their electronics work.
Given the ease of writing strong encryption applications, I'm amazed
that civilian communications are seldom -- very seldom -- encrypted.
Deployment and interoperability without introducing major
vulnerabilities is much harder than just designing algorithms and
writing code.  It involves changing peoples' habits, patterns, and
Remember, the cypherpunks cracked Clipper and DES, deployed the
world's most widely used email encryption, secured any Web traffic
that chooses to be secure, built a lot of the most popular network
encryption.  We beat back NSA's controlling hand, and encouraged a
global spread of encryption expertise.  We secured most of the
Internet's control traffic (using ssh - thanks Tatu) to make it harder
to break into the infrastructure.  We're the A-team.
But our cellphones are still trivial to track and intercept; the vast
majority of email, web, and IM traffic is totally unencrypted;
ordinary phone calls are totally wiretap prone; our own new
technologies like 802.11 have no decent encryption and no likelihood
of a real fix that works everywhere by default; we know the government
IS TODAY wiretapping tons of innocents in a feeding frenzy of
corruption; the US government has mandated Stasi-like wiretap
capabilities in every form of new communication (even where the law
gives them no power, they arrogate it and largely succeed); the
wiretappers have largely built an international consensus of cops to
track and wiretap anybody anywhere; practical anonymity has
significantly shrunken in the last decade; and even more traffic is
moving onto wireless where legal or illegal interception is
undetectable.  We still fight endless intra-community battles that
delay or derail deployment of existing encryption.  The most
widespread large-scale hard-to-crack systems are being deployed
AGAINST the public interest -- by the copyright mafia.
If *we*, the victors in the crypto wars, couldn't get decent
encryption deployed, even among ourselves, why would you expect that a
government bureacracy could do it among itself and its clients?

@_date: 2003-03-31 12:13:49
@_author: John Gilmore 
@_subject: GPS phones confiscated from reporters in Iraq  
It's nice to see that the US military realizes the terrible possibilities
from tracking the movements of ordinary people (who happen to be soldiers
or with soldiers).
When will they get on the bandwagon demanding that person-tracking
phones be banned -- rather than required -- by the FCC?

@_date: 2003-05-18 15:24:34
@_author: John Gilmore 
@_subject: The War on David Nelson  
If you want to break the system under its own weight, I also suggest
using lots of "Kiss My Face" honey scented hand cream.  Someone
recently told me setting off the nitrogylcerin censors (oops, I mean
sensors) at that spot where they wipe down your bag with little pads
and then put them through a quick chemical analysis.  When she set it
off, they went down a checklist of "Did you do X recently?" until they
got to "Did you put on hand cream recently?"  They let her through, of
course; you probably can't blow up an airplane with hand cream.  The
problem was with their sensorship, not with her.
If even 1% of travelers refused to show an ID, the system would also
break down under its own weight.  Do your part.  There is no law or
regulation that requires you to show ID.  You are all being sheep for
violating your own privacy, for no reason, when ordered by people who
have no authority.  Demand that they show you such a law, and refuse
to show ID until they identify one.  As you go up the chain of
command, you will find that you have the option to be searched rather
than show an ID.  In regimes where the laws are secret, the only way
to find out what the law is, is to not follow orders.
PS: I doubt that sending a complaint to TSA results in them adding you
to the no-fly list.  It's random and arbitrary, but not THAT random
and arbitrary.  If you want to see the complaints of some of the
ordinary people who TSA mousetraps every single time they enter an
airport, (not just the David Nelsons), check EPIC's FOIA results.  The
dozens of complaints forwarded via Congresspeople are well worth

@_date: 2003-05-28 12:28:36
@_author: John Gilmore 
@_subject: baseline privacy ... not  
Gee, that's funny.  You could replace "cable modem" with "cellphone"
and get the same answer.  I wonder whether NSA, or sloth fostered by
government-granted monopolies, are the cause in each case?

@_date: 2003-11-16 17:05:43
@_author: John Gilmore 
@_subject: Clipper for luggage  
That's a good idea for cheaply monkey-wrenching the whole illegal
search apparatus.  For about 5c you can get a nut and bolt.  Blow 50c
and get ten, and carry the spares on with you (the security screeners
will love you, but there is no prohibition on carrying nuts and bolts
on board.)  If you want to make their life more difficult you can put
loctite on the nut, so it will take a pair of wrenches to remove it.
TSSA will either have to spend a lot of time unscrewing the nut and
bolt, or will have to chop it off with bolt cutters.  If they cut it,
you know they opened your bag, and you can complain (or file a lawsuit
for illegal search without warrant).  If they don't, they have to
waste lots of their time on it, so they can't do it to very many
people.  It's a win-win situation, and it gets better the more people
do it.

@_date: 2003-11-22 11:43:00
@_author: John Gilmore 
@_subject: US antispam bill is death to anonymity 
This bill makes it a crime to use any false or misleading information
in a domain name or email account application, and then send an email.
That would make a large fraction of hotmail users instant criminals.
It also makes it a crime to remove or alter information in message
headers in ways that would make it harder for a police officer
to determine who had sent the email.  Anonymizers will be illegal
as soon as this bill becomes law.
There are MANY, MANY other things wrong with it -- including the fact
that most of its provisions apply to *ALL* commercial email, not just
BULK commercial email -- and that it takes zero account of the First
Amendment, attempting to list what topics someone can validly send
messages about, while outlawing all other topics that relate to
commercial transactions.
If it passes, I think I can make a criminal out of just about any
company.  Companies are liable for spam that helps them, even if they
had no part in sending it.
Read the bill yourself:
  And weep.  And then call your Congressman.
Everyone's common sense goes out the window when the topic is spam.
They're willing to sacrifice whatever principles they have.  And
you already know how few principles Congress had left.  Congress Poised for Vote on Anti-Spam Bill
Declan McCullagh
Published: November 21, 2003
Congress has reached an agreement on antispam legislation and could
vote on it as early as Friday afternoon, a move that would end more
than six years of failed attempts to enact a federal law restricting
unsolicited commercial e-mail.
Negotiators from the U.S. Senate and House of Representatives said
Friday that the legislation was a "historic" accomplishment with
support from key Democrats and Republicans in both chambers. "For the
first time during the Internet-era, American consumers will have the
ability to say no to spam," House Energy and Commerce Committee
Chairman Billy Tauzin, R-La., said in a statement. [...]
If the measure becomes law, certain forms of spam will be officially
legalized. The final bill says spammers may send as many "commercial
electronic mail messages" as they like--as long as the messages are
obviously advertisements with a valid U.S. postal address or P.O. box
and an unsubscribe link at the bottom. Junk e-mail essentially would
be treated like junk postal mail, with nonfraudulent e-mail legalized
until the recipient chooses to unsubscribe. [...]
One hotly contested dispute has been resolved: The bill would pre-empt
more restrictive state laws, including one that California enacted in
September. That law established an opt-in standard and was scheduled
to take effect on Jan. 1. With final passage of this bill, the core of
California's law would never take effect. [...]

@_date: 2003-11-24 02:05:44
@_author: John Gilmore 
@_subject: US antispam bill is death to anonymity  
So, I get non-commercial emails all the time, from topica mailing
lists and from people forwarding New York Times articles and such.
They come with embedded ads, that the sender cannot turn off.  These
ads are for the benefit of the helper site (e.g. topica).  Are these
messages commercial email, or not?  Is the sender penalized if their
email address or domain name was registered with privacy-protecting
circumlocutions (like addresses and cities of "123 Main St, Smallville")?
So, I get emails at various times from people I've never met, saying,
"I hear that you give money for drug policy reform, would you give
some to my nonprofit X for project Y?"  Is that a commercial email?
It proposes a financial transaction.  Are these people subject to the
anti-spam bill?  Do they have to do anything different in their lives
if it passes?  I think they will.
The larger point is that people in the United States don't generally
have to closely examine the content of their daily communications,
to censor out any possible mention of commerce, money, business, finance,
products, services, etc, to avoid legal liability.  We have a First
Amendment right to communicate without being penalized for our
communications.  We also have a right to speak without the government
putting words in our mouth (like requiring us to put in keywords,
or include a postal return address.  That last requirement was
deliberatly knocked down by the Supreme Court within the last few years,
building on existing precedents that protected anonymous speech.)
He's right.  Congress should be commended for only spending 55 pages
on the details of this important topic.
No, but outlawing anonymizers *is* one of them.  Anyone who wants to
get an anonymizer shut down can just send a commercial email through it.

@_date: 2003-10-02 15:34:35
@_author: John Gilmore 
@_subject: Monoculture / Guild 
The Guild, such as it is, is a meritocracy; many previously unknown
people have joined it since I started watching it in about 1990.
The way to tell who's in the Guild is that they can break your protocols
or algorithms, but you can't break theirs.
While there are only hundreds of serious members of the Guild -- a
comfortable number for holding conferences on college campuses -- I
think just about everyone in it would be happier if ten times as many
people were as involved as they are in cryptography and security.
Then ten times as many security systems that everybody (including the
Guild members) depends on would be designed properly.  They certainly
welcomed the Cypherpunks to learn (and to join if they were serious
I consider myself a Guild Groupie; I don't qualify but I think
they're great.  I follow in their footsteps and stand on their shoulders.
Clearly there are much larger numbers of Guild Groupies than Guild
members, or Bruce Schneier and Neal Stephenson wouldn't be able to
make a living selling books to 'em.  :-)
PS: Of course there's whole set of Mystic Secret Guilds of
Cryptography.  We think our openness will defeat their closedness,
like the free world eventually beat the Soviet Union.  There are some
good examples of that, such as our Guild's realization of the
usefulness of public-key crypto (we reinvented independently, but they
hadn't realized what a revolutionary concept they already had).  Then
again, they are better funded than we are, and have more exemptions
from legal constraints (e.g. it's hard for us to do production
cryptanalysis, which is really useful when learning to design good

@_date: 2003-10-10 14:27:57
@_author: John Gilmore 
@_subject: Ease of setting up IPSEC 
Rich $alz said:
Has anybody on this list tried setting up FreeS/WAN recently, by
following the Quick Start instructions?  It's pretty simple.
We've been making it simpler in just about every release.  Now you
basically have to download the RPM, install it, it spits out a public
key, and you install that public in your DNS in-addr records.  Then
the software automatically brings up VPN tunnels on demand, to any
other machine that's done the same thing.
A lot of the hair in other IPSEC implementations comes from having to
set up and transport keys, to sign things with X.509 certs and check
the signatures, to figure out what subnets are protected with which
keys, etc.  We push those jobs into the DNS, so it gets done once, and
then every node on the network can just look up the answer.
PS:  Yes, this approach has issues:  but ease of setup shouldn't be one
of them.

@_date: 2003-09-08 21:20:48
@_author: John Gilmore 
@_subject: Who needs secure wireless / tappable wireless infrastructure 
Or when innocent civilians need secure wireless infrastructures, to be
able to coordinate to avoid murderous US military forces.  See, for
  which I found via SF writer James P. Hogan's blog:
  Prudent citizens should now know that before stepping into the street
to hail a taxi, they should use a secure phone to determine whether
any American tanks are in the area.  But beware of American
direction-finding equipment -- make those calls short!

@_date: 2003-09-08 21:45:43
@_author: John Gilmore 
@_subject: Code breakers crack GSM cellphone encryption  
OK, then, where is it?  I looked on:
   under Crypto 2003 -- no papers there.  The title of the
      paper, presented in Session 15, is:
        Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication
   under Conference Proceedings -- Crypto 2003 not there.
   under Cryptology ePrint archive -- no Biham or GSM papers there.
   -- latest paper is from 2000.
   -- access denied
   -- a news item about the GSM crack, but no paper.
I'm even a dues-paying IACR member, but I don't seem to have online
access to the papers from recent conferences.  I'm sure a copy will
show up in the mail a few months from now.  Let me guess -- the devils
at Springer-Verlag have stolen IACR's copyrights and the researchers
were dumb enough to hand their copyright to IACR...

@_date: 2003-09-24 02:12:16
@_author: John Gilmore 
@_subject: Please submit public comments on CAPPS 2 / JetBlue 
[Moderator's note: This isn't about crypto, but I have a tradition of
forwarding John's appeals on such topics. --Perry]
There's something you can do *right now* that will stop pending abuses
of air travelers' private data.  And your chance to do it will expire
this coming Tuesday (Sept 30).
The government has published a Privacy Act notice about its CAPPS-2
program, which would require all airlines to provide JetBlue-style
information (full PNRs) to the government -- all the time, before
every flight.  It's like another JetBlue database dump that happens
again and again, day after day, month after month, airline after
airline.  Affecting everyone who ever flies.
CAPPS-2 is the real thing, for which the Torch Concepts/JetBlue
contract was one of the test runs.  The government is taking public
comments on the CAPPS-2 proposal, by email or postal mail, between now
and September 30th.  After that, if you send them your opinion,
they'll ignore it (even more than usual).
Address your email like this:
  To:  privacy at dhs.gov
  Subject:  DHS/TSA-2003-1
Then tell them whatever you want.  If the government is honest, then
they would stop CAPPS-2 if they got individual notes from 10,000
people saying "KILL CAPPS-2!  Don't sacrifice the privacy of 600
million travelers each year in a foolish attempt to catch less than a
dozen actual terrorists each year."  If they are dishonest and don't
care what the public thinks, then they would at least be on notice
that 10,000 honest and involved people are watching them.
You can write them pages and pages of details on how terrible CAPPS-2
is, and how corrupt they are to propose it, instead of a short note.
But I suggest reading the details of what they propose, if you're
going to that much trouble.  You can find their proposal (in
proprietary PDF format) here:
  They claim that they're putting up the comments for public viewing on
 but if they have done so, I can't find them there.
You can find an earlier round of 282 overwhelmingly negative public
comments on CAPPS-2 here, if you click "Simple Search" and enter
"1437": As a brief overview, CAPPS-2 would require airlines to collect
peoples' full legal name, residence address, home phone number, and
date of birth (none of which is currently used by airlines today)
before they can even make a flight reservation.  They would be
required to hand this information, and everything else in the PNR
(flight reservation), to the government, LONG BEFORE the flight takes
off.  Then the government (or its "contractors") would do the same
kind of data matching that Torch Concepts did, hooking up your flight
reservations to credit databases and many other government and private
databases.  The difference is that if YOUR data was one of those
"anomalous records" (that didn't fit one of the standard patterns of
your airline's customers), you would be singled out to be specially
searched, and/or kept off the airplane.
Torch Concepts' report blew the whistle on this secret program.  The
report is at   On page 22,
Torch found two major groupings of JetBlue customers:
  (1) Young Middle Income Home Owners with Short Length-of-Residence
  (2) Older Upper Income Home Owners with Longer Length-of-Residence
Everybody else they categorized into "anomalous records".  If you're
an oldster who moved to Florida recently -- or a renter -- or a lower
income person of any type -- you're anomalous.  You're going to get
that special government search whenever you fly on JetBlue, if TSA
succeeds in imposing CAPPS 2.
(Oh, perhaps their final system will be more subtle than this clumsy
contractor was, but the basic problem is the same: the government will
forcibly identify each traveler, evalulate their lifestyle from
database records, and then make snap decisions about what civil rights
that person will have while traveling.  They propose to permanently
withhold the right to anonymity; grant or withhold the right to
travel; and grant or withhold the right not to be searched without
probable cause.  I thought rights were something that you had *all the
time*, not just if the government likes your lifestyle.)
CAPPS-2 also proposes that this "airport checkpoint" also be used to
try to catch various kinds of criminals, rather than solely to make
flying supposedly safer.  It would also be used to catch foreign
visitors whose visa has expired or whose paperwork is snarled.  And
once the public is used to it, of course I expect it would be expanded
to stop people for everything up to and including parking tickets.
The judges say that searching the general public without cause, in
order to catch criminals, is unconstitutional.  But don't depend on a
judge to guard your rights.  Complain to the government yourself,
right now!
Here's the best part.  Besides the "blacklists" that today's CAPPS-1
system uses, the CAPPS-2 system will have "whitelists".  Anyone with a
government security clearance, or a "position of trust and
confidence", will never get singled out, screened, or delayed.  They
will be able to show up at the airport half an hour before their
flight, like everyone used to be able to do, and just walk on board.
There'll be one rule for "Party members" and another rule for the
"proletariat".  CAPPS-2 assumes you are guilty until proven innocent

@_date: 2003-09-24 03:41:44
@_author: John Gilmore 
@_subject: DirecTV Hacker Is First Person Convicted Under DMCA 
DirecTV Hacker Is First Person Convicted Under Digital Millennium Copyright Act
Man Faces 30 Years In Prison, Millions In Fines For Selling Illegal Hardware
UPDATED: 1:51 p.m. PDT September 22, 2003
Spertus said Whitehead -- also known as Jungle Mike -- paid a co-conspirator $250 a month to continually update software to circumvent the latest DirecTV security measures. Whitehead then used the software to create and sell modified DirecTV access cards, the prosecutor said.
The conduct violated the DMCA, which bars trafficking in technology primarily designed to get around security measures to access a copyrighted Copyright 2003 by NBC4.tv. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.  No fair uses of this
material may be made.  (I added that last sentence myself.)

@_date: 2003-09-24 03:43:29
@_author: John Gilmore 
@_subject: TSA shares a post office (box?) with NSA? 
The CAPPS-2 Privacy Act notice says:
System manager(s) and address:      Director, CAPPS II, TSA, P.O. Box 597, Annapolis Junction, MD 20701-0597. Annapolis Junction PO boxes have a long history of being NSA addresses.
Is this one?  That would be very interesting.

@_date: 2004-01-01 19:09:22
@_author: John Gilmore 
@_subject: What's wrong with Victor's approach to spam 
Somehow, my personal emails are always part of that "false positive
rate" among self-satisfied anti-spammers like Victor.
Luckily I resisted the barrage of unsolicited phone calls from Morgan
Stanley, seeking to get my investment business.  So I don't have to
worry about Victor censoring the email from me to my broker.  But I'm
curious:  do the 'false positives' that result in a loss to a Morgan
Stanley customer get made whole out of Victor's paycheck?
Return-Path: MAILER-DAEMON
Delivery-Date: Wed Dec 31 14:52:07 2003
Return-Path: Received: from localhost (localhost)
This is a MIME-encapsulated message
The original message was received at Wed, 31 Dec 2003 14:51:55 -0800
from localhost.localdomain [127.0.0.1]
   ----- The following addresses had permanent fatal errors -----
    (reason: 554 Service unavailable; [209.237.225.253] blocked using dnsbl.ms.com, reason:    ----- Transcript of session follows -----
... while talking to mx2.morganstanley.com.:
<<< 554 Service unavailable; [209.237.225.253] blocked using dnsbl.ms.com, reason: 554 5.0.0 Service unavailable
<<< 554 Error: no valid recipients
Reporting-MTA: dns; new.toad.com
Received-From-MTA: DNS; localhost.localdomain
Arrival-Date: Wed, 31 Dec 2003 14:51:55 -0800
Final-Recipient: RFC822; Victor.Duchovni at morganstanley.com
Action: failed
Status: 5.0.0
Remote-MTA: DNS; mx2.morganstanley.com
Diagnostic-Code: SMTP; 554 Service unavailable; [209.237.225.253] blocked using dnsbl.ms.com, reason: Last-Attempt-Date: Wed, 31 Dec 2003 14:52:01 -0800
Return-Path: Received: from toad.com (localhost.localdomain [127.0.0.1])
Just checking whether your super effective "spam filters" allow me to
respond to the message that you sent to cryptography at metzdowd.com.

@_date: 2004-01-03 00:22:26
@_author: John Gilmore 
@_subject: digsig - when a MAC or MD is good enough?  
The flaw in this ointment is the "intent" requirement.  Corporate
lawyers regularly advise their client companies to shred all
non-essential records older than, e.g. two years.  The big reason to
do so is to impair their availability in case of future litigation.
But if that intent becomes illegal, then the advice will be to shred
them "to reduce clutter" or "to save storage space".
What's the point of keeping a message digest of a logged item?  If the
log can be altered, then the message digest can be altered to match.
(Imagine a sendmail log file, where each line is the same as now, but
ends with the MD of the line in some gibberish characters...)

@_date: 2004-01-05 15:10:47
@_author: John Gilmore 
@_subject: Walton's Mountain notaries (identity requirements) 
It's a lot more complicated than that, Carl.  Society can't demand impossible
conditions from its citizens, as a precondition to existence.  (This is
true even if the condition is possible for 99% of the citizens; the other
1% have rights too.)
(I'm switching topics here from ID-for-notarization to general identification.)
The Supreme Court is currently deciding a case called Hiibel
v. Nevada, in which Nevada passed a law requiring people to show ID to
a cop, and Mr. Hiibel did not do so when the cop demanded it.  He was
arrested and convicted for this.  All the courts in Nevada upheld this
conviction, but his public defenders appealed to the Supreme Court,
and it took his case.  Most of the briefs have been filed, and are on
the web; oral argument will probably occur in March or April.  See:
  If ID is required of ordinary people going about their business on the
street, then the process by which they get an ID becomes an issue of
constitutional concern.  The government can't make up arbitrary rules,
like "people who can't find anyone who knew them when they were born
can be locked up at the whim of any cop".  (This would discriminate
against the most elderly, among many other problems!)
Anything that the government requires people to do in order to not be
criminals, has to be doable by every citizen, or the requirement is
unconstitutional.  They can't pass a law saying "We can throw you in
jail unless you produce a pass signed by George Washington", if George
is dead and not signing any more passes.  They similarly can't pass a
law saying "We can throw you in jail unless you produce a pass signed
by the Department of Motor Vehicles", unless the DMV will give a pass
to anybody.  Anybody includes homeless people, those who decline to
provide personal information to government officials, those whose birth
was not recorded anywhere (or whose records burned up years ago), etc.

@_date: 2004-07-09 13:27:11
@_author: John Gilmore 
@_subject: EZ Pass and the fast lane ....  
Am I missing something?
It seems to me that EZ Pass spoofing should become as popular as
cellphone cloning, until they change the protocol.  You pick up a
tracking number by listening to other peoples' transmissions, then
impersonate them once so that their account gets charged for your toll
(or so that it looks like their car is traveling down a monitored
stretch of road).  It should be easy to automate picking up dozens or
hundreds of tracking numbers while just driving around; and this can
foil both track-the-whole-populace surveillance, AND toll collection.
Miscreants would appear to be other cars; tracking them would not
be feasible.
The rewriteable parts of the chip (for recording the entry gate to
charge variable tolls) would also allow one miscreant to reprogram the
transponders on hundreds or thousands of cars, mischarging them when
they exit.  Of course, the miscreant's misprogrammed transponder would
just look like one of the innocents who got munged.
[I believe, by the way, that the EZ Pass system works just like many
other chip-sized RFID systems.  It seems like a good student project
to build some totally reprogrammable RFID chips that will respond to a
"ping" with any info statically or dynamically programmed into them by
the owner.  That would allow these hypotheses to be experimentally tested.]

@_date: 2004-07-09 14:31:02
@_author: John Gilmore 
@_subject: EZ Pass and the fast lane ....  
[By the way, die at dieconsulting is being left out of this conversation,
 by his own configuration, because his site censors all emails from me.  --gnu]
If they could read the license plates reliably, then they wouldn't
need the EZ Pass at all.  They can't.  It takes human effort, which is
in short supply.
Actually, cellphones DO have other identifying information in them,
akin to license plates.  And their "toll gates" are cell sites.
It's not clear what your remark about phones having no cars has to do
with the issue of whether EZ Pass is likely to be widely spoofed.
(1) Same one they have for releasing viruses or breaking into
thousands of networked systems.  Because they can; it's a fun way to
learn.  Like John Draper calling the adjacent phone booth via
operators in seven countries.  (2) The miscreant gets a cheap toll
along with hundreds of other people who get altered tolls.
[Cory Doctorow's latest novel (Eastern Standard Tribe, available free
online, or in bookstores) hypothesizes MP3-trading networks among
moving cars, swapping automatically with whoever they pass near enough
for a short range WiFi connection.  Sounds plausible to me; there are
already MP3 players with built-in short range FM transmitters, so
nearby cars can hear your current selection.  Extending that to faster
WiFi transfers based on listening preferences would just require "a
simple matter of software".  An iPod built by a non-DRM company might
well offer such a firmware option -- at least in countries where
networking is not a crime.  Much of the music I have is freely

@_date: 2004-06-08 23:25:21
@_author: John Gilmore 
@_subject: Security clampdown on the home PC banknote forgers  
My proposal, when this stuff surfaced a few months ago, was that we build
two filter modules for the GIMP:
  *  One *detects* the banknote pattern, putting up a little
     constellation symbol or currency symbol in the corner of the GUI,
     or optionally some sort of unobtrusive pop-up.  And does nothing else.
  *  One *inserts* the banknote pattern into existing images.  This makes
     them unprocessable by PhotoShop and other government-monkeywrenched
     proprietary software or printers.  Isn't mandatory DRM wonderful?
In my malicious moments, I think GIMP should ship with both of these
filters turned on by default.
A third filter would remove the banknote pattern from images -- or would
cripple it sufficiently well that it is not detectable by other software.
Every country is going to have to work out for itself whether it
thinks that free countries can ban expressive works such as software.
Though we set early precedents in the US (Bernstein & Junger), this is
still not considered a settled question (or DeCSS and DVD X-Copy would
not have lost in court).  The less honest judges are still willing to
twist fundamental principles, in order to get the result that
Hollywood wants.  Now in the EU we'll see the first round of TRUE
mischief that the wrong answer can cause.

@_date: 2004-06-08 22:54:04
@_author: John Gilmore 
@_subject: Passwords can sit on disk for years  
Intel, Microsoft and Hollywood are solving this for us.  Their new
hardware can't be virtualized, so it can't leak the
monopolists/oligopolists' keys.  In their scheme, of course, OUR keys
don't get the same level of protection as monopolist keys.

@_date: 2004-06-17 10:31:06
@_author: John Gilmore 
@_subject: A National ID: AAMVA's Unique ID 
Our favorite civil servants, the Departments of Motor Vehicles, are about
to do exactly this to us.
They call it "Unique ID" and their credo is: "One person, one license,
one record".  They swear that it isn't national ID, because national
ID is disfavored by the public.  But it's the same thing in
distributed-computing clothes.
The reason they say it isn't a national ID is because it's 50 state
IDs (plus US territories and Canadian provinces and Mexican states) --
but the new part is that they will all be linked by a continent-wide
network.  Any official who looks up your record from anywhere on the
continent will be able to pull up that record.  Anyplace you apply for
a state license or ID card, they will search the network, find your
old record (if you have one) and "transfer" it to that state.  So
there's no way to escape your past record, and no way to get two cards
(in the absence of successful fraud, either by citizens or DMV
This sure smells to me like national ID.
This, like the MATRIX program, is the brainchild of the federal
Department of inJustice.  But those wolves are in the sheepskins of
state DMV administrators, who are doing the grassroots politics and
the actual administration.  It is all coordinated in periodic meetings
by "AAMVA", the "American Association of Motor Vehicle Administrators"
(  Draft bills to join the "Unique ID Compact", the
legally binding agreement among the states to do this, are already
being circulated in the state legislatures by the heads of state DMVs.
The idea is to sneak them past the public, and past the state
legislators, before there's any serious public debate on the topic.
They have lots of documents about exactly what they're up to.  See
  Unfortunately for us, the real
documents are only available to AAMVA members; the affected public is
not invited.
Robyn Wagner and I have tried to join AAMVA numerous times, as
"freetotravel.org".  We think that we have something to say about the
imposition of Unique ID on an unsuspecting public.  They have rejected
our application every time -- does this remind you of the Hollywood
copy-prevention "standards committees"?  Here is their recent
rejection letter:
  Thank you for submitting an application for associate membership in AAMVA.
  Unfortunately, the application was denied again. The Board is not clear as
  to how FreeToTravel will further enhance AAMVA's mission and service to our
  membership. We will be crediting your American Express for the full amount
  charged.
  Please feel free to contact Linda Lewis at (703) 522-4200 if you would like
  to discuss this further.
  Dianne   Dianne E. Graham   Director, Member and Conference Services   AAMVA   4301 Wilson Boulevard, Suite 400   Arlington, VA 22203   T: (703) 522-4200 | F: (703) 908-5868      At the same time, they let in a bunch of vendors of "high security" ID
cards as associate members.
AAMVA, the 'guardians' of our right to travel and of our identity
records, doesn't see how listening to citizens concerned with the
erosion of exactly those rights and records would enhance their
"mission and service".  Their mission appears to be to ram their
secret policy down our throats.  Their service is to take our tax
money, use it to label all of us like cattle with ear-tags, and deny
us our constitutional right to travel unless we submit to being
We protest.  Do you?

@_date: 2004-11-21 16:20:30
@_author: John Gilmore 
@_subject: Gov't Orders Air Passenger Data for Test  
Effective at what?  Preventing people from traveling?
The whole exercise ignores the question of whether the Executive Branch
has the power to make a list of citizens (or lawfully admitted non-citizens)
and refuse those people their constitutional right to travel in the United
Doesn't matter whether there's 1, 19, 20,000, or 100,000 people on the
list.  The problem is the same: No court has judged these people.
They have not been convicted of any crime.  They have not been
arrested.  There is no warrant out for them.  They all have civil
rights.  When they walk into an airport, there is nothing in how they
look that gives reason to suspect them.  They have every right to
travel throughout this country.  They have every right to refuse a
government demand that they identify themselves.
So why are armed goons keeping them off airplanes, trains, buses, and
ships?  Because the US constitution is like the USSR constitution --
nicely written, but unenforced?  Because the public is too afraid of
the government, or the terrorists, or Emmanuel Goldstein, or the
boogie-man, to assert the rights their ancestors died to protect?
PS: Oral argument in Gilmore v. Ashcroft will be coming up in the
Ninth Circuit this winter.

@_date: 2004-10-05 19:07:59
@_author: John Gilmore 
@_subject: Interesting report on Dutch non-use of traffic data 
From EDRI-gram via Wendy Seltzer:
4. Dutch police report: traffic data seldom essential
Telephone traffic data are only necessary to solve crimes in a minority of
police investigations. Most cases can be solved without access to traffic
data, with the exception of large fraud investigations.
These are the conclusions of a Dutch police report produced at the request
of the Dutch ministry of Justice. The report was recently obtained by the
Dutch civil liberties organisation Bits of Freedom through a public access
The report undermines the Dutch government's support to the EU draft
framework decision on data retention. The report makes no case for the
proposed data retention as Dutch police already uses traffic data in 90%
of all investigations. The police can already obtain, with a warrant, the
traffic data that telecommunication companies store for their own billing-
and business purposes. The report also shows that the use of traffic data
is a standard tool in police investigations and it not limited to cases of
organised crime or terrorism.
The report is the result of an evaluation of past investigations by the
Dutch police of Rotterdam. Two-thirds of all investigations could have
been solved if no traffic data would have been available at all. The three
main purposes of traffic data in police investigations are: network
analysis (searching for associations of a person to other individuals),
tactical support for surveillance and checking of alibis (through GSM
location data).
Police investigators can compensate a possible lack of traffic data by
other investigative methods such as wiretapping, surveillance, a
preservation order for traffic data and a longer investigative period. The
report states that police officers seldom ask for traffic data older than
six months.
The report was never sent to the Dutch parliament although members of
parliament previously asked for research results about the effectiveness
of mandatory data retention. After Bits of Freedom published the report
new questions have been raised in the Dutch parliament about the reason
for withholding the report.
The use of (historic) traffic data in investigations (April 2003, in Dutch)
(Contribution by Maurice Wessling, EDRI-member Bits of Freedom)

@_date: 2004-10-28 10:16:39
@_author: John Gilmore 
@_subject: MCI set to offer secure two-way messaging with strong encryption  
Note that they don't say it's "end to end" encryption:
And presumably wiretappable there.

@_date: 2004-09-30 15:25:57
@_author: John Gilmore 
@_subject: Linux-based wireless mesh suite adds crypto engine support  
I don't -- do you?
Crypto hardware that does algorithms can be tested by periodically
comparing its results to a software implementation.  Production
applications should probably be doing this -- maybe 1% of the time.
Crypto hardware that generates "random" numbers can't be tested in
production in many useful ways.  My suggestion would be to XOR a
hardware-generated and a software-generated random number stream.  If
one fails, whether by accident, malice, or design, the other will
still randomize the resulting stream.  Belt AND suspenders will keep
your source of randomness from being your weakest link.

@_date: 2005-04-03 23:45:20
@_author: John Gilmore 
@_subject: DRM comes to digital cameras: Lexar LockTight 
Lexar Media has come up with a Compact Flash card that won't actually
work until you do a nonstandard, proprietary handshake with it.  They
worked with a couple of camera makers (and built their own CF reader
and Windows software) to implement it.  Amazingly, it doesn't actually
store the photos encrypted on the flash; it just disables access to
the memory until you do something secret (probably answer a
challenge/response with something that shows you have the same secret
key that those cameras do).  I don't know of anyone competent who's
taken one apart and figured out what the actual security properties
    They also have "Active Memory" which appears to be another idea for
what can be done by making a separate memory on the CF card that can't
be accessed by the standard protocols.  Idle hands are the devil's work.
They haven't figured out anything useful for it to do: at the moment
their custom software copies copyright notices off the secret memory
onto the photos, after you transfer them to a PC.  Of course, the software could've done that WITHOUT the secret memory, just keeping the
copyright info in a file in the standard flash file system.
What Lexar gets out of it is to charge twice as much for these CF cards,
raising them out of the commodity market.  (Assuming anybody buys.)
They're pitching it to cops, who are spending somebody else's money.

@_date: 2005-04-30 02:29:32
@_author: John Gilmore 
@_subject: Export controls kill Virgin SpaceShipTwo 
First crypto, now space travel.  The lunatics in Washington are
working hard to drive another industry that's critical to US interests
Did they think that after collecting $20M in prepayments from
passengers, Sir Richard Branson would give up, on orders from DC?  No,
he'll clone Rutan's work somewhere else, as best he can, and build a
space industry where it's welcome.  Either that, or Rutan will take
his head and export it to where he can run a business without
  Red Tape For SpaceShipTwo
  by Irene Mona Klotz
  Cape Canaveral (UPI) Apr 26, 2005
  ...
  The problem is U.S. export controls issues ...
  "At this point, due to uncertainty about possible licensing
  requirements, we are not able to even view Scaled Composites' designs
  for the commercial space vehicle," Whitehorn said. "After U.S.
  government technology-transfer issues are clarified and addressed if
  deemed necessary, we hope to place a firm order for the spacecraft."
  ...
  Despite a price tag of $200,000, about 100 people have signed contracts
  for rides on Virgin Galactic's spaceliner and agreed to pay the money
  upfront, while another 29,000 or so aspiring astronauts have agreed to
  put down deposits of $20,000 each.

@_date: 2005-12-12 14:36:48
@_author: John Gilmore 
@_subject: "Live Tracking of Mobile Phones Prompts Court Fights on Privacy" 
[See the details at EFF:
   including the three court orders, and EFF's argument to the first court.
 The real story is that for years prosecutors have been asking
 magistrates to issue court orders to track cellphones in real time
 WITHOUT WARRANTS.  They're tracking people for whom they can't get
 warrants because they have no probable cause to believe there's any
 crime.  They're fishing.  The public never knew, because it all
 happens under seal.  One judge who had previously issued such orders
 got an attack of conscience, and surprisingly PUBLISHED a decision
 against such a secret DoJ request.  EFF noticed and offered legal
 analysis, and that judge and two others started publicly refusing
 such requests.  DoJ won't appeal, because without an appeals court
 precedent against them, they can keep secretly pulling the wool over
 the eyes of other magistrates, and keep tapping the locations of
 ordinary people in realtime without warrants.  --gnu]
No cookies or login required:
Published Saturday, December 10, 2005
Live Tracking of Mobile Phones Prompts Court Fights on Privacy
By MATT RICHTEL
New York Times
Most Americans carry cellphones, but many may not know that government
agencies can track their movements through the signals emanating from
the handset.
In recent years, law enforcement officials have turned to cellular
technology as a tool for easily and secretly monitoring the movements
of suspects as they occur. But this kind of surveillance - which
investigators have been able to conduct with easily obtained court
orders - has now come under tougher legal scrutiny.
In the last four months, three federal judges have denied prosecutors
the right to get cellphone tracking information from wireless
companies without first showing "probable cause" to believe that a
crime has been or is being committed. That is the same standard
applied to requests for search warrants.
The rulings, issued by magistrate judges in New York, Texas and
Maryland, underscore the growing debate over privacy rights and
government surveillance in the digital age.
With mobile phones becoming as prevalent as conventional phones (there
are 195 million cellular subscribers in this country), wireless
companies are starting to exploit the phones' tracking abilities. For
example, companies are marketing services that turn phones into even
more precise global positioning devices for driving or allowing
parents to track the whereabouts of their children through the
Not surprisingly, law enforcement agencies want to exploit this
technology, too - which means more courts are bound to wrestle with
what legal standard applies when government agents ask to conduct such
Cellular operators like Verizon Wireless and Cingular Wireless know,
within about 300 yards, the location of their subscribers whenever a
phone is turned on. Even if the phone is not in use it is
communicating with cellphone tower sites, and the wireless provider
keeps track of the phone's position as it travels. The operators have
said that they turn over location information when presented with a
court order to do so.
The recent rulings by the magistrates, who are appointed by a majority
of the federal district judges in a given court, do not bind other
courts. But they could significantly curtail access to cell location
data if other jurisdictions adopt the same reasoning. (The
government's requests in the three cases, with their details, were
sealed because they involve investigations still under way.)
"It can have a major negative impact," said Clifford S. Fishman, a
former prosecutor in the Manhattan district attorney's office and a
professor at the Catholic University of America's law school in
Washington. "If I'm on an investigation and I need to know where
somebody is located who might be committing a crime, or, worse, might
have a hostage, real-time knowledge of where this person is could be a
matter of life or death."
Prosecutors argue that having such information is crucial to finding
suspects, corroborating their whereabouts with witness accounts, or
helping build a case for a wiretap on the phone - especially now that
technology gives criminals greater tools for evading law enforcement.
The government has routinely used records of cellphone calls and
caller locations to show where a suspect was at a particular time,
with access to those records obtainable under a lower legal
standard. (Wireless operators keep cellphone location records for
varying lengths of time, from several months to years.)
But it is unclear how often prosecutors have asked courts for the
right to obtain cell-tracking data as a suspect is moving. And the
government is not required to report publicly when it makes such
Legal experts say that such live tracking has tended to happen in
drug-trafficking cases. In a 2003 Ohio case, for example, federal drug
agents used cell tracking data to arrest and convict two men on drug
Mr. Fishman said he believed that the number of requests had become
more prevalent in the last two years - and the requests have often
been granted with a stroke of a magistrate's pen.
Prosecutors, while acknowledging that they have to get a court order
before obtaining real-time cell-site data, argue that the relevant
standard is found in a 1994 amendment to the 1986 Stored
Communications Act, a law that governs some aspects of cellphone
The standard calls for the government to show "specific and
articulable facts" that demonstrate that the records sought are
"relevant and material to an ongoing investigation" - a standard lower
than the probable-cause hurdle.
The magistrate judges, however, ruled that surveillance by cellphone -
because it acts like an electronic tracking device that can follow
people into homes and other personal spaces - must meet the same high
legal standard required to obtain a search warrant to enter private
"Permitting surreptitious conversion of a cellphone into a tracking
device without probable cause raises serious Fourth Amendment
concerns, especially when the phone is monitored in the home or other
places where privacy is reasonably expected," wrote Stephen W. Smith,
a magistrate in Federal District Court in the Southern District of
Texas, in his ruling.
"The distinction between cell site data and information gathered by a
tracking device has practically vanished," wrote Judge Smith. He added
that when a phone is monitored, the process is usually "unknown to the
phone users, who may not even be on the phone."
Prosecutors in the recent cases also unsuccessfully argued that the
expanded police powers under the USA Patriot Act could be read as
allowing cellphone tracking under a standard lower than probable
As Judge Smith noted in his 31-page opinion, the debate goes beyond a
question of legal standard. In fact, the nature of digital
communications makes it difficult to distinguish between content that
is clearly private and information that is public. When information is
communicated on paper, for instance, it is relatively clear that
information written on an envelope deserves a different kind of
protection than the contents of the letter inside.
But in a digital era, the stream of data that carries a telephone
conversation or an e-mail message contains a great deal of information
- like when and where the communications originated.
In the digital era, what's on the envelope and what's inside of it,
"have absolutely blurred," said Marc Rotenberg, executive director of
the Electronic Privacy Information Center, a privacy advocacy group.
And that makes it harder for courts to determine whether a certain
digital surveillance method invokes Fourth Amendment protections
against unreasonable searches.
In the cellular-tracking cases, some legal experts say that the Store
Communications Act refers only to records of where a person has been,
i.e. historical location data, but does not address live tracking.
Kevin Bankston, a lawyer for the Electronic Frontier Foundation, a
privacy advocacy group that has filed briefs in the case in the
Eastern District of New York, said the law did not speak to that
use. James Orenstein, the magistrate in the New York case, reached the
same conclusion, as did Judge Smith in Houston and James Bredar, a
magistrate judge in the Federal District Court in Maryland.
Orin S. Kerr, a professor at the George Washington School of Law and a
former trial attorney in the Justice Department specializing in
computer law, said the major problem for prosecutors was Congress did
not appear to have directly addressed the question of what standard
prosecutors must meet to obtain cell-site information as it occurs.
"There's no easy answer," Mr. Kerr said. "The law is pretty uncertain
Absent a Congressional directive, he said, it is reasonable for
magistrates to require prosecutors to meet the probable-cause
Mr. Fishman of Catholic University said that such a requirement could
hamper law enforcement's ability to act quickly because of the
paperwork required to show probable cause. But Mr. Fishman said he
also believed that the current law was unclear on the issue.
Judge Smith "has written a very, very persuasive opinion," Mr. Fishman
said. "The government's argument has been based on some tenuous
premises." He added that he sympathized with prosecutors' fears.
"Something that they've been able to use quite successfully and
usefully is being taken away from them or made harder to get,"
Mr. Fishman said. "I'd be very, very frustrated."

@_date: 2005-12-19 03:44:51
@_author: John Gilmore 
@_subject: NSA director on NSA domestic wiretaps (to Cong in Oct 2002) 
Paragraph 40, below, is about as bald a statement as an NSA director
could make, saying he needs help to decide what he should be allowed
to wiretap about US persons.  We, the privacy community, did not
respond.  We were a bit surprised, but that was about the extent of
the support we offered.
Of course, we were living in a time where being anti-paranoia or
anti-war or anti-president was considered treasonous by the president,
and by most of the people who elected him, and many who worked for
him.  And we were living in the lost time when we expected the
government to follow clearly written laws, until such time as they
were rewritten.  And nobody had ever gotten NSA to stop doing ANYTHING
corrupt, without either suing them, beating them in the legislature,
or shining some bright sunlight on one of their secrets -- in some
cases it took all three.  The door of the NSA Director's office has
never been open for privacy activists to come in and review their
secret programs for sanity and constitutionality, though it should be.
His challenge to the NSA work force -- "to keep America free by making
Americans feel safe again" -- is as bogus as TSA's "We're upholding
the right to travel by making travel feel safe, even while we keep
innocent YOU off the plane".  It begs the question -- who do we need
to feel safe FROM?  Governments are historically thousands of times as
likely to injure you than 'terrorists'.  Do you feel safe from Bush
and NSA and TSA today?  Are you really sure your government isn't
tapping and tracing you, building databases about who you call and who
you travel with, with or without a warrant from some rubber stamp
Indeed, what good would it have done if the whole privacy and crypto
community had risen up to say, "You should follow the law!"?  Bush was
intent on breaking it in secret ANYWAY, and rather than exposing his
treason, NSA followed his orders.  Mr. Hayden did not pose the
question as, "We are now wiretapping the foreign communications of US
persons without warrants, in violation of the FISA; do you think this
is OK?", though he was doing so at the time he made this speech.  But
that's the question that he and his successor will have to face civil
and criminal charges over.
"Statement for the record by Lieutenant General Michael V. Hayden, USAF,
Director, National Security Agency... 17 October 2002"
2.  We know our responsibilities for American freedom and security at
    NSA. Our workforce takes the events of September 11, 2001 very
    personally. By the very nature of their work, our people deeply
    internalize their mission. This is personal.
25. The final issue - what have we done in response - will allow me to
    give some specifics although I may be somewhat limited by the
    demands of classification. I will use some of the terms that
    Congress has used with us over the past year.
26. It was heartening, for example, to hear Congress echo the phrase
    of our SIGINT Director, Maureen Baginski, in the belief that we
    need to be "hunters rather than gatherers." She believed and
    implemented this strategy well before September 11th, and then she
    applied it with a vengeance to al-Qa'ida after the attacks.
36. There is a certain irony here. This is one of the few times in the
    history of my Agency that the Director has testified in open
    session about operational matters. The first was in the mid 1970s
    when one of my predecessors sat here nearly mute while being
    grilled by members of Congress for intruding upon the privacy
    rights of the American people. Largely as a result of those
    hearings, NSA is governed today by various executive orders and
    laws and these legal restrictions are drilled into NSA employees
    and enforced through oversight by all three branches of
    government.
37. The second open session was a little over two years ago and I was
    the Director at that time. During that session the House
    intelligence committee asked me a series of questions with a
    single unifying theme:
    How could I assure them that I was safeguarding the privacy rights
    of those protected by the U.S. constitution and U.S. law? During
    that session I even said - without exaggeration on my part or
    complaint on yours - that if Usama bin Laden crossed the bridge
    from Niagara Falls, Ontario to Niagara Falls, New York, U.S. law
    would give him certain protections that I would have to
    accommodate in the conduct of my mission. And now the third open
    session for the Director of NSA: I am here explaining what my
    Agency did or did not know with regard to 19 hijackers who were in
    this country legally.
38. When I spoke with our workforce shortly after the September 11th
    attacks, I told them that free people always had to decide where
    to draw the line between their liberty and their security, and I
    noted that the attacks would almost certainly push us as a nation
    more toward security. I then gave the NSA workforce a challenge:
    We were going to keep America free by making Americans feel safe
    again.
39. Let me close by telling you what I hope to get out of the national
    dialogue that these committees are fostering. I am not really
    helped by being reminded that I need more Arabic linguists or by
    someone second-guessing an obscure intercept sitting in our files
    that may make more sense today than it did two years ago. What I
    really need you to do is to talk to your constituents and find out
    where the American people want that line between security and
    liberty to be.
40. In the context of NSA's mission, where do we draw the line between
    the government's need for CT information about people in the
    United States and the privacy interests of people located in the
    United States?
    Practically speaking, this line-drawing affects the focus of NSA's
    activities (foreign versus domestic), the standard under which
    surveillances are conducted (probable cause versus reasonable
    suspicion, for example), the type of data NSA is permitted to
    collect and how, and the rules under which NSA retains and
    disseminates information about U.S. persons.
41. These are serious issues that the country addressed, and resolved
    to its satisfaction, once before in the mid-1970's. In light of
    the events of September 11th, it is appropriate that we, as a
    country, readdress them. We need to get it right. We have to find
    the right balance between protecting our security and protecting
    our liberty. If we fail in this effort by drawing the line in the
    wrong place, that is, overly favoring liberty or security, then
    the terrorists win and liberty loses in either case.
42. Thank you. I look forward to the committees' questions.

@_date: 2005-01-13 20:36:44
@_author: John Gilmore 
@_subject: Network World: NIST dubious about 802.11 TKIP; wants AES 
NIST mulls new WLAN security guidelines
By Ellen Messmer
The National Institute of Standards and Technology, the federal agency responsible for defining security standards and practices for the government, plans to issue new guidelines pertaining to wireless LANs in the near future.
The decisions NIST reaches, possibly as early as this month, will broadly affect federal agency purchases of WLAN equipment, because federal agencies are required to follow NIST recommendations. According to William Burr, manager of NIST's security technology group, the agency is focusing on whether to approve the IEEE's 802.11i WLAN security standard for encryption and authentication as a government standard. The IEEE approved 802.11i last July, but Burr says NIST is not keen on some aspects of it.
Specifically, NIST has reservations about the so-called Temporal Key Integrity Protocol (TKIP), which is the key management protocol in 802.11i that uses the same encryption engine and RC4 algorithm that was defined for the Wired Equivalent Privacy protocol (WEP).
The 40-bit WEP, used in many early WLAN products, was criticized widely in the past two years as having too short a key length and a poor key management scheme for encryption. TKIP is a "wrapper" that goes around WEP encryption and ensures that TKIP encryption is 128 bits long.
TKIP was designed to ensure it could operate on WLAN hardware that used WEP. In contrast, the 128-bit Advanced Encryption Standard (AES), which NIST already has approved, requires a hardware change for most older WLAN equipment.
"We just don't feel that the TKIP protocol cuts the grade for government encryption," Burr says. He adds that the RC4 encryption algorithm is not a Federal Information Processing (FIPS) standard and probably won't ever be because network professionals see RC4 as rather weak in terms of message authentication and integrity.
NIST is more inclined to approve AES for WLAN security, and in fact Burr pointed to the NIST document 800-38C, published last summer, for encryption that includes the AES algorithm.
As far as the key management scheme for key exchange and setup is concerned, NIST might introduce a new key-management technology that's been jointly developed with the National Security Agency.
Senior Editor Ellen Messmer covers security for Network World. Contact her at .

@_date: 2005-07-22 04:24:42
@_author: John Gilmore 
@_subject: [Clips] Venona Ten Years Later: Lessons for Today  
This was a good observation, but the next sentence muddled it with typical American self-blindness. I want the same answer about how not just the Washington elite, but
even army kids from Iowa, fail to comprehend WHY we prohibit torture,
provide fair trials and legal representation, due process of law, and
why we have a constitution or civil rights at all.  Do they not
comprehend the true nature of a United States with arbitrary searches,
travel papers, pervasive surveillance, no effective Leg. or
Jud. checks on arbitrary executive power, no federalism checks on
unlimited federal power, indefinite imprisonment of US citizens at the
will of the President, indefinite imprisonment without trial of
non-citizens seized by force anywhere in the world, and wars of
occupation?  It's caled an expanding totalitarian state, kiddies, and
every totalitarian stste tells its citizens how they are the freest
country in the world.  Get out and compare for yourself!
Then tell me what the "basic tenets of modern society" are.
    John Gilmore (posting from Greece)
PS:  Add in a lapdog press too.  Try reading the foreign press on the web.
They actually ask hard questions of pols and slam them for evading.  And all their sources aren't anonymous "highly placed govt officials".

@_date: 2005-06-03 09:11:16
@_author: John Gilmore 
@_subject: Digital signatures have a big problem with meaning  
Even mere hash checks are turning up obscure data corruptions.  Some
people reported that BitTorrent would never finish certain files,
getting to 99.9% and stalling.  The problem is that their NAT box was
replacing its external IP address with its internal address --
anywhere in a packet.  This is called "Game mode" in some NAT boxes.
Their router was corrupting random binary data (and altering the TCP,
UDP, and Ethernet packet checksums!).  They never noticed until
BitTorrent used end-to-end application-level SHA1 hash checks and
retransmission to detect and correct it.

@_date: 2005-03-04 15:53:51
@_author: John Gilmore 
@_subject: SSL Cert prices ($10 to $1500, you choose!) 
For the privilege of being able to communicate securely using SSL and a
popular web browser, you can pay anything from $10 to $1500.  Clif
Cox researched cert prices from various vendors:

@_date: 2005-03-22 09:06:41
@_author: John Gilmore 
@_subject: DOT neg rulemaking re ID standardization (call for membership of 
[Here's where an unconstitutional National ID will get created by the
back door.  Do we have anybody in this community who cares?  I can't
participate, because I can't travel to Washington for meetings,
because I don't have the proper ID documents.  I note that they did
not think to include a representative of "undocumented people"...
[Federal Register: February 23, 2005 (Volume 70, Number 35)]
[Proposed Rules]
[Page 8756-8761]
 From the Federal Register Online via GPO Access [wais.access.gpo.gov]

@_date: 2005-05-04 23:49:58
@_author: John Gilmore 
@_subject: Network World: 10-node Quantum Crypto net under Boston streets 
NETWORK WORLD NEWSLETTER: OPTICAL NETWORKING
Today's focus:  Hooked on photonics
By Amy Schurr
CAMBRIDGE, MASS. - Chip Elliott is every hacker's worst Elliott, principal scientist at BBN Technologies, leads a team building the world's first continuously operating quantum cryptography network, a 12-mile snoop-proof glass loop under the streets of Boston and Cambridge.
Quantum cryptography uses single photons of light to distribute keys to encrypt and decrypt messages. Because quantum particles are changed by any observation or measurement, even the simplest attempt at snooping on the network interrupts the flow of data and alerts administrators.
While the technology is still in the pilot stage, Elliott envisions a day when quantum cryptography will safeguard all types of sensitive traffic. "It's not going to overnight replace everything we have," he says. But it will be used to augment current technologies.
Defense funding
BBN's research is funded by the Pentagon's Defense Advanced Research Projects Agency , so it's likely the government would be first in line to roll out the super-secure technology. Elliott predicts financial firms will deploy quantum cryptography within a few years and estimates that businesses in general will deploy within five years. The technology also could move to the consumer market - for example, in a fiber-to-the-home scenario to protect the network between a home and service provider.
"People think of quantum cryptography as a distant possibility, but [the network] is up and running today underneath Cambridge," Elliott says. The team of nine researchers from BBN, four from Boston University and two from Harvard University, have put together "a set of high-speed, full-featured quantum cryptography systems and has woven them together into an extremely secure network," he says.
The system is essentially two networks - one for quantum key distribution and one that carries the encrypted traffic. And although it's probably the world's most secure network, it's not protecting any real secrets, at least not yet. For this pilot phase, BBN encrypts normal Internet traffic such as Web pages, Webcam feeds and e-mail.
The network has 10 nodes. Eight are at BBN's offices in Cambridge, one is at Harvard in Cambridge, and another is across the Charles River at BU's Photonics Center.
In keeping with the traditional naming convention that IT security professionals use, the nodes are named Alice, Bob, Ali, Baba, Amanda, Brian, Anna, Boris, Alex and Barb.
For the complete story, please go to: To contact: Amy Schurr
Amy Schurr is an editor for Network World's Management Strategies and Features sections. If you have any career topics you'd like her to cover or want to comment on this newsletter, you can reach her at .
Copyright Network World, Inc., 2005

@_date: 2005-05-08 17:47:25
@_author: John Gilmore 
@_subject: Export controls: US wants to export-license fundamental research again 
The export control snakes are trying to crawl out of their snakepit
again.  By tiny wording changes, they're trying to overturn the
exemptions that protect First Amendment activity from being restricted
by the export controls.  We have until May 27 to file written
Remember that the government "voluntarily" changed the export controls
after losing the Bernstein case on First Amendment grounds.  Now the
hawks want to change them back.  They've been quietly writing up
"Inspector General" reports for years.  Here's the latest:
  Here's the proposed regs:
  Send comments to:
  To: scook at bis.doc.gov
  Cc: gnu at toad.com (please)
  Subject: RIN 0694-AD29
By replacing an "and" with an "or", they want to require every
university, research lab, and company to be required to segregate
foreign students, researchers, scientists, and staff from the Honest
Amurricans.  This is to keep these evil furriners away from fast
computers, oscilloscopes, and even GPS systems -- even if the
foreigner is doing fundamental research protected by the First
Amendment.  This includes censorship of the *manuals* for anything
export-controlled.  It's the same old definition game they played with
crypto exports: "technology" means anything, and "export" means "let a
foreigner see", therefore the ban on "exporting technology" translates
The Commerce Dept. needs to hear, loud and clear, that if the
regulations restrict fundamental research, the REGULATIONS should be
changed, not the fundamental research.  The Inspector General who
recommended that the export controls be tightened needs to be reminded
that he supposedly works for a free country.
I suggest citing the fact that a US appeals court decided that the
export controls were a prior restraint on free expression and violated
the First Amendment when applied to scientists and educators doing
fundamental research (Bernstein v. USDoJ):
  (later withdrawn by the court due to Justice Dept. trickery).  A later
appeals court held that the First Amendment protected the publication
of software from the export controls, though it did not decide what
level of scrutiny was appropriate: Junger v. Daley, 209 F.3rd 481 (6th
Cir. 2000):
  In the recent shenanigans, Commerce Dept. inspectors visited NIST and
NOAA, and found that even though both agencies are doing "fundamental
research", the inspectors think these agencies should have to
segregate foreign researchers and get "deemed export" licenses for
their research.  They were shocked to discover one German machine tool
at NIST's Manufacturing Engineering Lab, and two fermenters at
universities, that foreigners aren't allowed to have.  They were also
shocked to find that the operating manual for the machine tool was
right there next to the machine!  They suggested that if a foreign
resercher MERELY READ the manual, that an illegal "deemed export"
would have occurred.  Reading NIST's response on PDF pages 55-59 is
very enlightening; the inspectors' report deliberately twists the
situation to make it look far worse than it was.  It reminds me of the
Wen Ho Lee case.
(By the way, the inspectors censored half a sentence from their own
report on PDF page 42, numbered page "32" at the bottom of the page.
The text behind the censorship reads "In addition, the 5-axis machine
tool is located immediately to the right of the entrance to the
machine shop and is not segregated from other equipment."  The report
also included two uncensored color photos of the machine, in case you
were a foreigner wondering how to identify it.  It's so comforting to
know that highly competent security professionals like this are in
charge of censoring all scientific research in the country.)
The inspectors also claim that because the Bush White House forced a
"pre-review" policy on publication of government scientific research
"that might help terrorists", the First Amendment "fundamental
research" definition no longer applies to NIST -- even though the
pre-review has never turned down a paper yet.
The Inspector General's report never once mentions the First Amendment.
E.g. on page 23, it says:
  "The rationale for eliminating foreign nationals with permanent
  resident status from deemed export controls appears to have been that
  persons who hold such status have made a committment to the United
  States and most likely will not return home."
Actually, the issue is that permanent residents have the same
First Amendment rights that citizens do.  This includes freedom of
inquiry (the right to do research) and the right to publish the
results.  These inspectors instead seem to follow the Lt. Calley "burn
the village in order to save it" model.
The vast majority of the "deemed exports" have been in computers (661
in 2003), telecomm and information security (357), and electronics
(338).  In 2003, no other category had more than 80 licenses applied
for.  This is all about computers and crypto -- it's not about nuclear
weapons (0 in 2003).
The vast majority of the foreigners involved came from China (367),
Russia (238), and India (83), out of 846 total.  (These stats are on
page 15.)
The inspectors, and the Commerce Dept., also propose a rule that says
the country where you were born trumps your current citizenship.  The
export regs are different for each country, so someone who fled Hong
Kong and took up Canadian citizenship would be considered Chinese
under these rules.  The racist implications seem to be strongly
focused on denying access to high-tech equipment to people of Chinese
and Muslim descent when they're studying or working in the United
Forwarded-By: David Farber Forwarded-By: Jennifer Mankoff Dear Chairs of PhD-granting Physics Departments,
I am writing to alert you to a possible threat to research in your
department and to urge you and your faculty to write to the Department
of Commerce (DOC) in response to its "Advance notice of proposed
rulemaking" published in the Federal Register on March 28, 2005. The
notice calls for comments that must be received by May 27, 2005. As
discussed below, the leadership of the American Physical Society feels
this issue is so important that you should seek to provide thoughtful
and accurate responses by your university administration, your
department and individual faculty who might be affected by the
recommended changes. We believe that your comments can make a
The proposed rulemaking by the DOC is a response to recommendations
presented by the Department's Inspector General. Implementation of
these recommendations would cause two major changes:
1) The operation of export-controlled instrumentation by a foreign
national working in your department would be considered a "deemed
export", even if that person were engaged in fundamental research. As
a consequence, a license would be required for each affected foreign
national (student, staff or faculty member) and for each export
controlled instrument. Typical export-controlled instruments are
high-speed oscilloscopes, high-resolution lithography systems,
high-end computers and GPS systems. The situation is complicated by
the fact that the list of instruments is different for each country.
2) U.S. organizations would be required to apply for a deemed export
license for students, employees or visitors who are foreign nationals
(but not U. S. naturalized citizens or permanent residents) and have
access to controlled technology if they were born in a country where
the technology transfer in question would require an export license,
regardless of their most recent citizenship or permanent residency.
For example, transfer of technology to a Chinese scientist who has
established permanent residency or citizenship in Canada would be
treated, for export licensing purposes under the proposed guidelines,
as a deemed export to a Chinese foreign national. (The list of
export-controlled instruments for Chinese nationals is particularly
The Department of Commerce officials who have the responsibility for
developing new policies and practices in response to the Inspector
General's recommendations are anxious to determine what the impact of
implementing those recommendations would be. They must seek a balance
between increases in national security that might result from the
implementation of the new rules and the decrease in national security
that would result from negative impacts to US research and
In initial discussions by the APS Panel on Public Affairs (POPA) it
was thought likely that consequences would be: a) research would slow
down significantly due to the need to obtain licenses for each foreign
national and, particularly, Chinese student, staff member, postdoc, or
faculty member using export controlled instrumentation. We believe
that a separate license would have to be obtained for each instrument.
In this regard, it should be noted that the relevant DOC office has
the staff to handle about 800-1000 license requests per year.  Present
times to process a license request are typically 2-3 months. b)
instruments would have to be secured to ensure that those who do not
have the required license could not use them. c) the number of Chinese
and other foreign national students would decrease markedly as their
"second-class" status on campus became apparent, thus ultimately
weakening the nation's science and technology workforce. d) the
administrative costs of research would rise markedly. e) national
security would ultimately be weakened as a consequence of a loss of
leadership in economic and technology development.
We urge you, therefore, to have faculty members who are
experimentalists respond to the DOC's notice by estimating, as
accurately as possible, the impact on their research. This would
involve a determination of which instruments are probably export
controlled for each nation "represented" by foreign nationals in the
laboratory. (The person responsible for export control administration
in the institution should be able to help with this.) You should then
send the DOC either a comment from the department as a whole or,
better yet, individual comments, which state the number and types of
instruments involved, the number of students, staff or postdocs from
each affected nation and the likely number of licenses to be requested
if the recommendations are implemented. It would also be helpful if
comments contained a brief description of the type of research
performed in the laboratory. Estimates of the consequences of three
months delays in research for each new foreign national student and
each new export controlled instrument will also be valuable.
You may regard this as rather burdensome, but it is our belief that
implementation of the Inspector General's recommendations will be far
more burdensome. Therefore, we hope that you will get every
experimentalist to reply.
To submit your comments, you can go to  and enter
the Key Phrase "Revision and Clarification of Deemed Export Related
Regulatory Requirements." You can also view the proposed new
regulation at this site and note what a large effect changing an "and"
to "or" can make.
Best regards,
Judy Franz
Executive Officer

@_date: 2005-09-16 18:39:38
@_author: John Gilmore 
@_subject: Clearing sensitive in-memory data in perl  
The C language is not the problem.  The C library is not the problem.
Both of these things were fixed during ANSI standardization, so that
standard-conforming programs will not fail runtime checks for
overrunning arrays (strings are just arrays of characters).
There have been various C implementations that did these checks,
including both compilers and interpreters.  Some are free, some are
proprietary.  (I proposed to fund adding these checks to GCC, but the
price I was quoted was too high for me.)  I fault the people who don't
use such tools -- not the C language.
(Aside: What ever happened to Saber C?  Oh, it was renamed to
Centerline CodeCenter, never made it out of the Unix workstation
market, used "FlexLM" per-cpu licensing crap, has gone morbid, and was
acquired a year ago by ICS.com, a graphics library company, with a
promise to port it to Linux.  There's no evidence of such a port, and
the "product support matrix" was last updated in June 2001.  The
product doesn't appear on ICS's product pages.  I wonder how cheaply
the source could be bought and freed up, to bring it back to life?  It
was a nice product, fifteen years ago.)
The reason there's fewer security bugs in PL/1 programs than C
programs is because almost nobody has written programs in PL/1 since
about 1985.  Google did find me a compiler you can download -- it runs
on VMS, on Vaxes or Alphas.  Anybody still running those space-heaters
is welcome to program in PL/1.  The rest of us have real work to do,
and it's likely to get done in C or C++.

@_date: 2005-09-18 20:44:09
@_author: John Gilmore 
@_subject: [Clips] Contactless payments and the security challenges  
Interesting article, but despite the title, there seems to be no
mention of any of the actual security (or privacy) challenges involved
in deploying massive RFID payment systems.  E.g. I can extract money
from your RFID payment tag whenever you walk past, whether you
authorized the transaction or not.  And even assuming you wanted it
this way, if your Nokia phone has an RFID chip in it, who's going to
twist the arms of all the transit systems and banks and ATM networks
and vending machines and parking meters and supermarkets and
libraries?  Their first reaction is going to be to issue you an RFID
themselves, and make you juggle them all, rather than agreeing that
your existing Nokia RFID will work with their system.  If you lose
your cellphone, you can report it gone (to fifty different systems),
and somehow show them your new Motorola RFID, but how is each of them
going to know it's you, rather than a fraudster doing denial of
service or identity theft on you?
Then there's the usual "tracking people via the RFIDs they carry"
problem, which was not just ignored -- they claimed the opposite:
"This kind of solution provides privacy, because the token ID is
meaningless to anyone other than the issuing bank which can map that
ID to an actual account or card number."  That is only true once --
til anyone who wants to correlates that token ID "blob" with your
photo on the security camera, your license plate number (and the RFIDs
in each of your Michelin tires), the other RFIDs you're carrying, your
mobile phone number, the driver's license they asked you to show, the
shipping address of the thing you just bought, and the big database on
the Internet where Equifax will turn a token ID into an SSN (or vice
verse) for 3c in bulk.
The article seems to have a not-so-subtle flavor of boosterspice.
Anybody got a REAL article on contactless payments and security

@_date: 2005-09-19 16:20:07
@_author: John Gilmore 
@_subject: Defending users of unprotected login pages with TrustBar 0.4.9.93  
Perhaps the idea of "automatically" redirecting people to alternative
pages goes a bit too far:
How convenient!  So if I could hack your server, I could get all
TrustBar users' accesses -- to any predefined set of pages on the
Internet -- to be redirected to scam pages.
A redirect to an "untrustworthy" page is just as easy as a redirect to a
"trustworthy" page.  The question is who you trust.
Also providing a handy platform for slightly modified versions, that will
take their cues from a less "trustworthy" list of redirects.

@_date: 2005-09-25 22:25:45
@_author: John Gilmore 
@_subject: An overview of cryptographic protocols to prevent spam  
I stopped reading on page V -- it was too painfully obvious that Amir
has bought into the whole censorship-list based "anti-spam" mentality.
It was hard to get from paragraph to paragraph without finding
approving mentions of blacklists.  I am a victim of many such
blacklists.  May Amir never appear on one, or his unthinking
acceptance of blacklisting might change.  His analysis made me think
of clinical reviews of experiments done on human subjects in prison
camps -- careful to focus on the facts while ignoring the obvious
moral problems.
Interspersed were discussions of various kinds of port blocking.  The
Internet is too good for people who'd censor other peoples'
communications, whether by port number (application) or by IP address
(person).  It saddens me to see many of my friends among that lot.

@_date: 2006-04-05 14:44:06
@_author: John Gilmore 
@_subject: Unforgeable Blinded Credentials  
Hmm, why doesn't this blacklisting get mentioned in IBM's DAA page?
What sort of blacklist would this be?  What actions would being listed
on it trigger?

@_date: 2006-08-07 17:12:45
@_author: John Gilmore 
@_subject: SSL Cert Prices & Notes 
Howdy Hackers,
Here is the latest quick update on SSL Certs. It's interesting that generally prices have risen. Though ev1servers are still the best commercial deal out there.
The good news is that CAcert seems to be posistioned for prime time debut, and you can't beat *Free*. :-)
SSL Certificate Authorities	Verification	Subdomains Too
 				Low	High	Low	High
 	Verisign		$399	$995
 	Geotrust		$189	$349	$599	$1499
 	Thawte			$149	$199	$799	$1349
 	Comodo / instantssl	$49	$62.50	$449.95
 	godaddy.com		$17.99	$74.95	$179.99	$269.99
 	freessl.com		$69	$99	$199	$349
 	ev1servers		$14.95	$49
 	CAcert 			Free	Free	Free	Free
Inclusion Status:  	Clif

@_date: 2006-12-23 12:48:50
@_author: John Gilmore 
@_subject: Big NSA expansion in Augusta, GA 
This comes from an interesting "SIGINT and more" blog from
the Augusta "Metro Spirit", a local weekly newspaper.  Excerpts:
... Augusta is about to get a $340-million taste of Sweet Tea.
The National Security Agency is building a massive new operations
facility, dubbed project Sweet Tea. It will come complete with all the
amenities: a workout room, nursing areas, a mini-shopping center, a
credit union, an 800-seat cafeteria and thousands of exclusive parking
spaces. Secret parking spaces.
There are, of course, actual operational national security-type
elements to the project. For example, it will include a new shredder
facility (for all those classified documents) and an antenna farm (to
help listen in on enemy combatants like Osama bin Laden and Princess
Di).  ...
The document says the main new structure, a 525,000- square-foot
Regional Security Operations Center, should be complete by May 2010.
The NSA and its allies in the U.S. Congress have been pushing this
project for years. The Defense Department requested a $340.8 million
appropriation for the Georgia Regional Security Operations Center back
in February. And a construction award was scheduled for Sept. 25, NSA
documents show.
Maybe the deal was awarded on schedule. Maybe there was a
delay. Either way, it wasn't announced until Dec. 8, one day after the
Metro Spirit started calling around with questions. The announcement
was one of only eight press releases that the usually silent spy
agency had issued all year.  ...
Indeed, there is reason to believe that the NSA-Georgia project's
actual cost will be even higher than the $340 million that's
known to have been appropriated.
A military source familiar with cost analysis told the Metro Spirit
that the facilities may wind up costing more than $1 billion.  ...
Clyde Taylor, military legislative assistant to Georgia Sen. Saxby
Chambliss, said his office spent a couple of years obtaining the
appropriation. Taylor also gave credit to Georgia Rep. Charlie
Norwood, whose office issued its own press release last Friday.
The need for the new NSA facility is driven by the growth in overseas
surveillance activities, Taylor said. He said that the agency plans to
move linguists and analysts down from its Fort Meade, Md.,
headquarters to the Augusta listening station, which targets the
Middle East.

@_date: 2006-02-10 09:15:26
@_author: John Gilmore 
@_subject: GnuTLS 1.2.10 - Security release 
OpenPGP: id=B565716F; url=
X-Hashcash: 1:21:060209:gnutls-dev at gnupg.org::zaOuZtWmJFhp9CnX:7K5h
X-Hashcash: 1:21:060209:help-gnutls at gnu.org::jeAkm4ig/gb/UmeB:9RnD
X-Hashcash: 1:21:060209:info-gnu at gnu.org::Ii3w27rTBUk11ps6:Qt4B

@_date: 2006-02-13 15:15:49
@_author: John Gilmore 
@_subject: HDCP support in PCs is nonexistent now? 
HDCP is Intel-designed copy prevention that uses strong crypto to
encrypt the digital video signal on the cable between your video card
(or TV or DVD player) and your monitor.  There is no need for it --
you are seeing the signal that it is encrypting -- except for DRM.
Despite a bunch of PC graphics chips and boards having announced HDCP
support, according to the above article, it turns out that none of
them will actually work.  It looks like something slipped somewhere,
and an extra crypto-key chip needed to be added to every existing
board -- at manufacturing time.  My wild ass guess is that the
original design would have had software communicate the keys to the
board, but Hollywood has recently decided not to trust that design.
This is going to make life very interesting for the HD-DVD crowd.
Intel's grand scheme was to corrupt the PC to an extent that Hollywood
would trust movies, music, etc, to PCs.  Intel decided to learn from
an oligopoly what they know about extending a monopoly into the
indefinite future, by combining legislative bribery with technological
tricks.  Now it appears that even though they have largely succeeded
in pushing all kinds of crap into PC designs, Hollywood doesn't trust
the results enough anyway.  The result may well be that HD-DVDs that
contain movies can only be played on dedicated equipment (standalone
HD-DVD players), at least for the first few years.  Or, you'll need a
new video board, which nobody sells yet, when you buy your first
HD-DVD drive.  Or the DRM standards involved will have to be somehow
Anybody know anything more about this imbroglio?
PS:  Of course, the whole thing is foolish.  DVD "encryption" has been
cracked for years, and circumvention tools widely distributed
worldwide, despite being too illegal to appear in out-of-the-box
products.  DVD encryption has provided exactly zero protection for DVD
revenues -- yet DVD revenues are high and rising.  In short, unless
Hollywood was lying about its motivations, DRM has so far been useless
to Hollywood.  Yet it has done great violence to consumers, to
computer architecture, to open competition, and to science.

@_date: 2006-01-22 23:26:36
@_author: John Gilmore 
@_subject: CodeCon + Chinese New Year Treasure Hunt 
[Moderator's note: Not our usual fare, but since a bunch of people
reading this list will be at codecon I thought I'd forward it
anyway. --Perry]
An annual mass treasure hunt, roaming part of San Francisco for clues,
occurs on Saturday, Feb 11th, the middle day of CodeCon.  By skipping
2-1/2 evening hours of Codecon, you could dodge dragons and fireworks
in Chinatown while puzzling out history, rhymes, or puns with a roving
team of friends.  Rain or shine.  Bring good walking shoes & outdoor
gear.  Let's made some Codecon teams (or you can just show up).  The
huge Chinese New Year parade will be happening at the same time,
providing an even more colorful backdrop than the usual bizarre
Chinatown ambiance.  Info and rules are here:
  Here's a sample clue:
  King Ludwig had one; so did William Randolph Hearst. These are some
  others. If you know what they are, you'll know which street to
  search on: Edinburgh Dover Windsor Corfe Arundel.
  It's a great way to see some odd and interesting corners of SF's Chinatown,
North Beach, and the waterfront.  See you there.

@_date: 2006-05-05 12:32:29
@_author: John Gilmore 
@_subject: Encrypted disk storage 
I'm sure I've seen modern disk drives that allow reformatting to use
sectors of 516 or 520 or 524 bytes rather than 512 bytes.  This would
require some generalization in the low-level I/O buffering code, but
would permit both integrity and transparency at the filesystem level.
It might also throw fits with forensic software (or even Live CDs
inserted by a thief or intruder) that expect 512 byte sectors.

@_date: 2006-05-08 17:53:08
@_author: John Gilmore 
@_subject: "boarding passes", identity, and security 
But what does this have to do with anything?  It doesn't take a
database to "print your own boarding pass".  On the vast majority of
ticketed services, your printed ticket IS your boarding pass.  Like
trains, theatres, concerts, ferries, buses...
So, Steve, what you're saying is that the airlines threw an extra
hurdle in their customers' way for some years, then decided to take it
away for certain customers.  Well, not really take it away -- just let
some of you use a computer with a printer to kludge around it.  And
you mistook that for progress?  They could have just printed the
seat numbers on the ticket, and actually honored the seat numbers on the ticket; that would be REAL progress.
The best plane service in the nation was the old Eastern shuttle that
ran between DC and NYC and Boston.  You showed up -- no tickets -- and
walked on board.  It left once an hour, all day, like a train.  The
fare was collected on board, the same way the stewards go down the
aisle and serve drinks.  It cost everybody the same price.
None of today's endless crap.

@_date: 2006-05-19 15:32:13
@_author: John Gilmore 
@_subject: May 24: National Day of Outrage at NSA/Telco surveillance 
Some alternative media groups have called for a national day of protests
against the telcos' latest sleazy activities, including their cooperation
in NSA's illegal surveillance of innocent citizens.
  Events are already scheduled in Boston, Chicago, San Francisco, and
NYC.  You can register your own local event by sending mail to
protest at saveaccess.org.
Curiously, nobody in Washington, DC or Baltimore is protesting yet.
Perhaps a contingent should form outside NSA, with signs showing the
NSA employees on their way to/from work just what we think of their
disrespect for the constitution, the law, and the public.  Do we have
a local volunteer to organize it?
PS: I don't agree with all the things these people are protesting, but
I admire their energy.  I haven't seen cryptographers and cypherpunks
with protest signs -- yet.  But I hope to see you out there on May 24th.

@_date: 2006-05-26 18:53:08
@_author: John Gilmore 
@_subject: Hayden's statement from Oct 2002 on liberty and security 
While testifying to a joint hearing of the House and Senate
intelligence committees a year after 9/11, Michael Hayden, as NSA
Director, testified about NSA's response to 9/11.  In closing, he
38. When I spoke with our workforce shortly after the September 11th
    attacks, I told them that free people always had to decide where
    to draw the line between their liberty and their security, and I
    noted that the attacks would almost certainly push us as a nation
    more toward security. I then gave the NSA workforce a challenge:
    We were going to keep America free by making Americans feel safe
    again.
39. Let me close by telling you what I hope to get out of the national
    dialogue that these committees are fostering. I am not really
    helped by being reminded that I need more Arabic linguists or by
    someone second-guessing an obscure intercept sitting in our files
    that may make more sense today than it did two years ago. What I
    really need you to do is to talk to your constituents and find out
    where the American people want that line between security and
    liberty to be.
40. In the context of NSA's mission, where do we draw the line between
    the government's need for CT information about people in the
    United States and the privacy interests of people located in the
    United States?
    Practically speaking, this line-drawing affects the focus of NSA's
    activities (foreign versus domestic), the standard under which
    surveillances are conducted (probable cause versus reasonable
    suspicion, for example), the type of data NSA is permitted to
    collect and how, and the rules under which NSA retains and
    disseminates information about U.S. persons.
41. These are serious issues that the country addressed, and resolved
    to its satisfaction, once before in the mid-1970's. In light of
    the events of September 11th, it is appropriate that we, as a
    country, readdress them. We need to get it right. We have to find
    the right balance between protecting our security and protecting
    our liberty. If we fail in this effort by drawing the line in the
    wrong place, that is, overly favoring liberty or security, then
    the terrorists win and liberty loses in either case.
42. Thank you. I look forward to the committees' questions.
Now we know a small part of what he was really talking about.  At
least he had the balls to mention it.  But who among us could suspect
that when Congress responded by Patriot Act tune-ups making many kinds
of wiretapping easier, NSA's reaction was to ignore the laws, treating
the illegality of its operations as a "classified technique" for
surprising the "secret enemy under our beds".  Anyone who had said NSA
was a rogue that ignored the laws, before or after 9/11, was either
called paranoid, unrealistically cynical, or "against us and for the
Read this again:
    Practically speaking, this line-drawing affects the focus of NSA's
    activities (foreign versus domestic), the standard under which
    surveillances are conducted (probable cause versus reasonable
    suspicion, for example), the type of data NSA is permitted to
    collect and how, and the rules under which NSA retains and
    disseminates information about U.S. persons.
Now we find out that NSA has crossed each of these lines.  It is now
focusing domestically.  It now uses a "reasonable suspicion" standard
adjudicated by its own staff.  It is collecting all types of data "and
how!", apparently retaining that data indefinitely, and disseminating
it as it sees fit (to the FBI, at least).
In the open crypto community, we noticed this curious part of his
speech, but generally didn't engage with him.  Personally I felt that
whatever I said would be ignored, just as my concerns were ignored
during the entirety of the 1990's, in the Clipper Chip debacle and the
Export Control madness.  We were ignored until we forced change upon
NSA with the courts and, in partnership with business, in Congress.
We are having to take the same routes today (though business is now
against us, since business is up to its eyeballs in spying).
Did anyone else respond to Mr. Hayden at that time, and if so, what
reaction did you get?
PS: NSA's web site SIGINT FAQ still says they don't
"unconstitutionally spy on Americans".  It raises some guff about the
Fourth Amendment and strictly following the laws.
( But I hear that if you're
discussing something classified, it's not only acceptable to lie, but
it's actually required.

@_date: 2006-10-10 10:44:54
@_author: John Gilmore 
@_subject: TPM & disk crypto  
Having "remote attestation" that provides signed checksums of every
stage of the startup process, which are checked by guys at the RIAA or
guys at the bank, will lead to legions of unhappy customers who find
their system boots fine, but is denied access to both the bank and the
music store.  (Seventy thousand totally valid configurations are not
going to be checked and confirmed by either one.)  But their system
will access the Darknet just fine.

@_date: 2006-09-26 16:01:16
@_author: John Gilmore 
@_subject: National Security Agency ex-classified publication indexes now online 
[The Memory Hole also publishes an interesting list of FOIA logs,
 listing who asked NSA for what, across many years.  I see a lot of
 friends in there.   -- gnu]
HUGE CACHE OF NATIONAL SECURITY AGENCY INDEXES PUBLISHED ONLINE
By Michael Ravnitzky , mikerav at verizon.net
This page, just published online at Russ Kick's site, The Memory Hole:
Or you can get there from his home page at
contains indexes of four periodicals published by the National Security
Agency, plus a listing of publications from the NSA's Center for Cryptologic
History. These indexes haven't been publicly released until now, and many of
the Cryptologic History publications weren't previously known to the public:
Cryptologic Quarterly Index
NSA Technical Journal Cumulative Index
Cryptologic Spectrum Index
Cryptologic Almanac Index
Center for Cryptologic History Publications
You can request from NSA a copy of any of the reports listed in these
hundreds of pages of indexes, using the instructions provided.

@_date: 2007-02-15 06:10:12
@_author: John Gilmore 
@_subject: Intel finally plans to add the NSA instruction 
Page 7 of the PDF describes the POPCNT "application-targeted accelerator".
PS:  They don't give much detail, but they seem to be adding a grep
instruction too (at least fgrep), and a zlib accelerator.  Anybody know
more, while it's still early enough to get them to change the most bogus

@_date: 2007-01-27 15:56:57
@_author: John Gilmore 
@_subject: News.com: IBM donates new privacy tool to open-source Higgins 
IBM donates new privacy tool to open-source
  By  Joris Evers
  Staff Writer, CNET News.com
  Published: January 25, 2007, 9:00 PM PST
IBM has developed software designed to let people keep personal  information secret when doing business online and donated it to the  Higgins open-source project.
  The software, called "Identity Mixer," was developed by IBM  researchers. The idea is that people provide encrypted digital  credentials issued by trusted parties like a bank or government agency  when transacting online, instead of sharing credit card or other  details in plain text, Anthony Nadalin, IBM's chief security architect,  said in an interview.
  "Today you traditionally give away all of your information to the man  in the middle and you don't know what they do with it," Nadalin said.  "With Identity Mixer you create a pseudonym that you hand over."
  For example, when making a purchase online, buyers would provide an  encrypted credential issued by their credit card company instead of  actual credit card details. The online store can't access the  credential, but passes it on to the credit card issuer, which can  verify it and make sure the retailer gets paid.
  "This limits the liability that the storefront has, because they don't  have that credit card information anymore," Nadalin said. "All you hear  about is stores getting hacked."
  Similarly, an agency such as the Department of Motor Vehicles could  issue an encrypted credential that could be used for age checks, for  example. A company looking for such a check won't have to know an  individual's date of birth or other driver's license details; the DMV  can simply electronically confirm that a person is of age, according to    The encrypted credentials would be for one-time use only. The next  purchase or other transaction will require a new credential. The  process is similar to the one-time-use credit card numbers that  Citigroup card holders can already generate on the bank's Web site.
  IBM hopes technology such as its Identity Mixer helps restore trust in  the Web. Several surveys in past years have shown that the seemingly  incessant stream of data breaches and threats such as phishing scams  are eroding consumer confidence in online shopping and activities such  as banking on the Web.
  To get Identity Mixer out of the lab and into the real world, IBM is  donating its work to Higgins project, a broad, open-source effort  backed by IBM and Novell that promises to give people more control of  their personal data when doing business online. Higgins also aims to  make the multiple authentication systems on the Net work together,  making it easier for people to manage Internet logins and passwords.
  "We expect Higgins to get wide deployment and usage. You'll get the  ability by using Higgins to anonymize data," Nadalin said.
  Higgins is still under development. A first version of the projects  work is slated to be done sometime midyear, said Mary Ruddy, a Higgins  project leader. "We were thrilled to get this donation to Higgins, IBM  has done a lot of good work."

@_date: 2007-03-07 02:25:24
@_author: John Gilmore 
@_subject: Man sues Microsoft for snake oil security that lets the FBI in 
Forwarded-By: Brad Templeton <4brad at templetons.com>
The plaintiff is suing Microsoft (and already got a settlement from
Compaq and Circuit City) because in spite of the security tools they
sold him, the FBI forensic lab was able to get at his data in a criminal

@_date: 2007-05-08 12:03:09
@_author: John Gilmore 
@_subject: Was a mistake made in the design of AACS?  
This approach was rejected by the computer industry, in particular
with respect to DVDs.  Computer companies like Intel, HP, Dell, and
Sony wanted to be able to compete to be a "consumer electronics"
platform, playing music, video, photos, etc.  Indeed, many of the
advances in consumer electronics have come from computerization, such
as digital music (DATs and CDs), MP3 players, digital video, fax
machines, digital cameras and digital photo storage, color photo
printers, ...
I do recall that it took most of a decade for computer "CD-ROM" drives
to be able to digitally read audio CDs, and then later to record them.
Silicon Graphics gets major kudos for breaking that artificial barrier.
False.  It's like saying "Then finding a record album on a cassette tape
would instantly imply that it's pirated."  No, it would instantly imply
that it's been copied onto a medium of the consumer's choice.  Consumers
are (and should be) free to record copyrighted works onto media of their
own choice, for their own convenience, without needing the permission or
concurrance of the copyright owner.
Congratulations, Nico, you fell into Hollywood's favorite word:
"pirated".  It takes discipline to stop thinking in the grooves that
they have worn in your brain.

@_date: 2007-05-27 22:56:56
@_author: John Gilmore 
@_subject: LA Times: US funds super wiretap system for Mexico 
Mexico to boost tapping of phones and e-mail with U.S. aid
Calderon is seeking to expand monitoring of drug gangs; Washington also may have access to the data.
By Sam Enriquez, Times Staff Writer
May 25, 2007
MEXICO CITY - Mexico is expanding its ability to tap telephone calls and e-mail using money from the U.S. government, a move that underlines how the country's conservative government is increasingly willing to cooperate with the United States on law enforcement.
The expansion comes as President Felipe Calderon is pushing to amend the Mexican Constitution to allow officials to tap phones without a judge's approval in some cases. Calderon argues that the government needs the authority to combat drug gangs, which have killed hundreds of people this year.
Mexican authorities for years have been able to wiretap most telephone conversations and tap into e-mail, but the new $3-million Communications Intercept System being installed by Mexico's Federal Investigative Agency will expand their reach.
The system will allow authorities to track cellphone users as they travel, according to contract specifications. It includes extensive storage capacity and will allow authorities to identify callers by voice. The system, scheduled to begin operation this month, was paid for by the U.S. State Department and sold by Verint Systems Inc., a politically well-connected firm based in Melville, N.Y., that specializes in electronic surveillance.
Although information about the system is publicly available, the matter has drawn little attention so far in the United States or Mexico. The modernization program is described in U.S. government documents, including the contract specifications, reviewed by The Times.
They suggest that Washington could have access to information derived from the surveillance. Officials of both governments declined to comment on that possibility.
"It is a government of Mexico operation funded by the U.S.," said Susan Pittman, of the State Department's Bureau of International Narcotics and Law Enforcement Affairs. Queries should be directed to the Mexican government, she said.
Calderon's office declined to comment.
But the contract specifications say the system is designed to allow both governments to "disseminate timely and accurate, actionable information to each country's respective federal, state, local, private and international partners."
Calderon has been lobbying for more authority to use electronic surveillance against drug violence, which has threatened his ability to govern. Despite federal troops posted in nine Mexican states, the violence continues as rival smugglers fight over shipping routes to the U.S.-Mexico border, as well as for control of Mexican port cities and inland marijuana and poppy growing regions.
Nonetheless, the prospect of U.S. involvement in surveillance could be extremely sensitive in Mexico, where the United States historically has been viewed by many as a bullying and intrusive neighbor. U.S. government agents working in Mexico maintain a low profile to spare their government hosts any political fallout.
It's unclear how broad a net the new surveillance system will cast: Mexicans speak regularly by phone, for example, with millions of relatives living in the U.S. Those conversations appear to be fair game for both governments.
Legal experts say that prosecutors with access to Mexican wiretaps could use the information in U.S. courts. U.S. Supreme Court decisions have held that 4th Amendment protections against illegal wiretaps do not apply outside the United States, particularly if the surveillance is conducted by another country, Georgetown University law professor David Cole said.
Mexico's telecommunications monopoly, Telmex, controlled by Carlos Slim Helu, the world's second-wealthiest individual, has not received official notice of the new system, which will intercept its electronic signals, a spokeswoman said this week.
"Telmex is a firm that always complies with laws and rules set by the Mexican government," she said.
Calderon recently asked Mexico's Congress to amend the country's constitution and allow federal prosecutors free rein to conduct searches and secretly record conversations among people suspected of what the government defines as serious crimes.
His proposal would eliminate the current legal requirement that prosecutors gain approval from a judge before installing any wiretap, the vetting process that will for now govern use of the new system's intercepts. Calderon says the legal changes are needed to turn the tide in the battle against the drug gangs.
"The purpose is to create swift investigative measures against organized crime," Calderon wrote senators when introducing his proposed constitutional amendments in March. "At times, turning to judicial authorities hinders or makes investigations impossible."
But others argued that the proposed changes would undermine constitutional protections and open the door to the type of domestic spying that has plagued many Latin American countries. Colombian President Alvaro Uribe last week ousted a dozen generals, including the head of intelligence, after police were found to be wiretapping public figures, including members of his government.
"Calderon's proposal is limited to 'urgent cases' and organized crime, but the problem is that when the judiciary has been put out of the loop, the attorney general can basically decide these however he wants to," said John Ackerman, a law professor at the National Autonomous University of Mexico. "Without the intervention of a judge, the door swings wide open to widespread abuse of basic civil liberties."
The proposal is being considered by a panel of the Mexican Senate. It is strongly opposed by members of the leftist Democratic Revolution Party. Members of Calderon's National Action Party have been lobbying senators from the former ruling party, the Institutional Revolutionary Party, for support.
Renato Sales, a former deputy prosecutor for Mexico City, said Calderon's desire to expand federal policing powers to combat organized crime was parallel to the Bush administration's use of a secret wiretapping program to fight terrorism.
"Suddenly anyone suspected of organized crime is presumed guilty and treated as someone without any constitutional rights," said Sales, now a law professor at the Autonomous Technological Institute of Mexico. "And who will determine who is an organized crime suspect? The state will."
Federal lawmaker Cesar Octavio Camacho, president of the justice and human rights commission in the lower house of Congress, said he too worried about prosecutorial abuse.
"Although the proposal stems from the president's noble intention of efficiently fighting organized crime," he said, "the remedy seems worse than the problem."
sam.enriquez at latimes.com
Carlos Mart??nez and Cecilia S??nchez of The Times' Mexico City Bureau and Times staff writer Henry Weinstein in Los Angeles contributed to this report.

@_date: 2007-11-14 21:23:02
@_author: John Gilmore 
@_subject: Intelligence Official: Say Goodbye To Privacy  
Here's Dr. Donald MacLean Kerr, Jr's original speech that led to the
AP story about security, privacy, and anonymity.  It's more nuanced
than what the AP reported, but still basically wrong.  At least
there's somebody high up who's saying we don't have to trade privacy
for security.  (Instead he says we have to trade anonymity for
OK, Dr. Kerr, what's your address, birth date, and SSN?  Where do your
kids live?  What, don't you trust us?  You want all that info about
us, and trust is a two-way street.
What do we have to trade, to be secure from you and the rest of the
corrupt federal government -- which is working tomorrow on granting
itself retroactive impunity for crimes of high treason?  Why should I
trust *you* more than I trust the average illegal immigrant?  Those
immigrants aren't part of an organization designed to do mass murder
and get away with it; you are.
Dr. Kerr, read _The Transparent Society_ by David Brin.  He's thought
about it more than you have.  He thinks to be safe from tyranny, we'll
have to get rid of both privacy AND anonymity.  But this will be much
safer than losing privacy and anonymity AND being subject to tyrants.

@_date: 2007-11-21 11:57:27
@_author: John Gilmore 
@_subject: Wikileaks: NSA funding of academics 
Grant code 'MDA904' - National Security Agency
The NSA has pushed tens or hundreds of millions into the academy
through research grants using one particular grant code.  ...

@_date: 2007-10-23 02:21:59
@_author: John Gilmore 
@_subject: NSA solicited illegal Qwest mass wiretaps right after Bush inauguration 
Nacchio affects spy probe
His court filings point to government surveillance months before 9/11
By Andy Vuong
The Denver Post
Article Last Updated: 10/20/2007 11:38:08 PM MDT
  Extras
  Previously sealed documents filed by former Qwest chief executive Joe Nacchio in connection with his top-secret defense strategy, in which he argued that he was privy to classified information that led him to believe the company was in line to receive lucractive government contracts. The documents were released Wednesday at the request of The Denver Post.
  [Follow the link above to get the links to these court documents.  --gnu]
      * Read file 1, Sept. 29, 2006 (PDF, 46 pages).
      * Read file 2, Oct. 31, 2005 (PDF, 84 pages).
      * Read file 3, Jan. 4, 2007 (PDF, 10 pages).
      * Read file 4, Oct. 31, 2006 (PDF, 25 pages).
      * Read file 5, Jan. 22, 2007 (PDF, 3 pages).
      * Read file 6, Feb. 20, 2007 (PDF, 25 pages).
      * Read file 7, May 25, 2007 (PDF, 12 pages).
Recent revelations about former Qwest chief executive Joe Nacchio's classified-information defense, which went unheard during his insider-trading trial, are feeding the furor over the government's warrantless-wiretapping program.
Nacchio alleges the National Security Agency asked Qwest to participate in a program the phone company thought was illegal more than six months before the Sept. 11, 2001, terrorist attacks, according to court documents unsealed at the request of The Denver Post.
Nacchio also maintains that when he refused to participate, the government retaliated by not awarding lucrative contracts to Qwest.
Previously sealed transcripts released at the same time as the court documents indicate the government was prepared to counter Nacchio's claims.
Though specifics about the wiretapping program were redacted from the court documents, Nacchio's attorney Herbert Stern said in May 2006 that Nacchio rejected requests from the government for customers' phone records in fall 2001.
The recently unsealed documents push that time frame back to February 2001 and indicate the NSA may have also sought to monitor customers' Internet traffic and fax transmissions.
Nacchio's claims could affect President Bush's controversial efforts to grant legal immunity to large telecommunications companies such as AT&T, which has been sued in connection with the surveillance program.
"The Nacchio materials suggesting that the NSA had sought telco cooperation even before 9/11 undermines the primary argument for letting the phone companies off the hook, which is the claim that they were simply acting in good faith to help the president fight the terrorists after 9/11," said Kevin Bankston, a staff attorney for the Electronic Frontier Foundation, a civil-liberties group.
"The fact that these materials suggest that cooperation with the program was tied to the award of certain government contracts also contradicts their (phone companies') claims that they were simply acting in good faith to help fight the terrorists when it appears that they may have been motivated by financial concerns instead," Bankston said.
Up to this point, discussions on Capitol Hill over telecom immunity have focused on government surveillance efforts spurred by the Sept. 11 terrorist attacks.
"This is, sooner or later, going to be the stuff of congressional hearings because a new starting point has been established for this controversy. A new starting point seven months before 9/11," said Ron Suskind, author of "The One Percent Doctrine," which reported examples of how companies worked with the government in its fight against terrorism after Sept. 11.
"The idea that deals were getting cut between the government and telecom companies in secret in the early part of 2001 creates a whole new discussion as to intent, motivation and goals of the government," Suskind said.
Last week, Rep. John Conyers Jr., D-Mich., chairman of the House Judiciary Committee, asked federal intelligence officials for more information about Nacchio's allegations.
"The extent to which this is true could shed light on the efficacy of this program and raise questions about the reasons behind its implementation," Conyers wrote on his blog.
For his part, Nacchio wanted to introduce the claims to show he didn't sell Qwest stock illegally in early 2001. The government alleged Nacchio dumped Qwest stock because he had inside information that the Denver company's financial health was deteriorating. He was convicted on 19 counts of insider trading in April after a month-long trial and sentenced to six years in prison.
He remains free on $2 million bond pending his appeal, which, among other charges, is challenging rulings U.S. District Judge Edward Nottingham made related to the classified-information defense.
Nacchio has maintained he was upbeat about Qwest because he had top-secret information that the company would receive hundreds of millions in government contracts. But Nacchio didn't call any witnesses to back up that contention after Nottingham denied his requests to present evidence about the alleged retaliation.
Nacchio's connections to top-secret agencies date back to late 1997, according to court documents filed in 2000 and early 2001 and unsealed this month.
The first meeting Nacchio had with officials from a clandestine agency - including a three-star lieutenant general - was at Qwest's offices in Denver. The officials sought to use Qwest's fiber-optic communications network for government purposes.
The agency issued a request for information after the meeting and "quickly concluded that only Qwest had the capability to fulfill the contract requirements." It eventually wanted Qwest to extend its European network to the Middle East.
Qwest's work was "sufficiently important to (blacked out) that the agency would constantly monitor Qwest's financial health, particularly after the dot-com bubble burst." The agency would call Qwest whenever its stock fluctuated, and even voiced concerns to the company about a potential takeover bid by Deutsche Telekom, Nacchio alleges in the court documents.
During 2000 and early 2001, Nacchio routinely met with clandestine agencies to discuss how to protect the U.S. government's systems from cyber-warfare.
In September 2000, an "Army customer" wanted help from Qwest.
On Feb. 27, 2001, Nacchio met with NSA officials in Fort Meade, Md., to discuss a contract called "Groundbreaker."
That contract wasn't classified, but Nacchio contends NSA officials wanted Qwest to participate in another program. Nacchio said "it was a legal issue and that they could not do something their general counsel told them not to do. ... Nacchio projected that he might do it if they could find a way to do it legally," according to a recounting of the meeting by James Payne, former head of Qwest's government division.
About the same time, AT&T was working on a project with the NSA dubbed "Pioneer-Groundbreaker." According to allegations in a lawsuit filed against AT&T and other large telecom companies, "Pioneer- Groundbreaker" called for the construction of a data center that would allow the NSA to tap into AT&T's network and monitor phone calls, fax transmissions and Internet activity.
The House Committee on Energy and Commerce this month launched an investigation into the warrantless- wiretapping program.
In response to questions from the committee, Verizon said that since 2005 it has provided customer information to federal authorities in hundreds of emergency cases without court orders. The company said the information it has provided includes call records, IP addresses and credit card and bank account numbers.
Andy Vuong: 303-954-1209 or avuong at denverpost.com

@_date: 2008-04-25 10:48:29
@_author: John Gilmore 
@_subject: "Designing and implementing malicious hardware"  
It would be very interesting to examine some of the DES Cracker gate
array chips with these tools.  Though the chips worked great in
simulation, and each search engine came from exactly the same VHDL
source code, some number of the 24 search engines on each manufactured
chip have subtle errors that corrupt some part of the DES calculation,
causing <1% of both false-negatives and false-positives when doing
brute force key search.  Particular patterns would rapidly fail.  Each
chip also failed its standard test vectors after manufacturing.  We
accepted and used the chips anyway, working around the issues by
writing fault-tolerant software, because we couldn't afford to do
another 6- to 8-week chip fabrication turnaround (plus the time needed
to characterize and fix the problem).
Was this subtle error an introduced malicious change?  A bug in the
chip compiler?  Or was it caused by a poorly characterized process
variation in the factory?  We never found out.
A flakey DES cracker that worked on some keys and not on others would
have made less of a public policy impact.  We might never have
announced it at all, if it was failing in ways we couldn't understand
and couldn't compensate for.  Based on unattributable stories we heard
from other cryptographers, we had been concerned from that if we did
the work publicly, NSA would figure out a way to screw us (by leaning
on the chip vendor through some executive-level contact: "Do this for
the good of your country", then they'd refuse to make our chips, or
break the chips somehow).
Luckily, since we had a mix of working and non-working engines, we
could adjust the software to use all engines in the first pass, but
then only use known-good search engines to re-search blocks of keys
that had been searched by known-flakey search engines.  The result was
a cracker that works at its designed speed for most keys and
plain/ciphertext combinations, but which works about 40% slower on <1%
of such combinations.
I suspect error rather than malice here, but this raises an
interesting issue.  Even if you find a chip design bug that lets
people break in, can it be designed to LOOK LIKE error rather than
malice?  Clearly the data bus comparator and reserved cache line thing
from LEET would show up as malice, if ever discovered.  But there are
subtler and perhaps harder to exploit changes that would leave much
lighter traces of deliberate sabotage.
PS:  Lots of security bugs in software look like error, yet we know
that some are malicious (like the one briefly introduced into the Linux
kernel sources by breaking in to one of the machines holding the source
tree):

@_date: 2008-07-21 14:16:04
@_author: John Gilmore 
@_subject: WPost: Cybersecurity Will Take A Big Bite of the Budget  
[News report below.]
This highly classified little-publicized multi-billion dollar "vague"
program to secure Federal computers seems doomed to failure.  People
like you and I, in the unclassified private sector, design and build
and program all those computers and networks.
But of course we've never heard of this initiative.  And we probably
don't share its goals.
NSA's occasional public efforts to secure the civilian infrastructure
have been somewhat interesting.  Not that they've succeeded: they
crippled DES, wouldn't admit it was broken, and tried to force us all
to use it; the IPSEC they designed was painfully complex, impossible
to administer, easy to penetrate, and wouldn't scale; the export
controls they championed torpedoed civilian efforts to secure
ANYTHING; and Secure Linux seems to be no more secure than any other
Linux.  Do we know of *any* honest and successful NSA effort to raise
the integrity and security of the public infrastructure (even at the
expense of their ability to illegally tap it)?
Now that NSA, the President, and Congress have gone totally to the
Dark Side, we'd better assume that any such initiative does not have
the public's best interests at heart.  The theory is that the public's
computers will be easy for the government to break into, while
Wiretapper-General McConnell can shield every unconstitutional thing
he does from the prying eyes of the public and the courts?  It'd be
better for private-sector engineers to follow our own muses, rather
than become the rats following government-contractor Pied Pipers into
a totalitarian sewer.
Let's guess why they would classify this effort at all.  For "security
through obscurity"?  So that "foreigners" won't find out how to secure
their own computers against NSA intrusions (ahem, foreigners build ALL
our computers)?  Merely to hide their own incompetence?  Or because
the effort would be quickly identified as malfeasance, like trying to
impose a national ID system and routine suspicionless checkpoint
searches on a free people?
Forwarded-By: Melissa Ngo Cybersecurity Will Take A Big Bite of the Budget
By Walter Pincus
Monday, July 21, 2008; A13
President Bush's single largest request for funds and "most important  initiative" in the fiscal 2009 intelligence budget is for the  Comprehensive National Cybersecurity Initiative, a little publicized  but massive program whose details "remain vague and thus open to  question," according to the House Permanent Select Committee on  A highly classified, multiyear, multibillion-dollar project, CNCI --  or "Cyber Initiative" -- is designed to develop a plan to secure  government computer systems against foreign and domestic intruders and  prepare for future threats. Any initial plan can later be expanded to  cover sensitive civilian systems to protect financial, commercial and  other vital infrastructure data.
"It is no longer sufficient for the U.S. Government to discover cyber  intrusions in its networks, clean up the damage, and take legal or  political steps to deter further intrusions," Director of National  Intelligence Mike McConnell noted in a February 2008 threat  assessment. "We must take proactive measures to detect and prevent  intrusions from whatever source, as they happen, and before they can  do significant damage." His conclusions echoed those of a 2007  interagency review that led to CNCI's creation.
During debate on the intelligence authorization bill last week, Rep.  Jim Langevin (D-R.I.), a member of the House intelligence committee  and chairman of the Homeland Security subcommittee on emerging  threats, described cybersecurity as "a real and growing threat that  the federal government has been slow in addressing."
Without specifying funding figures, which are classified, Langevin  said the panel approved 90 percent of the funds requested for CNCI but  warned that the committee "does not intend to write the administration  a blank check."
The committee's report recognized that as the initiative develops, "it  will be imperative that the government also take into account the  interests and concerns of private citizens, the U.S. information  technology industry, and other elements of the private sector."
Such a public-private partnership will be "unlike any model that  currently exists," said the committee, which recommended a White House  study leading toward establishment of an oversight panel of lawmakers,  executive branch officials and private-sector representatives. The  panel would review the intelligence community's development of the  The committee said it expects the policy debates over the initiative  to extend into the next administration, and major presidential  candidates have addressed the issue.
On the same day the intelligence bill passed the House, Sen. Barack  Obama (D-Ill.) told an audience that, "as president, I'll make  cybersecurity the top priority that it should be in the 21st century."  He vowed to appoint a national cyber adviser to coordinate policy to  secure information -- "from the networks that power the federal  government, to the networks that you use in your personal lives."
In a July 1 speech, Sen. John McCain (R-Ariz.) addressed  cybersecurity, as well. "To protect our energy supply, air and rail  transport, banking and financial services, we need to invest far more  in the federal task of cyber security," he said. Neither Obama nor  McCain mentioned the cybersecurity initiative underway.
National security and intelligence reporter Walter Pincus pores over  the speeches, reports, transcripts and other documents that flood  Washington and every week uncovers the fine print that rarely makes  headlines -- but should. If you have any items that fit the bill,  please send them to fineprint at washpost.com.

@_date: 2008-06-12 12:35:49
@_author: John Gilmore 
@_subject: Why doesn't Sun release the crypto module of the OpenSPARC? Crypto export restrictions  
A hardware "design" is not hardware.  Only a naive parsing of the
words would treat it so.  A software design is not treated like
software; you are free to write about how ATM machine crypto is
designed, even if you can't export ATM machine crypto software without
a license (because it's proprietary and not mass-market).
A hardware design is a lot like software.  It's human written and
human readable, it's trivial to reproduce, it's compiled automatically
into something that can execute, and if you write it into hardware,
then it does something.
The court case that EFF won against the export controls was won on
those grounds: the government can't suppress the publication of
human-written and human-readable text, on the grounds that somebody
somewhere might put it into a machine that does things the government
doesn't like.
Sun may be chicken on the point, and the government did a sneaky trick
to technically avoid having a Ninth Circuit precedent set on the
topic, but a similar precedent was set by Peter Junger's case in
another circuit.  I think Sun would be well within its rights to ship
VHDL or Verilog source code that implements crypto under an open
source license.  And I'd be happy to point them at good lawyers who'd
be happy to be paid to render a more definitive opinion.

@_date: 2008-10-16 15:24:06
@_author: John Gilmore 
@_subject: Chip-and-pin card reader supply-chain subversion 'has netted millions from British shoppers'  
[British shoppers were promised high security by switching from credit
cards to cards that have a chip in them and require that a PIN be entered
for each transaction.  That was the reason for changing everything over,
at high cost in both money and inconvenience to shops and shoppers.  Perhaps
chip-and-pin HAS reduced overall fraud -- but check out this elaborate scheme that beat their security for tens of millions of UK pounds.  Now, why is this being announced by the US National Counter-
intelligence Executive, Joel Brenner?  Because none of the banks or
stores is willing to admit it?  Still, why publicize it at all?  I
find his quote very telling: "Previously only a nation state's
intelligence service would have been capable of pulling off this type
of operation."  How would he know this?  Which nation-states have done
similar types of operation, and why isn't he telling the public about
THEM instead of about these other criminals?
I've long suspected that NSA's (still secret) budget (approved by a
tiny number of manipulated Congressmen) has been, uh, augmented, by
its ability to manipulate financial markets using inside information
obtained from domestic and global mass wiretaps.  You don't suppose
NSA is behind the recent market volatility, do you?  It's easiest to
skim off billions when trillions are hurriedly sloshing around in a
panic.  --gnu]
Forwarded-By: Kurt Albershardt Clever (and a tad frightening)

@_date: 2008-10-26 00:27:46
@_author: John Gilmore 
@_subject: data rape once more, with feeling.  
"Usability research" about how to track web users?  How Google-like.
Can't you just dump a 25-year cookie on them from twelve different directions, and be done with it?
Explicitly ignoring the trust model between the end users and the RP,
and the trust between the end users and the IDP.  Why should end users
trust your web site?  Why should they trust an IDP like Google?
It's not that every website that requires a login is a privacy swamp.
But the big ones pretty much all are, and those are the ones who want
to impose this new model without bothering the end user's little head
about whether he should trust them.
And if every little wiki that just uses logins to slightly limit spam
today, began using "federated identity", then ALL of them would become
privacy swamps.
Let's see an example of how you're automating how the USER might
require the SITE to agree to a Terms of Service.  Doesn't seem to be
part of the model, which is that the SITE has something valuable it
needs lawyers to protect, while the USER is just an
eyeball-with-attached-wallet to be sold to the highest bidder.
I wonder why not!  Perhaps they do not want to be tracked, numbered,
wiretapped, monitored, herded, logged, datamined, folded, spindled,
and mutilated.  Perhaps they just want to look at a web site without
tying their reading habits to their social security number and
their medical records.
Similar percentages describe how many people lie through their teeth
to get into random websites.  So half won't even login, half of those
will lie like hell; a quarter of the people either think you're
trustworthy, or are too stupid to care.  Which fraction is "federated
identity" aimed at?  Catching the liars, i.e. fencing in the people
who actually take care to protect their privacy?  Yeah, those are
the guys this community wants you to screw as hard as you can. :-(
That's an interesting assumption.  Why would you assume that AOL would
give users the choice?  AOL is not famous for choice.  Wouldn't AOL
just read the user's 25-year AOL cookie, and redirect the browser back
to buy.com with full account information supplied, without any
interaction with the user at all?  AOL could probably even charge the
RP a few bucks for doing so.  How simple.  How evil.  Franchising your
privacy violations.
You can pretty well guarantee that RP websites will somehow decline to
trust any IDP that provides privacy to the end user -- like
mailinator.com, for example.  A few web sites that "send you an email
to verify you are who you say you are" already blacklist mailinator,
though it's usually easy to bypass the blacklist by using one of its
alternative domain names.

@_date: 2009-08-18 13:14:25
@_author: John Gilmore 
@_subject: 2 serving time in UK prisons for refusing to decrypt on demand 
[But we don't know who they are!   --gnu]
Two convicted for refusal to decrypt data
Up to five years in jail after landmark prosecutions
By Chris Williams
Posted in Policing, 11th August 2009 13:17 GMT
Two people have been successfully prosecuted for refusing to provide authorities with their encryption keys, resulting in landmark convictions that may have carried jail sentences of up to five years.
The government said today it does not know their fate.
The power to force people to unscramble their data was granted to authorities in October 2007. Between 1 April, 2008 and 31 March this year the first two convictions were obtained.
The disclosure was made by Sir Christopher Rose, the government's Chief Surveillance Commissioner, in his recent annual report.
The former High Court judge did not provide details of the crimes being investigated in the case of either individual - neither of whom were necessarily suspects - nor of the sentences they received.
The Crown Prosecution Service said it was unable to track down information on the legal milestones without the defendants' names.
Failure to comply with a section 49 notice carries a sentence of up to two years jail plus fines. Failure to comply during a national security investigation carries up to five years jail.
Sir Christopher reported that all of the 15 section 49 notices served over the year - including the two that resulted in convictions - were in "counter terrorism, child indecency and domestic extremism" cases.
The Register has established that the woman served with the first section 49 notice, as part of an animal rights extremism investigation, was not one of those convicted for failing to comply. She was later convicted and jailed on blackmail charges.
Of the 15 individuals served, 11 did not comply with the notices. Of the 11, seven were charged and two convicted. Sir Christopher did not report whether prosecutions failed or are pending against the five charged but not convicted in the period covered by his report.
To obtain a section 49 notice, police forces must first apply to the National Technical Assistance Centre (NTAC). Although its web presence suggests NTAC is part of the Home Office's Office of Security and Counter Terrorism, it is in fact located at the government's secretive Cheltenham code breaking centre, GCHQ.
GCHQ didn't immediately respond to a request for further information on the convictions. The Home Office said NTAC does not know the outcomes of the notices it approves.
NTAC approved a total of 26 applications for a section 49 notice during the period covered by the Chief Surveillance Commissioner's report, which does not say if any applications were refused. The judicial permission necessary to serve the notices was then sought in 17 cases. Judges did not refuse permission in any case.
One police force obtained and served a section 49 notice without NTAC approval while acting on "incorrect information from the Police National Legal Database", according to Sir Christopher. The action was dropped before it reached court.
Readers with further information about the convictions can contact the reporter in confidence here.

@_date: 2009-08-20 01:26:29
@_author: John Gilmore 
@_subject: Certainty  
I tried telling this to Linus within a few weeks of the design, while
he was still writing git.  He rejected the advice.  Perhaps a
delegation of cryptographers should approach him -- before it's too
His biggest argument was that the important git trees would be "off-net"
and would not depend on public trees.  I think git is getting enough
use (e.g. by thousands of development projects other than the Linux
kernel) that those assumptions are probably no longer valid.
His secondary argument was that git only uses the hash as a
collision-free oracle, not a cryptographic hash.  But that's exactly
the problem.  If malicious people can make his oracle produce
collisions, other parts of the git code will make false assumptions
that can be exploited.
His final argument is the same one I heard NSA make to Diffie and
Hellman about DES in 1976: "the crypto will never be the weakest link
in the system, so it doesn't really have to be that strong".  That
argument was wrong then and it's wrong now.  The cost of using a
strong cryptosystem isn't significantly greater than the cost of using
a weak cryptosystem; and cracking the crypto HAS become the weakest
link in the overall security of many systems (CSS is an obvious one).
  It's interesting watching git evolve.  I have one comment, which is
that the code and the contributors are throwing around the term "SHA1
hash" a lot.  They shouldn't.  SHA1 has been broken; it's possible to
generate two different blobs that hash to the same SHA1 hash.  (MD5
has totally failed; there's a one-machine one-day crack.  SHA1 is
still *hard* to crack.)  But as Jon Callas and Bruce Schneier said:
"Attacks always get better; they never get worse.  It's time to walk,
but not run, to the fire exits.  You don't see smoke, but the fire
alarms have gone off.  It's time for us all to migrate away from
SHA-1."  See the summary with bibliography at:
  Since we don't have a reliable long-term hash function today, you'll
have to change hash functions a few years out.  Some foresight now
will save much later pain in keeping big trees like the kernel secure.
Either that, or you'll want to re-examine git's security assumptions
now: what are the implications if multiple different blobs can be
intentionally generated that have the same hash?  My initial guess is
that changing hash functions will be easier than making git work in
the presence of unreliable hashing.
In the git sources, you'll need to install a better hash function when
one is invented.  For now, just make sure the code and the
repositories are modular -- they don't care what hash function is in
use.  Whether that means making a single git repository able to use
several hash functions, or merely making it possible to have one
repository that uses SHA1 and another that uses some future
WonderHash, is a system design decision for you and the git
contributors to make.  The simplest case -- copying a repository with
one hash function into a new repository using a different hash
function -- will change not only all the hashes, but also the contents
of objects that use hash values to point to other objects.  If any of
those objects are signed (e.g. by PGP keys) then those signatures will
not be valid in the new copy.
Adding support now for SHA256 as well as SHA1 would make it likely
that at least git has no wired-in dependencies on the *names* or
*lengths* of hashes, and let you explore the system level issues.  (I
wouldn't build in the assumption that each different hash function
produces a different length output, either, though these two happen
As to your SHA1 concerns:
Actually, even the theoretical breaking has not been proven for a pre-existing SHA1 hash (ie you need to control both the starting point for it), and more importantly, git really uses the SHA1 has a _hash_, not necessarily as a cryptographically secure one.
IOW, security doesn't actually depend on the hash being cryptographic, and
all git really wants is to avoid collisions, ie it wants it to hash the
contents well. That, sha1 definitely does, and even an md5sum would
suffice (but having 160 bits instead of "just" 128 obviously adds to the
space, so that's always a bonus).
Of course, the fact that sha1 is also very expensive to try to fool is a big bonus, since it means that it's just another layer on the real security model. But the _real_ security comes from the fact that git is distributed, which means that a developer should never actually use a public tree for his development.
For example, I've got two separate firewall layers (and a NAT) in between me and the internet, and my personal tree is on that machine. I never actually trust or use the external trees - I just push the result to them. This is something you cannot do with a centralized SCM server like SVN or
other traditional crud. A centralized one obviously has to be accessible
to all the developers, which means that it's forced to be open enough to
be much more easily attackable, and also means that there is a single point of failure also from a security standpoint. In contrast, even if somebody were to compromise my machine, that does _not_ automatically compromise the trees of other developers. They'd still have all the pristine objects, and never even fetch an object from me that has the same name (ie sha1 hash) as one they already have.
In other words, to really break a git archive, you need to
 - be able to replace an existing SHA1 hash'ed object with one that hashes
   to the same thing (_not_ the breakage that has been  shown to be    possible already)
 - the replacement has to still honor all the other git consistency checks    (even "blob" objects have them: they need to have a valid header with a
   valid length, so it's not sufficient to just find another object that    hashes to the right thing, you have to find an object with a valid    header that hashes to the right thing)
 - you have to break in to _all_ archives that already have that object    and replace it quietly enough that nobody notices.
Quite frankly, it's not worth worrying about. It's a hell of a lot easier
to just break a source archive with other means (ie pay a developer ten
million dollars to just insert the back door you want inserted).

@_date: 2009-01-25 14:40:45
@_author: John Gilmore 
@_subject: Proof of Work -> atmospheric carbon 
Computers are already designed to consume much less electricity when
idle than when running full tilt.  This trend will continue and
extend; some modern chips throttle down to zero MHz and virtually zero
watts at idle, waking automatically at the next interrupt.
The last thing we need is to deploy a system designed to burn all
available cycles, consuming electricity and generating carbon dioxide,
all over the Internet, in order to produce small amounts of bitbux to
get emails or spams through.
Can't we just convert actual money in a bank account into bitbux --
cheaply and without a carbon tax?  Please?

@_date: 2009-01-29 13:22:37
@_author: John Gilmore 
@_subject: full-disk subversion standards released  
If it comes from the "Trusted Computing Group", you can pretty much
assume that it will make your computer *less* trustworthy.  Their idea
of a trusted computer is one that random unrelated third parties can
trust to subvert the will of the computer's owner.

@_date: 2009-01-30 16:08:07
@_author: John Gilmore 
@_subject: full-disk subversion standards released  
Given the charter of TCG, to produce DRM standards, it's pretty clear
what activity their tool is designed to be used for.
The theory that we should build "good and useful" tools capable of
monopoly and totalitarianism, but use social mechanisms to prevent
them from being used for that purpose, strikes me as naive.  Had you
not noticed obvious indications like the corruption of the Executive
Branch by NSA, RIAA and MPAA (including the shiny new president), the
concurrence of the Legislative Branch in that corruption, and the
toothlessness of the States and the Judicial Branch in failing to
actually reign in major federal constitutional violations?
Yes, I'm analogizing DRM to wiretaps and jiggered voting machines.
But isn't DRM like a wiretap deep inside your computer -- a foreign
agent that spies on you and reports back whatever it chooses, against
your will?  Worse, it's like a man-in-the-middle attack, buried inside
your computer.  If Hollywood succeeded in injecting DRM into all our
infrastructure, who among us would seriously believe the government
would not muscle its way in and start also using the DRM capabilities
against the citizens?  The Four Horsemen of the Infopocalypse are
alive and well.  Are you one of those guys in *favor* of sex offenders
being allowed free access to children on the Internet, buddy?  It's
so simple, everyone will just prove they aren't a sex offender before
being granted access.  It's just like getting on a plane.
(TCG has excised all mention of DRM from recent publications -- but I
have the original ones, which had DRM examples explaining the
motivation for why they were doing this work.  I'll append one such
example, for those who can't readily search the archives back to 2003.
Skip down to "TCPA" in the body below.)
I used to run a commercial time-sharing mainframe in the 1970's.
Jerrold's wrong.  The owner of the machine has desires (what he calls
"demands") different than those of the users.
The users, for example, want to be charged fairly; the owner may not.
We charged every user for their CPU time, but only for the fraction that
they actually used.  In a given second, we might charge eight users
for different parts of that fraction.
Suppose we charged those eight users amounts that added up to 1.3
seconds?  How would they know?  We'd increase our prices by 30%, in
effect, by charging for 1.3 seconds of CPU for every one second that
was really expended.  Each user would just assume that they'd gotten a
larger fraction of the CPU than they expected.  If we were tricky
enough, we'd do this in a way that never charged a single user for
more than one second per second.  Two users would then have to collude
to notice that they together had been charged for more than a second
per second.
(Our CPU pricing was actually hard to manage as we shifted the load
among different mainframes that ran different applications at
different multiples of the speed of the previous mainframe.  E.g. our
Amdahl 470/V6 price for a CPU second might be 1.78x the price on an
IBM 370/158.  A user's bill might go up or down from running the same
calculation on the same data, based on whether their instruction
sequences ran more efficiently or less efficiently than average on the
new CPU.  And of course if our changed "average" price was slightly
different than the actual CPU performance, this provided a way to
cheat on our prices.
Our CPU accounting also changed when we improved the OS's timer
management, so it could record finer fractions of seconds.  On average,
this made the system fairer.  But your application might suffer, if its
pattern of context switches had been undercharged by the old algorithm.)
The users had to trust us to keep our accounting and pricing fair.
System security mechanisms that kept one user's files from access by
another could not do this.  It required actual trust, since the users
didn't have access to the data required to check up on us (our entire
billing logs, and our accounting software).
TCPA is being built specifically at the behest of Hollywood.  It is
built around protecting "content" from "subscribers" for the benefit
of a "service provider".  I know this because I read, and kept, all
the early public design documents, such as the white paper
  (This is no longer available from the web site, but I have a copy.)
It says, on page 7-8:
  The following usage scenarios briefly illustrate the benefits of TCPA
  compliance.
  Scenario I: Remote Attestation
  TCPA remote attestation allows an application (the "challenger") to
  trust a remote platform. This trust is built by obtaining integrity
  metrics for the remote platform, securely storing these metrics and
  then ensuring that the reporting of the metrics is secure.
  For example, before making content available to a subscriber, it is
  likely that a service provider will need to know that the remote
  platform is trustworthy. The service provider's platform (the
  "challenger") queries the remote platform. During system boot, the
  challenged platform creates a cryptographic hash of the system BIOS,
  using an algorithm to create a statistically unique identifier for the
  platform. The integrity metrics are then stored.
  When it receives the query from the challenger, the remote platform
  responds by digitally signing and then sending the integrity
  metrics. The digital signature prevents tampering and allows the
  challenger to verify the signature. If the signature is verified, the
  challenger can then determine whether the identity metrics are
  trustworthy. If so, the challenger, in this case the service provider,
  can then deliver the content. It is important to note that the TCPA
  process does not make judgments regarding the integrity metrics. It
  merely reports the metrics and lets the challenger make the final
  decision regarding the trustworthiness of the remote platform.
They eventually censored out all the sample application scenarios like
DRM'd online music, and ramped up the level of jargon significantly,
so that nobody reading it can tell what it's for any more.  Now all
the documents available at that site go on for pages and pages saying
things like "FIA_UAU.1 Timing of authentication. Hierarchical to: No
other components. FIA_UAU.1.1 The TSF shall allow access to data and
keys where entity owner has given the 'world' access based on the
value of TCPA_AUTH_DATA_USAGE; access to the following commands:
TPM_SelfTestFull, TPM_ContinueSelfTest, TPM_GetTestResult,
TPM_PcrRead, TPM_DirRead, and TPM_EvictKey on behalf of the user to be
performed before the user is authenticated."
But the historical record is clear that DRM was "Usage Scenario for TCPA.
Now, back to Hollywood.  If you have not read "This Business of Music"
(a thick book on how musicians can arm themselves with knowledge to
get slightly less screwed by the record industry -- including sample
contracts and explanations of the impact and history of each
provision), you won't know the long history of why Hollywood can be
trusted only to cheat everyone they deal with.
A music-industry contract equivalent to charging for 30% more seconds
than you deliver, is the provision for "breakage".  No artist gets
paid for more than 90% of the albums that the record company sells,
because in the days of shellac records, about 10% of them would break
in shipping.  That problem largely went away with vinyl records, and
went even further away with CDs.  Today's actual breakage is way under
1%.  But record companies won't sign a contract that pays the artist
for more than 90% of the albums shipped on CD.  That 10% underpayment
of musicians goes straight back to the record company's profits.
Their DRM software will cheat users the same way -- or a different way -- or a hundred different ways.  And TCPA will make it un-auditable
by us.

@_date: 2009-07-23 19:24:41
@_author: John Gilmore 
@_subject: Fast MAC algorithms?  
This is my experience, too.  And I would add "and lots of packets".
The only crypto "overhead" that really mattered in a real application
was the number of round-trip times it took to negotiate protocols and
keys.  Crypto's CPU time is very very seldom the limiting factor in
real end-user application performance.
I have never seen a network card or chip whose "advanced capabilities"
included the ability to speed up TCP.  Most such "advanced" designs
actually ran slower than merely doing TCP in the Linux kernel using an
uncomplicated chip.  I saw a Patent Office procurement of Suns in the
'80s that demanded these slow "TCP offload" boards (I had to write the
bootstrap code for the project) even though the motherboard came with
an Ethernet chip and software stack that could run TCP *at wire speed*
all day and night -- for free.  The super whizzo board couldn't even
send back-to-back packets, as I recall.  Some government contractor
had added the "TCP offload" requirement, presumably to inflate the
price that they were adding a percentage markup to.
As a crypto-relevant aside, last year I looked at using the "crypto
offload" engine in the AMD Geode cpu chip to speed up Linux crypto
operations in the OLPC.  There was even a nice driver for it.
Summary: useless.  It had been designed by somebody who had no idea of
the architecture of modern software.  The crypto engine used DMA "for
speed", used physical rather than virtual addresses, and stored the
keys internally in its registers -- so it couldn't work with virtual
memory, and couldn't conveniently be shared between two different
processes.  It was SO much faster to do your crypto "by hand" in a
shared library in a user process, than to cross into the kernel, copy
the data to be in contiguous memory locations (or manually translate
the addresses and lock down those pages into physical memory), copy
the keys and IVs into the accelerator, do the crypto, copy the results
back into virtual memory, and reschedule the user process.  In typical
applications (which don't always use the same key) you'd need to do
this dance once for every block encrypted, or perhaps if you were
lucky, for every packet.  Even kernel crypto wasn't worth doing
through the thing.  And the software libraries were not only faster,
they were also portable, running on anything, not just one obsolete
Hardware guys are just jerking off unless they spend a lot of time
with software guys AT THE DESIGN STAGE before they lay out a single
gate.  One stupid design decision can take away all the potential gain.
Every TCP offloader I've seen has had at least one.

@_date: 2009-07-27 04:58:52
@_author: John Gilmore 
@_subject: The latest Flash vulnerability and monoculture  
For Adobe Flash, there are three separate implementations -- Adobe's
proprietary one, GNU Gnash, and Swfdec.
Gnash is focused on long-term reliability and compatability, like the
rest of the GNU programs.  Its browser plugin executes the flash
interpreter in a separate process (which draws in the browser's
subwindow).  It can use either gstreamer or ffmpeg to play video or
audio.  This summer, the development focus is on implementing Flash
10's new class library (they redid it all).
Swfdec is focused on playing popular video sites well (sooner than
gnash).  They share some common regression-testing infrastructure, but
the implementations came from different code bases.

@_date: 2009-06-03 14:53:53
@_author: John Gilmore 
@_subject: How to wiretap or identify a GSM phone - and enable the masses 
David Burgess, a software/radio engineer formerly employed in building
GSM-tapping equipment, has turned his efforts to publicly implementing
the GSM standards in free software under GPLv3.  He hopes to provide
low-cost GSM communication service to billions in underserved regions
of the world.  He also hopes to demystify the cellular networks for
a generation of hackers.
His OpenBTS software builds on the GNU Radio framework and the USRP
computer/radio interfaces to implement a fully functional GSM network
base station, making voice and SMS calls with ordinary GSM handsets,
and back-hauling via VoIP networks.  Early code was tested at Burning
Man last year, and he hopes to provide free communcation service to BM
participants this year (as well as doing some testing in field
conditions under serious load).
His understanding of the GSM protocols comes from reading the published
standards documents, which are written in bureaucratese but can be
decrypted without a secret key.  However, a former customer of his
has been suing him for alleged disclosure of trade secrets, claiming
that either the GSM protocol or perhaps the way to wiretap a GSM phone
is secret (the published court documents make vague allegations, as
David's blog, "The OpenBTS Chronicles" has a variety of interesting
posts, one of which links to a German patent on an IMSI-catcher which
lets wiretappers force a phone to identify itself, and to a UK High
Court decision that upholds it (and also reveals a Nokia patent on how
to do a man-in-the-middle attack on a GSM phone).  Clearly the things
revealed in these documents are not trade secrets.  But they may be of
interest to this list.
I also found that David's posting on "The Value of Knowing How Stuff
Works" struck a chord with me.

@_date: 2009-03-03 11:53:24
@_author: John Gilmore 
@_subject: Activation protocol for car-stopping devices 
This is a protocol designed for nasty guys who want to steal your car,
which would forcibly stop the car regardless of the wishes of the
driver, remotely from anywhere on the Internet?  And it's mandated by
the government?
These are not "tracking devices", as your subject line said; they
actively intervene in driving -- much more dangerous.
As usual, it sounds like a great tool when used responsibly -- against
stolen cars, though it will probably cause collisions, which could
hardly be called "accidents" since they are easily foreseeable.  And
it's a terrible tool when used any other way (by criminals against cop
cars, for example; or by Bulgarian virus authors against random cars;
or by breaking into the DENATRAN and stealing and posting all the
secrets; or by an invading army).
It reminds me of the RFID passport design process: One entity figures
out what would make ITS life easier (reading your passport while
you're in line at the border), mandates a change, and ignores the
entire effects on the rest of society that result.
Why would you limit anything to 64 bits, or think it's OK that with 5
days of calculation *anyone* could do this to your mother's or
daughter's car?
Shouldn't tracking or disabling the car require the active cooperation
of the car's owner, e.g. by the owner supplying a secret known only to
them, and not recorded in a database anywhere (in the government, at
the dealer, etc)?  That way, if the protocol is actually secure, most
of the evil ways to use it AGAINST the owner would be eliminated.

@_date: 2009-03-03 17:05:32
@_author: John Gilmore 
@_subject: Judge orders defendant to decrypt PGP-protected laptop  
Balls.  This is a straight end-run attempt around the Fifth Amendment.
The cops initially demanded a court order making him reveal his
password -- then modified their stance on appeal after they lost.  So
he can't be forced to reveal it, but "on a technicality" he can be
forced to produce the same effect as revealing it?  Just how broad is
this technicality, and how does it get to override a personal
constitutional right?
If the cops bust down your door and you foolishly left your computer
turned on, are they entitled to make you reveal your encryption
passwords anytime later, because your encrypted drive was accessible
when they ran in screaming at your family and shooting your dog?
Suppose they looked it over and typed a few things to the screen?
Suppose they didn't?  Suppose they used a fancy power-transfer plug to
keep it running as they walked it out the door, but they tripped and
dropped it and it powered off?  That's a technicality, isn't it?
Don't forget, this is a nuisance case.  It's about a harmless Canadian
citizen who's a permanent US resident, who crossed the Canadian border
with his laptop.  A guy smart enough to encrypt his drive.  On the
drive, among other things, was a few thousand porn images downloaded
from the net.  Legal porn.  The border guards, who had no business
even looking at his laptop's contents, trolled around in it until they
found some tiny fraction of the images that (they allege) contained
underage models.  (How would *he* know the ages of the models in
random online porn?  Guess he'd better just store no porn at all,
whether or not porn is legal.  That's the effect that the bluenoses
who passed the "child porn" laws want, after all.)  That's the "crime"
being prosecuted here.  This isn't the Four Horsemen's
torture-the-terrorist-for-the-password hostage situation where lives
are at stake and the seconds are ticking away.  This is a pointless
search containing the only evidence of a meaningless censorship
non-crime.  If the feds can force you to reveal your password in this
hick sideshow, they can force it anytime.
Suppose the guy had powered off his laptop rather than merely
foolishly suspending it.  If the border guards had DRAM key recovery
tools that could find a key in the powered-down RAM, but then lost
the key or it stopped working, would you think he should later be
forced to reveal his password?
Suppose they merely possessed DRAM key recovery software, but never
deployed it?  Hey, we claim that you crossed the border with that key
in decaying RAM; fork over that password, buddy!
Don't give them an inch, they'll take a mile.  Drug users can now not
safely own guns, despite the Second Amendment.  Not even guns locked
in safes in outbuildings, because the law passed against "using a gun
in a drug crime" has been expanded by cops and judges to penalize
"having a gun anywhere on the property even though it was never
touched", and even when the only drug crime was simple possession.
Five year mandatory minimum sentence enhancement.  (Don't expect NRA
to help -- their motto is "screw the criminals, leave us honest people
alone".  That's no good when everybody's a criminal, especially the
honest people like this guy, who had nothing to hide from the border
guards and helped them search his laptop.)
There is no such document as "an unencrypted version of the Z drive".
It does not exist.  It has never existed.  One could in theory be
created, but that would be the creation of a new document, not the
production of an existing one.  The existing one is encrypted, and
the feds already have it.
I'm still trying to figure out what the feds want in this case if the
guy complies.  They'll have a border guard testify that he saw a
picture with a young teen in it?  They'll show the jury a picture of a
young teen, but won't "authenticate" it as a picture that came off the
hard drive?  It can just be any random picture of a young teen, that
could've come from anywhere?  How will that contribute to prosecuting
this guy for child porn?
Maybe they're just bored from training themselves by viewing official
federal child porn images (that we're not allowed to see), or
endlessly searching gigabytes of useless stuff on laptops.  Instead
they want the thrill of setting a precedent that citizens have no
right to privacy in their encrypted hard drives.  Let's not help them
by declaring this guy's rights forfeit on a technicality.

@_date: 2009-03-10 10:53:47
@_author: John Gilmore 
@_subject: Chinese hackers break iTunes gift certificate algorithm 
Chinese hackers crack iTunes Store gift codes, sell certificates
By Charles Starrett
Senior Editor, iLounge
Published: Tuesday, March 10, 2009
A group of Chinese hackers has succeeded in cracking Apple?s algorithm  for encoding iTunes Store Gift Certificates, and are creating  discounted certificates using a key generator. Outdustry reports that  a number of the codes are available on the site Taobao, with $200  cards selling for as little as $2.60. The owner of the Taobao shop  offering the cards admitted that the codes are created using key  generators, and that he paid to use the hackers? service. He also said  that while the price of the codes has dropped steadily, store owners  make more money as the number of customers grows.

@_date: 2009-05-26 18:49:48
@_author: John Gilmore 
@_subject: consulting question.... (DRM) 
It's a little hard to help without knowing more about the situation.
I.e. is this a software company?  Hardware?  Music?  Movies?
Documents?  E-Books?  Is it trying to prevent access to something, or
the copying of something?  What's the something?  What's the threat
model?  Why is the company trying to do that?  Trying to restrain
customers?  Competitors?  Trying to build a cartel?  Being forced to
do it by a cartel?  Is their product embedded?  Online?  Hardware?
Software?  Battery powered?  Is it on a phone network?  On the
Internet?  On no network?  What country or countries does the company
operate in?  What jurisdictions hold its main customer bases?  How
much hassle will its customers take before they switch suppliers?
What kind of industry standards must the company adhere to?  What
other equipment or data formats do they have/want to interoperate
Most DRM is probably never cracked, because the product it's in
never gets popular enough that anyone talented wants to crack it.
If they only sell a thousand units, will they be happy?  Or do they
hope/plan/need to sell millions of units?
Most DRM exists to build a cartel -- to make an artificial monopoly --
not to prevent *customers* from copying things, but to prevent
*competitors* from being able to build compatible or interoperable
equipment.  This is largely because US reverse-engineering law makes
such a cartel unenforceable in court, unless you use DRM to make it.
Why should we bother?  Isn't it a great idea for DRM fanatics to
throw away their money?  More, more, please!  Bankrupt yourselves
and drive your customers away.  Please!
It's only the DRM fanatics whose installed bases of customers
are mentally locked-in despite the crappy user experience (like
the brainwashed hordes of Apple users, or the Microsoft victims)
who are troublesome.  In such cases, the community should
intervene on behalf of the users -- not to prevent the company
from wasting its time and money.

@_date: 2009-05-27 23:03:43
@_author: John Gilmore 
@_subject: consulting question.... (DRM)  
Many people have had the same idea before.  The "software license
manager" field is pretty full of little companies (and divisions of
big ones).  Your prospect might be able to find a niche in there
somewhere, if they study their competition to see what's missing and
how they can build up an edge.  But customers tend to hate software
that comes managed with license managers, so it takes an exceptional
company to fight the uphill sales battle to impose them.  (And having
a company switch from License Manager A to License Manager B requires
reissuing licenses to every customer, an extraordinary customer-
support hassle.)  Only in markets where the customer has no effective
choice (of a competing DRM-free product) does it tend to work.
My last startup, Cygnus, sold un-license-managed compilers,
competiting with some entrenched companies that sold license-managed
compilers.  We kept seeing how our own automated overnight software
builds would fail using our competitors' compilers because the license
manager would screw up -- or merely because the local net or Internet
was down.  Or it would hang overnight awaiting an available license,
and doing no work in the meantime.  Our compiler always ran when you
asked it to.
We got tens of thousands of people to switch to our (free) GNU C and
C++ compilers, and enough of them paid us for support and development
that our company kept growing.  Our best selling point against Sun's
compilers, for example, was that ours didn't use any license manager.
Once you bought or downloaded it, it was yours.  It would run forever,
on as many machines as you liked, and you were encouraged to share it
with as many friends as you could.  It was simple for us to invade
their niche when they had deliberately forsworn a feature set like that.
PS:  Our trade-show giveaway button one year was "License Managers Suck";
     it was very popular.
PPS: On a consulting job one time, I helped my customer patch out the
license check for some expensive Unix circuit simulation software they
were running.  They had bought a faster, newer machine and wanted to
run it there instead of on the machine they'd bought the "node-locked"
license for.  The faster their simulation ran, the easier my job was.
Actually, I think we patched the Unix kernel or C library that the
program depended upon, rather than patch the program; it was easier.

@_date: 2009-10-13 12:56:24
@_author: John Gilmore 
@_subject: EFF Warns Texas Instruments to Stop Harassing Calculator Hobbyists (for cracking public keys) 
FYI.  As I understand it, TI calculator boot ROMs use a 512 bit RSA
public key to check the signature of the software they're loading.
When hobbyists who wanted to run their own alternative OS software on
their calculator calculated the corresponding private key and were
thus able to sign their own software, TI sent them DMCA takedowns
claiming they had cracked TI's DRM.  As with the CSS keys, a
publish/takedown chase ensued.  Wikileaks has had the censored keys up
since August.  EFF is now representing the hobbyists, and may stand to
collect legal fees from TI.  Here's Schneier's take:
  Electronic Frontier Foundation Media Release
For Immediate Release: Tuesday, October 13, 2009
Jennifer Stisa Granick
   Civil Liberties Director
   Electronic Frontier Foundation
   jennifer at eff.org
   +1 415 436-9333 x134
EFF Warns Texas Instruments to Stop Harassing Calculator
Baseless Legal Threats Squash Free Speech, Innovation
San Francisco - The Electronic Frontier Foundation (EFF)
warned Texas Instruments (TI) today not to pursue its
baseless legal threats against calculator hobbyists who
blogged about potential modifications to the company's
programmable graphing calculators.
TI's calculators perform a "signature check" that allows
only approved operating systems to be loaded onto the
hardware.  But researchers were able to reverse-engineer
signing keys, allowing tinkers to install custom operating
systems and unlock new functionality in the calculators'
hardware.  In response to this discovery, TI unleashed a
torrent of demand letters claiming that the
anti-circumvention provisions of the Digital Millennium
Copyright Act (DMCA) required the hobbyists to take down
commentary about and links to the keys.  EFF represents
three men who received such letters.
"The DMCA should not be abused to censor online discussion
by people who are behaving perfectly legally," said Tom
Cross, who blogs at memestreams.net. "It's legal to engage
in reverse engineering, and its legal to talk about reverse
In fact, the DMCA explicitly allows reverse
engineering to create interoperable custom software like
the programs the hobbyists are using.  Additionally, TI
makes its software freely available on its website, so
there is no connection between the use of the keys and
unauthorized distribution of the code.
"This is not about copyright infringement.  This is about
running your own software on your own device -- a
calculator you legally bought," said EFF Civil Liberties
Director Jennifer Granick.  "Yet TI still issued empty
legal threats in an attempt to shut down discussion of this
legitimate tinkering.  Hobbyists are taking their own tools
and making them better, in the best tradition of American
For the full letters sent to Texas Instruments by EFF on
behalf of their clients:
For this release:
About EFF
The Electronic Frontier Foundation is the leading civil
liberties organization working to protect rights in the
digital world. Founded in 1990, EFF actively encourages and
challenges industry and government to support free
expression and privacy online. EFF is a member-supported
organization and maintains one of the most linked-to
websites in the world at      -end-

@_date: 2009-10-17 02:23:25
@_author: John Gilmore 
@_subject: Possibly questionable security decisions in DNS root management  
DSA was (designed to be) full of covert channels.
It's more bizarre than you think.  But packet size just isn't that big
a deal.  The root only has to sign a small number of records -- just
two or three for each top level domain -- and the average client is
going to use .com, .org, their own country, and a few others).  Each
of these records is cached on the client side, with a very long
timeout (e.g. at least a day).  So the total extra data transfer for
RSA (versus other) keys won't be either huge or frequent.  DNS traffic
is still a tiny fraction of overall Internet traffic.  We now have
many dozens of root servers, scattered all over the world, and if the
traffic rises, we can easily make more by linear replication.  DNS
*scales*, which is why we're still using it, relatively unchanged,
after more than 30 years.
The bizarre part is that the DNS Security standards had gotten pretty
well defined a decade ago, when one or more high-up people in the IETF
decided that "no standard that requires the use of Jim Bidzos's
monopoly crypto algorithm is ever going to be approved on my watch".
Jim had just pissed off one too many people, in his role as CEO of RSA
Data Security and the second most hated guy in crypto.  (NSA export
controls was the first reason you couldn't put decent crypto into your
product; Bidzos's patent, and the way he licensed it, was the second.)
This IESG prejudice against RSA went so deep that it didn't matter
that we had a free license from RSA to use the algorithm for DNS, that
the whole patent would expire in just three years, that we'd gotten
export permission for it, and had working code that implemented it.
So the standard got sent back to the beginning and redone to deal with
the complications of deployed servers and records with varying algorithm
availability (and to make DSA the "officially mandatory" algorithm).
Which took another 5 or 10 years.
RSA was the obvious choice because it was (and is) believed that if
you can break it, you can factor large numbers (which mathematicians
have been trying to do for hundreds of years).  No other algorithm
available at the time came with such a high pedigree.  As far as I
know, none still does.  And if we were going to go to the trouble of
rewiring the whole world's DNS for security at all, we wanted real
security, not pasted-on crap security.
The DNSSEC RSA RFC says:
     For interoperability, the RSA key size is limited to 4096 bits.  For
   particularly critical applications, implementors are encouraged to
   consider the range of available algorithms and key sizes.
That's standard-speak for "don't use the shortest possible keys all
the time, idiot".  Yes, using 1024-bit keys is lunacy -- but of course
we're talking about Verisign/NSI here, the gold standard in crap
security.  The root's DNSSEC operational procedures should be designed
so that ICANN does all the signing, though the lord knows that ICANN
is even less trustworthy than NSI.  But at least we know what kind of
larceny ICANN is into, and it's a straightforward squeeze for their
own lavish benefit, forcibly paid by every domain owner; it doesn't
involve promising security and not delivering it.
Even using keys that have a round number of bits is foolish, in my
opinion.  If you were going to use about 2**11th bits, why not 2240
bits, or 2320 bits, instead of 2048?  Your software already handles
2240 bits if it can handle 2048, and it's only a tiny bit slower and
larger -- but a 2048-bit RSA cracker won't crack your 2240-bit key.
If this crypto community was serious about resistance to RSA key
factoring, the most popular key generation software would be picking
key sizes *at random* within a wide range beyond the number of bits
demanded for application security.  That way, there'd be no "sweet
spots" at 1024 or 2048.  As it is today, if NSA (or any major country,
organized crime group, or civil rights nonprofit) built an RSA key
cracker, more than 50% of the RSA keys in use would fall prey to a
cracker that ONLY handled 1024-bit keys.  It's probably more like
80-90%, actually.  Failing to use 1056, 1120, 1168-bit, etc, keys is
just plain stupid on our (the defenders') part; it's easy to automate
the fix.

@_date: 2009-10-20 05:48:51
@_author: John Gilmore 
@_subject: Possibly questionable security decisions in DNS root management  
Yes, that was stupid, but it was done very early in the evolution of
the Internet (when there were only a hundred machines or so).
Another bizarre twist was that the Berkeley "socket" interface to UDP
packets would truncate incoming packets without telling the user
program.  If a user tried to read 512 bytes and a 600-byte packet came
in, you'd get the first 512 bytes and no error!  The other 88 bytes
were just thrown away.  When this incredible 1980-era design decision
was revised for Linux, they didn't fix it!  Instead, they return the
512 bytes, throw away the 88 bytes, and also return an error flag
(MSG_TRUNC).  There's no way to either receive the whole datagram, or
get an error and try again with a bigger read; if you get an error,
it's thrown away some of the data.
When I looked into this in December '96, the BIND code (the only major
implementation of a name server for the first 20 years) was doing
512-byte reads (which the kernel would truncate without error).  Ugh!
Sometimes the string and baling wire holding the Internet together
becomes a little too obvious.
There's no prior negotiation.  The very first packet sent to a root
name server -- a query, about either the root zone or about a TLD --
now indicates how large a packet can be usefully returned from the
query.  See RFC 2671.  (If there's no "OPT" field in the query, then
the reply packet size is 512.  If there is, then the reply size is
specified by a 16-bit field in the packet.)
In 2007, about 45% of DNS clients (who sent a query on a given day to
some of the root servers) specified a reply size.  Almost half of
those specified 4096 bytes; more than 80% of those specified 2048 or
4096 bytes.  The other ~55% of DNS clients didn't specify, so are
limited to 512 bytes.
For a few years, there was a foolish notion from the above RFC that
clients should specify arbitrarily low numbers like 1280, even if they
could actually process much larger packets.  4096 (one page) is, for
example, the size Linux allows client programs to reassemble even in
the presence of significant memory pressure in the IP stack.  See:
  Any client who sets the bit for "send me the DNSSEC signatures along
with the records" is by definition using RFC 2671 to tell the server
that they can handle a larger packet size (because the DNSSEC bit is
in the OPT record, which was defined by that RFC).
"dig . ns  doesn't use an OPT record.  It returns
a 496 byte packet with 13 server names, 13 "glue" IPv4 addresses, and
2 IPv6 "glue" addresses.
"dig +nsid . ns  uses OPT to tell the name server
that you can handle up to 4096 bytes of reply.  The reply is 643 bytes
and also includes five more IPv6 "glue" addresses.
Older devices can bootstrap fine from a limited set of root servers;
almost half the net no longer has that restriction.
Anycast is a simple, beautiful idea, and I'm glad it can be made to
work in IPv4 (it's standard in IPv6).
That's an interesting assumption, but is it true?  Most IP-based
devices with a processor greater than 8 bits wide are able to
reassemble two Ethernet-sized packets into a single UDP datagram,
giving them a limit of ~3000 bytes.  Yes, if either of those datagrams
is dropped en route, then the datagram won't reassemble, so you've
doubled the likely failure rate.  But that's still much lower overhead
than immediately falling back to an 8-to-10-packet TCP connection,
particularly in the presence of high packet drop rates that would
also cause TCP to use extra packets.
I've seen papers on the prevalence of 1024-bit keys, but don't have a ready URL.  It's a theory.  Any comments, NSA?
I don't think most crackers would have trouble solving moduli smaller than
their maximum moduli; that's not the issue.  There are two answers:
  *  Security is economics.  As targets of crackers, we should spread
     out, rather than clumping.  Because it would hand you the keys to
     80-90% of everything, the economic incentive to build a 1024-bit
     cracker is irresistable, as soon as it becomes economically
     feasible at all.
     In the absence of a major mathematical breakthrough, we can
     assume that an 1120-bit cracker would be more expensive than a
     1024-bit one.  It would have wider datapaths and burn more power.
     To actually crack an 1120-bit key would take much, much longer
     than to crack a 1024-bit one on the same hardware.  Anyone who
     uses an 1120-bit key is safe for some number of years after all
     the interesting 1024-bit keys have fallen -- but they've only
     paid a small performance and storage penalty for that safefy.
     Similarly, a site using 1168-bit keys would remain safe, even if
     an 1120-bit cracker was built.
     If the user population spread out such that no more than 5% of us
     were using any given key size, the economic incentive to build a
     cracker would shift toward the more expensive bigger crackers,
     making all of us safer -- even the people whose keys were
     shortest.  It isn't worth building a 1024-bit cracker if that
     only breaks you into 5% of the infrastructure (and if the really
     important infrastructure is smart enough not to be in that 5%).
     It isn't worth building the 1120 if that only gets you 15%.
     Maybe you have to be able to afford to crack 1400 or 1500 bits
     before you can crack 50% of the infrastructure, and your
     management won't spend the money on the cracker until it's *that*
     useful.  That's likely to give the public a significant safety
     margin, compared to what happens if everybody's keys sitting at
     1024 bits.
  *  Some kinds of crackers may only work, or may work best, when their
     maximum number of bits is a power of two.  Thus perhaps the
     cracker-jack has to either build a 1024-bit cracker or a *much*
     slower and more expensive 2048-bit cracker.  (The 2048-bit
     cracker could crack 1168-bit keys, but it's more expensive to
     build it, so it might not be built at all, or at least your
     1168-bit keys will be safe until the cracker-jack can afford
     2048-bit hardware.)

@_date: 2009-10-20 14:44:48
@_author: John Gilmore 
@_subject: Possibly questionable security decisions in DNS root management  
I was looking at RFC 2536 from March 1999, which says "Implementation
of DSA is mandatory for DNS security." (Page 2.)  I guess by March 2005
(RFC 4034), something closer to sanity had prevailed.

@_date: 2010-08-02 16:40:36
@_author: John Gilmore 
@_subject: lawful eavesdropping by governments - on you, via Google 
And, by the way, what ever happened with the Google "lawful access"
system and China?  Inside the Google internal network is a whole
wiretapping subsystem designed to answer orders and requests from cops
and governments all over the globe (including US warrants, subpoenas,
National Security Letters, and court orders, as well as those of other
countries).  The trigger that gave Google the sudden courage to tell
the Chinese where to stuff it, was that they analyzed the malware
which had succeeded in penetrating their internal network, and
discovered that it was designed to specifically try to break into
Google's internal wiretapping system -- presumably so China could do
covert wiretaps into the mountain of up-to-the-minute personal data
that is Google -- wiretaps that wouldn't get reported to the US
government or to Google management or to anybody else.
So, six months later, Google and the Chinese government had a nicely
staged negotiated moment where each of them could claim victory, and
things have gone more or less back to normal on the surface.  But
nobody on either side has said anything about what kind of access the
government of China is getting to Google's internal network.  My guess
is that their detente also involved some negotiation about that, not
just about censored or non-censored searches.  Anybody know more?
PS: One of the great things about having a big global company that
collects and retains massive data about individuals is that
governments can get that data with simple subpoenas.  Most of the time
they could never get a judge to sign a warrant, or the legislature to
pass a law, to collect the same information directly from the data
subject (i.e. you).  Why?  A terrible US Supreme Court decision
(California Bankers Association) from decades ago decided that you
have zero Fourth Amendment protection for data that third parties have
collected about you.  The government can't collect it themselves, by
watching you or searching your house or your communications, but they
can grab it freely from anybody who happens to collect it.  (In a
classic blow-a-hole-in-the-constitution-and-They-Will-Come maneuver,
numerous laws now *require* businesses to collect all kinds of data
about their customers, employees, etc, IN ORDER that governments can
later look at it with no Fourth Amendment protection for the victims.)
Google, of course, needed no law from the Feds to inspire them to make
a database entry every time you move your mouse from one side of the
screen to the other.  Or open your Google phone.  Or call their "free"
411 service.  Or read your email.  Or visit any web site (free "Google
Analytics" is on most, even if there are no ads).  Or ...

@_date: 2010-08-14 20:25:32
@_author: John Gilmore 
@_subject: non 2048-bit keys 
Can I abuse a phrase and call this "binary thinking"?
There is no reason that the next step after 1024 bits has to be 2048 bits.
How about 1032 bits?  Or 1040?  Or 1104?
How about 1200 bits?  How about 1536?  How about 1600?  1808?
I have a theory that if everyone picked a pseudo-random key size above
1024 and below 2048, rather than standardizing on Yet Another Fixed
Keysize, we'd avoid making a powerful incentive for bad guys to build
a key-cracker optimized for one size.  Which incentive we have
currently created at 1024 bits.  It's the Microsoft Windows of key
sizes -- the target that gets you 90+% of the market.  So pick a
larger size than 1024 that your server load can live with, even if it
isn't 2048.  And don't tell anybody else what size you picked :-).

@_date: 2010-08-16 18:19:49
@_author: John Gilmore 
@_subject: Has there been a change in US banking regulations recently?  
"Enemy"?  We don't have to be the enemy for someone to crack our
security.  We merely have to be in the way of something they want;
or to be a convenient tool or foil in executing a strategy.
Given the prevalence of Chinese crypto researchers at the open crypto
conferences, I suspect that China is as much of a threat as the US's
National Security Agency, Russia's Sluzhba Vneshney Razvedki, India's
Research and Analysis Wing, Japan's J??h??honbu, Israel's
Mossad, or Brazil's Ag????ncia Brasileira de Intelig????nc.  A small
country with a good economy -- there are dozens more -- could also be
such a threat, if they focused on this area.  The big ones can crack
RSA keys AND do all the other things big countries do.
Many people on this list provide significant civilian or military
infrastructures depended on by millions.  When we know at least ten
nations are grasping at having the power to take down arbitrary
civilian infrastructures via cyberspace, we had better assume that
somebody among them can spend tens of millions of dollars *per year*
on key cracking.  And how much work is it, really, for us to use
longer keys?
Not all of us are in the US.  Those of us in the US perhaps have come
to a complacency about being a superpower - we haven't fought a war on
our own land, in which significant numbers of our own civilians died,
in what, a century?  The US government's idiotic response to 9/11 has
made more enemies around the world every year, while simultaneously
destroying the value of our currency.  The best time for a foreign
"enemy" to stop funding our $0.X trillion dollar a year debt would be
right after taking down much of our civilian infrastructure.  And
perhaps it might be hard for Washington to raise a billion dollars a
day in international bond sales, even from friendly countries, when
the international financial networks had been subtly or completely
compromised?  Hell, half the people in this country would starve two
days after their ATM cards stopped working.  The whole point of the
trillion dollar Bush and Obama bailouts (which were done by moving a
few bits in a federal funds transfer network somewhere) was to avoid
the specter of long lines around the block at bank branches, full of
angry people failing to turn bits in bank accounting databases into
paper or gold money.  Such a spectre would be easy for a cracker to
create -- and then how much confidence will people have in either the
currency or the government?
What keys secure that funds transfer network?  Suppose an attacker
merely multipled a random 10% of the transfers by 1000?  Somebody
wires you a thousand dollars, you have a 10% chance of it becoming a
million.  Wire a million, it might come through as a billion.  Then
you look at strategy: should they pay themselves back immediately for
the cost of cracking the keys, then be quiet?  Or should they just
make everyone a billionaire and make the entire currency worthless?
Did you think Adi Shamir's work on TWINKLE and TWIRL was theoretical?
Israeli leadership is paranoid enough to regularly shoot their friends
as well as their enemies, and usually in advance, on the theory of
weakening them *before* they turn against Israel.  And Israel would
have a lot more geopolitical power in a world without superpowers.
Did you think nobody else was designing or building such things?
Thank Adi for publishing - but what he published might not have been
his very best design.  Why did this community wait until a DES
cracker cost only $250,000 to build before thinking, duuh, maybe we
should defend our infrastructure against DES crackers.  How many
countries had secret DES crackers before I built one publicly?
To this day, no country has admitted having one -- yet I have been
privately told that government experts were aware that the cost of
building one was in the $250K range.  Do you think they learned that
merely by twirling a pencil at their desk, in agencies with budgets
way over $100 million a year?
(A private industry expert also told me that they'd been hoping the
first public DES cracker would happen at least a year later than it
did, to give them more time to secure their networks, e.g. before
their bosses found out how vulnerable the previous design was.)
In 2003, Shamir's estimate was that TWIRL could factor a 1024-bit
number in a year at a cost of about $10M US dollars.  More recent
estimates are here:
  Either that page hasn't been updated since 2006-7 or there's been no
published research since then.  I encourage others to post more
surveys of the cost of cracking RSA keys using dedicated hardware.
A typical academic analysis, such as 1996's "Minimal Key Lengths
for Symmetric Ciphers to Provide Adequate Security" said things like:
  Because ASICs require a far greater engineering investment than
  FPGAs and must be fabricated in quantity before they are economical,
  this approach is only available to serious, well-funded operations
  such as dedicated commercial (or criminal) enterprises and government
  intelligence agencies.
But that was bullshit.  Two years later, a team of about six guys
designed and built a 1-week DES cracker for much less than what it
costs to buy a condo in San Francisco.  Circuit layout and fabrication
services were readily available in the commercial market.
Anybody who builds and deploys one machine that can crack RSA-1024 in
a year will build more.  The design is paid for; and it's cheaper to
build them in quantity 10 than in quantity 1.  Every year the tech can
get better, too.  After they've built 50, which perhaps only take six
months to crack a key, will YOUR key be one of the 100 keys that they
crack this year?  How about next year?
Smart allied countries - or criminals - would split up the work,
attack different keys, and swap results, spreading the cost around --
two countries with banks of 50 6-mo machines could crack twice as deep
down into the infrastructure, rather than both of them wasting their
time cracking the same keys.
It wouldn't even take much coordination; they could offer a key
they've already cracked, to trade for another one.  If somebody burns
them and gets a cracked key while failing to provide one, big deal;
they get one freebie.  But if your partner keeps feeding you cracked
keys that were on your list but you hadn't gotten to, you'd keep doing
deals; it *halves* the cost of your cracking.
Who'll do the math to figure out how to crack ten thousand keys in
parallel in hardware?  Such a device might not crack any particular
key in a year, but it'll crack *some* of those keys in a year,
depending on luck.  Having such a machine would produce some
interesting results if you were in the cracked-key trading market.
You could probably trade some to people who value them more than you
do, in return for keys that you value more.
Now do you see why it's a bad idea that 90+% of keys are 1024 bits?
When that size became vulnerable, it brought market forces to bear on
the problem.  If in fixing that mistake we make another sharp focus at
some other size, as soon as that size becomes barely vulnerable,
another key-cracking market will appear.  It would be better if we had
a hundred small markets at different sizes.  There might be six
critial keys you really want to crack with your new, expensive, slow,
right-at-the-limit-of-viability 1200 bit cracker - but only six.  To
get the 1300-bit keys you'll need more years of design and
semiconductor evolution.

@_date: 2010-08-17 20:25:56
@_author: John Gilmore 
@_subject: 2048-bit RSA keys  
A good paper by top academics.
The 768-bit team started crunching in early 2007 and completed three
years later in December 2009.  They used fewer than a thousand
commercially available unspecialized computers, connected by
commercially available interconnects.  Their intermediate results fit
on less than a dozen $150 2TB disk drives.  And one of their results
is that it's better to scale up the part of the process that scales
linearly with minimal communication (sieving), to reduce the complexity
of the nonlinear parts.
(Given their prediction that they won't be done with a 1024-bit number
within 5 years, but they will be done "well within the next decade",
which 1024-bit number are they starting to factor now?  I hope it's a
major key that certifies big chunks of the Internet for https today,
rather than one of those silly challenge keys.)
Their reported time and difficulty results are great lower bounds
on the capabilities of the covert or criminal -- but don't mistake
them for upper bounds!
No open-community academic has ever designed, built and deployed
special-purpose hardware for factoring numbers of this size.  Yet they
have published designs that claim order-of-magnitude speedups or
better on time-consuming parts of the process.  EFF read similar
published paper designs for DES cracking.  When a few years later we
built the actual device, we discovered that the basic structure of the
academics' designs really did work.  There are good reasons to believe
that the covert community *has* built RSA cracking hardware as good or
better than what's been publicly designed.  And in some places covert
agencies and organized crime are partners, thus merely stealing large
amounts of money, as opposed to military objectives, might motivate a
covert key crack.
Here is Europe's consensus report on recommended key sizes, also
co-authored by Lenstra:   ECRYPT2 Yearly Report on Algorithms and Keysizes (2010).
    For RSA, "we recommend |N| >= 1024 for legacy systems and |N| >= 2432
  for new systems."
A more accessible table of ECRYPT2-2010 recommendations:
    RSA
  Bits	Security level
  ----    --------------
  1008:  Short-term protection against medium organizations,
  1248:  Very short-term protection against agencies,
  1776:  Legacy standard level
  2432:  Medium-term protection
  3248:  Long-term protection
  15424:  "Foreseeable future"

@_date: 2010-07-22 17:59:50
@_author: John Gilmore 
@_subject: What if you had a very good patent lawyer... 
It's pretty outrageous that anyone would try to patent rolling barcoded
dice to generate random numbers.
I've been generating random strings from dice for years.  I find that
gamers' 20-sided dice are great; each roll gives you a hex digit, and
anytime you roll a 17 thru 20, you just roll again.  One die will do;
you just roll it as many times as you need hex digits.
Presumably pointing a camera at ordinary dice could automate the data
collection -- hey, wait, let me get my patent lawyer!

@_date: 2010-10-07 03:16:28
@_author: John Gilmore 
@_subject: Computer "health certificate" plan: Charney of DoJ/MS 
BBC reports that Microsoft's idea seems to be that if your computer
doesn't present a valid "health certificate" to your ISP, then your
ISP wouldn't let it be on the net, or would throttle it down to a tiny
bandwidth.  The Health Certificate would, of course, be provided by
Intel and Microsoft, but only from machines with Treacherous Computing
hardware, after examining everything in your computer to make sure
Intel and Microsoft approve of it all.  (This is the same DRM
procedure they've been pushing for a decade -- the system would
cryptographically "attest" to arbitrary information about what's
running in your machine, using proprietary hardware and software you
have no control over and no ability to inspect, and the outsiders
would decide not to deal with you if they didn't like your
attestation.  The only change is that they've revised their goal from
"record companies won't sell you a song if you won't attest" to
"nobody will give you an Internet connection if you won't attest".)
Homebrew computers and Linux machines need not apply.  They don't
explain how this would actually be implemented -- in Ethernet
switches?  In DSL routers or NAT boxes?  In ISP servers?  They're not
quite sure whether the health certificate should *identify* your
device, but they're leaning in that direction.  But they're quite sure
that it all needs doing, by voluntary means or government coercion,
and that the resulting info about your "device health" should be
widely shared with governments, corporations, etc.
This proposal comes from Microsoft VP Scott Charney, well known to
many of us as the former Chief of the Computer Crime and Intellectual
Property Section in the Criminal Division of the U.S. Department of
Justice, or as he puts it, "the leading federal prosecutor for
computer crimes from 1991 to 1999".  He joined Microsoft in 2002 and
is running their "Treacherous Computing" effort as well as several
other things.
The vision that Charney is driving is described in six papers
here (one of which is the one the BBC is covering):
  He's pushing the "Public Health Model" because public health
bureacracies have huge, largely unchecked powers to apply force to
people who they disfavor.  Along those lines, he converts the public
health departments' most draconian measure, used only in extreme
circumstances - quarantine - into the standard procedure for his New
Internet: quarantine EVERY device -- unless and until it "proves" that
it should evade the quarantine.
In his "Establishing End to End Trust" paper (another of the six), he
lays out the computer security problem and decides that defense isn't
enough; authentication, identification, and widespread auditing are
the next step in solving it.  He concludes:
  As we become increasingly dependent on the Internet for all our
  daily activities, can we maintain a globally connected, anonymous,
  untraceable Internet and be dependent on devices that run arbitrary
  code of unknown provenance?  If the answer to that is "no," then we
  need to create a more authenticated and audited Internet environment
  -- one in which people have the information they need to make good
  trust choices.
He makes halfhearted attempts to address privacy and anonymity issues,
but ultimately decides that those decisions will be made somewhere
else (not by the user or consumer, of course).  His analysis
completely ignores the incentives of monopoly hardware and software
providers; of corrupt governments such as our own; of even honest
governments or citizens desiring to act secretly or without
attribution; of advertisers; of the copyright mafia; of others
actively hostile to consumer and civil freedom; and of freedom-
supporting communities such as the free software movement.  It ignores
DRM, abuse of shrink-wrap contracts, copyright maximalization,
censorship, and other trends in consumer abuse.  It's designed by a
career cop/bureaucrat/copyright-enforcer and implemented by a
monopolist - hardly viewpoints friendly to freedom.
I'd recommend merely ignoring his ideas til they sink like a stone.
But it looks like Intel and Microsoft are actively sneaking up on the
free Internet and the free 10% of the computer market by building in
these techniques and seeking partnerships with governments, ISPs,
telcos, oligopolists, etc to force their use.  So some sort of active
opposition seems appropriate.
Perhaps Linux systems should routinely delete all the
manufacturer-provided device attestation and identification keys from
every Treacherous Computing device they ever boot on.  (This won't
affect keys that the *user* stores in their TPM if they want to.)  If
a significant part of the Internet is physically incapable of
attesting to the monopolists, ISPs will never be able to require such
attestation.  I've certainly deleted those keys on my own PCs that
came with such crap -- so far, no downside.  Let's keep it that way.
Security measures should report to the system owner -- not to the ISP
or the manufacturer.  The owner of the machine should determine which
software it's appropriate for it to run.  This whole idea of
collectivist "approval" of your computing environment gives me the
willies.  In their model, you'd be perfectly free to write a new piece
of software, sort of the way you are perfectly free to design and
build a new house.  First you spend tens of thousands of dollars on a
government-licensed architect and a similarly licensed structural
engineer.  Then you submit your plans to a bureaucrat, and wait.  And
wait.  And they demand changes.  And you negotiate, but they really
don't care what you want; you NEED their approval.  So you wait some
more, then give in to whatever they want.  Don't forget to use union
labor to build your software -- it'll be required.  And any bureaucrat
can come by after an alcoholic lunch to "inspect" your software -- and
if you don't properly kiss their ass and/or bribe them, their "red
tag" will physically keep your software from being usable on every
computer.  Periodically politicians will write bizarre new
requirements into the rules (e.g. you can't use PVC pipe because that
would put local plumbers out of work; or you can't use portable
languages because then your software might run on competing platforms),
and you'll just have to follow orders.  At least that's how the
Planning Department and Building Inspection Department work here in
San Francisco.  I don't see why a software monopoly enforced from the
top would work any different.  Writing software for any Apple platform
except the Mac is already like that.

@_date: 2010-09-13 20:58:57
@_author: John Gilmore 
@_subject: Intel plans crypto-walled-garden for x86 
"In describing the motivation behind Intel's recent purchase of McAfee
for a packed-out audience at the Intel Developer Forum, Intel's Paul
Otellini framed it as an effort to move the way the company approaches
security "from a known-bad model to a known-good model." Otellini went
on to briefly describe the shift in a way that sounded innocuous
enough--current A/V efforts focus on building up a library of known
threats against which they protect a user, but Intel would live to
move to a world where only code from known and trusted parties runs on
x86 systems."
Let me guess -- to run anything but Windows, you'll soon have to jailbreak even laptops and desktop PC's?

@_date: 2010-09-17 16:43:33
@_author: John Gilmore 
@_subject: Something you have, something else you have, and, uh, something else you have  
No, they don't use the phone number to validate anything.  I routinely
ignore the instructions to "call from your home phone".  I call in from
random payphones to "activate" my cretin cards, and they activate just
Perhaps there's a database record made somewhere with the phone number
of that payphone -- but the card is active, and I could be stealing money from it immediately.
Note also that their ability to get that phone number depends on the
FCC exemption that allows 800-numbers to bypass caller-ID blocking.
If the FCC ever comes to its senses (I know, unlikely) then making
somebody call an 800-number will not even produce a phone number.

@_date: 2013-12-05 00:37:18
@_author: John Gilmore 
@_subject: [Cryptography] Kindle as crypto hardware 
Reusing previous generation mass market devices (like a Kindle) is
a good idea.  My suggestion is a Sharp Zaurus.  eBay has them for
$30-80, they run a pretty stock micro Linux from flash or microdrive.
Full chicklet keyboard for passphrases and programming.
Nokia N800 is similar but slightly later.  No keyboard tho.
Main issue is:
Do you have an existing remote-key-access protocol in mind, that's
supported by any existing software?  What does *that* code expect?
I was thinking you wanted this to keep your keys securely, and
deliver encryption, decryption and signing services on demand,
like a crypto-in-hardware token, but one you could actually trust.
But Phillip's example was to use this as the UI for a secret sharing
application and then destroy it.  And Kent's was as a password locker,
where he'd read passwords from the screen and type them into another
device.  What function are we trying to solve here?
USB-slave or Ethernet are possible bets for high security and high
likelihood of connecting successfully to laptops or desktops.  Wifi is
a bit too open for a crypto protocol, and Bluetooth compatability is
an oxymoron.  But if you want to provide remote keying material to a
smartphone app, you're stuck with them.
Some Zauruses had PCMCIA slots and you could get a Wifi-G card or even
a 10-Mbit Ethernet card.  Most or all had no networking built-in.
Some had USB slave access.
Nokia N800 had only crappy WiFi (mine dies at random intervals) and
no slots except SD cards.
What ever happened to those old Java-rings or iButtons with the 1-wire
interface?  They were designed for almost exactly this application.
(Touch the ring to a point on a device or wall, it authenticates or
decrypts.  No UI beyond that, except for initial programming.)

@_date: 2013-12-16 22:29:30
@_author: John Gilmore 
@_subject: [Cryptography] The next generation secure email solution 
But at the same time the system has to not say, "I can't deliver your
message to that person because an invisible gnotzaframmit that I won't
describe to you seems to be unavailable to me in the flabogrommit."

@_date: 2013-12-25 14:52:20
@_author: John Gilmore 
@_subject: [Cryptography] Can we move this list to an online forum please? 
(Feel free to start your own online forum about encryption, but a
bunch of us like it here, ain't gonna move, and don't appreciate the
signal-to-noise ratio of people trying to convince us to do so.)
I would encourage the moderators to strongly squelch meta-traffic
about "should this really be a mailing list".

@_date: 2013-12-28 12:11:42
@_author: John Gilmore 
@_subject: [Cryptography] Whether Henry Spencer's key-leak would be 
I tend to disagree.
Back in 1983 on the Sun engineering Ethernet, we discovered that a
significant fraction of the packets were failing the CRC checks in
their Ethernet chips -- more than twice the "1 in billions" fraction
suggested in the Ethernet specs.
We never ran down why.  We were hard-pressed to just ship products to
willing customers -- we had a 6-month backlog of orders -- and the
higher level protocols were detecting the errors, retrying, and
recovering, so the practical effect was a minor retry every once in
a while.  The Engineering net was the most diverse and complicated
Ethernet we ran internally to Sun; we didn't see this problem on
simpler test networks or other networks there.
Such a problem could have been anywhere in the protocol stack -- from
the physical fat-yellow-ethernet cables, to the transceiver circuitry,
the transceiver cables, the sending Ethernet chips, their DMA engines
and memory-timing circuitry that supplied them, the RAM chips that
held the packets, the receiving Ethernet chips and their RAM and DMA,
the software that built the packets, etc.  Our engineering
organization had most of the right people to run down the problem (we
had few analog engineers, but otherwise we had BUILT much of this
hardware and software ourselves).  But we had neither the time nor the
motivation to do so.
At customer sites, even if they had the time to spend on it, and the
motivation to do so, they would seldom have had the expertise to run
down the actual problem.  Which would be very good for the attacker,
if the actual problem was that a subverted component was leaking key
materials into the packets.
In today's Ethernets, a defender would also have to look at the CRC
error rates in each switch, not merely on the endpoint systems, since
many switches check CRCs and filter out erroneous packets.  So, if an
attacker can subvert the firmware in a switch, a great way to leak
information from a host is to deliberately send it in packets with a
bad CRC.  That packet will only travel on the physical cable between
the host and the switch, making it VERY hard for a defender to detect
it.  (The switch would have to leak the data to the outside world in
some other way -- perhaps in a different deliberately-mangled packet
being sent to a different and more remote destination.)
What's the CRC error rate on *your* local Ethernet?  How does it compare
to the IP checksum failure rate?  The TCP checksum failure rate?  When
is the last time you looked?
PS: In many Ethernet packets there are many "don't-care" bits.  For
example, the minimum Ethernet packet size is 64 bytes (46 bytes of
payload).  The minimum size of an IP packet is smaller -- 20
bytes of IP and 20 bytes of TCP -- so every time a TCP acknowledgement
packet is sent without data, there are 6 bytes of "padding" at the
end of the TCP/IP packet carried inside the 64-byte Ethernet packet.
According to RFC 894, these padding bytes are supposed to be sent as
zeroes, but there is no requirement to check them at the receiving
end.  Do *you* know how many padding bytes sent or received on your
network are nonzero?
Sending minimum sized packets happens frequently.  Most TCP
connections have a phase in which a lot of data flows in one
direction, and none in the other direction.  For example, when
accessing a web page with HTTP, the client sends a request (while the
server sends nothing but ACKs), then the client sends nothing but ACKs
while the server sends the contents of the web page.  Some of the
packets used to open or close a TCP connection are also often of
minimum size.  DNS queries are also often shorter than the minimum
Ethernet packet size too.  So an attacker could leak many bytes per
second without even using packets with bad CRC's.

@_date: 2013-06-28 16:00:06
@_author: John Gilmore 
@_subject: [Cryptography] Snowden "fabricated digital keys" to get access to 
The Daily Beast
Greenwald: Snowden's Files Are Out There if 'Anything Happens' to Him
by Eli Lake Jun 25, 2013 1:36 PM EDT
Snowden has shared encoded copies of all the documents he took so that they won't disappear if he does, Glenn Greenwald tells Eli Lake.
As the U.S. government presses Moscow to extradite former National Security Agency contractor Edward Snowden, America's most wanted leaker has a plan B. The former NSA systems administrator has already given encoded files containing an archive of the secrets he lifted from his old employer to several people. If anything happens to Snowden, the files will be unlocked.
Glenn Greenwald, the Guardian journalist who Snowden first contacted in February, told The Daily Beast on Tuesday that Snowden "has taken extreme precautions to make sure many different people around the world have these archives to insure the stories will inevitably be published." Greenwald added that the people in possession of these files "cannot access them yet because they are highly encrypted and they do not have the passwords." But, Greenwald said, "if anything happens at all to Edward Snowden, he told me he has arranged for them to get access to the full archives."
The fact that Snowden has made digital copies of the documents he accessed while working at the NSA poses a new challenge to the U.S. intelligence community that has scrambled in recent days to recover them and assess the full damage of the breach. Even if U.S. authorities catch up with Snowden and the four classified laptops the Guardian reported he brought with him to Hong Kong the secrets Snowden hopes to expose will still likely be published.
A former U.S. counterintelligence officer following the Snowden saga closely said his contacts inside the U.S. intelligence community "think Snowden has been planning this for years and has stashed files all over the Internet." This source added, "At this point there is very little anyone can do about this."
The arrangement to entrust encrypted archives of his files with others also sheds light on a cryptic statement Snowden made on June 17 during a live chat with The Guardian. In the online session he said, "All I can say right now is the U.S. government is not going to be able to cover this up by jailing or murdering me. Truth is coming, and it cannot be stopped."
Last week NSA Director Keith Alexander told the House Permanent Select Committee on Intelligence that Snowden was able to access files inside the NSA by fabricating digital keys that gave him access to areas he was not allowed to visit as a low-level contractor and systems administrator. One of those areas included a site he visited during his training that Alexander later told reporters contained one of the Foreign Intelligence Surveillance Act (FISA) Court orders published by The Guardian and The Washington Post earlier this month.
[John here.  Let's try some speculation about what this phrase,
"fabricating digital keys", might mean.]

@_date: 2013-10-31 20:04:17
@_author: John Gilmore 
@_subject: [Cryptography] What's a Plausible Attack On Random Number 
This is sort of like "BOOTP for RNGs".  It sounds like an interesting
R&D project.  Deliberately relying on external inputs (even the timing
of external inputs) invites attackers, of course.  And spraying output
from your well-fed RNG out to the world invites a different class of
attackers.  Which is why this is more like a multi-year research
effort as opposed to an implement-it-and-forget-it service.
Multicast is a good technology for this.  Except that in IP multicasting,
the sender specifies how far the packets should spread through the net
(by setting the TTL and/or the multicast address).  In this case, a
randomness-starved receiving host might prefer to ask, "Could I have
some multicasts from several hops away, please?" -- but the existing
IP multicast protocols don't let it do that.
Let's do it another way.
Since all that a randomness-starved, unconfigured host can trust to be
"random" is the CPU tick counter when the packet arrives, the system
might as well ask for multicast "time updates" as "randomness
updates".  There are already protocols and deployed servers for that,
and indeed the unconfigured host is probably already using some
variant of them to set or tune up its clock!
There is a multicast group defined for time signals.  NTP can
multicast time packets.  The address is 224.0.1.1 in IPv4, FF0x::101
in IPv6 (typically FF02::101 for a link-local multicast or FF05::101
for a site-local multicast).  One challenge is that NTP tries to severely limit the amount of
bandwidth that it uses (since it runs all the time).  The default
seems to be for a multicast server to send one multicast packet every
64 seconds.  That won't get a client much entropy very quickly, so the
client might have to send some multicast requests ("manycasts") asking
for time updates (which can provoke unicast responses that will reach
the client and tickle its CPU tick counter's entropy).  There is also
a "burst" mode in modern NTP clients, which will send eight packets
at multi-second intervals, trying to get a good characterization of
the delay and jitter between itself and its time servers, which will
increase the number of packets used quickly in initial synchronization.
Making the entropy service subtly depend on the time-sync service
seems like a recipe for some likely failure modes.  I wonder what we
can suggest to make them more obvious when occurring?  Maybe
more entropy; is NTP and/or rngd running?" when it blocks.

@_date: 2013-11-06 16:08:33
@_author: John Gilmore 
@_subject: [Cryptography] randomness +- entropy 
You're right -- but this conflicts with "secure boot" schemes that
want to know the image is "authentic" (i.e. signed by a private key
that isn't on the local machine) before it is permitted to run.
(A secondary and more tractable problem is that the running system may
not know exactly where it was booted from, and may not have write
access to that location.  Consider a network boot via PXE or TFTP --
or a system booting from a read-only drive, which in other
circumstances we would applaud as a security improvement.)
It would be unsound to make such systems fail -- since
there are millions already in use, and if putting a newer
OS release on them would cause them to fail, people won't
bother putting a newer OS release on them.  So, if they don't
fail, what should we do to declare them "unsound" in a way
that the system user can detect (and ignore or fix)?

@_date: 2013-11-26 20:03:19
@_author: John Gilmore 
@_subject: [Cryptography] Microsoft announces new email encryption misfeature? 
We're pleased to announce the upcoming release of Office 365 Message
  Encryption, a new service that lets you send encrypted emails to
  people outside your company. No matter what the destination --
  Outlook.com, Yahoo, Gmail, Exchange Server, Lotus Notes, GroupWise,
  Squirrel Mail, you name it -- you can send sensitive business
  communications with an additional level of protection against
  unauthorized access.
  "Messages are 2048-bit encrypted using SHA-256 with the private key
  of the Office 365 tenant domain. Recipients have no knowledge of
  this key, so when they receive the message, they'll see that it
  contains an encrypted attachment together with some instructions as
  to how to view the content.  ...
  Clicking on the attachment opens a new browser window connected to a
  page on the Office 365 Message Encryption portal (a company can
  customize this page with its logo and some directive text to tell
  end users what to do). The user then has to authenticate themselves
  using a Windows Live or Office 365 ID before content can be
  decrypted and presented in an Outlook Web App-like interface.  ...
  It's also important to realize that this implementation is strictly
  browser-based for now in terms of accessing encrypted message
  content and that there is no offline client support. In other words,
  Office 365 Message Encryption likes to seek reassurance from the
  mother ship when [you try to read] encrypted messages.
In this message I'm reverse engineering this offering from the clues
in the announcement, so take this with a boulder of salt.  Maybe
somebody here knows the actual design?
The encryption is done in the Microsoft Exchange server, not in the
client.  A nice centralized insecure place.  No changes to the
sender's user interface; a sysadmin writes rules for which outgoing
emails will get encrypted.  Users can't tell which of their emails
will end up encrypted at the other end (i.e. which emails will come
through garbled and unreadable offline).
It ends up sending a very spam-like message to the recipients,
containing an attachment and a demand that they "open the attachment",
"follow the instructions", and "sign in using the following email
address" (theoretically your own).  I don't know any security
professionals who recommend following such directions in a received
Apparently despite the "no matter what the destination"
marketing-speak in the announcement, it only works for recipients who
are on Microsoft Hotmail or who use MS-Office Exchange (otherwise you
don't HAVE a password that can authenticate you to a Microsoft "secure
e-mail portal").
It claims that "when the receiver replies to the sender, or forwards
the message, those emails are also encrypted".  This is a bizarre
claim that seems to indicate that the recipient can only reply or
forward this email via the Microsoft website rather than via their own
mail reader.  Sounds highly confusing to ordinary folks, who expect to
use their chosen email interface to forward or reply to their emails.
The illustration looks like it also dumps a Microsoft Word editing box
into your browser, just so you can send your Microsoft-proprietary
encrypted reply in a Microsoft-proprietary word processor format?
It doesn't seem to interoperate with *any* other encrypted email
systems.  The idea is apparently that your email client invokes a web
browser to access a web site to retrieve the email.  Either that, or
your web browser locally runs Javascript from the attachment, which is
even more dangerous since it might well be a fake email.
A doctor I use tried sending me encrypted emails with a scheme similar
to this, a few years ago.  I could never read them -- but I don't
"open" attachments nor type my passwords into random remote websites.
Looks like an epic fail to me, but people who pay Microsoft for
operating systems may be dumb enough to use it.  Anybody know more?

@_date: 2013-10-01 17:41:40
@_author: John Gilmore 
@_subject: [Cryptography] encoding formats should not be committee'ized 
If only we could channel the late Jon Postel.  Didn't you ever notice
how almost all the early Arpanet/Internet standards use plain text
separated by newlines, simply parsed, with a word at the front of each
line that describes what is on the line?  Like, for example, the
header of this email message.  And the SMTP exchange that delivered it
to your mailbox.
It makes everything so easy to debug...and extend...and understand.
And it turns out to often be more compact than binary formats.
Much better than binary blobs that not even their mother could love.

@_date: 2013-10-05 11:00:45
@_author: John Gilmore 
@_subject: [Cryptography] System level security in "low end" environments 
Such environments are getting very rare these days.  For example, an
electrical engineer friend of mine was recently working on designing a
cheap aimable mirror, to be deployed by the thousands to aim sunlight
at a collector.  He discovered that connectors and wires are more
expensive than processor chips these days!  So he ended up deciding to
use a system-on-chip with a built-in radio that eliminated the need to
have a connector or a wire to each mirror.  (You can print the antenna
on the same printed circuit board that holds the chip and the
What dogs the security of our systems these days is *complexity*.  We
don't have great security primitives to just drop into place.  And the
ones we do have, have complicated tradeoffs that come to the fore
depending on how we compound them with other design elements (like
RNGs, protocols, radios, clocks, power supplies, threat models, etc).
This is invariant whether the system is "low end" or "high end".
That radio controlled mirror can be taken over by a drive-by attacker
in a way that would take a lot more physical labor to mess up a
wire-controlled one.  And if the attack aimed two hundred mirrors at
something flammable, the attacker could easily start a dangerous fire
instead of making cheap solar energy.  (Denial of service is even
easier - just aim the mirrors in random directions and the power goes
away.  Then what security systems elsewhere were depending on that
power?  This might just be one cog in a larger attack.)  Some of the
security elements are entirely external to the design.  For example,
is the radio protocol one that's built into laptops by default, like
wifi or bluetooth?  Or into smartphones?  Or does it require custom
hardware?  If not, a teenager can more easily attack the mirrors --
and a corrupt government can infect millions of laptops and phones
with malware that will attack mirror arrays that they come near to.
For products that never get made in the millions, the design cost
(salaries and time) is a significant fraction of the final cost per
unit.  Therefore everybody designs unencrypted and unauthenticated
stuff, just because it's easy and predictable.
For example it's pretty easy to make the system-on-chip above send or
receive raw frames on the radio.  Harder to get it to send or receive
UDP packets (now it needs an IP address, ARP, DHCP, more storage, ...).
Much harder to get it to send or receive *authenticated* frames or UDP
packets (now it needs credentials; is it two-way authenticated, if so
it needs a way to be introduced to its system, etc).  Much harder
again to get it to send or receive *encrypted* frames or UDP packets
(now it needs keys too, and probably more state to avoid replays,
etc).  And how many EE's who could debug the simple frame sending
firmware and hardware, can debug a crypto protocol they've just
implemented (even making the dubious assumpion that they compounded
the elements in a secure way and have just made a few stupid coding

@_date: 2013-10-10 14:31:32
@_author: John Gilmore 
@_subject: [Cryptography] PGP Key Signing parties 
It's just a practice.  I agree that building a small amount of automation
for key signing parties would improve the web of trust.
I have started on a prototype that would automate small key signing
parties (as small as 2 people, as large as a few dozen) where everyone
present has a computer or phone that is on the same wired or wireless
An important user experience point is that we should be teaching GPG
users to only sign the keys of people who they personally know.
Having a signature that says, "This person attended the RSA conference
in October 2013" is not particularly useful.  (Such a signature could
be generated by the conference organizers themselves, if they wanted
to.)  Since the conference organizers -- and most other attendees --
don't know what an attendee's real identity is, their signature on
that identity is worthless anyway.
So, if I participate in a key signing party with a dozen people, but I
only personally know four of them, I will only sign the keys of those
four.  I may have learned a public key for each of the dozen, but that
is separate from me signing those keys.  Signing them would assert to
any stranger that "I know that this key belongs to this identity", which
would be false and would undermine the strength of the web of trust.

@_date: 2013-10-14 18:53:58
@_author: John Gilmore 
@_subject: [Cryptography] "/dev/random is not robust" 
I'll be the first to admit that I don't understand this paper.  I'm
just an engineer, not a mathematician.  But it looks to me like the
authors are academics, who create an imaginary construction method for
a random number generator, then prove that /dev/random is not the same
as their method, and then suggest that /dev/random be revised to use
their method, and then show how much faster their method is.  All in
all it seems to be a pitch for their method, not a serious critique of
They labeled one of their construction methods "robustness", but it
doesn't mean what you think the word means.  It's defined by a mess of
greek letters like this:
  Theorem 2. Let n > m, , ?? ??? be integers. Assume that G :
  {0, 1}m ??? {0, 1}n+ is a deterministic (t, ??prg )- pseudorandom
  generator. Let G = (setup, refresh, next) be defined as above. Then
  G is a ((t , qD , qR , qS ), ?? ??? , ??)- 2 robust PRNG with
  input where t ??? t, ?? = qR (2??prg +qD ??ext +2???n+1 )
  as long as ?? ??? ??? m+2 log(1/??ext )+1, n ??? m + 2
  log(1/??ext ) + log(qD ) + 1.
Yeah, what he said!
Nowhere do they seem to show that /dev/random is actually insecure.
What they seem to show is that it does not meet the "robustness"
criterion that they arbitrarily picked for their own construction.
Their key test is on pages 23-24, and begins with "After a state
compromise, A (the adversary) knows all parameters."  The comparison
STARTS with the idea that the enemy has figured out all of the hidden
internal state of /dev/random.  Then the weakness they point out seems
to be that in some cases of new, incoming randomness with
mis-estimated entropy, /dev/random doesn't necessarily recover over
time from having had its entire internal state somehow compromised.
This is not very close to what "/dev/random is not robust" means in
English.  Nor is it close to what others might assume the paper
claims, e.g. "/dev/random is not safe to use".
PS: After attending a few crypto conferences, I realized that
academic pressures tend to encourage people to write incomprehensible
papers, apparently because if nobody reading their paper can
understand it, then they look like geniuses.  But when presenting at
a conference, if nobody in the crowd can understand their slides, then
they look like idiots.  So the key to understanding somebody's
incomprehensible paper is to read their slides and watch their talk,
80% of which is often explanations of the background needed to
understand the gibberish notations they invented in the paper.  I
haven't seen either the slides or the talk relating to this paper.

@_date: 2013-10-22 14:51:02
@_author: John Gilmore 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
Any computer that checks parity on its main memory, or uses ECC error
correcting codes on its main memory, will have to initialize its main
memory after power-on.  In x86's this is often done by the BIOS.
Failing to do this initialization would cause later main memory reads
of uninitialized variables to produce spurious parity or ECC errors
(spurious in the sense that the memory chip has not failed to retain a
value previously written to it).
Many modern memory controller chips automatically support parity or ECC,
depending on which memory DIMMs are plugged in.  One extra bit per byte
allows parity protection or (with 64-bit buses) ECC.

@_date: 2013-10-22 18:11:31
@_author: John Gilmore 
@_subject: [Cryptography] programable computers inside our computers 
It is probably ALREADY running there.
With regard to the invisible single-chip computer that sits on every
server's motherboard (thanks, you idiots at Intel), I can't say it any
better than Dan Farmer:
  IPMI: Express Train to Hell, v2.0
  dan farmer/zen at trouble.org

@_date: 2013-10-28 00:20:58
@_author: John Gilmore 
@_subject: [Cryptography] [RNG] on RNGs, VM state, rollback, etc. 
What else is on your LAN besides a network switch?  Do you have a
printer with an Ethernet jack?  Or a DSL modem?  Or a cable modem?  Or
a low-end commercial NAS box?  All of these typically run an embedded
system using some old version of Linux, and never get their firmware
updated to close zero-day security holes.  If one of them can be taken
over, and can convince your network switch to send them all the
packets (perhaps by ARP flooding or ARP spoofing), then that embedded
system can wiretap any LAN transaction it likes.  Many DSL modems contain a small switch, which if it's the only switch
in a small home or office network, would make all packets among local
nodes accessible to malware running in that DSL modem.
Many cheap Ethernet switches are 'intelligent' meaning that they have
an embedded processor that offers a Web configuration interface.
Such devices are fertile malware targets.
Automated global-scale attacks against such embedded systems are
certainly feasible.  Could the injected code be sufficiently subtle to
detect and store or report entropy events like packet timing, without
becoming sufficiently obvious that the malware's presence is detected
on the network?
PS: On the "big iron" rather than "small network" end of things, don't
forget virtual machines, in which a compromised VM hypervisor has full
access to all the packets (and to many other aspects of the machines
running under it).

@_date: 2013-10-28 11:56:53
@_author: John Gilmore 
@_subject: [Cryptography] DSL modems - how would we detect wholesale 
And most DSL modems are provided by your giant telco DSL provider --
such as AT&T -- which we already know has a long history of covertly
sucking up to NSA.  Besides their longstanding cooperation on domestic
and foreign fiber taps, they also produced the first-and-only Clipper
Chip subverted "telephone security device" for making voice calls that
"nobody but NSA" could listen to.  How hard would it be, really, for
them to subvert all their DSL modems to wiretap your LAN?
And how would you know if they had done so?  It's so convenient that
all AT&T DSL modems have a high bandwidth upstream connection to
AT&T's central office switches.  And even better that consumers have
no idea what packets are going up and down over that DSL signalling,
because they have no equipment for monitoring raw 2-wire DSL lines
(the way they could fairly easily detect inappropriate packets
traveling on an Ethernet, with a little free software and a little
replugging of Ethernet equipment).
Your DSL modem could be doing its main job (carrying your external
Internet traffic) using whatever fraction of the available bandwith
that requires in each millisecond, and using any spare capacity on the
DSL wire to mirror a select fraction, or all, of your local LAN
traffic up to the central office switch.  The switch would nominally
discard this 'filler traffic' -- but AT&T would be able to copy it to
NSA upon request, either by individual targeting of particular
customers, or wholesale.  In the better subverted DSL modems, the
filler/tap traffic would be fully encrypted between the modem and the
switch, so that even if you got professional equipment for monitoring
the DSL wire back to the central office, all you would see is 'random'
filler packets all the time.
Suppose AT&T and NSA really had no interest in doing this to you --
unlikely, I know -- but the Chinese manufacturers of DSL modems did
have such an interest?  The threat model is very similar, except the
Chinese would have to subvert the AT&T central office switches
covertly, without AT&T's willing cooperation, to extract your LAN
traffic from them.
You can guard against this threat by only plugging one Ethernet jack
into your DSL modem, and having that lead directly to a Linux or BSD
gateway box that is under your own control.  That way, the DSL modem
has no physical access to the rest of your LAN, and you can monitor
the upstream Ethernet to make sure that the only packets going to the
DSL modem are those that you intended to go upstream.

@_date: 2013-09-06 10:27:58
@_author: John Gilmore 
@_subject: [Cryptography] IA side subverted by SIGINT side 
Then be "shocked, shocked" that the muscular exploitation side of an
intelligence agency would overrule the weak Information Assurance
side.  It happens over and over.
It even happens in companies that have no SIGINT side, like Crypto AG,
when somebody near the top is corrupted or blackmailed into submission.
As late as 1996, the National Academy of Sciences CRISIS panel was
tasked by the US Congress with coming up with a US crypto policy that
would be good for the whole nation, updating the previous policy that
was driven by spy agency and law enforcement excesses to sacrifice the
privacy and security of both people and companies.  After taking a
large variety of classified and unclassified input, the panel's
unanimous consensus suggested that everybody standardize on 56-bit
DES, which they KNEW was breakable.
Diffie, Hellman and Baran persuasively argued in the 1970s when DES
was up for standardization that a brute force DES cracker was
practical; they recommended longer keys than 56 bits.  See for example
this contemporaneous 1976 cassette recording / transcript:
  Subsequent papers in 1993 (Weiner, "Efficient DES Key Search") and in
1996 (Goldberg & Wagner, "Architectural Considerations for
Cryptanalytic Hardware") provided solid designs for brute-force DES
key crackers.  Numerous cryptographers and cypherpunks provided input
to the CRISIS panel as well.  They even cited these papers and input
on page 288 of their report.
I have never seen a subsequent accounting by the CRISIS panel members
for this obviously flawed recommendation.  It was rapidly obsoleted by
subsequent developments when in June 1997 Rocke Verser coordinated a
team to publicly crack DES by brute force in months; when in 1998 EFF
revealed its DES Cracker hardware that cost $250K and could crack DES
in a week; and when in 2000 the export regs were effectively removed
on any strength encryption in mass market and free software, a change
forced upon them by EFF's success in Dan Bernstein's First Amendment
The panel members included substantial information-assurance folks
like Marty Hellman and Peter Neumann, Lotus Notes creator Ray Ozzie,
and Willis Ware (an engineer on WW2 radars and the Johnniac, who later
spread computers throughout aviation design and the Air Force, ended
up at RAND, and served on the 1974 Privacy Act's Privacy Protection
Study Commission).  But several of those people (and others on the
panel such as Ann Caracristi, long-term NSA employee and 2-year deputy
director of NSA) also have a long history involved with classified
military work, which makes their publicly-uttered statements unlikely
to reflect their actual beliefs.
PS: The CRISIS panel also recommended that encryption of any strength
be exportable "if the proposed product user is willing to provide
access to decrypted information upon a legally authorized request".
They assumed the ongoing existence of a democratic civilian government
and a functioning independent court system in the United States -- an
assumption that is currently questionable.  I don't think the panel
foresaw that a single "legally authorized request" would come with a
gag order from a secret court, would purport to "target" a single
unnamed individual, but would nevertheless require that information
about every person making a phone call in the United States be turned
over to a classified government agency for permanent storage and
exploitation.  Nor did they see that the government they were part of
would be committing serious international war crimes including political
assassination, torture, indefinite detention without trial, and wars
of aggression, on an ongoing basis.  Either that, or maybe NSA
blackmailed the committee members into these recommendations, just as
J. Edgar Hoover blackmailed his way through 40 years of unchecked
power.  Trouble is, Hoover eventually had to die; NSA, not being
human, does not have that natural limit.

@_date: 2013-09-06 17:22:26
@_author: John Gilmore 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
Speaking as someone who followed the IPSEC IETF standards committee
pretty closely, while leading a group that tried to implement it and
make so usable that it would be used by default throughout the
Internet, I noticed some things:
  *  NSA employees participted throughout, and occupied leadership roles
     in the committee and among the editors of the documents
  *  Every once in a while, someone not an NSA employee, but who had
     longstanding ties to NSA, would make a suggestion that reduced
     privacy or security, but which seemed to make sense when viewed
     by people who didn't know much about crypto.  For example,      using the same IV (initialization vector) throughout a session,
     rather than making a new one for each packet.  Or, retaining a
     way to for this encryption protocol to specify that no encryption
     is to be applied.
  *  The resulting standard was incredibly complicated -- so complex
     that every real cryptographer who tried to analyze it threw up
     their hands and said, "We can't even begin to evaluate its
     security unless you simplify it radically".  See for example:
             That simplification never happened.
     The IPSEC standards also mandated support for the "null"
     encryption option (plaintext hiding in supposedly-encrypted
     packets), for 56-bit Single DES, and for the use of a 768-bit
     Diffie-Hellman group, all of which are insecure and each of which
     renders the protocol subject to downgrade attacks.
  *  The protocol had major deployment problems, largely resulting from
     changing the maximum segment size that could be passed through an
     IPSEC tunnel between end-nodes that did not know anything about
     IPSEC.  This made it unusable as a "drop-in" privacy improvement.
  *  Our team (FreeS/WAN) built the Linux implementation of IPSEC, but
     at least while I was involved in it, the packet processing code
     never became a default part of the Linux kernel, because of
     bullheadedness in the maintainer who managed that part of the
     kernel.  Instead he built a half-baked implementation that never
     worked.  I have no idea whether that bullheadedness was natural,
     or was enhanced or inspired by NSA or its stooges.
In other circumstances I also found situations where NSA employees
explicitly lied to standards committees, such as that for cellphone
encryption, telling them that if they merely debated an
actually-secure protocol, they would be violating the export control
laws unless they excluded all foreigners from the room (in an
international standards committee!).  The resulting paralysis is how
we ended up with encryption designed by a clueless Motorola employee

@_date: 2013-09-06 19:13:17
@_author: John Gilmore 
@_subject: [Cryptography] NSA hates sunshine 
Yeah, just like employees at big companies are "forbidden" to reveal
how they are collaborating with NSA.
Years ago I heard what happened when George Davida filed a patent on
something related to encryption, all the way back in 1978, and
eventually received a communication from the government telling him
that his patent was subject to patent secrecy, that it would never
issue, and that he could not even tell anyone that it had been
suppressed, nor could he ever tell anyone how his invention worked.
In theory, the law was all on the NSA's and the patent office's side.
But in fact, they were in a very weak position.
Instead of acquiescing, Davida shouted it to the housetops, engaged
the press and his university about censorship of academic freedom,
involved his Congressperson, etc.  Within months, the secrecy order
was rescinded.
NSA hates sunshine.  NSA secrecy relies on the cowardice of most
people.  Courage is all it takes to beat them.
If NSA tries to shut you up, just shine a lot of attention on their
attempt to shut you up.  Spread the information that they are trying
to suppress, far and wide.  Send copies to a dozen random post-office
boxes in different cities, asking the recipient to physically bring it
in to their local newspaper.  Leave your cellphone at home, then stash
copies in places that you don't frequent, so that government agents
can't come raid your house and office and steal all copies of what
they're trying to suppress.  In my case I posted something like this
(a suppressed paper by Ralph Merkle) to Usenet, and it was suddenly on
thousands of servers overnight.
NSA habitually decides that the publicity that their activities get
from any continued effort to suppress the information is FAR worse
than the damage caused by the initial release of the info.  Any
efforts they make to shut you up, prosecute you, jail you, etc give
you a perfect soapbox, and the attention of the news media and the
public.  Keep repeating the info, from your jail cell if necessary,
and you're likely to win.  Because if NSA relents, your revelations
become "last week's news" and get a lot less public attention.  When
NSA found out I had copies of an early encryption tutorial that they
considered classified (I was suing them under FOIA to get a copy, but
then found copies in a public library), they first tried to persuade
my lawyer to "bring in all the copies so we can secure them in a safe
place".  That's NSA-ese for "throw them down a deep hole where you'll
never see them again".  When we refused, and instead contacted the New
York Times, which printed a story about the attempted suppression, NSA
and DoJ buckled within one day.  (Indeed, the way I found out they had
suddenly declassified the document is that they called the NYT
reporter to tell him.  They never did tell me; I got the news from the
As part of suing the government, the Al Haramain foundation
accidentally received a government report making it clear that the
government had illegally wiretapped their phone calls.  They noticed
this but it took the government 60 days to notice.  Unfortunately,
instead of making hundreds of copies of the document, and spreading
them all over the world and to the press, they did what the government
asked, and destroyed all their copies of the document.  Once all
copies of the document were gone, NSA went to the court and claimed
first that the whole thing was a state secret and couldn't proceed,
and then second that the group didn't have any standing to challenge
the wiretaps in court because Al Haramain (now) had zero evidence that
the taps had even occurred.  The foundation and their lawyers have
literally spent years of work recovering from that one mistake, and
only the kind indulgence of a smarter than average judge enabled their
lawsuit to survive at all.  See this story by one of their lawyers:
  Don't make the same mistake when NSA, or their minions at the FBI or
FISA or DoJ come to threaten YOU to suppress information that came to you
through no fault of your own.

@_date: 2013-09-07 18:50:06
@_author: John Gilmore 
@_subject: [Cryptography] Opening Discussion: Speculation on "BULLRUN" 
DNSSEC authenticates keys that can be used to bootstrap
confidentiality.  And it does so in a globally distributed, high
performance, high reliability database that is still without peer in
the world.
It was never clear to me why DNSSEC took so long to deploy, though
there was one major moment at an IETF in which a member of the IESG
told me point blank that Jim Bidzos had made himself so hated that the
IETF would never approve a standard that required the use of the RSA
algorithm -- even despite a signed blanket license for use of RSA for
DNSSEC, and despite the expiration of the patent.  I thought it was an
extreme position, and it was very forcefully expressed -- but it was
apparently widely enough shared that the muckety-mucks did force the
standard to go back to the committee and have a second algorithm added
to it (which multiplied the interoperability issues considerably and
caused several years of further delay).
PS: My long-standing domain registrar (enom.com) STILL doesn't support
DNSSEC records -- which is why toad.com doesn't have DNSSEC
protection.  Can anybody recommend a good, cheap, reliable domain
registrar who DOES update their software to support standards from ten
years ago?

@_date: 2013-09-09 20:03:02
@_author: John Gilmore 
@_subject: [Cryptography] Points of compromise 
By the way, it was a very interesting exercise to actually write out
on graph paper the bytes that would be sent in a TLS exchange.  I did
this with Paul Wouters while working on how to embed raw keys in TLS
(that would be authenticated from outside TLS, such as via DNSSEC).
Or, print out a captured TLS packet exchange, and try to sketch around
it what each bit/byte is for.  The TLS RFCs, unlike most Jon Postel
style RFCs, never show you the bytes -- they use a "high level
description" with separate rules for encoding those descriptions on
the wire.
There is a LOT of known plaintext in every exchange!
Known plaintext isn't the end of the world.  But it makes a great crib
for cryptanalysts who have some other angle to attack the system with.
Systems with more known plaintext are easier to exploit than those
with less.  Is that why TLS has more known plaintext than average?
Only the NSA knows for sure.

@_date: 2013-09-11 13:20:13
@_author: John Gilmore 
@_subject: [Cryptography] Laws and cryptography 
There is a tension between fundamental freedoms and crypto controls.
Often fundamental freedoms win (as they should).  The Wassenaar
Arrangement is a private agreement among a bunch of governments -- it
is not a treaty -- and has no legal force at all.  What matters are
the statutes in your own country, and how they are interpreted.
I don't know of any cryptographers who have been punished under crypto
export controls, anywhere in the world, for publishing papers about
encryption.  So invent your own cryptosystem if you want, write about
it, and publish!
Human-written software was considered to be different from
human-written papers for a while; in the US it took three court cases
(Bernstein v. US being the first winner) to sort this out.  In the
1990s, Europe did not control freely published ("mass-market and
public-domain") software, and by 2000 that was true in the US also.
Unless you want to find and pay a lawyer with relevant expertise, the
best way to get a more-or-less definitive answer for your particular
country is to look in Bert-Jaap Koops' "Crypto Law Survey".  He has
been maintaining it for decades, and actually did his PhD thesis on
global regulations about encryption.  See:
  Can't answer, or won't?  In the United States, both the NSA and the
agencies responsible for the export controls (State Department and
Commerce Department) have been known to lie to the public,
unofficially, about what is actually allowed.  Their tendency is to
talk you into assuming that you have no rights, even if the law is
clear that you do.  Or they will tie you up in knots over how you
might be able to comply with finicky regulations, without ever telling
you that you are exempt from those regulations.  We even caught them
lying officially once or twice (e.g. refusing export of Kerberos
authentication software on the bogus theory that someone, someday,
might adapt it to do encryption).

@_date: 2013-09-11 20:00:06
@_author: John Gilmore 
@_subject: [Cryptography] Perfection versus Forward Secrecy 
There doesn't seem to be much downside to just calling it "Forward
Secrecy" rather than "Perfect Forward Secrecy".  We all seem to agree
that it isn't perfect, and that it is a step forward in security, at a
moderate cost in latency and performance.

@_date: 2013-09-11 21:23:29
@_author: John Gilmore 
@_subject: [Cryptography] Matthew Green on BULLRUN: briefly censored 
Johns Hopkins University censored this exact blog post by Prof. Green,
because of a complaint from its local defense contractor affiliated
with NSA, the Applied Physics Laboratory
The university gets slight credit for backtracking one day after the
censorship story hit Twitter and the press.  So the blog post is now
back (and is still worth reading).
Here's the story:
        Now, why is it that so many folks with links to NSA think like
totalitarians?  It's wonderful seeing them crawl out of the woodwork
and try to give orders to the public about what it is allowed to
think, what it is allowed to read, and what it is allowed to write.
It's only wonderful because the huge public counter-reaction protects
us -- the totalitarians reveal their true colors, but they don't
actually get to tell us what to do.  Thank you, fellow denizens of the
world, for creating your own freedom, by making a lot of noise when
some NSA-affiliated idiot tries to take it away.
PS: How much NSA tax money does JHU's Applied Physics Lab get?  I don't
know, but here's a guy on LinkedIn who worked at NSA in the past,
works at the Lab today, and brags that he's managing a $120M contract
from NSA:

@_date: 2013-09-14 20:37:07
@_author: John Gilmore 
@_subject: [Cryptography] A lot to learn from "Business Records FISA NSA 
This is one of the documents that an EFF Freedom of Information
lawsuit asked for.  The government had been claiming they could not
release ANY FISA court orders or submissions.  When the President
ordered the intelligence community to declassify more info in order to
present a fuller picture of the issues that Edward Snowden's leaked
documents raised, they went back through all the relevant documents
and, last week, released hundreds of pages in a rough dozen documents,
that they had initially claimed were exempt.  I read this document the
other night and learned a lot.  I encourage y'all to read it -- and
other recently released documents.
These are not "leaked" documents from Mr. Snowden.  These are
officially released documents from the NSA and Department of Justice.
While their choice of "what to release and what to black out" may have
been self-serving, the documents themselves are real and official.
They candidly describe a particular part of NSA's internal operations
that relate to the telephone metadata collected about on everyone in
the US.
Their main goal in writing this document was to convince the FISA
court (which had ordered them in 2009 to stop accessing the telephone
metadata after NSA told the court that some of it had leaked outside
the boundaries of the FISA court order) that they had their processes
in hand and that the court should let them go back to accessing the
Their main goal in declassifying it is, I believe, to convince the public that they are being very diligent to the court's orders and
to the limits that the court places on them.  And to detail all the
internal restrictions, checks and balances that they go through
while collecting, processing, accessing and releasing this telephone
metadata.  To show "the whole elephant".
And to that extent, they succeeded, both with the court and with me.
(However, I think the secret court made a fundamental error in ruling
that collecting info about everybody's phone calls is "relevant" to
any particular terrorism investigation.  That assertion reminds me of
John Yoo's since-repudiated assertions from the early Bush days, like
"it isn't torture unless you really intend to cause great bodily
injury" and "the President has inherent power to do anything he
wants".  When you start from a severely false premise, you can go a
long way into the wilderness before you notice your error.)
What NSA and DCI and DoJ also revealed, around the edges of this
document, is a lot of small bits of information about how the NSA
technical and managerial infrastructure works.  Much of this is
information that we "already knew", or could have guessed based
on already existing information, but some of it is not.  This document
supplies some context that help to fit the puzzle pieces together.
Things I learned there include:
  *  NSA's internal infrastructure runs on Unix.  (Linux is a branch of
     Unix.)  Their analysts log in to Unix machines with logins and
     passwords, as we do, and they use the standard Unix/Linux file
     access controls ("user, group, and other" permissions).
  *  They use web servers and web browsers and HTML and URLs to deliver
     their data to their "customers" at the FBI, CIA, and NCTC.
  *  NSANET, their internal Internet, is not encrypted!  (It is almost
     certainly protected by link encryption and fiber signal strength
     monitoring when it crosses from one place to another, but not
     inside their secured buildings.)  It's just a bunch of machines
     plugged into Ethernets, running standard protocols, like what all
     of our infrastructure uses.
  *  I'm guessing the reason NSANET isn't encrypted is because they
     don't seem to have any better encryption protocols for general
     use inside NSA than we do outside.  E.g. they don't seem to have
     automatic end-to-end encryption.  So in order to be able to buy
     standard machines and plug them in and use them, they have to run
     their whole net unencrypted.  (I think it's funny that because my
     old effort to embed automatic Opportunistic Encryption in Linux and
     IPSEC failed, therefore NSA's internal network isn't encrypted.
     Like they couldn't do it themselves!)
  *  They use a "PKI" (public key infrastructure) to control access to
     some databases inside NSANET.  When they wanted to stop one part of
     NSA's tech infrastructure from accessing the telephone metadata,
     they removed the "certificate" that gave it access credentials.
In other words, when it comes to general purpose computing, they are
running on almost exactly the same kind of infrastructure we are --
nothing better.  This makes sense, but I had expected that with
billions of tax dollars every year they had made some improvements in
the security, authenticity and integrity of their protocols and
software.  (But, I worked at Sun, which spent billions of dollars a
year on engineering their hardware and software, and Sun's machines
weren't much better than their competitors' at security, authenticity
or integrity either.)  We in the outside world *invented* all of NSA's
infrastructure.  They buy it from us, and are just "users" like most
computer users.  (Yes, they have programmers and they write code, but
their code seems mostly applications, not lower level OS improvements
or protocols.  I'm not talking about the parts of NSA that find
security holes in other peoples' infrastructure, nor the malware
So go read the document anyway!  Don't believe what I tell you...
draw your own conclusions.
Also it seems that:
  *  The vast majority of the information that they are squirting
     around inside NSA, searching and correlating, comes with no
     particular restrictions other than those that they impose
     internally (like not revealing things that disclose their sources
     and methods) and the general restrictions on releasing
     information about US persons.  They got that data "legally", or
     anyway, "fair and square", by stealing it from signals in other
     countries, and they can do what they want with it.  Having to
     deal with a judge who can put arbitrary restrictions on what they
     can do with a large database is a novel experience for them, and
     one that neither their personnel nor their infrastructure is
     properly set up to handle.  That's why they found that data was
     "leaking" from the telephone metadata database nine or ten ways
     that they hadn't yet noticed until they did an end-to-end review.
     The leaks were mostly fairly minor, but if they hadn't been
     forced to do the review, it's clear that more and more of NSA
     would have just been treating the telephone metadata like any
     other piece of stolen data.
  *  Their "need to know" culture and the maze of classifications and
     code words often prevents the right hand from knowing what the
     left hand is doing.  This is deliberate and is to help figure out
     who the insider threats ("moles") are, based on who had access to
     what info before it leaked outside NSA.  But the result is also
     that nobody is really in charge.  There are too many details that
     don't percolate up and down the chain of command, so stuff
     happens that isn't supposed to happen.  Like, the programmers who
     wrote the code for accessing the stored database of telephone
     metadata knew that it could only be accessed with a search term
     ("selector") that met the court's standard for "RAS" ("Reasonable
     Articulable Suspicion"), so they coded the software to check for
     that.  But the separate programmers who wrote the code for
     IMPORTING new data into the database from the telcos, didn't know
     that, so they wrote an "Alert list" (renamed "Activity Detection
     List" during the review) that would send a note to an analyst
     whenever new data came in for any selector on the list (e.g. when
     someone of interest to that analyst made a phone call).  These
     selectors were not restricted to those that met the court's
     standards, and indeed most of the selectors on the list did NOT
     meet the standard (it had 1,935 RAS approved selectors and 15,900
     unapproved ones).  This is not because they tried to get around
     the court -- but because they were not in control of their own
     infrastructure, because of lack of internal sharing of relevant
     information.  Free cultures really do outperform authoritarian
     ones!
This is all useful information.  I recommend that folks also read other
documents that came out of that FOIA case -- there are about a dozen,
all listed on the EFF web site here:
  In that list, this one is called "June 25, 2009 -- Implementation of
the Foreign Intelligence Surveillance Court Authorized Business
Records FISA".

@_date: 2013-09-17 14:56:05
@_author: John Gilmore 
@_subject: [Cryptography] An NSA mathematician shares his from-the-trenches 
Forwarded-By: David Farber Forwarded-By: "Annie I. Anton Ph.D." NSA cryptanalyst: We, too, are Americans
Summary: ZDNet Exclusive: An NSA mathematician shares his from-the-trenches view of the agency's surveillance activities.
By David Gewirtz for ZDNet Government |	September 16, 2013 -- 12:07 GMT (05:07 PDT)
An NSA mathematician, seeking to help shape the ongoing debate about the agency's foreign surveillance activities, has contributed this column to ZDNet Government. The author, Roger Barkan, also appeared in last year's National Geographic Channel special about the National Security Agency.
The rest of this article contains Roger's words only, edited simply for formatting.
Many voices -- from those in the White House to others at my local coffee shop -- have weighed in on NSA's surveillance programs, which have recently been disclosed by the media.
As someone deep in the trenches of NSA, where I work on a daily basis with data acquired from these programs, I, too, feel compelled to raise my voice. Do I, as an American, have any concerns about whether the NSA is illegally or surreptitiously targeting or tracking the communications of other Americans?
The answer is emphatically, "No."
NSA produces foreign intelligence for the benefit and defense of our nation. Analysts are not free to wander through all of NSA's collected data willy-nilly, snooping into any communication they please. Rather, analysts' activity is carefully monitored, recorded, and reviewed to ensure that every use of data serves a legitimate foreign intelligence purpose.
We're not watching you. We're the ones being watched.
Further, NSA's systems are built with several layers of checks and redundancy to ensure that data are not accessed by analysts outside of approved and monitored channels. When even the tiniest analyst error is detected, it is immediately and forthrightly addressed and reported internally and then to NSA's external overseers. Given the mountains of paperwork that the incident reporting process entails, you can be assured that those of us who design and operate these systems are extremely motivated to make sure that mistakes happen as rarely as possible!
A myth that truly bewilders me is the notion that the NSA could or would spend time looking into the communications of ordinary Americans. Even if such looking were not illegal or very dangerous to execute within our systems, given the monitoring of our activities, it would not in any way advance our mission. We have more than enough to keep track of -- people who are actively planning to do harm to American citizens and interests -- than to even consider spending time reading recipes that your mother emails you.
There's no doubt about it: We all live in a new world of Big Data.
Much of the focus of the public debate thus far has been on the amount of data that NSA has access to, which I feel misses the critical point. In today's digital society, the Big Data genie is out of the bottle. Every day, more personal data become available to individuals, corporations, and the government. What matters are the rules that govern how NSA uses this data, and the multiple oversight and compliance efforts that keep us consistent with those rules. I have not only seen but also experienced firsthand, on a daily basis, that these rules and the oversight and compliance practices are stringent. And they work to protect the privacy rights of all Americans.
Like President Obama, my Commander-in-Chief, I welcome increased public scrutiny of NSA's intelligence-gathering activities. The President has said that we can and will go further to publicize more information about NSA's operating principles and oversight methodologies. I have every confidence that when this is done, the American people will see what I have seen: that the NSA conducts its work with an uncompromising respect for the rules -- the laws, executive orders, and judicial orders under which we operate.
As this national dialogue continues, I look to the American people to reach a consensus on the desired scope of U.S. intelligence activities. If it is determined that the rules should be changed or updated, we at NSA would faithfully and effectively adapt. My NSA colleagues and I stand ready to continue to defend this nation using only the tools that we are authorized to use and in the specific ways that we are authorized to use them. We wouldn't want it any other way.
We never forget that we, too, are Americans.
Roger Barkan, a Harvard-trained mathematician, has worked as an NSA cryptanalyst since 2002. The views and opinions expressed herein are those of the author and do not necessarily reflect those of the National Security Agency/Central Security Service.

@_date: 2013-09-17 16:50:38
@_author: John Gilmore 
@_subject: [Cryptography] Gilmore response to NSA mathematician's "make rules 
Re: In his Big Data argument, NSA analyst Roger Barkan carefully
skips over the question of what rules there should be for government
*collecting* big data, claiming that "what matters" are the rules for
how the data is used, *after* assuming that it will be collected.
Governments seldom lose powers; they work to grow their powers, to
loosen the rules that govern what they can do.  NSA's metadata
database has fewer restrictions today than it did when it was
collected, all carefully "legal" and vetted by a unaccountable
bureacracy that has its own best interests at heart.  My own Senator
Feinstein claims from her "oversight" post that whatever's good for
NSA is good for America; my Congresswoman Pelosi worked hard to defeat
the bill that would have stopped the NSA phone metadata program in
its tracks; and both of them run political machines that have made
them "lifetime" congresspeople, no matter how out-of-step they are
with their constituents.  NSA and these overseers conspired to keep
the whole thing secret, not to avoid "tipping off the terrorists" who
already knew NSA was lawless, but to avoid the public backlash that
would reduce their powers and maybe even reverse a decade of hugely
growing secret budgets.
Having watched the Drug War over the last 50 years, NSA for 30 years,
and TSA/DHS over the last decade, I have zero faith that NSA can
collect intimite data about every person in America and on the planet,
and then never use that data for any purpose that is counter to the
interest of the people surveilled.  There will always be
"emergencies", always "crises", always "evildoers", always
"opportunities", that would be relieved "if we could just do X that
wasn't allowed until now".  So what if general warrants are explicitly
forbidden?  And if searching people without cause is prohibited?  We
could catch two alleged terrorists -- or a few thousand people with
sexual images -- or 750,000 pot smokers -- or 400,000 hard-working
Mexican migrants -- every year, if we just use tricky legalisms to
ignore those pesky rules.  So the government does ignore them.  Will
you or your loved ones fall into the next witchhunt?  Our largest city
was just found guilty of forcibly stopping and physically searching
hundreds of thousands of black and latino people without cause for a
decade -- a racist program defended both before and after the verdict
by the Mayor, the Police Commission, the City Council, and state
legislators.  NSA has secretly been doing warrantless, suspicionless,
non-physical searches on every American with a phone for a decade, all
using secret gerrymandered catch-22 loopholes in the published
constitution and laws, defended before and after by the President, the
Congress and all the courts.  Make rules for NSA?  We already have
published rules for NSA and it doesn't follow them today!
So Mr Barkan moves on to why NSA would never work against the
citizens.  The US imprisons more people than any country on earth, and
murders far more than most, but it's all OK because those poor,
overworked, rule-bound government employees who are doing it are
"defending freedom".  Bullshit they are!  Somehow scores of countries
have found freedom without descending to this level of lawlessness and
repression.  NSA cannot operate outside of this context; rules that
might work in a hypothetical honest and free government, will not work
in the corrupt and lawless government that we have in the United
NSA employees are accountable for following the rules, Mr. Barkan?
Don't make me laugh.  There's a word for it: impunity.  EFF has
diligently pursued NSA in court for most of a decade, and has still
gotten no court to even consider the question "is what NSA did legal?"
Other agencies like DoJ and HHS regularly retain big powers and
budgets by officially lying about whether marijuana has any medical
uses, rather than following the statutes, despite millions of
Americans who use it on the advice of their doctor.  None of these
officials lose their jobs.  Find me a senior federal official anywhere
who has ever lost their job over major malfeasance like wiretapping,
torture, kidnapping, indefinite imprisonment, assassination, or
malicious use of power -- let alone been prosecuted or imprisoned for
it.  Innocent citizens go to prison all the time, from neighborhood
blacks to medical marijuana gardeners to Tommy Chong and Martha
Stewart -- high officials never.
Re Big Data: I have never seen data that could be abused by someone
who didn't have a copy of it.  My first line of defense of privacy is
to deny copies of that data to those who would collect it and later
use it against me.  This is exactly the policy that NSA supposedly has
to follow, according to the published laws and Executive Orders: to
prevent abuses against Americans, don't collect against Americans.
It's a good first step.  NSA is not following that policy.
Where Big Data collection is voluntary, I do not volunteer, thus I
don't use Facebook, Google, etc.  When collection is involuntary, like
with NSA's Big Data, I work to limit their power, both to collect, and
to use; and then I don't believe they will follow the rules anyway,
because of all the historical evidence.  So I arrange my life to not
leave a big data trail: I don't use ATMs, I pay with cash, don't carry
identification, don't use Apple or Google or Microsoft products, etc.
Your government will not make a big announcement when it has become a
police state.  So if you're a patriot, you'd better practice now: how
to avoid stupid mistakes that would let a police state catch you when
telling the truth to your fellow citizens becomes a crime -- like it
did for Mr. Snowden, Ms. Manning, Mr. Ellsberg, Mr. Nacchio,
Mr. Assange, and Ms. Mayer (who claims she's been dragged silently
kicking and screaming to spy on her customers rather than be
prosecuted for telling them the truth).  NSA and its Big Data will not
be defending you when the secret police come to bust you for
publishing secrets.  NSA will be on the cops' and prosecutors' side.
They have recently filed legal memos declaring that they don't have to
help the defense side in any criminal trials, even when NSA has
exculpatory data, and even when NSA provided wiretapped Big Data that
led the prosecutors to you.  Defending the citizens from the excesses
of government isn't their job.  Defending their turf, their budget,
and their powers is their job.

@_date: 2013-09-17 17:01:33
@_author: John Gilmore 
@_subject: [Cryptography] An NSA mathematician shares his 
Techdirt takes apart his statement here:
    NSA Needs To Give Its Rank-and-File New Talking Points Defending
  Surveillance; The Old Ones Are Stale
  from the that's-not-really-going-to-cut-it dept
  by Mike Masnick, Tue, Sep 17th 2013
  It would appear that the NSA's latest PR trick is to get out beyond
  the top brass -- James Clapper, Keith Alexander, Michael Hayden and
  Robert Litt haven't exactly been doing the NSA any favors on the PR
  front lately -- and get some commentary from "the rank and file."
  ZDNet apparently agreed to publish a piece from NSA mathemetician/
  cryptanalyst Roger Barkan in which he defends the NSA using a bunch
  of already debunked talking points. What's funny is that many of
  these were the talking points that the NSA first tried out back in
  June and were quickly shown to be untrue. However, let's take a
  look. It's not that Barkan is directly lying... it's just that he's
  setting up strawmen to knock down at a record pace.

@_date: 2013-09-17 18:02:27
@_author: John Gilmore 
@_subject: [Cryptography] FISA court releases its "Primary Order" re telephone 
The FISA court has a web site (newly, this year):
  Today they released a "Memorandum Opinion and Primary Order" in case BR 13-109 ("Business Records, 2013, case 109"), which lays
out the legal reasoning behind ordering several telephone companies
to prospectively give NSA the calling records of every subscriber.
That document is here:
  I am still reading it...

@_date: 2013-09-27 00:59:15
@_author: John Gilmore 
@_subject: [Cryptography] RSA equivalent key length/strength 
Can the client recover and do something useful when the server has a
buggy (key length limited) implementation?  If so, a new cipher suite
ID is not needed, and both clients and servers can upgrade asynchronously,
getting better protection when both sides of a given connection are
running the new code.
In the case of (2) I hope you mean "yes we really do PFS with an
unlimited number of bits".  1025, 2048, as well as 16000 bits should work.

@_date: 2014-04-13 14:12:40
@_author: John Gilmore 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
No.  BitTorrent was rehosted on UDP, using its own delay-sensitive end
to end transmission protocol, and it works fine (and better than when
it was using TCP).  RTP (VoIP) also works pretty well on UDP.  Not to
mention the classic use of UDP, DNS.
See in general
 for why
TCP clogs the queues at routers.  The very short summary is that TCP
only throttles back after a packet is dropped, but in the last 20
years everyone has been adding RAM buffers to routers so that they
would never drop a packet.  The right cure is to fix TCP so it
throttles back when it notices packet delay, but nobody's
doing that because fixed-TCP would perform "worse" on single
connections than unfixed-TCP, so instead they're moving off TCP
to their own protocols that do the same.

@_date: 2014-04-15 16:29:26
@_author: John Gilmore 
@_subject: [Cryptography] I don't get it. 
What is this, Marxist development?  While we're at it, let's shoot the
guys who advocate shooting gcc developers too.  ;-?
I think we need a better email system.  Better in the sense that it
will only communicate secure ideas.  And which can prevent writers
from spreading bad ideas.
The job of a compiler is to make the machine do what the program says
to do.  It is not to "understand" the program or to decide whether it
is a "good" program.  It is not to second-guess the programmer about
what the program says to do.  I'm sorry, Mr. Fairbrother, that the GCC
team built a great compiler that does what it's supposed to do.  Feel
free to stop using it anytime you like.
And feel free to write a better compiler.  We did, you can too.  You
can even start with ours and improve it to be better.  Which
improvement are you going to make first?  You could start by adding a
-Wsecurity option that programmers could use to more strictly diagnose
code that is part of a security system.  What particular "top five or
ten" checks would you add?
It is clear that you are no expert in bugs.
GCC goes substantially further than most compilers in diagnosing
dubious programmer instructions.  Besides the errors and warnings that
it catches by default, it lets you turn on a broad range of other
warnings.  In projects that have eliminated all such warnings, gcc
also lets you make any new warning into an error that will force the
code to be looked at and revised by the programmer.  But you can never
make such things foolproof, because the world is always producing new
and better fools.
Disclaimer: I wrote tiny amounts of code in GCC, and co-founded and
co-managed the business (Cygnus Support) that brought GCC from working
well on a few architectures, to working reliably on a dozen
architectures.  Throughout the 1990s, we raised millions of dollars a
year from customers, and spent it on making better free-as-in-freedom
compilers for them and for everyone.  So shoot me, too.

@_date: 2014-04-17 17:46:39
@_author: John Gilmore 
@_subject: [Cryptography] bounded pointers in C 
This is fixed in ANSI C.  The language definition does not require
implementations to check pointer or array bounds -- but everything in
the language that prevented implementations from doing it was removed.
Accessing outside the legitimate boundaries of the object is
"undefined" in the language, and the compiler can do what it wants
when that happens, including make the program die instantly.
We have had many such implementations over the years -- but none of
them have become popular, or even standard.
A "bounded pointers" implementation of gcc was built in the late
1990s-2000 by Greg McGary (see
  It worked well
enough to compile and run GNU libc and the GNU textutils and
fileutils.  This was some sort of official GCC project, even.  But
apparently it was never adopted into mainline gcc -- I don't know why
not.  See  ,
 ,
He seems to be greg at mcgary.org and was contributing to GCC mailing
lists as late as 2009.  Apparently the code lived on a branch
of the GCC source tree ("bounded-pointers-branch") so that branch
may still be accessible somewhere.  Indeed, doing this command:
  svn co svn://gcc.gnu.org/svn/gcc/branches/bounded-pointers-branch
seems to have brought me some code, which might be Gary's latest code,
or might not be.  The last changes in it seem to be from about 2000,
according to the ChangeLog.  But I can't get it to build -- the
configuration files seem messed up.
Others built similar but less straightforward bounds checkers, like
"Mudflap" ( but
apparently that isn't popular, standard, or available in any public
GCC release either.
And many other C environments, such as "Saber C" and "Purify" have
done similar checks.  It isn't rocket science.  It works.
Actually building and maintaining C implementations that check pointer
boundaries seems to require a level of intense desire that so far
*nobody* has created or sustained for more than a year or two.

@_date: 2014-04-22 11:17:16
@_author: John Gilmore 
@_subject: [Cryptography] It's all K&R's fault 
You would think so, and so would I, but as usual in security software,
we would be wrong.  There are key management problems in making the
swap partition usable by the BIOS, by subsequently booted copies of
this OS or different OSes, etc:
  "Encrypted swap no longer mounted at bootup"
    "Reinstalling over a previous installation with encrypted swap displays a "Continue without swap" warning dialog"
    "encrypted swap reused from previous install when wiping and reinstalling"
    "Do not offer hibernate with encrypted swap"
    "gnome-power-manager hibernates even when using encrypted swap"

@_date: 2014-04-27 19:17:26
@_author: John Gilmore 
@_subject: [Cryptography] GCC bug 30475 (was Re: bounded pointers in C) 
You might well be correct about the law.  But you are not describing
the fact situation that this thread is about.
Telling the GCC developers that "that guy over there wrote code whose
security checks get skipped because the language standard that GCC
implements doesn't define the behavior of the way that guy wrote those
checks" is not "a clear security issue brought to the [gcc]
developer's attention".
Under this theory, the committee of 50+ people who contributed wording
to the C Language Standard(s) are also liable for damage caused by
every security bug that resulted from people depending on behavior
that the standard did not define.  In this theory of liability, theirs
would be an error of omission (they did not define the behavior of
integer arithmetic in C with big numbers, therefore they are liable
because some idiot ten years later wrote security sensitive code that
used big numbers?).
Basically, nobody's forcing you to use this software (or this
implementation language).  You got it for free, probably without
having *any* direct interaction with the developers.  In effect, you
copied it from a library, like xeroxing a public domain book, or
building a personal copy of a gadget by getting the drawings from the
patent office.  If you don't like it, don't use it.  Oh, hypothetical
lawsuit filer, you're claiming that *someone else* somewhere on the
Internet used it and you were injured thereby?  And you don't even
have a contract with that someone else (e.g. Google, Facebook), nor
any economic relationship with them?  Your claim is even more tenuous.
PS:  Lawyer a not am I.  And if I was, I would be charging you for
this advice (while disclaiming any damages you might incur by listening
to it or following it :-).

@_date: 2014-08-12 09:50:47
@_author: John Gilmore 
@_subject: [Cryptography] Dumb question -> 3AES? 
Are you losing your memory?  Enigma?  Purple?  DES?  MD4?  MD5?
(I admit that DES was deliberately weakened by NSA to make it
crackable -- but what other big-name crypto algorithms do we use, that
may also have that characteristic?)
Sometimes you have different motives than other people who spend all
their days working on crypto.
For example, many people in cryptography don't seem to think about
developing societal resistance to mass surveillance attacks; they
focus their efforts on preventing targeted attacks.  I have been
advocating that in RSA key generation, we should randomize not only
the key, but the number of bits in the key (within safe and computable
limits).  This is because the current over-dependence on 1024-bit keys
is a magnet for some large corrupt overfunded agency to build a brute
force 1024-bit factoring machine.  If instead society was actively
using a broad range of key sizes between 1024 and 4800 bits, a
1024-bit RSA cracker would only get them <5% of the keys.  And
building a much more expensive 1100-bit RSA cracker would only get
them <6% of the keys, etc.  Today if they can build a 999-bit RSA
cracker, they won't waste their money there, because they know the
payoff is nil; but they'll strain their ingenuity and budgets to get
to 1024 bits, whereupon they can crack 95% of the RSA keys in actual
So why doesn't our popular RSA-based software randomize its key
lengths at key generation time?  It's a matter of where the designers
and maintainers have focused.  Diversity of focus can be useful
against wily adversaries.

@_date: 2014-02-02 19:34:47
@_author: John Gilmore 
@_subject: [Cryptography] cheap sources of entropy 
So, if an attacker running malware in a hypervisor (or SMM) knew you
were depending on disk drive timings for the random numbers that
create your encryption keys, how easily could they attack you by
rigidizing those interrupt timings, e.g. delaying your virtual machine
interrupts at to the next even 1/60th of a second?
How much easier would this be if they could read the source code for
your "extract entropy from disk drive timings" code, and even adapt
their malware's behavior to various versions of that widely deployed code?

@_date: 2014-02-07 15:18:49
@_author: John Gilmore 
@_subject: [Cryptography] RNG exploits are stealthy 
Because this attack is stealthy.  Rigidized interrupt timing is
invisible to the users, invisible to the sysadmin, barely visible to
the running OS, and not specific to the OS running under the VM or
SMM.  It generates no Internet traffic -- at all.  It works with each
new operating system release.  Yet it could allow a remote attacker
halfway across the net -- like NSA -- do a successful brute-force
search for keys generated from that interrupt timing.
Such a stealth exploit could survive for a long time, deployed on
millions or billions of machines, without ever being detected for what
it is: an attack on a random number generator.
It's the sort of thing I'd expect NSA to be doing, particularly when
they want it to "not be attributable to NSA" when eventually discovered.
On the other hand, code in a VM hypervisor or SMM BIOS that extracts
crypto keys from an OS would have to report those keys to its eeevil
masters somehow.  And even if clever programmers manage to write code
that doesn't need to know the internals of the guest OS, by
e.g. "looking for DRAM containing high entropy", then encrypting
that and sneaking it out in spare bytes of existing packets, anyone
who later discovers that code will have a pretty good idea what it
is doing and who it is doing it for.

@_date: 2014-02-18 01:30:50
@_author: John Gilmore 
@_subject: [Cryptography] Overzealous encoding 
It's instructive to print out some hexadecimal TLS packet traces and
then take a copy of the TLS RFC to try to understand what they
represent.  Paul Wouters and I did this while working on the TLS
out-of-band public key extension.  It's worth doing even one packet,
or two.  Sharpen your pencil and try it!
The first thing you'll notice is that the TLS RFC (2246) is written in
a "high level C-like language" style.  It lacks those simple to
understand ASCII-art octet boxes that appear in all of Jon Postel's
RFCs (for IP and TCP and such).  In other words, the main text of the
RFC doesn't show you ANYTHING about the encoding on the wire.  As in
ASN.1, that info is buried.  In TLS it interleaved in the
definition/description of the "presentation language" in section 4.
The second thing you'll notice is that there is an awful lot of
known plaintext in the packet traces.  As you decode the bytes,
a bunch of them will be totally predictable (like length values
and algorithm specifiers and version numbers).
As in the X.500 example Peter gave, the presentation language hides so
much information about the actual packet encodings that the designer
can get waylaid into sending dozens of bytes of predictable and
useless plaintext without even knowing it.  That tends to make Eve and
Mallory smile.

@_date: 2014-01-01 11:25:36
@_author: John Gilmore 
@_subject: [Cryptography] What is a secure conversation? (Was: online 
You are talking about two different things.
The "bomb design codes" are software programs built to simulate
nuclear reactions in particular physical constructions.  These have
the original Manhattan Project calculations (done on punched cards,
with tabulators, as described in Feynmann's autobiography) as remote
The "nuclear codes" ("launch codes") are a short sequence that arms the
missiles before sending them into the sky.  The "key" of a security
system that keeps particular bombs from exploding.
Both of them need a lot of other infrastructure to be useful to an
adversary.  Launch codes from the '60s are probably declassified now
anyway.  Nuclear simulation software in FORTRAN from the '60s would
still be useful in designing nuclear bombs.  (Actually building them,
getting them to targets, and detonating them would be big challenges.)

@_date: 2014-01-02 22:01:49
@_author: John Gilmore 
@_subject: [Cryptography] nuclear arming codes 
By the way, this topic IS relevant to cryptography.  Gustavus Simmons,
 , cryptographer at
Sandia Labs and co-founder of the IACR, was involved in the creation
of the Permissive Action Links (PALs) that prevent the bombs from
arming unless they receive the right launch code.
In fact there's an allegation that public-key crypto was invented for the PALs, before the Stanford crowd did it:
  PS: Gus Simmons was also key to making the test-ban treaties work, by
providing cryptographic protocols that allowed sensors to be placed in
each others' countries, that would report back only what the treaty
allowed them to report, with no covert channels for additional
information, and verification that the sensor packages had not been
tampered with.

@_date: 2014-01-06 12:13:22
@_author: John Gilmore 
@_subject: [Cryptography] defaults, black boxes, APIs, 
This has already been done.  No change to the C language or libraries
is required; the ANSI C committee was diligent in defining the
language to only work when your reach didn't exceed your grasp.
"Saber C" and "valgrind" already implement this.  Saber C is now known
as CodeCenter, and its C++ variant is ObjectCenter.  It is a
commercial product of Integrated Computer Solutions, which bought it
from Centerline Software and now seems to have stuck it on a shelf:
      You will need rules.  If only that "the awarding of the prize will
be at the entire discretion of XXXX".  Else we'd just be handing
the prize to a twenty year old compiler (CodeCenter) that's sitting
on a dusty shelf without anyone using it.
I may know where one can be found, for a good competition.

@_date: 2014-07-14 16:30:23
@_author: John Gilmore 
@_subject: [Cryptography] Security clearances and FOSS encryption? 
Personally I take every statement by a known security clearance holder
with a grain of salt.  Prominent holders of such clearances have been
caught in major lies ("We do not hold files on hundreds of millions of
Americans") and when caught, they justify themself by claiming that
they were forced to lie by the security rules, rather than claiming
that they are just inveterate liars who were hired by the security
services BECAUSE they were good at lying.  They are specifically
trained in security-mindedness seminars to manipulate the public
perception of what they do -- by withholding relevant information at a
minimum, by distorting when information must be given ("I work for the
Defense Department" for NSA employees -- see Cryptome's copy of the
NSA employee manual), and by lying when neither of the above will
I have personally been told some bits of information when they weren't
classified, by someone with a security clearance, and then later been
told, "Some of what I told you earlier has become classified so I
can't talk about it any more, nor can I tell you which part."  In my
personal opinion, knowing the content of our discussion, this
information was classified in order to cover up extremely high level
malfeasance, and/or to manipulate public opinion to be more favorable
to the US Government than it would be if the truth was known.  Neither
of these is a valid or authorized reason to classify.  But that never
matters to those who classify.  A decade later, that information is
still putatively classified.  The official investigation of the
incident by a responsible agency was stopped, and replaced with an FBI
"investigation" the results of which are still classified and unreleased.
I think your comment was about the old FBI, whose jobs were blackmail,
theft, ruthless suppression of deviants and minorities, and
whitewashing their image to look like crimefighters.  See Rex Stout's
novel The Doorbell Rang for a fictional account of how the FBI was
acting in the 1940's and 1950's (from a great author with a military
background).  See
for a true account of what the FBI was doing in the 1960s and how it
was exposed.  Noam Chomsky republished a description of the 1000ish
files that these activists took from a small FBI office:
  According to its analysis of the documents in this FBI office, 1
  percent were devoted to organized crime, mostly gambling; 30 percent
  were "manuals, routine forms, and similar procedural matter"; 40
  percent were devoted to political surveillance and the like,
  including two cases involving right-wing groups, ten concerning
  immigrants, and over 200 on left or liberal groups. Another 14
  percent of the documents concerned draft resistance and "leaving the
  military without government permission." The remainder concerned
  bank robberies, murder, rape, and interstate theft.
The word has not yet come out about what the FBI was doing in the
1980s, 1990s, 2000s and 2010s, but we have seen plenty of nasty shit,
like going after CISPES, Judi Bari, thousands of innocent Muslims
imprisoned and/or investigated and/or discriminated against (look up
Rahinah Ibrahim for someone who took 10+ years of activism in court to
find and correct the FBI's erroneous record on her, which prevented
her from graduating from Stanford or pursuing her career, via FBI
putting her on the no-fly list and canceling her US student visa for
no valid reason), etc, etc, etc.
The new FBI is into "precrime", seeking to "stop terrorism before it
happens", and is also a major designated front for NSA (e.g. any law
that gives the FBI a power, like the Foreign Intelligence Surveillance
Act, is also giving that power to NSA).  Whenever NSA needs to force a
company to reveal customer data or keying material, somehow not the
NSA, but the FBI ends up applying to a court for an order requiring
that material.
The FBI's secret court filings are even blatant about "...and turn it
over to NSA at this location", largely I expect because they never
expected them to ever be seen by the public.  Thank you, Mr. Snowden!
The FBI is not your friend, and is not a crime-investigating agency.
It is the US's internal secret police agency.  But it has a big budget
for publicity and a lot of friends in Hollywood who help it whitewash
its image.

@_date: 2014-07-19 14:40:42
@_author: John Gilmore 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
Spammers have largely mastered the art of pasting together text so that
it passes through filters, looking like human generated conversation or
documents.  Whose free software are they using for that, and can we
adapt it to slide in steganographic messages?  :-)  But that tactic almost misses the point.  In human communication,
steganography has to look like what human would send.  But in
technical communication, it just has to look like what a machine would
So for example, how many bits can you communicate in one or more
Received: lines inserted into emails?  The emails themselves can be
completely innocuous (or even borrowed from thru-traffic sent by
others).  How many bits in cookies made up and sent along with
innocent HTTP requests and replies?  How about in the essentially
unused "Type of Service" byte in every IPv4 packet?  Or the 2-byte
Identification field, a nonce that is only used when fragmenting
packets, and is uncheckable at the receiving end?  Or the huge unused
"flow label" in every IPv6 packet?  Even better, there's a naughty
trick that involves noticing how frequently Internet gateways route
Ethernet "frames", rather than the IP packets within them.  Large
numbers of IP packets are acks, shorter than the minimum Ethernet
frame size, so the IP packet is padded with many bytes of zeroes or
garbage to fill out the frame.  If you put encrypted data into that
padding, rather than random garbage, how many hops will it survive as
it passes through the Internet?  Only experimentation will tell you,
but I suspect that for a significant fraction of paths, including WiFi
paths, that padding will make it all the way to the destination host!
The beauty of this is that the steganographic data is completely
*outside* the IP packets, so anybody who is only looking at the IP
packets will miss it.  It would take someone looking for high entropy
in the frame padding on particular flows to be able to detect it,
and even then if it's encrypted they will have trouble to determine
its meaning.

@_date: 2014-07-24 17:36:04
@_author: John Gilmore 
@_subject: [Cryptography] hard to trust all those root CAs 
NSL's don't involve a judge.  Nor even a prosecutor.  They are an
investigative tactic, used by the FBI (or the FBI proxying for NSA),
long before a prosecutor is usually involved.
The more likely it is that you will disclose a government request for
snitching on your customers, the less likely it is that that request
will ever arrive.  Shining sunlight on spook activities is the best
way to make them crawl back into their hole.
Chuckle chuckle, just like the headlines about marijuana reform for
decades.  First they laugh at you, etc.  But the joke doesn't excuse
the iron fist you are trying to invoke to influence people.
Mr. Kelsey, you usually don't fall to this level of "be afraid, the
[government] terrorists are coming" propaganda.
Ladar Levison, Mr. Lavabit, the last guy to do exactly what was
suggested, is still out walking the streets -- and starting new
companies that offer to protect their customers from covert
surveillance.  As often occurs, the spooks were less interested in
smashing a guy who's standing up for the rights of the public, than
they were in preventing a detailed public airing of what they were up
to when they ran into him.

@_date: 2014-06-18 11:25:30
@_author: John Gilmore 
@_subject: [Cryptography] Implementing constant-time string comparison 
A bugfree C compiler, even if it could see through the return
expression's subterfuge, would be unable to shortcut the loop if the
arguments were merely declared as pointers to volatile storage (eg
volatile const u8 *x).  C compilers are required to avoid
optimizations that remove, insert, or reorder accesses to volatile
variables, since such accesses are defined by the programmer to have
side-effects that accesses to normal storage locations do not.  This
applies equally well to cryptanalysis-related timing side-effects as
it does to hardware registers that do odd side-things when you read or
write them.

@_date: 2014-06-18 16:21:56
@_author: John Gilmore 
@_subject: [Cryptography] [cryptography] Dual EC backdoor was patented by 
Has anyone considered the idea that Certicom patented the back door so
that they could sue anyone who tried to use it for the next 20 years?
Cryptography Research invented some cool attacks on smart cards that
watch the chip's power consumption while it calculates crypto.  They
not only patented the attack, to prevent others from deploying it at
scale.  They also patented all the countermeasures they could think
of.  Now every smart card maker ends up paying them to secure their
chips against this attack (unless the maker invents a new
countermeasure that CR didn't think of).
Ultimately this patent portfolio got CR acquired by Rambus in 2011 for
$342M: they won the startup lottery.

@_date: 2014-06-19 17:10:08
@_author: John Gilmore 
@_subject: [Cryptography] Shredding a file on a flash-based file system? 
We may try to advance, but we're standing on ground that's moving
backwards at a pretty good clip.
Oh yes.
With "SSD" drives these researchers filled them with patterned data,
then tried erasing them various ways (including the "security erase whole
drive" ATA command).  They then took the drives apart and read out the
contents of each flash chip.  The results were really ugly.  I believe
this is the definitive paper so far:
    With "USB memory sticks" the situation is even worse.  They have no
"secure erase whole drive" command specified, nor does the USB Mass
Storage command set provide any way to ask that individual blocks be
With both types, they are traditionally "overprovisioned" with more
blocks than are advertised, both so that they can continue to operate
after some blocks fail, and so that they can still allow writes into
completely full drives.  (Flash can only erase in "erase blocks" which
are typically dozens of blocks.  And they can't overwrite existing
blocks, they can only write to already-erased blocks.)  When erasing
an erase block, all the still-needed data from that erase block needs
to be copied to blocks in some other erase block, so you end up with
multiple copies of each block, scattered around in arbitrary places on
the flash chips.  And after you "overwrite" a block from the
interface, the data that used to be in that block is still there in
the flash chip; your new data is in a different block somewhere.
Each flash-based device has a "controller" chip with a CPU and
proprietary firmware that manages this process, maintaining free-lists
and damaged-lists and the mappings between block numbers on the
interface to which blocks of flash actually contain that data.  This
firmware is often buggy, comes with unknown provenance, and typically
has terrible performance under some conditions (that vary from drive
to drive).  Unfortunately most were tested only with FAT filesystems,
so more complicated or performant filesystems may interact badly with
their firmware.
Furthermore, you have no idea what flash chips nor controller are in
your package, even if it comes from a reputable manufacturer.  Bunnie
of Chumby fame had manufacturing problems with SD cards, so chased down his sources of supply.  Turned out many were counterfeit, and even
the "name brand" ones with identical labeling often had controllers
and flash chips from different vendors inside:
  The OLPC project, which was the first to ship laptops that had flash
and no disk drives, tested many flash chips and SD cards to
destruction.  They found a huge variety of provenance and performance
as well:
    "A batch of 2GB class 2 microSD cards obtained a year ago from a
  particular manufacturer averaged around 10 TB written before
  failing, with few transient errors. A batch of 2GB class 2 microSD
  cards from the same manufacturer today failed with more than half
  corrupting their FS after only 1TB of writes. The devices wear out
  around 2-4 TB of writes."
This basically never happens.  First, you don't know whether the
actual flash chip is storing your data "true" or "inverted", so you
don't know if 1->0 or 0->1 is the fundamental write operation.  [If it
was smart, it would decide to write each block either true or
inverted, with a flag saying which, by counting the number of erased bits
flipped in true versus inverted writes, thereby making the chip last
longer.  But they probably aren't smart.]  Second, each block written
is written with error checking and control bits, which are a hash
value of the contents of the block.  In these bits you are almost
guaranteed to turn a 0 to a 1 even if the contents of the block only
turn 1's to 0's.  So EVERY write to a block of managed flash involves
allocating a new block and putting the old block on the "to be erased"
list.  The only place that flash controllers can probably do the trick
of writing a block multiple times by only turning 1's to 0's, is in
its own internal data structures that keep track of which blocks are
in use.
It would be an interesting project to reverse-engineer a flash
controller's microcode.  Most of the ones in SD cards seem to be ARM
chips, which the free world has good tools for investigating, once you
figure out how to read out the microcode.  Or even more interesting to
write some GPL'd flash controller microcode, whose performance you
could understand and/or improve.
Perry said:
Modern hard drives do remap bad sectors automatically to other
sectors, but they still have a *very strong* tendency to overwrite an
old sector's data when you overwrite it from the interface.  And,
modern densities leave little or no remnant of the old data, unlike
the generations of drives that Peter Gutmann studied.  However, your
OS's filesystem may not overwrite *files* in place, so the "shred"
command may or may not be useful depending what filesystem you are
Also, modern drives contain the ATA "Security Erase" command that will
erase the entire user-accessible parts of the drive in one pass, with
some doing "Security Erase Enhanced" that also zeroes all the remapped
sectors and user-inaccessible areas of the drive.  This command is not
only faster than shipping terabytes of zeroes across the bus, but it's
likely to be more resistant against subsequent microscopic analysis.
(But: after erasing a drive, I still read the whole thing back and
compare for all zeroes.  This guards slightly against buggy erase
firmware, but not against smart malevolent firmware.)
Whenever I cross the US border with a disk drive, I run the Security
Erase command on it first.  Go ahead, bastards, search my laptop, make
my day!  Then on the other side, I reinstall Linux from a virgin
distribtion image on SD or USB.  "hdparm --security-help" will give
you the commands.  First lock the drive with a password like "foo",
then erase it with the same password:
  hdparm --security-set-pass foo /dev/sdz && hdparm --security-erase-enhanced foo /dev/sdz &
Don't forget that final & or your terminal will hang for the duration
of the erase (half an hour or more).  ^Z does not interrupt it.
If you boot the laptop from the "live" distribution image, you can open a
terminal and issue this command against your main hard drive (/dev/sda).
In theory, once the erase command starts, the drive will not respond to
any other commands until you complete the erase, even if power is lost
partway through.  This is probably not as well tested as the basic erasure.
There exists a new category of "hybrid" hard drives that contain flash
chips, and transparently store a tiny fraction of the hard drive's
contents in the flash chip to improve the average access time.  Here's
an example:
  (The customer reviews are interesting: apparently all the bugs aren't
out of the more complex firmware yet.  And the firmware seems oriented
toward flash-cacheing the files that are read right after the drive is
reset, e.g. at boot time, so your machine will visibly boot faster and
you will notice the speedup and smile.)  For data security (ability to overwrite or erase), these drives are
not your friend.  Luckily these drives come at a premium price (as
well as being marketed as containing flash), so you can avoid them
if you care.

@_date: 2014-03-10 13:47:05
@_author: John Gilmore 
@_subject: [Cryptography] RC4 again (actual security, 
Well, that's going a bit far.  My take is that nation-state attackers:
*  Are about as smart as we are -- they're recruiting from the same pool.
*  Have much better funding than we do.
*  Persevere more than we do.
*  Have operational experience at cracking codes and protocols in real
   life.
I think the last two are the most important.  There is just no substitute
for sitting in the traffic flow, trying to extract useful information from
it.  And evolving your tools over years, to do a better and better job.  We can help to catch up with and surpass them by:
*  Defunding them and funding us.  (Politically & socially)
*  Becoming more persistent.
*  Doing "red team" style attacks on our own infrastructure, and learning
   from them.  Do some packet capturing on your OWN internet, and see what
   you can learn.  Build some free software tools for doing that learning;
   see how far you can evolve them.
PS: With your packet capturing tools, look for UDP packets containing
encrypted stuff that you don't recognize.  When you find the code
that's generating them, see if it contains an RC6 implementation.
Congratulations, you've probably found an NSA implant!  (Thanx2Jake at 2013CCC.)

@_date: 2014-03-26 21:40:29
@_author: John Gilmore 
@_subject: [Cryptography] Full Disclosure closing down, rising from ashes 
Fyodor has revived it, with agreement from its former host.

@_date: 2014-05-27 16:40:06
@_author: John Gilmore 
@_subject: [Cryptography] client certificates / client-side proxy 
I'm confused.  What's the difference between a "client-side proxy"
versus a "wifi access point that hijacks the first web access" versus
a "man in the middle"?
It seems to me that a client-side proxy has a broad scope for
mischief, since by definition all the browser's traffic has to go
through it.  To the extent that it has ANY user interface, it has to
ALTER the received HTML in order to present information to the user,
or solicit information from the user.  This is not a recipe for either
good UI design nor end-to-end security.

@_date: 2014-11-14 14:59:18
@_author: John Gilmore 
@_subject: [Cryptography] ISPs caught in STARTTLS downgrade attacks 
"Cypherpunks write code" was an inspiring principle from the '90s.  It
caused a lot of people to go the one step further than just learning,
teaching, and criticizing about crypto (which are still valuable).

@_date: 2014-11-18 20:29:37
@_author: John Gilmore 
@_subject: [Cryptography] STARTTLS, 
Censorship of customer communications is always a "best practice"
according to some people.  Blocking communications based on the port
number in use?  That seems to many people to be heinous, "picking winners and losers", discriminating against
traffic based on what the endpoint services are, etc.  Wasn't
Network Neutrality supposed to outlaw all such discrimination?
Or, is it a catchphrase for "only the politically correct people
are allowed to censor or discriminate against traffic"?
The fact that some ISPs covertly built that censorship into a
supposedly transparent network must be why I never get any spam these
days.  But it doesn't matter to zealots whether their methods actually
work or not.  They're mad at spammers, and "Hulk smash" is their main
response.  Reason, principle, protocols, and respect all went out the
Anti-spammers have done far more damage to the Internet than spammers.
Now they are claiming that we can't be permitted to encrypt our
Internet connections because then their censorship scheme would stop
working?  I don't see any spammers claiming that end users should not
be permitted to encrypt their emails nor any other traffic.  To take
the privacy of our communications into our own hands, it is the
anti-spammers who stand in our way, not the spammers.
Exactly -- an anti-spammer group, the "Messaging, Malware and Mobile
Anti-Abuse Working Group", MAAWG.  Hmm, there seem to be more M's in
there than in their domain name.  Perhaps once they started advocating
censorship for one reason ("Messaging"), they found all sorts of
other reasons for it, too.  When you have a censorship hammer, every
problem looks like a need for censorship.
       (who regularly, daily, gets his personally typed emails - just
        like this one -- censored without recourse, by the ISPs of
        recipients who rely on unreliable third party censorship
        blacklists.)

@_date: 2014-11-19 17:31:04
@_author: John Gilmore 
@_subject: [Cryptography] Why mobile and consumer ISPs shouldn't censor 
I want to explore two of the assumptions in the above, that seem to be
decisive for some people in the debate:  "mobile" and "consumer".
The theory seems to be that in a "mobile" Internet provider (that is,
one run by a cellphone company), more censorship is justifiable.  And
that in a "consumer" Internet provider, like one that sells
residential DSL or cable service, more censorship is justifiable.  In
this theory, an uncensored Internet should only be available to end
user nodes that are servers and backbone ISPs, because they can be
trusted to handle it, and they have the bandwidth to deal with the
Let's talk about "consumer" first.  The Internet is a peer-to-peer
network.  That has always been its strength, and one of the big things
that distinguished it from the "master/slave" networks that preceded
it like IBM's RJE, SNA, public networks like Telenet and Tymnet, and
early computer communication services like MCI Mail, CompuServe and
The Source.  The Internet started with every peer able to talk to
every other peer, with no nodes relegated to mere "clients" or
"consumers".  TCP is designed to make a working connection even if
both nodes simultaneously and spontaneously reach out to each other,
as opposed to having a "server" side lying in wait and a "client" side
initiating connections.  New applications and protocols such as
multicast, instant messaging, VoIP, video conferencing, distributed
source code control systems like git, Mobile IP, BitTorrent, Kademlia,
federated social networking, and many others, including the Web which was invented dozens of years after the Internet, depend on this
peer-to-peer behavior.  When address exhaustion and NAT threatened
peer-to-peer since the 1990s, the network evolved to continue offering
peer-to-peer support, including IPv6 as the big fix, plus UPNP, NAT
Traversal, dynamic DNS, supernodes, and other NAT circumvention
In a peer-to-peer network it doesn't work to designate some portions of
the network as "consumers" or "clients" who don't get full access, and
other portions of the network as "providers" or "servers" who do get
full access.  Servers can be placed anywhere in the network, and
frequently are placed on "consumer" networks.  For example, in the
homes of engineers or entrepreneurs, in consumer Network Attached
Storage boxes, in ethernet video cameras, and even in flying $500
quadcopters.  Consumers (e.g. people) should have all the same rights
on the network as providers (e.g. websites).  Consumer devices
(e.g. tablets) should have all the same rights on the network as
provider devices (e.g. data center servers).  A device's location on
the network is not and should not be relevant.  Many of the most
transformative innovations have come from individual consumers like
Bram Cohen or Linus Torvalds who created new protocols that run at the
edge of the network (BitTorrent and git).
Now let's talk about "mobile".  The theory is that mobile networks
somehow should get more authority to censor or block traffic, because
they have less total bandwidth available, or because their endnodes
are "only" cellphones, or for reasons like those.  Those arguments are
largely specious, too.
First, cellphones have evolved into full blown pocket computers, and
there are more of them in the world than there are desktop computers.
If the broad social move from desktops to pocket computers means that
their billions of users get fewer rights and capabilities than they
had in the previous generation, there's something rotten at the heart
of that theory.  EFF was founded more than 20 years ago to counter
exactly this kind of creeping removal of well accepted civil rights
via technological change.  Cellphone users should have all the same
rights against censorship and rights to encrypt their transmissions,
as desktop computer users and as server operators.  Software that runs
as a mobile "app" should have the same rights on the network as
software that runs as a Linux desktop "package".  And by the time when
our cellphones shrink to run in our wristwatch, our eyeglasses, or in
our bloodstreams, our always-on network should not deprive us of
rights that we had back in the day when we had to unpack our computer
from a bulky suitcase.
Second, it is easy for "mobile" networks to provide connectivity to
full blown desktop computers or servers.  USB mobile dongles are
readily available and cheap.  Mobile-based WiFi hotspots are readily
available and cheap.  The endnodes that connect to such hotspots, or
use those dongles, should get no worse censorship and encryption
policies than when they connect to a hardwired WiFi hotspot or to an
Ethernet cable.
Third, telephone companies are now actively claiming that they cannot
affordably provide wired communications services, so they are asking
regulators to be able to withdraw wired services and offering ONLY
"mobile" networks to their customers in entire regions.  This got the
most press coverage after East Coast floods destroyed wired
infrastructure, but it is a covert nationwide strategy and every day a
telco petitions a government somewhere to eliminate the telco's core
requirement to provide wired service to every customer who wants it.
So not only do "mobile" users in those regions become second-class
customers, but EVERY user in those regions becomes a second class
customer.  If every user gets a more-censored Internet in this
transition, we're back to the dystopia of technological evolution and
telco manipulation destroying the valuable and important civil rights
that we all once had.
Fourth, let's examine the "low bandwidth" theory.  In many places on
the earth, 3G and 4G and 5G mobile bandwidth exceeds the readily
available bandwidth from wired Internet providers.  DSL lines only
reach tens of thousands of feet from a central office, relegating
rural home users to dialup modems or satellite or other wireless
feeds.  Yet mobile cellular networks in rural areas often cover large
geographical areas that hold few subscribers.  This means that each
subscriber gets a correspondingly large share of the total available
bandwidth of the cell site, often making mobile cellular the highest
available end user bandwith network.
Fifth, even where wired networks offer higher bandwidth than mobile,
the absolute bandwidth offered on mobile networks today vastly exceeds
the bandwidth that was available just a short time ago.  The original
ARPAnet's backbones were 56 kilobit/sec leased lines, as were the
original high speed ISDN Internet connections offered in the 1990s.
When the NSFnet took over from the ARPAnet, it ran on big 1500 kilobit
(1.5 Megabit, T1) backbones.  Almost every server in the mid-1990s had
no better connection to the Internet.  The NSFnet was later upgraded
to a T3 (45 megabits) backbone, roughly the downstream speed of
today's consumer cable modem -- but that was enough for the entire
North American continent.  Most initial Internet users were on 14.4
kilobit dialup modems, eventually rising to 56 kilobit dialup.  When
the telco monopolies were forced to allow entrepreneurs to change the
signalling on the last-mile wire to your telco central office, ADSL
lines that ran a whole megabit or more (in one direction) became
cheaply available to consumers and ordinary businesses.  So getting
back to the "mobile" theory, if your server is perfectly happy on a
1.5 megabit connection, why should you should get your access
censored, your encryption blocked, and your application choices
limited, depending whether your connection is a T1 line or a "mobile"
Sixth, after natural or man-made disasters, wired connectivity is
often destroyed, flakey or unavailable.  Mobile networks are much
quicker to repair after a flood, war, or earthquake, and may not go
down at all.  For the resilience of our infrastructure, which includes
Internet services and not just backbone connectivity, end users should
be able to switch both their "clients" and their "servers" onto
whatever networks are functioning, at any time.  A company that runs
its own mail server should not have mail delivery fail, or refuse
encryption, because it was wise enough to provision itself with backup
connectivity via a mobile network.  If after a tornado you put your
web server on port 80 on a mobile network while running the server on
battery backup, the cellphone company should not censor it.  In disasters
the network has to be flexible, not rigid and coercive.
All these theories about why it's OK to censor Internet access, block
certain services based on the whim of the ISP, and prevent end users
from encrypting their traffic, come root from the monopoly
nature of the underlying access media.  In the heyday of the Internet,
before these monopolies learned how to manipulate the regulators to
prevent it, the monopolies were prohibited by law from telling you
what phone numbers you could call, what ISP you could dial into, what
protocols you could run over that modem, or who in the rest of the
world you could communicate with.  The telco couldn't stop you from
calling the Internet -- much as they dearly would have loved to --
because they were a common carrier.  And if your ISP developed crazy
ideas about censorship, you could just dial into another ISP who had
policies that suited you -- or start your own ISP and attract
customers who like having full rights and freedoms.  I did exactly
that in the 1990s, when the available ISPs told me that I as a
"consumer" couldn't split down and share my net connection with
anybody else.
The heart of today's "network neutrality" issue is that by falsely
conflating the underlying broadband access media with "the Internet",
and then deciding to leave both free of regulation, the regulators
have abandoned that prohibition on discrimination.  The FCC now allows
the regulated monopolists to decide who you can talk to and what you
can say to them.  The fix is not to regulate the Internet.  The fix is
to regulate the underlying broadband access media -- the phone wiring,
cable wiring, fibers to your house or neighborhood, and wireless
infrastructure -- while preventing the infrastructure companies from
forcing you to choose a particular "Internet" provider over that
access medium.  Thus over your cable modem you could buy Internet
access from any of a dozen providers; over your cellular phone you
could buy Internet access from the same dozen.  The signals would be
carried over a different medium, but neither the cable company nor the
cellphone company could dictate which ISP you must use or on what
terms you must access the Internet.
We see this problem again and again in different corners of different
issues, including this "anti-spammers versus consumer privacy" issue,
but it's really the same issue.  The access providers don't want to be
common carriers who are obliged to carry all traffic for everyone --
because there's more money in getting a government granted monopoly
and then being able to selectively sell access to that region,
piecemeal, to the highest bidders.  Like Comcast deciding that it
won't take Netflix's traffic unless Netflix pays extra.  Like T-Mobile
deciding that you can't access  from your phone (try it)
because it publishes about the politics of drugs, and "drugs are bad".
And like spam-weary ISPs deciding that you can't encrypt your email
transmissions because it would make their particular choice of
ineffective antispam measures even more ineffective.

@_date: 2014-10-02 02:32:03
@_author: John Gilmore 
@_subject: [Cryptography] NSA versus DES etc.... 
Let's go a bit deeper into this.  Politeness?  Common interest?  Really?
Suppose Nation X reveals big Nation U's sooper secret crypto
algorithms.  Then Nation U is embarrassed -- and possibly has to go to
great trouble and expense to update all their crypto algorithms.
The only time Nation X has a real interest in keeping the algorithms
secret is when Nation X has cracked them and don't want Nation U to
know it yet, since they might change to an as-yet-uncracked system.
But if Nation U is running its spooks on crackable crypto, in these
days of gigahertz fingernail sized embedded systems, Nation U's secret
bureacracy is sounding new lows in incompetence.
It's likely that Nation X could get away with revealing the secret
algorithms without implicating themselves; they could find some
hacker, academic, activist, freedom-of-information maven, journalist,
or someone else to actually do the public posting.  They may only have
to gently steer some of these folks in the direction of asking the
question, or to finding information that has been left lying around on
some obscure user-contributed web site from some long-dormant IP
address.  Or the classic brown paper envelope that "fell off a truck".
So what's the real reason?  "It just isn't done"?  Come on, these
guys do every other *&$( thing they aren't supposed to do -- why
not this one?

@_date: 2014-10-22 07:12:37
@_author: John Gilmore 
@_subject: [Cryptography] Best internet crypto clock:  hmmmmm... 
That's fascinating!  Turning an annoyance of audio engineers and home
stereos into a forensic tool.
But once the technique is known, it can be forged, by pasting a
recording of the "hum" from one time or place, into a recording made
or edited at another time or place.  It would be amusing to file a
FOIA request for the FBI's recordings of US power networks' hum, and watch them squirm trying to find a reason why you couldn't have it.
Also, in theory, even if nobody was recording the hum continuously,
the hum could be extracted from two or more existing recordings and
compared to determine whether they happened at the same time (or
copied to another recording).  For example, two concerts that were
recorded at the same time should show the same hum if they were done
within the same power grid.
Isn't there also some research showing that over time, you can tell
what time zone a remote computer is in, by pinging it for timestamps,
and noticing when its oscillators run minutely faster during the heat
of the day, and slower during the cool of the night?

@_date: 2014-10-31 10:40:07
@_author: John Gilmore 
@_subject: [Cryptography] EFF, 
Cryptography followers are invited to attend this court hearing in
Washington, DC on November 4, opposing NSA's mass collection of
telephone records.  Observe government lawyers using twisted arguments
and new meanings of simple words to justify spooky outrageous
behavior!  Support civil rights attorneys using principled arguments
rooted in constitutional and societal norms to defend YOUR rights!
Perceive the wheels of justice or just-us grinding the constitution
into effect or into irrelevance!  Show the judges and the press that
the public cares whether NSA gets away with using totalitarian
methods!  See the constitutional issues around mass surveillance
actually be discussed in an open, public court that actually hears
from someone other than the government!  The good guys won this case
at the district court (the judge declared the NSA's actions
unconstitutional), and the government had to appeal it to stop the
ruling from killing off the program.  This case could be very
interesting, and these judges could make the final decision if the
Supreme Court decides not to review their decision.
Please be respectful, wear a costume (banker or politician duds
suggested), and arrive without contraband, weapons, penknives,
cameras, nor most other tools for resisting official oppression.
Bring either a lawyer (who can sign you in) or bring
unconstitutionally required identifying documents, or the US Marshals
at the door will not admit you to this "public trial".
I won't be there (wrong coast), but perhaps a DC local will organize a
nearby place to have lunch afterward and discuss the hearing.
Electronic Frontier Foundation Media Release
For Immediate Release: Friday, October 31, 2014
Dave Maass
  Media Relations Coordinator
  Electronic Frontier Foundation
  press at eff.org
  +1 415 436-9333 x177
Media Alert: EFF, ACLU to Present Oral Argument in NSA Spying Case on Nov. 4 Court Should Rule That Mass Telephone Records Collection Is Unconstitutional in Klayman v. Obama  Washington, D.C. - The Electronic Frontier Foundation (EFF) will appear before a federal appeals court next week to argue the National Security Agency (NSA) should be barred from its mass collection of telephone records of million of Americans.  The hearing in Klayman v. Obama is set for 9:30 am on Tuesday, Nov. 4 in Washington, D.C. Appearing as an amicus, EFF Legal Director Cindy Cohn will present oral argument at the U.S. Court of Appeals for the District of Columbia Circuit on behalf of EFF and the American Civil Liberties Union (ACLU), which submitted a joint brief in the case. Conservative activist and lawyer Larry Klayman filed the suit in the aftermath of the first Edward Snowden disclosure, in which The Guardian revealed how the NSA was collecting telephone records on a massive scale from the telecommunications company Verizon.  In December, District Court Judge Richard Leon issued a preliminary injunction in the case, declaring that the mass surveillance program was likely unconstitutional. EFF argues that the call-records collection, which the NSA conducts with claimed authority under Section 215 of the USA PATRIOT Act, violates the Fourth Amendment rights of millions of Americans.  Separately, EFF is counsel in two other lawsuits against the program -- Jewel v. NSA and First Unitarian Church of Los Angeles v. NSA -- and is co-counsel with the ACLU in a third, Smith v. Obama. What: Oral Argument in Klayman v. Obama Who: EFF Legal Director Cindy Cohn When: 9:30 am (ET), Nov. 4, 2014 Where: E. Barrett Prettyman U.S. Courthouse and William B. Bryant Annex
Courtroom 20
333 Constitution Ave., NW Washington, D.C. 20001 For background and legal documents:  The audio of the oral arguments is expected to be available on the court's website sometime after the  For this release:
About EFF
The Electronic Frontier Foundation is the leading organization protecting civil liberties in the digital world. Founded in 1990, we defend free speech online, fight illegal surveillance, promote the rights of digital innovators, and work to ensure that the rights and freedoms we enjoy are enhanced, rather than eroded, as our use of technology grows. EFF is a member-supported organization. Find out more at

@_date: 2014-09-11 01:06:39
@_author: John Gilmore 
@_subject: [Cryptography] distributing fingerprints etc. via QR codes etc. 
I have half an application written that shares PGP keys over IP
multicast on the local area network (e.g. via a wifi access point).
It exchanges keys and lets you sign each others key and sends the
signed one back to the originator and such.  I got hung up when the
gpg program wouldn't actually implement its command line, demanding
that a human type things to it in order to sign keys.  When the
maintainer decided that that was a feature, not a bug, I gave up on
trying to make the program work, and never got back to it.  It's called
It was designed to work for both large key-sharing parties, and for
pairs of individuals who want to grab and possibly sign each others'
It's in Python (attempting portable multiplatform Python2/Python3, but
that turned out to be amazingly hard -- as if it had never occured to
anyone to try doing it before).
Anybody want to mess with it?

@_date: 2014-09-16 14:34:14
@_author: John Gilmore 
@_subject: [Cryptography] RFC possible changes for Linux random device 
Wouldn't it be simpler to make each open of /dev/urandom start a
different stream of PRNG results, by hanging the state from the
open-files structure rather than from the task structure?
When the file descriptor is closed, the context gets deleted.  (And securely
overwritten?  With zeroes?  With random numbers :-)?
BTW, I don't see why the getrandom() implementation posted to the
linux kernel mailing list arbitrarily limits the caller to 256 bytes
of random numbers.  It returns "EINVAL", defined as "The buflen value
was invalid", but nothing in the interface spec says that values
greater than the arbitrary number of 256 bytes are invalid.
And given John Denker's concerns about retaining entropy, shouldn't
the caller be able to explicitly state how much entropy they want to
consume?  E.g. "Give me 192 bits of entropy and fill this 4kb page
with pseudorandom numbers derived from that entropy.  And a side of
fries..."  This would let callers that don't need actual entropy pass
a zero (and get solely prng results), while those generating long-term
keys should ask for as much entropy as their application requires.
And perhaps, if an "entropy" parameter is created, we will get a much
clearer definition about when to set it to what values?  Even from
John Denker there seems to be a lot of handwaving about entropy versus
randomness.  When generating a 2275-bit RSA key for long term PGP use,
shouldn't I ask for 2275 bits of entropy?  Why or why not?  What about
when generating an ephemeral Diffie-Hellman key for forward secrecy on
an https access that will likely be closed in seconds?  Or when
generating an ephemeral Diffie-Hellman key for the IPSEC VPN that will
protect my traffic over the next week?  And how will the software know
the duration of time that the key will be used for, anyway; currently
the code doing key-creation has little to no idea.  Give us some real
examples and real reasons for the choices suggested.
Also, do we want the kernel to mark pages that it puts entropy into as
"non-swappable" or "erase upon freeing", to keep the kernel from
making or retaining copies of the page that could be exploited by
adversaries?  If we're defining a new system interface, are there more
crypto/security/randomness issues we can address with it?

@_date: 2014-09-18 21:57:11
@_author: John Gilmore 
@_subject: [Cryptography] [cryptography] Email encryption for the wider 
That's solvable, but it doesn't help.
She can send you email at derek at ihtfp.com once, and when your replies
all come from:
  From: lkjasdflksdlkjp2338tnlsdfh848492-hds8fs0D at ihtfp.com (Derek Atkins)
then when she replies to you, she'll be sending encrypted emails.  But
there's another problem.  Anybody can send her email like this:
  From: kljasdfoiu347323h3478239-guoHFDH at completely.different (Derek Atkins)
and she can't tell which is the real Derek -- and she may even start
sending her replies to that address, since any gobbledygook address is
as good as any other one.  But, but, but... letting her know she's
talking to the right Derek is a big part of what encrypted emails are
supposed to provide.

@_date: 2014-09-19 20:15:35
@_author: John Gilmore 
@_subject: [Cryptography] Simple non-invertible function? 
I don't get this non-invertible thing (sorry, haven't read Bart
Preneel's paper).  I don't see why Sandy is trying to glue this
concept into /dev/random, either.  Here's a counterexample.
Sandy says:
OK, so make a secure block cipher with a 16-bit block.  Do "encrypt(x, key) xor x".  Sandy says that this is somehow magically
non-invertible, but if I just run it for every value of X (all
65536 of them), I can build a table that DOES let me invert it.
So, just "encrypt(x,key) xor x" is not good enough, but the security
depends on some unstated properties of "encrypt".  And it appears that
the security of this construct isn't categorical or theoretical,
merely practical, if it depends on how wide the blocks are (or on how
big a table one can easily populate with current generation hardware).

@_date: 2014-09-19 21:16:01
@_author: John Gilmore 
@_subject: [Cryptography] new wiretap resistance in iOS 8? 
And why do we believe them?
  *  Because we can read the source code and the protocol descriptions
     ourselves, and determine just how secure they are?
  *  Because they're a big company and big companies never lie?
  *  Because they've implemented it in proprietary binary software,
     and proprietary crypto is always stronger than the company
     claims it to be?
  *  Because they can't covertly send your device updated software that
     would change all these promises, for a targeted individual, or on
     a mass basis?
  *  Because you will never agree to upgrade the software on your
     device, ever, no matter how often they send you updates?
  *  Because this first release of their encryption software has no
     security bugs, so you will never need to upgrade it to retain
     your privacy?
  *  Because if a future update INSERTS privacy or security bugs, we
     will surely be able to distinguish these updates from future
     updates that FIX privacy or security bugs?
  *  Because if they change their mind and decide to lessen our privacy
     for their convenience, or by secret government edict, they will
     be sure to let us know?
  *  Because they have worked hard for years to prevent you from
     upgrading the software that runs on their devices so that YOU can
     choose it and control it instead of them?
  *  Because the US export control bureacracy would never try to stop
     Apple from selling secure mass market proprietary encryption
     products across the border?
  *  Because the countries that wouldn't let Blackberry sell phones
     that communicate securely with your own corporate servers,
     will of course let Apple sell whatever high security non-tappable
     devices it wants to?
  *  Because we're apple fanboys and the company can do no wrong?
  *  Because they want to help the terrorists win?
  *  Because NSA made them mad once, therefore they are on the side
     of the public against NSA?
  *  Because it's always better to wiretap people after you convince
     them that they are perfectly secure, so they'll spill all their
     best secrets?
There must be some other reason, I'm just having trouble thinking of it.

@_date: 2014-09-20 18:44:47
@_author: John Gilmore 
@_subject: [Cryptography] FOIA win produces years of CIA internal 'house 
A FOIA lawsuit by CIA whistleblower Jeffrey Scudder has forced the CIA
to publish some declassified *Studies in Intelligence* articles from
the 1970s to 2000s".  Studies in Intelligence is CIA's "in-house
journal for the intelligence professional".
  These are poorly indexed so far, but almost 255 articles were
released, usually with minor redactions like the date of publication,
author's name, and a few sentences or paragraphs.
Here are a few interesting ones:
  "A Brave, New World"
      Details pre-9/11 CIA/NSA suspicion and obstruction, and post-9/11
    CIA/NSA cooperation.  Most paragraphs are marked (S) for Secret,
    but appear without redactions, demonstrating a typical case of
    overclassification.
  "American Cryptology During the Korean War"
      "Within years of the end of World War II, however, American
    cryptology was a hollow shell of its former self.  When the
    soldiers and sailors went home in 1945, so did the cryptologists."
  "A Cryptologist Encounters the Human Side of Intelligence"
      ... "This is one of the few occupations in which criminality is
    not only legal, it is rewarded.  It must be difficult sometimes to
    keep one's moral compass pointed north.  ...  One CIA officer
    turned the golden rule on its ear when he wrote his own credo:
    'Admit nothing, deny everything, and make counter-charges.'
    ... Cryptology, by comparison, is as clean as a freshly laundered
    shirt... The intercept floor resembles a laboratory or a high tech
    'clean room' with lots of gizmos and people in clean military
    uniforms listening intently to radios.  ... In that SIGINT
    blockhouse...there is barely a recognition that what you are doing
    is part of foreign intelligence collection...  On occasions when I
    have questioned NSA audiences, most refuse to believe that they
    are engaged in spying."
And by the way...
The above CIA FOIA site, and The Washington Post story on the CIA's
release of this "trove", both carefully neglect to say that Jeffrey
Scudder's Freedom of Information lawsuit is why the CIA suddenly
released these articles.  For submitting this FOIA request, Scudder
was forced out of the CIA over minor transgressions, and had his home
burgled by the FBI, after an 18-year CIA career:
    The Intercept has better coverage:
  It's become fashionable at agencies to imply that they are being
"transparent" and "open" by neglecting to mention that the reason they
are publishing newly released documents is because a court ordered
them to do so because they lost a lawsuit that somebody else filed to
pry them loose.  For example, EFF FOIA lawsuits forced the release of
many of the memos and court decisions on wiretapping that the Director
of National Intelligence is taking credit for releasing on their own
website.  Scudder tried many times, as part of his official CIA job,
to get thousands of these old articles released to the public and the
National Archives by the CIA, as an honest transparency move about the
agency's history.  But he failed.  He had every right to file a FOIA
request for the articles, as a private citizen, to force them to
undergo the declassification review that the bureacracy had blocked.
He didn't even talk to the public or the press with the issue, like
most whistleblowers have done.  But the agency screwed him and tried
to have him prosecuted anyway!
Since the spook agencies lie, mislead, and destroy whole careers about
even these tiny, easy to check things like why documents became
public, who can believe them on the big important issues?  It's as if
they can't help lying, even when the truth would serve them better.
Their 'moral compasses' seem to be unmoored from both morality and
their own self-interest.

@_date: 2014-09-21 20:46:11
@_author: John Gilmore 
@_subject: [Cryptography] Improper censorship in Schwartzbeck article 
Thanks for the pointer.  The article ends with the government pushing
key escrow as a condition of export, which fell by the wayside 15
years ago, but it has good info on older crypto history like the
weakening of DES, and NSA's attack on RSA.
Both copies of this article show the scars of invalid agency
censorship.  For example, on the first page they whited out the names
of the first three directors of the Center for Communications Research
at NSA's captive think tank (a university-run nonprofit on the campus
of Princeton University).  However, their names are public knowledge
and were even in The Puzzle Palace, published 30 years ago.
For reference, they are: Cornell professor of mathematics J. Barkley
Rosser (1958-61); University of Chicago mathematics chairman Abraham
Adrian Albert (1961-1962); University of Illinois/Sandia Corporation
mathematician Richard A. Leibler (1962-1977).
These facts are not classified and there is no reason to waste time
pasting boxes over them.
Who knows what other "sooper top secret stuff" is hiding behind those
white boxes (that don't quite cover the descenders of some letters, in
certain places).

@_date: 2014-09-25 11:28:10
@_author: John Gilmore 
@_subject: [Cryptography] Of writing down passwords 
Can you post a demo of that?
Many of us have seen the Blade Runner movie scene where a photo is
magnified to show who took it via a captured reflection.  But whenever
I've tried something similar, the pixels are just too big and they
fuzz out.  I wasn't aware that e.g. laptop/phone cameras had good
enough optics and resolution to use reflections from eyes or
eyeglasses to read handwriting that's out of view of the camera.
It should be possible to improve the resolution by integrating
multiple successive photos (the way human eyes do), but the typical
way to get multiple images in tiny cameras is to ask them to send
video, and then you also have to deal with coding artifacts of highly
compressed video frames.
PS: EFF offers the "Laptop Camera Cover Set", small privacy stickers
to cover the little cameras on your gadgets.  They are designed with
post-it like glue so that they can be removed without residue when you
want to use the camera, and re-stuck to block it during other times.
These also come as part of the "EFF Sticker Pack".  See:

@_date: 2014-09-25 14:10:46
@_author: John Gilmore 
@_subject: [Cryptography] NSA versus DES etc.... was: iOS 8 
I think "middleperson in key management" is too weak a description.
As I recall, you had to get all the keying material from NSA!
The theory as I recall it was that the basic encryption scheme was
insecure if keyed with 99.99% of random strings, but secure if the
keys were generated in certain ways.(*)  This meant they didn't
have to worry about the CCEP boxes leaking out to undesirables like us.
NSA would not tell you the key generation criteria, so if you just
made up your own keys, the traffic was easy for them to read.
In such a program, you could never know whether the keys NSA was
supplying you with were real "good keys" or easy to subvert "weak
keys".  They didn't even promise not to keep copies of the keys they
sent you, so they could read your traffic either way.  And if they
didn't like what you did with the boxes, they'd just stop giving you
keying material, leaving you hung out to dry with no recourse other
than re-using old keys.
No wonder this scheme didn't catch on...
(*) Many number-theoretic cryptosystems, like RSA, have this property.
If you run RSA with arbitrary numbers, it's easy to crack; you need
to key it only with the product of two large primes.

@_date: 2015-04-26 23:56:45
@_author: John Gilmore 
@_subject: [Cryptography] NSA releases 52, 
NSA cleverly/stupidly did not provide a dump of the whole set of
Friedman documents, nor did their spreadsheet actually give the real
URL of each document.  None of the released files will appear in
search engines, web archives, etc, because readers have to get to each
one via typing something into a search box.  Thanks, NSA, we know
you're out there serving the public interest with every move you make.
I have created a list of the actual URLs of all 7,619 documents.  The NSA spreadsheet's "Filename" column gives the 14-digit number of
each document, which matches the last component of its name in the URL
list.  Usefully, the spreadsheet also includes (IN ALL UPPER CASE, HOW
MILITARY) a short description of each document.  The list is enclosed
Feeding the list of URLs to "xargs wget -x -c" on a Linux machine will
pull down a copy of all of the files in one command.  They occupy
about 2.4 gigabytes.
I recommend that interested parties make a personal copy of the
documents.  Several times in the past, NSA has "re-classified" various
documents from the Friedman collection after initially deciding that
they were not classified.  And in one case NSA got a court to rule
that it was legal to "re-classify" them as long as they could seize up
all the formerly-public copies, by hook or by crook (American Library
Association v. Faurer (Director of NSA), 631 F.Supp. 416 (1986)).  In
that case it was shown that over a period of decades, *every*
different NSA person who looked over the documents declassified some
that others had classified, and re-classified some that had been
released for years.  And despite that evidence of the total
arbitrariness of NSA's classification decisions, the courts let them
get away with it.  So DO pass copies to other people whose IP
addresses do not appear in NSA's website logs.  Then if some of the
documents mysteriously disappear from NSA's website, and if even more
mysteriously they also disappear from your own hard drives, the
community will still have copies.
I was told years ago that copies of Friedman's documents were stolen
by NSA from the stacks of the New York Public Library.  Luckily, I
found that the NYPL had microfilmed many of their holdings.  Those
microfilms were stolen, too.  But those microfilms had been copied by
other libraries, and in one of those we eventually re-found the
documents.  Lots Of Copies Keeps Stuff Safe.

@_date: 2015-08-15 11:54:09
@_author: John Gilmore 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
This seems like yet another example of Binary RSA Myopia.
If the cost of RSA at 2048 bits is too high, why not use 2016 bits?
Or 1984 bits?  Or 1600 bits?  Or 1216 bits?  (NSA's 1024-bit RSA-
cracker won't work on a 1216-bit prime.  It probably won't even work on a 1056-bit prime, since myopia has caused fools to 'standardize' on
1024-bit keys and now a huge majority of TLS keys are 1024 bits.)
And I'm not sure why you say 'RSA really hits diminishing returns
above 2048 bits".  Do you mean, using myopia, that you don't think the
price/performance of 4096 bits is worthwhile?  Then why didn't you say
so?  My RSA OpenPGP key has 3200 bits and it seems to have no
difficulties in price/performance or interoperation.

@_date: 2015-12-08 14:07:12
@_author: John Gilmore 
@_subject: [Cryptography] Who needs NSA implants? 
Because diverting them will let NSA flash BIOS trojans (or hard drive
firmware trojans).  All three of the issues that you mentioned are
resolved if you merely wipe the hard drive upon reciept.  NSA prefers
exploits that survive hard drive erasure and installation of a fresh
OS of your choice.

@_date: 2015-02-17 12:05:10
@_author: John Gilmore 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
This is a fascinating discussion.
It seems, both from listening to Mr. Snowden's descriptions of the
work environment inside NSA, and from some of the documents that he
released through the press or that have been declassified since his
inspiration, that NSA's internal networks are generally not encrypted.
They seem to operate in the clear (except with link encryption when
they go outside secure facilities).
This to me is the height of foolishness, unless NSA finds itself unable
to develop crypto protocols any better than the open world.
NSA basically uses the same computers and the same operating systems
internally as what we see in the sane world.  They have nothing
better!  And since in the sane world, we can't get working end-to-end
encryption in a way that a large organization can sanely manage, they
don't seem to have it either.
NSA likes to project an image of importance and invulnerability, but I
am coming to believe that instead they are more like petty thieves,
like jays stealing pretty trinkets from the public for their
collection.  Thieves that are paid with tax dollars and receive
impunity for their misdeeds can do a lot of minor damage.  Thieves
that can covertly influence policies imposed by a government that has
run roughshod over all the fences that were designed 200 years ago to
keep it from becoming despotic are much more dangerous.  Weinberg's
Second Law, "If builders built buildings the way programmers write
programs, then the first woodpecker that came along would destroy
civilization," was more prescient than Weinberg realized.  NSA are the
first woodpeckers, and they are doing their best to destroy
civilization for their own benefit.  We now see plenty of other birds
noting the rich pickings and flocking together with NSA.  Their
ugly colors are more readily visible to us -- as destructive malware,
cyber thieves, organized crime, and cyber war against civilian
I find it ironic that my efforts toward scalable "opportunistic
encryption" protocols and implementations might eventually allow NSA
to encrypt their own internal network.  But it will be worth it if
we can also encrypt the large bulk of the sane world, helping to
protect the public against the birds that would be happy to eat the
seed corn of civilization -- fundamental rights of privacy and
personal autonomy.
It's particularly ironic since NSA has been actively working in
standards committees to defeat public efforts to standardize better
encryption, particularly end-to-end encryption.  The reason they can't
encrypt their own networks is because they have used their influence
and our tax dollars to deter the rest of us from doing so.  Not only
have they shot themselves in the foot -- we're ALL limping because
the thieves have taken over the asylum at NSA.
Back in the '90s during the First Crypto War, it took me a while but I
eventually figured out that in the organization of the US Government,
there was nobody below the level of the Vice President whose job it was
to figure out what the best crypto policy was for the whole society.
Every agency had the incentive to tug the crypto policy in whatever
direction would make that agency's job easier.  State Dept, Commerce
Dept, NSA, Justice Dept, euphemistic Dept. of War, all sat on the
internal secret committees and all pulled for their own convenience.
The policies that resulted varied, basically depending on which agency
had strong personalities pushing hardest at the time.  Not a one of
the agencies had an eye out for the *public* interest.  (Further, NSA
sees its interest as making it easier to steal and disrupt
information, rather than easier to secure information, even though it
theoretically has both missions.)
Eventually I realized that *I* was in a better position to figure out
a good crypto policy for the whole society, than anyone inside the
government.  Even when participants like NSA weren't telling the
whole story and were actively lying about the impacts of various
policies.  Because at least I didn't have a hidden agenda of making
my own job easier.
That made me feel better about putting serious effort into creating
crypto code and crypto policies that matched my intuitions about what
would be best for the public.  And explained why the government was
clueless and incompetent at doing so.
Even the National Academy of Sciences' CRISIS report, a supposedly
independent body who took input from everybody, ended up recommending
that the policy should push everyone to use DES (by making everything
stronger non-exportable), even though multiple academics starting with
Diffie in the '70s had broken DES by brute force in paper designs.  By
then EFF was (privately) well on its way to building the physical
machine that proved to the world that DES was useless and therefore
that the CRISIS report was fatally flawed.
And nobody except EFF and a few supporters cared that the export
controls on encryption violated the fundamental rights of freedom of
speech and academic freedom of inquiry.  For that argument, we got
zero support from government agencies (though the initial clue did
come from a little known publication of the DoJ Office of Legal
Counsel, DoJ and NSA opposed us every step of the way), no support
from Congress, no support from the White House, little support from
commercial companies, and not even much from academics.
Constitutional rights seemed to them a quaint anachronism, a speed
bump that had already been passed decades ago, useless in the
hurly-burly of the market and in the political compromise process.
But that was the argument that ultimately forced a policy change, via
the federal courts, because the export controls on software
publications were BLATANTLY unconstitutional, a quintessential
licensing regime for censoring protected speech.
But the thieves found a defense against that.  In the intervening
decades they have hobbled the federal courts via the "state secrets
doctrine" -- so now anytime you sue them over something that the
spooks are doing, they just throw the case out.  But how else could
the federal government defend indefinite imprisonment without trial,
torture, indiscriminate mass wiretapping, a prior restraint licensing
scheme for travel, and their other serious crimes infringing
fundamental personal rights that we haven't yet discovered?  If the
courts had remained honest, these crooks would be out of a job.  From
the agencies' point of view (again not seeking the best result for the
public), it's better to screw up the courts than to get caught at
destroying another few cornerstones of civilization.

@_date: 2015-02-17 13:32:44
@_author: John Gilmore 
@_subject: [Cryptography] Equation Group Multiple Malware Program, 
The simplest thing it can do is to read out malware-infested boot
code, but only right after reset (or during a pattern of accesses that
the BIOS uses during startup).  Anytime those sectors are read after
that, they appear to be perfectly normal.  And if you write to them,
they don't overwrite the malware.
If the system isn't using Full Disk Encryption, then there's lots more
that active disk drive malware can do.  All the instructions for all
the software that runs in the system are on the disk drive.  You
effectively have a man-in-the-middle attack between the CPU and the
drive platters, which can substitute data (e.g. the password file,
your .ssh config and cache), code (the login program), keys, web
server content (to attack people who access your web server), etc.
A mere disk drive, if properly programmed, can make your web server
serve different pages to different clients (including attack malware
to certain clients), examine the logfiles being written to itself in
order to detect incoming web accesses and respond individually to
them, etc.  Systems are just not written to assume malware in disk
drives, so they don't hide information from it that would help it
do nefarious things.
Once there's a MITM beachhead in the drive firmware, it can also be
configured to backdoor any number of likely installation candidates,
not just the software that happens to be installed on the drive at
that time.  So reinstalling the OS, or a later OS, or any OS
that boots with GRUB, say, doesn't present a problem to NSA.
If the system is using full disk encryption, then it needs to know the
keys and algorithms so it can see plaintext, but is otherwise the
same.  (Note that many disk drives now offer full disk
DRIVE-CONFIGURED AES encryption, done transparently to the software.
Just how much do you trust those drives?)

@_date: 2015-02-23 17:22:35
@_author: John Gilmore 
@_subject: [Cryptography] trojans in the firmware 
You have an odd definition of "everyone".  Disk drives still matter.
The One Laptop Per Child (OLPC) project was the first to design and
ship laptops with no hard drive, only flash.  They started with bare
flash, using Linux flash file system support.  It turned out that that
was a problem for several reasons.
The Linux flash file systems were initially built for very small
devices.  When run on larger raw flash chips, they added many seconds
to the boot time, merely to scan all the flash blocks and build an
in-memory index of them.  OLPC spent some programmer time and improved
the flash file systems, but they were still pretty clunky and had
almost zero mass market burnin time, compared with the disk-based file
systems like ext2, ext3, ext4, or even the oddball file systems like
ZFS or ReiserFS, that have millions of daily users.
But much more important to OLPC was the dynamics of the flash market.
With each new generation of flash parts, the interfaces to the parts
changed.  They needed more or fewer wires, more or less voltage, more
or less current, more or less time for each kind of access.  They have
increasingly bizarre programming and erasure algorithms.  Merely
reading a cell in some raw flash chips perturbs the data stored in
nearby cells!  It became clear that long-lived systems designed to use
a particular flash interface would be stuck at that year's capacity
forever, with commodity parts harder and harder to find cheaply,
despite or rather because of rapid capacity increases across the
industry over time.  For example, their early devices used
single-level flash cells; later ones used multilevel cells.  The
single level cells can be erased and reused hundreds of thousands of
times; in multilevel cells the limit was only a few thousand.  But
multilevel had higher capacity.
They ultimately solved their design problem, procurement problem, and
programming problem by designing to use *mediated* flash, that comes
bundled with a dedicated controller -- SD cards, microSD cards, and
eMMC modules, all of which have the same electrical interface and
programming interface.  See e.g.:
  This lets their laptops upgrade what used to be a gigabyte of flash to
up to 32 gigs without changing the interface.  Of course this solution
adds other issues -- such as finding bugs or malice in the firmware in
the flash controllers.  Flash controller firmware is *complicated*
because it implements its own file system to pretend that flash chips
are just a series of working read/write sectors like disk drives.
They also do "wear leveling", moving the data around on the chips to
avoid doing too many writes to any given flash sector, and to try to
hide the erase latency.  Complex firmware means that bugs are
unavoidable, and obscure performance issues are very likely.  And there
are no free software implementations of flash controller firmware, so
you can't scrutinize the code yourself without using serious forensic
What distinguishes SD cards from ordinary MMC flash cards is that the
SD cards contain malicious firmware designed by Hollywood to let the
cards store information that the owner cannot access (we would call it
"DRM" but they call it "Secure Digital").  99% of devices with an SD
card slot don't use the DRM features (which were designed for MP3
players), but SD had much better marketing than MMC, so consumers
started demanding "SD" cards.  Of course this DRM leaves a nice back
door for NSA or other bad actors to cache information in SD cards
against the will of the card's owner or the owner of the system it
runs in.  I don't know of anyone who's surveyed the DRM
implementations on modern SD cards to even see how much of it works at
this point.  Since almost nobody uses the DRM code, vendors may be
leaving it out but using the SD logo anyway.  And if they keep it in,
that unexercised code is another highly probable source of exploitable
There are proprietary commands for reflashing the firmware on many SD
cards (which usually boot their controllers from firmware stored on
the flash chips themselves).  I expect that NSA knows them, just like
it has stolen or compelled the info needed to reflash disk drive
firmware for ten companies' disk drives.
Oh, one more big issue about the flash market.  Nobody, not even the
big vendors, can tell whether a given product was made by their
factory and sold through legitimate channels, or was made by a
fly-by-night vendor and labeled with a mainstream company's logo.
(There's a 3rd level - ghost runs -- which are made in mainstream
factories during off hours by corrupt employees without the company
knowing it.)  The only way to really tell is to destroy the device by
peeling it apart and analyzing it with instruments.  See:
    So even when buying a reputable brand from one of their main licensed
distributors, you have no idea who actually made or programmed
the chips you receive.  Have fun securing a system that boots from one
of these!

@_date: 2015-02-23 19:12:16
@_author: John Gilmore 
@_subject: [Cryptography] trojans in the firmware 
That would be somewhat true on a disk drive, but not on a flash drive.
On a flash drive, the "Secure Erase" command is supposed to restore
the flash chips to their original performance, i.e. it's supposed to
actually erase all the write blocks, so that they can be efficiently
written without a slow erase cycle later.  This will take the same
amount of time, regardless of whether the data was encrypted or not.
Merely dumping the key leaves all the erase blocks full of data,
requiring that each one go through a slow erase cycle before each one
can be overwritten by new data.
The Secure Erase Extended command is also supposed to zeroize the
areas of the drive that might contain user data but which aren't
accessible via the normal read and write commands -- such as spare
sectors that may have had user data in the past, before or after an
error recovery sequence.  This is what distinguishes it from the
regular Erase command.
The UCSD Non-Volatile Systems Lab (a spinoff from their disk drive
research hotbed, the Center for Magnetic Recording Technology) tested
the Secure Erase features in a selection of SSD's, by writing
patterned data to every sector, then issuing Secure Erase, then
pulling out the drive and taking it apart and reading the flash chips
directly.  They didn't name the vendors, but they did find at least
one vendor whose "Secure Erase" command returned immediately without
an error, but didn't actually erase ANYTHING.  And many which only did
partial erasures, leaving the data patterns visible in some parts of
the flash chips.  See:
  The paper below, from USENIX 2013, describes some of the challenges
involved in the bizarre internal write performance of modern flash
chips (which Secure Erase is supposed to re-initialize):

@_date: 2015-01-05 14:54:16
@_author: John Gilmore 
@_subject: [Cryptography] Imitation Game: Can Hollywood be fixed? 
... somebody should write up a master wiki list of all the things that
Hollywood did 'wrong' in the movie, i.e. that are not historically
accurate.  Because, of course, a lot of people will see the movie who
will never learn more about Turing, and they'll believe all the nonsense.
Like, Denniston wasn't trying to fire Turing all the time, Turing
didn't have a romance nor marriage with the sole female code clerk,
Bletchley was solving lots of Enigma messages by hand methods and
small bombes, long before Turing's larger chained bombes came into
operation, etc, etc, etc.
Hmm, the Wikipedia page has an entire section on "Historical accuracy",
but they want sourced statements there, not original research, so
we should publish elsewhere.  See:
  I thought Neal Stephenson's treatment of Turing in Cryptonomicon,
while clearly fiction, did a lot less violence to the facts.

@_date: 2015-01-06 15:42:05
@_author: John Gilmore 
@_subject: [Cryptography] SSH vulnerability when using passwords 
A friend pointed me to this page:
  But neither that nor the ssh wikipedia page mention an SSH
vulnerability that lets an attacker guess the letters of a remote
login password used under SSH.  I remember this attack being mentioned
years ago, perhaps in a crypto conference rump session?
The attack works when you don't use keys or shared secrets -- just the
usual login/password processing standard for the remote end's
operating system.  This is by far the simplest way to use SSH -- it
requires no public keys, no private keys, no one-time pads.  All it
requires is that you can remember your login name and password, which
everyone is already used to.  It lets you log in to a remote system
from *anywhere* that has a copy of ssh, without bringing any keying
material along.
The problem is that the ssh protocol sends each letter of the password
as an entire packet of encrypted stuff -- but there are only a small
number of possible letters that might have been typed.  So the entropy
in each of those early ssh packets is very low.  The attack makes
guesses about which letters you might have picked, is able to
eliminate many of the guesses by showing that the crypto would have
produced something different in that case, relies partly based on the
inter-letter timing as I recall, and recovers passwords or partial
passwords a significant fraction of the time.  Then, of course, those
passwords are extremely useful in breaking into the remote system at
any time afterward.  Which is why NSA would store them for later use.
Where is this attack written up in detail?  And how is it best
defended against?

@_date: 2015-01-14 01:41:15
@_author: John Gilmore 
@_subject: [Cryptography] The Crypto Pi 
I suspect you should use /dev/random and run the rngd daemon to move
entropy from /dev/hwrng into /dev/random.  It will feed some data to
pool, and will refill the kernel's entropy pool (by default, to 50%)
anytime it gets below there.
Perhaps tell rngd that there isn't much entropy per bit?  I don't
know how that setting helps or hurts.
The problem on Linux is probably that they failed to run rngd, the
hardware randomness daemon.  (In Debian/Ubuntu it's in the "rng-tools"
The problem on FreeBSD is that /dev/random doesn't wait for entropy,
it just makes pseudorandomness as fast as it can.

@_date: 2015-01-27 15:13:10
@_author: John Gilmore 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
Your employer Google can - it owns the fibers among its data centers
(and many other fibers, I believe).  Clearly, Jerry's remark ("If you
own the link") was addressed to link-level encryption.
Actually, there *is* a cost of running dummy traffic or continuous
random bits.  It's the cost of the excess power required, plus the
one-time cost of acquiring the hardware and software required.  The
power consumption of Ethernet interfaces, for example, has been
reduced significantly when idle, over the last decade
(  But
Jerry's main point, that that cost is minimal in the overall long term
cost of acquiring and operating an active link, is accurate.
The traffic that goes down a fiber to the other end of the link IS to
every possible recipient.  Or rather, traffic to every possible
recipient at the other end of the fiber, goes down that fiber.  So,
yes.  But of course link-level chaff traffic will be stripped out at
the other end of the link, so it will not reach every possible
recipient; indeed, it will not reach ANY possible recipient.
If you merely encrypt transport layer packets (IP packets), as in
IPsec, their destinations are visible.  So you have to do link
level encryption to prevent traffic analysis.
I think it's significant that there is no RFC standard for encrypted
link-level traffic, either with or without with dummy traffic, and no
free software implementation either.  We on this list could do a lot
to improve that situation.
Most local and long-haul networks are now based on Ethernet protocols
(with an increasing market share in future).  A typical fiber-optic
link that carries Internet traffic has an Ethernet switch at both
ends, since they are so much cheaper and more flexible than ATM and
SONET switches, the main alternatives.  The mass market for Ethernet
has driven prices to where a $500 retail switch plus a few $100 GBIC
adapters can drive multiple 1-gigabit fibers over 50 to 80 km, plus
switch dozens of local gigabit Ethernet ports.  Doing this with telco
equipment is orders of magnitude more expensive.
We should consider writing an RFC based on RFC 894, "Standard for the
transmission of IP datagrams over Ethernet networks", that provides
for link-level encryption of all packets transmitted, and for
transmitting dummy encrypted traffic that the recipient will
automatically discard.  Plus a similar version for IPv6 (RFC 2464),
and possibly for PPP (RFC 1661), which is used over modems, and I
think is also used over DSL.
Or, should this be done directly at the Ethernet layer and at the PPP
layer, rather than at the interface between the physical layer and the
link layer?  The Ethernet layer is a much bigger mess, with more
signalling complexity, multiple layers of headers for VLAN support,
There is a similar standard for WiFi (which is Ethernet based) but
only for connection-based ("access point") WiFi, which is very unlike
ordinary Ethernet.  WiFi encryption has major issues, though, such as
using the same key for every node (if you know the WPA password, you
can snoop all the traffic, not just the traffic for your own node),
and failing to encrypt by default (if there is no WPA password, then
traffic is passed in the clear).
If such an Ethernet standard catches on, the protocol will be pushed
into switch hardware and Ethernet host interface chips, and will
result in minimal costs to deploy encrypted links with dummy traffic.
The key is to make it like everything else in Ethernet -- you plug it
together and it just works.  I don't want to pre-judge the design of
such a standard, but it should act like OTR in the sense that it
automatically detects compatible endpoints, negotiates session keys
automatically to encrypt against passive attacks (e.g. via
Diffie-Hellman), yet provides a manual option to lock down the
endpoint identities to detect and prevent active attacks.
PPP already has a link-level encryption standard, RFC 1968, updated by
RFC 2420 for 3DES.  But no AES option has been standardized for PPP,
nor is there any key negotiation protocol; these PPP standards assume
that DES or 3DES keys have been manually pre-negotiated and locally
stored.  Also, the existing standard leaks the value of the last byte
of plaintext, in its padding algorithm.  So, there remains serious work
to build a usable PPP encryption standard too.

@_date: 2015-01-29 15:39:10
@_author: John Gilmore 
@_subject: [Cryptography] traffic analysis 
Here is one way that doesn't even require encryption, merely authentication:
  Rivest designed this as more of a defense against the export controls
on encryption, than as a serious alternative to encrypting your packets,
but it can be used in combination with encryption to aid in traffic

@_date: 2015-01-29 16:59:04
@_author: John Gilmore 
@_subject: [Cryptography] traffic analysis -> let's write an RFC? 
Ben replied:
For those with shorter memories, back in 2005 when there was a huge
glut of fiber capacity, Google bought what the press alleged was hundreds
of millions or billions of dollars' worth of dark fiber.
    "First, a little historical context. During the dot-com boom of the
  late 1990s and early 2000s, big telecom companies rushed to build up
  network capacity, anticipating a vast increase in Internet traffic
  from all corners of the world. But these companies overextended
  themselves and ended up building way more cables than what was
  necessary at the time. In doing so, they drove the prices of fiber
  and data way down and ended up bankrupt and broken.
  "Enter Google. Born in the late 1990s and flush with cash just a few
  years later, the search giant seized an unparalled opportunity to
  start buying up this dark fiber capacity on the cheap."
    "Business 2.0 has learned from telecom insiders that Google is
  already building such a network, though ostensibly for many
  reasons. For the past year, it has quietly been shopping for miles
  and miles of "dark," or unused, fiber-optic cable across the country
  from wholesalers such as New York's AboveNet. It's also acquiring
  superfast connections from Cogent Communications and WilTel, among
  others, between East Coast cities including Atlanta, Miami, and New
  York. Such large-scale purchases are unprecedented for an Internet
  company, but Google's timing is impeccable. The rash of telecom
  bankruptcies has freed up a ton of bargain-priced capacity, which
  Google needs as it prepares to unleash a flood of new,
  bandwidth-hungry applications."
            "Google controls more network fiber than any other organization."

@_date: 2015-01-31 19:59:21
@_author: John Gilmore 
@_subject: [Cryptography] De-Anonymizing 
The classic way to do this is to swap cards with other people who
you happen to meet while waiting in line at the cash register.  Not
only do you point out to them how any card will do, and how the
retailer never told them that the whole point of the cards was to
track them, but you will 'mix' with people you never saw before and
are likely to never see again.
Also, at some of those stores, you can get a new card every time you
go, and it works even before you are "supposed to" fill out a mess of
personal details about yourself.  Tell 'em you never had one.
Generally the clerks are sympathetic rather than mentally bought into
the management's efforts to track everybody.

@_date: 2015-07-06 18:55:18
@_author: John Gilmore 
@_subject: [Cryptography] Crypto Wars 
Neal Stephenson's "Cryptonomicon" is fiction but was strongly inspired
by the Crypto Wars.  I recommend it.
"Cracking DES" was a key part of the Crypto Wars (disclaimer: I edited
it).  It's out of print but available online here:
  Thanks, Internet Archive!  The online version seems to be gone from
the Cryptome site, the JYA.com site, and the EFF.ORG sites.  But
the EFF FAQ and overview pages are still here:

@_date: 2015-05-06 16:00:16
@_author: John Gilmore 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
So, of course you'd confiscate all employees' cellphones, tablets, and
other personal communication devices at every moment while they are on
the premises.  And prevent them from receiving any such "outsider"
devices, e.g. via postal mail delivered to their office address.
Better hope the employees' cochlear implant doesn't come with Internet
access, (which if it doesn't today, it will soon), since you can't
legally discriminate against the disabled...
How hard would it be for an employee to put a cellular WiFi
access point into their car in the parking lot, and use some
internal device to talk WiFi to it from inside the building?
Or perhaps the employee would just go outside for a cigarette break
whenever they needed to convey confidential information to outsiders
that the auditors couldn't read.  An accomplice hundreds of yards away
with a parabolic mike could hear anything they said while standing 20
feet from the entrance door with their cigarette.  And that's if they
didn't bother to plant a covert mic in the "outside" shrubbery nearby!
Guess you'd better not let the employees go home at all -- who knows
what they'll say if they merely have access to uncensored
communications.  There's plenty of info that can be used to make money
even hours or days after you first learn it; merger and acquisition
negotiations provide an obvious example.

@_date: 2015-11-17 02:26:28
@_author: John Gilmore 
@_subject: [Cryptography] Long-term security (was Re: ratcheting DH 
This is perhaps a ray of black light.  I was forced to upgrade an old
smoke alarm to one of these units (because the Feds have forced every
manufacturer to stop selling the old ones).  The first "new" one I got
was way too sensitive -- merely turning on a gas burner on the stove
would set the thing to shrieking.  I replaced that with a second one.
It had the same problem.  Eventually, one day when we were cooking, we
took the thing down and smashed it -- it was the only way to get it to
shut up.  You can't just remove the &%*( battery (it locks in
place), and apparently you can't buy one that actually can tell
cooking on a stovetop from a dangerous fire.
Luckily I found a few old units that a friend replaced -- so, armed
with a few 9-volt batteries I will be able to have a working smoke
alarm.  Or at least I'll have an "old" non-working smoke alarm that I
can live with, instead of a "new" non-working smoke alarm that I can't
live with.
Let's not use this as a great example of how to force users to upgrade
(to non-working devices they are forbidden to control).

@_date: 2015-10-22 02:32:34
@_author: John Gilmore 
@_subject: [Cryptography] Other obvious issues being ignored? 
[Re the non obvious issue of compilers versus crypto code]
The attitude expressed so far is incredulity that a compiler would
assume that the programs it is compiling do not violate the standard
for the language.  I have worked on many programs, and whenever I
found such a problem (typically called a "portability problem"), where
the code was assuming something that the language did not guarantee, I
fixed the program rather than bitching about the compiler.  (Back in
the 1980s I went through the entire Vax 4.3+ BSD Unix source code,
built it all with GCC, and revised all the nonportable code from
Version 7 Unix, e.g. that assumed it was being built on a 16-bit
PDP-11 by an early generation C compiler.  I wrote an academic paper
about what I found, including a lot of amusing nonportable C
constructs, but it was rejected by the reviewers and never published.
The code improvements were adopted by Berkeley, which also let them
adopt GCC as their standard compiler.)
In the case of signed integer overflows, there's a really simple way
to make the code portable: Declare those variables "unsigned" rather
than "int".  I know this is too much trouble for some people.  I don't
recommend buying crypto code from such people.
GCC in particular has a -Wstrict-overflow option that will diagnose
code that is being optimized with the compiler's assumption that it
does not cause overflows.  Consciencious programmers can use this
option to identify code that was nonportable, so they can rewrite it.
See GCC maintainer Ian Lance Taylor's blog post:
  The main further bitch seems to be that "side effects get optimized
away", such as assigning zero to a variable that is subsequently never
referenced.  (The "side effect" in that case is that a chunk of raw
memory or registers no longer contains a cryptographically sensitive
value after the assignment.)  The standard C language has a cure for
this, too, the "volatile" declaration.  I have the same advice about
getting crypto code from people who are unwilling to type that extra
Subterfuge rather than portable code seems like a novice response.  If
the code is rewritten to be portable, it will work in every release of
every compiler (modulo the occasional compiler bug).

@_date: 2015-10-26 22:36:17
@_author: John Gilmore 
@_subject: [Cryptography] EFF press release: Wednesday Hearing in NSA Spying 
[The public is invited to watch justice be done or undone.  Remember
 our old cypherpunk dress-up days in the Bernstein case?  The judges
 will notice if a polite, interested crowd shows up, and will probably
 pay more attention.  --gnu]
This is a friendly message from the Electronic Frontier Foundation. View it
in a web browser [1]. ELECTRONIC FRONTIER FOUNDATION MEDIA ALERT For Immediate Release: Monday, October 26, 2015
 Contact: Rebecca Jeschke
 Media Relations Director and Digital Rights Analyst
 press at eff.org
 +1 415-436-9333 x177
 Wednesday Hearing in NSA Spying Case
 EFF Battles Against Government Stalling in Jewel v. NSA
 Pasadena, CA - The Electronic Frontier Foundation (EFF) will urge an
appeals court Wednesday to reject the government's attempts to block an
appeal in Jewel v. NSA, EFF's long-running lawsuit battling
unconstitutional mass surveillance of Internet and phone communications.
The hearing is set for 2:00 pm on October 28 before the United States Court
of Appeals for the Ninth Circuit in Pasadena, California.
 At issue in the appeal is the NSA's tapping into the fiber optic cables of
America's telecommunications companies--a digital dragnet that subjects
millions of ordinary people to government spying on their online
activities. A mountain of evidence from whistleblowers and the government
itself confirms the Internet backbone spying, yet a district court judge
ruled earlier this year that there wasn't enough publicly available
information to rule if the program is constitutional.
 EFF appealed to the Ninth Circuit, but the government claims that the
appeal is premature and entwined with other issues that are still being
litigated in the lower court. EFF Special Counsel Richard Wiebe will argue
Wednesday that the appeals court should reject the government's delay
tactics, and finally address whether backbone spying is legal and
 What:
 Jewel v. NSA [2]
 Who:
 EFF Special Counsel Richard Wiebe  When:
 Wednesday, Oct. 28
 2:00 pm
 Where:
 Richard H. Chambers US Court of Appeals
 Courtroom 1
 125 South Grand Avenue
 Pasadena, CA 91105
 For this release:
  [3]  About EFF  The Electronic Frontier Foundation is the leading organization protecting
civil liberties in the digital world. Founded in 1990, we defend free
speech online, fight illegal surveillance, promote the rights of digital
innovators, and work to ensure that the rights and freedoms we enjoy are
enhanced, rather than eroded, as our use of technology grows. EFF is a
member-supported organization. Find out more at  Electronic Frontier Foundation, 815 Eddy Street, San Francisco, CA 94109
[1] [2] [3]

@_date: 2015-10-29 00:40:06
@_author: John Gilmore 
@_subject: [Cryptography] Oracle archeologists discover memory tagging 
What Oracle (Sun) has done here is quite a bit different from the
Burroughs tagged memory.  Here's a developer tip on how it works:
  The hardware is storing an extra software-controlled 4-bit value along
with each 64-byte cache line in main memory.  When enabled, hardware
compares this value to the top bits of the address being used to
access the memory, and faults if they don't match.  This is an
interesting use of the copious spare bits available in 64-bit
addresses.  It requires an extra ~1% of main memory and cache memory
width, and probably some extra pins on the processor.
Oracle enables this feature using surprisingly simple modifications to
malloc(), so that adjacent objects (and objects re-allocated on top of
freed objects) are always 64-byte aligned and always get a different
4-bit tag.  User programs can write these memory tags.  Of course, not
every object will have its own tag (there are only 13 tags available)
so a wild access has a 12-in-13 chance of faulting -- still vastly
better odds than without this feature.  This works even with old
binaries, since they link with the new malloc.  If the
memory-tag-writing overhead is low enough, the compiler could also
generate code in new programs to do the same thing with each stack frame
(giving each frame (one or more) tags different from adjacent frames).
By contrast, old Burroughs system memory tagged each 48-bit memory
word based on what kind of data it contained (for a small number of
kinds defined specifically by hardware).  E.g. all data words in all
objects had tag 0, whereas array descriptors had tag 3.  The tags were
not checked against bits in the address used to access them, but
instead against the type of instruction used to access them.  User
programs could not change these tags.  This did not provide protection
against pointers accessing data from outside their arrays, which is
what the Sun/Oracle feature is designed to do.  (That was done by a
separate descriptor mechanism in Burroughs machines, unrelated to the
tagged memory.)

@_date: 2015-09-23 14:23:50
@_author: John Gilmore 
@_subject: [Cryptography] rejecting 3rd party web accesses/cookies 
See EFF's Privacy Badger plugin for Firefox and Chrome.  It does
almost exactly this.  It sends the Do-not-track header with every http
and https request.  It looks for 3rd party inclusions into web pages
that are pulled in from web pages in multiple domain names.  It
examines their cookies to see if any of them have more than nominal
entropy.  If they are tracking you DESPITE your do-not-track request,
it blocks future accesses to those 3rd party inclusions automatically,
unless they have published a legalese privacy policy that meets its
Since enforcing that across the board would break too many web sites,
it has a whitelist of a few 3rd party inclusions that are still
needed.  Rather than completely blocking those, it blocks all cookies
and referrers when accessing them.
You can readily see what it's done, and can change what it does for each
3rd party site by sliding a slider among "block / block cookies / allow".
Privacy Badger also has special code for those insidious social-media
"like buttons" that track your every move on the web whenever they are
loaded into the page you're viewing, even when you don't click on
them.  If it catches them tracking you, it replaces them with a local
equivalent, which only accesses the social media company if you
actually click on the button.
Since most ads come from 3rd party sites with lots of trackers, those
tend to get blocked very rapidly -- a nice side effect of protecting
your privacy.
And it's free software, so you can examine and improve it.

@_date: 2016-04-04 00:53:56
@_author: John Gilmore 
@_subject: [Cryptography] Have you seen... 
It is a good article, though short on independent confirmations.
Hmm, the article doesn't mention stuffing ballot boxes at all.  The
interviewed "election hacker" used methods like spying on the
opposition, breaking into their computers and cellphones, sending fake
Tweets or phone calls designed to enrage their supporters among
voters, etc.  As far as I noticed, there was no mention of actually
changing votes in government computers after they were cast.  It was
all about using dirty tricks to convince people to vote for the wrong
We do see this sort of thing in California elections.  A favorite
trick for defeating close initiatives is to buy a bunch of media time
in the last week before the election, and put out endless repetitions
of high impact ads full of lies told by credible people.  The ads are
tested on focus groups for the biggest short-term impact, regardless
of their long term truth or convincingness.  By the time the other
side can respond, bam, it's election day and their voter support has
dropped below 49.99%, and the initiative loses.  And who cares if
the voters find that they have been lied to -- two days AFTER
the election?  Elections don't get rerun just because politicians
tell lies, that's expected.  Sorry, you lost, bye.
A friend of mine ran an initiative that would require that people be
given drug treatment three times -- and failed it each time -- before
they could be put in prison for drug crimes (California Proposition
5).  The largest political contributor in the state, the prison
guards' union, told my friend as he wrote it that they wouldn't oppose
it.  The only problem was that they were screws and their mouths were
moving.  They quietly arranged to film two TV commercials -- one
featuring Attorney General Jerry Brown, who wanted their support for
his later campaign for Governor (which he won); the other featuring
the worst machine politician left in California, Senator Dianne
Feinstein.  In the ads, they called the initiative the "Drug Dealer's
Bill of Rights" and said lots of other things about it.  You can see
the ads on YouTube.  In the last three weeks of the election,
$1,000,000 from the prison guards' union suddenly showed up in the
anti campaign.  (There were also $50K to $250K chunks contributed at
the last minute by politicians who had apparently raised more than
they needed for their own gerrymandered campaigns.)  That money was
used to buy airtime for these lying TV commercials in the last week of
the campaign.
The initiative failed.  A few days before the election it was obvious.
There's an extremely entertaining Democracy Now program featuring a
debate between Atty Gen Jerry Brown and my friend (Ethan Nadelmann) in
which they end up screaming at each other on the air, despite the best
efforts of moderator Amy Goodman.  See or hear:

@_date: 2016-04-05 00:17:11
@_author: John Gilmore 
@_subject: [Cryptography] Secure universal message addressing 
The key idea here is a bad idea.
I don't want everyone I interact with to have the same identifier for
me.  That's the problem with Social Security Numbers.  With a single
identifier, all the interactions with me can be cross-correlated to
track me everywhere I go.  Typically this is done NOT for my
benefit, but to give some third party an advantage over me.
Every online service that I interact with gets a different identifier
for me.  Every one gets a different email address for me.  If you send
email to one, they mostly lead to the same mailbox, though that's not
obvious from the addresses, and is under my later control.  (Some of the
email addresses that websites demand of me lead to places like
mailinator.com, which offers free disposable email addresses that will
let you read the one email message that "verifies" that this is a "real"
email address, and then quietly file and discard all the spam that the
websites send there subsequently.)
Provider A has no idea that I'm the same guy as Provider B's customer Joe.
They don't need to know, and I prefer that they not know.  And, from my point of view, this is why they died.  I had zero
interest in helping third parties keep track of me everywhere, using
the same identifier on widely varying sites.  It's already hard enough
work to keep Google out of my underwear when I don't even have an
account with them.  If I had the same account everywhere?  Let's not
go there.  "Login with your Facebook account?"  No thanks!!!
ssh public key authentication has this problem too.  Its default is to
assume that you want to use your same local identification to identify
you to every remote site that you try to access.  What a clueless
idea.  Luckily, ssh has survived despite this.  If you avoid its whole
public-key-per-user aspect, you can use it reliably with usernames and
passwords, different on every site.

@_date: 2016-04-15 00:21:23
@_author: John Gilmore 
@_subject: [Cryptography] USB 3.0 authentication 
As the designers say, "USB has evolved from a data interface capable
of supplying limited power to a primary provider of power with a data
The USB 3.1 specs are available here:
  Within the zip file is a "USB Power Delivery" directory and a
"USB_PD_R2_0 V1.2 -20160325.pdf" power delivery spec.  This 500+-page
spec defines a vast infrastructure that allows USB devices and cables
to negotiate their power-related specs, allowing up to 5 amps of power
at 20 volts (100 watts) to be fed in either direction through the USB
cables and connectors, as negotiated.  If you remember how much heat
an incandescent 100-watt bulb produces, you'll be sure you don't want
that much power going down some of the flimsy USB cables that you've
seen in the past.  The spec uses "Power Marking" to differentiate
legacy connectors and cables from "Power Delivery" compliant
connectors and cables.  The PD compliant cables include a chip in the
connectors that can describe the cable's capabilities to the USB
ports.  Power sources send protocol messages that describe what
voltages and currents they can offer; power sinks pick among them.
The power sources are required to check the cable and not offer
options that would overload the cable.
I think that's the authentication that you're talking about.  I am up
to page 111 and haven't seen any crypto authentication yet; it looks
like a pretty standard 1-wire protocol with 4b5b coding for framing,
CRC for error checking, and such.  It operates in the clear as far
as I have seen -- but I encourage you to check the parts I haven't
yet read...
PS: USB has gotten pretty flexible; their new small USB C-connector is
usable at both ends of a cable, and allows the power source/sink and
and the data master/slave relationships to be swapped, independently,
by the devices at both ends.  It does data at up to 10 Gbits/sec and
power at up to 100w.  The connector is also self-symmetric so you can
plug it in upside down or rightside up.  And there are ways to negotiate
into other "modes" so you can run other protocols down the same cable,
the first of which is DisplayPort.  They're getting smarter...

@_date: 2016-08-20 11:47:48
@_author: John Gilmore 
@_subject: [Cryptography] UK report on "Bulk Powers Review" of GCHQ 
BBC news - Britain's spies should be allowed to continue harvesting large amounts of data from emails, the government's reviewer of terror legislation has said. UK Government report:

@_date: 2016-12-01 11:56:01
@_author: John Gilmore 
@_subject: [Cryptography] "Great. Now Even Your Headphones Can Spy on You" 
Modern sound chips have a few-wire serial interface to the system,
provide multiple fast and wide D/A and A/D converters, variable sample
sizes and bit rates, put both input and output drivers on most of
their analog I/O pins, and offer an internal mux so that any
particular pin can be connected to any particular converter.  This
flexible interface was spec'd as part of Intel's HD Audio spec:
    This allows software to change the audio ports for 2-channel, 5-channel
or 6-channel sound, for example.
All these factors -- low pin count, flexibility, and high integration

@_date: 2016-12-02 15:44:54
@_author: John Gilmore 
@_subject: [Cryptography] TV set power correlates to TV channel? 
I read the reference you provided, and it doesn't seem to say anything
to prove your point.
The TI application report suggests that the control circuitry for LED
TV sets will adjust the power draw based on the heat output of the
LEDs (to reduce their heat), based on the aging of the LEDs over time,
and based on the brightness setting (which might be set by a user or
perhaps by an ambient light sensor).  But I didn't see anything in the
report that discussed changing the LED power based on the signal being
displayed on the screen.  So I don't see how this report suggests that
"you can now correlate to near certanty which channel is being watched
based solely on power consumption".  What did I miss?
"LED" TVs really use the LEDs just for the backlight (older TVs used
fluorescent lights for backlighting).  The picture itself is formed by
an LCD (liquid crystal display) panel that sits in front of the
backlight.  LCD displays -- not their backlights -- do draw different
amounts of power at each instant, based on what fraction of the color
subpixels have their transistor driven to the voltage for black versus
the voltage for illumination.  For example, a small PixelQi laptop LCD
screen that I have the specs for, draws 199 mA at 3.3V when showing an
entirely black screen, and 280 mA at 3.3V when showing an entirely
white screen.  That power draw is inherent to how LCDs work, but
whether that effect would be visible at the AC power cord depends on
many factors -- and the Application Report isn't even about that.)

@_date: 2016-12-07 12:29:57
@_author: John Gilmore 
@_subject: [Cryptography] Is glibc right on randomness 
Here is the glibc bug report:
  Perhaps some real users who want to call getrandom() in their portable
code should pile on?  The holdup seems to be:
  joseph at codesourcery.com 2014-08-11 15:46:58 UTC
  If you wish to propose a new API for glibc, please make the proposal on   libc-alpha, taking an active lead in driving the discussion to consensus.
  joseph at codesourcery.com 2015-02-24 16:27:07 UTC
  If you want progress on this, take a lead in the general discussion
  on libc-alpha of when glibc should provide bindings to Linux kernel
  syscalls, seeking to understand the differences of views expressed,
  find common ground and drive the discussion to consensus.  Once we
  have agreed principles on bindings for syscalls, then we can
  consider which new or old syscalls should have such bindings added
  under those principles.
However, the bug is assigned to Florian Weimer, not to Joseph.  Still,
no progress has been made among these "maintainers" in the two years
since the kernel syscall was added.

@_date: 2016-12-17 12:30:18
@_author: John Gilmore 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras to be 
The reason people hate the "Secure" features of SD cards is that they
were designed to work AGAINST the owner of the SD card and the camera.
They provide DRM, not user security or user empowerment.
SD cards' crypto was CPRM, designed by Hollywood as a reaction to the
first MP3 players (which preceded the Apple iPod by some years).  They
wanted a way to sell you music that you could not usefully copy, by
encrypting it on the card with a secret that was unique to the card.
Licensed music players would know how to handshake with the card's
crypto engine to play back the encrypted songs -- but ordinary "copy"
commands would copy the encrypted songs onto other media that didn't
have a crypto engine or onto an SD card that had different crypto
secrets -- so the copied songs would not be playable.
The music industry's attempts to sell DRM'd music failed in the
market, as consumers adopted MP3 players and converted existing music
recordings (e.g. from vinyl records and CDs) into MP3s.  The closest
they came to succeeding was with the iPod, which became a runaway
success despite Apple only offering downloads of DRM'd music, locking
the player to a specific Mac computer, refusing to allow songs to be
read back from an iPod for sharing, etc.  But to gain initial
acceptance, the iPod would play back MP3s, so users learned to share
their MP3s using non-Apple tools and then manually load their iPod
with MP3s.  Eventually Apple convinced Hollywood to let them sell
users the unrestricted MP3s that users wanted to buy, and that was the
end of DRM for music.  iPods had no removable media at all, and never
used SD cards.
SD cards and MMC cards were very close to identical, and their
standards tracked each other as they evolved, except for the extra
crypto engine in SD cards.  But "SD" became the brand that every
consumer looked for, rather than MMC, for reasons I don't know.  Due
to selling in higher volumes, they became cheaper than MMC cards.
    I don't believe that this is true.  Many devices support SD cards but
do not interact with the encryption.  The average USB SD card reader
is an example.  And there's no such thing as an "encrypted card".  All
SD cards support crypto.  When encryption is used, individual file
contents are separately encrypted, while the filesystem metadata is in
the clear.
What modern devices actually interact with the crypto engine in SD cards?

@_date: 2016-12-22 15:40:11
@_author: John Gilmore 
@_subject: [Cryptography] Photojournalists & filmmakers want cameras, 
Here's an example.  Some thugs with a rented box truck broke into a
cannabis dispensary storefront in a residential neighborhood near
mine, some years ago.  These thugs appeared to be hired or imported by
the DEA.  Their typical pattern had been to break all the glass, smash
open the door, take all the drugs, money and electronics they could
find, then disappear and never file charges.  Straight smash-and-grab
harassment, done under official U.S. government auspices.  A local
activist group had set up a call tree for similar future incidents;
they activated their call-tree, and dozens of people came to witness
the attack.  I was one of the witnesses.
I brought an ordinary pocket camera (pre-cellphone).
I recorded video of these thugs carrying things out of the store, from
the public sidewalk.  Several of them grabbed me physically, twisted
the camera out of my hand, and went into it to see what I had recorded.
They were unable to see anything useful and eventually let me go (with
fingers that hurt for several days from their unwarranted assault).  They claimed that if their pictures were taken then they would be
unable to do their job.  My response is that they shouldn't be
breaking into a store on a public street if they don't want people to
take their pictures.  (I have since heard that the DEA can't get its
local agents to harass medical marijuana storefronts, because they get
so much informal criticism from their neighbors who all voted to make
those storefronts legal.  So DEA took to secretly flying in thugs from
all over the country to do these smash-and-grab raids, and then flying
them home again before they can be brought to justice.)
It turned out that my camera was poor enough that it couldn't take
good video while I was in motion (walking along the sidewalk), so
though I still have the audio track of my assault at their hands, I
did not get any decent photos of the perpetrators.  However, there was
a professional news camera person set up on the public sidewalk
halfway across the street at a mid-street transit stop (this was on
Judah St. in SF), who later told me that they had "made a deal" with
the thugs that the press would not film them in an identifiable way, as
long as they could get their shots of the smashed glass and broken
door of the storefront.
So in this case, the news media failed the public by explicitly refusing
to document the perpetrators of the incident; my camera failed me by
taking very blurred images.  But if it had taken crisp pictures, then
the thugs would've seen the pictures and deleted them.  I would have
wanted to put the camera into a mode where it would take pictures but
never show them (without some key that was not resident in the camera
or its media).
Since these thugs had no legal leg to stand on, I would've welcomed
intervention by local police, or by the thugs trying to seize my
camera or my media.  That would have provoked more publicity about
their actions which they were deliberately trying to cover up, which
would have included a lawsuit by me to recover the media if they had
seized it.
See, not all the people who don't want their pictures seized are
helpless in the face of official thugs.  When the thugs are violating
their own laws, which seems to be a very common occurrence, then the
people have redress against them.  The camera could have had valuable
evidence for that redress, as we have seen in many cases such as that
of Oscar Grant (shot dead in cold blood by BART police, who tried to
confiscate all cameras among the observers, but a train left the
station during the incident and the cameras of those passengers
You can't always do that.

@_date: 2016-02-06 19:58:39
@_author: John Gilmore 
@_subject: [Cryptography] Basic auth a bit too basic 
Well, the fact that Firefox hasn't done it means that the job has been
outsourced to the a popular extension.  Cookie Monster has a menu item
for it (pop-up menu -> View Cookies -> Delete Cookies for
example.com).  It also lets you delete cookies more generally, as well
as providing easy ways to set recurring policies like "For sites I
haven't otherwise mentioned, never accept cookies", "Never accept
cookies for the site I'm currently viewing", "Always turn all cookies
into session cookies for this site", or "Accept cookies temporarily
from this site, then after this session, go back to not accepting
[Are there are any "auth credentials" other than cookies that are used
 by more than a tiny fraction of web sites?]
In general, Firefox maintainers seem to take the attitude that it's OK
if Google, Facebook, and NSA can track you everywhere you go on the
web.  Fixing that seems much less important to them than making sure
that every website everywhere always works the way the third-party
designers intended -- even when the designers were malevolent (or
merely ignorant and manipulated by one of the big tracking companies).
Extensions that plug into Firefox, like NoScript, Cookie Monster,
Privacy Badger, Request Policy, RefControl, etc, are willing to break
some web sites in return for giving the *end user* control over who is
tracking them.
Merely getting rid of your browser's Referer headers by default will
eliminate a large amount of tracking, while breaking only a tiny
number of sites (that require Referer to defend against cross-site
scripting, a foolish idea since Referer lines are easy to spoof).  Try
adding RefControl to your browser and setting the default to "Block".

@_date: 2016-02-13 10:47:49
@_author: John Gilmore 
@_subject: [Cryptography] NSA's FAQs Demystify the Demise of Suite B 
This sounds like an obsolete 1990's argument -- why bring it up now?
What hassle and expense of dealing with export regulations?
In 2000 we forced the US export regs to change, so that no free
software, and no mass market software, has hassle or expense.  (That
was forced with set of a First Amendment lawsuits arguing successfully
that the government could not burden the "publication" of software,
just as it cannot burden the "publication" of English text
descriptions of encryption.)
The only folks who have to deal with US export controls on crypto these
days, as I understand it, are those who build custom, proprietary
software; or those who build hardware custom designed for
cryptanalysis (like a DES cracker).  When I last looked, the rest of the world had resisted the US desire to lock down mass market encryption products (sold to anybody
via mail order, or web stores, or in physical stores) and the desire
to lock down free software encryption products (like Firefox or
GNU Privacy Guard).  So again, in the rest of the world, only
custom proprietary software, or hardware, have to think about export

@_date: 2016-02-16 00:04:01
@_author: John Gilmore 
@_subject: [Cryptography] Proof that the NSA does not have a quantum 
You don't have to rob individual humans to create money.  You can
merely bet against them in markets.
It would be simple for NSA to play the stock market if it wants to
'create' money.  Look at how much money is made in speculating on
mergers and acquisitions -- then wonder how much better you could do
that job if you already had wiretaps on all of the relevant parties
(including the buyer, the seller, the regulators who'll have to
approve the deal, the market makers in those stocks, the disgruntled
major stockholders, etc).
And mergers are just a tiny corner of the overall stock market.  It
would also be easy to take money from ordinary people who were betting
the wrong way on an upcoming government report (e.g. about the level
of jobs or inflation or crops or manufacturing), or on what the
country's central government will do with interest rates.  The NSA
vacuum cleaner isn't quite the time-machine that would let you go back
and win horse races or do stock picks already knowing the winners.  But
if it even gives you an hour's notice of something that the public will
soon know, then there is *plenty* of money to be made.
NSA wouldn't even have to do that to American companies -- there are
trillions of dollars invested in foreign companies, on foreign stock
and commodities markets, where all it takes is one NSA "customer" to
ask, "what's the London stock exchange going to do tomorrow?" to give
NSA total cover for putting all the relevant taps into place.  Either
NSA or the "customer" could do all the rest.  (Thanks to Mr. Snowden,
we already have confirmation that they were tapping Brazilian oil
companies to extract economic information.)
One could even argue that developing the capability to use wiretapped
advance information to make money in a market is a useful offensive
"cyber" weapon for wartime, since it could be used to quietly and
painlessly extract resources from a country at war with the United
States, for the benefit of the United States or at least of one of its
agencies.  And hey, how can you really know it would work in a war,
unless you test and refine it all the time in peacetime?
The level of impunity that federal government employees now enjoy is
virtually unlimited - the only ones prosecuted are those who threaten
to blow the whistle on the rest.  It's surprising to me that NSA seems
to care at all what budget Congress appropriates to them.  But maybe
they feign care about that in one of those "whole departments running
deception operations".

@_date: 2016-02-19 19:07:04
@_author: John Gilmore 
@_subject: [Cryptography] Apple 3rd Party dilemma 
For a discussion of where this doctrine came from, what effects it
has had over the years, and where it may be going, see EFF's blog
posting from last June:

@_date: 2016-01-13 18:06:39
@_author: John Gilmore 
@_subject: [Cryptography] TRNG review: Arduino based TRNGs 
Because if you made an integrated chip with an I2C interface that claimed
to produce random numbers, nobody in this community would trust it!
And anything that shipped in high volume would just integrate the
random number circuitry into a VLSI chip (as modern CPUs have done),
rather than using even a tiny extra chip and the wires to talk to it.

@_date: 2016-01-18 08:35:43
@_author: John Gilmore 
@_subject: [Cryptography] Plan to End the Crypto War 
The discussion is slightly interesting, but please notice that it
isn't relevant to Chaum's proposed design.  Beating the stuffing out
of a propped-up strawman isn't much of a feat of strength.  cMix
doesn't have a golden key, it doesn't split that key, etc.
It's worth reading Chaum's cMix design to at least *understand* what
you may want to criticize.  Try:

@_date: 2016-01-18 12:35:12
@_author: John Gilmore 
@_subject: [Cryptography] OpenSSL minimal "safe" configuration? 
I am sympathetic, but have just had two experiences that suggest
slash-and-burn-the-insecure-crap may not be the best approach.
I am having trouble with ssh-ing to various boxes around my office now
that I've "upgraded" the security of ssh to avoid low security D-H.
Turns out that many embedded systems are not interoperable with
curve25519, so if you disable ssh-ing out with low security stuff,
rather than just making curve25519 the preferred default, ssh
connections fail.
Similarly, some people are now having trouble sending me email,
because my MTA doesn't use the latest Diffie-Hellman parameters.
STARTSSL, as implemented in the field, has this curious attribute that
if the command is not recognized, the mail gets delivered in
plaintext; but if the command is recognized and then the SSL
negotiation fails, the mail is not delivered, remains queued at the
sender, and is eventually dropped.  When email senders disable older
D-H parameters, they don't fall back to plaintext, they create an
inability to communicate.
All this takes even more sysadmin and debugging time than the initial
investment of ten minutes required to "cut the insecure crap" -- and
may require reverting to using insecure crap some of the time.

@_date: 2016-03-01 16:16:04
@_author: John Gilmore 
@_subject: [Cryptography] The FBI can (almost certainly) crack the San 
Ron Garret suggested:
You may even be able to do a simpler attack, by just filtering out all
the system's attempts to write to the flash chip.  The standard flash
chip interface has a Write Protect signal; shorting that pin to
permanently "on" should prevent any alteration.  Then the software
won't be able to erase keys, won't be able to increment the count of
bad attempts, etc.  The question is whether the system will fail for
other unrelated reasons if its flash chip becomes mysteriously
I think the attack Ron suggested will work, as long as the system's
only nonvolatile storage is in a separate flash chip.  You can also
speed it up substantially by replacing the flash chip with a
flash-chip emulator.  The emulator has a Zero Insertion Force socket
to accept the old flash chip, but it also has enough RAM to save a
full copy of the flash chip.  The emulator can be commanded externally
to copy the flash chip's contents into its RAM.
The emulator acts just like a flash chip to the system, but it is not
actually writing data to the flash chip that sits piggyback on it, nor
erasing the flash chip; it's simulating all the reads, writes, and erases by
using its own RAM instead.
So, after every five attempts, you recopy the original flash chip
contents into the emulator's RAM, then do the next five attempts.  If
the phone tries to erase the keys in flash, it's just erasing the RAM
copy.  If the phone tries to remember how many times you tried a wrong
password by writing the count to flash, you are restoring that count
after every five tries.  You may not even have to reboot the phone,
depending on whether it retrieves the number of password attempts from
the flash chip or from its ordinary main memory.(*)
As the Apple iOS Security document (October 2012 version page 20,
October 2014 version page 49, September 2015 version page 58) says:
  Effaceable Storage: A dedicated area of NAND storage, used to store
  cryptographic keys, that can be addressed directly and wiped
  securely.  WHILE IT DOESN'T PROVIDE PROTECTION IF AN ATTACKER HAS
  PHYSICAL POSSESSION OF A DEVICE (emphasis mine), keys held in
  Effaceable Storage can be used as part of a key hierarchy to
  facilitiate fast wipe and forward security.
Here's the flash chip detailed interface spec, and emulators:
    (*): If the phone only stores the number of password attempts in main
memory, just cut off its power and reboot it after every 5 guesses,
and you'll get an infinite number of guesses without having to mess
with the flash chip.  (But you'll have to wait for it to reboot each
time.)  If it stores it in flash and retrieves it from flash, then
you'll need the flash emulator to restore the flash after every 5

@_date: 2016-03-03 00:54:53
@_author: John Gilmore 
@_subject: [Cryptography] Side channel attack on OpenSSL ECDSA on iOS and 
This is nice work, showing how you can extract the secret keys from
many ECDSA signing operations by putting a loop of wire next to the
phone or tablet doing the signing, running that into an audio port
on an attacking computer, and analyzing the resulting signal to
detect doublings and additions to extract secret key or nonce bits.
They can do the same from power fluctuations on the USB charger port.
(Paul Kocher turned the smartcard world upside down with a similar
power-analysis side channel attack back in the '90s; see reference
KJJR11 in the paper.  Smartcard makers rushed to hire his company to
fix their products, which were previously all leaking their secrets.
He patented both the attack and many methods of defeating it; the
resulting royalty stream of perhaps a penny per smartcard chip (times
4 billion chips/year) has made his company very successful.  Rambus,
another tech company that made its money licensing DRAM interface
patents, bought the company in 2011 for $342M.  See:
   and
 )
The best part of the paper is how the authors notify all the software
projects before publication, most of which work to fix the
vulnerability by using constant-time algorithms.  Except OpenSSL,
which says:
  "hardware side-channel attacks are not in OpenSSL's threat model",
  so no updates are planned to OpenSSL to mitigate our attacks.
Clue -> OpenSSL maintainers?
Or should that be Clue -> OpenSSL users: don't use OpenSSL if you want
your users' private keys to stay private.  A student who's read this
paper might be sitting near you (or offering you a free charge).
PS: I wonder if Apple's code-signing machine is protected from this
attack.  J. Edgar Hoover successor's next motion to a judge: "Defeat
the terrorists by forcing Apple to put a simple wire loop near the
machine Apple uses to sign its software."

@_date: 2016-03-04 12:34:50
@_author: John Gilmore 
@_subject: [Cryptography] Side channel attack on OpenSSL ECDSA on iOS and 
Thanks, Rich, for providing more context than the paper authors did.
I also note that a very recent update to Ubuntu's OpenSSL package
provided more constant-time modexp implementations:
    * SECURITY UPDATE: side channel attack on modular exponentiation
    - debian/patches/CVE-2016-0702.patch: use constant-time calculations in
      crypto/bn/asm/x86_64-mont5.pl, crypto/bn/bn_exp.c,
      crypto/perlasm/x86_64-xlate.pl, crypto/constant_time_locl.h.
    - CVE-2016-0702
      (also for rsaz-x86_64.pl and rsaz-avx2.pl, on some Ubuntu versions.)
        These were pushed out as a security update in the last few days.

@_date: 2016-05-01 00:13:20
@_author: John Gilmore 
@_subject: [Cryptography] USB 3.0 authentication: market power and DRM? 
I love the concept of the new Power Delivery modes (100w of power, by
sending up to 20v at 5A over suitable cables).  If done right, I can
see people wiring their house and business wall outlets (and cars)
with much safer, more compact, and Internet-enabled USB-PD sockets,
replacing 110v or 220v wiring for a lot of uses.  Particularly in
places where the power source is DC anyway (like solar or cars) and/or
where they want data or video connectivity as well as power.
But I don't see how authentication fits in technically.  It looks like
it's there to build monopolies.
The alleged problem statement seems to be: Some expensive devices will
decline to spend the money to protect themselves from overvoltage or
overcurrent situations, thereby being damaged by out-of-spec power
supplies.  We need to authenticate chargers so this won't happen.
Let's examine this from an engineering point of view, then look at the politics.
The One Laptop Per Child folks built their ~$100 laptops with power
inputs that accept 11 to 24 V usable, -32 to +40V tolerated without
damage.  This lets them be used with all kinds of janky third world
power, direct plug-ins to solar panels (the laptop does MPPT to
optimize charging direct from solar too), etc.  So it'll charge at +12
thru +24V, will decline to accept power at +35V or -12V or -24V but be
undamaged.  If you exceed this, e.g. by feeding 220V AC power to that
input by accident, it will blow an internal fuse that's easy for a
hardware tech to repair.  But that's a well designed yet cheap device.
Expensive USB3-PD devices could use similar circuitry to protect their
expensive devices from overvoltage or overcurrent.  Or, they could
spend years in standards committees designing authentication.  But I
don't see how the standards committee solves the problem.
Let's suppose that an expensive phone does USB3 authentication of its
putative power source and decides that the authentication FAILS.  Oh
my god, it's been attached to a "counterfeit" charger or a "defective"
cable!  How does it protect itself?
If it doesn't have circuitry that disconnects it from the power wires,
it will fry anyway.
But if it does have circuitry that disconnects it from the power
wires, why not trigger that disconnect based on measuring overvoltage or
overcurrent, rather than triggering it on failed authentication?
It seems to me that a counterfeit charger could short 110V down
the USB3 cable, with or without authentication.  What protects
the phone from that?
Similarly, what prevents a counterfeit charger from using a chip and a
flash image (including a signed certificate) that's identical to the
one in a certified, tested, approved, paid-up charger.  The
counterfeiter only has to clone that real chip one time, then they can
put it in all their products.  Or they could actually buy the real
chips on the open market, and just clone the firmware and the cert.
Yet their shoddy wiring, Grade Z external components, faulty housing,
etc, around that chip could still short 110V down the cable during the
wrong phase of the moon.  So the authentication will pass, but the
voltages and currents will at sudden times be dangerous.  I guess your
expensive phone will fry anyway, despite the crypto, because you
didn't spend 20c on protective components in the phone.
What am I missing here?  It looks like the alleged solution doesn't
solve the alleged problem.  Perhaps there's something else going on here.
"Can and do" is a misnomer here.  There are no counterfeit USB3-PD
chargers, because there are essentially no USB3-PD chargers on the
market yet.  I've been looking.  So there isn't a problem "yet"
from fake USB3-PD gear...
Perhaps you are talking about counterfeit USB2 chargers, that don't
even negotiate the voltage, just have resistor / capacitor networks
that signal the option to draw >500ma power at 5v?
Now let's look at the politics.
It is well understood in the consumer electronics industry how to use
authentication requirements to exert market power.  To be able to
build a peripheral devie that plugs into an iPhone, you have to
include a chip made only by Apple.  The phone won't talk to you
without having that chip in your thingy to answer a crypto challenge
sent by the iPhone.  Apple will only sell the chip to you if you give
them a significant part of the purchase price of the peripheral.  The
chip authentication is the technical hook that drives you to sign a
contract with Apple to become an "Apple Certified Peripheral".  There
are no "Apple uncertified peripherals" in the market, they don't sell
because they don't work, because Apple forces them to not work, using
Apple's control over the iPhone firmware to not let them work.  Didn't
you wonder why every iPhone dock and iPhone charger and iPhone cable
was vastly overpriced?  Even from a variety of competing third-party
manufacturers?  That's Apple raking in their 40% or whatever.  And if
your gadget competes too well against one of Apple's peripherals,
maybe they won't certify you at all.  Like the authentication-checking
on apps in the "app store": at any time, Apple can put you right out
of business, at their whim, and you have no recourse.
My initial suspicion is that THIS is what the USB3 "authentication"
spec is for.
A very similar scheme is the technical hook that forces you to sign a
contract to put DRM into your products in order to be able to make an
HDMI product that will interoperate with other HDMI products.  In that
case it isn't even to extract money for a single vendor -- it is to
exert market power on behalf of a group that doesn't even make devices

@_date: 2016-11-14 23:04:00
@_author: John Gilmore 
@_subject: [Cryptography] highlights of crypto history 
The Wikipedia article claims that the ciphers were actually *provided
to* the conspirators by Queen Elizabeth's spymaster Walsingham --
which would make them easy to decrypt and forge (both of which were
done).  It does not cite a source for this claim (and The Codebreakers
doesn't make this claim either).  Other sources claim that Mary made
extensive use of ciphers and that more than 100 different ones were
found in her quarters after she was arrested for this plot.  Perhaps
someone who knows the historical record can edit in a reference?
BTW, here is a page from the UK National Archives about "a page of
ciphers used by Mary Queen of Scots c. 1586":
  The page contains both a code and a cipher.

@_date: 2016-11-16 00:32:33
@_author: John Gilmore 
@_subject: [Cryptography] On the deployment of client-side certs 
The whole concept that the user has "an identity" that is somehow
relevant to every website, is part of the problem.
Using the same identity or cert with every website is like using the
same username and password with every website -- a very bad idea for
security and privacy.

@_date: 2016-11-16 16:24:56
@_author: John Gilmore 
@_subject: [Cryptography] On the deployment of client-side certs 
It seems to depend on centralized "cloud" servers.  You forgot to
mention this.  (Normally, what is colloquially known as a "mesh" does
not depend on servers that are not part of the set of clients.  The
clients "mesh" with each other, not with some third party.)
Its dependency on this/these servers is undefined and undocumented.
Which operations work when the server is down or the network to it is
unavailable, and which do not?
The link to "how to set up your own portal":
  is a 404.  And the source code links are nonexistent.

@_date: 2016-11-20 23:12:57
@_author: John Gilmore 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
The beauty of allegedly-random numbers is that you can, and should,
xor them with OTHER independent allegedly-random numbers to produce
something that is no weaker than the strongest of them.
So if you trust RDRAND and somebody else trusts interrupt timing,
mash the two together and you can both trust the result.
And if somebody someday violates your trust in RDRAND, either with a
microcode patch, a virtual machine override, or by having subverted
the chip design years ago, the bastards will also have to predict or
subvert the interrupt timing in order to reduce their search space
enough to break your TLS.
PS:  Isn't Haskell a portable language?  There are plenty of systems
that run on systems other than modern x86 chips.  How could it depend
on RDRAND and remain portable?

@_date: 2016-11-21 13:01:19
@_author: John Gilmore 
@_subject: [Cryptography] Security of Ubuntu RNG pollinate? 
Dustin Kirkland wrote a randomness client and server for feeding
Ubuntu virtual machines that don't have good randomness sources, which
is now a standard feature in Ubuntu since 14.04 Cloud instances.  See:
    This now runs on the first boot of the virtual machine, before the
scripts that create the machine's SSH host keys, for example.  The
idea is to improve the unpredictability of those keys.  In theory this
is a good idea.
I am wondering just how hard this would be for NSA or another passive
attacker to break.
I looked briefly at the client code.  The client, pollinate, is a
shell wrapper around curl (which does an http or https access).  The
protocol is an HTTP or HTTPS POST access, passing "challenge=" and a
sha512 hash of (by default) 64 bytes of "random"(*) data as a nonce.
It passes a User-Agent value that includes the exact software versions
of pollinate, curl, and cloud-init, as well as identifying the Ubuntu
release, Linux kernel version, and CPU type.  The result of the access
is a sha512 hash of whatever was passed, and some random data.  The
client checks that the hash matches, then hashes the entire response
as well as some debug and microsecond packet timing info, and writes
that hash of the response to /dev/urandom.  The code is shipped with
the certificate of host "entropy.ubuntu.com" and by default makes an
HTTPS access to it, checking via the certificate that it hasn't been
My initial concern is that the entire transaction is visible to
Since the entire problem is that the host has little or no actual
randomness, the "random" nonce is not only probably not random, but
actually lets NSA validate their guess about the initial state of the
kernel RNG.  Similarly, the User-Agent string lets NSA determine
exactly what software is running in the system.  Example:
  User-Agent: pollinate/4.23-0ubuntu1~14.04 curl/7.35.0-1ubuntu2.9 cloud-init/ Ubuntu/14.04.5/LTS GNU/Linux/3.16.0-77-generic/i686 AMD/Athlon(tm)/64/X2/Dual/Core/Processor/3800+
Is there really any point in passing this detailed system info, EXCEPT
to help an attacker, rather than just using curl's standard User-Agent
("curl/7.35.0")?  A recent change *also* adds the platform uptime and
idle time "to help detect abuse"!  See:
  Do we agree that if the transaction used plaintext HTTP, and if NSA
happened to be capturing those packets, then NSA would be able to read
the non-random nonce and the truly random response, and then model the
effect of the pollination on the kernel RNG, thus knowing how to
predict future "random" numbers from that kernel?
(The transaction uses TCP, which uses kernel randomness to produce
the client's initial sequence number, thus giving NSA another check
against their model of the kernel's random pool.)
However, the transaction uses "secure" HTTPS.  But since this HTTPS
connection has no reliable source of randomness to draw upon, an
eavesdropper can break Diffie-Hellman by guessing the non-random
"random" value retained by the client in the HTTPS negotiation.  (The
server's nonce in response is available to both the client and the
eavesdropper.)  Thus I suspect that an eavesdropper can look inside
the poorly secured HTTPS transaction and see the plaintext.  And thus
the eavesdropper can predict future "random" numbers from the client's
kernel -- like those used to generate the host's private key for SSH.
What am I missing?
An additional security issue is that this command is apparently run
just once, at system startup, in a just-installed machine that's
running a bare Ubuntu release.  It tells the eavesdropper exactly what
release is running, and also tells them the IP address of a brand-new
virtual machine that has never yet seen any security updates since the
release was cut.  Merely monitoring the IP addresses of accesses to
entropy.ubuntu.com will notify any intelligence agency of great target
machines to deploy a zero-day or already-fixed attack against.  If
they spend the effort to crack the HTTPS transaction, they also find
out its exact configuration, but probably the IP address is good
enough for most nefarious purposes.
(Of course, the passage of time after the access to entropy.ubuntu.com
will increase the likelihood of known security holes becoming patched.
And passage of time will also increase the likelihood that the kernel
random number generator's state will evolve in ways that increase the
search space enough to make it infeasible to model the kernel RNG
state.  But how much time?  Minutes, days, weeks, years?  I don't
think anyone except NSA has tried to calibrate this against actual
virtual machines.  Know any grad students looking for a security
I think that pollinate only runs by default in the Cloud Ubuntu
images, so it won't report to NSA that you have just installed a
brand-new unpatched desktop or server Ubuntu release.  However there
are other default Internet accesses in brand-new machines, e.g. for
timezone lookups, the initial apt-get update, or Firefox's initial
access to the Google web blacklist database that gives you an initial
Google cookie before your Firefox window has even finished painting on
the screen, that probably do make them pretty easy to identify, if you
know where to wiretap.
Can anyone confirm or deny these suspicions of mine?
And how would one improve the pollinate protocol or implementation, if
possible, to improve its resilience against these attacks?

@_date: 2016-11-27 22:06:02
@_author: John Gilmore 
@_subject: [Cryptography] Use of RDRAND in Haskell's TLS RNG? 
Hi David!  This is a bad reason to not allow raw access.  You have confused the means with the end.
The problem *isn't* that raw access is available.  The problem is that
you defined a *mode bit* that makes reads of the Whitened device
produce data that was Raw rather than Whitened.  By setting that mode
bit, one program could violate another program's assumptions.  The
mode bit is the attack vector.  The raw access is not.
If you had merely not used a static mode bit for raw access, this
would not be an issue.  If e.g. you had two instructions, one for raw
and one for whitened; or if the raw data was available some other way
such as via a control register read, that would not cause any security
issue.  Intel actually did this for the RDSEED instruction -- which
gets around the final level of RDRAND (AES counter mode) but foolishly
didn't take this opportunity to provide a distinct path to read the
raw bits before the whitener.  They even took two shots over four
years to get this right, and still got it wrong.
PS: Don't I recall that a VM kernel can violate the specs of RdRand or
RdSeed anyway, by disabling the instruction and emulating it?  As I
recall, this was explained as a way to make "dualed" or
"i-tell-you-three-times" systems produce the exact same random
numbers, so they wouldn't take divergent execution paths.  But of
course including a way to make the CPU trap when it encounters the
unprivileged RDRAND instruction makes RDRAND into a "virtual random
number" facility that causes the same "attack vector" problem.  A
virtual machine can't trust the random numbers that the RdRand
instruction gives it, because the VM kernel can subvert the
instruction.  So not only did you fail to provide a secure RNG for
virtual machines, but then you failed to allow raw access due to
claiming to avoid an attack scenario that your architecture can't
avoid anyway.
Citation: The RDRAND instruction description on page 1661 doesn't say that the
VM kernel can trap upon its execution -- but another part of the
manual does.  See page 3738 under "VMX Non-Root Operation":
  RDRAND. The RDRAND instruction causes a VM exit if the "RDRAND
  exiting" VM-execution control is 1.
and page 4468 of the PDF under "Enclave Operation" for the details.

@_date: 2016-11-28 01:30:35
@_author: John Gilmore 
@_subject: [Cryptography] RNG design principles 
The UEFI table seems as good a way as any to pass this info.
What are the specs for the "random" seed that is passed in?
It would be best if the interface distinguished the output of a True
Random Number Generator (TRNG) from the output of a keyed
cryptographic pseudo random number generator (PRNG).  [For the same
reason that RDRAND and RDSEED are two different opcodes.]
Is this table an appropriate way to pass a system-unique seed (that
doesn't change from reboot to reboot, but which makes this system
unique from every other system or VM that was installed with identical
OS software)?
Is this table also an appropriate way to pass a TRNG output that was
freshly generated just microseconds ago during the boot process?  If
a system had more than one of such values (e.g. a system-unique seed and
one or more local TRNG outputs) then how should that system configure
this UEFI table parameter?  Should it appear multiple times in the
table, should the table entries be concatenated, etc?
The patch itself
  The reboot notifier uses get_random_bytes() to overwrite the seed
shortly before a kexec.  The result is that even if the seed was
originally TRNG output, at this point it becomes a less useful PRNG
output.  It might be better to XOR the new random data into the prior
seed, so that if it was truly random it would not lose that property.
Or would it be better if the kernel destroyed the seed in RAM
immediately after consuming it?  That way, malware that arrives later
and escalates its privileges can't poke around main memory to see what
the initial seed was.  If so, the kernel should perhaps replace the
seed with fresh random data once it thinks it has plenty of locally
generated entropy (e.g. seconds or minutes later), and mark whether
that data is from a TRNG or PRNG.

@_date: 2016-11-30 04:07:55
@_author: John Gilmore 
@_subject: [Cryptography] RNG design principles 
I like the direction that this conversation is taking.  It's starting
to get implementable for a real improvement in early unpredictability,
even in poorly designed environments.
Grub should not need to do any hash functions, nor crypto processing.
Its role should be completely administrative, "shuffling papers" to
get the random bits from where they came from, to where they are going
in the early kernel.
The passed data structure should be sufficiently general that it can
pass a set of RNG seeds, each one distinct and each one marked with
its characteristics.  The kernel RNG initialization code should be the
place to decide how and whether to use multiple offered seed sources.
The characteristics passed with each seed should include:
  True vs. Pseudo random bit source
  Static vs. Updated vs. Ephemeral random bit storage
  Observable vs. Local
  Estimated "entropy" of the entire seed
Static would be a seed set once at system install time, for example.
It is expected that the same seed would be seen by many boots of the
system, but the seeds on ten otherwise identical systems would differ.
Updated would be a seed that gets updated once or twice per boot, so
that only in unusual cases will the same seed be seen by many boots.
This requires that the seed be stored on writeable media, and that
there's code elsewhere to find and update it.
Ephemeral would be a seed extracted from an RNG, rather than from
Observable would be a seed that has arrived via a network transaction,
which is more likely to be observed by an enemy than a transaction
that merely accesses a Locally generated or stored seed.  If Grub has
done a DHCP access (to find out its own IP address while booting from
a network) then, with no extra packets, it could also have requested a
seed from its DHCP server (using a protocol that has an Internet-Draft
but which has not yet been implemented).  If it received one, it would
pass in an Observable seed.  The same flag should be used for seeds
read from a network filesystem (or an iscsi drive).  Note that
Observable seeds are also Man-in-the-Middleable by active attackers.
The code elsewhere in the system that revises the stored seed after
sufficient unpredictability has been gathered by the kernel, should
only write to seeds with the "Updated" characteristic (which means
that this characteristic, as well as the bits, needs to be stored in
the file).
I would recommend that the kernel RNG initializer should never
consider either a Static seed or an Observable seed to contribute to
its "entropy" estimate.  Even if these seeds had been originally
generated by a TRNG, its long term presence over months or years (or
flying across the network) makes it more likely to be predictable to
an attacker.  It is possible that the RNG initializer should never
trust the entropy estimate of *any* of these seeds.
Can we define that data structure so that either an EFI table can
point to it, and/or a new record type added to the x86 bootloader
setup_data interface (defined here)?:

@_date: 2016-10-08 11:34:39
@_author: John Gilmore 
@_subject: [Cryptography] Security Fatigue 
Not sure what the point is.  I presume it relates to the first file
inside the 600MB "2016 LABE Level 1 photo.zip" file having the name
"/".  But the Linux unzip command detects and bypasses it anyway:
  warning:  stripped absolute path spec from /
  mapname:  conversion of  failed
What does this have to do with "security fatigue"?

@_date: 2016-10-17 04:35:57
@_author: John Gilmore 
@_subject: [Cryptography] changing crypto policy?  Not Deborah Ross 
While current Congressional oversight of the intelligence agencies is
irrelevant or actively harmful, Deborah Ross doesn't seem like the
reformer that L. Jean Camp suggested she might be.  Try Ross's
National Security plans page here:
  There she proposes lots of useless or harmful but "tough" measures.
Here's the most relevant one for us:
  Strengthen our intelligence capabilities: When Republicans shutdown
  the federal government for 16 days in 2013, roughly 70% of our
  nation's intelligence personnel were off the job.  These analysts
  are on the front lines of identifying and disrupting terrorist
  plots, and helping us learn about the inner workings of ISIS.  They
  should have the resources they need and certainty that outside
  parties won't politicize or sabotage their operations.
Ross's whole paragraph is deliberately misleading.  During that
government shutdown, all NSA and other classified personnel key to our
military and anti-terrorism programs remained on duty.  The list of
such departments in each federal agency during the shutdown is archived here:
    Here she says a tiny bit more about intelligence:
    Protecting Americans is Deborah's top priority. She believes our
  national security is strongest when we use all the tools at our
  disposal: a modern military, the most sophisticated and capable
  negotiators, and an intelligence community that will stay one step
  ahead of our enemies.
Doesn't sound like somebody whose first priority would be to terminate
NSA's domestic spying, NSA interference with computer security, reform
government secrecy, chop NSA's budget as punishment for past bad
behavior, etc.  A quick web search for "Deborah Ross" and "NSA" turned
up exactly one article, which includes nothing from her or her aides
that even addresses NSA, wiretaps or intelligence agencies.  It's
about her opponent Richard Burr being a big NSA-lover and working with
Sen. Feinstein to build a better police state.  Ross absolutely could
have made mass surveillance one of her campaign issues, since her
opponent is hip-deep in it, but she didn't:
  Also, electing her would not get her onto the Intelligence committee.
It would merely remove her opponent from it.
If any Cryptography list member actually wants to vote for someone
whose stated policy is to stop all NSA domestic wiretapping and
"metadata" collection, skip Deborah Ross and vote for Gary Johnson for
President.  He has a clue on the issues that concern this mailing list:
    PS: Recently leaked Podesta emails confirm that Ms. Clinton has no
plans to improve anything on the NSA front:
  "Clinton won't budge on mass surveillance stance, leaked emails reveal"

@_date: 2016-10-29 21:38:58
@_author: John Gilmore 
@_subject: [Cryptography] How to prove Wikileaks' emails aren't altered 
Prepare to be shocked and offended by the post-9/11 idiocy of government
  Post office photocopies envelopes of all mail sent in the US, says NY Times
    Jul 03, 2013 06:40 pm, by Martha Neil
  It isn't just when using the Internet and making cellphone calls that Americans should assume what used to be considered private information may be known by corporate and/or government entities.
  In what started as an effort to combat anthrax threats, the U.S. Postal Service photocopies the envelopes of all mail sent in this country, the New York Times (reg. req.) reports. It isn't known how long the government saves the images of the 160 billion or so pieces of mail sent annually, or exactly how they are stored and used.
  The Mail Isolation Control and Tracking program had been secret, but
  was revealed by the FBI last month in discussing an investigation of
  ricin-tainted letters reportedly sent to President Barack Obama and
  New York City's mayor, among others.
  "In the past, mail covers were used when you had a reason to suspect
  someone of a crime," Mark D. Rasch told the newspaper. He previously
  served as director of the federal Justice Department's computer
  crime unit.
  "Now it seems to be 'Let's record everyone's mail so in the future
  we might go back and see who you were communicating with.' " Rasch
  said. "Essentially, you've added mail covers on millions of
  Americans."
PS: So if you send a postcard, they have saved all the "content" as
well as the "envelope" information...

@_date: 2016-09-17 14:46:37
@_author: John Gilmore 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Simply warning about all detected undefined behavior would produce so
many warnings that nobody would care to read them, and nobody could do
anything to resolve them.  See -Wstrict-overflow=5 in the gcc manual,
for example.
  int foo(a) { return a+1; }
This function is undefined, because adding 1 to A might cause an
integer overflow if the value of A happens to be INT_MAX, which in
most cases cannot be detected at compile or link time.

@_date: 2016-09-18 15:05:20
@_author: John Gilmore 
@_subject: [Cryptography] Ada vs Rust vs safer C 
Ron Garret said:
As Florian said, GCC and Clang do take advantage, to produce faster,
smaller code for the very very common case where no overflow occurs.
If you want your addition done mod 2^n, declare your variables
"unsigned".  Unsigned arithmetic has well defined, portable behavior
in C.  If you declare them as signed integers, the program's behavior
at overflow is undefined.
Two's complement arithmetic is very convenient for hardware, but has
some counterintuitive mathematical properties at the margins.  The C
language is designed to let the obvious hardware instructions be used
for its constructs, so it classes those edge cases as undefined
behavior (that indeed may vary from one machine architecture to
This design decision was better for execution speed than for software
reliability.  In future revisions of the C standard, someone should
argue that maybe we have enough execution speed now but not enough
software reliability.  So maybe the language should evolve in a way
that defines these edge cases (and requires slower code on some
oddball architectures).  C is a living language and it does evolve.

@_date: 2016-09-20 00:08:26
@_author: John Gilmore 
@_subject: [Cryptography] Ada vs Rust vs safer C 
There is usually a reason that long, complicated functions are long
and complicated!  And it's not stupidity.  It's the complicated nature
of the job that that function is trying to do.
Inserting pragmas that are basically comments is unlikely to produce
any NEW bugs in the code.
Refactoring a long, complex function is very likely to introduce new
So what makes refactoring the "correct" option?  Suppose there is
*not* a complete test suite that gets 100% test coverage of the code
in question?  (That's extremely common.)  If so, then you don't just
have to rewrite the function; you also have to write test cases from
scratch, and validate them on the old code before trying them on the
new code.  Just doing that is a major project, and when you're done
you haven't started the refactoring yet.
Or you could just refactor, introduce bugs, and not really try very
hard to detect them before shipping the buggy code.  But I thought the
point of the exercise was to REDUCE vulnerabilities in the code, not
increase them.
PS: There is/was a long, complicated function in GDB
("wait_for_inferior") that figures out what to do when the program
being debugged stops running and the kernel reports back its status.
It was 600 lines long (including many comments) when I became the GDB
maintainer.  It was very hard to modify it without introducing new
bugs, but modifications were needed to fix bugs and support new
machine architectures and new object file formats and such (even
though such things were heavily paramaterized, some new capabilities
needed to be created in this function).  Over the three or four years
that I maintained GDB, I did gradually split things out of it to make
it more maintainable.  But only toward the end of that period was I
able to make even small changes without introducing subtle bugs.  This
is the kind of code where you hesitate to fix Bug X because you know
you'll introduce Bug Y and Bug Z in the process.  These were not the
sort of bugs that our test suite would find.  These were the sort that
only the thousands of people running GDB in the field, on unusual
architectures and in unique circumstances, would find.  Only after
several years of this, did I have enough understanding *in detail* of
what that function did, to be able to modify it without invalidating
some assumption that was depended upon 300 lines away.
PPS: It's one kind of job for an academic or researcher, whose main
objective is to write a paper or get a degree, to build
non-production-quality tools for analyzing programs.  It's another
kind to keep a large, complex, production program running reliably
while evolving it.  In the real world you can't just say "rewrite your
production 600 line function so my flaky research tool won't fail on
it".  If you do, the customers/users of your tool just go away,
quietly or with parting insults.

@_date: 2016-09-27 23:11:45
@_author: John Gilmore 
@_subject: [Cryptography] Bamford: Over Eight Years, 
Forwarded-By: "Dave Farber" by James Bamford in Foreign Policy, 2016-09-25.

@_date: 2017-04-11 13:06:43
@_author: John Gilmore 
@_subject: [Cryptography] Key escrow scheme 
This seems to me to be the single-point-of-failure-prone step.  Why do
you think that the "cloud" will still deliver up this small opaque bag
of bits many years from now when the share-holders so desparately need
it?  Is somebody paying the bill for that storage all along?  Can that
person decline to pay, some year, and unilaterally make the recovery
key disappear?  Or, if the bill-payer is authorized over this account,
can't they just submit a request to delete this blob?  What happens to
your system's data when the cloud storage company fails (after clouds
are replaced by vapor, for example, as the trendy new tech company VC
I thought that the point of secret-sharing was to distribute the
needed information in such a way that no single point of failure could
cause the information to be lost.  And that e.g. a 3-out-of-7 secret
share would mean that four failures would still not disable the
ability of the remaining three to recover the secret.  Thinking this
way suggests that the entire "recovery blob" should be stored INSIDE
the shared secret, rather than being encrypted by the shared secret.

@_date: 2017-12-04 12:16:03
@_author: John Gilmore 
@_subject: [Cryptography] Cryptocurrency: CME Approved, Coin Paychecks, FED, 
Short selling is a key signal in the market.  It provides a way to
make money by betting that a stock is priced too high (for any
reason).  A market that lets you bet that a stock will go up (by
buying it, "going long" on it) but that prevents you from betting that
a stock will go down (by shorting it) tends to produce less accurate
information about the underlying company - particularly when it costs
too much.  Since that situation is when "long" owners of the shares are
likely to lose money, short sellers can provide a warning to discourage
buyers from investing when they're likely to lose.  Of course, those
prospective buyers would actually have to LOOK and HEED that warning
to avoid losses; it's not automatic.
I recently read a great book about stock trading that goes into great
detail about short selling.  It's a classic from 1923:
  Reminisces of a Stock Operator
  Edwin Lefevre
Well worth reading if you have any serious interest in stocks.  The
protagonist worked his way up from making penny margin trades on
stocks as a teenager in the 1890s, to being a big wheel in the New
York stock exchange.  You can borrow it (one person at a time, in your
browser) from the Internet Archive here:
  To borrow books, you'd need to make a free Internet Archive "library
card" (i.e. login account).

@_date: 2017-12-26 00:21:41
@_author: John Gilmore 
@_subject: [Cryptography] Rubber-hose resistance? 
Not only SSDs, but also spinning magnetic hard drives, implement ATA
Secure Erase.  It is the standard way to erase a magnetic hard drive
these days, and generally does a better job than "writing zeros on
every block" or "writing random numbers on every block" because the
firmware actually knows the characteristics of the drive, can erase
remapped blocks, etc.  It's also much faster and doesn't tie up the
CPU nor bus with transferring all those zeroes.
ATA Secure Erase erases the *entire* drive.  When I travel
internationally, I bring a USB stick with a bootable Linux
distribution.  Before crossing a problematic border like the US
border, I boot the USB stick and use it to erase the internal hard
drive completely.  Any data that needs to stick around permanently was
already scp'd to a server that I trust, before erasure.  After
crossing the border, I boot the USB stick and reinstall Linux on the
hard drive.  (If the goons have had access to my USB stick, all it has
on it is a standard Linux distro.  And rather than depend on it after
they meddle with it, I can just grab a fresh Linux distro from a
reliable Linux distro server.)
There's a similar sort of erasure called the TRIM or "discard" command,
that tells the drive that a certain list or range of blocks are now
garbage and need not be retained while garbage collecting an SSD or
SDcard.  For example, the mke2fs command in Linux issues a TRIM for
the entire partition that it is creating a filesystem in, unless the
"-E nodiscard" option is set.
Foolishly, though, there is no SECURE TRIM command that would require
the firmware to explicitly erase all copies of the data from those
blocks -- so TRIM is ***NOT*** any good for data security, just for
performance.  Since stale copies of blocks from any part of an SSD or
SDcard can be intermixed with other blocks from anywhere on the medium,
you have to securely erase the *entire* medium to be sure of removing
all the copies of your previously written data.
PS: Apple's laptops screw you from being able to securely erase your
internal hard drive, by "locking" the drive in the BIOS.  You can
still erase it, but you have to open the case and power cycle the
drive by removing it briefly.  The lock is released by the power
cycle, and THEN you can issue a Secure Erase command (e.g. using the
Linux hdparm command).  Another good reason to skip Apple products.

@_date: 2017-12-26 00:41:14
@_author: John Gilmore 
@_subject: [Cryptography] Rubber-hose resistance? 
One follow-up re ATA Secure Erase: Some SSD's really do not implement
it correctly.  One even claimed to have executed it, but really did
absolutely nothing!  Others left parts of the prior data in the flash
chips, accessible to forensic probing.  I pointed out the 2010/2011
USENIX paper about this to the list on 19 June 2014 (see below).  Does
anyone know of any updated papers that test Secure Erase on more
modern SSDs, hard drives, and SDcards?  Or any improvements on USB
memory sticks to allow them to be securely erased?
Comments: In-reply-to Thierry Moreau  message dated "Thu, 19 Jun 2014 03:57:59 -0000."
X-Mailman-Approved-At: Thu, 19 Jun 2014 20:15:51 -0400
X-Mailman-Version: 2.1.15
Precedence: list
List-Id: The Cryptography and Cryptography Policy Mailing List
List-Unsubscribe: ,
List-Archive: List-Post: List-Help: List-Subscribe: ,

@_date: 2017-01-31 22:09:43
@_author: John Gilmore 
@_subject: [Cryptography] FOI of NSA's cryptanalysis of DES 
Then think again!
The United States FOIA does not discriminate on the basis of who is
requesting the information.  It works for non-US-citizens as well as
for citizens.
Also, see    for an easy way to file a FOIA request,
track it through the process, and scan in and publish the resulting
documents online.  Experienced FOIA requesters help nudge your request
along.  It's free, though you are welcome to donate if you like it!
There you can also see thousands of fulfilled requests (with
responses), tens of thousands of pending requests, and more than a
million released pages of material.

@_date: 2017-02-25 22:36:15
@_author: John Gilmore 
@_subject: [Cryptography] SHA1 collisions make Git vulnerable to attakcs 
I tried to fix this when git was young, when it would've been easy.
Linus rejected the suggestion and didn't seem to understand the
threat.  He wired assumptions about SHA1 deeply into git.  In the next
few years, nasty people will teach him the threat model, with ungentle
manipulations of his and many other peoples' source trees.
It's interesting watching git evolve.  I have one comment, which is
that the code and the contributors are throwing around the term "SHA1
hash" a lot.  They shouldn't.  SHA1 has been broken; it's possible to
generate two different blobs that hash to the same SHA1 hash.  (MD5
has totally failed; there's a one-machine one-day crack.  SHA1 is
still *hard* to crack.)  But as Jon Callas and Bruce Schneier said:
"Attacks always get better; they never get worse.  It's time to walk,
but not run, to the fire exits.  You don't see smoke, but the fire
alarms have gone off.  It's time for us all to migrate away from
SHA-1."  See the summary with bibliography at:
  Since we don't have a reliable long-term hash function today, you'll
have to change hash functions a few years out.  Some foresight now
will save much later pain in keeping big trees like the kernel secure.
Either that, or you'll want to re-examine git's security assumptions
now: what are the implications if multiple different blobs can be
intentionally generated that have the same hash?  My initial guess is
that changing hash functions will be easier than making git work in
the presence of unreliable hashing.
In the git sources, you'll need to install a better hash function when
one is invented.  For now, just make sure the code and the
repositories are modular -- they don't care what hash function is in
use.  Whether that means making a single git repository able to use
several hash functions, or merely making it possible to have one
repository that uses SHA1 and another that uses some future
WonderHash, is a system design decision for you and the git
contributors to make.  The simplest case -- copying a repository with
one hash function into a new repository using a different hash
function -- will change not only all the hashes, but also the contents
of objects that use hash values to point to other objects.  If any of
those objects are signed (e.g. by PGP keys) then those signatures will
not be valid in the new copy.
Adding support now for SHA256 as well as SHA1 would make it likely
that at least git has no wired-in dependencies on the *names* or
*lengths* of hashes, and let you explore the system level issues.  (I
wouldn't build in the assumption that each different hash function
produces a different length output, either, though these two happen
As to your SHA1 concerns:
Actually, even the theoretical breaking has not been proven for a pre-existing SHA1 hash (ie you need to control both the starting point for it), and more importantly, git really uses the SHA1 has a _hash_, not necessarily as a cryptographically secure one.
IOW, security doesn't actually depend on the hash being cryptographic, and
all git really wants is to avoid collisions, ie it wants it to hash the
contents well. That, sha1 definitely does, and even an md5sum would
suffice (but having 160 bits instead of "just" 128 obviously adds to the
space, so that's always a bonus).
Of course, the fact that sha1 is also very expensive to try to fool is a big bonus, since it means that it's just another layer on the real security model. But the _real_ security comes from the fact that git is distributed, which means that a developer should never actually use a public tree for his development.
For example, I've got two separate firewall layers (and a NAT) in between me and the internet, and my personal tree is on that machine. I never actually trust or use the external trees - I just push the result to them. This is something you cannot do with a centralized SCM server like SVN or
other traditional crud. A centralized one obviously has to be accessible
to all the developers, which means that it's forced to be open enough to
be much more easily attackable, and also means that there is a single point of failure also from a security standpoint. In contrast, even if somebody were to compromise my machine, that does _not_ automatically compromise the trees of other developers. They'd still have all the pristine objects, and never even fetch an object from me that has the same name (ie sha1 hash) as one they already have.
In other words, to really break a git archive, you need to
 - be able to replace an existing SHA1 hash'ed object with one that hashes
   to the same thing (_not_ the breakage that has been  shown to be    possible already)
 - the replacement has to still honor all the other git consistency checks    (even "blob" objects have them: they need to have a valid header with a
   valid length, so it's not sufficient to just find another object that    hashes to the right thing, you have to find an object with a valid    header that hashes to the right thing)
 - you have to break in to _all_ archives that already have that object    and replace it quietly enough that nobody notices.
Quite frankly, it's not worth worrying about. It's a hell of a lot easier
to just break a source archive with other means (ie pay a developer ten
million dollars to just insert the back door you want inserted).
Knew that -- but "Attacks never get harder, only easier."
I thought the Chinese team had reported four pairs of colliding plaintexts -- they just hadn't revealed exactly how they generated
them.  Or are you distinguishing "finding" from "generating" a collision?
Here's the mailing list for git:
  Somewhere in there it told me where to find the sources, which include
a design document about how it works.  Ah, there it is:
    Basically, it assumes, deeply embedded, that if two blobs have the
same hash, they ARE THE SAME BLOB.  You can destroy its integrity by
feeding it various blobs which happen to hash to the same values.  He
seems to think that the only possible attack is that someone would go
in and modify the database by hand -- rather than feeding it new input
that confuses it.
PS (added 25 Feb 2017):
If you assume NSA is six months or a year ahead of the open
academic/industrial sector in attacking SHA1, what would they have
already subverted using a similar attack?  Hmm, check the "cmp" and
"diff" sources!  If you don't trust the SHA1 hashes that say two trees
are the same, the second step is comparing the trees of files
directly.  Making an input pattern that causes cmp and diff to always
say, "yup, no differences here!" would allow any fraudulently inserted
modifications to spread much further.

@_date: 2017-01-31 16:27:10
@_author: John Gilmore 
@_subject: [Cryptography] FOI of NSA's cryptanalysis of DES 
This would be an interesting topic.
There is a Congressional document which is the unclassified version
of a congressional analysis of whether DES was weakened by NSA.  See:
  U.S. Senate Select Committee on Intelligence. "Unclassified Summary:
  Involvement of NSA in the Development of the Data Encryption
  Standard" (Staff Report), 98th Cong., 2d sess., April 1978
I found a copy of this years ago in a federal Depository Library (on
paper).  There is also a copy scanned into the Internet Archive:
  A (possibly different) version was also published in IEEE
Communications Magazine 16(6):53-55 in December 1978 (DOI:
In hindsight, the key sentence in this document is: "In the
development of DES, NSA convinced IBM that a reduced key size was
sufficient;".  If true, it would indicate that NSA's most pungent
corrupting influence was made to IBM, not to NBS.  Horst Feistel,
the designer at IBM, died in 1990; I don't know where his papers
have ended up.
The classified version has never been released.
Neither this document nor the classified version are covered by FOIA,
since they are Congressional documents.  However, I would not be at
all surprised if there were copies of the staff report (and inputs to
it) in various Executive Branch agencies (NBS, NSA), where they could
be FOIAd.  Sending them a FOIA request would trigger a mandatory
declassification review, which might lead to some releases, IF they
have and can find copies of the documents, which is a big IF for a
1978 document.
In fact, the unclassified summary almost HAD to be written by someone
in the Executive Branch who had a copy of the classified summary.
This is because classification decisions always occur in the Executive
Branch.  (Did you note that when the FIJA Court wanted to release some
of its opinions, it had to submit them to the DoJ for
The unclassified summary also says, "Over 200 pages of private and
public papers and documents were also analyzed".  I suggest trying
to FOIA those papers, as well as the staff report.
Also, if there was a proverbial "honest man" on the Senate
Intelligence Committee, you could write a letter to them, asking that
this document be unearthed and declassified.  My Senator there is
Dianne Feinstein, apologist for authoritarianism, who does not qualify.
(At the time of the report, Joe Biden and Gary Hart were on the committee.)
You could also check into what has happened to the former committee
staff (William G. Miller, Earl D. Eisenhower, and Audrey H. Hatry are
identified in the unclassified summary).  I know that some of the
staff of the Church Committee went on to write good books about what
they were doing and what they learned "back in the day" while
investigating NSA.
But you are asking for something slightly different -- a cryptanalysis
of DES, not an inquiry into its subversion.  You could try asking, but
you'll probably get a stonewall response, unless you can identify some
particular document (like the above) that they can't help but find if
they look in the right files.  Maybe try asking for the designs of
DES-cracking machines built by NSA.  We know they have done it, almost
certainly before we did it.
There is also a chronology of the development of DES in the Office
of Technology Assessment's "Defending Secrets, Sharing Data" booklet,
in Appendix C, which is reproduced here:
  That whole book appears to be digitized into a set of PDFs here:

@_date: 2017-06-08 13:37:26
@_author: John Gilmore 
@_subject: [Cryptography] Depending on Google to protect your anonymity?! 
Did you think that Google wasn't storing copies of all the documents
that you ask it to translate?  And tying them to the cookies and IP
addresses and other activities of that same Internet user?  Google
isn't Santa Claus, though they seem to have convinced the public they
are.  Whenever they offer a free service, it's for a reason, and the
reason is usually to track everything you do and then monetize that.
NSA has some incredibly damn intrusive laws that they've rammed or
blackmailed through Congress.  But even NSA could never get a law
passed that would require the level of detailed tracking that Google
has managed to create, merely by offering "free" stuff to webmasters
(fonts, captchas, searches, analytics, ...) and to users (browsers,
searches, shopping, translation, hosting, email, ...).
I can't wait til I find the secret way into the Google archives for
detective agencies.  How much would it cost to be watching every
interaction a given user has with Google, all day, all week long?  The
Feds can require this with a subpoena or warrant.  I want to be able
to buy it, too.  It's a serious approximation of David Brin's
"Transparent Society".  What websites is a merger-and-acquisition
lawyer visiting in this hour?  What draft contracts are my competitors
editing on Google Docs today?  What are all the emails to and from
interesting_person at gmail.com about?  What Youtube videos is the Queen
of England watching?  Oh, and what documents are low level State
Department employees translating this week via that "free" Google
Translate service?

@_date: 2017-06-09 16:39:39
@_author: John Gilmore 
@_subject: [Cryptography] Depending on Google to protect your anonymity?! 
Doesn't work for me.  But I don't have a Google account.
Google works hard to track me everywhere I go, whether or not I have a
Google login...  but because I don't, they won't show me what they're
logging about me.

@_date: 2017-06-21 12:20:02
@_author: John Gilmore 
@_subject: [Cryptography] doing traffic analysis for good - analysing TLS 
It works great until the adversary catches on; then it doesn't work at all.
Years ago, Tor traffic used to disguise itself as TLS browser
sessions.  It turned out that some national firewalls were able to
tell Tor traffic from https traffic due to things like this (the set
of crypto parameters they would negotiate, or which certificate chains
they sent).  Tor had to evolve to look MUCH more like browser traffic,
to continue making it through the firewalls.

@_date: 2017-03-01 13:17:23
@_author: John Gilmore 
@_subject: [Cryptography] Bizarre "latent entropy" kernel patch 
While poking around at recent kernel patches, I found this one:
  Merge tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
  Pull gcc plugins update from Kees Cook:
 "This adds a new gcc plugin named "latent_entropy". It is designed to
  extract as much possible uncertainty from a running system at boot
  time as possible, hoping to capitalize on any possible variation in
  CPU operation (due to runtime data differences, hardware differences,
  SMP ordering, thermal timing variation, cache behavior, etc).
  At the very least, this plugin is a much more comprehensive example
  for how to manipulate kernel code using the gcc plugin internals"
* tag 'gcc-plugins-v4.9-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
  latent_entropy: Mark functions with __latent_entropy
  gcc-plugins: Add latent_entropy plugin
The patch adds a bunch of "__attribute__((latent_entropy))" tags to
various kernel functions and data structures, then adds a plugin to
the GCC compiler itself.  That plugin uses those attributes to insert
extra, invisible code into each tagged function, which appears to use
XOR or ADD to add a different pseudo-random constant to a 64-bit
global variable, whenever a different execution path is taken in a
tagged function.  (I.e. each side of an "if" statement dumps in a
different "random-looking" constant.)
It's not designed to be cryptographically secure.  It's not designed
to be secure at all.  It almost looks like security-by-obscurity,
like calculating the resulting 64-bit number would be so cumbersome
that "most attackers won't bother".
I *think* the design goal is to make the address space layout
different, on every different piece of hardware that identical kernels
boot on, and probably different every time you boot an identical
kernel on the same piece of hardware (if it's feeding in the realtime
clock value, for example, which is not in this patch, but is perhaps
done elsewhere).
This non-cryptographically-secure non-random 'randomness' seems to be
collected solely so that it can be fed into the long-existing Address
Space Layout Randomization code, which makes it hard for heap-overflow
or stack-overflow attacks to know where anything is in the address
Of course, neither the code, nor its sketchy documentation, actually
says that anywhere!
If anybody actually knows the motivation for this bizarre
implementation, don't just let us know -- improve the documentation!
Which currently just says:
config GCC_PLUGIN_LATENT_ENTROPY
+      bool "Generate some entropy during boot and runtime"
+      depends on GCC_PLUGINS
+      help
+        By saying Y here the kernel will instrument some kernel code to
+	 extract some entropy from both original and artificially created
+	 program state.  This will help especially embedded systems where
+	 there is little 'natural' source of entropy normally.  The cost
+	 is some slowdown of the boot process (about 0.5%) and fork and
+	 irq processing.
+	 Note that entropy extracted this way is not cryptographically
+	 secure!
+	 This plugin was ported from grsecurity/PaX. More information at:
+	     * +	     * The "grsecurity.net" site is reasonable to examine.  The
"pax.grsecurity.net" site is totally unreadable to anyone who
doesn't already know what it is about (which includes me).
It desperately needs an introductory paragraph, or an "About" link
that works!  By trolling around in there I did find this general
description of what they think they're up to:

@_date: 2017-11-23 14:03:50
@_author: John Gilmore 
@_subject: [Cryptography] Intel Management Engine pwnd 
According to geeks who worked on the product, there are several such
switches in the CPU and chipset.  The missing bit is a DOCUMENTED off
or disconnect switch that actually works.  Why Intel refuses to do
this is a mystery to me -- and as Frank Zappa presciently suggested,
when you can't figure out why somebody would do something, the answer
is probably MONEY.  Which would indicate that Intel have probably been
paid off by somebody (NSA, the Chinese government, ???) to force a
covert backdoor on every user of their products.  This exact issue is
why I have refused to buy Intel gear for many years, and I can't be
the only one.
Note that Intel is *still* not offering an off-switch -- just a patch
for the current exploit.  Not a patch against every future exploit.
Meanwhile, for figuring out whether your equipment has this problem
and needs a new binary blob of slightly improved exploit-laden
firmware installed, Intel only offers a proprietary program that comes
with a pre-download license as long as your arm, banning reverse
engineering, sharing with others, and use with any system that wasn't
built by Intel:
  I'm sure there's a way to read the Javascript, decline to accept the
terms, and still figure out how to download the software, so you can
at least avoid the appearance of making a contract with Intel about
this, which would let you reverse-engineer it according to the laws of
your jurisdiction.  Of course, the bad guys are just going to click
"agree", download, reverse engineer anyway, and start making exploits.
So this extra layer of legal bullshit only serves to deter the honest
people whose security Intel has deliberately screwed for a decade with
their built-in backdoor.

@_date: 2017-11-25 13:15:19
@_author: John Gilmore 
@_subject: [Cryptography] Intel Management Engine pwnd 
Jerry Leichter  said:
Heat & power management does not require anything that can communicate
with the outside world (e.g. with an Ethernet chip).
I disagree.  People I've met who were on Intel CPU design teams tell
me that the CPUs do not require a running ME.  Apparently the
dependency, if there is one, is in the chip set (the support chips).
That's consistent with what was reverse-engineered by the people at
Positive Technologies who claim to have discovered the ME
vulnerability that Intel just patched:
    PS: By the way, modern Intel USB3 chips also provide a JTAG debug
interface that's accessible from any USB3 port.  This interface
provides detailed hardware-level control and debugging.  In theory
this is disabled in consumer devices, but it can be enabled in the
BIOS.  By exploiting an ME vulnerability, an attacker could
permanently switch this mode on for all the Intel devices on a
network, making such machines vulnerable to a later irresistable
physical attack merely by plugging in a malicious USB device.  This
hard to detect manipulation would, for example, enable malicious
re-entry into the network after malware has been cleaned out of it.

@_date: 2017-10-12 20:35:53
@_author: John Gilmore 
@_subject: [Cryptography] ? recommendations for secure communications 
No great tech skills => don't tell your tipsters to use a computer!
Tell them to use the postal service, putting their info into a plain
envelope with no (or not your) return address.  Use an ordinary,
common stamp, not a postage meter.  Don't lick the stamp; use
self-adhesive ones.  Make sure the envelope has enough postage for the
weight, if it's more than two or three pages.  Drop it in a mailbox
that doesn't have a camera pointing at it.  Wear surgical or other
gloves when touching the letter and envelope.
Don't print out or copy the info on a laser printer; use an inkjet
printer (modern fax machines often also work as inkjet-based
copiers).  Use ordinary paper from an office supplies store.
Rather than figure out how to get your inkjet printer to reliably
print the address on an envelope, you could use a 'window' envelope
that shows the destination address that's printed on the ordinary
paper inside it.

@_date: 2017-10-14 17:52:01
@_author: John Gilmore 
@_subject: [Cryptography] Suggestions for wearable wireless technology ? 
Put a USB port on it for the doctor's office.  You can use it
for charging the device, too.  I know, that's not wireless, so it
needs different FDA certification to make sure it won't shock the
patient.  (USB Type C is a small connector, reversible, and high
enough power for fast charging.)

@_date: 2017-10-16 18:25:18
@_author: John Gilmore 
@_subject: [Cryptography] Millions of high-security crypto keys crippled 
Today's revelation that Infineon "Trusted Platform Module" TPM chips
produce insecure RSA keys has a bright side.  Those chips were largely
created for Internet-based DRM schemes.  It means that it should be
possible to take a public key generated by such a TPM chip, factor it
to produce the matching private key, and then break the DRM by signing
or "attesting" to any damn thing the corporate mothership wants to
The mothership won't be able to disable all the TPM-created keys, since
that would force them to deny service to ALL their customers.
Hmm, I wonder if Infineon TPMs contain the broken library burned into
ROM or flash, or whether they get the library loaded from the system
they are securing.  Presumably if the library is loaded from the
outside, it's signed, and the signature is checked, so that you can't
just load ANY old library into it.  Was it signed with a TPM-generated
RSA key, perhaps?  Shall we find the matching private key?  :-)
Is this sort of scheme used to force registration of Microsoft OS's,
perhaps?  Or require that modern x86 tablets can only boot Microsoft
OS's?  Or secure Blu-Ray drives, maybe?  Howabout that
turncoat-Tim-Berners-Lee-approved DRM-for-websites that the quislings
at Mozilla have already secretly rolled out by having "free software"
Firefox quietly download, install and run a secret proprietary binary
plugin after you install it?
Let's have some fun.
PS:  I don't need to point out to THIS audience the irony in the
"Trusted" platform module that nobody can actually trust (or verify).
We've been complaining about that for a decade or more.
PPS: The essence of the test for broken keys is a list of primes and a
list of remainder masks.  "git clone
 and examine roca/roca/detect.py:
Here's the test:
        for i in range(0, len(self.primes)):
            if (1 << (modulus % self.primes[i])) & self.prints[i] == 0:
                return False
        self.primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101,
                       103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167]
        self.prints = [6, 30, 126, 1026, 5658, 107286, 199410, 8388606, 536870910, 2147483646, 67109890, 2199023255550,
                       8796093022206, 140737488355326, 5310023542746834, 576460752303423486, 1455791217086302986,
                       147573952589676412926, 20052041432995567486, 6041388139249378920330, 207530445072488465666,
                       9671406556917033397649406,
                       618970019642690137449562110,
                       79228162521181866724264247298,
                       2535301200456458802993406410750,
                       1760368345969468176824550810518,
                       50079290986288516948354744811034,
                       473022961816146413042658758988474,
                       10384593717069655257060992658440190,
                       144390480366845522447407333004847678774,
                       2722258935367507707706996859454145691646,
                       174224571863520493293247799005065324265470,
                       696898287454081973172991196020261297061886,
                       713623846352979940529142984724747568191373310,
                       1800793591454480341970779146165214289059119882,
                       126304807362733370595828809000324029340048915994,
                       11692013098647223345629478661730264157247460343806,
                       187072209578355573530071658587684226515959365500926]
So you divide the RSA public key by this set of small primes, and
examine the remainder.  If the bit in the matching "prints" number that
matches the remainder is on, the key is not vulnerable.  If any of the
remainders indexes a bit that's zero, the key is vulnerable.
All the "prints" numbers are even, i.e. their low order bit is zero:
if the remainder when dividing by any small prime is zero, then, ahem,
the number is easy to factor!
What does this tell us about how the keys were poorly generated, and
what does it tell us about how to factor them?  (We'll find out in
early November, but we might as well think about it.)

@_date: 2018-12-11 17:12:12
@_author: John Gilmore 
@_subject: [Cryptography] Decrypting the Encryption Debate 
And where can we download it for free if we don't register with the government?

@_date: 2018-01-04 16:07:44
@_author: John Gilmore 
@_subject: [Cryptography] Speculation considered harmful? 
Not true.
The attack succeeds by doing a speculative load, which faults, but
which actually picks up the addressed data anyway -- and then uses
that data for a future operation (which leaves some visible trace).
The obvious place to fix that is: When a load faults, don't pick up
the addressed data.  Every architecture that has memory-mapped I/O
ports already has to deal with this; the permissions check has to
happen BEFORE committing the transaction, else memory-mapped registers
that change upon being read, will be touchable by unprivileged code.
If the CPU knows the physical memory address of the page being
addressed, then it also knows the permission bits; they are in
exactly the same page-table entry.  And it can't do the memory access
without knowing the physical memory address.
There's a second place you could fix this, if you can't do the
obvious.  Speculatively loaded data values also carry alongside them,
indications about faults (so that if the execution path commits,
rather than being nullified, the CPU will take the fault).  If a
speculative execution tries to use (as an input) a data value WHICH
COMES WITH A FAULT, then stop speculating.  This would solve the data
leak that occurs when the data value is used to calculate an address
of a subsequent access (thus altering the cache in a way that can be
Indications are that AMD got this right in their implementations, and
Intel got it wrong.  ARM seems to be more of a mixed bag.
PS: I did architectural validation for some early SPARC processors,
and we found and fixed all sorts of stuff like this -- in simulation,
before we shipped the chips.
PPS: The branch target buffer exploit is different; not sure about
that one yet.

@_date: 2018-01-05 15:19:38
@_author: John Gilmore 
@_subject: [Cryptography] Speculation considered harmful? 
The TLB cache doesn't leak any information.  The address you are
accessing is not secret; you supplied it.  What is secret (and
protected by the permission bits) is the data that resides at that

@_date: 2018-01-07 17:41:05
@_author: John Gilmore 
@_subject: [Cryptography] Announcing XSTREAM v0.1: misuse-resistant 
Am I confused?  Doing Diffie-Hellman requires talking with another
party.  For data-at-rest, who is the other party?  And how does your
library communicate with that other party?

@_date: 2018-01-14 23:16:33
@_author: John Gilmore 
@_subject: [Cryptography] Speculation considered harmful? 
Why are we discussing random failed computer architectures like VLIW on
a cryptography mailing list?
Fixing the Meltdown & Spectre attacks doesn't require tearing down all
of computing and starting over -- no matter how much the cranks and
losers of the architecture wars would like it to.
It just requires fixing a few implementation bugs (like always stop
speculating when you get a memory fault).  AMD and ARM already got it
much closer to correct than Intel did, and presumably all of them will
improve on this in the next generation of chips, in the same way that
they have installed hardware to eliminate Rowhammer.

@_date: 2018-01-23 17:45:29
@_author: John Gilmore 
@_subject: [Cryptography]  Paul Kocher et al explain Spectre very well. 
I really didn't understand the Spectre attacks until I read the
paper co-authored by Paul Kocher here:
  I recommend reading it if you want to think about how to respond.

@_date: 2018-05-08 23:21:20
@_author: John Gilmore 
@_subject: [Cryptography] secure authentication ... as opposed to passwords 
To what extent does Kerberos solve this problem?
As I recall from using it for various things over the last 30 years,
its UI is stupid; like PGP, it assumes that you want one identity to
authenticate you to everyplace you go, instead of making it easy to
use separate identities with separate vendors; it still hasn't learned
about the Domain Name System and instead has its own strange idea of
not-quite-domain "REALMS"; etc.  But does it do the basic thing --
authentication without sending plaintext passwords -- that you're
asking for?  It's already an Internet standard, and even widely used
on Microsoft platforms.  Could it be de-crufted for use in web
Also, the last (Ubuntu) kerberos I tried to use seemed to still be
using a SHA1 HMAC, even though it is also using aes256
("aes256-cts-hmac-sha1-96")?  And it seemed to have some single-DES
keys lying around "for compatability with K4", along with an RC4-HMAC
and a 3DES key?  At first glance, that looks perfect for downgrade

@_date: 2018-05-17 20:57:12
@_author: John Gilmore 
@_subject: [Cryptography] Police want encrypted radios 
I'm confused.  The top cops in the whole country say that every system
doing end to end encryption needs a back door so that someone with
"lawful access" can get access to it.  Do the cops' radios have
backdoors built in to them?
Suppose, for example, the FBI suspects the local cops are corrupt.
The FBI should be able to get a secret warrant from a federal court
that would tap the local cops' encrypted radios without *any* of the
local cops being able to know.  So where's the back door that can
crack those radios?  How could they be buying encrypted radios without
back doors, when it's against everything the top cops recommend for
the whole society?
Let me guess -- it's a case of "one rule for you and a different rule
for me".  Quite typical of cops.

@_date: 2019-12-25 23:23:49
@_author: John Gilmore 
@_subject: [Cryptography] OpenSSL: rsa_builtin_keygen: key size too small 
Trying it with every possible IP address sounds like a fun way to get
a paper and a CVE.  We all know how well most software handles "things
that aren't supposed to happen".  Even if it doesn't catch modern openssl's,
there are lots of other implementations...

@_date: 2019-02-19 11:11:17
@_author: John Gilmore 
@_subject: [Cryptography] Looking for reviewers with cryptographic 
Be warned that if you volunteer for this position, your work will
almost certainly be seized by IEEE for their profit, and locked behind
a restrictive copyright and a paywall so that the public can't read it.
IEEE and its "digital library", where papers go to die, has a long history
of exactly this strategy.  I looked at the LOCS website (above) and
didn't find anything to convince me that *this* IEEE journal won't
follow their default repressive rules.
Pick a real open-access journal to put your efforts into -- they will
benefit the public and the whole academic community.  Rather than
"volunteer" your work for free for a greedy publisher who is sucking
scarce funds out of academic libraries' budgets, for their own benefit.

@_date: 2019-01-10 16:51:20
@_author: John Gilmore 
@_subject: [Cryptography] Digital dyes for tracing digital leaks ? 
UCSD researchers used a technique like this to discover that the
"Security Erase" command to SSD flash drives typically fails to securely
  "Reliably Erasing Data from Flash-Based Solid State Drives"     "3.1 Validation methodology
  Our method for verifying digital sanitization operations uses the
  lowest-level digital interface to the data in an SSD: the pins of the
  individual flash chips.
  To verify a sanitization operation, we write an identifiable data
  pattern called a fingerprint (Figure 3) to the SSD and then apply the
  sanitization technique under test.  The fingerprint makes it easy to
  identify remnant digital data on the flash chips. It includes a
  sequence number that is unique across all fingerprints, byte
  patterns to help in identifying and reassembling fingerprints, and a
  checksum. It also includes an identifier that we use to identify
  different sets of fingerprints. For instance, all the fingerprints
  written as part of one overwrite pass or to a particular file will
  have the same identifier. Each fingerprint is 88 bytes long and
  repeats fives times in a 512-byte ATA sector.
  Once we have applied the fingerprint and sanitized the drive, we
  dismantle it. We use the flash testing system in Figure 2 to extract
  raw data from its flash chips. The testing system uses an FPGA running
  a Linux software stack to provide direct access to the flash chips.
  Finally, we assemble the fingerprints and analyze them to determine if
  the sanitization was successful."

@_date: 2019-03-10 15:10:47
@_author: John Gilmore 
@_subject: [Cryptography] **Copyright seizure approaching** SpaCCS 2019 
Privacy and Anonymity in Computation, Communication and Storage
Don't submit your paper to this conference!  When researchers refuse to
supply their papers to the publishers who extort monopoly fees from
academic librarians, they have found the easiest way to tear down these
If you submit any paper to this conference, you will be forced
to assign your entire copyright in the paper to "Springer Nature
Switzerland AG", now and forever, for their profit and your loss
and the public's loss (see below).
Jun Feng is a program chair.  He should know to warn authors that
the whole conference is a scam on academic authors, which steals their
copyrights in order to extract large fees from academic libraries.  But
perhaps he did not mention this because he's helping to run the scam.
I recommend publishing your work in Open Access conferences and journals
in which (1) you are free to retain your copyright and control your
rights, and (2) the public is free to read your paper without paying
exhorbitant fees to a walled-garden publisher that prevents public
access to your scholarship.  See
 .  For example, the USENIX
Association runs many respected conferences and does Open Access
publication of their proceedings (  The Public
Library of Science journals are also Open Access
(  Open access publishing increases your impact,
because all potential readers can actually read your paper.  Many
funders and academic institutions *require* that your work be published
with open access, because they have seen how the academic publishing
monopoly has damaged academic libraries (and science in general).
For general info about the highly profitable scams around academic
publishing, see:
  That page links to the deliberately misnamed "LNCS Consent to Publish
  which says:
  "Author hereby grants and assigns to Springer Nature Switzerland AG,
  Gewerbestrasse 11, 6330 Cham, Switzerland (hereinafter called
  Publisher) the exclusive, sole, permanent, world-wide, transferable,
  sub-licensable and unlimited right to reproduce, publish, distribute,
  transmit, make available or otherwise communicate to the public,
  translate, publicly perform, archive, store, lease or lend and sell
  the Contribution"... (and dozens more clauses grabbing every possible
  way for people to read your work)
  "The copyright in the Contribution shall be vested in the name of
  Publisher."
Stealing your copyright is significantly more than merely giving
"consent to publish" your paper.  So if you're the type who signed
whatever legalese they claim is "required", without reading it, you have
just had your work legally stolen from you, and you didn't even know it.

@_date: 2019-05-19 04:06:10
@_author: John Gilmore 
@_subject: [Cryptography] Network Time Protocol security 
There is an effort underway to design and standardize improved methods
of securing the NTP time-synchronization protocol.  Here's an overview
of the effort, plus pointers to a published RFC that documents the
requirements that they are trying to satisfy, and to the current
      The draft protocol is being implemented now by two or more NTP
implementations to begin interoperation testing.
There is a long history of half-assed or broken crypto applied to various
iterations of NTP (pre-shared keys, Autokey, etc).  None has yet had that
essential combination of ease of deployment and lack of vulnerability.
Before this gets standardized and deployed, has anybody on this list
analyzed the threat model and the draft mechanisms to see if they would
actually accomplish the goal of cryptographically securing the
worldwide accurate time distribution overlay network?

@_date: 2019-09-05 02:47:07
@_author: John Gilmore 
@_subject: [Cryptography] Lockdown copyright survey paper on proof 
No, it's marketed as "free to download" but that means free as in beer,
not in freedom.  You can't log in unless you create an account, and you
can't create an account unless you accept their "terms and conditions"
and provide an email address and a password.  I haven't seen what else
happens if you actually do give them an email and a (useless except for
tracking you) password, but it involves at least spamming you, according
to the terms.
Perry, why are you sending us to a proprietary captive portal designed
to track us and market to us?  Haven't the authors published this paper
on their own website or in a "preprint" archive that's publicly
accessible?  The site claims that authors retain their copyrights
and are free to publish articles themselves ("self-archiving by
author permitted"):
  If the authors actually do retain their copyrights then they should
be free to additionally release the paper under a CC license.
Perhaps we need to update "Friends don't send friends to Google" to
include proprietary journal publishers too.  I was surprised and pleased
to recently notice that the entire University of California (UC) system
has stopped subscribing to all 2500 proprietary Elsevier journals, after
extensive negotiations, because Elsevier wouldn't budge on allowing free
public access to all papers written by researchers from all UC campuses?
    Let's follow their lead in refusing to support academic publishers'
business models that lock down useful information forever, even when the
research was paid for by taxpayers in the first place.

@_date: 2020-02-20 01:46:31
@_author: John Gilmore 
@_subject: [Cryptography] UK "HCSEC" UK-cleared engineers try to prove Huawei 
A recent inflammatory Washington Examiner article pointed me at this
  which details the UK "National Cyber Security Center" (formerly known as
GCHQ, i.e. the UK's NSA) successful effort to build a 50-person team of
classified spooks with full access to Huawei source code.  They are
attempting to test the assertion that all the Huawei products sold in
the UK are secure.
As expectable, they are finding plenty of typical industry-wide security
They fault Huawei's software development process for failing to provide
reproducible binaries.  The oversight board claims "the end-to-end
assurance that a particular source code set is precisely that used to
build a particular binary would normally be satisfied as a side effect
of a modern software engineering process."  That's a good theory, though
it's unfair to apply it only to Huawei, since it appears to me that it
is so far seldom achieved in practice in real operating system or
firmware development.  See for example the Reproducible Builds project
which is attempting to approach this for major free software operating
systems:  and has not fully succeeded
with any of them.  (Cygnus Support achieved sustained reproducible
builds and cross-builds of its GNU software development tool releases in
the mid-1990s, but this effort covers a much more extensive set of
The report also says that their team has found hundreds of security issues
in the Huawei products (such as LTE routers), including:
  unprotected stack overflows in publicly accessible protocols, protocol
  robustness errors leading to denial of service, logic errors,
  cryptographic weaknesses, default credentials and many other basic
  vulnerability types.
Since the team is operating under UK classification rules, the details
of these vulnerabilities probably haven't been reported to anybody but
the UK spooks (for exploitation, of course) and their NSA friends.  It's
not clear whether Huawei has been told, or whether any end-users of
Huawei equipment have been told, what these vulnerabilities are.
The report also concludes that:
  These findings are about basic engineering competence and cyber
  security hygiene that give rise to vulnerabilities that are capable of
  being exploited by a range of actors. NCSC does not believe that the
  defects identified are a result of Chinese state interference.
There's a further issue that all of Huawei's products are based on:
  various old and soon-to-be out of mainstream support versions of a
  widely used [unidentified] third-party real time operating system,
  which Huawei has chosen to continue to use within products whose end
  of life date is significantly longer.  ...  Furthermore, the operating
  system in question is based on a single memory space, single user
  model (as was prevalent at its time of design), which further
  increases risk as a single vulnerability in any process running under
  this operating system is sufficient to allow compromise of any
  component running in the same operating system instance.
  ...  Huawei?s intent [is] to move off the operating system that is
  soon-to-be out of mainline support to their own real time operating
  system, based on the open source Linux kernel.
In checking a few sub-components in particular, they ran into other issues
with "component and lifecycle management":
  NCSC then selected a commonly used component, the OpenSSL library, and
  specific queries were performed on the Huawei development
  database. This showed that there were an unmanageable number of
  versions of OpenSSL permitted to be used in products, including
  versions that are not on the main development train, that have known
  vulnerabilities and that are unsupported.   ...
  In the first version of the software, there were 70 full copies of 4
  different OpenSSL versions, ranging from 0.9.8 to 1.0.2k (including
  one from a vendor SDK) with partial copies of 14 versions, ranging
  from 0.9.7d to 1.0.2k, those partial copies numbering 304. Fragments
  of 10 versions, ranging from 0.9.6 to 1.0.2k, were also found across
  the codebase, with these normally being small sets of files that had
  been copied to import some particular functionality. There were also a
  large number of files, again spread across the codebase, that had
  started life in the OpenSSL library and had been modified by Huawei.
  In the later version, there were only 6 copies of 2 different OpenSSL
  versions, with 5 being 1.0.2k and one fork from a vendor SDK. There
  remained 17 partial copies of 3 versions, ranging from 0.9.7d to
  1.0.2k. The fragments from the 10 different versions of OpenSSL
  remained across the codebase as do the OpenSSL derived files that have
  been modified by Huawei. More worryingly, the later version appears to
  contain code that is vulnerable to 10 publicly disclosed OpenSSL
  vulnerabilities, some dating back to 2006.
It's an interesting document, though you have to wade through some 46
pages to find the fun bits.  Overall, the effort looks like a win/win,
in that the UK gets a much more detailed, bigger clue about the actual
security of Huawei gear (both in the UK and worldwide), and Huawei gets
a world-class team beating on every aspect of their software development
and configuration control process and their code, all the way down to
looking for (and finding) sprintf's that should be snprintf's.

@_date: 2020-01-02 13:12:12
@_author: John Gilmore 
@_subject: [Cryptography] how to detect breakage -- lures etc.?? 
It's well worth reading the books about the breaking of Enigma, like The
Hut Six Story by Gordon Welchman.  The key lesson is that breaking
Enigma on an ongoing basis required breaking the very early versions,
which were the least secure, and understanding how the Germans used them.
The German military produced many variants of Enigma throughout the war,
each one stronger than the previous ones.  After a period of being able
to read German traffic, everything would "go dark" when a new version of
Enigma was deployed.  Then the teams would feverishly work on figuring
out what had been changed, for months, until occasional messages could
be read, and eventually almost all messages.
The small Polish team that first broke Enigma, broke the version with
three rotors, and was able to read messages "almost every day" for most
of a year, until December 15, 1938, when the Germans introduced two more
rotors with different wiring, any three of which could be inserted into
the machine in any order.  The Poles were able to figure out the wiring
of the two new rotors, but unable to put forth the effort to break the
whole system, and their country was about to be attacked and overrun.
So they handed over their work product to the UK and France, including
working Enigma machines.  The UK was able to build systems and
procedures that could break the 5-rotor-in-3-slots Enigma, and begin
reading German messages.
Throughout the war, different, stronger variants of Enigma were used by
different groups (e.g. the German Navy, the top generals, diplomats,
etc).  For example, a fourth rotor slot was added to some; and a
plugboard was added to provide additional scrambling.  Each variant had
to be figured out.  Knowing how the basic machine worked, and what the
habits of the machine operators were, was essential in reducing the
combinatorial space to something barely solvable.  And even solving
these required inventing and evolving brand-new code breaking machines
unlike anything ever built before.
The high order bit is: Break the easy, early systems; develop expertise
in the communication habits of the enemy; and that gives you "cribs"
(suspected plaintexts or somewhat visible correlations) that enable you
to break the harder, later systems, which would otherwise be way beyond
your capabilities.
I suspect that this advice is still good.  Which is why inventing and
deploying cipher systems without also having a wiretapping, codebreaking
operation close at hand, is such a chancy business.

@_date: 2020-06-03 21:22:28
@_author: John Gilmore 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
That sounds like the club that was so crowded that nobody goes there anymore.
I *have* been advised by someone who ought to know better, that merely
moving off the most popular two or three platforms vastly reduces the
likelihood of penetration.  Like, don't use a Microsoft or Apple OS.
But: you have to think about who might want to attack and what their
goals are.  If breaking into servers is what matters to one of your top
adversaries, then Linux is likely the first platform attacked; if
smartphones, Android; etc.

@_date: 2020-06-22 23:58:14
@_author: John Gilmore 
@_subject: [Cryptography] Side channel nomenclature 
NSA's job is to do the second while convincing you that it's the first.
They approached Sun in the 1990s to subvert their network encryption
system, suggesting that rather than using a product of two large primes,
they should use a product of three primes (one small enough to factor
out).  I don't know what Sun would have done in other circumstances, but
they turned down that offer on the grounds that once the back door was
later discovered, it would be obvious from external examination that it
was "exfiltration" and not mere "poor design".  In other words, it
lacked deniability.
Since the only difference between first and second is the motivation
of the designer, which is unknown and unknowable, is there really a
worthwhile distinction?

@_date: 2020-05-04 03:06:39
@_author: John Gilmore 
@_subject: [Cryptography] NSA security guidelines for videoconferencing 
BW> Unless the algorithm is rot0 or the user is a savant, some software
BW> is being trusted. And I doubt that even a savant could handle video
BW> encryption at frame rate.
There's a pretty good reverse-engineering of the Zoom Web client here,
by some people who specialize in doing streaming-video-over-internet
(webrtc) in browsers:
  The same site has other articles analyzing various other video
conferencing methods.  Here's one:
  "Does your video call have End-to-End Encryption?  Probably not..."
  Here is a quick demo from three weeks ago of how they used a new
Insertable Streams javascript API in a beta Chrome version to prototype
true end-to-end encryption for the free software Jitsi web application.
(Spoiler: they superencrypt the actual video stream, since the raw
stream is getting sent over TLS, and TLS is negotiating keys with an
endpoint at a media relay service, not at the other user.  They are
still working out all the details of key agreement -- anybody want to

@_date: 2020-05-25 19:00:25
@_author: John Gilmore 
@_subject: [Cryptography] Zoom publishes draft cryptographic design for 
Blog post:
  Draft design:
    E2E Encryption for Zoom Meetings
  Josh Blum, Simon Booth, Oded Gal, Maxwell Krohn, Karan Lyons, Antonio
  Marcedone, Mike Maxim, Merry Ember Mou, Jack O?Connor, Miles Steele,
  Matthew Green(*), Lea Kissner, and Alex Stamos(**)
  Zoom Video Communications
  * Johns Hopkins University
  ** Stanford University
  May 22, 2020
  Version 1
  Introduction
  Hundreds of millions of participants join Zoom Meetings each day. They
  use Zoom to learn among classmates scattered by recent events, to
  connect with friends and family, to collab- orate with colleagues and,
  in some cases, to discuss critical matters of state. Zoom users
  deserve excellent security guarantees, and Zoom is working to provide
  these protections in a transparent and peer-reviewed process. This
  document, mindful of practical constraints, proposes major security
  and privacy upgrades for Zoom.
  We are at the beginning of a process of consultation with multiple
  stakeholders, including clients, cryptography experts, and civil
  society. As we receive feedback, we will update this document to
  reflect changes in roadmap and cryptographic design.
       ...
  This proposal lays out a long-term roadmap for E2E security in Zoom in
  four phases.  The first phase is an upgrade of the meeting key
  exchange protocol to use public-key cryptography, hiding all secret
  keys from the server. The next three phases harden the notion of a
  user?s identity?even across multiple devices?to help maintain server
  honesty in the key exchange and to give hosts better information when
  allowing or disallowing participation in a meeting. We provide the
  most detail for the earliest stages. We surmise that the specifics of
  later phases will change with more implementation and deployment
  experience.
       ...
  In an ideal world, all public key cryptographic operations could
  happen via Diffie-Hellman over Curve25519 [3], and EdDSA over Ed25519
  [4]. Relative to others, this curve and algorithm family have shown
  a consistent track record for resilience to common cryptographic
  attacks and implementation mistakes. These algorithms are currently in
  review for FIPS certification but, unfortunately, are not currently
  approved. Therefore, in some cases (like government uses that require
  FIPS certification) we must fall back to FIPS-approved algorithms,
  like ECDSA and ECDH over curve P-384. The protocol below has support
  for both algorithm families, and for now ?doubles up? all public key
  operations. This technique eliminates error-prone branching. Once
  certification succeeds, we can safely ?no-op? the operations over
  P-384.
PS: I think I see a denial-of-service attack in section 3.7.3,
"Participant Join (Leader)".  If a second participant Mallory can post
an *invalid* signed binding claiming to be for a first participant Bob,
to the meeting's crypto bulletin board, then the leader's algorithm
aborts ("If verification fails, it aborts"), it doesn't post the
encrypted meeting key for Bob, and Bob is locked out of the meeting.
Fixing this may mean more tightly specifying how the "bulletin board"
works, as opposed to letting any participant post any message to it
at any time.
