
@_date: 2002-04-15 17:04:53
@_author: Anton Stiglic 
@_subject: Error in Applied Cryptography 2nd Ed 
I've seen definitions of "primitive" in this sense.   There is
"primitive element" and "primitive polynomial", which are two different things.  See for example:
In definition 2.214, the HAC talks about a primitive *element*, in section 4.5 it talks about an irreducible *polynomial*.

@_date: 2002-08-08 09:55:58
@_author: Anton Stiglic 
@_subject: deterministic primality test 
Sent: Wednesday, August 07, 2002 11:05 PM
I don`t think that would work either.  If n is any odd integer > 3 (not of
the form
a^b, b>1), then you will set r = 2 in step 2, go throw step 3 (2 is smaller
than n)
and step  4 (gcd(n,2) will equal 1 so you don't exit yet), and  step 5 (2 is
and bump into the same problem in step 6 (2-1 = 1 doesn't have a prime
I guess you can fix this by rather having step 5. changed to
5'.  if (r is an odd prime)
Does that make sense?
Note that it seems that Lenstra and Pomerance believe the result is correct.
See for example
(I found that link from a post by Mads Rasmussen on sci.crypt).
It is also worth noting as well the names that appear in the
section of the paper, if these people approved of the result (their names
being there
doesn't necessarily apply that fact however), it gives some good weight to
the result.

@_date: 2002-08-08 14:28:06
@_author: Anton Stiglic 
@_subject: deterministic primality test 
Sent: Thursday, August 08, 2002 1:56 PM
That's true.  They also mention that the r that will satisfy the criteria
(the criteria being that r-1 has a prime factor q >= 4*sqrt(r)*log(n)
and q|o_r(n)) will be in the interval [c1*(log n)^6, c2*(log n)^6],
for positive integers c1 and c2.
In the proof they say
"Choose constants c1 >= 4^6 and..."
So I guess that in step 2 you could actually start with
r = 4^6
and the algorithm would work (and you wouldn't have the problem
with r=2,  2-1 not having a prime factor).
This is common not only among many number theorists but cryptography
theorists and other theoreticians as well.  They can be convinced that an
algorithm can work just by analyzing pseudo-code, they often don't
implement anything at all.  Or some will simply use special languages that
allow you to implement math algorithms at a high level (such as MAGMA),
but the code produced is not something you would use in production software.
Of course they are some theoreticians that are very good at implementing
algorithms (I'm thinking of prof. D.J Bernstein for example), but many don't
implement anything.

@_date: 2002-12-17 10:34:18
@_author: Anton Stiglic 
@_subject: Micropayments, redux 
Here is an email I sent to a private list right after I heard about
the Peppercoin startup.  I summarized PayWord, the lottery
scheme, and the new schemes which form the basis of what
Peppercoin wants to develop (Peppercoin is *not* developing
the older lottery scheme).  Might be useful to someone!
As I suspected the micropayment scheme they want to develop is
based on their recent paper, for which I attended a presentation by
Ron Rivest at RSA conference last February.
The idea is simple, it's based on a scheme Rivest published in a
paper in '97 called "Electronic lottery tickets as micropayments".
Here is a small description for those who are interested:
U: user who wants to purchase something
M: merchant
B: bank
In the descriptions let's assume that each micropayment is worth 0.01$
(the schemes can be generalized to accept different amounts).
*PayWord:  recall that PayWord was one of the first micropayment
schemes Rivest and Shamir came up with.  In payword, a user U
chooses an initial value x_{n-1} and computes a chain of hashes
all the way down to x_0
            x_i = Hash(x_{i+1})
U then signs x_0.   U gives M x_0 and the signature on x_0.  When U
wants to make a payment to M, U provides the next value in the hash
chain, x_i (he initially gives x_1, then x_2, etc...).
Each x_i is worth some predetermined amount of money,
0.01$ in our example.
At the end of the day M will deposit the payments to the bank B:
M simply provides x_0 and U's signature on x_0, along with the x_i
with largest index i, the bank can verify by simply hashing x_i
all the way down to x_0 and verifying U's signature on x_0.
This provides a way for the merchant to aggregate payments of a
specific user.
*Lottery Scheme.
This is a probabilistic scheme.  A payment can have a value 1/p
with some probability p.  For instance a payment can value 10$
with probability 1/1000 (we continue to assume that each
merchandise is sold at 0.01$)
This time the Merchant computes a hash chain w_0, ..., w_{n-1} in
the same way that a user computes a hash chain in PayWord.
When U wants to give a payment to M, M provides the root value
w_0 to the user and the user computes his own hash chain with
w_0 integrated into his commitment x_0 (the user signs x_0
*and* w_0 and some other information in the same sig).
For the i th payment, U gives x_i to M,  the payment is worth
10$ if x_i == w_i mod 1000, and worth nothing otherwise.  As you
can see there is a probability of 1/1000 that the payment is
worth 10$, 999/1000 that it is worth 0$.
This allows the merchant to
 *aggregate payments from different users*
something you couldn't do with PayWord.
One problem with this scheme is that the payment protocol is interactive,
the other problem is that if U is unlucky he might
(with prob 1/1000) need to pay 10$ on his first payment of a merchandise
worth 0.01$.  If he is very unlucky he might pay another 10$ on his
second payment, this might be enough to discourage the user from
using the system anymore.
*The new schemes
Rivest and Micali propose 3 new schemes that circumvent the problems
of the above schemes.
Note that they assume that computing digital signatures is not a problem
anymore due to advancement of hardware performance, a user can compute
a digital signature on each transaction.  You still want the property
of aggregating payments however, so that the bank doesn't have to many
to verify.
1st scheme (MR1):
We still have a probability p that a payment be worth something or not.
There is a public function F which takes an arbitrary bit string and
a value between 0 and 1 in a uniform way (F can make use of a hash
Let T be the encoding of a transaction (contains info like U, M and B's id,
merchandise description, value of payment, etc..).  Note that here U or M
defines what the payment is worth in the transaction, but for simplicity we
will assume that it's always 0.01$ and that p is fixed to 1/1000 as
U pays M for a T by providing a signature C = sig_U(T)  (U signs T).
Call C a check.
M, upon reception of C compute his own signature on C and verifies if
     F(sig_M(C)) < p,
if so then the check is deemed payable, if not it is not worth anything.
U cannot figure out beforehand if a check will be payable or not because
for what U is concerned with, sig_M(C) is like a random variable chosen
when M applies his signature.
Advantage of this scheme:  the payment is non-interactive.
MR2 scheme:
There is still the "psychological" (as Rivest and Micali state it) problem
that a user faces where he might need to pay 10$ right off for his first
payment.  Users like to be charged the amount corresponding exactly to
what they bought.
This solution solves the problem.
As in MR1,  U pays M by computing a signature C = sig_U(T) on T, but this
time U includes date/time and a serial number SN in T.  SN should start
at 1 and be incremented by 1 for each payment.
As in MR1, check C is payable if F(sig_M(C)) < p, but the deposit differs
on the side of the bank:
  let maxSN_U denote the maximum serial number of a payable check of U
  processed by B so far (initially maxSN_U = 0).  Assume that C is a new
  payable check.  Then B credits M's account with 1/p cents and if SN
  is greater than maxSN_U, B will debit U's account by SN - maxSN cents,
  and set MaxSN_U to be SN.  An exception might occur if B notices
  that the new check has the same serial number as a previously processed
  check, or if the new check's serial number and time are out of order,
  or for some other reasons.  The bank may keep statistics and throw out
  of the system (by revoking certificates) users whose payable checks cause
  exceptions.  B may also throw out merchants (who might have a to high
  rate of payable checks: merchants might collaborate with users to get
  more payable checks, users have nothing to loose).  This is the part that
  I see as hard to implement, I discussed this briefly with Ron Rivest after
  his presentation but he seemed convinced that it was not a problem, we
  will see!
So in this scheme a user is never charged more than what he actually
spends, the risk of excessive payments (which Rivest and Micali
qualify as small) is shifted from the user to the bank (R&M argue that
Banks are more accustomed to managing substantial risk so they
will not mind the rare risk of a small excessive payment).
MR3 is based on MR2 but renders the deposit of M to B more efficient.
The paper of course has more details on the schemes, and allot of proposed
little variants.
I'm anxious to see how the company will develop the technology!

@_date: 2002-12-17 10:46:26
@_author: Anton Stiglic 
@_subject: Micropayments, redux 
There is no risk for the user in the new schemes (which is what Peppercoin
is developing)!
Read the paper and/or presentation Zulfikar referenced in his post.
The risk is for the Bank.

@_date: 2002-09-04 11:55:39
@_author: Anton Stiglic 
@_subject: Cryptographic privacy protection in TCPA 
Jan Camenisch works for IBM, it's no surprise that the scheme is being
The scheme is not very efficient compared to Brands', but I would guess
if you don't mind doing allot of computation.
It is based on zero-knowledge proofs.  The basic idea of using
zero-knowledge proofs
to create an unlikable anonymous credentials system is actually pretty
intuitive and simple, and
people have taught about it before Camenisch & Lysyanskay have.  You can
probably think
about it yourself and come up with a similar scheme (not necessarily
provably secure however)
The novelty in my point of view is simply the choice of the setting in which
they work in (group of
quadratic residues modulo a composite), so that their scheme can apparently
be proven secure
under the strong RSA assumptions and the decisional DH assumption.
Camenischs work on group signatures and "Proving in zero-knowledge that a
number n is
the product of two safe primes" seem to have lead to the result.

@_date: 2003-04-08 16:24:27
@_author: Anton Stiglic 
@_subject: Via puts RNGs on new processors 
In fact to be a bit more precise, for FIPS 140-2  level 3 the module
needs to provide a call for the statistical tests, and it may automatically
start the tests on power up. For FIPS 140-2 level 4, the module must
execute the statistical tests on power up.
----- Original Message -----
Sent: Tuesday, April 08, 2003 4:02 PM
majordomo at wasabisystems.com

@_date: 2003-04-28 13:25:27
@_author: Anton Stiglic 
@_subject: two number theory questions 
algorithms to compute z=g^{b^{-1}} mod p?
If you can tell whether of not g^x mod p (for unknown x) is a quadratic
residue or not, then there is a way to
efficiently do what you ask (think about binary extended Euclidean
efficient algorithms to compute z=g^b mod p? How about computing y from z (b
is still unknown)?
About computing y from z:
You can prove that if for any x, given only g^x (mod p), you can compute
then the Diffie-Hellman problem would be easy mod p.
May I ask why are you asking these assignment-like questions?

@_date: 2003-08-22 11:42:45
@_author: Anton Stiglic 
@_subject: PRNG design document? 
That's a good reference on PRNGs.  There is also the work on Yarrow, The best implementation of Yarrow that I know of is
the implementation available on Couternpane's site is of an old version of
Yarrow, and had some security bugs (don't know if they were fixed...).
Also interesting is David Wagner's collection of links to stuff about

@_date: 2003-08-27 09:46:29
@_author: Anton Stiglic 
@_subject: PRNG design document? 
Sent: Friday, August 22, 2003 1:00 PM
As you mentioned, the FIPS-140-2 approved PRNG are deterministic, they take a random seed and extend it
to more random bytes.  But FIPS-140-2 has no provision for generating the seed in the first place, this is where something like Yarrow or the cryptlib
RNG come in handy.
So if you want FIPS-140-2 compliance, generate a seed using something based on Yarrow or cryptlib RNG
(or if you have a good hardware RNG use that to generate the seed), and then apply a FIPS approved
PRNG to the seed.
NIST should really approve something like Yarrow or Peter Gutmann's design...

@_date: 2003-12-01 11:20:10
@_author: Anton Stiglic 
@_subject: Problems with GPG El Gamal signing keys? 
Sent: Thursday, November 27, 2003 11:23 AM
Maybe we can learn that code re-use is tricky in cryptography:  indeed, if
the signing function and encryption function did not use the same gen_k
function, the author of the code would have done the optimization that
causes the vulnerability in the signing function because this has never been
recommended (while for encryption it is a well known recommendation).
Maybe we can learn that using the same key for two different things is
really really not a good idea!  If the vulnerability was restricted to
it would have been less severe, being able to decrypt all confidential
that were created in the past is much more severe.  Allot of applications
one single key for both signing and encryption, while this doesn't seem to
immediately dangerous I don't think it's a good idea. For example when I
an email from someone that is signed, Outlook will save the public signature
that comes with the message and use it to encrypt if I decide to send an
message to that person.
I never understood why having separate keys for signing and encrypting was
so complicated to implement?    Also in the PoP protocol of X.509, a
using the private key is used to prove possession of the private key
corresponding to a public encryption key.  While the different padding used
in signature and encryption schemes make it difficult to find an obvious
vulnerability with this, I don't think it's a good idea.
You have to be very careful when using the same key pair for encrypting and
signing.  The subtle error found in GnuPG about using small k is a good
example.  Another thing to consider is that ElGamal encryption with base
g = 2 is safe but insecure for signatures...  It's just simpler to have two
distinct pairs of keys.
By the way, is the paper by Phong Q. Nguyen describing the vulnerability
available somewhere?  Or maybe someone could describe the cryptanalysis
steps to retrieving the private key from the signature when using smaller
random k, I would appreciate.  ElGamal with smaller k looks allot like
DSA, exept in DSA you work with a generator of a smaller subgroup and
your k is chosen in this smaller subgroup...

@_date: 2003-12-07 09:36:04
@_author: Anton Stiglic 
@_subject: safety of Pohlig-Hellman with a common modulus? 
Sent: Saturday, December 06, 2003 7:58 PM
If you don`t know M and k, there are several values M', k' such that
M'^k' mod p == M^k mod p.   For example, if M is a generator of the
group mod p, than all other generators M' will have a corresponding k'
that will give you this value.
Think about known plaintext attack or chosen plaintext attack.  A symmetric
cipher should be secure against these attacks and much more...
In these attacks you know the bases of several values...

@_date: 2003-12-07 14:11:14
@_author: Anton Stiglic 
@_subject: yahoo to use public key technology for anti-spam 
; Sent: Sunday, December 07, 2003 8:44 AM
But you should be sending mails via *your* SMTP server, and should be
connecting to that SMTP server using SSL and authentication.  Open relays
encourage spam.  People shouldn't be relaying mail via just any SMTP server.

@_date: 2003-12-08 10:15:34
@_author: Anton Stiglic 
@_subject: yahoo to use public key technology for anti-spam  
So I`m guessing you have all your emails forwarded to one mail account
and fetch them all from there, and when you reply or send a new email
you just use one of your SMTP servers, which doesn't necessarily correspond to the incoming (POP or IMAP or whatever) server you
received the mail from.  Is that correct?
In that case I guess it becomes problematic.
If you just receive your mail from one incoming server I don't see a
problem of having your mail be sent via the SMTP on same machine
where your incoming mail server resides. If the signature just certified that the mail was relayed via an SMTP
server where the user authenticated himself I think that would be a good idea (SMTP server that necessarily on the same machine than
the incoming mail server).  Than at least you would know that the email you received was send by someone who authenticated himself to some SMTP server, and not just someone that sent the email via an open relay.
If you want something better it seems that it requires the sender to have possession of his private signature key and sign the emails he sends, but that's not a user-friendly solution and I think we all
agree that it won't work in practice (not transparent enough...)

@_date: 2003-12-12 09:44:16
@_author: Anton Stiglic 
@_subject: "Zero Knowledge Authentication"? (was Cryptolog Unicity Software-Only Digital Certificates) 
So anybody knows exactly what this zero-knowledge authentication is
that they use?
This part about storing private keys on a server is not novel.  The company
that I work for has a similar solution with respect to this, it's called

@_date: 2003-12-12 09:53:05
@_author: Anton Stiglic 
@_subject: Postgraduate programs 
focusing on crypto related themes in the world?
government policies on several countries and knowing your suggestions >on
good schools is a key component for my paper.
You can start by looking here:
 --Anton

@_date: 2003-12-15 11:46:59
@_author: Anton Stiglic 
@_subject: PKI root signing ceremony, etc. 
We had something similar going on at Zeroknowlege Systems for the PKI
of the Freedom servers.  But the password that protected the private key
was in split knowledge, and the observer wrote down what he observed
in a log journal...
You also want to make sure that the computer you are doing this on is not connected to another machine or network.

@_date: 2003-02-04 16:43:51
@_author: Anton Stiglic 
@_subject: question about rsa encryption 
Read the section on Hastad's Broadcast Attack from Boneh's excellent survey paper
"Twenty years of attacks on the RSA cryptosystem"
The paper covers these basic facts about RSA, you can
get it at
The section on RSA in HAC will also answer your question.

@_date: 2003-02-05 10:03:01
@_author: Anton Stiglic 
@_subject: question about rsa encryption 
No, it also applies to the public exponent if the messages you encrypt are
related in a simple way (something like OAEP will make them *not* related
in that simple way and prevent the attack).  Funny thing is that the attack
described in the paper by Boneh that *you* cited, which I also mentioned
in my last post...
There are also attacks on low private exponents, but that`s something else
(good randomized padding doesn't prevent that)...

@_date: 2003-02-07 12:22:45
@_author: Anton Stiglic 
@_subject: password based key-wrap (Re: The Crypto Gardening Guide and Planting Tips) 
Sent: Thursday, February 06, 2003 8:07 PM
One of the problems is exactly that.  There is no known proof of security
for PBKDF2 (it might be possible to come up with one, but to the best
of my knowledge nobody did so far).  Ironically, there are some proofs
of security for the older version of the same standard, PBKDF1 (which
was replaced by PBKDF2 only because the output of PBKDF1 was of
fixed length, so you couldn't derive much key material).  You can prove
some things about PBKDF1 relating to the fact that an adversary cannot
compute the result of PBKDF1 without having to compute a certain required
amount of hashes (this is the stretching part).  The details
of that are in the paper "Secure Applications of Low-Entropy
Keys" by Kelsey, Schneier, Hall and Wagner:
But I do think that PBKDF2 sounds reasonable, and I wouldn't be
surprised if we can prove something about it's security in some reasonable
model.  I would use PBKDF2 if I needed to wrap a session key
with only a password.
In general, the problems with existing proposed key derivation
functions is that they are all based on ad-hoc constructions.
There is a skunks work group trying to come up with a
proposal for a key derivation function which is based on some
provable secure results.

@_date: 2003-02-12 11:58:45
@_author: Anton Stiglic 
@_subject: CDMF  [was Re: Columbia crypto box] 
I can't find a description of it on the web (I had it on a paper somewhere
I don't have access to it now),
but I think that what it does is encrypt the intial 56-bit key (64 bits with
additional parity bits) using a fixed value 56-bit key.  Then, given the
result, it would substitute 16 (non parity) bits with fixed values and
the result so that it had correct parity, effectively tranforming keys with
random bits to only 40 random bits.  (maybe I'm recalling wrongly however
I think it was something like that...)
Can you do E-D-E with this, using double-length keys, to effectively get a
strength of 80 bits? (at a first glance, I don't see what would prevent you
from doing this..).
If it's a library that you were exporting, with an API for encryption and
nothing would prevent someone from using such a library to do strong

@_date: 2003-02-19 11:30:29
@_author: Anton Stiglic 
@_subject: AES-128 keys unique for fixed plaintext/ciphertext pair? 
While your reformulation of the problem is interesting, the initial question
was regarding plaintext/ciphertext pairs, which usually just refers to the
of elements from {0,1}^n, {0,1}^n, where n is the block cipher length.
Think of encrypting n-bit keys....
If you just want to consider the plaintext set to be the subset consisting
of ASCII text in English you should specify so...

@_date: 2003-02-21 12:46:07
@_author: Anton Stiglic 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security Advisory: Timing-based attacks on SSL/TLS with CBC encryption  
If I'm not mistaken, the OpenSSL spec says that you should
MAC the (compressed) message, and then encrypt the message
and the MAC.
This composition is not generically secure, on the other hand you
can prove some nice things about the composition encrypt-then-
MAC assuming certain conditions, see for example
David Wagner's post on sci.crypt for a discussion about
(using CBC-DES with a random IV and then HMAC,
with a KDF that derives independent keys for the encryption
and the MACing (the KDF in SSL looks like it can do this)
would satisfy these conditions.)
I now always recommend encrypt-then-MAC.
If SSL required encrypt-then-MAC, a programmer
would more naturally start by verifying the MAC, then decrypt
the message, so Vaudenay's attack would be caught first by
the MAC verification and the implementation would probably
return an error after the MAC verification and not leak the
information needed to discover the plaintext.
So even though the attack is not directly the result of the SSL
protocol spec, a spec which would favor encrypt-then-MAC
would be better in my point of view and the security holes
relating to this SSLattack in implementations might have much
less of a chance of existing.
 --Anton

@_date: 2003-02-25 10:41:51
@_author: Anton Stiglic 
@_subject: [Bodo Moeller <bodo@openssl.org>] OpenSSL Security Advisory: Timing-based attacks on SSL/TLS with CBC encryption  
By Pad-t-MAC-t-encrypt, do you mean a scheme where
the MAC is also encrypted, or is left aside (the encrypt and authenticate method).
There are problems with the latter as well, see appendix C of the
paper from Krawczyk...
If it's the first, then I guess that what you mean by Pad-t-MAC-t-encrypt is that you first pad the message (and IV and whatever other context) such that when you append the MAC
(e.g 160 bits with SHA1-MAC) to the ciphertext the resulting size is a multiple of the block cipher size.  So when you decrypt, you don't
check the padding, but then after verifying the MAC you would take
out the padding (and I guess verify it...).  You can't play with the
padding, because the MAC will fail.  But if you are using CBC-DES
as a MAC, you need to make sure that the MAC is verified first, not check the padding first, if not you *might* fall in to a similar trap (I'm not certain a vulnerability would exist in that context, but it sounds
Pad-t-MAC-t-encrypt sounds like an interesting avenue, but why would you propose that for TLS 1.1 instead of just proposing the safe
Pad-t-Encrypt-t-MAC?  If there is going to be a change, might as well go with something that is provably secure, or is there some reason (compatibility or something) to prefer Pad-t-MAC-t-Encrypt that I do not see here?

@_date: 2003-01-04 14:53:40
@_author: astiglic@okiok.com 
@_subject: Implementation guides for DH? 
For RSA, Silverman and Rivest have a paper arguing that *strong* primes
are not currently beleived to be needed (see the paper for the def
of strong prime).  In DH key exchange, when you work in a group (mod
a prime) you want to make sure that there are no little subgroups that
an attacker can exploit (choosing a *safe* prime (p = 2q + 1, q and p
prime, or p = Rq + 1, with p and q sufficiently large), and working
in the subgroup of order q guarantees you this, so it usefull to have
these kind of primes for DH.

@_date: 2003-01-20 14:46:02
@_author: Anton Stiglic 
@_subject: Prime numbers guru 'factors' down success 
Sent: Monday, January 20, 2003 11:47 AM
Technically you can't say that.  P is in NP, so if a problem is found to be
in P, it's not out of NP.  Note as well that the *problem* is situated in a
class, not the *algorithm* that solves it.
It was known before Agrawal and al. result that primality testing was in ZPP
for example (a class "between" P and NP).
The following FAQ explains this in a bit more of detail:
But the core of Laurie's argument is correct, in that it's not true that
only since
Agrawal and al. result can we determine if large integers are primes or not,
allowing for exponentially small probability of errors there are previously
algorithms that do much better than the algorithm from Agrawal and al.

@_date: 2003-01-21 09:49:59
@_author: Anton Stiglic 
@_subject: Key Pair Agreement? 
Call it kosherized public key generation.  Kosherization is not a term often
used in theoretical cryptography, but it is often used in practice
Hold on, what you have kosherized is the public parameters of DSA, but
you haven't really kosherized the public key, y  (IINM).
Given P, Q, G (chosen by say Scott, or kosherized by Alice), Alice could
up with a cooked-up public key y.
It would seem difficult to impose some structure on y, since Scott will want
choose a random x, in which case G^y % P will look random.
This is different from RSA, where the public key is the pair e, N, e can be
to 3, and you can impose some structure on N (as Wagner pointed out).

@_date: 2003-01-21 15:46:58
@_author: Anton Stiglic 
@_subject: Key Pair Agreement? 
It is easy in both cases, here are examples I easily came up
(low order DEADBEEF))
p = A093CD75C6B7A577B99897B323BE30936448D25E6F0E3ED3FC6FEA2BD8229B62994A\
q = BDDA82F4039620F351EBAEA1EC4AC4D3595DFBE44CB9554669419182720AE6E6113F\
p*q (high order DEADBEEF)
p = DEADBEEFB782E95FCB1B04C7A90F501FEC0F6FE8B5EDC31A0996DBF9CC088C7C07AE\
q = 10000000023A78BC3DE4851ED3909CB37A402021766289052B9F7C9F00826E49117D\
p*q

@_date: 2003-01-27 10:21:43
@_author: Anton Stiglic 
@_subject: Shamir factoring machine uninteresting? 
I'd say that most people are interested in such attacks only when they are
actually implemented.  So far the concept of TWIRL seems to be only
Some theoretical attacks attract attention because if implemented
they would really change the way we think of crypto.   TWINKLE is such an
example (when the concept of TWINKLE first came out, allot of people
were talking about it), so is the work of D.J. Berstein on factoring.
But to decide whether or not a theoretical attack can be practical or not
is difficult and time consuming.  Take for example the XSL attack on
block ciphers such as AES, in which there seems to have been an error
(pointed out by Coppersmith) which invalidates the results, look at the
error in the proof of OAEP (a theoretical result which was widely accepted
for some time, but Shoup found an error in the proof).
The best demonstration is to actually implement it.
In the abstract of the TWIRL paper, it says that TWIRL can enable
the NFS sieving step more cost effectively, with 3-4 orders of magnitude
more than TWINKLE, but TWINKLE was never implemented (and if
I'm not mistaken, there is doubt about whether or not it can be
implemented?), and 3-4 orders is not that big of a magnitude.
The abstract also says that the NFS sieving step of 512-bit RSA keys
can be done in less then 10 minutes by a 10K device.  10K is not that
much to spend on research, so if this can really be implemented I'm thinking
that someone can do it soon.
Personally, I'll wait and see if someone comes up with a proof of concept,
and if so then I'll take the time to read the paper.  For now, I already
512-bit RSA keys as insecure (because 512 bit integers have already been
factored, and I always allow for a cushion factor so I'm sure it can be
even more efficiently).  For now, there are many other results which I would
like to read about which are of interest to me at the present time as a
cryptographer with an eye on implementation.  This is not to say that
I really respect the work of Shamir and I'm sure that the TWIRL paper has
some interesting results.

@_date: 2003-01-27 14:52:03
@_author: Anton Stiglic 
@_subject: Shamir factoring machine uninteresting? 
I worte ->
I take that back.  When considering cost, 3-4 orders of magnitude is

@_date: 2003-01-29 10:37:11
@_author: Anton Stiglic 
@_subject: EU Privacy Authorities Seek Changes in Microsoft 'Passport' 
[Talking about Microsoft Passport...]
It does make some sense.  The more people who are developing the system
who know better, the more they may influence higher management.
I'm sure that you know that in a big company like Microsoft, it's not the
architect or cryptographer that decides what is shipped out, but managers
don't care about security but more about $.
The more security-conscious people who start working for Microsoft, the
they will have more power to influence the decisions of higher management.
Microsoft has the most widely used software products, it's a good place for
someone to try to influence good security practices.
If you are a security person or cryptographer, you can either decide to work
some small company which has good security practices and your opinions be
considered, but their products not widely spread, or for a big company with
widely spread products but which has bad security practices, and try to
change things
(even though your opinions are less considered).   In which case does the
person or cryptographer have the most impact on the world of software

@_date: 2003-07-09 09:13:14
@_author: Anton Stiglic 
@_subject: Fwd: [IP] A Simpler, More Personal Key to Protect Online Messages 
Yes, that works.  When I want someone to send me confidential
email, and that someone doesn't know much or anything about crypto, I usually just send an S/MIME signed email, and let his MUA (usually
Outlook or Outlook Express) do the work of saving the certificate
and all.
The way I see IBE being useful is as a corporate solution for
encrypting messages.  Inside a corporation everyone will use the same public parameter (which could probably come with the software installation).  And in most corporate crypto solutions
you want key escrow, which IBE gives you as well for free :)
The benefit is that you don't need to deal with users public keys:
you don't need to get them from some repository or ask the
person to send it to you by email and stuff.  So say that you are
with your laptop away, and don't have the persons public key
certificate, you can still send him/here email directly (without asking
anyone to send you his/her public key).  I admit the feature is
of limted value however.
In the Boneh-Franklin paper one suggestion is to use
bob at company.com || current-year
which would make public keys good for one year (which sounds
reasonable, especially within a corporation).   Of course, the software will include the year when creating the public key, the users wouldn't need to do it explicitly.  If you really want to be able to revoke
public keys, you need more granularity and use something like
bob at company.com || current-date, and that does become
anoying for the users (need to fetch your private key everyday).
One interesting thing about IBE is that you can transform any such scheme into a Non-interactive forward-secret cryptosystem as
Adam Back pointed out:
(his web server might be down, but you can look at the cached version
on Google...).

@_date: 2003-07-09 13:57:36
@_author: Anton Stiglic 
@_subject: replay & integrity 
I don't believe that is enough.  Take for example
the SSL 2.0 ciphersuite rollback vulnerability or the SSL 3.0 key-exchange algorithm vulnerability . Any kind
of rollback attack is serious, and won't be protected
by signatures in the bulk data (and those signature might
be weakened by forcing a rollback to a possible weaker
So maybe I can't replay a complete financial transaction, because at some high layer there is replay prevention,
what about replaying some init protocol request?
Is that not annoying?  Would a bank not care that their ATMs are not working for a day because someone
is executing a DoS attack on the lower layers of the protocols of their system? I think not, you need replay protection on both levels. How can a secure socket be dubbed secure if it doesn't
protect against these basic attacks?
To quote from Wagner and Schneier`s paper, Analysis
of the SSL 3.0 protocol:
"Replay attacks are a legitimate concern, and as they are
so easy to protect against, it would be irresponsible to fail
to address these threats."

@_date: 2003-07-16 15:13:14
@_author: Anton Stiglic 
@_subject: Looking for an N -out-of-M split algorithm 
Just as Perry mentioned, look into Shamir Secret Sharing.
There are also implementations of this, see for example
(I'm not certain if I ever used that one in particular, so I don't know if
it's good,
but I'll let you do the research...).

@_date: 2003-06-20 14:33:47
@_author: Anton Stiglic 
@_subject: Security of DH key exchange 
Sent: Friday, June 20, 2003 5:02 AM
g^a and
I don't know of any references that will explain this explicitly, but the
reasoning is simple:  You model h as a random oracle, which would imply that
if the minimum entropy of g^(ab) is at least n bits, then h(g^{ab}) will be
indistinguishable from a value chosen randomly for the set of n-bit strings.
For information on general about DH, you can look at the following

@_date: 2003-06-26 11:07:41
@_author: Anton Stiglic 
@_subject: pubkeys for p and g 
I'm not certain I understand your questions, but here are some answers (I
In the DH protocol you have what we call public parameters, p and g.
p is a large prime integer, which defines a group Z*p, g is a generator
defines a subgroup in Z*p.
You can use fix values for p an g.
Now, participants will choose private and public keys.  The private key
is simply chosen as a random number x, whose value is between 1 and
p-1.   The public key associated to x will be y = g^x mod p.
Participants keep x secret and y is public.
You can say that (y, g, p) is the public key, or simply say that y is the
key if g and p (the public parameters) are implicitly known.
Participants can choose a different x and associated y on each execution
of the protocol, or have long term private public key pairs.

@_date: 2003-03-06 10:37:51
@_author: Anton Stiglic 
@_subject: Proven Primes 
Sent: Thursday, March 06, 2003 6:47 AM
I'm not aware of such a list.  If you can't find any you can generate the
list yourself using ECPP (Elliptic Curve Primality Proving), an
implementation of
which is available here
The result of ECPP is guaranteed (no probability of error), and provides a
of primality for integers that are proven to be prime.
A competing algorithm is the Jacobi Sums test, but it is much more
so implementation errors are not to be disregarded, with ECPP the
verification of a primality certificate is simple to implement, so you can
sure that there were no errors in the implementation of the proving
There is also the new algorithm by Agrawal, Kayal and Saxena, but I don't
believe that it is efficient in practice for the sizes of integers you are
looking at.
Also note that if you assume the Extended Riemann Hypothesis (ERH) to
be true, you can use the Miller-Rabin algorithm in a deterministic fashion
polynomial time with no probability error.
The ECPP package is easy to use and fast.
The site gives benchmarks for proving 512-bit primes:
Pentium III (450MHz)                4.4 sec
Solaris 5.7                         9.5 sec
Alpha EV56 (500MHz)                 4   sec
I suggest you generate potential Sophie Germain primes q using your favorite
library (I use OpenSSL for example) and then use the ECPP package to verify
that in fact both q and 2q + 1 are really prime.

@_date: 2003-03-06 10:52:50
@_author: Anton Stiglic 
@_subject: Scientists question electronic voting 
Sent: Thursday, March 06, 2003 2:14 AM
An extortionist could provide their own camera device to the voter, which
a built in clock that timestamps the photos and does some watermarking, or
something like that, which could complicate the counter-measures. But this
problem already exists with current non-electronic voting scheme.
It depends on the value attributed to a vote (would an extortionist be
willing to provide these custom devices?).

@_date: 2003-03-06 12:26:07
@_author: Anton Stiglic 
@_subject: Scientists question electronic voting 
Well the whole process can be filmed, not necessarily photographed...
It's difficult to counter the "attack".  In you screen example, you can
the vote and then immediately photograph the "thank you", if the photographs
include the time in milliseconds, and the interval is short, you can be
to some degree that the vote that was photographed was really the vote that
was casted.
You can have tamper resistant film/photograph devices and whatever you want,
have the frames digitally signed and timestamped,
but this is where I point out that you need to consider the value of the
vote to
estimate how far an extortionist would be willing to go.

@_date: 2003-03-06 13:00:18
@_author: Anton Stiglic 
@_subject: Proven Primes 
It's been a while since I tried it, I don't remember which platform and
OS  I used (a pentium with some sort of Linux) but I know that I didn't have
problems using it.
I think that ECPP comes with a Maple certificate verifier, which might
be what you are looking for.  I think you can also convert certificates
to Mathematica format.  So once you have these certificates of primality
it's easy to verify them.  But I haven't tried any of those features...

@_date: 2003-03-07 10:00:28
@_author: Anton Stiglic 
@_subject: Scientists question electronic voting 
But that brings up my point once again:  These problems already exist
with current paper-ballot voting schemes, what exactly are you trying to achieve with an electronic voting scheme?  To you simply want to make the counting of the votes more reliable, and maintain the security of all
other aspects, or improve absolutely everything?

@_date: 2003-03-07 14:08:04
@_author: Anton Stiglic 
@_subject: Proven Primes 
Chapter 4 of the HAC gives a good introduction to all of this.
There are probabilistic primality tests (e.g. Miller-Rabin), there are
proving algorithms (e.g. Jacoby Sum Test, ECPP), some of which give a
of primality that can be verified using a different algorithm. Some of the
tests work
on integers of special forms (e.g. Mersenne numbers), others work on all
There are also algorithms that generate integers that are guaranteed to be
(e.g. Maurer's algorithm),  these are not tests...

@_date: 2003-03-10 09:23:19
@_author: Anton Stiglic 
@_subject: prime proofs 
The contribution of Pratt was to be the first to publish a proof
that the certificate can be verified in polynomial time (thus proving
that PRIMES is in NP).
----- Original Message -----
Sent: Friday, March 07, 2003 5:06 PM
majordomo at wasabisystems.com

@_date: 2003-03-11 11:56:53
@_author: Anton Stiglic 
@_subject: Proven Primes 
Sent: Tuesday, March 11, 2003 11:28 AM
No, but you can speed up modulo multiplication.  The OAKLEY RFC
   The high order 64 bits are forced to 1.  This helps the
   classical remainder algorithm, because the trial quotient digit can
   always be taken as the high order word of the dividend, possibly +1.
   The low order 64 bits are forced to 1.  This helps the Montgomery-
   style remainder algorithms, because the multiplier digit can always
   be taken to be the low order word of the dividend.
At one point in time some of my colleagues got the optimization with the
high order bits set to 1 in C code going on very well, I don`t remember if
we implemented the optimization with the low order bits set to 1.

@_date: 2003-03-14 11:10:26
@_author: Anton Stiglic 
@_subject: Diffie-Hellman 128 bit 
Sent: Thursday, March 13, 2003 4:48 PM
128-bit prime DH would be trivially breakable, maybe you mean that
it uses128-bit secret keys (and a larger prime, such as 512-bit prime at
In any case, you can probably get all the information you are looking
for in this manuscript:

@_date: 2003-03-14 11:14:27
@_author: Anton Stiglic 
@_subject: Encryption of data in smart cards 
Yes, but wasn`t the discussion about countermeasure to just reading
the contents of the smart card.  If you can read the encrypted data,
and it`s encrypted under a key derived from a PIN, you have all
the time and chances you want to try all PINs.  That`s the reason
why it doesn`t work.

@_date: 2003-03-17 11:16:13
@_author: Anton Stiglic 
@_subject: Diffie-Hellman 128 bit 
No, just use the Number Field Sieve algorithm (this is mentioned in section
3.5 of the manuscript I gave you the link to).
You could read section 3.6 of the Handbook of Applied Cryptography for
a basic introduction to the problem of discrete logarithm.

@_date: 2003-03-17 14:24:42
@_author: Anton Stiglic 
@_subject: Diffie-Hellman 128 bit 
Sent: Friday, March 14, 2003 9:32 PM
Sorry, I mentioned using NFS in my previous reply, which is probably not
the way you want to go about this (since it's not as efficient for small
and more complicated to code).
Index-Calculus with Gaussian integers is indeed a good way.
You can look at the paper from LaMacchia and Odlyzko
which Derek and maybe someone else pointed out..
They easily calculated discret logs modulo a 192-bit integer.

@_date: 2003-03-24 11:33:30
@_author: Anton Stiglic 
@_subject: Cryptoprocessors compliant with FIPS 140-2 
The list of all FIPS 140-1 and 140-2 validated modules can be
found here
(this includes software and hardware modules).
For "Mitigation of Other Attacks", the FIPS 140 evaluation doesn't
look at these.  Some vendors might consider these attacks and implement
some kind of protection, but these will not be evaluated.
Documentation for a specific module might discuss countermeasures
to these attacks if they have been implemented.
----- Original Message -----
Sent: Friday, March 21, 2003 11:14 AM
majordomo at wasabisystems.com

@_date: 2003-03-25 00:20:59
@_author: Anton Stiglic 
@_subject: Brumley & Boneh timing attack on OpenSSL 
Sent: Monday, March 24, 2003 1:20 PM
If you are a client, and you manually control the signature generation (like
you use PGP to sign email messages), I wouldn't implement blinding.
But if you are a server (or a client that automatically responds to
that signs message for some reason, and you receive many requests, I would.
RSA decryption, yes for servers.
Again, if you are automatically answer to requests, yes I would.  In the
Freedom network, servers had non-ephemeral keys and did a DH key
exchange with clients (client side used ephemeral keys and was anonymous),
we implemented blinding on the server side to counter timing attacks because
we had a hunch that they could work over network connections.
No, I wouldn't.  I would be very surprised if you could do timing attacks on
one execution of a modulo exponentiation, unless there is some way to trick
a server in using the same secret on different inputs, even though it's
to do ephemeral DH.
Yes if you automatically answer to requests.  Paul Kocher's initial paper on
subject explicitly mentions DH, RSA and DSS.
If there is a possibility that you can be used as an oracle, and you have a
key, you should be careful.

@_date: 2003-05-05 09:51:55
@_author: Anton Stiglic 
@_subject: The Pure Crypto Project's Hash Function 
Sent: Sunday, May 04, 2003 2:57 AM
There is MASH-1 and MASH-2, based on modulo arithmetic
(see for example the Handbook of Applied Cryptography, section 9.4.3).
They are relatively recent proposals, I don't know if there has been any recent successful cryptanalysis on them.
They are based on sqmodn, which was broken by Coppersmith.
It's not the kind of hash algorithm I would feel comfortable with
for cryptographic purposes, but it surely was more widely cryptanalyzed than what you proposed.

@_date: 2003-05-13 15:53:16
@_author: Anton Stiglic 
@_subject: Payments as an answer to spam 
I don't agree with that point.  PODS implies
PKI, which is not easy at all.  That's the
beauty of schemes like hashcash, they need
very little administration overhead.
Non-interactive Hashcash can be implemented completely transparently from the regular users point of  view.  PODS can't, if you want to protect
your private key, you need to request a smart card or at least a passphrase;  Unless
you have an option "don't ask for my passphrase
again" like they have in Windows, see P. Gutmann's paper "Where do your encryption keys want to go It already is integrated in mail clients.  Outlook is probably the most used MUA, and it implements S/MIME.  PGP has plugins for several MUAs.
There are free plugins based on GPG.
What prevents widespread use of the crypto implemented in these MUA is PKI
"I don't want to bother getting a certificate from Implementationally speaking :), I don't see hashcash as having the same problems, at all.  Have Microsoft integrate it in their MUA
and it's a done deal (hopefully in a standard way of course, so that others can be compatible...).
The only question that remains for me is if Hashcash-
like schemes will really frustrate spamers, so the question is if it's worth to integrate it.

@_date: 2003-05-14 10:38:18
@_author: Anton Stiglic 
@_subject: Modulo based hash functions [was: The Pure Crypto Project's Hash Function] 
A bit belated, but might be useful to someone...
I was reviewing some parts of Stinson's book and was
reminded of some schemes I had forgotten about.
One hash function is described in one of the exercises, it is basically the same thing that Ron Rivest described in his
post to this tread.  It consists of an exponentiation operation, with the base being a fixed generator of largest order, the exponent
being the value to be hashed, and the modulo being an RSA integer
(assume factorization is unknown).
Stinson references the following paper for this scheme:
J.K. Gibson.  Discrete logarithm hash function that is collision
free and one way.  IEE Proceedings-E, 138 (1991), 407-410
Another hash function described in detail (with a detailed proof
of security and an example) in Stinson's book is due to Chaum, van Heijst and Pfitzmann.
You have a large prime p with q = (p-1)/2 also prime.  a and b
are two primitive elements of Z_p, you assume that the value of log_a (b) is not publicly known (and that it is difficult to compute).
The hash function is defined as
    H(x1, x2) = a^x1 * b^x2 mod p,
where x1 and x2 are values in the range  [0, q-1].
You can extend this hash function (or any other finite domain hash
function where the length in bits of the input is greater than that of
the output), to a hash function with infinite domain using the well known techniques due to Damgard and Merkle.
The reference given for this scheme is the following:
D. Chaum, E. van Heijst and B. Pfitzmann.  Cryptographically strong undeniable signatures, unconditaionally secure for the signer.  Lecutre Notes in Compute Science, 576 (1992), 470-484.  (Advances in Cryptology - CRYPTO '91).
If you want some kind of provable security, have someone you
trust to generate the public parameters, and don't mind your
hash functions being slow, these are both good options.
P.S.  Damgard and Merkle references are:
I.B.Damgard.  A design principle for hash functions.  Lecutre
Notes in Computer Science, 435 (1990), 416-427.  (Advances
in Cryptology - CRYPTO '89).
R.C.Merkle.  One way hash functions and DES.  Lecutre Notes
in Computer Science, 435 (1990), 428-446.  (Advances in Cryptology
- CRYPTO '89).

@_date: 2003-05-15 11:03:38
@_author: Anton Stiglic 
@_subject: Payments as an answer to spam 
PODS using something ? la PGP would not
imply PKI, but still a centralized server
as you said.
Payment systems don't need a PKI, but
a centralized server as you said, and
also needs some kind of financial
institution to bootstrap things (which is easier
to do in a closed system, than in an open
system like the Internet).
Yes, you'll have the money to pay for the
centralized servers, but the system will become
more complex, which might infringe it's
widespread deployment and use.  Hashcash
is simple
While I don't like the fact that almost everybody uses
Microsoft products, it's a fact that I have to live with.
If there is something to be added on a client, having
Microsoft add it will force everyone to use it.
It's just a fact of life.  Take Microsoft Word as
an example.
Do you have a proposal which would not involve
Microsoft integrating something, and that would
be used by everyone?
Agreed, that's why you have to put it in all the MUAs,
and after a couple of years everyone will start using it
transparently.  It takes time, but that's the only way to
do it.

@_date: 2003-05-20 10:23:59
@_author: Anton Stiglic 
@_subject: Primality Algorithm 
Sent: Tuesday, May 20, 2003 5:20 AM
No, he actually meant 3 :)
Well, it depends...
If you want to generate a 1024 prime say, by choosing random candidates
from a uniform and independent distribution (or by incremental search
starting from a random candidate), and testing the candidates with rounds
of Miller-Rabin until you get an integer that passes the Miller-Rabin tests,
then 3 rounds of Miller-Rabin is enough to get a probability of error
a integer that is in fact composite) of less than 2^80.
This is explained in section 4.4 of the Handbook of Applied Cryptography.
You can follow the references for the papers describing the original results
by Damgard, Landrock and Pomerance,  and also by Brandt and Damgard
in the case of incremental search.  The paper by Beauchemin, Brassard,
Crepeau, Goutier and Pomerance is also interesting for calculating the more
easy to find error bound of 2^80 when using 40 rounds or Miller-Rabin, the
reasoning is given in note 4.47 of HAC;  I say easier to find
 bound but the result is not straight forward from
the fact that 1 round of Miller-Rabin will declare a composite to be prime
probability less than 1/4, as many people wrongly deduce.
If you are given a candidate from an unknown or not uniform and independent
distribution, and you are asked to test if its prime or not, then you need
40 rounds
of Miller-Rabin (so that the probability that a composite passes as a prime
be less
than 2^80).

@_date: 2003-05-20 11:48:29
@_author: Anton Stiglic 
@_subject: Modulo based hash functions [was: The Pure Crypto  Project's Hash Function] 
The n used in the hash should be independant from the n used in the
signature scheme.
If it's the same, the signature on a message m will be
(g^m % n)^d % n which is simply g^dm % n,
thus if you have a signature on x and one on y,
you can simply multiply the signatures together and you
will get
g^dx * g^dy  % n = g^(d*(x + y)) % n
                            = (g^(x + y))^d % n
so it's normal that such a signature will verify on the message x+y.
This is a bit what David Wagner complained about (interaction
of a number-theoretic based hash function used in a number-theoretic
cryptosystem), except here you are using the hash in a wrong way
(using same modulus as in your signature scheme) and this bad
interaction was clearly predictable.
If you have n0 as the modulus for your hash, and n1 as the
modulus for your signature scheme, the signatures on x and
on y would be
  (g^x % n0)^d % n1
  (g^y % n0)^d % n1
combining both of these by simply multiplying will give you
   (g^x % n0)^d * (g^y % n0)^d  % n1
 =  ((g^x % n0)*(g^y % n0))^d % n1
this should be a signature on an unknown value, except if there
is some bad interaction with the operations of the hash function
and that of the signature function (which is clearly the case if
n0 = n1 since the overall function becomes homomorphic...)
With your numerical example, this would be a signature
on the hash 2949*1284 = 3786516
the message m that hashes to that value is
----- Original Message ----- "Anton Stiglic" Sent: Monday, May 19, 2003 3:09 AM
majordomo at metzdowd.com

@_date: 2003-05-20 12:00:01
@_author: Anton Stiglic 
@_subject: Fallacious use of 'bank' in net payment systems 
Sent: Saturday, May 17, 2003 11:52 AM
I completely agree.  This is why I said "some kind of financial
institution", but that was bad wording, issuer is better.  You only need someone that can collect money from users for the tokens it issues, and this is easier done in a closed system (the
point I wanted to make).
Depending on the digital payment scheme you want to implement, the issuer can be an ISP, a phone company, cable provider, or
even a grocery store...  It's easier to get a digital payments system
working if you are in a closed system since different issuers don't
need to collaborate (to transfer payments, etc...)

@_date: 2003-05-22 10:26:13
@_author: Anton Stiglic 
@_subject: Primality Algorithm 
Sent: Monday, May 19, 2003 8:29 AM
Phil Carmody has an excellent collection of links to things related to the
result, including links to implementations in C, C++ and other languages:

@_date: 2003-05-30 10:29:30
@_author: Anton Stiglic 
@_subject: "PGP Encryption Proves Powerful" 
So what happened to passphrase guessing?  That's got to be
one of the weakest links.  Unless their private key wasn't
stored on the device?

@_date: 2003-11-17 17:58:15
@_author: Anton Stiglic 
@_subject: A-B-a-b encryption 
Sent: Sunday, November 16, 2003 12:50 PM
Also described in HAC, protocol 12.22.
It's like basic DH, except it provides key transport instead of key

@_date: 2003-11-18 09:19:48
@_author: Anton Stiglic 
@_subject: Are there...one-way encryption algorithms 
"David Wagner"  wrote in message
news:bp9c6h$kpe$1 at abraham.cs.berkeley.edu...
If I'm not mistaken you are wrong.  Pohlig-Hellman proposed an encryption
scheme based on discret log, the description of the OP was for a
key transport protocol.
In Pohlig-Hellman, what you do is have Alice and Bob share secret
keys k and d such that k*d == 1 mod (p-1), where p is some prime.
To encrypt a message M Alice computes M^k mod p, and Bob
can decrypt by computing (M^k)^d mod p == M mod p.
This is commonly referred to as the Pohlig-Hellman symmetric-key
exponentiation cipher.
It is described in patent 4,424,414 which you can find here
Also mentioned in HAC, chapter 15, section 15.2.3, (iii).
The algorithm that was described by the OP is really Shamir's
three-pass algorithm, also known as Shamir's no-key protocol.

@_date: 2003-11-27 09:58:08
@_author: Anton Stiglic 
@_subject: Problems with GPG El Gamal signing keys? 
The note talks about GPG Elgamal encryption and signature schemes
using small value of k (where k is the random value that you pick for
each signature, each encryption).  For encryption choosing a small k
is o.k. (by small I mean something like 160 bits when you have a 1024
bit prime), but this was never recommended for the signature scheme,
and the note states that this would in fact be a security
vulnerability.  The note says that with one signature using a certain
private key x, generated using a small random k, you can compute the
private key x.  So if you are also using this key for decryption, the
private key found could also be used to decrypt everything that was
encrypted to you.
I haven't put any taught yet in how you would retrieve the private key
given the signature (I just read this email), but it sounds plausible.
One thing I can confirm however is that GnuPG 1.2.3 (the latest
version available from the GnuPG we site) indeed has is that both the
encryption and signature schemes use a small k.
If you have the source code, just take a look at cipher/elgamal.c,
there is a function gen_k( MPI p ) that is called by both
do_encrypt(...) and sign(...)  functions.  In the function gen_k, k is
chosen to be of size nbits, where nbits is smaller than the size of
the prime.  Look at the comment in the code:
 /* IMO using a k much lesser than p is sufficient and it greatly
     * improves the encryption performance.  We use Wiener's table
     * and add a large safety margin.
     */
    nbits = wiener_map( orig_nbits ) * 3 / 2;
wiener_map maps sizes of primes to sizes for k and q.  For example,
for a 1024 bit prime, the function will return 165, so in this case
nbits would be 165*3/2 = 247.
I give credit to Phong Nguyen which the note says was the person who
observed this and came up with the attack.

@_date: 2003-10-02 08:31:16
@_author: Anton Stiglic 
@_subject: VeriSign tapped to secure Internet voting 
So how will these civilians get a certified public key, and how will the
key be protected?  Is there a special policy for the issuance of these kind
of certificates?

@_date: 2003-10-03 10:14:42
@_author: Anton Stiglic 
@_subject: anonymous DH & MITM  
That seems to make sense.   In anonymity providing systems often you
want one side to be anonymous, and the other to identify itself (like in
anonymous web surfing).  In this case, if you are using DH to exchange
keys, what you want is something like half-certified DH (see for example
section 2.3 of [1]), where the web server authenticates itself.  With half
certified DH, Alice (the user that is browsing in my example) can be
assured that she is really talking to Bob (web server she wanted to
communicate with), and not a MITM.
[1]

@_date: 2003-10-03 13:58:13
@_author: Anton Stiglic 
@_subject: DH with shared secret 
Sent: Friday, October 03, 2003 5:13 AM
Not exactly the same thing, but you get the same properties:  SKEME.
See section 3.3.2, Pre-shared key and PFS, of
SKEME:  A Versatile Secure Key Exchange Mechanism for internet,
Hugo Krawczyk.

@_date: 2003-10-03 15:07:12
@_author: Anton Stiglic 
@_subject: anonymous DH & MITM  
How do they create the secure channel in the first place?  We are talking
MITM that takes place during the key agreement protocol.
That's false.  Alice and Bob can follow the basic DH protocol, exactly, but
Mallory is in the middle, and what you end up with is a shared key between
Alice and Bob and Mallory.
The property you are talking about, concerning the *exactly one other party*
can read the message is related to the *key authentication*  property,
in [1] (among other places), which enables you to construct authenticated
But how do they share the initial secret?  And with true anonymity you don't
want linkability.  Pseudonymity is a different thing, with pseudonymity you

@_date: 2003-10-06 11:13:25
@_author: Anton Stiglic 
@_subject: how to defeat MITM using plain DH, Re: anonymous DH & MITM 
; "Tim Dierks" Sent: Friday, October 03, 2003 6:44 PM
You are correct on that point.
DH-based encryption/decryption will not work since Alice and Bob did NOT
share a
Bob can
That is true, but doesn't apply in practice when one party wants to remain
Most protocols have it that Alice and Bob verify that they share the same
key once, and
then let them go on with their lives.
If you do some kind of continuous verification, MITM can just disrupt the
between Alice and Bob, and Alice and Bob will then restart a DH agreement
from scratch.
You can't use previous secret since you will break anonymity (could be done
pseudonymity however, or when both parties reveal their identity...), Alice
and Bob will
have never realized that there was a MITM.

@_date: 2003-10-06 11:20:52
@_author: Anton Stiglic 
@_subject: anonymity +- credentials 
Sent: Friday, October 03, 2003 6:05 PM
The state of the art is Brands' credentials.
A technical overview of digital credentials by Stefan Brands:
A white paper from Zeroknowledge systems during the time
Stefan visited ZKS:
Description of example protocols that use this stuff in practice,
which I wrote with Ariel Glenn, Ian Goldberg and Fr?d?ric
L?gar? during the time we implemented the protocols at
ZKS :

@_date: 2003-10-06 11:27:23
@_author: Anton Stiglic 
@_subject: anonymous DH & MITM  
; "Tim Dierks" Sent: Friday, October 03, 2003 4:51 PM
Seems to be an important part, especially in an anonymous network...
My point was that you can't do that, thus making the rest of your proposal
I didn't see this as being a definition, I saw this as a suggestion for a
which I believe cannot be achieved (again, assuming both parties want to
remain anonymous).
The best you could probably do is have a system where users are anonymous
and detain anonymous credentials when they register, and have users use
credentials to demonstrate that they registered, but without having them
exactly who they are.  This way, you can probably prevent MITM who did
not register...
Sorry I forgot, here it is:
[1]  Authenticated Diffie-Hellman Key Agreement Protocols.  Simon
Blake-Wilson, Alfred Menezes.

@_date: 2003-10-06 11:43:21
@_author: Anton Stiglic 
@_subject: anonymous DH & MITM  
Sent: Friday, October 03, 2003 8:19 PM
You started by talking about anonymous communication, but ended up
suggesting a scheme for pseudonymous communication.
Anonymous != pseudonymous.
Let us be clear on that!
It is an important difference.
For example, if you take Stefan Brands digital credentials, and issue
a multi-show credential, the showings of the credential can be linked
it is not anonymous but pseudonymous in some sense (even though the
showings cannot be linked to the issuing).  An open problem would be to
have something similar (something as efficient) which allows you to
issue a single credential which can be shown multiple times in an
unlikable way (completely anonymous).
Camenisch and Lysyanskaya came up with a scheme that allows
you to demonstrate possession of a credential multiple times in a way
that these are unlikable, however their solution is far from being efficient
in practice. You are much better off using Brands' credentials and just
have multiple credentials be issued, which when shown will be unlikable.

@_date: 2003-10-07 09:13:48
@_author: Anton Stiglic 
@_subject: anonymity +- credentials 
There were however several projects that implemented and tested the credentials system.  There was CAFE, an ESPRIT project.
At Zeroknowledge there was working implementation written in Java, with a client that ran on a blackberry.
There was also the implementation at ZKS of a library in C that implemented Brands's stuff, of which I participated in.
The library implemented issuing and showing of credentials,
with a limit on the number of possible showing (if you passed
the limit, identity was revealed, thus allowing for off-line
verification of payments for example.  If you did not pass the
limit, no information about your identity was revealed).  The underlying math was modular, you could work in a subgroup of Z*p for prime p, or use Elliptic curves, or base it on the RSA problem.  We plugged in OpenSSL library to test all of these cases.
Basically we implemented the protocols described in [1], with some of the extensions mentioned in the conclusion.
The library was presented by Ulf Moller at some coding
conference which I don't recall the name of...
It was to be used in Freedom, for payment of services, but you know what happended to that projet.
Yes, most of the stuff is patented, as is Chaum's stuff.
Somebody had suggested that to build an ecash system
for example, you could start out by implementing David
Wagner's suggestion as described in Lucre [2], and then
if you sell and want extra features and flexibility get the
patents and implement Brands stuff.  Similar strategy would seem to apply for digital credentials in general.
Do you have any references on this?
[1] [2]

@_date: 2003-10-07 10:12:05
@_author: Anton Stiglic 
@_subject: NCipher Takes Hardware Security To Network Level 
to be
I believe that is typical of most software crypto modules that are FIPS 140
certified, isn't it?
It classifies the module as multi-chip standalone.
This is why you get requirements of the type that it should run on Windows
single-user mode, which I take to mean have only an admin account.  This
privilege escalation attacks (regular user to root) that are easily done.
I think this is reasonable, since you really are relying on the OS and the
PC for the
security of the module.
More scary to me is stuff like
"DSSENH does not provide persistent storage of keys.  While it is possible
store keys in the file system, this functionality is outside the scope of
this validation."
This is where Microsoft's CSPs do the dirty work, and use what is called
the Data Protection API (DPAPI) to somehow safeguard keys somewhere
in your system.

@_date: 2003-10-07 11:54:57
@_author: Anton Stiglic 
@_subject: NCipher Takes Hardware Security To Network Level 
Sent: Tuesday, October 07, 2003 11:07 AM
Windows in
the PC
run as
Did you read the security policy of Netscape Security Module?  Basically,
if you want to get the configuration that is FIPS 140 certified, you need
to install the module on a PC and add tamper resistant seals over
interfaces, junctions and fasteners of all doors and covers in the enclosure
of the PC, so that you can't open the cover without the fact being
noticeable.  I suggest adding some duct tape in strategic positions for
security :).
By reasonable I mean in the framework of having a general purpose software
cryptographic library be certified FIPS.  I'm not saying I find this secure.
When I see a software library being certified FIPS 140, I say to myself it
implement the cryptographic algorithms in a descent way, has a descent
random number generator, and stuff like that.  I don`t care much about the
physical boundary that they artificially determine.
If I want high security, I will go with hardware.  At the end of the line,
you want to protect is your secret keys, and if you don't have a tamper
hardware (that zeroizes your secrets when someone tries to poke at it)
to do that it is difficult if not impossible.

@_date: 2003-10-10 10:25:19
@_author: Anton Stiglic 
@_subject: NCipher Takes Hardware Security To Network Level 
I agree 100% with what you said.  Your 3 group classification seems
But the problem is how can people who know nothing about security evaluate
which vendor is most committed to security?
For the moment, FIPS 140 and CC type certifications seem to be the only
for these people...  Unfortunately these are still to general and don't
always give
you an accurate measurement of how dedicated to security the vendor was...
This seems to be a big open-problem in practical security!

@_date: 2003-10-14 09:48:52
@_author: Anton Stiglic 
@_subject: Internal format of RSA private keys in microsoft keystore. 
Sent: Friday, October 10, 2003 1:20 AM
If you could acquire a context, you could export the private key into a blob and then read it from that, but you can't acquire a context.
As Tom mentioned, the keys are encrypted in the container.
The FIPS 140 security policies for M$'s CSPs say that the task of protecting the keys in the system is delegated to Data Protection API (DPAPI).  There is a brief explanation in the security policies, see for example
section "Key Storage".
You might be able to find more detailed information somewhere else...
Good luck!

@_date: 2003-10-14 10:36:57
@_author: Anton Stiglic 
@_subject: NCipher Takes Hardware Security To Network Level 
I'm not totally convinced of this...  Someone with little knowledge about
cars might see the difference between a KIA and a Mercedes in one test
drive, but I would think that most affordable cars seem to drive the same
in a simple test drive (at least from my experience).  But what
a person will do is talk to his friends and get feedback, he'll learn that
some type of cars have a bad reputation and others seem to be good.
This is also done in security, take for example host security modules used
by banks, most banks make their choice  based on the vendors reputation.
Unfortunately this choice is often influenced by publicity (and the more a
certain company sells, the more money it makes, the more publicity it can
afford, the more it will sell, even if their product is not the best).
There is a marketing rule that state that there is one product that
its field in every category and gets about 80% of all sells, then there are
other products that battle for second place, all others get almost nothing.
(example for cola Coke is number 1, with Pepsi
coming second).  I don't think security products make an exception to
Another way people choose products is if they are recommended.  For
example, I buy a certain toothpaste because it is recognized by the
Canadian dental association.  This is a sort of certification.  There are
certainly other example of products in everyday life that get this type
of certification that influence people's choices.  Of course, publicity
also has some degree of influence here as well.
There are no official security associations recognized by the government
that include most of the security experts we know, rather what exists is
certain standards that the government itself decides upon and are used
(FIPS 140, CC).  This lack of an independent security association to
which any security expert can become a member of is maybe the root
of the problem?

@_date: 2003-10-23 10:25:37
@_author: Anton Stiglic 
@_subject: SSL, client certs, and MITM (was WYTM?) 
are very low (read:
I'm not certain this was the consensus.
We should look at the scenarios in which this is possible, and the tools
are available to accomplish the attack.  I would say that the attack is more
easily done inside a local network (outside the network you have to get
of the ISP or some node, and this is more for the "elite").
But statistics show that most exploits are accomplished because of employees
within a company (either because they are not aware of basic security
or because the malicious person was an employee within), so I find this
(attack from inside the network) to be plausible.
Take for an example a large corporation of 100 or more employees, there has
got to be a couple of people that do on-line purchasing from work, on-line
banking, etc...  I would say that it is possible that an employee (just
curious, or
really malicious) would want to intercept these communications....
So how difficult is it to launch an MITM attack on https?  Very simple it
seems.  My hacker friends pointed out to me two softwares, ettercap and
Cain is the newest I think, and remarkably simple to use.  It has a very
GUI and it doesn't take much hacking ability to use it.  I've been using it
recently for educational purposes and find it very easy to use, and I don't
consider myself a hacker.
Cain allows you to do MITM (in HTTPS, DNS and SSHv1) on a local
network.  It can generate certificates in real time with the same common
name as the original.  The only thing is that the certificate will probably
be signed by a trusted CA, but most users are not security aware and
will just continue despite the warning.
So given this information, I think MITM threats are real.  Are these attacks
being done in practice?  I don't know, but I don't think they would easily
be reported if they were, so you  can guess what my conclusion is...

@_date: 2003-10-23 10:46:57
@_author: Anton Stiglic 
@_subject: SSL, client certs, and MITM (was WYTM?) 
I know of some environments where this is done.  For example
to protect the connection to a corporate mail server, so that employees can read their mail from outside of work.  The caching problem is easily solved in this case by having the administrator distribute the self-signed cert to all employees and having them import it and trust it.  This costs no more than 1 man day per year.
This is near 0 cost however, and gives some weight to Perry's
I have a hard time believing that a merchant (who plans
to make $ by providing the possibility to purchase on-line)
cannot spend something like 1000$ [1] a year for an SSL certificate, and that the administrator is not capable of properly installing it within 1-2 man days.  If he can't install
it, just get a consultant to do it, you can probably get one
that does it within a day and charges no more than 1000$.
So that would make the total around 2000$ a year, let's generously round it up to 10K$ annum.
I think your 10-100 million $ annum estimate is a bit [1] this is the price I saw at Verisign
I'm sure you can get it for cheaper. This was already discussed on this list I think...

@_date: 2003-09-02 12:10:23
@_author: Anton Stiglic 
@_subject: PRNG design document? 
Sent: Friday, August 29, 2003 3:45 PM
Right.  So I don't actually have the original ANSI X9.17 document (and it is
no longer available in the ANSI X9 catalogue).  My references are
HAC section 5.3.1
and Kelsey, Schneier, Wagner and Hall's paper
In both of the above references, ANSI X9.17 PRNG is described as taking
a 64-bit seed s along with a DES E-D-E encryption key k.
The encrypted time is XORed with the seed and this result is encrypted to
obtain the output, the seed is updated by encrypting the last output XORed
with the encrypted time.
So there is possibility of re-keying (the key that is used for the
and re-seeding (explicitly, not relying on the self-re-seeding...).
It is important to chose both a random seed and random key, and FIPS 140
has no provision for this.

@_date: 2003-09-02 15:51:47
@_author: Anton Stiglic 
@_subject: PRNG design document? 
Here are two references that might also be helpful:
These are reports on the analysis of two RNGs, I found them well written.

@_date: 2003-09-05 13:32:21
@_author: Anton Stiglic 
@_subject: OpenSSL *source* to get FIPS 140-2 Level 1 certification 
Really exiting news.  If I'm not mistaken, this would be the first free,
crypto library that has FIPS 140 module certification!  Other free
libraries have algorithms that have been FIPS 140 certified, but the whole
hasn't been certified (exemple Cryptlib and Crypto++).
And OpenSSL crypto module runs on all kinds of platforms.  Really nice!
----- Original Message ----- Sent: Friday, September 05, 2003 10:50 AM

@_date: 2003-09-05 16:15:22
@_author: Anton Stiglic 
@_subject: OpenSSL *source* to get FIPS 140-2 Level 1 certification 
You are correct, I just saw Crypto++ in the list of FIPS 140 validated It is the latest entry, added today.
Congratulations to Wei Dai!
I was not aware of NSS before, their might be others as well which I am not aware of then.
OpenSSL`s *source code* being evaluated remains exiting.
Thanks for the information Joshua and Rich!

@_date: 2003-09-08 10:17:15
@_author: Anton Stiglic 
@_subject: cryptographic ergodic sequence generators? 
This is essentially because if your output sequence of n-bit blocks were
really random,
you would expect to see a collision between two n-bit blocks after seeing
about 2^(n/2)
block outputs (birthday paradox), but using a block cipher with a counter
gives you no
collision before 2^n block outputs.  This is indeed why in the Yarrow design
suggest to re-key after 2^(n/3) block outputs.

@_date: 2003-09-08 11:43:38
@_author: Anton Stiglic 
@_subject: Code breakers crack GSM cellphone encryption 
I think this is different however.  The recent attack focused on the A5/3
encryption algorithm, while the work of Lucky, Briceno, Goldberg, Wagner,
Biryukov, Shamir (and others?) was on A5/1 and A5/2 (and other crypto
algorithms of GSM, such as COMP128, ...).

@_date: 2003-09-08 14:24:23
@_author: Anton Stiglic 
@_subject: Code breakers crack GSM cellphone encryption 
Sent: Monday, September 08, 2003 1:39 PM
That`s what I meant to say but did not use the right words to say.
The attack does however seem novel.
I haven`t seen the paper on the web yet (all I know is that it was
presented at Crypto 03 which I did not attend), I`m anxious to get my hands
on it.

@_date: 2003-09-24 13:15:22
@_author: Anton Stiglic 
@_subject: End of the line for Ireland's dotcom star 
And Thawte got bought by Verisign, so no more competition...
Interestingly, last time I checked, it was cheaper to buy from Thawte than it was from Verisign directly.

@_date: 2004-04-05 14:00:23
@_author: Anton Stiglic 
@_subject: [Mac_crypto] Apple should use SHA! (or stronger) to authenticate  software releases 
The attacks by Dobbertin on MD5 only allow to find collisions in the
compression function, not the whole MD5 hash.
But it is a sign that something might be fishy about MD5.
MD5 output is 128 bits.  There are two types of collision finding
attacks that can be applied.  In the first you are given a hash value
y = H(x), for some x, and try to find a different input x' that hashes
to the same output:  H(x) = H(x') = y.  This relates to 2nd-preimage
resistance.  This can be done on MD5 in 2^128 work factor.
The other attack is to find to arbitrary inputs x, x' such that
H(x) = H(x').  This relates to collision resistance.  This can be done
with good probability in 2^64 work factor.  Now, the problem
of having a malicious source code hash to the same value as good/valid
source code seems to be related more to the former, that is you have
some code that is checked-in, that gives some hash value Y, and you
want to find a different code (malicious one) that hashes to the same value.
You might be able to play with the valid code as well, giving you more
flexibility for the search of a collision, but you can't play to much
having this noticed by other developers.
I think that there are many other problems that are more of concern.  For
example hacking a web site (or mirror site) that contains code for download,
and changing the code along with the hash value of the code, or preventing
a developer from inserting some kind of trap door or Trojan.
But if you are given the choice between using MD5 and SHA1, I'd prefer
SHA1, but I wouldn't be concerned with someone using MD5 isntead of SHA1
for the time being. In other words, if I were to do a risk analysis, I would
the use of MD5 instead of SHA1 as one of the major risks.

@_date: 2004-04-06 09:40:52
@_author: Anton Stiglic 
@_subject: [Mac_crypto] Apple should use SHA! (or stronger) to authenticate  software releases 
I wanted to write "I would *not* identify the use of MD5 instead of SHA1 as
of the major risks".  In other words, using MD5 instead of SHA1 would be low
compared to the other threats that exist.
Sorry, the mistake changes to whole sense of the phrase.

@_date: 2004-04-29 10:51:01
@_author: Anton Stiglic 
@_subject: Is there a Brands certificate reference implementation? 
Stefan Brands started his own company,
There isn't much on the web site yet, but if you click on the image you get
the info
email address.
The code that was developed for Brands credentials at ZKS was never
released.  There was also code written during the ESPRIT project called
A description of protocols for Brands credentials can be found here
A more elaborate reference is the technical paper that can be found here

@_date: 2004-08-09 23:20:37
@_author: Anton Stiglic 
@_subject: Microsoft .NET PRNG (fwd) 
There is some detail in the FIPS 140 security policy of Microsoft's
cryptographic provider, for Windows XP and Windows 2000.  See for example
where they say the RNG is based on FIPS 186 RNG using SHS.  The seed is
based on the collection of allot of data, enumerated in the security policy.
I would guess that what is written is true, less NIST would look very bad if
someone reversed engineered the code and showed that what they certified was
So based on that it would seem that the PRNG in recent Microsoft
cryptographic providers is o.k.

@_date: 2004-08-11 23:44:23
@_author: Anton Stiglic 
@_subject: Microsoft .NET PRNG (fwd) 
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Ed Gerck
Sent: 10 ao?t 2004 13:42
Yes that's true.  The security policy explains that the safeguarding of
private keys is done outside the crypto boundary.  (as someone mentioned to
me in personal email you need to have a look at the fine print of such
accreditations, this is an example of a fine print).
Note however that the OS uses the crypto provider to encrypt the private key
using a secret that is generated based on (or protected by a key generated
based on, don't remember off the top of my head) the user's password.
The strength of the system is based on the user's Windows password, which I
think is reasonable (anyone who can login as the user can use his private
keys, stored in his container, anyways)...

@_date: 2004-12-02 21:33:36
@_author: Anton Stiglic 
@_subject: SSL/TLS passive sniffing 
I found allot of people mistakenly use the term certificate to mean
something like a pkcs12 file containing public key certificate and private
key.  Maybe if comes from crypto software sales people that oversimplify or
don't really understand the technology.  I don't know, but it's a rant I
have.  I guess the threat would be something like an adversary getting access to a
web server, getting a hold of the private key (which in most cases is just
stored in a file, allot of servers need to be bootable without intervention
as well so there is a password somewhere in the clear that allows one to
unlock the private key), and then using it from a distance, say on a router
near the server where the adversary can sniff the connections.  A malicious
ISP admin could pull off something like that, law authority that wants to
read your messages, etc.
Is that a threat worth mentioning?  Well, it might be.  In any case,
forward-secrecy is what can protect us here.  Half-certified (or fully
certified) ephemeral Diffie-Hellman provides us with that property.
Of course, if someone could get the private signature key, he could then do
a man-in-the-middle attack and decrypt all messages as well.  It wouldn't
really be that harder to pull off.

@_date: 2004-12-23 07:36:06
@_author: Anton Stiglic 
@_subject: The Pointlessness of the MD5 "attacks" 
I disagree; I think it might be possible with the current cryptanalysis on
MD5.  The collisions that can be currently produced only flip a couple of
bits, and you can add what you want before and after the 1024-bit block.
Imagine some code that reads the image (or whatever bit-string) as a textual
string, in one case it doesn't read the whole bit-array because there is a
null string-terminating character, in the other case (collision) the
character is not present and causes a buffer overflow.  I think something
like that can be done today.

@_date: 2004-01-05 15:04:56
@_author: Anton Stiglic 
@_subject: CIA - the cryptographer's intelligent aid? 
The thing about CIA is that it is commonly used in security (not
courses to mean Confidentiality, Integrity (of systems) and Availability
of Authentication).  Availability of systems, services and information.
For crypto I always talked about CAIN or PAIN (like in no PAIN
no gain, or cryptography is allot of PAIN).  -- note, I also prefer the word
Confidentiality over Privacy, the latter being to high level and I usually
it to mean the hiding of who is communicating with who (anonymity
When introducing digital signatures I always state that they provide
(as do MACs, which I introduce beforehand) but also the possibility of
non-repudiation.  And then I go on stating that it is very hard, if not
to in fact implement non-repudiation.

@_date: 2004-01-07 12:06:59
@_author: Anton Stiglic 
@_subject: [Fwd: Re: Non-repudiation (was RE: The PAIN mnemonic)] 
Sent: Wednesday, January 07, 2004 7:14 AM
I don't think the word "authentication" has the same problem as
but you do need to be careful how you define it.
So here we are talking about entity authentication (as opposed to data
the latter really has a unambiguous definition, at least I hope it does!).
The way you should define entity authentication
is by stating that it is a process of verifying that an entity possesses the
credentials associated to a user that entity claims to be.  This entity
might be the rightful
user, or it might be someone who stole the credentials from the rightful
user.   If someone
stole my ATM card and my PIN, he/she can successfully authenticate
him/herself to an
ATM and withdraw money.  The word "authenticate" is appropriate in this last
But I see that most definitions that have been collected here:
are not careful about this.
The thing about non-repudiation is that it is something that even most laws
do not
permit.  See for example:
Non-repudiation applied to digital signatures implies that the definition
states that
only one person possibly had possession of the private signing key and was
about the fact that it was used to sign something.
In most jurisdictions a person has the right to repudiate a signature
or electronic), and thus non-repudiation does not work.  People have the
right to
repudiate signatures since it might be the result of a forgery, fraud, the
signer might have
been drunk or something at the time of signing or forced to sign (like with
a gun to his
head).    Repudiation is possible but non-repudiation is not.
I know some people who use the term "accountability" instead of
to express the property needed in certain systems (commercial
infrastructures where
users login and need to be accountable for their acts).  This seems like a
better term
to be used in certain contexts, but I'm still thinking about it...

@_date: 2004-07-05 09:23:47
@_author: Anton Stiglic 
@_subject: authentication and authorization (was: Question on the state of the security industry) 
[mailto:owner-cryptography at metzdowd.com] On Behalf Of John Denker
Sent: 1 juillet 2004 14:27
Identity has many meanings.   In a typical dictionary you will find several
definitions for the word identity.  When we are talking about information
systems, we usually talk about a digital identity, which has other meanings
as well. If you are in the field of psychology, philosophy, or computer
science, identity won't mean the same thing. One definition that relates to
computer science that I like is the following:
"the individual characteristics by which a thing or person is recognized or
A digital identity is usually composed of a set of identifiers (e.g. Unix
ID, email address, X.500 DN, etc.) and other information associated to an
entity (an entity can be an individual, computer machine, service, etc.).  "Other information" may include usage profiles, employee profiles, security
profiles, cryptographic keys, passwords, etc.
Identity can be stolen in the sense that this information can be copied,
revealed to someone, and that someone can use it in order to identify and
authenticate himself to a system and get authorization to access resources
he wouldn't normally be allowed to.
The following document has a nice diagram on the first page of appendix A:
I came up with a similar diagram for a presentation I recently gave, but
instead of talking about primary and secondary identifying documents I
mention primary and secondary identifying information in general, and I also
have an "identifiers" circle situated beside the bigger circle, containing
identifiers that belong to an entity but are not linkable to the entity
(talking about nyms and pseudonyms).  Recall that there are basically 3
types of authentication:  individual authentication (such as via biometrics,
where you use primary identifying information to authenticate someone),
identity authentication (where the identity may or may not be linkable to an
individual), and attribute authentication (where you need reveal nothing
more than the possession of a certain attribute, such as can be done with
Stefan Brands digital credentials).

@_date: 2004-07-06 11:20:39
@_author: Anton Stiglic 
@_subject: authentication and authorization 
Well, there is nt established technical definition for "digital identity",
but most definitions seem to focus to what I defined it as.
The term "digital identity" is not intended to help you solve the problem.
In a digital identity there are parts that an individual wants to keep
private, other parts can be public (others should be divulged to only
certain individuals, possibly via a zero-knowledge proof that will convince
the verifier, without giving him enough information to be able to prove the
property to someone else).  You can refer to the different parts of a
digital identity using different terms if you want, but the term "digital
identity" usually includes all of those parts.  Relating to the real world,
you might have a fetish for high-healed pink leather boots, which is part of
your identity (something that characterizes you), but not want others to
know about that.  But its still part of your identity, just as your SSN
number is.
You are mixing up two problems, that of defining digital identity, and that
of preventing unauthorized individuals to access resources that they are not
supposed to (via identity theft for example), as well as privacy.
You are talking about the problem of non-repudiation here...
I agree with that last part.  It relates to the whole thing about attribute,
vs identity vs individual authentication that I mentioned.  I favour
attribute authentication in most cases.  And with stuff like Digital
Credentials you can also have accountability even with attribute
authentication (for example if forced by law).
Again, this relates exactly to my discussion about attribute, identity and
individual authentication.  Things like Digital Credentials is what is going
to help you out, not re-defining the term "digital identity".

@_date: 2004-07-08 09:07:40
@_author: Anton Stiglic 
@_subject: authentication and authorization (was: Question on the  state of the security industry) 
Agreed.  This is where federated identity management becomes a tricky
problem to solve.  It is important to get something like the Liberty
Alliance right.
A solution that I like can be found here (there is also a ppt presentation
that can be found on the site):
Yes, theory is far more advanced than what is used in practice.
With Zeroknowledge proofs and attribute authentication, based on secrets stored on smart cards held by the proper owners, and possibility
to delegate part of the computation to a server (so clients can authenticate on low powered devices), without revealing information about the secret, etc...
I agree that what you call "static data" authentication paradigm
is the cause of many problems, including identity theft.  It is one reason why Identity Management is a hot topic these days; businesses
are loosing control of all these "static data" associated to the various
systems they have, and when an employee leaves a company he often has an
active account on some system even months after his departure.
This is the de-provisioning problem.
Not to sure about the wording however, if you take a zeroknowledge
Proof to authenticate possession of an attribute, prover will hold
some static data (some sort of secret), the only difference is that
the verifier doesn't need to know the secret, and in fact you can't
learn anything from looking at the communication link when the proof
is executed.  You can't learn anything either by modifying the protocol
from the verifier's point (malicious verifier).  But if you can steal
the secret that the prover possesses, than you can impersonate her.
--Anton

@_date: 2004-07-08 15:31:47
@_author: Anton Stiglic 
@_subject: identification + Re: authentication and authorization 
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Ed Gerck
Sent: 7 juillet 2004 14:46
Yes and no.  The problem is that most authentication and authorisation
schemes today are actually identification and authentication and
authorisation schemes.  Even when you read CISSP study guides, they always
describe it in 3 steps, identification, authentication and authorisation.
The thing is that we can do without identification.  Identification is not
necessary, even if you want accountability.  In
Identification-authentication-authorisation schemes, identification is the
process of pin-pointing an exact individual from a set of individuals (e.g.
SSN allows you to define a unique united-states citizen), authentication is
the process of verifying that the individual claiming to be who he
identified himself as, is really that individual.   But most systems don't
really need identification, all they need is a proof that the individual
possesses a certain attribute.  It is possible to do authentication and
authorisation, without doing the identification part!   For example, it is
possible to prove that you are a united-states citizen that has a valid SSN
number, without actually giving out information about SSN.
Why is identity theft a bad thing?  Usually, you don't want your identity to
be stolen because you could be accused of something due to accountability
that is associated with your identity.  The problem is not that someone can
authenticate himself to a system he is not suppose to have access to, the
problem is that a thief can identify himself as you and authenticate himself
as you, and than do bad things (like transfer your money).
The problem is not really authentication theft, its identity theft, or if
you want to put it even more precisely, it's "identity theft and
authenticating as the individual to whom the identity belongs to".  But the
latte doesn't make for a good buz-word :)
Here is another way of seeing it.  Consider a system where you need to
authenticate yourself as a citizen, of some region, that is 18 years of age
or older, in order to participate in some gambling thing say.  One way to
implement the authentication and authorisation in the system is to have each
individual identify themselves, and then authenticate themselves.  If the
individual is part of a set of individuals that are known to be over 18,
then the individual is given access.  Another way to implement it is to have
each individual prove that they are over 18 without identifying themselves,
using Stefan Brands digital credentials say.  If the authentication is
successful, the un-identified individual is given access.  In the latter
case, you don't really care about authentication theft unless there is some
sort of accountability (with Stefan's digital credentials, you can embed the
identity in the tokens that are presented for authentication, the identity
can only be revealed under certain circumstances, for example excessive use
or if require by a law, it could be revealed by a third party).
I do agree that stronger authentication does help, preferably authentication
based on zero-knowledge protocols, since these reveal less information about
the individual's identity that can be used to impersonate the individual.
Once we
understand this, a solution, thus, to what is called  "identity theft"
is to improve the *authentication mechanisms*, for example by using
two-factor authentication. Which has nothing to do with identification,
impersonation, or even the security of identification data.
In further clarifying the issue, it seems that what we need first is
a non-circular definition for identity. And, of course, we need a
definition that can be applied on the Internet.  Another important
goal is to permit a safe automatic processing of identification,
authentication and authorization [1].
Let me share with you my conclusion on this, in revisiting the
concept of identification some time ago. I found it useful to ask
the meta question -- what is identification, that we can identify it?
In short, a useful definition of identification should also work
reflexively and self-consistently [2].
In this context, what is "to identify"? I think that "to identify"
is to look for connections. Thus, in identification we should look
for logical and/or natural connections. For example:
- between a fingerprint and the person that has it,
- between a name and the person that answers by that name,
- between an Internet host and a URL that connects to it,
- between an idea and the way we can represent it in words,
- conversely, between words and the ideas they represent,
- etc.
Do you, the reader, agree?
If you agree you have just identified. If you do not agree, likewise
you have identified! The essence of identification is thus to find
connections -- where absence of connections also counts.
Identification can thus be understood not only in the sense of an
"identity" connection, but in the wider sense of "any" connection.
Which one to use is just a matter of protocol expression, need, cost
and (very importantly) privacy concerns.
The word "coherence" is useful here, meaning any natural or logical
connection. To identify is to look for coherence. Coherence with and
between a photo, a SSN, an email address, a public-key and other
attributes: *Identification is a measure of coherence*.
The same ideas can be applied to define "authentication" and
"authorization" in a self-consistent way, without overlapping with
each other.
Ed Gerck
[1] The effort should also aim to safely automate the process of reliance
by a relying-party. This requires path processing and any algorithm to
eliminate any violations of those policies (i.e., vulnerabilities) that
might be hard to recognize or difficult to foresee, which would
interfere with the goal of specifying a wholly automated process of
handling identification, authentication and authorization.
[2] This answer should be useful to the engineering development of all
Internet protocols, to all human communication modes, to all
information transfer models and anywhere one needs to reach beyond
one's own point in space and time.

@_date: 2004-07-13 15:36:16
@_author: Anton Stiglic 
@_subject: EZ Pass and the fast lane .... 
My 2 cents on the subject...
The automatic toll fee system I am most familiar with is that of Kapsh (used
to be Combitech).  They have implemented automatic toll fee collection in
many countries around the world (in Europe, Asia, Australia, south
I think they usually implement a combination of 1) a system that queries a device in the car, which identifies the car
owner, and then charges the owner in a central database (incrementing the
amount that is due)
2) license plate scanning for accountability purposes.
When you do crypto to authenticate the communication between the toll device
and the device in the car, you need to do fast crypto.  Where I work, we
used to be in the hardware arena and had a project designing an HSM for a
toll fee system.  The requirements where that it had to be based on DES/3DES
and you had to be able to do DES/3DES operations on single, small length
messages, rapidly.  This last part is a bit tricky, it's not the same as
getting good average speed on longer messages, you need to take into account
the communication between the PC and the HSM which accounts for allot of
overhead on a single, small length message;  IO memory mapping is a good way
to go, also preparing keys in RAM can help just a bit, but for us IO memory
mapping gave the most significant speed-up.  There a paper from IBM on this
subject (can't find the reference now), with the same conclusions.
License plate scanners seem to be effective these days.  I related story to
the toll fee license plate scanning, Toronto police are using a license
plate recognition device to scan parked cars in order to attempt to identify
stolen cars:
They were able to recover 153 stolen cars in a 3-month test period.
They say they can scan 1000 license plates an hour, but this includes the
time to send the information to a central point and do a search in a

@_date: 2004-07-15 16:23:12
@_author: Anton Stiglic 
@_subject: Humorous anti-SSL PR 
The article says
"The weaknesses of SSL implementations have been well known amongst security
professionals, but their argument has been that SSL is the best tool
currently on offer. The fact that it can be spoofed and is open to man in
the middle attacks is played down."
O.k., so if there is a vulnerability in a particular implementation there
might be a possible MITM attack.  Also possible to do MITM if user doesn't
do proper verification.  But I wouldn't say that SSL implementations in
general are suspect to MITM attacks.
Later in the article it is written:
"What we can be certain of is that it is not possible to have a
man-in-the-middle attack with FormsAssurity - encryption ensures that the
form has really come from the claimed web site, the form has not been
altered, and the only person that can read the information filled in on the
form is the authorized site."
O.k., so how do they achieve such assurances?
Eric's comment about condoms being effective is right, so bad analogy as

@_date: 2004-07-16 13:41:05
@_author: Anton Stiglic 
@_subject: Verifying Anonymity 
The lack of understanding of how a solution works applies to most security
products and in general to all computer products.  Most people don't have a
clue how an SSL encrypted session really protects your credit card number in
transit, but allot of people are starting to realize that they should use it
(they understand to some extent the problem SSL attempts to solve).
With anonymity systems, I don't think understanding how a solution works is
a problem to its wide-spread use, the problem is more that of understanding
the *problem the solution attempts to solve*.  People still don't understand
the consequences of privacy invasion on the Internet (the problem).  Once
they do, they will be willing to pay for a solution from any trusted
company, without needing to understand how the solution actually works.

@_date: 2004-07-16 13:51:48
@_author: Anton Stiglic 
@_subject: New Attack on Secure Browsing 
What I get is a bad certificate, and this is due to the fact that the
certificate is issued to store.pgp.com and not Interestingly (maybe?), when you go and browse on their on-line store, and
check something out to buy, the session is secured but with another
certificate, one issued to secure.pgpstore.com.

@_date: 2004-07-19 11:40:05
@_author: Anton Stiglic 
@_subject: dual-use digital signature vulnerability 
About using a signature key to only sign contents presented in a meaningful
way that the user supposedly read, and not random challenges:
The X.509 PoP (proof-of-possession) doesn't help things out, since a public
key certificate is given to a user by the CA only after the user has
demonstrated to the CA possession of the corresponding private key by
signing a challenge.  I suspect most implementation use a random challenge.
For things to be clean, the challenge would need to be a content that is
readable, and that is clearly only used for proving possession of the
private key in order to obtain the corresponding public key certificate.
X.509 PoP gets even more twisted when you want to certify encryption keys (I
don't know what ietf-pkix finally decided upon for this..., best solution
seems to be to encrypt the public key certificate and send that to the user,
so the private key is only ever used to decrypt messages...)

@_date: 2004-06-21 11:43:58
@_author: Anton Stiglic 
@_subject: recommendations/evaluations of free / low-cost crypto libraries 
A list can be found here
There are several things that you might want to consider, other than the
language in which the library was written of course.
You might want to consider the cryptographic algorithms that are supported,
and support for standards such as various PKCS standards.  For example,
although JCE is a standard framework, not all JCE providers implement the
same functionality.  Some may allow you to create a PKCS object or not,
some will only let you read one.  And creating a PKCS object can be done
in various ways, you might need to provide all of the keys that go in the
PKCS object at once, or you can add them incrementally (we actually
changed a JCE provider because of this point).  Some allow you to talk to a
cryptographic hardware via PKCS and some not.
You also might be interested in software performance, and if operations can
be accelerated by hardware you might have in hand.
You might also be interested in FIPS accreditation.  For example, Crypto++
and the NSS library are FIPS 140 accredited.  OpenSSL is in the process of
being certified (there was a discussion about that in this list around
September 2003).
Architecture and OS platform compatibility is another important issue.
You might also be interested in the size of the compiled executable once it
is statically linked with the library (some libraries do much better than
others on this point).

@_date: 2004-06-30 11:54:00
@_author: Anton Stiglic 
@_subject: recommendations/evaluations of free / low-cost crypto libraries 
[mailto:owner->cryptography at metzdowd.com] On Behalf Of Peter Gutmann
Indeed.  Adam started that list in 1996, but I don't think he put allot of
time updating it in recent years.  Still, I think it's a good list for
someone who is starting to look for crypto libraries.
It would be nice gift to the community if someone came up with a similar,
updated list.

@_date: 2004-05-26 09:30:46
@_author: Anton Stiglic 
@_subject: The future of security  
Sent: Tuesday, May 11, 2004 11:36 AM
Something like hashcash / client puzzles / Penny Black define a set
of authorized email (emails that come with a proof-of-work), and then
provide a cryptographic solution.   This is not a full-proof solution (as
described in the paper Proof-of-Work Proves Not to Work), but a good partial solution that is probably best used in combination
with other techniques such as white-lists, Bayesian spam filters , etc...
I think cryptography techniques can provide a partial solution to spam.

@_date: 2004-05-26 10:06:34
@_author: Anton Stiglic 
@_subject: SSL accel cards 
I successfully used a Broadcom PCI card on a Linux (don't remember
what Linux and kernel version, this was close to 2 years ago).
If I remember correctly it was the BCM5820 processor I used
(the product sheet mentions support for Linux, Win98, Win2000,
FreeBSD, VxWorks, Solaris).
I was able to use it on a Linux and on a Windows (where I offloaded
modexp operation from MSCAPI crypto provider).
The Linux drivers where available from Broadcom upon request, there was
also a crypto library that called the card via the drivers, but at the time
I looked at it the code wasn't very stable (e.g. I had to debug the RSA
key generation and send patches since it did not work at all, later versions
had the key generation part working properly).
The library might be stable by now.
I also made the Broadcom chip work with OpenCryptoki on a Linux,
I submitted the code for supporting Broadcom in OpenCryptoki.
No, but they might find out how poorly written they are??? Don't know the

@_date: 2004-10-13 22:10:06
@_author: Anton Stiglic 
@_subject: New IBM Thinkpad includes biometrics 
I wonder how well it can counter the attacks discussed by researchers in the
last few years.  Like reactivating a fingerprint authentication by breathing
on the sensor's surface containing residue fat traces of the finger, or
placing a bag of water.  Or the jelly finger trick.
The biometric authentication might very well make the laptop less secure
than password-based authentication.

@_date: 2004-09-07 23:06:39
@_author: Anton Stiglic 
@_subject: Maths holy grail could bring disaster for internet 
Looks like they are saying that if one can disprove the Riemann hypothesis,
then one could break (presumably) public key crypto, (presumably) by
factoring or computing DL.  But I am not aware of any factoring or DL
algorithm that can be drastically sped up if Riemann hypothesis is proven to
be false?
Here the author quotes the mathematician:
 "The whole of e-commerce depends on prime numbers. I have described the
primes as atoms: what mathematicians are missing is a kind of mathematical
prime spectrometer. Chemists have a machine that, if you give it a
molecule, will tell you the atoms that it is built from. Mathematicians
haven't invented a mathematical version of this. That is what we are after.
If the Riemann hypothesis is true, it won't produce a prime number
spectrometer. But the proof should give us more understanding of how the
primes work, and therefore the proof might be translated into something
                                     *****
that might produce this prime spectrometer. If it does, it will bring the
     *****
whole of e-commerce to its knees, overnight. So there are very big
This wording, with the word *might*, is more accurate, and not at all
equivalent to the assertion the author makes at the beginning.
Another bad article.

@_date: 2004-09-22 21:30:16
@_author: Anton Stiglic 
@_subject: Time for new hash standard 
I believe hash127 acts like an almost universal family of hash functions,
thus the word hash in it makes sense even though it is a MAC (but I might
not be recalling properly).
About MACs being easier to build, I agree it seems to be easier because of
the secret key involved.
If you don't like SHA1, I would suggest SHA-225/256/384/512, or something
based on a different design philosophy such as Tiger.  Another interesting
alternative is hash functions based on a block cipher such as AES.
-----Original Message-----
[mailto:owner-cryptography at metzdowd.com] On Behalf Of "Hal Finney"
Sent: 20 septembre 2004 15:44
Russell Nelson suggested:
I believe this is a MAC, despite the name.  It seems to be easier to
create secure MACs than secure hash functions, perhaps because there are
no secrets in a hash, while in a MAC there is a secret key that makes
the attacker's job harder.

@_date: 2005-08-04 07:53:58
@_author: astiglic@okiok.com 
@_subject: Standardization and renewability 
This is kind of the ISO approach.  For example, you standardize some
cryptographic protocol but give several choices of the protocol that
acheive the same goal.  If you make use of a cryptographic primitive in
the protocol (such as hash function, or symmetric algorithm, or public key
algorithm, etc.) you simply refer to another standard that defines several
So, for example, if MD5 breaks, you only need to modify the hash algorithm
standard to take it out, and in the mean time everybody can swith to
another hash algorithm already defined in the hash standard.
Suggesting key rotation is also useful, but often hard to implement in
practice.  You also want to allow for various key sizes, and various
security parameter size in general (nonce, IV, MAC size, etc.). Suggesting a minimum that is considered secure today.  Ex. use of 1024 bit
RSA keys, up to 4096 bits, something like that.

@_date: 2005-08-10 11:45:27
@_author: astiglic@okiok.com 
@_subject: solving the wrong problem 
'chindogu' seems almost appropriate but maybe not exact

@_date: 2005-08-26 12:06:55
@_author: astiglic@okiok.com 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
Some info on primality testing.
Miller-Rabin probabilistic primality tests work really well when you are
searching for a prime and picking candidates from a uniform random
distribution, also works well if you pick an initial candidate from a
uniform random distribution and then increment on that initial candidate,
until you find a probable prime. See the references
Damgard, Landrock and Pomerance (1993), Average case error estimates for
the strong probable prime test, Mathematics of Computation 61 (203).
Brandt, Damgard (1993) On generation of probable primes by incremental
search.  CRYPTO 92.
Summaries of the results can be found in the Handbook of applied
On a side note about Miller-Rabin, there is something allot of people get
wrong.  The basic results we know is that one iteration of the
Miller-Rabin test will err in declaring a composite integer to be prime
with probability less than 1/4, while t iterations will err with
probability (1/4)^t.
You can find a proof for this is textbooks such as that of Koblitz.
If X stands for "n is composite", and Y_t stands for RepeatRabin(n, t)
returned "prime", where RepeatRabin is the algorithm that executes t
iterations of Miller-Rabin's test and outputs "composite as soon as an
iteration fails, "prime" if all iterations passed.
Now, given the basic theorem mentioned above, all we can say is that
Prob[Y_t | X ] <= (1/4)^t, in English ?if n is composite, then
RepeatRabin(n,t) will return ?prime? with probability less than or equal
to (1/4)^t?.  Much more interesting is to figure out Prob[X | Y_t], that
is ?if RepeatRabin(n,t) returns ?prime?, what is the probability that X is
actually composite and we got screwed??.  It just happens to be that
Prob[X | Y_t] <= (1/4)^t when n is chosen from a uniform random
distribution, because in such cases prob[Y_1 | X] is actually much smaller
than ?.
Beauchemin, Brassard, Cr?peau, Goutier, Pomerance. The generation of
random numbers that are probably prime?, Journal of Cryptology, 1, 53-64.
Ok, back to the main topic.
So Miller-Rabin is good for testing random candidates, but it is easy to
maliciously construct an n that passes several rounds of Miller-Rabin.  The Miller-Rabin probabilistic primality test actually comes from a true
primality test, called Miller test, which is polynomial (but not efficient
in practice) and works assuming the Extended Riemann Hypothesis.
On proposed algorithm is to use two iterations of the Miller-Rabin test
followed by a single iteration of the Lucas probable prime test.  The
advantage of this test is that there is yet no known composite integer
that passes even a single Miller-Rabin test to the base 2 followed by a
single Lucas probable prime test.  There is also an open challenge
regarding this (something like 640$ coming directly from the pockets of
Pomerance and al.).  See
Pomerance (1984) Are There Counter-Examples to the Baillie-PSW Primality
This is the algorithm mentioned by Hal.  No, there is no proof that you
can?t find a counter-example, but Pomerance hasn?t found one yet, and
that?s good enough for me for the time being!
If you want primality certificates, and not just a randomized test that
has some probability of given you a wrong answer, look at Elliptic curve
primality test and Maurer?s algorithm.  These are both described in the
ISO 18032 ?prime number generation? standard and ANSI X9.80 (this just
goes to show that these are not purely academic creations, but stuff you
can use in practice).
Elliptic Curves for Primality Proving (ECPP) is used like Miller-Rabin in
order to generate a prime:  chose random candidates until one passes the
test; but in addition it produces a certificate that allows you to verify
primality using a different algorithm (much less complicated than the one
used to generate a prime, so this allows you to validate the correctness
of the implementation of the prime generating algorithm as well).
Atkin, Morain (1993).  Elliptic curves and primality proving.  Mathematics
of Computations.
Maurer?s method doesn?t pick and test random candidates, rather it
constructs, in a special way, an integer that is guaranteed to be prime.
Don?t be concerned about secrecy of prime generated with Maurer?s method,
the method generates primes that are almost uniformly distributed over the
set of all numbers (this is different from another algorithm called
Shawe-Taylor, which is similar in functioning but only reaches 10% of all
primes of a specified set).
Maurer?s method is much easier to code than ECPP. See
Maurer (1995) Fast generation of prime numbers and secure public-key
cryptographic parameters.  Journal of Cryptology, 8(3), 123-155
Maurer.  Fast generation of secure RSA-moduli with almost maximal
diversity.  EUROCRYPT'89.
So, in conclusion, there is allot of good stuff to choose from!

@_date: 2005-08-29 11:37:38
@_author: astiglic@okiok.com 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
What I wanted to say is the method "generates primes that are close to
uniformly distributed over the set of primes in the specified interval",
as stated in Maurer's papers.  In other words, the distribution of primes
created is similar that that when using the method of picking uniformly at
random candidates in an interval and passing the Miller-Rabin test
(except, of cours, there is no probability of error (picking a
pseudo-prime)), which most crypto libraries do.

@_date: 2005-08-29 11:57:47
@_author: astiglic@okiok.com 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
Carmichael numbers with Fermat's test is a worst case example.  A
Carmichael number will pass Fermat's test for all bases.  So if you give
some one a Carmichael number (which is not a prime), and the person
verifying the primality uses Fermat's test, you are sure to fool him.
For Miller-Rabin, as you mentioned, it can be proven that in worst case,
for a particular n,  there is only 1/4 of bases for which the test will
return "prime" when in fact the candidate is composite.  But what I said
is that you can "maliciously construct an n that passes several rounds of
Miller-Rabin".   In worst case, you can construct a prime that will pass
several rounds of Miller-Rabin, for specific bases.
Of course, if the person testing n uses random bases, the more tests the
person does the greater the chance the person will catch the fact that n
is in fact composite.  But if you knew, in advance, the sequence of
candidates that will be used, than it's a different game.
The Miller true primality test I referred to in my initial post will
always work assuming the Extended Riemann Hypothesis (ERH).  In such a
test, you start with base 2 and test, as long as the test passes increment
by one, up to Poly(size(n)).  I would have no problem believe ERH and
using such an algorithm.  In the worst case, if my code happened to
generate a pseudo-prime, than I would have disproved the ERH hypothesis
and could make allot of money out of that
If at the end all iterations passed, n is surely prime if ERH is true. The value of Poly(size(n)) that would need to be used for large n will be
much smaller than 1/4*n, but (the big ick, considering all of the
constants in the actual polynomial that needs to be considered) is still a
very large number.
In any case, go with Maurer's test, it really rocks and has been

@_date: 2005-08-29 12:08:48
@_author: astiglic@okiok.com 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
That's a very old algorithm.  It was an intersting result at the time
(1986) because it is a primality proving algorithm that is not based on
any hypothesis and runs in *expected polynomial time* on almost all inputs
(note the *expected*).  It uses elliptic curves.  However, the algorithm
is not efficient in practice, and we have better theoretical results today
(Agrawal-Kayal-Saxena that is a true primality test that works in
polynomial time on all inputs.  Note that such a deterministic
polynomial-time primality testing algorithm implicitly provides a trivial
certificate of primality, which simply consists of the prime number
Goldwasser and Kilian's result was extended by ADleman and Huang, later
Atkin developed a simialr algorithm known as the Elliptic Curves for
Primality Proving (ECPP), which I also referred to in my initial post.
Morain worked on implementing Atkin's algorithm.  This is efficient. Morain has a web page with lot's of info and nice working code:
The advantage of Maurer's construction, however, is that it is much
simpler to code.

@_date: 2005-12-01 22:56:04
@_author: Anton Stiglic 
@_subject: Encryption using password-derived keys 
It can be useful to derive a key encryption key from the password, and not
use the key derived from the password to directly encrypt data you want to
protect, when the resulting ciphertext can be found in different places
where your encrypted key won't necessarly also be found.  For example, to
encrypt files, when the encrypted files found themselves on a backup disk,
but the key is stored somewhere else (encrypted with a password based key).
This can prevent someone who has access to the ciphertext from executing a
brute force attack.
If however your ciphertext always travers with your encrypted key, you don't
gain much of an advantage (the weak point is the password-based key which
can be brute forced or dictionary attacked).
I don't recommend just XORing for the protection of the key.  If ever your
Key Derivation Function doesn't really act like a good pseudo-random
function, or if you use the same password and salt to derive the same key to
protect two different keys, you will be screwed. I rather recommend
encrypting with something like AES, and I also recommend to compute a MAC
over the ciphertext to turn it into a strong encryption, and avoid attacks
such as what have been found with the HSM and the way they stored keys
outside the HSM.  For further details on that point, see for example section
4.3 of the following paper (follow the references given there)
-----Original Message-----
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Jack Lloyd
Sent: November 29, 2005 11:09 AM
The basic scenario I'm looking at is encrypting some data using a
password-derived key (using PBKDF2 with sane salt sizes and iteration
counts). I am not sure if what I'm doing is sound practice or just pointless
overengineering and wanted to get a sanity check.
My inclination is to use the PBKDF2 output as a key encryption key, rather
using it to directly key the cipher (with the key used for the cipher itself
being created by a good PRNG). For some reason the idea of using it directly
makes me nervous, but not in a way I can articulate, leading me to suspect
worried over nothing.
So, assuming using it as a KEK makes sense: At first I thought to use XOR to
combine the two keys, but realized that could lead to related key attacks
just flipping bits in the field containing the encrypted key). That is
not a problem with good algorithms, but, then again, why take the chance; so
was thinking instead using NIST's AES-wrap (or perhaps a less weirdly
variant of it that uses HMAC for integrity checking and AES in CBC mode for
Am I thinking about this far harder than I should?

@_date: 2005-12-05 08:13:28
@_author: Anton Stiglic 
@_subject: Fermat's primality test vs. Miller-Rabin 
O.k., so if I read this right, your new results concord with the analysis of
Pomerance et al.   That would make much more sense.
When you say "on average about 120-130 times the test fails", out of how
many is that?

@_date: 2005-12-21 19:56:14
@_author: Anton Stiglic 
@_subject: another feature RNGs could provide 
Yes, and the set of keys define a subset of all of the possible permutations
(working on the same size input as the block cipher).  The set of all
permutations is a group, but a subset of that is not necessarily a subgroup.
Most security proofs of modes of operations, and others, model a block
cipher as a random permutation.

@_date: 2005-07-08 15:48:30
@_author: astiglic@okiok.com 
@_subject: EMV [was: Re: Why Blockbuster looks at your ID.] 
Interesting statistics.
Seems like it's the same thing in Canada
Reported $227M in credit card fraud in 1999, droped at $200M in 2003.
But these are still considerable numbers, and the thinking that Banks
manage the risk and it's not worth them going over to smart card
technology so they won't, which was mentioned in a few replies, I think no
longer holds (probably because of the falling cost of the technology, so
even if fraud $ is down as mentioned, ratio of fraud cost / cost of
technology that is more secure still leads financial institutions to want
to go to a more secure technology).
Europe already has EMV, and Canada plans to have an infrastructure (card
readers) that support it by 2007.  Probably U.S. will follow
And here, for example, is a quote from Visa Canada
"Visa Canada Member financial institutions will implement chip at their
own pace.  It is expected that within seven years, almost every Visa card
in Canada will feature chip technology and most merchants will have the
equipment to accept and fully benefit from these cards."
That was written in June 2003.

@_date: 2005-07-11 11:56:10
@_author: astiglic@okiok.com 
@_subject: Why Blockbuster looks at your ID. 
It's just that the drivers license number is a unique number that acts as
an index to another database (and often used as authentication material as
well), which the merchant has to business knowing.

@_date: 2005-07-11 12:36:43
@_author: astiglic@okiok.com 
@_subject: EMV [was: Re: Why Blockbuster looks at your ID.] 
Interesting, they have a card (smart card)? and key fob version.  I hope
their key fob version is not as insecure as the SpeedPass RFID transponder
token used by Exxon/Esso, which has recently been broken
The SpeedPass implemented an authentication algorithm (I think it was a
CRC-like challenge response based on a secret that defined the polynomial
used) based on a 40-bit key.  Bono & al. figured out the algorithm (based
on a patent, which described the algorithm generically, they figured out
the constants that were chosen).
The question is why did they use a 40-bit secret?  Is there some
technological constraint preventing the use of something better?
The other thing is that many of the smart cards also have a magnetic
strip, so your security level is as strong as the weakest point (magnetic
stripe type payments).  Untill all the cards are smart cards, readers will
accept both type.

@_date: 2005-06-03 16:14:18
@_author: astiglic@okiok.com 
@_subject: Papers about "Algorithm hiding" ? 
Well, everyone who has Windows on their machine (at least a Windows 95
updated version and up, I think) has at least Microsoft's crypto provider,
 and MS CAPI to use it!  Most broswers implement HTTPS, so you have crypto
there as well.
I think we are already in a state where practically everybody that has a
computer has crypto available, and it's not difficult to use it!
Another alternative is the cyphersaber type of thing, where you could just
implement your crypto-code on the fly, as needed.

@_date: 2005-06-08 11:15:40
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
One thing that irritates me is that most security audits (that verify
compliance with regulations) are done by accountants.  No disrespect for
accountants here, they are smart people, but most of them lack the
security knowledge needed to really help with the security posture of a
company, and often they don't work with a security expert.  I saw allot of
requirements by security auditors that looked pretty silly.
I believe a mix of accountants with security experts should be used for
security audits

@_date: 2005-06-08 13:33:45
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
Another area where I predict vendors will (should) offer built in
solutions is with database encryption.  Allot of laws require need-to-know
based access control, and with DBA's being able to see all entries that is
a problem.  Also backups of db data can be a risk.
Oracle, for example, provides encryption functions, but the real problem
is the key handling (how to make sure the DBA can't get the key, cannot
call functions that decrypt the data, key not copied with the backup,
There are several solutions for the key management, but the vendors should
start offering them.

@_date: 2005-06-08 17:20:56
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
Yes, encrypting indexed columns for example is a problem.  But if you
limit yourself to encrypting sensitive information (I'm talking about
stuff like SIN, bank account numbers, data that serves as an index to
external databases and are sensitive with respect to identity theft),
these sensitive information should not be the bases of searches.
If they are not he basis of searches, there will be no performance
problems related to encrypting them.
So my answer to people that have the perception you mentioned is that if
you want to encrypt sensitive information and that would cause performance
problems, then there are problems with your data architecture privacy wise
(you should re-structure your data, use it differently, etc.).

@_date: 2005-06-09 16:07:50
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
There are exceptions, I grant you that, but my hypothesis is that in most
cases you can do without indexing on the "sensitive" data you have.
Encrypting everything in your database, I say that will never work.  If
you do that, then you will have performance trouble, and nobody want's
that.  You can do stuff like encrypt everything at the OS level, but that
doesn't help protect database backups and it doesn't prevent your DBA to
look at data he's not supposed to.  If you encrypt everthing at the DBMS
level or at the application (client or middleware) level, than you cannot
encrypt indexed data.

@_date: 2005-06-09 17:00:28
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
My terminology might have been misleading.  By "indexes to external
databases", I don't mean that the application that uses the database
actually talks to the external databases (it doesn't use the info as a key
to that external database)

@_date: 2005-06-09 17:07:08
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
One of my favourites was that "PINs had to be hashed"  (these were PINs
for authentication in a proprietary application/system.  The justification
(given by the auditor) was that people who had access to the database,
should not be able to see the PINs in clear.  These where 4 digit PINs. So
the developers just SHA-oned the PINs.  Later on, the developers had to
export the PINs into another application, that had its own way to protect
the PINs, so they launched a brut force attack on all of the PINs, of
course this was easy because the input space was very small and the hash
function did not involve any secret key, no salt, no iterations...  Talk
about protection!

@_date: 2005-06-10 13:11:45
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
"Ben Laurie wrote"
No, they shouldn't!  If you think they should, you are missinformed.  At
least in Canada, the Privacy Act protects the SIN, Equifax cannot demand
See for example
which says the following:
"Even credit reporting companies can?t demand a SIN to generate a credit
report. Trans Union Canada and Equifax Canada both have the ability to
generate such reports without a SIN. If you ask these same companies to
generate a credit report in the United States, they both require a Social
Security Number."
And if Equifax Canada can generate reports without a SIN, I don't see why
Equifax in any other country couldn't.  Of course, they like to have the
SIN, since it makes things more convenient, but they don't really need it!
 That is the problem in most cases.

@_date: 2005-06-10 15:16:00
@_author: astiglic@okiok.com 
@_subject: encrypted tapes (was Re: Papers about "Algorithm hiding" ?) 
They'll ask for it, but you don't have to give it.  They can collect it,
but they don't have to do searches on it.
It's the typical ask for SIN if the user gives it use it (as in Adam
Shostack's example with cell phone), but if they don't then ask for 2
other identity cards.  In most cases, I don't have to give my SIN, but
almost everybody asks for it.
Equifax will always ask for the SIN but they don't have the right to
demand it.
"Equifax suggests that to prevent these inaccuracies, consumers should
always give their full name and SIN number on application forms (this
facilitates updating of files and prevents confusion of two files).
However, this solution to the problem does not take into account that
consumers have a valid interest in protecting their privacy with respect
to their SIN."
The problem is with forms that make it look like you have to give your
SIN, when in fact the law says you don't have to.  Providing other
identification can be troublesome, so allot of people just end up giving
their SIN.

@_date: 2005-11-10 13:55:36
@_author: Anton Stiglic 
@_subject: Fermat's primality test vs. Miller-Rabin 
That is true but is not the result of a direct conclusion.  Let X
represent the event that n is composite, and Y_t the event that
MILLER-RABIN(n,t) declares n to be prime.  Because for a composite n there
is at least 3/4 of a's that fail the test, we can conclude that Pr(Y_t |
X) <= (1/4)^t.
But the probability I think you are referring to (the one that is usually
considered the most interesting) is P(X | Y_t).  It happens to be the case
that P(X | Y_t) is in fact <= (1/4)^t when using uniform random
candidates, but to come to that conclusion you need to consider the fact
that the error probability of Miller-Rabin is usually far smaller than
(1/4)^t (and apply Bayes theorem and a theorem on the distribution of
prime numbers).  See Note 4.47 in the Handbook of applied cryptography, or
the following paper:

@_date: 2005-11-10 13:36:51
@_author: Anton Stiglic 
@_subject: Fermat's primality test vs. Miller-Rabin 
That is not true, in several counts.
Firstly Miller-Rabin probabilistic primality test doesn't generate a
number, it verifies a number for primality.
Secondly, the Miller-Rabin probabilistic primality test is not based on
Fermat's Little theorem, or so called pseudoprime test, but rather on the
strong pseudoprime test, which derives from a theorem that says that if n
is an odd prime, n-1 = 2^s * r with r odd, then for any a such that
gcd(a,n) = 1 either a^r == 1 (mod n)  or  a^(r*2^j) == -1 (mod n) for some
j, 0 <= j <= s-1.   See Handbook of a applied cryptography fact 4.20.
I'm affraid the reference you gave is incorrect.

@_date: 2005-11-15 19:55:25
@_author: Anton Stiglic 
@_subject: Fermat's primality test vs. Miller-Rabin 
If you just took the exponent 80 and divided it by 6 to get ~13, I don't
think that is the right reasoning.  Look at table 4.3 of the Handbook of
applied cryptography: for t = 1 (one iteration) and for a 500-bit candidate,
we have probability p(X | Y_1) <= 2^-56, which is better than what you
concluded.  (X representing the event that the candidate n is composite, Y_t
representing the event that Miller-Rabin(n, t) declares n to be prime).
The results in table 4.3 and 4.4 of HAC are for randomly (uniform) chosen
candidates, and I think you need to do a basic sieving (don't remeber if
that is necessary, but I think it is).  The result is due to the fact that
under these conditions, the strong pseudoprime test does in fact much better
than 1/4 probability of error ( value of P(Y_t | X) is very low ), this
result is due to Damgard, Landrock and Pomerance, based on earlier work of
Erdos and Pomerance.

@_date: 2005-11-21 22:39:17
@_author: Anton Stiglic 
@_subject: Fermat's primality test vs. Miller-Rabin 
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Joseph Ashwood
Sent: November 18, 2005 3:18 AM
Do you do an initial sieving to get rid of the more obvious primes?  I'm
guessing you don't since you seem to have a result contradictory to what has
been proven by Damgard, Landrock and Pomerance.  If you look at table 4.3 of
HAC (which comes from Damgard & al. paper), it says that if your candidates
come from a uniform random distribution, then for 500 bit candidate, the
probability that a candidate n is composite when one round of miller-Rabin
said it was prime is <= (1/2)^56.  You are finding that the probability is
about 1/2, that seems very wrong (unless you are not doing the sieving,
which is very important).  Am I misunderstanding something?
Well I think I explained it pretty clearly.  I can try to re-iterate.  Let X
represent the event that a candidate n is composite, and let Y_n denote the
event that Miller-Rabin(n,t) declares n to be prime, where Miller-Rabin(n,t)
means you apply t iterations of Miller-Rabin on n.
Now the basic theorem that we all know is that P(Y_t | X) <= (1/4)^t (this
is problem in one of Koblitz basic textbooks on cryptography, for example).
But this is not the probability that we are interested in, we are (at least
I am) more interested in P(X | Y_t).  In other words, what is the
probability that n is in fact composite when Miller-Rabin(n, t) declared n
to be prime?  Do we agree that this is the probability that we are
interested in?
You are looking for P( Comp Y_t | X), where Comp Z is the complementary
event of Z. In our case, Comp Y_t is the event that Miller-Rabin(n,t) proves
n to be composite. Is that what you are looking for?
I don't understand what you are trying to point out.  If you chose your
candidates uniformly at random, do the sieving before applying the
Miller-Rabin tests, then for 512 bit number it is sufficient to apply 5
rounds to get probability of error lower than (1/2)^80.  You should take a look at Damgard & al's paper, they did a very good

@_date: 2005-09-02 10:32:55
@_author: astiglic@okiok.com 
@_subject: Fwd: Tor security advisory: DH handshake flaw 
I agree.  Either assume that the code on the PC is valid, or don't.  If
you don't, anything can have a back door in it, the encryption or
signature code, the Miller-Rabin test, the RNG, the encoding scheme you
use, etc.

@_date: 2006-02-13 23:28:42
@_author: Anton Stiglic 
@_subject: general defensive crypto coding principles 
I don't believe MtE is good advice, and I have yet to see a decent reason
why one would want to use that instead of EtM. Of course when we talk about EtM, the MAC should be applied over all
plaintext headers and trailers (including IV used for encryption, algorithm
identifier, protocol version, whatever).
Allot of attacks could have been prevented with EtM, including the Vaudenay
padding attack, the Chosen-Ciphertext Attacks against PGP and other email
encryption protocols described by Schneier, Katz and Jallad
as well as the attacks on Host Security Modules key blocks (well in this
case the bad was simply that their were to integrity checks, 2 key
Triple-DES keys were protected by a master triple-DES key by encrypted the
left part and right part independently) and other such types as described by
Clulow and others
Ferguson gave an explanation why in his book with Schneier they recommend
But the arguments he gives pertain to other problems; see for example the
comments given by Wagner which I agree with
I had come up with a list of advices for crypto implementation some time ago
myself.  These included (from memory)
- Use good RNGs, even for things other than the generation of keys (such as
for generating IVs, challenges, etc.)
- Use standard algorithms, and use them in secure ways (choose a good mode
of encryption, adequate key sizes, pick the IVs the way you are supposed to
securely, usually either randomly or for counters make sure you have no
- Use standard protocols (don't try to re-invent TLS or IPSec)
- Encrypt then authenticate over ciphertext and all plaintext headers and
- Use independent keys for different functionalities.  If needed, derive
independent keys based on a single secret using a good key derivation
- Limit the amount of time you handle secrets (zeroize after use...)
- Don't let yourself be used as a random oracle (I think Ross Anderson said
it this way first), this includes limiting information that is leaked about
errors, avoiding timing attacks and such (this is hard to do in practice).

@_date: 2006-01-27 23:52:57
@_author: Anton Stiglic 
@_subject: a crypto wiki 
I agree.  The cryptodox page looks nice, but I would rather see the content
go in wikipedia, which is worked on, and looked at, by many more people, a
really beautiful community work.
-----Original Message-----
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Whyte, William
Sent: January 26, 2006 10:07 AM
There's also a crypto portal in Wikipedia itself:
FWIW, I'd rather see energy focused on the Wikipedia
version, which more people are likely to use.
William

@_date: 2006-07-12 14:44:53
@_author: Anton Stiglic 
@_subject: Interesting bit of a quote 
Does that mean that you (the company) are safe if all of the personal
information in the database is simply encrypted with the decryption key
laying right there alongside the data?  Alot of solutions do this, some go
to different lengths in trying to obfuscate the key.

@_date: 2006-03-03 22:57:13
@_author: Anton Stiglic 
@_subject: NPR : E-Mail Encryption Rare in Everyday Use 
Regularly signing email is not necessarily a good idea.  I like to be able
to repudiate most emails I send...
 --Anton

@_date: 2006-10-20 07:26:51
@_author: Anton Stiglic 
@_subject: Traffic Analysis References 
You will find a couple of references on traffic analysis applied to
anonymous networks here
-----Original Message-----
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Leandro Meiners
Sent: October 19, 2006 2:09 PM
Dear list,
Can anybody point me to any good references regarding traffic analysis?
Leandro Federico Meiners
GnuPG key fingerprint:
7B98 C0F5 42A3 2BEE 44AF
9D19 936F 5957 27DF AE74

@_date: 2006-09-19 23:55:38
@_author: Anton Stiglic 
@_subject: Exponent 3 damage spreads... 
I tried coming up with my own forged signature that could be validated with
OpenSSL (which I intended to use to test other libraries).  I haven't
succeeded, either because in the particular example I came up with OpenSSL
does something that catches the invalid signature, or I messed up somewhere
(the likelihood of which is far from negligible).  Unfortunately, I don't
have much more time to play with this.  I decided to share the methodology I
used with those of you who are interested in case the info is helpful to
anyone, or someone can tell me why the signature I produced doesn't get
validated by OpenSSL.
I followed the instructions of Hal Finney's excellent post:
 at metzdowd.com/msg06537.html
I started out by generating 3072 RSA key pair, with public exponent e = 3.
openssl genrsa -des3 -3 -out my.key 3072
the resulting key can be found bellow, the passwords is "test" if you ever
want to use it.
Then I created the corresponding public key certificate:
openssl req -new -x509 -days 1001 -key my.key -out my.cer
The public key certificate can be found bellow as well.  You can import this
in Windows key store, for example.
I then created a plaintext file, messageAunicode.txt, for which I computed a
signature on (a valid signature).  The idea was then to forge a signature on
an alternate messageBunicode.txt, without using the private key of course.
The two files can be found in attachment, they are in Unicode because I
wanted to also try this out with a VBscript implementing a signature
function using CAPICOM. (you can get a file in Unicode by opening it with a
simple text editor that allows you to save as Unicode, such as notepad, and
erase any extra bytes (header) with a hex editor such as XVI32).
The hashes of these files are
openssl dgst -sha1 messageAunicode.txt
SHA1(messageAunicode.txt)= eb8302606217ae549fe6ab1345f0b4c804195367
openssl dgst -sha1 messageBunicode.txt
SHA1(messageBunicode.txt)= 5d89b46034e0f41a920b2fa964e230ebb2d040b0
Now, create the valid signature over messageAunicode.txt to see what the
output looks like:
openssl dgst -hex -sha1 -sign my.key messageAunicode.txt
Enter pass phrase for my.key:
Now, let's do some bignumber math.
I wanted to look at the value obtained when you verify a signature (RSA
encrypt the signature with exponent 3).  I use BC, a bignumber calculator I
like allot:
A version that can be installed on Windows:
I use a fast modexp function I implemented for BC, available at
You can load it by simply calling bc like this:
bc modgroup.bc
I did the calculations all in hex, so typed the following in bc
Now, denote by s the signature given above, e = 3 and m is the modulus in
the public key certificate I generated.  When pasting the values into BC,
the hex digits need to be in capital letters.  You can get the value of the
modulus by using a ASN.1 viewer, such as ASN.1 Editor or Peter Gutmann's
ASN1dump.  Here are the BC calculations:
modexp(s, 3, m)
You can see the 01 FF FF FF .. FF pattern followed by Which is the ASN.1 header (with 00 in front of it) followed by Which is the hash of messageAunicode.txt as we calculated above.
Now, let's do the calculations in Hal's post, using the above ASN.1 header
(which stays the same for all signatures using the above key and the SHA1
algorithm), with the hash of messageBunicode.txt in order to forge a
signature over the content of messageBunicode.txt Since we are calculating in hex, here are the equivalences of the exponents
which are given in decimal in Hal's post:
288d = 120h
1019d = 3FBh
34d = 22h
Now, in bc, using the notations of Hal's post :
n = 2^120 - d
s = 2^3FB - (n*2^22)/3
Now let's look at s^3
This has the form we are looking for, the 01 FF FF ... FF header that ends
with 00, and then we have which is the d we started out with, and the rest is the GARBAGE part.
Only one problem, s^3 is larger than m, so if we computed modexp(s, 3, m)
the result would be rounded out modulo m and we would loose the above
So what I did (and this might be a mistake, but I believe it works because
of the explanation I will give), is took s as the signature but cutting out
00 at the end in order to produce s':
sp^3 will start with the same digits as s^3, but will be smaller.
Let's look at the modexp calculation
modexp(sp, 3, m)
So sp is my tentative forged signature.  I saved it to a file in order to
try to verify it with OpenSSL.  I don't know how to make openssl command
line to verify a signature that is represented in hex (it can output a
signature in hex, but I can't seem to find how to verify one in this format,
seems to only accept binary).  I look for a hex2bin converter, which is
harder to come by than you would think.  Finally, not wanting to code
something (to lazy) I used XVI32 hex editor in some unorthodox way to
transform the hex signature in binary.  The result can be found in
attachment.  Unfortunately (or fortunately), I can't get OpenSSL to validate
this signature.
openssl dgst -prverify my.key -sha1 -signature opensslB-fake.sig
Enter pass phrase for my.key:
Verification Failure
(I copied here the verification command that uses the private key, you can
of course use the verification command with just the public key, at the
moment of writing this I simply couldn't remember how to make it work).
If you know what is happening, please tell me.  Hope the info is useful to
-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: DES-EDE3-CBC,162F46975BB386BE
-----END RSA PRIVATE KEY-----
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----

@_date: 2006-09-21 08:47:44
@_author: Anton Stiglic 
@_subject: Exponent 3 damage spreads... 
Thanks for taking the time to look at this.
But I recounted, and I count 765 hex (with the formatting I get in my mail,
11 lines of 68 hex + 17 hex at the end), which gives 3060 bits.  Considering
that the first hex is 1 and can be represented in 1 bit, not for, that would
give 3060 - 3 = 3057 bits.
The modulus is the same size, but starts with 1D instead of 1F (the
beginning of s^3), so s^3 is bigger.  My bc library has a function called
bits which returns the number of bits, I get 3057 in both cases, see bellow
(also look at the value of m - s, which is negative, and modexp(s, 3, m)
which doesn't have the form we want, but modexp(s/100, 3, m) does).  But I seem to remember now that in openssl, mod(x, y) doesn't always return
a value which is between 0 and y, maybe it would accept my s.  Will try it.
m - s^3
modexp(s, 3, m)
modexp(s/100, 3, m)
-----Original Message-----
Sent: September 20, 2006 6:21 PM
This is not correct.  I counted, and the number shown above has 762
hex digits.  It is 3057 bits long, compared to m which is 3072 bits.
It is not bigger than m, and does not need to be adjusted.  3057 is
precisely the correct number of bits for a PKCS-1 padded value for a
3072 bit exponent.

@_date: 2006-09-21 11:29:52
@_author: Anton Stiglic 
@_subject: Why the exponent 3 error happened: 
As other's have mentioned, I don't believe the small RSA exponent (e = 3)
is to blame in Bleichenbacher's attack.
Indeed, the mathematical problem of computing the cubic root of m modulo
an rsa modulus n, for a *fixed*, arbitrary m, is still considered to be
hard (no one has shown the opposite).
What Bleichenbacher demonstrated is that computing the cubic root of m' ||
G, where G can be any value, garbage, and is sufficiently large, is easy.
These are two different problems, and the vulnerability is due to the fact
that these libraries allow for the variant G part.
I don't see ASN.1 as being faulty either.  The ASN.1 simply acts as a
value that allows you to determine what hash algorithm to use.  If the
encrypted signature would be of the form:
  What-ever-padding, hash, header
and implementations would directly go to the least significant bits in
order to retrieve the header (which should be of fixed size), and then
retrieve the hash, we wouldn't have this problem.
I believe you should put the most sensitive information in the least
significant bytes, which are harder to manipulate (Bleichenbacher's attack
plays with the most significant bytes, the least significant bytes are
basically random in his calculations, he doesn't have control over them).
This reminds me of the RSA lsb hardness problem theorem
I have notes explaining it right here, section 8.4.1:
The theorem basically says that if you can predict the least significant
bit of the plaintext given the corresponding RSA ciphertext, than you can
compute the whole plaintext.
The theorem doesn't directly apply however (RSA signature verification
uses the encryption operation, not decryption), but may be of some
The problem is that we (crypto community) still don't have a good way of
writing specs.  This is in fact a hard problem.  And the problem doesn't
get easier with the increasing complexity of the specs.  We need simple
algorithms and protocols, which allow just enough flexibility, and we need
a good and precise way to write specs for these.
On one side you have theoretical cryptographers / mathematicians who work
in an abstract level, develop algorithms and protocols, but don?t have
much interest in implementing these other than possibly in a prototype
form.  On the other end, you have developers who excel in coding and
system integration but don?t necessarily understand the theoretical
background in all its details.  Specifications act as a bridge between
these two worlds, but this bridge is not very solid today.  We need to do
allot more effort into building stronger bridges.

@_date: 2006-09-22 01:10:28
@_author: Anton Stiglic 
@_subject: Exponent 3 damage spreads... 
O.k., thanks to Hal Finney for pointing out to me in a private email that my
modulus wasn't in fact the right size.  I have had some problems with the
openssl key generation (doesn't always seem to generate the exact modulus
size I ask for).  In attachment, the forged signature opensslB-fake-bin.sig on
messageBunicode.txt which can be validated using the new key I generated
bellow.  I took the same s that I computed beforehand, without reducing it
this time.  The value s is independent of the value of the modulus of the
public key (only dependency is the size)
So here are the instructions, once again (but I bit simplified):
I followed the instructions of Hal Finney's excellent post:
 at metzdowd.com/msg06537.html
I started out by generating 3072 RSA key pair, with public exponent e = 3.
openssl genrsa -des3 -3 -out my.key 3072
(make sure that the modulus is really 3072 bits, no less no more).
the resulting key can be found at the end of this post, the passwords is
"test" if you ever want to use it. I also included the public key
certificate by itself.  All in PEM format.
I created a plaintext message messageBunicode.txt on which I want to forge a
signature.  The file can be found in attachment, it is in Unicode because I
wanted to also try this out with a VBscript implementing a signature
function using CAPICOM, in which all plaintext is transformed into UNICODE
(pretty annoying!). The hash of this file is
openssl dgst -sha1 messageBunicode.txt
SHA1(messageBunicode.txt)= 5d89b46034e0f41a920b2fa964e230ebb2d040b0
Now, let's create a valid signature over messageBunicode.txt using the
private key, just to see what the output looks like:
openssl dgst -hex -sha1 -sign my.key messageBunicode.txt
Enter pass phrase for my.key:
Now, let's do some bignumber math.
I wanted to look at the value obtained when you verify a signature (RSA
encrypt the signature with exponent 3).  I use BC, a bignumber calculator I
like allot:
A version that can be installed on Windows:
I use a fast modexp function I implemented for BC, available at
You can load it by simply calling bc like this:
bc modgroup.bc
I did all the calculations in hex, so typed the following in bc
Now, denote by sp the signature given above, e = 3 and m is the modulus in
the public key certificate I generated.  When pasting the values into BC,
the hex digits need to be in capital letters.  You can get the value of the
modulus by using an ASN.1 interpreter, such as ASN.1 Editor or Peter
Gutmann's dumpASN1 program.  Here are the BC calculations:
I verify that my modulus is of the right size:
C00 hex is 3072, good.
Now do RSA verification:
modexp(sp, 3, m)
You can see the 01 FF FF FF .. FF pattern followed by which is the ASN.1 header (with 00 in front of it) followed by which is the hash of messageBunicode.txt as we calculated above.
Now, let's do the calculations fro  Hal's post, using the above ASN.1 header
(which is the same for all signatures using the above key and the SHA1
algorithm, and you don't need a valid signature to get the value, it is
predictable), with the hash of messageBunicode.txt in order to forge a
signature over the content of messageBunicode.txt Since we are calculating in hex, here are the equivalences of the exponents
which are given in decimal in Hal's post:
288d = 120h
1019d = 3FBh
34d = 22h
Now, in bc, using the notations of Hal's post :
n = 2^120 - d
(you can verify that n is divisible by 3)
s = 2^3FB - (n*2^22)/3
Now let's look at s^3
This has the form we are looking for, the 01 FF FF ... FF header that ends
with 00, and then we have which is the d we started out with, and the rest is the GARBAGE part.
Let's look at the modexp calculation
modexp(s, 3, m)
I saved this value in a file in order to try to verify it with OpenSSL.  I converted the value to binary. The result can be found in attachment.  I
added leading 00s to make it the size of a normal signature (I think you
have to do this to get the signature to be validated by Openssl).  An unpatched version of Openssl validates it:
C:\crypto\openssl-0.8.3c>openssl dgst -prverify ..\my.key -sha1 -signature
ensslB-fake-bin.sig ..\messageBunicode.txt
Enter pass phrase for ..\my.key:
Verified OK
Great, it works!
(I used the private key to validate it here, but you can of course use just
the public key).
-----BEGIN RSA PRIVATE KEY-----
Proc-Type: 4,ENCRYPTED
DEK-Info: DES-EDE3-CBC,80575E895271B8B9
-----END RSA PRIVATE KEY-----
-----BEGIN CERTIFICATE-----
-----END CERTIFICATE-----

@_date: 2006-09-25 16:28:31
@_author: Anton Stiglic 
@_subject: interesting HMAC attack results 
Very interesting, I wonder how this integrates with the following paper
which basically says:
Abstract: HMAC was proved in [2] to be a PRF assuming that (1) the
underlying compression function is a PRF, and (2) the iterated hash
function is weakly collision-resistant. However, recent attacks show that
assumption (2) is false for MD5 and SHA-1, removing the proof-based
support for HMAC in these cases. This paper proves that HMAC is a PRF
under the sole assumption that the compression function is a PRF. This
recovers a proof based guarantee since no known attacks compromise the
pseudorandomness of the compression function, and it also helps explain
the resistance-to-attack that HMAC has shown even when implemented with
hash functions whose (weak) collision resistance is compromised.
Perry E. Metzger
Sat, 23 Sep 2006 05:52:04 -0700
  Cryptology ePrint Archive: Report 2006/319
Forgery and Partial Key-Recovery Attacks on HMAC and NMAC Using Hash
Scott Contini and Yiqun Lisa Yin
  Abstract. In this paper, we analyze the security of HMAC and NMAC,
  both of which are hash-based message authentication codes. We present
  distinguishing, forgery, and partial key recovery attacks on HMAC and
  NMAC using collisions of MD4, MD5, SHA-0, and reduced SHA-1. Our
  results demonstrate that the strength of a cryptographic scheme can be
  greatly weakened by the insecurity of the underlying hash function.
[I Heard about this paper from ekr's blog.]

@_date: 2007-01-30 22:52:41
@_author: Anton Stiglic 
@_subject: Intuitive cryptography that's also practical and secure. 
I am not convinced that we need intuitive cryptography.  Many things in life are not understood by the general public.
How does a car really work: most people don't know but they still drive one.
How does a microwave oven work?
People don't need to understand the details, but the high level concept
should be simple:  If that is what you are trying to convey, I agree with
I guess we could very well do with some cryptographic simplifications.  Hash
functions are one example.  We have security against arbitrary collisions,
2nd pre-image resistance, preimage resistance.  Most of our hash functions
today don't satisfy all of these properties:  "Oh SHA1 is vulnerable to
aribitrary collisions attacks, but it is still safe agains 2nd pre-image
attacks, so don't worry!" Why do we need all of these properties?  In most cases, we don't.
Mathematical masturbation might be to blame?   Block cipher encryption.  How many modes of operations exist?  Some use a
counter, others need a random non predictable IV, others just need a non
repeatable IV?  Do we need all of this?
I often find myself explain these concepts to non-cryptographers.  I'm often
taken for a crazy mathematician.
What is the length of a private key?  In 1024-bit RSA, your d is about 1024
bits.  But is d your private key, or is it (d,N),  in which case there is
more than 1024 bits!  No, N is public, the known modulus, but you need it to
decrypt, you can't just use d by itself.  Oh, in DSA the private key is much
shorter.  You actually also need a random k, which you can think of as part
of your key, but it's just a one time value.  Are we talking about key
lengths, of modulus lengths really?
When you encrypt with RSA, you need padding.   With Elgamal, you don't need
any, complicated story.  And don't use just any padding.  You would be
foolish to use PKCS v1.5 padding, everybody knows that right?  Use OAEP.
It is provably broken, but works like a charm when you encrypt with RSA!
Going back to the million dollar paranormal challenges:  Something like a
Windows SAM file containing the NTLM v2 hash of the passphrase consisting of
the answer might be something to consider?  Not perfect but...
-----Original Message-----
[mailto:owner-cryptography at metzdowd.com] On Behalf Of Matt Blaze
Sent: January 26, 2007 5:58 PM
I was surprised to discover that one of James Randi's "million dollar
paranormal challenges" is protected by a surprisingly weak (dictionary-
based) commitment scheme that is easily reversed and that suffers from
collisions. For details, see my blog entry about it:
    I had hoped to be able to suggest a better scheme to Randi (e.g., one
based on a published, scrutinized bit commitment protocol).   I don't know of any that meets all his requirements, the most important
(aside from security) being that his audience (non-cryptographers
who believe in magic) be able to understand and have confidence in it.
It occurs to me that the lack of secure, practical crypto primitives and
protocols that are intuitively clear to ordinary people may be why
cryptography has had so little impact on an even more important problem
than psychic debunking, namely electronic voting. I think "intuitive
cryptography" is a very important open problem for our field.

@_date: 2007-01-31 06:52:52
@_author: Anton Stiglic 
@_subject: Private Key Generation from Passwords/phrases 
Yes indeed.  The rainbow-tables style attacks are important to protect
against, and a salt does the trick.  This is why you can find rainbow tables
for LanMan and NTLMv1 hashed passwords, but not for NTLMv2.
This to me is the most important property achieved with a salt, and the salt
doesn't have to be that big to be effective.
