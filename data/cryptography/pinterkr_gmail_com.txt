
@_date: 2013-12-24 19:03:52
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Arnold Reinhold (at Tuesday, December 24, 2013, 6:21:29 AM):
yep, plenty. for example all that knows the principle of not using
branching/indexing on secret. pbkdf2 does not do that, and therefore
safe against cache timing attacks. the same can not be said about
either bcrypt, which uses secret based s-boxes, but especially not
scrypt, which uses secret based memory access wildly.
one could also ask how safe it is to sprinkle the secret all over the
RAM, increasing the risk of getting swapped to disc, or being
recoverable by cold boot attack.
there is a lot to fear about scrypt. don't forget, we live in the era
of side channel attacks. the safety of scrypt against direct attacks
does not grant much in the real world.
to be totally safe, we would need a KDF that obeys all coding
standards, runs in constant time, executes the exact same operations
every time for every password, avoids using secret for indexing or
branching, does not write secret to physical RAM, preferably keeps any
secrets in the CPU, yet uses a large chunk of memory in a way that can
not be optimized away.
i don't think we have any other option than capturing djb, and not
letting him out of the basement until he comes up with a solution.

@_date: 2013-12-25 01:37:55
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Bill Frantz (at Wednesday, December 25, 2013, 12:27:29 AM):
one day an attack is not common, the next day it is. you probably
would have said in 1980: cache timing attacks are not common. just as
virtually anybody else said. but in 2013, there is a great interest in
timing-resistant AES implementations.
attackers will use whatever they can. and i'm betting a thousand bucks
on filling the RAM with sensitive data will be an attack vector sooner
or later.
it is not satisfactory to list the situations in which an attack is
not feasible. we want to know when it is. furthermore, we want
primitives not sensitive to attacks. i'm happy that you can turn your
laptop off before approaching airport customs. but there can be people
that can't avoid some situation in which a cold boot attack is
swap encryption is nice, but attacks against memory are not limited to
that. RAM is shared on HW level between CPUs, very hard to protect on
a VM, data travels on the bus which emits EM, etc. it is also a hot
topic to do encryption inside the CPU.
i would agree that these are less important issues than password
stretching. however, they are not contradictory. i can design a kdf
right here that does not leak the password all over RAM, does not
index or branch on secret, but does use a lot of RAM and CPU. the
downside of my design would be not being sequential memory hard, and
having a 20-40% additional CPU load compared to an unprotected
implementation (as the attacker will most certainly implement). this
is quite significant, but it is arguable whether it is more
significant than the upsides. but i trust better minds can create a
solution with the benefits from both worlds.

@_date: 2013-12-25 11:42:24
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Patrick Mylund Nielsen (at Wednesday, December 25, 2013, 3:37:40 AM):
goal can be nice, but it might fail at this goal if opens a backdoor.
i would only recommend scrypt (and bcrypt for that matter) in special
circumstances (if your attack model excludes cache timings). it is
okay, specialized solutions have a place in the industry. but you need
to know goddam well if you can use it or not. for general use, i must
recommend pbkdf2, even if it is an ugly little piece of design.

@_date: 2013-12-25 12:03:03
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Bill Frantz (at Wednesday, December 25, 2013, 5:25:03 AM):
we always learn very late when an attack goes from theoretical to
practical. if you are lucky, you read in in an academic paper. if not,
mainstream newspapers. considering that the industry needs years if
not decades to move from one method to the next, i assert that we are
already late.
yep, this is the game we (you and i) are playing right now, but this
is a game i refuse to play. if your attack model consists only of
things that i can come up with and you can not dismiss in some way, i
can assure you your system will be as far from being safe as it gets.
we don't choose side channel resistant methods because we can outline
a realistic attack using those. but because we don't want to leave the
possibility open, so we have one less problem.
also, consider a system protected by all the things you came up with.
futuristic casing, must be powered off physically before leaving it,
maybe a kill switch on the outside, all sort of hw locks and whatnot.
what is the user experience? i would probably say, f security, i don't
care anymore.
or make it not hurt. i think this latter is the more modern approach,
at least in medicine. the don't do it approach is more medieval.

@_date: 2013-12-25 12:25:37
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Serious paranoia... 
Bill Cox (at Wednesday, December 25, 2013, 2:35:59 AM):
i honestly don't understand why this mailing list is moderated. it
slows down posting, but seems having no other effect.

@_date: 2013-12-25 20:51:11
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Arnold Reinhold (at Wednesday, December 25, 2013, 8:29:20 PM):
1, i did not and 2, this is not the most important criteria. the most
important is safety.
i'm also not aware of any attacks against pbkdf2, or even a homegrown
repeated md5. just because it did not happen so far is not enough to
trust the algorithm.
the exact problem with side channel attacks is that the circumvent
other layers, opening other attack routes.
that is sure, me too.
like for example pbkdf2. (let me just stress like the thousandth time
that i don't like it. but it is safe, standard, and cpu-hungry.) in
comparison, scrypt is better in many situations, while worse or even
broken in some other situations. use with care.

@_date: 2013-12-25 23:22:00
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Passwords are dying - get over it 
Bill Frantz (at Wednesday, December 25, 2013, 12:27:30 AM):
this might be not as big of a problem as it seems. there are
memorization techniques surprisingly powerful:
i especially like this
showcased by derren brown in the heist. (disclaimer: i'm not citing
brown as a reference, but as an illustration how to perform mnemonic

@_date: 2013-12-26 10:53:20
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Why don't we protect passwords properly? 
Bill Frantz (at Thursday, December 26, 2013, 3:25:57 AM):
okay, i address this once more, because i feel we are running in
your argument is a double standard. i know zero cases when people
attacked pbkdf2. therefore, according to this logic, we have no reason
to move on. it is just as good. the very reason we want to move on is
preparation for the future. we want to be the ones taking the step
first, not the bad guys. hence my argument: watch out for side channel
first, this is a false dichotomy. second, side channel attacks are far
from being theoretical. they are out there in the wilderness already.
in cryptography, only pure technical solutions work. that is the cry
from the academia for years if not decades. please engineers, start to
use our methods, algorithms, recommendations. the solution for many
attacks and insecurities are discovered long ago, but we refuse to
incorporate them in our software. this have to change.
what is a "realistic" attack model? my attack model is one that can be
done. but not one that actually happened. i refuse to consider it bad.

@_date: 2013-12-26 21:14:34
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Serious paranoia... 
Theodore Ts'o (at Thursday, December 26, 2013, 8:33:50 PM):
here are two hypothetical attacks that affects RAM flooding, but most
likely does not affect usual secret handling, and are realistic:
1. i can confiscate/steal your computer a few minutes after logging
in, take the memory out, and recover bits of it with 2^-20
probability. the chance that i recover a password is slim. but if you
wrote password dependent data to 128MB, i can recover it with great
chance. it acts like a massive error correction mechanism.
2. what if i can't just read any memory, but occasionally and
unpredictably a random small piece of it, for example because you leak
it through a bug. my chances to see something i should not increases
with the amount of the data.
obviously, these are very contrived situations. but far from being
impossible. the question is not whether it is an issue or not. the
question is, how serious the threat is and what can we do about it. if
the solution is costly, we might accept the threat as a trade off, for
the time being. but it does not make the issue nonexistent. we can
still aspire to find a solution that does not have this attack angle.
we should aspire.
and that was my point. i would like to see an algorithm that is memory
hard, but the data written to the memory is scrambled by some random
parameter. i can design such an algorithm, it is not that hard. the
problem is, we need to do that effectively and in a way that does not
grant advantage to the bad guys.

@_date: 2013-12-26 22:46:42
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] A modification to scrypt to reduce side channel 
Arnold Reinhold (at Thursday, December 26, 2013, 9:29:24 PM):
it is. if you double the iteration count, you effectively double the
necessary hw to break it (because you need twice as many units to
deliver the crack in the same time frame). it is the same as with
scrypt. the difference is not that, but scrypt has a huge headstart by
making one hw unit much more expensive.
i believe this would severely reduce the hw cost, as you can share the
prepared memory block between multiple cores executing the 3rd step.
my main concern is that it still does not solve the bigger problem
that is not just my personal crusade, but rather, a recognized issue.
namely that the memory access pattern makes it susceptible to cache
timing attacks.
i'm wondering whether it is possible to maintain seq mem hardness
while having a fixed access pattern. i was thinking about gigantic
versions of existing primitives, like a keccak[25*2^28]. is that seq
mem hard? how close? i asked that on the PHC forum, but it did not
spark a whole lot of discussion (aka zero).
if not, maybe we could scramble the access pattern in some way. i'm
thinking about some random permutation of the index (mini block
cipher?). but that still leaks the frequency of hitting the same
location. like 12514323 and 53154232. you see the pattern there.

@_date: 2013-12-27 13:08:40
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Serious paranoia... 
Theodore Ts'o (at Friday, December 27, 2013, 1:02:51 AM):
that is exactly my problem. i can't. there are proposals out there, i
have an idea too, that don't use secret based indexing. however, those
are, including mine, not sequential memory hard, thus not in every
respect better than scrypt. it is a tradeoff.
and the only thing i could come up with to prevent filling the RAM
with secret is simply using encryption with a random key. it adds CPU
load which is a pure disadvantage, since the brute force
implementation can simply skip it. it is not a show stopper though,
but i doubt i can convince anybody to do that. luckily, it is an
implementation issue, and can be added at any time to any algorithm
with full backward compatibility.

@_date: 2013-12-31 22:52:53
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Strict memory hard hash functions 
Sergio Lerner (at Tuesday, December 31, 2013, 9:35:23 PM):
i have multiple excuses why i'm not addressing the core of your points
(one is that it is new year's eve, and i drank some champagne already,
second is i'm not perfectly a math guy).
but my question is: you are aware of the ongoing password hashing
competition  , aren't you?
i see one downside though. if i understand correctly, you need N^2
time if we want N memory blocks (blocks being a reasonable output size
of some PRF). it either limits us severely in memory use if we aim for
"regular" block sizes like 256 to 512 bits, or requires huge block
size. the latter approach seems to ease the problem to some degree.
can we consider like 65536 bit or higher block sizes? does that hurt
security? maybe we need to consider the internals of such a huge PRF?

@_date: 2014-12-03 18:14:51
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Construction of cryptographic software. 
Anton Titov (at Wednesday, December 3, 2014, 10:28:43 AM):
short answer: no
long answer: win95 family did
however, a similar issue is paging. and windows happily writes
anything into the page file, and leaves it there indefinitely.
hybernation works in a similar manner.
another similar issue is old office versions putting unused memory in
saved documents. obviously, it can't leak secrets in other processes,
but can leak database passwords, or passwords of other documents
handled in the same session. office does not do that anymore, but
other programs might.

@_date: 2014-02-05 21:20:18
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Random numbers only once 
Watson Ladd (at Tuesday, February 4, 2014, 6:12:55 AM):
apparently, he says much more than that. today on his brand new blog:
he argues that if we have a malicious source of entropy, and it can
access the other sources' raw data, it can manipulate the output of
the rng, and thus influence generated keys and nonces to some
(admittadly small) degree.
two dangers:
1, some schemes rely on perfect random nonces. cooked nonce is a
2, it can use randomness to communicate information to the outside
world. a driver or the CPU might have hard time phoning home. putting
bits in public keys and public nonces might open a channel.

@_date: 2014-01-03 08:45:45
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
John Kelsey (at Friday, January 3, 2014, 2:31:00 AM):
dual ec is easy to fix, but what is the point? it is even easier not
to use it, and use fortuna instead, which is better in every way
possible. people only use dual ec if they have to, to be compliant
with whatever standards. but then they can't change it, not even the
extraction part (heck, they can't even fix the mistakes in the
documentation, see the case of openssl).

@_date: 2014-01-03 18:02:04
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
i think you put too much burden on a prng. all prngs need a secret
seed. it is not an argument against them. the question is, what comes
after? rephrasing: supposed that we have some secret (for example true
random), how can we expand that into a random stream in a way that we
don't introduce *new* vulnerabilities. it is not the task of the prng
to solve the seeding problem, that should be handled separately.
in that sense, BBS has the benefit of having a proof in the standard
model. as opposed to AES based generators, that have formal proof
against some attacks only, while have a general proof in the random
oracle model. again, *in addition* to the problem of the seeding,
which they also have.
i'm not claiming that this is a practical advantage, or i would pay a
dime to get that. but it certainly represents *some* value.
that said, as i heard, dual-ec does not have a security proof. correct
me if i'm wrong.
Theodore Ts'o (at Friday, January 3, 2014, 4:57:40 PM):

@_date: 2014-01-03 23:50:15
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Dual_EC_DRBG backdoor: a proof of concept 
Theodore Ts'o (at Friday, January 3, 2014, 7:01:16 PM):
i call moving goalposts on this one. it can be the case that we don't
need prngs. we can grab enough unpredictable information from hw
sources that we can just extract all the randomness we need.
but not needing a solution does not make the solution incorrect. there
might be a case when we need it.
basically we have two schools on this, and i don't know where i
belong. one school says that you need true random source. no matter
how whitened or processed, it won't generate more entropy. the other
school says you need 128 bits of unpredictability, and then you can
extract megabytes of randomness with no risk at all. we understand
that there is only 128 bit uncertainty, but it is enough.
you can subscribe to the former school. but then, you don't want
fortuna either, nor any other prng. or you subscribe to the latter
school, but in this case, you have to admit that the quality of the
prng matters. and you also have to agree that bbs delivers some
security properties that for example aes does not.
are you sure of that? because i recall that someone said it is a myth,
it does not have a proof. unlike bbs that indeed has. anyway, i might
be wrong on that, but that is what i heard.

@_date: 2014-01-20 18:54:03
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] cheap sources of entropy 
ianG (at Saturday, January 18, 2014, 9:17:17 AM):
collecting entropy is easy. extracting entropy is also easy. providing
a general solution that reliably produces entropy is hard. just as an
example, take this camera project. the following questions come up,
from the top of my head:
- what if the camera breaks?
- what if the camera entropy production degrades with time?
- what if lighting conditions change?
- what if the driver is updated, and starts filtering noise?
- what if the driver is updated, and stops producing data?
- what if temperature variations affect entropy production?
- what if an attacker can listen on in EM, and read your camera output?
- what if another software on the same machine can read camera output?
- what if the janitor accidentally unplugs the camera?
in short: if you have an engineer, some free hours to spend, and you
want to generate some randomness at a certain location, with a
specific hardware, in a specific setting, it is always easy to do. but
these solutions do not transfer to different situations, and does not
apply to anyone with no engineer hours to spend.

@_date: 2014-01-20 20:50:50
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] cheap sources of entropy 
John Denker (at Monday, January 20, 2014, 7:59:47 PM):
it is not that much of a concern. whitening is easy. all you need is
an estimate on the entropy density (which is impossible, but this is a
whole different issue, see later).
and this is where things go fishy. theoretically and strictly
speaking, there is no such thing as lower bound on entropy. we can
only put upper bounds. if you dig deep enough, at the bottom, we
arrive at the many century (millennia?) old determinism problem. if
the world is deterministic, entropy is pretty hard to even interpret.
in the more practical setting, we always talk about the entropy as
seen by some specific observer. in this case, the observer is the
combined knowledge of the scientific community that tries hard to
quantify and formulate the world. if a scientist comes up with a
better model of handwaving or a better model of how noise on a video
camera works, the effective entropy drops.
so we actually don't want a proof. we only want a reasonably sound
guess on what the attacker can know. apply some safety margin, and you
are good to go.
another interesting point is that you can hardly say anything looking
at the data. you must evaluate the process. you can of course
eliminate some foreseen failure cases, like no input, low input or
repeated patterns. but you will never be able to tell apart
fraudulent, attacker controlled data, or arcane failures from true
if you mean, they produce very few entropy, i tend to agree. mouse
movements, waving or typing produces dozens to hundreds of bits per
second. this can be enough for some applications. after all, you need
only 128 bits, and you are good to go. also, a phone can possibly
gather data 24/7, accumulating it.
you are also correct that thermal noise can produce several orders of
magnitude more, and also much more stable.
but i disagree that thermal noise is fundamentally better. it is just
practically better.

@_date: 2014-01-21 20:15:01
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Auditing rngs 
John Kelsey (at Tuesday, January 21, 2014, 6:55:44 PM):
i think this reasoning is incorrect. it is security through obscurity.
we don't want the errors in our entropy source to be secret. we want
no errors there. in fact, we want direct access to the rawest output
of the TRNG, as well as complete information on how it works including
schematics, statistics, exact location on chip and all. we need full
isn't it the same situation as open source vs closed source? according
to the "secrecy is another layer of defense" argument, open source
should be less secure. the exact opposite is happening.
and i haven't even talked about the trust. post snowden, post RSA
debacle, post dual_ec, we want openness and honesty above all.
ask intel how happy they are with the acceptance of rdrand. i would
bet they are not so happy.

@_date: 2014-01-28 23:41:13
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] cheap sources of entropy 
James A. Donald (at Tuesday, January 28, 2014, 11:17:29 PM):
that might prove itself harder than it seems, if we don't have a good
estimate on the entropy. there is a solution though. fortuna rng does
it in a very clever way, it runs 32 parallel entropy collectors. it
uses the first of them for every reseeding. it uses the second one for
every second reseedings. it uses the third for every fourth
reseedings, and so on. even if we have no clue about the entropy
production, it will eventually recover from a compromised state.

@_date: 2014-01-29 18:08:42
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] cheap sources of entropy 
John Kelsey (at Wednesday, January 29, 2014, 3:09:48 AM):
except my remark was not about the topic in general, but how to reseed
a prng. making sure there is enough entropy is not the job of the OS,
but of the hardware and of the usage modes of the softwares installed.
the OS could not be tasked to collect enough entropy, as in most cases
this is either impossible or at least impossible to be sure of.
fortuna is a clever solution for the exact problem i proposed it for:
dealing with enough, but uncertain amount of entropy flowing in. it
solves the problem of robust reseeding.

@_date: 2014-03-03 18:44:24
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Btcoin -- bubble or investment opportunity? 
disclaimer: i have absolutely no intention to listen to the video, i
managed to watch 5 minutes of the hour long earlier one, not a single
argument came up, i moved on.
but here is this:
Phillip Hallam-Baker (at Monday, March 3, 2014, 12:15:23 AM):
which one then? because being stupid and not getting you rich quick is
miles apart. everyone believing anything will get them rich quick are
either dreamers or gamblers or geniuses, and only the latter can
actually hope to achieve this goal. this tells *nothing* about bitcoin
i see no evidence that Pluto has smaller hydrogen content than Io, but
it is largely due to the fact that i didn't even try. i personally
know a lot of places where you can actually pay with bitcoins. i don't
have a statistics about the investment/direct use ratio, but you seem
to only have a blank claim on the contrary. and even if it is
primarily used as investment ... so as gold. it does not invalidate it
as money. there is no rule what the correct ratio should be, as long
as the medium of exchange use exists.
so these arguments are non arguments, and i can only hope you have
something better in the video.

@_date: 2014-03-04 18:05:04
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Btcoin -- bubble or investment opportunity? 
Phillip Hallam-Baker (at Tuesday, March 4, 2014, 10:38:40 AM):
just as people buy oil or wheat as investment. it does not in any
degree changes their nature, they are primarily goods. bitcoin is
money. if anyone uses for investment, it is their business. bitcoin
might be bad investment, but it does not make it stupid, as you
claimed. it is money.
we also have a group of people that thinks the earth is hollow. talk
about straw man. but remember, you are still up to prove that bitcoin
is stupid. you are not on track.
and i already said that you can only not see that if you don't look.
actually, the huge relative volume of "speculative" traffic hurts
bitcoin, rather than makes it. should its value be more stable, you
would see many more using it.
bubble mentality is in everything that rises. does not matter. what
matters is the underlying value. investors do nothing else than hoard,
which temporarily drive up the price (lowering the amount in
circulation). as they stop doing that, normal operations are restored.
if you claim bitcoin is "stupid", you have to show that it represents
no value other than its expected future rise. but as i explained
already, it functions as money, therefore it has its use.
no shit

@_date: 2014-03-16 20:45:27
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Jerry Leichter (at Sunday, March 16, 2014, 8:23:29 PM):
this is the best argument *against* putting direct algo support in
CPUs. i deem the AES-NI instruction set rather harmful for the
industry. it would be much better if we put general purpose
instructions that help crypto. like huge register space (in the 8000
bit range), versatile parallelism, support for GF field operations
(prime and binary), better support for big num arithmetic, on-chip key
storage, etc. just like GPU designers sit down with game developers
and survey what they want, CPU developers should sit down with

@_date: 2014-03-17 22:03:58
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Apple's Early Random PRNG 
dj at deadhat.com (at Monday, March 17, 2014, 9:48:37 PM):
pretty much. what does it do, though, this is the question.

@_date: 2014-03-18 21:40:00
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Adam Sampson (at Tuesday, March 18, 2014, 9:16:14 PM):
i would really appreciate some expert opinions on curveCP. this thing
is out there for years, and all i hear is silence.

@_date: 2014-03-19 00:01:04
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Tony Arcieri (at Tuesday, March 18, 2014, 11:50:45 PM):
it would be rather premature to think about that. i doubt it is the
major obstacle here.
it is explained here:
tl;dr: stream based protocols area easy to sabotage. you damage one
packet or inject a new one, and the entire connection needs to be
reestablished. by using UDP, attacker has to damage all packets to
successfully disrupt communication.

@_date: 2014-03-19 08:47:53
@_author: =?iso-8859-1?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] We need a new encryption algorithm competition. 
Tony Arcieri (at Wednesday, March 19, 2014, 12:09:29 AM):
quote from the linked article:
"Overall, for the current implementation, the CurveCP latency penalty,
while present, seems negligibly small."
another quote from there:
"and the former possibly being handled differently in internet
it is just a configuration issue, not a protocol issue. so the
question looks like this: either we choose a more DoS sensitive
protocol or we reconfigure our routers. which one is it?

@_date: 2014-05-19 20:14:29
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] updating a counter 
Sandy Harris (at Monday, May 19, 2014, 3:58:44 PM):
here is the counterlogic to that. you already have a PRF with the
exact same blocksize as your block cipher: the block cipher itself.
so you could, if you are adamant on using less predictable counter,
just use some additional rounds in the block cipher. you can call the
first few cycles the "diffusion rounds", and the rest of the rounds
the "cipher rounds".
however. what is the design rationale behind the number of rounds in a
cipher? it is exactly to mask *any* correlation between the input and
the output. ciphers are designed to support straight up counter mode.
in other words, your cipher already contains the "diffusion rounds".
you don't need to do a thing.

@_date: 2014-05-22 23:24:42
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] How secure are hashed passwords? 
John R. Levine (at Thursday, May 22, 2014, 2:47:25 AM):
let's use this assumption, and see the numbers.
a website has a password hashing "budget". this budget can come from
two sources: user impatience and hardware cost. suppose the site has a
huge number of users, so the bottleneck will be the latter.
let's further suppose that they can afford computers up to the point
at which 100 users are authenticated per second per computer. it means
their password hashing budget is 10 computer-millisecond per user. to
go up to 20ms computer-millisecond per user, the would need to double
the hardware cost.
now suppose that the attacker can afford one million of those
computers. yeah, this is not a script kiddie, but it is good to be on
the safe side. that means they can try 100 million passwords per
second, 260 trillion passwords per month. this is over 40 bit. with
such a search space, you can cover most simple passwords, which is
majority of the users.
there is really no escape from choosing strong passwords. if your
password strength is around 60 bit, the above attack will fail. but
that is a lot. it is 6 random words or 12 random alphanumeric.

@_date: 2014-09-16 14:36:44
@_author: =?UTF-8?B?S3Jpc3p0acOhbiBQaW50w6ly?= 
@_subject: [Cryptography] Simple non-invertible function? 
there are plenty of pseudorandom permutations out there, like salsa
core (512 bit), cubehash core (1024 bit), keccak-f (25-1600 bit). you
can make a general prf out of them using:
Y = X + f(X)
where + is either xor or word-wise addition (of any word size).
disclaimer: i'm not a cryptographer, and i don't know if this
satisfies your security requirements. but i guess it does.

@_date: 2014-09-18 11:13:04
@_author: =?UTF-8?B?S3Jpc3p0acOhbiBQaW50w6ly?= 
@_subject: [Cryptography] [cryptography] Email encryption for the wider 
email is not dead, it is a zombie that walks around for at least 20
years. btw do you guys aware of IM2000?

@_date: 2015-08-08 22:52:30
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
Michal Bozon (at Saturday, August 8, 2015, 12:59:07 PM):
the reason for the SHAKE's is exactly to have something reasonable,
unlike the SHA3 instances, which are not.
as it happened, the keccak team submitted stupid parameters, because
the NIST call for submissions was unclear, and they didn't want to be
disqualified. old hash functions often have larger security against
preimage attacks than collision attacks. NIST wanted something that
has at least the same security as the SHA2 variants. so the keccak
team had to replicate the 256 bit preimage and 128 collision for the
SHA-256 drop-in. that requires 512 bit capacity.
it is especially crazy for the SHA3-512 version, which now has 512 bit
preimage security, which is for all intents and purposes a nonsensical
securit level. this comes at a terrible performance hit.
it is completely useless. you want one general security against
everything. therefore NIST proposed to change the parametrization to
have 256bit output, 256 bit capacity for the SHA3-256. that would have
a general 128 bit security. this was in agreement with the keccak
team's intent. they actually discussed it, and agreed to it. this is
how you use keccak if you are a sane person.
here comes the crypto celebrity mob. schneier and the like were quick
to jump on the "NIST weakens crypto again" bandwagon. the entire thing
was shameful. to save its nonexistent reputation, NIST backed off, and
decided to standardize the original stupid parameters. congrats to
everyone involved, djb included!
so to save the day, they added the SHAKE instances as a workaround.
they are pretty much what SHA3 should have been. if you don't
understand how a sponge works, you are very much free to use the SHA3
instances. but if you want to do actual cryptography, you should
choose the SHAKE's.

@_date: 2015-08-09 00:53:12
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
Watson Ladd (at Sunday, August 9, 2015, 12:18:38 AM):
if NIST had restarted the competition, that would have been a good
solution, though time consuming. standardizing a subpar algorithm
(parameter set) is not a good solution. crypto celebrities babbling
just to get clicks on their blogs is also not good.

@_date: 2015-08-12 13:23:32
@_author: =?UTF-8?B?S3Jpc3p0acOhbiBQaW50w6ly?= 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
well, in a sense, it is. overkill is by definition means no added
value. if it has any significant chance to be useful, we call it
security margin. overkill means you pay with loss of performance, for
nothing. it also means that some people will not be able to use it
(performance budget does not allow), so need to fall back on older or
less used algorithms. a broken algorithm is broken. it is much worse
than stupid or useless in cryptography. stupid has some value. broken
has none.
addition: afaik nist at one point considered adding a remark that
shakes are the preferred primitives. it is apparently missing from the
final document. which i find unfortunate.

@_date: 2015-08-14 22:24:28
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
ianG (at Friday, August 14, 2015, 9:15:06 PM):
and the designer did
don't forget keccak > sha3. they have a lot of interesting stuff that
is not standardized. like one-pass authenticated encryption.

@_date: 2015-08-15 20:21:45
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] SHA-3 FIPS-202: no SHAKE512 but SHAKE128; 
Phillip Hallam-Baker (at Saturday, August 15, 2015, 6:11:03 PM):
first of all using sha-3-512 seems very weird, it is the one primitive
most suffering the most severe performance hit from the overised
preimage. SHAKE256 seems to be the better option.
but the more interesting question is: why aes-gcm if you already have
keccak in there? keccak supports a one-pass authenticated encryption
scheme. getting rid of gcm seems to be a good thing in itself.

@_date: 2015-08-28 09:54:38
@_author: =?UTF-8?B?S3Jpc3p0acOhbiBQaW50w6ly?= 
@_subject: [Cryptography] AES Broken? 
i'm not entirely sure whether it is trolling, hoax-attempt or joke.
but one interesting use of this article is for the classroom. how many
errors you can collect? the student collecting the most errors wins.
any error spotted by only one student wins some extra points. etc.

@_date: 2015-07-05 22:19:25
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Best AES candidate brokenby the way that 
to be fair, AES was intended to be implemented with lookup tables.
granted, the original paper is a beautiful piece of math, but nobody
ever envisioned those calculations to be ever implemented in any real
life software. AES was created with the widespread implementation in
let's see this table:
 nist p256 naive implementation      nist p256 timing resistant impl
   AES literal implementation          AES intended implementation
both algorithms have a modern, safe but slow implementation and a fast
but vulnerable one. the fact that AES comes with a safe implementaton,
so you don't have to work it out, is nice and all, but bears very
minor practical relevance.

@_date: 2015-07-05 22:25:28
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Best AES candidate broken 
Tony Arcieri (at Sunday, July 5, 2015, 7:47:04 PM):
it is an interesting question what is cache timing (or other side
channel) resistant. in particular, salsa is ARX, which means it has
addition (32 bit). addition is a rather complex operation, and
on very low end systems, it might not be constant time, or can be
vulnerable to power analysis.
so it is pretty much a question of how far we are going to go. for
example keccak surpasses salsa in that regard.

@_date: 2015-07-06 20:16:55
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Best AES candidate brokenby the way that 
David Johnston (at Monday, July 6, 2015, 7:23:39 PM):
most definitely. now that we understand the problem, good crypto
primitives are designed in a way that the naive implementation is the
safe implementation. or at least it is easy to get it right, and it
isn't a huge compromise. that is the design philosophy behind salsa,
curve25519 or keccak, among others.
but aes is old, and nobody had a clue about side channels back then.
pity many people in the field still don't know about them.

@_date: 2015-07-23 23:48:55
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Whitening Algorithm 
Rob Seward (at Thursday, July 23, 2015, 4:50:03 AM):
i know you want review of your algo, but instead i give an idea that
might work for you.
use a small cryptographic sponge in duplex mode, for example
keccak[200, r=8] reduced to 6 rounds. this sponge instance has 96 bit
security, and requires only 25 bytes of memory. this is a very safe
solution, although of course a magnitude slower than yours, and also
needs a fair bit of code.

@_date: 2015-11-03 10:50:08
@_author: =?UTF-8?B?S3Jpc3p0acOhbiBQaW50w6ly?= 
@_subject: [Cryptography] Why Rijndael ? 
1, you specifically asked about NIST's rationale. who else could give
NIST's rationale better than NIST?
2, it is pre-9/11. back then NIST was much more trustworthy.
3, you were given a detailed description with a lot of information in
it. you can read, and assess yourself. whose opinion would you trust?
if you want a spiritual leader, go to schneier's blog, and become a
fanboy. let's see how far that gets you.

@_date: 2015-11-07 21:56:55
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
ianG (at Friday, November 6, 2015, 2:33:43 AM):
this sums up pretty much my problem with this "be on the safe side"
resuing the key is something like
AES(K, M) || HMAC-SHA256(K, M)
in contrast, if we derive separate keys from some master key, it
AES(SHA256(K||1), M) || HMAC-SHA256(SHA256(K||2), M)
how is that more secure? granted, there is a possiblity that SHA256 is
broken in a way that does not affect the second construct. but is this
logic would suggest throwing in as much crap as we can afford, like
replacing AES with AES(AES(AES(x))) and such. but nobody does that.
also, the second method fixes some user errors, like related keys. but
such problems should be solved at the root, and not masked by weird

@_date: 2015-11-08 18:43:34
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
John-Mark Gurney (at Sunday, November 8, 2015, 6:29:44 PM):
this was not the point at all, and you can fix it for yourself:
AES(K, M) || HMAC-SHA256(K, AES(K, M))
but as you brought it up, the notion that encrypt-then-MAC would be
the secure option over mac-then-encrypt is just as false the other
weird recommendations we often hear. in fact they are both equally
secure if implemented right. the only difference is when you implement
them wrong, you are more likely to get away with it with enc-then-mac.
in particular, since you detect malicious activity early, you simply
don't do most of the processing, and thus avoid any bugs or
insecurities there. but it is a patchwork. the implementation must be
solid regardless.
just to give an example, keccak guys recommend a single pass
authenticated encryption. this is encrypt-and-mac, but it is as secure
as it gets. also, we have the CAESAR competition, in which many (all?)
candidates are also encrypt-and-mac. so obviously, actual
cryptographers don't know about the enc-then-mac rule, only the
blogosphere does.

@_date: 2015-11-09 00:24:49
@_author: =?iso-8859-15?Q?Kriszti=E1n_Pint=E9r?= 
@_subject: [Cryptography] Literature on reusing same key for AES / HMAC? 
Jerry Leichter (at Sunday, November 8, 2015, 10:37:20 PM):
i can not imagine an article less relevant to actual cryptography.
with no exception, our sym enc algos are practically pseudorandom
functions/permutations. as soon as you can use the random oracle model
to approximate your primitives, this entire reasoning evaporates.

@_date: 2015-11-12 23:35:00
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] Post Quantum Crypto 
i would like to hear the opinion of someone who actually knows some
to my knowledge, most quantum experiments require many many repeated
attempts, simply because the detection rate is poor. the more stuff we
need to measure, the less chance a reading will be the actual result,
and not blank or noise. like for example if detecting photon
polarization has detection rate of 1%, then a simultaneous pair
detection will have 0,01%, and a triple detection will have 0,0001%.
1, is this the case with d-wave and quantum computers in general?
2, if so, is the detection rate / repeat number changes exponentially
with the number of qubits, like in the photon example above?
because if these are true, the detection probability might be the main
issue with QC. it can compute its thing in O(1), but we need O(2^n)
measurements, which is not an achievement over classical.

@_date: 2016-04-04 19:26:48
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] On the Impending Crypto Monoculture 
Florian Weimer (at Sunday, April 3, 2016, 11:12:11 AM):
compare oranges to oranges. aes vs chacha20, tls vs minimalt. minimalt
does not solve the ultimate pki problem, but one might argue that tls
does that terribly as well.

@_date: 2016-02-07 17:42:15
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] New block cipher competition 
Henry Baker (at Saturday, February 6, 2016, 3:29:01 PM):
that is post-what? post transdimensional alien invasion? post-magic?

@_date: 2016-02-08 10:16:15
@_author: =?UTF-8?B?S3Jpc3p0acOhbiBQaW50w6ly?= 
@_subject: [Cryptography] New block cipher competition 
i don't see the appeal. what is the benefit over 256 bit? i can see
two things maybe:
1, performance. but it kinda tops at 512 or 1024 bits these days. why go higher?
2, hides more information in disk encryption like scenarios where you
can't store nonce/IV. personally i don't think there is future to this
kind of encryption, but that's another issue. the main issue is, why
would we want a special block cipher for this reason, instead of some
hotfix mode of operation, like the abandoned elephant diffuser was.

@_date: 2016-03-29 22:58:03
@_author: =?utf-8?Q?Kriszti=C3=A1n_Pint=C3=A9r?= 
@_subject: [Cryptography] On the 'regulation proof' aspect of Bitcoin 
i'm not sure this topic belongs to this list, but that's what the
moderators are for :)
Phillip Hallam-Baker (at Tuesday, March 29, 2016, 6:17:02 PM):
there are two aspects of any policy: 1, what do you need to do, what
steps are neccessary, and 2, what is in your propaganda toolbox, how
can you justify.
all we can do to resist the chokehold of state power is 1, to make it
more difficult for them to do it, and 2, to make fuss, so maybe their
propaganda does not work all that well.
the first is purely technical, and bitcoin does a fairly good job,
although not perfect. it is totally decentralized, and it runs on many
many computers. to ban it, you need to enact very intrusive laws. it
is also roboust, so they can't secretly kill it, as the copyright
industry killed the emule network.
on the downside, bitcoin is too open and to trackable, so punishment
or taxation can be a real threat. should it be more anonymous and more
"dark", such threats would become weightless.
as for making fuss, i see much less options. bitcoin is *not* popular.
stop believing it is. many people sees it as dangerous and mysterious.
the would not miss it at all if its gone.
this game is not over.
