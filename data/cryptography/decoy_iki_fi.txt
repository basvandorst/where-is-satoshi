
@_date: 2001-10-02 19:23:23
@_author: Sampo Syreeni 
@_subject: Best practices/HOWTO for key storage in small office/home   
I think one of those tiny USB flash memories IBM makes would be even
better. Sufficient capacity, no moving parts, extremely discreet.
Or integrate some computing power into those IBM thingies, and use
remotely keyed encryption. Enough power is available through USB so that
you don't have to end up with battery power.
This isn't available now, though. I *really* hope IBM would take the hint.
It would make key management a whole lot easier.
Sampo Syreeni, aka decoy - mailto:decoy at iki.fi, tel:+358-50-5756111
student/math+cs/helsinki university, openpgp: 050985C2/025E D175 ABE5 027C 9494 EEB0 E090 8BA9 0509 85C2

@_date: 2001-09-24 23:37:17
@_author: Sampo Syreeni 
@_subject: <nettime> "Pirate Utopia," FEED, February 20, 2001  
It also presumes that people use the precise same steganographic algorithm,
I think. I've haven't seen the paper, though -- what's said there about the
issue of multiple stego methods out there?
Sampo Syreeni, aka decoy, mailto:decoy at iki.fi, gsm: +358-50-5756111
student/math+cs/helsinki university, openpgp: 050985C2/025E D175 ABE5 027C 9494 EEB0 E090 8BA9 0509 85C2

@_date: 2002-08-02 03:31:12
@_author: Sampo Syreeni 
@_subject: Challenge to David Wagner on TCPA 
So, is there some sort of guarantee that the transfer of control won't be
stopped by a check against cryptographic signature within the executable
itself, in the future? That sort of thing would be trivial to enforce via
licencing terms, after all, and would allow for the introduction of a
strictly limited set of operating systems to which control would be
transferred. I'm having a lot of trouble seeing the benefit in TCPA
without such extra measures, given that open source software would likely
evolve which circumvented any protection offered by the more open ended
architecture you now describe. Such a development would simply mean that
Peter's concern would be transferred a level up, without losing its
relevance. I'd also contend that this extra level of diversion is
precisely what TCPA, with its purported policy of "no trusted keys" aims
Again, such values would be RE'd and reported by any sane open source OS
to the circuitry, giving access to whatever data there is. If this is
prevented, one can bootstrap an absolutely secure platform where whatever
the content provider says is the Law, including a one where every piece of
runnable OS software actually enforces the kind of control over
permissible signatures Peter is so worried about. Where's the guarantee
that this won't happen, one day?
At the hardware level, yes. At the software one, it probably won't be,
even in the presence of the above considerations. After you install your
next Windows version, you will be tightly locked in with whatever M$
throws at you in their DLL's, and as I pointed out, there's absolutely no
guarantee Linux et al. might well be shut out by extra features, in the
future. In the end what we get is an architecture, which may not embody
Peter's concerns right now, but which is built from the ground up to bring
them into being, later.
More generally, as long as we have computers which allow data to be
addressed as code and vice versa, the ability to control use of data will
necessarily entail ability to control use of code. So, either we will get
systems where circumventing copyright controls is trivial or ones where
you cannot compile your own code. All the rest is just meaningless syntax.
In that light I bet you can guess why people are worried about TCPA and
its ilk.

@_date: 2002-02-02 22:47:16
@_author: Sampo Syreeni 
@_subject: FW: update.575 
[An article on categorizing textual strings by appending them on
reference documents and measuring aggregate compressibility snipped.]
This shouldn't be a big surprise, considering how close to the estimated
entropy of various sources current compression algorithms get. In essence,
compressors are statistical learners, and classification problems can be
formulated as partitionings based on statistical similarity. I just wonder
if the overhead of doing a significant number of compression runs against
known sources isn't a bit expensive compared to current methods of
Sampo Syreeni, aka decoy - mailto:decoy at iki.fi, tel:+358-50-5756111
student/math+cs/helsinki university, openpgp: 050985C2/025E D175 ABE5 027C 9494 EEB0 E090 8BA9 0509 85C2

@_date: 2002-02-12 01:09:01
@_author: Sampo Syreeni 
@_subject: Where's the smart money? 
I see no reason why sufficiently reliable RFID notes (say an MTBF in
average use of around 5; not technologically infeasible, yet around what
current print-only notes can take at max) could not be handled this way.
But if this is really such a problem, one would expect the issuer to be
able to invest a fair amount of money per bill in circulation into
verification methods in excess of what you'd typically see in a grocery
store -- a reasonable MTBF and enough circulation through the issuer would
lead to few notes getting into a bad shape to be passed this far up the
chain. Thus, failed notes could be replaced at a cost not much higher than
that incurred by routine check-ups, only with a greater delay.
Besides, there's a point in invalidating failed bills -- if this is not
done, where's the incentive for people to keep the stock in shape? A
monetary economy, by itself, *can* adapt to lost bills via deflation, and
bills going invalid is something nobody really wants to experience.
Also, it is likely that deflationary pressures arising out of economic
growth will completely drown out any effects lost notes might have on the
larger economy. The implication is, wear and tear of bills can be
accurately analyzed by treating them as a slowly devaluing physical good,
and the usual efficiency arguments apply.
Sampo Syreeni, aka decoy - mailto:decoy at iki.fi, tel:+358-50-5756111
student/math+cs/helsinki university, openpgp: 050985C2/025E D175 ABE5 027C 9494 EEB0 E090 8BA9 0509 85C2

@_date: 2002-01-20 01:05:32
@_author: Sampo Syreeni 
@_subject: password-cracking by journalists...  
If something, this would lead me to believe there is less redundancy in
what *is* written, and so less possibility for a dictionary attack.
At least Unicode codes these as the same codepoint, and treats the
different forms as glyph variants. Normalizing for these before the attack
shouldn't be a big deal.
This would facilitate the attack, especially if the root form is all that
is written -- it would lead us expect shorter passwords and a densely
populated search space, with less possibility for easy variations like
Sampo Syreeni, aka decoy - mailto:decoy at iki.fi, tel:+358-50-5756111
student/math+cs/helsinki university, openpgp: 050985C2/025E D175 ABE5 027C 9494 EEB0 E090 8BA9 0509 85C2

@_date: 2002-07-12 01:08:25
@_author: Sampo Syreeni 
@_subject: FC: Politech challenge: Decode Al Qaeda stego-communications! 
More likely "everybody who's anybody communicates via the Web".
Indeed. Even when that isn't a guarantee in the least with steganography,
it's far more likely that this precise uncertainty has seriously caught
the mainstream press in a cycle of unwarranted paranoia and delusion. Even
when there is no guarantee, that is certainly no guarantee that there is
indeed stego imbedded in some online images.
I'd really like to know who started this stream of
web-stego-osama-terrorism articles -- I wouldn't be in the least surprised
to know that the original source was someone closely affiliated with some
War on Terrorism people.
More seriously, most people have little idea of how industrial strength
steganography works. They consider the race against "terrorists using
stego" as a heroic battle of horsepowers which the side better equipped
with intellectual or economic assets will eventually win.
In practice, steganography isn't like that. Instead, there are sound
information thereotical bounds which limit the performance of any observer
whatsoever (mostly the best source model that can be created). It is quite
possible that even the best equipped counter-terrorist agency will remain
ignorant of terrorist communications taking place, provided that the
above-mentioned information theoretical bounds are adhered to. That's
simply because when they are, the presence of stego is unknowable. Period.
The larger public does not (and apparently cannot) understand this. In
consequence, it will be ready to pour formidable financing, worry,
resources to combat a threat which might not even be there. It's also
difficult in the extreme to see whether the worry has been incited on
purpose, or whether it is just a consequence of an initial reporter
failing to see the point at a crucial time. But whatever the reason for
the current hysteria, the full picture of stego is a very clear cut one.
In the ideal world of classical, Shannonian information theory,
steganography basically becomes a race in acquiring source statistics.
Whoever knows those statistics the best can best mimic and find the
redundancy in them. When such redundancy exists, it can be exploited to
transfer information unseen by anyone "less in the know", by choosing
among alternatives permitted by the redundancy. This precise idea also
applies to different levels of redundancy detectable by statistical source
models of different acuity. Those who have less accurate statistics will
be fooled by a stego transmission because they cannot discriminate between
entropy contributed by the source and that driven by the stegoist. To
those who have more accurate source models at hand, the task of telling
stego from non-stego becomes a simple Bayesian discrimination task, with a
certainty level dictated by how much information was sent by the stegoist
and the acuity of the model. In our finite world the latter is bounded by
the amount of information in all communications known to the observer.
In the case of stego methods widely distributed (say, LSb switching in
images) it is fairly easy to discriminate between stego and non-stego.
But that's only when we know that this particular method has been used. In
theory, we get a whole array of increasingly sophisticated source models
and steganographic codings utilizing them. Whoever has the best source
model can utilize the smallest redundancies and so can encode information
at the most unnoticeable level. The tradeoff is between noticeable
modifications to the source and steganographic bandwidth. Only when one
has a better model can one discriminate between stego and non-stego
transmissions, and only the best detection method and best source model
set the bounds for total stego bandwidth.
In practice, models aren't very exact. E.g. pictures can have all kinds of
content not adherent to what one might call the norm. In consequence,
images always contain a lot of redundancy. Especially so if we factor in
all the real life concerns, like compression/download time tradeoffs in
image transmission -- even if we know the perfect statistics for the
source image, it is impossible to tell whether statistical anomalies are
due to excessive compression or steganography being present. And so on.
It's quite possible to construct low bandwidth steganographic encodings
which are with high probability totally undecipherable into the known
So, in the end, there is no way to know whether a given communication has
embedded stego. It's usually fairly easy to tell whether a specific method
has been used if one has a good source model, true, but in the general
setting, neither does such a model exist nor do we have a guarantee that a
specific method has indeed been used. We're left out in the cold, not
knowing whether a communication has taken place.
That, then, becomes both the beauty of steganography and the beast of
inciting countless clueless reporters to suspect stego being present in
places it isn't. If one cannot know, one cannot know. A number of
reporters ought to be taught about knowable versus true/false if we're
ever to get rid of this web-stego-osama-terrorism nonsense.
Sampo Syreeni, aka decoy - mailto:decoy at iki.fi, tel:+358-50-5756111
student/math+cs/helsinki university, openpgp: 050985C2/025E D175 ABE5 027C 9494 EEB0 E090 8BA9 0509 85C2

@_date: 2002-07-28 02:18:23
@_author: Sampo Syreeni 
@_subject: building a true RNG 
I would be lead to pigeonhole; still, this does seem like an eminently
teachable moment to me.
However, what we're working with in the case of a typical RNG isn't
functions between finite buffer-fulls of data, but functions between
infinite sets of entire bitstreams which need to be implemented within a
finite memory constraint. Whatever the algorithm, it can have state. I'm
thinking that is the reasoning which leads people to think that the
problem is simple. In this framework one can very well gather statistics
and try and normalize them based on what one's seen so far. (The
minimalist example here is bias removal.)  After that, what goes into the
hash is (approximately) white and even with minimal assumptions (i.e. the
hash is a one-way permutation) our RNG would seem to approach ideality.
True, such an approach will always become a race in modelling the
statistics. But still, I would think the generator has the advantage if
sufficient margin is left for error. (That is, hash some constant times
the number of bits containing the entropy you suspect to be there, based
on the best model you've got.)

@_date: 2002-07-28 12:21:56
@_author: Sampo Syreeni 
@_subject: building a true RNG 
It's quite possible I don't get it in the finite case, but I don't see why
you can do this either. With statistical adaptation the n on the output
side will change based on the input stream, and |Y| cannot be both
constant and finite.
An example: presume we take a simple first order statistical model. If our
input is an 8-bit sample value from a noise source, we will build a 256
bin histogram. When we see an input value, we look its probability up in
the model, and discard every 1/(p(x)-1/256)'th sample with value x. When
this happens, the sample is just eaten and nothing appears in the output;
otherwise we copy. You *can* model this as a function from sequences to
sequences, but certain input sequences (like repeated zeroes) will make
the algorithm wait arbitrarily long before outputting anything. Hence, n
on the output side cannot be finite. (This might "lose entropy" in our
earlier, fuzzy sense, but it's near what I'm thinking. Once we discard the
right samples, higher order models would work just as well.)

@_date: 2002-07-28 12:38:38
@_author: Sampo Syreeni 
@_subject: building a true RNG 
[Answering to my own mail. Sorry.]
Actually the pedantic solution would be to put an arithmetic
compressor/coder between the input and output, using the best model we've
got. That still leaves model adaptation to be dealt with, but if we
discard a sufficient number of output bits at start (estimable from the
model), we *will* end up with (very nearly) flat statistics on the output.
Asymptotic optimality and all that... (The qualification comes from
limited precision arithmetic.)

@_date: 2002-10-01 21:24:08
@_author: Sampo Syreeni 
@_subject: Real-world steganography 
I'm not sure about HDCD as a technology, but the principle is sound. If we
can compress sound transparently, we can also transparently embed quite a
lot of data into the part which is perceptually irrelevant. We might also
depart with perceptual equivalence and go with perceptual similarity
instead -- e.g. multiband compress the audio, and embed data which allows
us to expand to a higher perceptual resolution. Whatever the
implementation, putting data in the gap between statistical (i.e.
computed against a Markov model) and perceptual (against a perceptual
similarity model) entropy which compensates for some of the perceptual
shortcomings (like total dynamic range) of a particular recording
technology seems like an excellent idea.
However, applications like these have very little to do with steganography
proper. In this case, we can (and want) to fill up the entire gap between
statistical and perceptual entropy estimates with useful data, leaving us
with signals which have statistical entropies consistently higher than
we'd expect of a typical recording with similar perceptual
characteristics. That is, the encoded signal will appear manifestly random
compared to typical unencoded material from a similar source, and we can
easily see there is hidden communication going on. Such encodings will be
of little value in the context of industrial strength steganography used
for hidden communication.
Steganography used in the latter sense will also have to be imperceptible,
true, but but here the entropic gap we're filling is the one between the
entropy estimates of our best model of the source material vs. that of the
adversary's. Be the models Markov ones, perceptual, something else, or
composites of the above. Consequently the margin is much thinner
(bandwidths are probably at least a decade or two lower), and the aims
remain completely separate.
Consequently, I don't believe encodings developed for the first purpose
could ever be the best ones for the latter, or that HDCD-like endeavors
really have that much to do with the subject matter of this list.

@_date: 2003-01-25 03:30:13
@_author: Sampo Syreeni 
@_subject: [IP] Master Key Copying Revealed (Matt Blaze of ATT Labs) 
One should also note that this particular problem doesn't affect disc
wafer designs, like ABLOY's. On other fronts such designs fail as badly as
pin tumbler one, of course. I don't know about the newer designs, though

@_date: 2003-06-03 23:13:36
@_author: Sampo Syreeni 
@_subject: CDR: Re: Maybe It's Snake Oil All the Way Down 
I don't think the cost of listening into a single call is the primary
issue, regardless of transmission technology. There are extra costs to
tracking a mobile user, true, but from the standpoint of law enforcement
agencies, these costs are rather minimal. (From the standpoint of a
private eavesdropper the difference is much greater, since the subject is
mobile and one cannot take advantage of the centralized points of failure
of the mobile communications network.) Rather it's the fact that the Big
Brother doesn't have the necessary total funds, and so doesn't listen into
a considerable proportion of calls as a whole.
The implication is, as the costs go down, it becomes possible to listen
into more calls, and the fear goes up. Especially so when speech
recognition and subsequent pattern analysis become computationally
feasible at a wider scale. When this is the case, it should be expected
that the use of crypto goes up. But right now, even people who "have
something to hide" do not perceive cleartext communication to be a risk
worth expending resources to thwart.
Why? As I see it, this is fundamentally an economic question, not a
technical one. It's about the risk of somebody listening in, taking notice
and acting adversely to the talker's own interest, versus speaking what
one wants without having to take expensive precautions. Currently such
risks mostly materialize when one *truly* has something to hide, that is,
one talks about something criminal, there is reason to believe law
enforcement agencies might be listening and one talks in terms which will
reasonably lead to conviction in the right circumstances.
The probability of that happening is surprisingly low, especially from the
security professional's somewhat paranoid viewpoint.
True. But in average people will shortly notice the development, and
prepare from there on. So far they haven't, and for a good reason -- such
surveillance is far too uncommon and inconsequential to actually be
Of course, if encrypted communications become dirt cheap and are properly
spun in the media, people will take on -- negligible cost combined with a
serious threat thwarted is a sure sell. This would be good, too, since the
risks of insecure communication tend to be sizable and also materialize
rarely -- those are precisely the circumstances in which people suffer
from the worst errors of judgment. But present, I think the costs
of real security seriously outweight the benefit, for most people. That
might change as much as a result of what people themselves do/think, as as
a result of what the Man, the Hacker or the technologically sophisticated
Neighbour does. Until such a change, crypto is, sadly, a fringe thing. No
matter how it's used.
Or just your neighbour. I mean, it doesn't take a cop, or a spy, or even a
an immoral person to listen in on you. All it takes is a little curiosity.
There's plenty of that going around.
couple of months of full-time effort. In no case more than half a year at
full steam.
The question is, who has a) the time, and b) the energy? Few do.
If it's feasible to encrypt the phone-to-base station link, it's equally
feasible to encrypt end-to-end. It's also cheap enough to do what PGP et
al. do, that is, combine public key methods with symmetric ones to achieve
both efficiency in in-band operation and convenience with key
distribution. Thus, there's no need to distinguish E2E encryption from the
rest, even in mobile, low-power equipment. If you need security, you might
as well do it right.
Try GSM's data features. They have extra error correction, true, and so
lower rates than the primary voice codec, but combined with the kinds of
high end voice codecs as the GSM halfband one, you can fit perfectly
usable speech within the data standard. After that, you don't even have to
worry about modulation -- you can just send bits. Fitting strong crypto
into that is ridiculously easy, and also relatively cheap.
Huh? Bare on-the-air encryption only proofs you against nosy neighbours
and the attendant probability of one of them giving you in for something
illegal. Those probabilities are quite low, compared to what "someone with
something to hide" would fear from law enforcement. E2E protects you
against both the threats, at little, no, or negative extra cost -- if your
chosen mobile standard permits access to a variant of the basic digital
interface, you can design you own protocol, usually with no more than half
the bitrate lost to FEC. Better voice codecs tend to be able to deal with
that, as witnessed by GSM's half rate codec. Consequently E2E's a pure win
compared to trusting your mobile provider.
But it also needn't be more expensive. In fact it's likely that in digital
incarnations of the mobile phone system, E2E's actually cheaper than the
alternative protocol change, provided the standard permits access to some
variant of the basic, digital interface. If you can send numbers, crypto
is easy to add on, it's not too difficult to add a proper, low-rate voice
codec, and so you have both intelligible voice and industrial strength

@_date: 2003-05-01 14:58:26
@_author: Sampo Syreeni 
@_subject: eWeek: Cryptography Guru Paul Kocher Speaks Out 
Yes, but we can also do with less.
Say we want to attack a maximally redundant inaudible watermark in audio.
The first step would likely be to bring the best variable rate audio codec
we can find to bear on the file. That'll seriously limit the stego
bandwidth available, and with a good probability kill shorter term
redundancy in the embedded mark. After all, the goal of lossy compression
is to remove as much inaudible redundancy as possible and substitute it
with arbitrary noise upon decompression. After that there's still a slim
margin of redundancy which hasn't been removed and can contain stego data,
but it will take a signal considerably longer than the original to detect
the mark with a decent rate of false positives. Mounting a successful
averaging attack no longer requires as many copies -- you might well get
along with two or three per tune. Where do we get those copies cheaply?
surrender individual, complete, watermarked copies which would facilitate
extraction. Two simple approaches I can come up with (for uncompressed
data) are
 1) enforced swarming which guarantees that any downloaded copy of a song
    will always be composed of a random selection of blocks drawn from at
    least n originals, and
 2) building the averaging operation into the network itself, so that only
    copies averaged over at least n originals are actually retrievable.
So I'll agree with Nomen Nescio. It's difficult to see how all the
different kinds of holes cooperative attacks bring about might be plugged
when the intelligence required can be freely shared in the form of
specialised P2P software.

@_date: 2006-04-23 02:13:11
@_author: Sampo Syreeni 
@_subject: "time reversal acoustics" 
Passive, linear, time-invariant, nondissipative systems are essentially allpass networks which simply redistribute energy in time and outputs. Like all LTI systems they can be completely specified in terms of their response to impulses but in this particular case the system is also extremely easy to invert: you just reverse both the system (outputs become inputs) and its impulse response (first sample becomes the last). Systems which respond approximately this way abound in acoustics, optics, circuit design, digital signal processing and who knows where.
In time reversal acoustics the input is typically some pointlike acoustic phenomenon like a spark, the output is a set of microphone signals and the system is some propagation medium with little absorption but an unlimited amount and complexity of internal reflections. The response of such a system in time and space can be accurately compared to what spread spectrum modulation does to a signal in the frequency domain: energy is spread in a complex but essentially linear fashion. Inverting the system by time reversion recompacts the energy at the original source so that the reverse system can be used as a spatially and temporally randomized focusing device/lens despite its complexity. And since there is no essential limit to the complexity, area or (to a lesser amount) temporal extent of the system response, the achievable spreading can be large enough to hide the decompacted signal under the local noise floor of the surrounding acoustic environment while still allowing recompaction at the focal point. Such spatial-temporal de/spreading is probably the cryptographic application that was imagined; its prime weakness is probably in the linearity of the So in a sense this is nothing new. Direct sequence spread spectrum has been used for decades now, systems as different as synthetic aperture radars, beamforming sonars and laser pulse compressor gratings do energy compaction all the time, and holographic crypto -- another highly dispersive physical system with a unitary response -- exists as a concept in literature. At least to me it seems that the most interesting parts of this are the realizations that a) highly dispersive media can be used as somewhat lossy collectors/focusing lenses with any point as the focal point provided their response can be inverted, b) such physical operations are available in the acoustic domain too and c) it might be possible to construct such media in a difficult to duplicate, one-shot fashion and then use them as computationally heavy black boxes in algorithms. (The latter part is analogous to efforts at creating physical one-time tokens for watermarking purposes.)

@_date: 2008-05-10 02:07:58
@_author: Sampo Syreeni 
@_subject: How far is the NSA ahead of the public crypto community? 
I wouldn't say algebraic geometry is such a pure and abstract specialty in this context. It has its roots firmly planted in multivariate polynomial algebra, and even at that time it was quite clearly the field that was most intimately connected with mechanistic solutions to groups of nonlinear polynomial equations over finite fields. Which then is exactly what a mathematician sees when presented with a symmetric cryptosystem to break. As evidence of that, Hilbert's basis theorem (which underlies Groebner bases, which in case relinearization and the bunch are an independently discovered special case of) was well known and appreciated at that time.
So, even if elliptic curve cryptography became later, the broader theory of algebraic geometry was *certainly* relevant to crypto even then, and should have easily been seen to be so.
Quite so. I think this is where one should be seeking for the signs of differential advantage. Not the broad fields of mathematical expertise which plausibly could have been acquired by the NSA for any of a number of reasons.
Band agnostic, keying rate adaptable and error tolerant algorithms in this department most likely fall in the advanced category even today, especially if computationally thrifty. I've certainly never seen anything of the sort in what DSP literature I'm aware of.

@_date: 2008-05-12 22:56:56
@_author: Sampo Syreeni 
@_subject: root kits in SMM mode 
In the information preservation circles which of course mind failure and error tolerance, they they talk about "hard/fast vs. soft/slow" failures. Soft failure implies the sort of slow, creeping, difficult to detect failure mode that constitutes corrosion, bit rot and the like. A hard fail is one that either happens, catastrophically, or doesn't happen at all. I.e. a hard fail is the prototypical "digital" failure mode and the soft one is its "analog" counterpart. For example the use of error correcting codes has lead to hard drives and high end microprosessors being the hard, binary failing type, by design, whereas voice communication seems very difficult to make anything but "soft".
That happens because CPU's are designed to fail fast and to be replaced, whereas analog tape degrades controllably and is gone when it is, gradually but expectedly. RAID tries to stage the process so that a) we can detect when harm was done, b) replace the faulty component, and then c) fully reconstitute the data with perfect ditital fidelity before irretrievable loss has even happened. Even that is predicated on the assumption that the distribution of loss is bounded in some sense, which it usually isn't, but at least we're approximating the relevant probabilistic loss-of-value curve with more precision...
I think the distinction between soft and hard failure well describes the failure mode that is inherent to secure(d) hardware, protected modes and the like. Here, there's a hard obstacle to be beaten if you want to surmount the protective cover; otherwise it just works and if it doesn't, then it fails completely. The first barrigade is what the designers concentrated at, in order to protect the soft, all-powered interior core/trench of the supervisor mode.
They wanted to produce an insurmountable obstacle, and don't get me wrong, perhaps they even succeeded at that. Perhaps they actually managed to prove the security under plausible assumptions.
But then, perhaps not, and after that the soft underbelly always lends itself to misuse. Even the designers of RMS Titanic didn't consider that one final, fatal failure mode, which then bought the ultimate, historic failure upon them. The design certainly was very good and based on the entire body of existing knowledge at the time, accessible to the designers. A priori it should have been more than sufficient to keep the ship afloat, ex post it suddenly wasn't.
In the security/crypto frame of thought, I would translate that as two separate things, a la Frank Knight (in "Risk, Uncertainty and Profit"): a) you'll want to control the known, quantifiable, statistical risk so that the costs and benefits equal out, and b) defend against what you genuinely do *not* know ("uncertainty") by building in additional categorical protections.
In the security context, and in both cases, you'll also want to follow economics as best you can. With regard to risk, you'll want to follow the continuous marginal effort/benefit curve of attack. It's always going to be nonlinear, so any fixed, single counter-measure likely won't be enough to cover all of the bases, because it would lead to a step/kink in your countervailing curve. Usually you also cannot device easy countermeasures that yield continuous and comparable incentives. So, you'll have to approximate from below; that leads to a number of counter-defences approximating the threat curve from below.
As for uncertainty, that cannot be quantified by definition, but we do have qualitative/categorical defences against it. Like using "different modes of defense/thought". "Distributed concerns." Quantitative burdens on distributed clouds of executives, arranged to be "less susceptible to influence than the general social network." There we have to apply the (very underdeveloped) theory of coalitional game theory, social choice, public choice and what not.
Finally, to return to the ground, in the scenario Perry describes, the problem is that nobody even *tried* categorical defense *beyond* the first breach. It was just assumed that the one impregnable wall (the one between SMM and the rest of the x86 modes) was enough and perfect. In reality, in order to defend against what you do *not* know, you'll need multiple walls, of different kinds and degrees of fortification, or a fundamentally different sort of authorization structure (which won't *remove* the problem, but *will* shift the threat curve in the single user, home application).
That eventual privacy catastrophe might never come about, but as Perry says, in the current state of things, it's bound to happen over and over again. As it just happened with SMM.

@_date: 2009-03-03 20:58:30
@_author: Sampo Syreeni 
@_subject: Judge orders defendant to decrypt PGP-protected laptop 
That is also why multi-level security and/or steganography exist. And why, eventually, every court order will mandate randomization of all data that wasn't decryptable. And why people will design stealthy methods of signaling to their disk that such deletion orders are to be disrespected. And why such drives will be forthwith banned. Et cetera, ad nauseam.
So it goes.

@_date: 2010-08-01 18:46:29
@_author: Sampo Syreeni 
@_subject: Hashing messages with lengths between 32 and 128 bytes is one 
Do not forget protocol replies either. The best protocols usually cut down the overhead to a minimum, either because of analyzability, or more commonly because of speed/latency. That means that cryptographically securing the *very* best protocols also implies various cryptographic primitives on very short messages.

@_date: 2010-07-27 05:40:07
@_author: Sampo Syreeni 
@_subject: A mighty fortress is our PKI 
I agree. But do we then have any quantitative research on how bad this sort of sharing really is, in excess of the basic cryptographic vulnerability? Does the social network research of recent years yield any numbers, for instance?

@_date: 2010-07-27 06:53:44
@_author: Sampo Syreeni 
@_subject: A mighty fortress is our PKI 
I'm not sure either. This is just a gut feeling. But *certainly* sharing as such can be quantified somehow, and numbers could be derived about the social network where sharing takes place. Then *certainly* the results can be correlated with their effect upon, say, average numbers of breaks/leaks/bad-stuff, over whatever statistical horizon we might have with those.
Quite a lot of stuff can be quantified, even if the relevant distributions prove rather spread out.

@_date: 2010-07-28 04:25:47
@_author: Sampo Syreeni 
@_subject: A mighty fortress is our PKI 
Personally what I wonder about is that there is precious little research on how difficult and/or worthwhile it is to circumvent the formal, mathematical crypto-stuff, as a whole. We all know that is bound to be the hardest part if somebody wants to hurt you, so why center your attention there? Why not go for the soft flesh instead?
Perry already caught me on that basic security questionnaire, when I asked for numbers and couldn't answer. Now I'm thinking the proper figure should probably be "ratio of investment into a security break, against benefit from the same". Including existing safeguards against said break. That should be fair enough, and should help us optimize against future security breaks at the margin, no?

@_date: 2010-07-29 03:25:40
@_author: Sampo Syreeni 
@_subject: A mighty fortress is our PKI, Part II 
Or in economic terms, asymmetric information. Can we for example learn something from the way insurers and the like who've been dealing with that for centuries solve the problem? And then apply it to protocol/market design?

@_date: 2010-09-03 19:16:00
@_author: Sampo Syreeni 
@_subject: RSA question 
I would imagine it'd be the result of fitting some reasonable exponential to both keylengths and extrapolating, which then of course blows up...for once *literally* exponentially. ;)

@_date: 2011-08-10 03:02:28
@_author: Sampo Syreeni 
@_subject: [Cryptography] Crypto being blamed in the London riots. 
Thus, why not turn the Trusted Computing idea on its head? Simply make P2P public key cryptography available to your customers, and then bind your hands behind your back in an Odysseian fasion, using hardware means? Simply make it impossible for even yourself to circumvent the best cryptographic protocol you can invent, which you embed in your device before ever unveiling it, and then just live with it?
Thus the need for credible precommitment, TC-style, at the hardware

@_date: 2014-04-02 01:27:48
@_author: Sampo Syreeni 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
Since I'm not a real cryptographer, take the following with a grain of salt. But it does constitute a real protocol for this sort of thing, so maybe you or somebody else can derive some benefit from it.
Essentially, you want a trusted source of well behaved bits you can be sure of isn't under suspicion. The binary development of pi serves well there for at least three separate reason. First, it's a fundamental constant with a history of study considerable longer than that of the other likely candidate, e. Secondly, it was proven transcendental early on, and is conjectured to be normal, with a reasonable chance of one day being proven so. And third, spigot algorithms exist to look arbirarily far into the development for verification purposes, with a bearable complexity profile.
That means you could probably take any thus far unused part of the development as your source. But I think it might just pay off to put in an extra step which guarantees that not only do you have to use the next portion nobody used yet, but to also expend an amount of effort doing so. Also, you probably shouldn't be able to choose where you start your extraction, at leaast if you get to choose how many bits you're going to extract at the same time. And of course the effort should be roughly (asymptotically) equal, whether you took two bits separately or a single two bit chunk.
Under those constraints the best I can do is to encode the number of bits you need using as early as possible a universal code, then starting with the last unused bit position seen in the freely available literature scan forward till you see the coded representation, and then extract the number of following bits represented by that code. The Levenstein code was the first universal code, but it suffers from the fact that it's a single bit too long for the application at hand because it encodes zero as well, doubling the hardness of the search step. Thus the best code for this sort of thing is probably the Elias Omega.
A construction like that leaves very little upto doubt as to your choice, and it comes with nice enough properties that it could actually be of general utility. Of course, what you then do with those bits is another matter. Once you have them, you still have quite a lot of leeway in how (especially in which order) you assign them to your crypto primitive's free variables. So that leeway should be minimized separately, too. However that's the kind of problem I can't even begin to touch, because really doing it right essentially amounts to canonicalizing and ordering the whole class of finite width boolean circuits. That is then a very hard problem indeed.

@_date: 2014-04-02 04:02:26
@_author: Sampo Syreeni 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
That is why I put in Elias's omega code, because to my knowledge it's the first optimal construction of a universal code Levenstein's, and it always starts from one, so that you can't repeatedly extract zero bits and by that way get around the idea that you shouldn't be able to precisely choose where you start your extraction.
Second, since the set of extant cryptographers is disributed and ostensibly a novel algorithm needing low-sleeveness numbers might even come from outside the known mainstream, I can't think of any lower complexity protocol which just lets you extract arbitrary amounts of nice bits from a commonly trusted source, than this one, combined with some kind of plausible enough notarization to establish precedence in case of dispute over who owns the currently-considered-to-be-spent-bits.
And third, the whole point is that you don't get to choose the length of the extraction and the bits extracted separately. They come as an integral whole. If you want to have a certain number (which in a good primitive means you have as many bits of evil design freedom), that'll translate to as long a linear search starting from the current status quo as there are values in the total range your extracted number might eventually have. After that, we're also using a reasonably efficient coding, at least asymptotically speaking: the omega code is pretty well distributed for this sort of thing, and if you consider the possibility of overlap, without formal proof I'd assert that the overlap more or less asymptotically compensates for the efficiency loss of prepending a length prefix in the first place.
Of course, the computational complexity of finding any reasonable amount of low-sleeveness bits isn't the real issue here, even if it might help disincentivize a naive crypto designer from building an optimization farm for the purpose. The real point is that the construction forces -- I hope, fingers crossed -- the search for any specific bit string to take on roughly the same cost as either just taking the next n bits without having choice in what n is, or alternatively to find some deterministic structure in the binary development which is subexponential in sequence length, and thus proving pi not to be normal.
Or if my intuition is broken here, then maybe the basic reasoning helps some real cryptographer strengthen the construction to something you can actually prove something nasty about; as far as the original question goes, it's a win any which way.
Those are precisely the spigot algorithms I mentioned. The requirement that you start with something which wasn't already spent comes from two reasons. First, obviously if you want your number to be low-sleeveness and you have to ask about it on-list, typically you'd want it to be something which wasn't already utilized and analyzed to death. For instance, the community might have noticed that despite the conjectured semi-randomness of the first bits of pi, statistical fluctuation still made them fall too far in the tail of the distribution to satisfy the finite, truncated probability assumptions you sometimes have to make in order to make your cryptosystem secure.
And second, you really do have to get to choose the number of bits you use for your constants. That is something we can't easily stop, and so it represents a degree of possible gaming, leading to sleeveness. We can just hope it's somehow evident from the proposed structure that no bits extracted were lost in the process (i.e. all of them behave much like ideal key bits in the proposed construction, or we even have some sort of objective, future proof of how many bits you need for any future circuit). But what we really, *really* don't want anybody to have is a second, independent optimand they can freely choose, because that'd at the very least lead to birthday sort of sleeveness in the hands of a rationally malevolent designer. Thus, make them fix at least the starting point of the search at the status quo -- which is required in any case if we need new, non-overlapping constants -- and then tie the length and the actual bits found irrevocably together.
That doesn't take care of the length selection problem. What my construction does is it forces each length to start at a different offset as well, so that you can't easily take advantage of series equivalences and the like to give structure to your bits, even after there's been a while since the last cryptographer used the construction to extract anything. And of course precedence in publication serves as a natural, already necessary, external source of randomness to the process: if you wait, running your ostensibly exponential optimization algorithms to do your evil deed, as soon as somebody else publishes something using the construction, you're forced to start again, on pain of considered having something up your sleeve if you don't.

@_date: 2014-04-14 01:51:15
@_author: Sampo Syreeni 
@_subject: [Cryptography] Heartbleed and fundamental crypto programming 
What you probably meant to say is that UDP doesn't stop you from building in congestion control at a higher protocol layer, but at the same time it has none by itself. As long as congestion control is handled as an end-to-end business, TCP -- when you can trust people to use it -- has it by design, UDP does not, and hence by definition you can't downright contribute to a wholesale congestion collapse *if* you use TCP. Using UDP you can, but you still don't have to. BitTorrent's ?TP/LEDBAT is extra benign by design even compared to TCP, and the various mechanisms built into the various RTP applications are currently pretty nicely-behaved too, but with no real guarantees given by the base RTP framework, per se.
So, it's basically bad logic to talk about UDP or even TCP wholesale. You have to take a look at which precise congestion mitigation measures are taken, over the whole protocol stack, and how each one of the stacks interacts with each other in the wild. The yardstick of course being the newest, most sophisticated incarnation of TCP, even if via LEDBAT kind of delay sensitive protocol work we already know TCP isn't too nice That's a bit tricky already. By its nature DNS is an embarrassingly parallel datagram based request-response lookup protocol. Even as of now there are no real, standardized mechanisms to make that sort of thing immune to congestion collapse. The only thing which keeps even the age old DNS/UDP from becoming a nuisance is that it's a relatively rarely used protocol with lots of caching of its own. It simply eats so little bandwidth compared to everything else that it pretty much never leads to marked congestion issues.
But in conditions of true congestion and network instability because of that, DNS's most common UDP incarnation would not react gracefully even now. The classier *implementations* of the protocol such as BIND do try to take care of the problem, e.g. by going into exponential back-off, and sometimes even trying to restrict the whole of the uplink protocol traffic amount from a single node. But such measures are not a binding part of the standard, and so if you happen to have an application (many P2P ones come to mind) in your hands which queries lots of disparate nameservers, many of which are dynamical with short cache renewal periods, under a bandwidth squeeze quite a number of DNS clients are quite willing to conspire towards a collapse. Even to shut themselves off into an online equivalent of a live-lock, mediated by their cache.
But also things like RED, which via packet loss, and more or less fairly, signals flows going through a router beforehand to throttle back. Combined with that sort of thing, long buffers work pretty well in the core of the network, even if the high bandwidths and thus inversely low queuing delays of today make such processing less necessary.
So what really fucks TCP up is an intermediate buffer which only serves a single endnode, and whose inputs as such cannot be assumed to follow any of the typical statistical distributions queuing theory relies upon. And which also has to serve a mixed load of bulk and real time traffic, with bimodal packet size and bandwidth disributions, with no more help from the endnode than your core router would have. Which also has to be exceedingly low cost.
That is, your cable modem. The core routing infrastructure can nowadays deal pretty well even with straight TCP, and even the kind where the application protocol used would want to do real time. What really messes things up is the middle-box-like implemented, blind, massive queue they build into your uplink, in order to run such weird multiple access protocols on your local cable as DOCSIS, without without losing advertisable total steady state bandwidth guarantees. When you do that, especially on the cheap, you'll inevitably end up with a huge, badly managed queue, and a higher level fix which essentially, *very* hard, tries to avoid that queue being there at all. That is, what the delay sensitive protocols -- in the absence of any signaling except perchance the emerging ECN notification one, not really accessible to middleboxes either -- like the BitTorrent one (?TP) and its IETF standardized variant (LEDBAT) in fact try to do. Both in practice and in the original design rationale.
So, that isn't really a problem with TCP at all. There is absolutely no reason why you couldn't do both TCP style loss-derived and ?TP/LEDBAT style delay-governed congestion avoidance. In fact, the BitTorrent derived work does just *that*: in order to guarantee TCP-friendliness, it falls back to implementing additive increase and exponential backoff upon packet loss. Essentially it implements the core TCP congestion control algorithm at a higher protocol layer, over UDP, just to be sure (and it had better too because not one of the ?TP derivatives I now of have been proven even statistically speaking fair amongst themselves, unlike TCP as).
And as it usually goes, those same ideas have already been adapted for the TCP stack itself. That's what the TCP Vegas incarnation of the CC loop is all about. Lots of research has followed from that, and the code is already in the Linux kernel. It isn't quite as aggressive as the LEDBAT variant, and certainly relies too much on the classical means of RTT measurement within TCP over torrent-like explicit, high accuracy timestamps. But rather certainly it's far less aggressive than any of the earlier incarnations of TCP, is already entrenched, works like a charm, and relies primarily on RTT instead of loss (timestamps work even better when the channel is asymmetric, as it is with that nasty upband buffer I already mentioned, so that is a qualitative difference too).
So, you are right. There is a severe incentive compatibility problem here. However, against all odds, it has already been mostly fixed in practice. Even TCP has that problem, because it's E2E. Nothing in the routing fabric enforces the flow control or congestion avoidance features of even TCP, so if you want to go around them, you can. In early incarnations of TCP you could do that simply by subverting a couple of assumptions in the implied control loop. I dunno if anything like that could be done today, but in any case, multiple parallel connections over either of TCP or UDP will always do the trick. That's where DDoS attacks come from, and why they are so difficult to defend And yet, for the vast majority of situations, designing a proper congestion avoidance protocol into software your typical enduser won't bother to change, just works. Like TCP did. So that many other protocols would too, for the most part. What mostly impacts such protocol development then isn't that the users have adverse incentives to deployment. That was already countermanded by the high cost of developing your own protocol stack de novo and interworking with the wider community. What the real problem is is that those two economical problems work against *any* new protocol, be it good or bad. The same thing which stops you from being a bad guy, also hinders you in Doing the Right Thing Better.

@_date: 2014-04-20 03:38:03
@_author: Sampo Syreeni 
@_subject: [Cryptography] It's all K&R's fault 
Don't do either. Get an intelligent compiler for an intelligent language, which inserts the bound checks when needed, and optimizes them away when possible. Win-win.

@_date: 2014-02-04 14:33:36
@_author: Sampo Syreeni 
@_subject: [Cryptography] cheap sources of entropy 
I wonder whether scrypt-like tricks can be used to force even a VM to deliver some of the underlying timing variability into the software. Okay, you're going to need some rather interesting assumptions, but...
Suppose you can somehow force your working set to be larger than any reasonable cache. At one point you're bound to spill onto disk, so that seek latency becomes an issue. If you know what's going to come back from the disk because you wrote it, but the adversary does not, you can now verify which writes actually came from disk and which ones didn't. Thus you just amplified any existing randomness you had a little bit if you measured the roundtrip time.
You could attack something like that at least by simulating the program or quantizing the times. Quantization you can combat at least by working with longer time scales, too: it only rounds out the lower order bits, but not longer term drift. If you collect that too, at some point you will have enough randomness to reseed. Simulation/prediction you can mitigate a little bit by issuing your own probes deterministically so that they don't leak (all the) state, but full simulation of course still works. So, can we make at least some simulation strategies relatively expensive?
A roundtrip even to external cache costs latency, as do context switches and the like. If you can somehow do something within the CPU which is so tightly locked into real time that it's below such latencies, yet has two different outcomes which are deterministic but based on state which is not immediately observable from the outside, maybe you could derive a deterministic execution path which is prohibitively expensive to emulate in real time, at least without custom hardware? My idea goes something like this:
Do a tight loop which times against the CPU's cycle counter, while exercising the L1 cache. Find the minimum over a longer time and store it; you can't go faster than no context switches, TLB misses &c. Now watch for a single non-minimum, store the cycle counter in a register, and if the next loop is again minimum time, exit. Now you know "something" happened in between besides straight execution, which will have affected the stored counter, and can be used to branch in a manner which can't be simulated with zero latency. Then compound the trick a dozen times; suddenly the internal state of the program ought to be such that it can't be replicated too easily. Now amplify that via the working set construction above, and repeat.
Nothing's absolutely sure in a VM environment, but as soon as you have those outside, linearly increasing clocks at your disposal, I'm betting you can make life pretty miserable for the adversary: what you need to emulate something like that isn't standard hardware, and when done right, will in any case lead to such extra latency in the inner, faster manipulation that when amplified, it'll show. If it doesn't, you can be reasonably sure the adversary doesn't know the entire state of your program in time, and then you can amplify that, one bit at a time, via real randomness like disk turbulence. Then in the future you can do a quantized rekey, and suddenly you have those 160 bits of real seed within your state.
Or, so goes the theory/sketch.

@_date: 2014-02-15 00:22:44
@_author: Sampo Syreeni 
@_subject: [Cryptography] The ultimate random source 
I wonder... Might there be a random process which actually *can* be verified? Say, something relying on quantum entanglement and the uncertainty principle? So that you could somehow both have a stream of random events and a separate verification stream which somehow lets you (statistically) prove, after the fact, that whatever the other guy got really was derived from a physically speaking indeterminate source?

@_date: 2014-02-15 00:45:21
@_author: Sampo Syreeni 
@_subject: [Cryptography] Another Bitcoin issue (maybe) (was: BitCoin bug 
I disagree. The idea was, from the start, that transaction fees would supplant mining fees in the end. Both are subject to the normal competition over nominal cost, and if you take a look at the monetary equation of exchange, both mechanisms work out pretty much the same on the same side of the equation. Profits from mining equal a percentage of M, profits from fees equal a percentage of V, and then in long term equilibrium both just work out to a fixed money supply combined with a market rate for transaction processing. So, it's good marketing for early adopters, combined with a steady state that is fully set by the market. What more can you ask for in a de novo non-state backed Then you'd be taking the position that steady inflation is a good thing. Satoshi took the opposite, gold bug position where a fixed (or highly inelastic) money supply and its attendant deflation in times of growth was seen as a good thing. Now we live with that: nobody says you should invest in bitcoin if you don't like its lack of macroeconomic stabilisation possibilities or somesuch thing. In fact, you could always fork the codebase and the blockchain and make yours better.
But if you can't make it fly and drive Bitcoin out of the market, isn't that just testament to the fact that the idea of mining really *was* just the thing you need in order to get this kind of a virtual currency

@_date: 2014-07-19 02:45:05
@_author: Sampo Syreeni 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
No. The trouble is that most people who build these stego applications, don't seem to read their literature at *all*. For some reason, unlike in the rest of the crypto circuit, those who actually code stego work at the script kiddie level, instead of the PhD one -- which really does exist even for stego, as part of the information theoretical viewpoint of things.
Seriously, that it just stupid. It has absolutely nothing to do with hardcore, statistical data hiding.
I mean, I've been thinking about how to do that for a couple of years now, so as to hide low rate text messaging within telephone audio calls. The best I've come up with are a couple of DFT and DHT compatible syncro waveforms, with baseband direct sequence spread sprectrum, resynchro algoriths keyed on GSM's line protocol, stochastic waveshaping to throw any cheap, network-wide statistical recognizer off, and whatnot.
And yet I can be nowhere sure they couldn't detect the transmission amongst the utility signal, en masse, in any case. So that I shouldn't even start *coding* my solution as of now.
So what *is* it with you people? Can't you see that steganography really starts and ends with information and coding theory, unlike cryptography? Its bounds really necessarily and from the start have to do with noise and uncertainty, whereas crypto protocols only deal with clean data and computational complexity (eventually, preferably, proven-to-be-hard one-way-functions). Steganography really is its own, separate field, eventhough it shares most of the randomness, signal processing, complexity and whatnot, framework, with current crypto proper. (Especially the symmetrical and streaming kind, BTW, which might be a problem aand a subject for further study.)
On the other hand, let's squish the brain and still do proper steganography. Also proper linguistics. With the silicon brain. How many covert bits can you really fit into a Twitter post, before NSA's silicon brain flags it as being terrorist? That's the steganographic competition for real! And we'd win it simply by numbers, if we just built a proper protocol...with the numbers actually utilizing it. So then, how do we build the protocol, and especially incentivize the numbers to adopt it?
Never use a conlang for this sort of thing. They're too easy to parse and much too dense to embed information in. Use something like my mother tongue, Finnish: rather regular even in orthography, but still a natural language ripe with opportunities for embedding. Especially in its various dialects.
That really only works with polysyntetic languages, like some of the Native American and Inuit ones. Even Finnish doesn't really carry that far. Klingon would work pretty well, as would Navajo and Taa, but pretty much anything else would be too easy to decrypt. And yet the latter actually leave quite a bit of free redundancy to be exploited.
At the same time, do you actually know of a grammar framework which could actually encapsulate either of your English or of my Finnish language, fully? Generatively? Fully describing both of their natural statistics, starting from a computational model, and one which actually leads to a computationally efficient recognition framework? So that the NSA, or the GCHQ, or what was it now in Sweden and Russia, can actually find your stego in real time?
Wasn't there supposed to be someone, somewhere, who was actually raised speaking Lojban as her (?) first language? Because while that'd be rather difficult to parse at first, over the long run the whole language has been designed to be machine parsable.
Not to mention the fact that might se'd might be a she. Oh my dear.
Undoubtedly so. But then, that's not what real steganography is about. It's not about willy nilly pushing a bit or two here or there into text, or using funny words. It's about taking a well statistically characterized carrier, in language/text, image, video, sound, whatevever, and imprinting a surreptitious message upon it, without disturbing any of its visible/audible/computationally-unearthable qualities, to some chosen degree. Preferably one that you could prove to be fully undetectable, but as it goes with even symmetric cryptosystems, we don't have such unconditional proofs as of yet...
Quite. But the way you make it so is different. Nowadays you base it on information and coding theory, and also cryptography proper. Certainly not on esoteric gryphs, because they might awake interest; no, you really ought to aim at kitteh-pic-meme-embedding or something akin to that. For carrier bandwidth, you see, and the endless variability.

@_date: 2014-07-19 03:15:01
@_author: Sampo Syreeni 
@_subject: [Cryptography] Steganography and bringing encryption to a piece 
I actually believe Feynman said pretty much the same thing about his code(s) with his wife, from his time at Los Alamos. And of course this is how it very well works if you happen to do your steganography right, *plus* your adversary also happens to know that you might be doing so (sorry about the long quote, but it's very much topical):
             So, very delicately amongst all these liberal-minded
             scientific guys, we finally got the censorship set up, with
             many rules. We were allowed to comment on the character of
             the administration if we wanted to, so we could write our
             senator and tell him we don't like the way things are run,
             and things like that. They said they would notify us if
             there were any difficulties.
             So it was all set up, and here comes the first day for
             censorship: Telephone! Briiing!
             Me: "What?"
             "Please come down.
             I come down.
             "What's this?"
             "It's a letter from my father."
             "Well, what is it?"
             There's lined paper, and there's these lines going out with
             dots - four dots under, one dot above, two dots under, one
             dot above,
             dot under dot...
             "What's that?"
             I said, ?It's a code."
             They said, ? Yah, it's a code, but what does it say?"
             I said, ?I don't know what it says."
             They said, ?Well, what's the key to the code? How do you
             decipher it?"
             I said, ?Well, I don't know."
             Then they said, ?What's this?"
             I said, ?It's a letter from my wife - it says TJXYWZ TWIX3."
             "What's that?"
             I said, ?Another code."
             "What's the key to it?"
             "I don't know."
             They said, ?You're receiving codes, and you don't know the
             key?"
             I said, ?Precisely. I have a game. I challenge them to send
             me a code that I can't decipher, see? So they're making up
             codes at the other end, and they're sending them in, and
             they're not going to tell me what the key is."
             Now one of the rules of the censorship was that they aren't
             going to disturb anything that you would ordinarily do, in
             the mail. So they said, ?Well, you're going to have to tell
             them please to send the key in with the code."
             I said, ?I don't want to see the key!"
             They said, ? Well, all right, we'll take the key out."
             So we had that arrangement. OK? All right. Next day I get a
             letter from my wife that says, ?It's very difficult writing
             because I feel that the _____ is looking over my shoulder."
             And where the word was, there is a splotch made with ink
             eradicator.
             So I went down to the bureau, and I said, ?You're not
             supposed to touch the incoming mail if you don't like it.
             You can look at it, but you're not supposed to take
             anything out."
             They said, ?Don't be ridiculous. Do you think that's the way
             censors work - with ink eradicator? They cut things out with
             scissors."
             I said OK. So I wrote a letter back to my wife and said,
             ?Did you use ink eradicator in your letter?" She writes
             back, ?No, I  didn't use ink eradicator in my letter, it
             must have been the____ and there's a hole cut out of the
             paper,
             So I went back to the major who was supposed to be in charge
             of all this and complained. You know, this took a little
             time, but I felt I was sort of the representative to get
             the thing straightened out. The major tried to explain to
             me that these people who were the censors had been taught
             how to do it, but they didn't understand this new way that
             we had to be so delicate about.
             So, anyway, he said, ?What's the matter, don't you think I
             have good will?"
             I said, ?Yes, you have perfectly good will but I don't think
             you have power." Because, you see, he had already been on
             the job three or four days.
             He said, ?We'll see about that!" He grabs the telephone, and
             everything is straightened out. No more is the letter cut.
             However, there were a number of other difficulties. For
             example, one day I got a letter from my wife and a note
             from the censor that said, ?There was a code enclosed
             without the key, and so we removed it."
             So when I went to see my wife in Albuquerque that day, she
             said, ?Well, where's all the stuff?"
             I said, ?What stuff?"
             She said, ?Litharge, glycerine, hot dogs, laundry.?
             I said, ?Wait a minute - that was a list?"
             She said, ?Yes."
             "That was a code, ? I said.?They thought it was a code -
             litharge, glycerine, etc." (She wanted litharge and
             glycerine to make a cement to fix an onyx box.)
             All this went on in the first few weeks before we got each
             other straightened out. Anyway, [...]
Most definitely, I've always envied Feynman for his wife. You don't just play such steganographic games with any wife, you see.

@_date: 2014-06-05 03:09:28
@_author: Sampo Syreeni 
@_subject: [Cryptography] Fork of TrueCrypt 
It gives off too much data wrt traffic analysis. What you really need is... What was it... Mixnets. Of course. Them olde thingies. Like my Tor exit node.
The Horror. Hammer down.

@_date: 2014-06-18 03:59:53
@_author: Sampo Syreeni 
@_subject: [Cryptography] bitcoins over the air 
In case people are interested in a project of a friend of mine, Joel Lehtonen (aka Zouppen) is on a fast track to implementing Bitcoin transaction and blockchain broadcast over the airwaves. Everything is half done, so he might need some help; at the same time that half-done then also includes half the funding, half the code, and half the negotiation with the Finnish monopoly DVB-T provider, Digita, to actually broadcast the stuff to a couple of million strong. Even if it's just a test, it's already well on its way to happening in a month or so; so it will.
If people are willing to chip in, especially with funding, deeper code knowhow, radio-fu, and the rest of the useful ones, do contact him, or me (aka decoy), on FB/G+/IRCNet/freenode, or better yet join  on the latter.
In particular we don't have any idea of how to push transactions back towards the network over any sort of cheap-to-free, preferably universal radio path. If you do, even in a quilted fashion, we'd really like to know. And since the only ones who're really allowed to experiment on them waves today are hams, those of you who possess that qualification, your input is in the direst of needs.
Finally, I hope this isn't too much off topic; it *is* about spreading a cryptocurrency and finding its optimal OTA protocols, after all. Not perhaps the most usual stuff on-list nowadays...but certainly the kind of across-the-board architectural stuff the list started out with, in the crypto days of yonder. I hope it fits.

@_date: 2014-03-03 04:07:00
@_author: Sampo Syreeni 
@_subject: [Cryptography] RAM memories as one source of entropy 
It can. But then, think about the usability factor, and the systemic underpinnings of how you derive trust-in-hardware. Especially think about how you'd assuage the FUD another person might feel over your home made generator.
Denker's work yields a source of randomness which is near-ubiquitous given current distributedly mass produced motherboards, and it does so with a proper, relatively easily measured, quantified and sanity-checked bounds on the minimum entropy rate. Of course by all means add to that if you can -- all it takes is a bitwise XOR -- but if you want to bring real entropy to the masses, easy access and proper bounds, monitorable, always trump extra complexity as the base source.
Then as it stands, strong (nonlinear, like derived from modern block ciphers, so that even prospective quantum algorithms are rendered moot as an attack) PRNG's with periodic reseeding from such a source ought to be enough for anybody, without blocking or other such inconveniences. Or is that not pretty much the consensus, based on the best and most paranoid knowledge we have?
So is this problem not pretty much solved? Shouldn't we just move along, since there's nothing to see here?

@_date: 2014-03-03 04:20:40
@_author: Sampo Syreeni 
@_subject: [Cryptography] Testing crypto protocol implementations 
Why not formalize the problem? True, formal protocol verification suffers from state space explosion, in general. But if you design your protocol to be easily verifiable, and match your verifier to what you're about to verify, you can skirt the problem to a high degree. In the absence of that, and/or in addition to it, if you have access to a similarly well-structured code base implementing the protocol, it should't be too hard to formalize well-covered white-box testing, intelligent fuzzing and whatnot, by automating at least every heuristic known to the testing community at large.
All of that should be doable at a generic level, if you're just willing to circumscribe the problem narrowly enough. No instrumentation in code, no problem specific code for test cases or anything like that, but just generic tools, able to handle say transactional grammars and a tightly regulated program flow encoded in plain C.
So, it's of course costly. But I don't think it'd need be for each implementation separately. I think it could be a single research project in verification of security minded protocol verification. The seeds of which already exist, in multiple places, if I'm not completely mistaken.

@_date: 2014-03-04 03:07:58
@_author: Sampo Syreeni 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
Yes. First, thinking like this is a naked cum hoc ergo propter hoc. What probably happened here was a copy-paste mistake. Counterfactually speaking, could it then have happened otherwise, without gotos being involved in any way? As easily? I think it could have.
Take for instance Java's structured exception facility. Someone might well be tempted to write code like:
try { if (error) throw Exception;} catch (Exception) Using that syntax you're not likely to make the same precise error, because it'd lead to something unsyntactical like and you can't just copy-paste it in:
try { if (error) throw Exception;} throw Exception; catch (Exception) However, if you're drowsy enough to make the first mistake and not notice it once you made it, you're also well eligible to commit e.g. the following one from sheer muscle memory and copy-paste habit:
try { if (!!complicatedErrorCheckingMethodName) throw Exception;} catch (Exception) {}.
The effect is essentially the same, the source of it is essentially the same, and clearly it has nothing to do with gotos at all.
So my point is, while you probably can guard against certain errors via syntactic means like forbidding gotos, you really, *really* can't guard against all of them, and even if your failure model is a bored-to-silliness coder repeating something via a careless paste operation, you're in general probably out of luck.
Second, as for gotos per se, I believe they're a misunderstood beast. Most who rail against them never read or at least understood Dijkstra's original demurral. His point wasn't that goto's are bad per se. It was that they invite the kind of undisciplined coding style and muddled, ad hoc thinking style about algorithms which leads to spaghetti, and soon thereafter to code you can't (provably) formally analyse or verify.
So, gotos don't automatically lead to trouble, nor are they evil an sich. With proper, slavishly elegant, high coding style they can actually serve you pretty well on occasion. It's just that they make it so very easy to hose yourself, and as such, it'd be better to leave them out of language syntax.
In this case, there was nothing wrong with the coding style per se. What probably happened was an inadvertent double paste which wasn't caught in time. As I argued above, that's not a problem which is intrinsically tied to gotos, and quite probably (absent some work in formal grammars I'm not aware of) something not something you can fully and/or usefully control via syntactic means. Instead it seems like a problem for quality (Though, as part of such QA it actually might be a good idea to institute some automated checks for repetition and the like between successive commits to any codebase... But the syntax of the base language, have it gotos or not, ain't then the issue or part of the error model; the textual encoding and the likely programmer induced faulty string operations on the source are.)
Also, let's see the performance numbers. As much as we'd like to see omnipotent compilers doing miraculous global transformations to program flow, especially in C-like low level languages neither the necessary semantics for the compiler to do its theoretical maximum, nor the willingness of the compiler developer to even try such program transformations, just aren't there. Thus, if the programmer is forced to do it via nested ifs, the result is often not as fast.
Then not only does the slowdown sometimes matter for real, but perhaps even more importantly a large proportion of novice programmers see a language which binds them as not worth using. Call it hubris or whatever, but if you can't shave off those few cycles whereever people want to shave them off, predictably, people will go elsewhere. That's really the reason we still have gotos in pretty much every language in existence, including many forms of LISP, of all things: if you try to force the programmer's hand, he'll fight back, and then all hell is loose. Just cf. ADA...
On that vein, what would then, in the ideal world, be a) the most extensive part of testing you could possibly automate, and b) what and how would then be left over to human process? Of course within a cost constraint? A sliding scale too, so that you could have various levels of reassurance in your product?
Yeah, I know a development framework or two myself. But I don't think any real consensus exists about what you should really do, and in what priority order, in order to get various levels of assurance. Especially when indexed by economic viability, instead of abstract, ad hoc, theoretical niceness/beauty standards.

@_date: 2014-03-04 03:28:16
@_author: Sampo Syreeni 
@_subject: [Cryptography] The GOTO Squirrel! [was GOTO Considered Harmful] 
But couldn't you basically parse the program flow of the emitting application, and guidedly mutate around every decision point within it? That's still within the definition of fuzzing, if I'm not entirely

@_date: 2014-03-27 06:26:38
@_author: Sampo Syreeni 
@_subject: [Cryptography] Dark Mail Alliance specs? 
How is it not? You have to be able to contact the cloud in some way. Once you've contacted it in some way, they know your real name. They can then kill you off if you do something suspicious, including you contacting the cloud in a way they can't make sense of.
Eventually, inevitably, they will kill you simply because they suspect you did something they didn't explicitly allow you to do. Under that nebulous threat model of all non-bureacratese being deathworthy, how precisely can you call any cloud secure?

@_date: 2014-03-30 21:11:04
@_author: Sampo Syreeni 
@_subject: [Cryptography] Amateur Radio Authentication - was: OpenPGP and 
But then you wouldn't operate a repeater over EME in any case. Typically the kinds of communications where you'd want the authentication field are higher rate digimodes, with enough capacity to throw in a full ECDSA signature some proper symmetric primitive.
The interesting thing is, many amateur modes are actually rather wasteful, so that if you threw in an optimized to the hilt authentication field using the most efficient modulation you can think of, it'd easily fit in the channel used without taking a whole lot of time compared to the utility signal.
My favourite example right now is plain Morse/CW. With its 1/3/7 dit commas it's nowhere near an optimal line code, not to mention that since it's equivalent to straight amplitude modulation at 100% depth, with suboptimally shaped base band pulses, it's technically as wasteful or more so than badly thought out DSB. If you really wanted, and was able to go around the -- frankly bizarre -- ban on independent sideband modulation, you could probably fit not only strong authentication but running ECC as well, into your standard narrow band CW slot, even at HST speeds. I haven't (yet?) tested how well something like that would work with a straight key and decoding by ear, but I wouldn't be surprised if you could sort of have even that at the same time -- and certainly could inject a strong authentication burst at tens of bits per second every here and there if you wanted to multiplex by time instead of frequency.
My point being, if you want authentication in the amateur bands, there are ways to fit it in where it's needed. Sure, that might mean you have to butcher your existing modes, but then what else is the technical side of ham work than that kind of tinkering? Especially in an age where hams unfortunately seem to lag somewhat behind the bleeding edge in bandwidth efficient modulation?

@_date: 2014-03-31 06:10:54
@_author: Sampo Syreeni 
@_subject: [Cryptography] ideas for (long) Nothing up my sleeve numbers 
To my eye randomness is much different from "sleeveness". If you have a totally random number, it's of course nothing-in-the-sleeve by definition: you couldn't have chosen it, nobody could have known it before God gave it to you, and in hir infinite wisdom se then also made it damn sure every extant cryptographer had a personal, blinding revelation, irrefutable by anything before, now, and thereafter, that you and you alone were given this number, it is random, and all of you lot are in perfect informational balance about the fact.
That's pretty much the naked definition of true randomness. I'd guess much of why e.g. John Denker objects to the naked version is because you really do need a benevolent and crypto-interested god behind that reasoning. That kind of real randomness ain't manna that drops from the sky, or automatically emerges from crypto-nice-looking nonlinear systems when you torture them enough. Instead it's a precious resource, which quite possibly might not exist at *all* outside of theology.
The only systems where we really have something which even *looks* truly random are the quantum ones, and certain derivatives of them where we can already fully explain the noise of a macroscopic system's noise as an aggregate of its quantum parts. That's what Denker bases his argument: things like Johnson-Nyquist noise are one of the few macroscopically measurable things which we understand fully enough to say that they're very likely truly random, and quantifiably so, under the assumption that the process of wavefunction collapse upon measurement is random.
Nothing-up-the-sleeveness requires nowhere near such assumptions, nor does it address the same problem. When you apply that one, you already have a system which you believe is indifferent to which precise sequence of bits you put in. At the same time you worry that since some other, Demonic people already put in hard to verify backdoors in their cryptosystems, or ostensibly could do so. Since both you and your scientific peers are all kind of paranoid because of it, you want to show that even if those free variables in your system weren't quite as insensitive to choice as you thought and your argument showed, it still would have been prohibitively costly for you to actually *choose* your constants for somebody's benefit.
So, the problems solved are very different. Randomness is about mathematical assumptions, and ones which are pretty much impossible to fulfil in any rational frame of mind. You can approach them in a manner which gets the job done and is rather believable when you trace the full argument right downto you source of physical randomness, spell out your assumptions, and whatnot. Denker's work is a case in point, there.
Sleeveness is instead about human trust relations. If we knew how to ascertain our trust in shared sources of randomness, the problem would be solved. But there really doesn't seem to be any way to fulfil the "shared" part as of now. Hence, the trust problem can't be solved wholesale. We need partial solutions, and one of them in cryptosystem design is to somehow show other people that the constants you chose for your variables weren't made up. In that application the basic assumption already is that even if you just set all of them to zero, it wouldn't matter, because the security of your system shouldn't depend on the precise values assigned those variables (okay, it might, but then always modulo some clear statistical criterion dependent only on your circuit topology; then you'd also not just assign random bits to your variables, but either give a clean reduction to an equiprobabilistically minimum risk vector of constants, or more likely just redesign so that your constant vector simply fulfils every condition a key input would, within the relevant construction; so that setting it to zero wouldn't do any harm either).
It's that trust aspect why you also don't need even (pseudo)random numbers. It's convenient that any numbers used for this purpose fulfil the freest statistical assumptions of equidistributed independence, but it isn't necessary by any means that they cannot be predicted. In fact predictability is a plus here, and the more the better, because that increases trust. All that you need in this game is binding precommitment by Nature, on your behalf, or something close enough in wiggle-room to make everybody believe you couldn't have made it up on the relevant (I'm not saying I will have the nerve, but I might just suggest a protocol for this in the coming days. Algorithm and all.)

@_date: 2014-10-10 04:49:13
@_author: Sampo Syreeni 
@_subject: [Cryptography] Cryptography, backdoors and the Second Amendment 
So is yours: obviously you can *have* and *use* it, it's just that you can't *export* it to the *terrorists* and the rest of the bad people who aren't you.
Perfectly consistent. Of course perfectly fucked up from the viewpoint of a foreign libertarian like me as well. But it really is fully consistent, and it was so from the very start, right downto the basic classical liberal ideology I as well share: "there is only one correct law, it is universal, if you don't share it then you haven't Been Enlightened yet, and thus we for very good reason don't Mind you too much". "Till you join our movement of universal rationality..."
So, then, as it's basically a valid argument, how about taking its contraposition? "As we then already know crypto is right, and it'ss used by precisely the right, righteous people all round, should it not be the case those who make a claim against are simply wrong."
Should it not in fact be, that making a case against free crypto should be taken as a prima facie case of the speaker being a fascist, against democracy, a luddite, and an all-round bad guy? Out to get immortalized as the next Hitler?

@_date: 2015-07-06 05:14:45
@_author: Sampo Syreeni 
@_subject: [Cryptography] Best AES candidate broken 
...not to mention AES/Rijndael is *not* the system that was broken here.

@_date: 2015-06-08 21:12:46
@_author: Sampo Syreeni 
@_subject: [Cryptography] let's kill md5sum! 
Except that it's not. Of course you do have a stochastic guarantee, with md5sum, but no hard one like you do with CRC32.
That *can* fuck you up, when you pass lots of short messages, like you do over a constrained radio channel. And it *will* fuck you up with current bandwidths, sooner or later. Without you knowing what then happens, because you haven't quantified the error-floor to coding bandwidth staircase.
Likely the problem isn't grave. But I don't much like the idea of mistaking ECC for cryptographic checksums. Especially in any application where the coding space is less than the square of log of all the bits likely to be produced in the lifetime of the protocol.

@_date: 2015-06-09 07:30:52
@_author: Sampo Syreeni 
@_subject: [Cryptography] reality +- mathematical guarantees 
True, but the error floor by S/N is flatter. That's why they did all of that linear and modulo coding work in the first place, instead of just randomizing the fuck out of it, per Shannon's original proof.
      "This is the conclusion of the theorem."
                          -- Claude E. Shannon
                            "Coding Theorems for a Discrete Source With
                             a Fidelity Criterion" (1959)

@_date: 2015-05-05 03:11:07
@_author: Sampo Syreeni 
@_subject: [Cryptography] "Trust in digital certificate ecosystem eroding" 
Or in otherwords, there's no solution to the problem of preemptive rubberhosing of the entire population. That's a game you can't win even in principle, so the only sane move is not to play.

@_date: 2016-08-09 07:18:35
@_author: Sampo Syreeni 
@_subject: [Cryptography] BBC to deploy detection vans to snoop on 
It's being claimed that they take a picture of a neighbourhood and on the fly correlate the lighting variation in people's windows with the on-going program. That'd work equally for all modes of transmission, and FFT based hardware to do that sort of thing efficiently is out there already because of the advances in synthetic aperture, active scanning radar technology.
As for the cat detection van... Obviously that skit was inspired by the earliest BBC detection vans, which probably relied on principles similar to those utilized in radar detectors. Detection of a common intermediate frequency in television sets, or perhaps even the line frequency harmonics from the tube itself.
Nobody expects the The British Broadcasting Corporation.

@_date: 2016-05-28 03:53:09
@_author: Sampo Syreeni 
@_subject: [Cryptography] Entropy Needed for SSH Keys? 
In vacuum (and mostly in dry air), you can calculate it to be exactly 299792458 m/s * 1e-9 s ~= 30 cm. So about a foot is the basic rule. No need to remember any of that, you can get it straight from Wikipedia.
In silico it's more complicated, because of the varying dielectric constant of different kinds of doping levels, surface structure, stray capacitance, and whatnot. But based on what I've seen of the possible permittivities, I'd wager the nanosecond can range anywhere from 28cm downto as little as 2cm, depending on the substrate.
In interference, it matters downto the twelth digit.
