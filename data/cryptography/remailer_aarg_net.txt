
@_date: 2001-11-18 12:30:05
@_author: AARG!Anonymous 
@_subject: Forward Security Question 
Hi All,
I have recently been reading about password-based authentication schemes, especially EKE and its variants.  The papers I've read on EKE, DH-EKE, and SPEKE all refer to their "perfect forward security," though I have been unable to find a formal definition of this property, or any detailed explanation of what this really means.  Does the "forward security" refer to the fact that if Eve knows a "K" Alice and Bob used two weeks ago, she cannot assume either of their identities for a current transaction?  Or does it mean that even if Eve knows the current "K" in use by Alice and Bob's session, she cannot impersonate either of them?  Or does it mean something else?
Can someone better explain how the "forward security" found in EKE/DH-EKE/SPEKE works?  Is it the same for each EKE variant, or does it work differently for each?
Ashamedly Confused,
- Anonymous.

@_date: 2002-08-01 16:15:18
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
You might be surprised to learn that under the TCPA, it is not necessary
for the TPM (the so-called "Fritz" chip) to trust *any* signing keys!
The TCPA basically provides two kinds of functionality: first, it can
attest to the software which was booted and loaded.  It does this by
taking hashes of the software before transferring control to it, and
storing those hashes in its internal secure registers.  At a later
time it can output those hashes, signed by its internal signature
key (generated on-chip, with the private key never leaving the chip).
The system also holds a cert issued on this internal key (which is called
the Endorsement key), and this cert is issued by the TPM manufacturer
(also called the TPME).  But this functionality does not require storing
the TPME key, just the cert it issued.
Second, the TCPA provides for secure storage via a "sealing" function.
The way this works, a key is generated and used to encrypt a data blob.
Buried in the blob can be a hash of the software which was running
at the time of the encryption (the same data which can be reported
via the attestation function).  Then, when the data is decrypted and
"unsealed", the hash is compared to that which is in the TPM registers
now.  This can make it so that data which is encrypted when software
system X boots can only be decrypted when that same software boots.
Again, this functionality does not require trusting anyone's keys.
Now, there is an optional function which does use the manufacturer's key,
but it is intended only to be used rarely.  That is for when you need to
transfer your sealed data from one machine to another (either because you
have bought a new machine, or because your old one crashed).  In this
case you go through a complicated procedure that includes encrypting
some data to the TPME key (the TPM manufacturer's key) and sending it
to the manufacturer, who massages the data such that it can be loaded
into the new machine's TPM chip.
So this function does require pre-loading a manufacturer key into the
TPM, but first, it is optional, and second, it frankly appears to be so
cumbersome that it is questionable whether manufacturers will want to
get involved with it.  OTOH it is apparently the only way to recover
if your system crashes.  This may indicate that TCPA is not feasible,
because there is too much risk of losing locked data on a machine crash,
and the recovery procedure is too cumbersome.  That would be a valid
basis on which to criticize TCPA, but it doesn't change the fact that
many of the other claims which have been made about it are not correct.
In answer to your question, then, for most purposes, there is no signing
key that your TPM chip trusts, so the issue is moot.  I suggest that you
go ask the people who misled you about TCPA what their ulterior motives
were, since you seem predisposed to ask such questions.
The point of being anonymous is that there is no persistent identity to
attribute motives to!  Of course I have departed somewhat from this rule
in the recent discussion, using a single exit remailer and maintaining
continuity of persona over a series of messages.  But feel free to make
whatever assumptions you like about my motives.  All I ask is that you
respond to my facts.
Of course, speculation is entirely appropriate - when labeled as such!
But David Wagner gave the impression that he was talking about facts
when he said,
   "The world is moving toward closed digital rights management systems
   where you may need approval to run programs," says David Wagner,
   an assistant professor of computer science at the University of
   California at Berkeley.  "Both Palladium and TCPA incorporate features
   that would restrict what applications you could run."
Do you think he was speculating?  Or do you agree that if he makes
such statements, he should base them on fact?  TCPA appears to have
no mechanism for the user to need approval in order to run programs.
That is how the facts look to me, and if anyone can find out otherwise,
I would appreciate knowing.  Maybe someone could ask David Wagner what
he based the above claim on?

@_date: 2002-08-01 16:45:15
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
We need to look at the text of this in more detail.  This is from
version 1.1b of the spec:
: This section introduces the architectural aspects of a Trusted Platform
: that enable the collection and reporting of integrity metrics.
: Among other things, a Trusted Platform enables an entity to determine
: the state of the software environment in that platform and to SEAL data
: to a particular software environment in that platform.
: The entity deduces whether the state of the computing environment in
: that platform is acceptable and performs some transaction with that
: platform. If that transaction involves sensitive data that must be
: stored on the platform, the entity can ensure that that data is held in
: a confidential format unless the state of the computing environment in
: that platform is acceptable to the entity.
: To enable this, a Trusted Platform provides information to enable the
: entity to deduce the software environment in a Trusted Platform. That
: information is reliably measured and reported to the entity. At the same
: time, a Trusted Platform provides a means to encrypt cryptographic keys
: and to state the software environment that must be in place before the
: keys can be decrypted.
What this means is that a remote system can query the local TPM and
find out what software has been loaded, in order to decide whether to
send it some data.  It's not that unapproved software "won't work",
it's that the remote guy can decide whether to trust it.
Also, as stated earlier, data can be sealed such that it can only be
unsealed when the same environment is booted.  This is the part above
about encrypting cryptographic keys and making sure the right software
environment is in place when they are decrypted.
But no, the TCPA does allow all software to run.  Just because a remote
system can decide whether to send it some data doesn't mean that software
can't run.  And just because some data may be inaccessible because it
was sealed when another OS was booted, also doesnt mean that software
can't run.
I think we agree on the facts, here.  All software can run, but the TCPA
allows software to prove its hash to remote parties, and to encrypt data
such that it can't be decrypted by other software.  Would you agree that
this is an accurate summary of the functionality, and not misleading?
If so, I don't see how you can get from this to saying that some software
won't run.  You might as well say that encryption means that software
can't run, because if I encrypt my files then some other programs may
not be able to read them.
Most people, as you may have seen, interpret this part about "software
can't run" much more literally.  They think it means that software needs
a signature in order to be loaded and run.  I have been going over and
over this on sci.crypt.  IMO the facts as stated two paragraphs up are
completely different from such a model.
That's true; in fact if you ran it earlier under TCPA and sealed some
data, you will have to run under TCPA to unseal it later.  The question
is whether the advantages of running under TCPA (potentially greater
security) outweigh the disadvantages (greater potential for loss of
data, less flexibility, etc.).
Right, the strongest case will probably be for DRM.  You might be able
to download all kinds of content if you are running an OS and application
that the server (content provider) trusts.  People will have a choice of
using TCPA and getting this data legally, or avoiding TCPA and trying to
find pirated copies as they do today.
I am inclined to agree; in fact I have made many postings (anonymously
of course) in recent weeks arguing that these systems will be entirely
voluntary.  If the functionality is useful, people will use it.
Software vendors who use TCPA will compete with those who don't.
The market will decide.  I am not as certain as you that TCPA will win,
but if it does, it will mean that TCPA is a good technology that solves
real problems for people.
The points I made earlier were that TCPA is unlikely to be mandated,
because it doesn't need to be; that TCPA should compete in the free
market with other solutions; and that this approach actually expands the
set of choices available to the participants.  More choice is always good.
Well, you would use TCPA.  But you have to look at how you got into that
situation.  The way it would have happened was by people voluntarily
adopting TCPA before it became a de facto standard, simply because it
was useful.
I have no credentials in this area other than a general knowledge of
crypto; I am just someone who was willing to devote some hours of his
free time to educating himself on this technology.  I agree that the
spec is written very poorly.
But let me put you on the spot: as someone who has a good
understanding of TCPA, what do you think of Ross Anderson's TCPA FAQ
at   For example,
how about his claim in answer 2, "Pirate software can be detected and
deleted remotely."  I must have missed that page of the TCPA spec.
And then he builds on this a couple of paragraphs later: "There will
be remote censorship: the mechanisms designed to delete pirated music
under remote control may be used to delete documents that a court (or
a software company) has decided are offensive...."  He further builds
on this later to claim (answer 11) that with TCPA, programs can be made
to ignore documents created by pirated versions of Word, etc.  All this
has so little to do with anything related to TCPA that it boggles my mind.
And then in answer 12 we're back to the claim that TCPA can stop computers
from booting.  Saddam better not buy a TCPA computer or the U.S. will
render it inoperative, using a "serial number revocation list", according
to the FAQ.  Are you aware of any such capability in TCPA?  I didn't see
any such data structure.
Ross Anderson means well, and so does David Wagner.  But in the long
run it hurts the credibility of critics when they make exaggerated and
unfounded claims.

@_date: 2002-08-02 15:30:03
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
Yes, my name is "AARG!".  That was the first thing my mother said after
I was born, and the name stuck.
Not really.  For Peter's information, the name associated with a
message through an anonymous remailer is simply the name of the
last remailer in the chain, whatever that remailer operator chose
to call it.  AARG is a relatively new remailer, but if you look at
 you will see that it is very
reliable and fast.  I have been using it as an exit remailer lately
because other ones that I have used often produce inconsistent results.
It has not been unusual to have to send a message two or three times
before it appears.  So far that has not been a problem with this one.
So don't read too much into the fact that a bunch of anonymous postings
have suddenly started appearing from one particular remailer.  For your
information, I have sent over 400 anonymous messages in the past year
to cypherpunks, coderpunks, sci.crypt and the cryptography list (35
of them on TCPA related topics).

@_date: 2002-08-02 16:56:42
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
Peter Trei envisions data recovery in a TCPA world:
It's not quite as bad as all this, but it is still pretty bad.
You don't have to send your data to Intel, just a master storage key.
This key encrypts the other keys which encrypt your data.  Normally this
master key never leaves your TPM, but there is this optional feature
where it can be backed up, encrypted to the manufacturer's public key,
for recovery purposes.  I think it is also in blinded form.
Obviously you'd need to do this backup step before the TPM crashed;
afterwards is too late.  So maybe when you first get your system it
generates the on-chip storage key (called the SRK, storage root key),
and then exports the recovery blob.  You'd put that on a floppy or some
other removable medium and store it somewhere safe.  Then when your
system dies you pull out the disk and get the recovery blob.
You communicate with the manufacturer, give him this recovery blob, along
with the old TPM key and the key to your new TPM in the new machine.
The manufacturer decrypts the blob and re-encrypts it to the TPM in the
new machine.  It also issues and distributes a CRL revoking the cert on
the old TPM key so that the old machine can't be used to access remote
TCPA data any more.  (Note, the CRL is not used by the TPM itself, it is
just used by remote servers to decide whether to believe client requests.)
The manufacturer sends the data back to you and you load it into the TPM
in your new machine, which decrypts it and stores the master storage key.
Now it can read your old data.
Someone asked if you'd have to go through all this if you just upgraded
your OS.  I'm not sure.  There are several secure registers on the
TPM, called PCRs, which can hash different elements of the BIOS, OS,
and other software.  You can lock a blob to any one of these registers.
So in some circumstances it might be that upgrading the OS would keep the
secure data still available.  In other cases you might have to go through
some kind of recovery procedure.
I think this recovery business is a real Achilles heel of the TCPA
and Palladium proposals.  They are paranoid about leaking sealed data,
because the whole point is to protect it.  So they can't let you freely
copy it to new machines, or decrypt it from an insecure OS.  This anal
protectiveness is inconsistent with the flexibility needed in an imperfect
world where stuff breaks.
My conclusion is that the sealed storage of TCPA will be used sparingly.
Ross Anderson and others suggest that Microsoft Word will seal all of
its documents so that people can't switch to StarOffice.  I think that
approach would be far too costly and risky, given the realities I have
explained above.  Instead, I would expect that only highly secure data
would be sealed, and that there would often be some mechanism to recover
it from elsewhere.  For example, in a DRM environment, maybe the central
server has a record of all the songs you have downloaded.  Then if your
system crashes, rather than go through a complicated crypto protocol to
recover, you just buy a new machine, go to the server, and re-download
all the songs you were entitled to.
Or in a closed environment, like a business which seals sensitive
documents, the data could be backed up redundantly to multiple central
file servers, each of which seal it.  Then if one machine crashes,
the data is available from others and there is no need to go through
the recovery protocol.
So there are solutions, but they will add complexity and cost.  At the
same time they do add genuine security and value.  Each application and
market will have to find its own balance of the costs and benefits.

@_date: 2002-08-03 10:55:19
@_author: AARG!Anonymous 
@_subject: Privacy-enhancing uses for TCPA 
Here are some alternative applications for TCPA/Palladium technology which
could actually promote privacy and freedom.  A few caveats, though: they
do depend on a somewhat idealized view of the architecture.  It may be
that real hardware/software implementations are not sufficiently secure
for some of these purposes, but as systems become better integrated
and more technologically sound, this objection may go away.  And these
applications do assume that the architecture is implemented without secret
backdoors or other intentional flaws, which might be guaranteed through
an open design process and manufacturing inspections.  Despite these
limitations, hopefully these ideas will show that TCPA and Palladium
actually have many more uses than the heavy-handed and control-oriented
ones which have been discussed so far.
To recap, there are basically two technologies involved.  One is "secure
attestation".  This allows machines to securely receive a hash of the
software which is running remotely.  It is used in these examples to
know that a trusted client program is running on the remote machine.
The other is "secure storage".  This allows programs to encrypt data
in such a way that no other program can decrypt it.
In addition, we assume that programs are able to run "unmolested";
that is, that other software and even the user cannot peek into the
program's memory and manipulate it or learn its secrets.  Palladium has
a feature called "trusted space" which is supposed to be some special
memory that is immune from being compromised.  We also assume that
all data sent between computers is encrypted using something like SSL,
with the secret keys being held securely by the client software (hence
unavailable to anyone else, including the users).
The effect of these technologies is that a number of computers across
the net, all running the same client software, can form their own
closed virtual world.  They can exchange and store data of any form,
and no one can get access to it unless the client software permits it.
That means that the user, eavesdroppers, and authorities are unable to
learn the secrets protected by software which uses these TCPA features.
(Note, in the sequel I will just write TCPA when I mean TCPA/Palladium.)
Now for a simple example of what can be done: a distributed poker game.
Of course there are a number of crypto protocols for playing poker on the
net, but they are quite complicated.  Even though they've been around
for almost 20 years, I've never seen game software which uses them.
With TCPA we can do it trivially.
Each person runs the same client software, which fact can be tested
using secure attestation.  The dealer's software randomizes a deck and
passes out the cards to each player.  The cards are just strings like
"ace of spades", or perhaps simple numerical equivalents - nothing fancy.
Of course, the dealer's software learns in this way what cards every
player has.  But the dealer himself (i.e. the human player) doesn't
see any of that, he only sees his own hand.  The software keeps the
information secret from the user.  As each person makes his play, his
software sends simple messages telling what cards he is exposing or
discarding, etc.  At the end each person sends messages showing what
his hand is, according to the rules of poker.
This is a trivial program.  You could do it in one or two pages of code.
And yet, given the TCPA assumptions, it is just as secure as a complex
cryptographically protected version would be that takes ten times as
much code.
Of course, without TCPA such a program would never work.  Someone would
write a cheating client which would tell them what everyone else's cards
were when they were the dealer.  There would be no way that people could
trust each other not to do this.  But TCPA lets people prove to each
other that they are running the legitimate client.
So this is a simple example of how the secure attestation features of
TCPA/Palladium can allow a kind of software which would never work today,
software where people trust each other.  Let's look at another example,
a P2P system with anonymity.
Again, there are many cryptographic systems in the literature for
anonymous communication.  But they tend to be complicated and inefficient.
With TCPA we only need to set up a simple flooding broadcast network.
Let each peer connect to a few other peers.  To prevent traffic
analysis, keep each node-to-node link at a constant traffic level using
dummy padding.  (Recall that each link is encrypted using SSL.)
When someone sends data, it gets sent everywhere via a simple routing
strategy.  The software then makes the received message available to the
local user, if he is the recipient.  Possibly the source of the message
is carried along with it, to help with routing; but this information is
never leaked outside the secure communications part of the software,
and never shown to any users.
That's all there is to it.  Just send messages with flood broadcasts,
but keep the source locked inside the secure part.  Messages can be
sent and received, and neither participants nor outsiders can tell
what the source of any message is.
As with the earlier example, such a system would never work without TCPA.
Rogue software would easily determine which direction messages were coming
from, and the anonymity provided would be extremely limited
But by eliminating rogues using secure attestation, and keeping the
sensitive data safe from molestation, we are able to achieve using a
very simple system what otherwise takes tremendous complexity.
Here's one more example, which I think is quite amazing: untraceable
digital cash with full anonymity, without blinding or even any
cryptography at all! (Excepting of course the standard TCPA pieces like
SSL and secure storage and attestation.)
The idea is, again, trivial.  Making a withdrawal, the client sends the
user's password and account ID to the bank (this information is kept in
secure storage).  The bank approves, and the client increments the local
"wallet" by that amount (also kept in secure storage).  To make a payment,
use the anonymous network for transport, and just send a message telling
how much is being paid!  The recipient increments his wallet by that
amount and the sender decrements his.  Deposit works analogously to
Again, that's all there is to it.  Nothing could be simpler.  Yet it
provides for secure (assuming TCPA is secure), anonymous, untraceable
payments.  The secure attestation is crucial, of course, to make sure
that people are running legitimate clients, otherwise cheating would
be rampant.  And the secure storage is equally crucial, otherwise any
software could increment the sum stored in the wallet and everyone would
accept and believe those payments.
I understand, of course, that this specific example is not very practical
unless we have an extremely secure version of TCPA.  If anyone who
can break the security can give themselves unlimited money, it means
that the security has to be essentially perfect.  So this is more of a
proof of concept than a realistic proposal.  But eventually, with TCPA
technology integrated into a tamper-proof, nanotech CPU with molecular
sensors and built-in self-destructs, possibly this might be good enough.
Or you could augment this solution with some crypto, similar with the
"wallets with observers" proposals from Chaum and from Brands.  Note that
we can make the client open-source, allowing anyone to verify that it
has no back doors or cheating potentials, which allows all users to
trust that it is not going to hurt them (a problem that takes great
complexity to solve with the observer protocols).  But still the bare
simplicity of the system should make clear how powerful something like
TCPA can be for this kind of application.
I could go on and on, but the basic idea is always the same, and hopefully
once people see the pattern they will come up with their own ideas.
Being able to write software that trusts other computers allows for an
entirely new approach to security software design.  TCPA can enhance
freedom and privacy by closing off possibilities for surveillance and
interference.  The same technology that protects Sony's music content
in a DRM application can protect the data exchanged by a P2P system.
As Seth Schoen of the EFF paraphrases Microsoft, "So the protection of
privacy was the same technical problem as the protection of copyright,
because in each case bits owned by one party were being entrusted to
another party and there was an attempt to enforce a policy."
( 3rd bullet point)
In fact, TCPA and Palladium have tremendous potential for enhancing
and protecting privacy, if people will just look at them with an
open mind.

@_date: 2002-08-05 00:10:12
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
Actually there seem to be some hardware differences between TCPA and
Palladium.  TCPA relies on a TPM, while Palladium uses some kind of
new CPU mode.  Palladium also includes some secure memory, a concept
which does not exist in TCPA.
More generally, the hardware can attest to many aspects of the current
machine state, including the cumulative hash of the software which
has booted so far.
TCPA does not get into this part, only the Palladium white paper
mentions this.  However it does seem to be a logical component for
effective trusted computing.
Right.  This plus the attestation are what allow an application to create
a "closed world".  See my earlier message for examples of how this could
be used to enhance privacy and anonymity.  What better example of a
closed world than your own secrets?
I don't think his comments make that much sense.  I'd be curious to read
your take on them.  What is he talking about with the non-malleable
root of trusted storage?  Trusted storage seems like one of the least
objectionable aspects.  Is he confusing this with the endorsement key,
used to make the remote attestations?  Or is this related to the idea
that you won't be able to boot your OS of choice?
This is of course one of the biggest criticisms of TCPA - that it could be
changed so that you will only be able to boot certified OS's.  Don't you
think that would have to be done by law, rather than as a preemptive
act by the technologists (for antitrust reasons if nothing else)?
Why would such a law be passed?  IMO the social changes necessary to even
begin to imagine such a drastic step are so huge that the technological
implementation seems minor in comparison.  I don't think it is fair to
criticize this proposal for such a far-fetched possibility.
TCPA doesn't currently cover certifying operating systems.  They talk
about certifying TPMs, about certifying PC hardware designs and
implementations.  Possibly in the next version they will get into
issues like this.  In the mean time, supposedly HP is going forward
with an OS that can use TCPA features.
I think this analysis is largely correct, except that it won't be
as monolithic as you make it sound.  There won't be just one content
supplier who judges all software, that's obviously impossible.  Rather,
each different supplier will make its own determination of which software
you can trust.  And likewise for non-DRM applications.  Banks will decide
which banking software to trust.  Game networks will decide which game
clients to trust, etc.
I agree that it would be nice to see more flexibility there.  The Chaum
blinding patent expires in 2005, so maybe around then we can start seeing
privacy CA's that use blind signatures, which solves that problem.
The spec is obviously trying hard to protect privacy, it's just that
the mechanisms to do it right are extremely complex compared to the
straightforward way.
Nobody's putting a gun to your head and making you download content.
If you can't agree to the conditions, go do something else.  There are
much worse things that can happen in the world than that copyright
becomes enforceable.
Why not give the market a chance?  Company A provides the data with
Draconian DRM restrictions; company B gives you more flexibility in what
you do.  All else being equal, people will prefer company B.  So they
can charge more.  In this way a balance will be reached depending on how
much people really value this kind of flexibility and how much they are
willing to pay for it.  You and I don't get to decide, the people who
are making the decisions about what content to buy will decide.
And nobody's got the root key to my computer.  You make this claim in many
places in the document.  What exactly is this "root key" in TCPA terms?
The endorsement key?  It's private part is generated on-chip and never
leaves the chip!
I don't follow this.  What root key owners?  What APIs?  Could you say
more about how TCPA will help with software rental?
This is a good point, but again it depends on the specific content
realm.  There are not just one or two - there are thousands of kinds
of content, or even more.  Not everyone is going to require FIPS 140
levels of certification!
But possibly Disney and Sony will.  My guess is that if there ever is
a Linux program that will play their movies, it will be because those
companies contracted to get it written.  You may see this in many contexts
- software applications don't get certified, rather they are supplied
by the vendors, or the vendors arrange to get them done.
This part I don't understand too much; it's not a TCPA concept, and there
is little known about Palladium.  Supposedly the idea is that this is a
place that code can run without being touched by debuggers or viruses.
I don't know what happens if a virus gets itself loaded into this area,
if that is even possible.  Maybe all the different compartments are
isolated from each other.
Does this seem like a bad feature to you?
Now you're starting to go paranoid.  All the TCPA certification master
keys do is to certify that a system is TCPA compliant.  They don't have a
remote control over your machine!  They are more analogous to Verisign
in the X.509 world.  Last I checked they hadn't taken over my box.
As far as the field upgrade, it has to be authorized by the owner.
I'm disappointed to see this kind of fantasizing in what has been a
well grounded document until now.  If you're going to make this kind
of charge, that TCPA gives a universal remote control to government,
you need to back it up in detail.
I don't agree with your characterization that TCPA enforces policies
against the owner's interests.  He has to voluntarily agree to everything,
from turning on TCPA, to booting a TCPA compliant program, to running
an application which some third party will trust, to accepting data from
that third party under agreed-upon conditions.  If at any step he didn't
feel that what he was doing was in his interests, he can stop and do
something else.
When you walk into a store and pay money for food, is that store enforcing
policies against your interests?  Only from the most shallow perspective,
for if such policies were not widely enforced, you and I and everyone
else would starve.  We all participate voluntarily in these institutions.
Each payment we make is in our interests.  And the same thing is true
if you receive some data with conditions on how it is manipulated.
As far as the concern about changes, I think the smart thing to do is
to fight the bad and promote the good.  Definitely we should oppose any
proposal to make TCPA non-voluntary, to force people to boot a certain
OS, to limit what they can do on their computers.  But presently none
of those features are in TCPA.  Rather than saying TCPA is bad because
someone could make all these hypothetical changes, it makes more sense
to judge TCPA on its own, as a system that emphasizes user choice.
Involuntary TCPA is bad, voluntary is good.  So we should not fight TCPA,
we should fight proposals to make it involuntary.
I think you are looking at it far too narrowly.  Yes, this will provide
many opportunities for Microsoft to write new kinds of software.  But the
same is true for every other software company!  Financial software, web
services, security software, accounting - anything that involves trust
and security can benefit from TCPA.  Look at the example I gave earlier
for a TCPA based anonymous comm network.  Multiply that a thousand fold.
It's stupid to just look at what one company can do with this, without
considering what a whole world of creative people can accomplish.
Yes, it can make reverse engineering much more difficult.  But I'd rather
see people put their creative efforts into creating new products rather
than copying and piggybacking off someone else's success.
Again, you need to justify this remote root control notion.  I don't
see it at all.  Go back to your four functions of TCPA/Palladium -
they were pretty accurate.  Where was the remote root control in there?
I'd say that it is a powerful technology with an almost infinite number of
potential applications.  Being able to trust software running on a remote
system is something that has never been possible before on the Internet.
We can only begin to see what will be possible with this capability.

@_date: 2002-08-05 12:30:11
@_author: AARG!Anonymous 
@_subject: Suggested entry into the TCPA spec 
Here is a suggestion for how to appraoch the TCPA spec based on the parts
I have found to be relatively good explanations.  The spec is available
from First read the first few pages up to page 7.  This provides an overview
and a block diagram, although at this point not all the terms will be
familiar.  One hint: the "root of trust for measurement" is the set of
hardware which has to be working for the boot measurement process to work:
the CPU, the TPM, the part of the BIOS that deals with measurements, the
motherboard, the secure connections of the chips to the motherboard.
If all this stuff is OK then the measurements will be accurate.
(Measurements basically are hashes of code and of machine configuration
status.)  The "root of trust for reporting" is the endorsement key, or
more fundamentally the cert on the endorsement key.  The cert is issued by
the manufacturer, AKA the "TPM Entity" or TPME.  That's what makes other
people believe your attestations.  And the "root of trust for storage"
is the storage root key, described in the section on protected storage.
There are a few more pages of introduction which aren't too clear,
then a long section of data structures which should be skipped until
you need to reference them.
This brings you to page 97, authorization and ownership.  I haven't
really studied this part.  Probably just read this one page to get
an idea of what is involved.  I still need to learn more about this.
I'd skip on to pages 136-137, on the measurement process and the
PCRs which hold the results of the measurement.
Then I'd read pages 145-150 on protected storage.  This part is pretty
well written.  It is a reasonably self contained part of the TPM
functionality.  You just need to know a little bit about the PCRs from
the earlier section to understand how data is locked to the specific
program which is running.
Then I'd read page 261 on the endorsement key, and then 267-269 on
how it is used to create a pseudonymous identity.  This is the part
about communicating with the Privacy CA.  BTW an expert told me he has
concerns about possible security loopholes in this protocol, but he is
communicating with TCPA about them.
I think if you just read these selections, about 15 pages, you will have
a much better idea of how the spec works.  Then you can read some of
the specific API descriptions to see more details about the functionality.
There is also a glossary at the end which can be helpful for some (but
not all) of the terminology.
There is another spec,
that describes the specific register and trap binding for implementing
the TCPA API on Intel PCs.  It is much shorter but it is pretty
incomprehensible until you have at least read the basics of the main

@_date: 2002-08-05 16:25:26
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
Sure, but how many pages would it take in the spec to describe the
protocol?  Especially given their turgid technical-writer prose?
Brands took a whole book to describe his credentials thoroughly.
In any case, I agree that something like this would be an excellent
enhancement to the technology.  IMO it is very much in the spirit of TCPA.
I suspect they would be very open to this suggestion.
They don't say much about patents or intellectual property licensing in
the documents I have found on their site.  It's not clear to me that the
so-called Palladium patents actually cover TCPA.  You'd have to look at
them in detail.
What software updates, exactly?  Spec reference?
No, I don't recall seeing this in the spec.  Hopefully as you have a
chance to study it you can point out this part.  I may well have missed
a portion.  If so then I agree that this is a potentially serious problem.
I was talking about the optional TPM_FieldUpgrade function described on
page 251 of the spec.  It is apparently intended for bug fixes and such.
I doubt that there will be that many bug fixes, or that users will
install them that often.  And if an upgrade does obvious bad things
like the various despotic features you fear, keeping you from booting
Linux or whatever, people can avoid installing it.  I don't see this as
a mechanism for someone to take over the world.
It's true what you say about the user having to trust the manufacturer
about the upgrade - but he has to trust the manufacturer anyway that
the chip works right.  Whatever monitoring process may be in place to
further that trust can also protect the upgrade as well.
Well, he can choose who he buys the TPM chip from, I suppose.
But upgrades are basically new firmware for the TPM chip, so they will
probably always come from the manufacturer.
Why exactly is this so much more of a threat than, say, flash BIOS
upgrades?  The BIOS has a lot more power over your machine than the
TPM does.
Everything I have read indicates that he can boot other operating systems.
And the spec seems to bear that out.  I don't see anywhere in there that
it would stop people from booting whatever software they want.
The only way that TCPA will become as popular as you fear is if it really
solves problems for people.  Otherwise nobody will pay the extra $25 to
put it in their machine.
That's largely the case already.  That's why so many people choose
Windows, to be compatible with what everyone else is doing.
You seem to judge things by the outcome: Windows being more popular
and powerful is bad.  I judge by process: letting people make their own
decisions is good, even if it leads to an outcome I personally don't like.
Where would that fit in the spec?  The spec is intentionally not about
policy; there is no such thing as a "TCPA policy module".  How would
TCPA stop people from running their own strong cryptography?  You are
extrapolating way, way beyond anything that is in this spec.  This is
just imagination and paranoia.
Be concrete.  What changes would have to be made to TCPA to get the
effects of a mandatory Clipper chip.  Would they be made in secret or
would some government have to pass a law before it happened?  Would the
changes happen in one country or all countries?  Paint me a scenario
that has some kind of connection to reality.  Otherwise this sounds
like South Park logic:
1. Get TCPA widely used
2. ...
3. Take over the world
Again, what specific TCPA features will they exploit to accomplish this?
That's already the case.  Face it: if government decided to enforce
mandatory key escrow, most users would not object and would be unable
to help themselves if they did, whether TCPA existed or not.
That's possible, and if we lived in a dictatorship I would be more
concerned.  But if new technologies make laws more enforceable, and
people are uncomfortable with the loss of freedom, they will vote to
relax restrictions.
And as I have pointed out, it is possible that TCPA could allow for
other applications that would actually magnify freedom.  The thrust of
the proposal is to improve the ability for applications to keep their
secrets, both locally and on the net.
I'm just looking at the TCPA spec and trying to evaluate it.  I don't
see all the bad things that people have said are in there.  Instead I
see a lot of effort to provide security while still protecting user
control and privacy.  It's true that some developers may use the new
power of TCPA for bad purposes, while other people will use it for good.
I say, let us focus our criticism, don't waste time trashing a proposal
because of things that are not in it.
I just don't see that TCPA is of that much use to them, given that they
already have essentially unlimited power.  Ultimately, in the West,
governments are the responsibility of the populace.
If the Chinese government were to do a TCPA-like system, I doubt that
it would look much like this one.
Even if so, that's no excuse for trying to stop people from making their
own decisions about what to do with their resources.  You shouldn't stop
people from using a technology because you are afraid that someone else
may come along and make it mandatory.
Absolutely!  I fully agree with these sentiments.  I think as you study
the spec in detail you will see that it does a very good job by these
standards.  But ultimately you can't let users take control of their TPM
chip and force it to lie to other people, without losing the whole point
of the system.  Doing that would be like insisting on a PKI where every
user could make arbitrary modifications to certs issued by other people.
Sure, in some sense it may increase freedom, but it's at the cost of
making the whole infrastructure worthless.
The thing that makes your certified key useful is the raw fact that you
can't change it.  By the same token, the thing that makes TCPA useful
is the fact that you can't get at sealed data or get the system to lie.
By voluntarily giving up this ability locally, you gain tremendous power
in interacting with other people.

@_date: 2002-08-06 11:55:14
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
Gadzooks, I meant "authoritative"!  Please accept my apologies, I should
have proofread more closely before sending.

@_date: 2002-08-06 11:55:24
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
A few questions and comments for Peter Biddle.  First, I want to
thank him for speaking up; it is great to get detailed answers from an
authoritarian source.
Since this is the cryptography list, could you say something about the
crypto involved:
 - Like TCPA, you have a secure chip, which you call the SCP, "secure
   crypto processor," somewhat analogous to the TPM, right?  And it
   presumably generates a unique per-machine public key, of which the
   secret component never leaves the chip?  And then someone (who?)
   issues a cert on this public key, and that is the basis for the trust
   in the remote attestation?
 - One feature missing from TCPA is secure memory.  This is some memory
   which can only be accessed by, what, trusted code?  Does this mean
   that only Microsoft-signed code can run secure?  (I don't think so,
   but many people have this idea.)
 - To clarify: you said the TOR could load long after boot, even days
   after booting?  So it is not really a microkernel that runs below all
   of Windows - it is more like a device driver which handles the secure
   memory and the SCP hardware, so if you aren't using Palladium-aware
   apps you don't need to load it?
 - There is also a new CPU mode, which I think is related to the secure
   memory.  Could you explain how these work together?  Is it that the
   secure memory is only visible in the new CPU mode, and/or that the
   new CPU mode can only run out of secure memory?
 - At some point the SCP must produce a hash or fingerprint of an
   application, or a portion of an application, for remote attestation
   (and also for local sealing/unsealing).  Is this done when the code
   is loaded into secure memory?  Or how/when?
 - Regarding privacy: in TCPA the user reveals his per-chip key only
   to the Privacy CA, when then gives him a cert on his newly-generated
   identity key.  Then remote entities only see the identity key cert.
   However this means that the remote entities must trust the Privacy CA
   since that is the only one who vouches for the legitimacy of the TPM.
   So the user in practice doesn't have that much choice of Privacy
   CA, it has to be one of the ones that the remote service provider
   has authorized to be trusted by it.  Does your system avoid this
   limitation?
 - What if someone wanted to make a Palladium-enhanced Napster?  They
   could use the security features to pass around lists of songs and
   users who had them, but not make this detailed information available
   to users.  This would make it harder for the record companies to get a
   list of all the participants in order to sue and shut them down.  The
   program could also share secure hashes of songs and make it difficult
   for the RIAA to load bogus songs into the system with rogue clients.
   Extensions to facilitate software piracy are left to the imagination.
   Is Palladium "open" enough even for this kind of hackerish application?
   Or will Microsoft be a gatekeeper who can control which apps can run?
 - Speaking of piracy, how exactly does Palladium solve the BORA problem?
   (Break Once Run Anywhere).  Would all apps be distributed sealed to
   a particular machine?  Would this mean that all programs would have
   to run in secure memory?  Otherwise when they are unsealed and loaded
   into insecure memory in plaintext, someone could save that data, right?
 - How about with DRM, if someone hacks their hardware and is able
   to break the restrictions on a piece of content (i.e. they manage
   to get a copy of the unsealed form).  Then they can distribute that
   anywhere, right?  Anyone will be able to play it once one person
   extracts it from the secure "vault".  So don't you still have a
   BORA problem?  Many people charge that this means that Microsoft has
   a secret plan to shut down pirate applications!  See Ross Anderson's
   "FAQ".
Thanks again for your informative comments.

@_date: 2002-08-06 15:15:17
@_author: AARG!Anonymous 
@_subject: USENIX Security TCPA/Palladium Panel Wednesday 
Amazing claims you are making there.  Claiming that the TPM will be
included on "all future motherboards"; claiming that an objective is
to meet the operational needs of law enforcement and intelligence;
claiming that TCPA members (all 170 of them?) have more access to his
computer than the owner; fantasizing about an "approved hardware list"
and "serial number revocation list" which don't exist in the spec(!);
further fantasies about a "list of undesirable applications" (where do
you get this stuff!).
On page 16, the OS is going to start the secure time counter (but TCPA
has no secure time feature!); synchronize time against authenticated
time servers (again, no such thing is in the spec); and download the
hardware and serial number revocation lists (nothing exists like this!).
I honestly don't understand how you can say this when there is nothing
like it in the TCPA specification.
Are you talking to insiders about a future revision?  Do you know for
a fact that TCPA will hae SNRL's and such in the future?
Or are you just being political, trying to increase pressure on TCPA
*not* to go with serial number revocation lists and the like, by falsely
claiming that this is in the design already?

@_date: 2002-08-06 15:20:02
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
This has to be true for the basic security goal of remote trust, right?
The purpose is so that the user can credibly convince a remote system that
he is running a certain program.  Explain to me how he could do this if
he were able to reload the TPM key with one of his own, or get access
to the private key?  Wouldn't that let him forge arbitrary messages?
You might as well complain that Verisign doesn't share their private key
with everyone.  Either way you lose the trust properties of the system.
We have had other systems which work like this for a long while.
Many consumer devices are sealed such that if you open them you void
the warranty.  This is to your advantage as a consumer; it means that you
can take the device in to get it fixed, and the intact seal proves that
you didn't mess with the insides and break it.  By your logic, consumers
ought to be able to bypass such seals since they own the device.  But if
this were true, don't you agree that it would make the seals useless?

@_date: 2002-08-06 15:30:13
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
I have in fact never claimed to be a TCPA insider; quite the opposite,
I have consistently explained that I am merely someone who has taken the
time to study the specification and other documents in order to educate
myself about the system.
My interpretation of the spirit of the proposal comes solely from
reading these documents.  They go to considerable lengths to protect user
privacy, even to the point that the main TPM key is an encrypt-only key,
not allowed to issue signatures!  I think this is to reduce the chance
of mistakenly using it to sign attestations.  Further, the protocol
with the Privacy CA is very complex and adds considerable complexity.
If they didn't care about privacy I don't think the design would devote
this much effort to it.
Maybe this is true, but I can certainly imagine reasons other than
a secret desire to compromise users' privacy.  Going with blinding
would make the spec more complex, and they might well have had their
hands full at the time just trying to get V1.0 out.  Then there are the
patent issues with either Chaum or Brands blinding.  Plus, Brands works
with very special-format keys, variants on discrete log keys, while the
spec generally assumes RSA keys (possibly going to ECC).  And finally,
they may simply not have been that familiar with blinding technology,
which isn't that widely known outside a small subset of the cryptographic
community.  TCPA is more of a security spec than a cryptographic one,
and it's likely that not one of the main developers had every read a
paper by Stefan Brands.
Besides, after reading Lucky's absurdly conspiratorial slide show I am
skeptical about how accurately he can be relied on to report information
about TCPA.  He obviously thinks they are the spawn of the devil
and is willing to say anything in public in order to discredit them.
Otherwise why would he have made so many charges at Defcon that are
utterly without foundation?

@_date: 2002-08-07 12:35:12
@_author: AARG!Anonymous 
@_subject: Palladium: technical limits and implications 
Obviously no application can reliably know anything if the OS is hostile.
Any application can be meddled with arbitrarily by the OS.  In fact
every bit of the app can be changed so that it does something entirely
different.  So in this sense it is meaningless to speak of an app that
can't be lied to by the OS.
What Palladium can do, though, is arrange that the app can't get at
previously sealed data if the OS has meddled with it.  The sealing
is done by hardware based on the app's hash.  So if the OS has changed
the app per the above, it won't be able to get at old sealed data.
And of course remote attestation will not work either, if the app has
been meddled with.
This means that an app can start running, attest to its "clean" status
to a remote server, download some data from that server, and seal it.
Then at a later time, IF the app is able to unseal that data, then it
is true that the app has not been meddled with and is not running
on virtualized hardware.
That is how I understand these sorts of claims.

@_date: 2002-08-07 12:50:29
@_author: AARG!Anonymous 
@_subject: Challenge to TCPA/Palladium detractors 
I'd like the Palladium/TCPA critics to offer an alternative proposal
for achieving the following technical goal:
  Allow computers separated on the internet to cooperate and share data
  and computations such that no one can get access to the data outside
  the limitations and rules imposed by the applications.
In other words, allow a distributed network application to create a
"closed world" where it has control over the data and no one can get
the application to "cheat".  IMO this is clearly the real goal of TCPA
and Palladium, in technical terms, when stripped of all the emotional
As I posted previously, this concept works especially well for open source
applications.  You could even have each participant compile the program
himself, but still each app can recognize the others on the network and
cooperate with them.  And this way all the participants can know that
the applications aren't doing anything different than what they claim.
This would be a very powerful capability with many uses that you might
find both good and bad.  I posted a long message earlier with three
examples of privacy-oriented applications: secure game playing, anonymous
P2P networking, and untraceable digital cash.  In addition it can be used
for DRM, restricting access to sensitive business or government data,
and similar applications.
For those of you who claim that such a technology is not necessarily
objectionable in itself, but that the implementations in TCPA and
Palladium are flawed, please explain how you could do it better.  How can
you maximize user control and privacy and minimize the potential for
government or corporate takeovers?
In other words, what *exactly* is wrong with the way that TCPA and
Palladium choose to do things?  Can you fix those problems and still
achieve the basic goal, above?

@_date: 2002-08-07 16:55:11
@_author: AARG!Anonymous 
@_subject: Palladium: hardware layering model 
I don't think this is right, as Peter said that the Palladium stuff could
load many days after boot.  So I don't think the "ring-0" mode underlies
normal supervisor mode as you have shown it.  Instead I think they are
relatively orthogonal.
I'm not sure how to draw it, but I would envision the TOR as a device
driver which controls two devices: the trusted execution space, which
is some special memory (on the cpu?), and the SCP, the crypto processor.
Let us suppose that there is a special instruction to load a block of
code into the trusted execution space and give it a new process ID.
At the same time this causes the SCP to hash it so that it can attest
to it later.  Let us also suppose that the ring-0 mode is used only when
running code out of the trusted execution space (TE space).
What is special about ring-0?  Two things: first, it can see the code
in the TE space so that it can execute it.  And second, it doesn't
trap into supervisor mode for things like debugger single-stepping.
I'm not familiar with the details of the Pentium family but on most CPUs
the debugger single-steps things by setting a flag and returning into
the code.  The code executes one instruction and then automatically traps
into supervisor mode, which hands off to the debugger.  This process must
be suppressed in ring-0 mode, and likewise for any other features which
can force a ring-0 process to trap involuntarily into supervisor mode,
which exposes the registers and such.
The TOR would then manage the various processes running in the TE space,
and their interactions with ordinary code, and possibly the interactions
of both with the SCP.  I'm not sure if the TOR runs in ring-0 mode and
in the TE space; probably it does, as the SCP can attest to it, and we
wouldn't want non-Palladium processes to debug it.
So really the whole TOR/SCP/TE-space/trusted-agents stuff is relatively
orthogonal to the rest of windows.  It's almost like you had a fully
functional 2nd CPU in there that you could load code into and have it run;
that CPU's memory is the TE space, its mode is the ring-0, it has access
to the SCP, and it runs the TOR and trusted agents.  But Palladium has
to use the regular CPU for this so they firewall it off with the ring-0
mode which locks it into this restrictive mode.
That's just a guess, probably wrong in many details, but consistent
with what I understand so far.  Mostly I am hoping to encourage Peter
to come forward and correct our misconceptions.
I have this as well; loading a user agent into TE space creates the hash
"fingerprint" which will be used for sealing and attestation; other ring-0
agents will have their own fingerprints and won't be able to unseal what
this agent does.  The SCP compares fingerprints at unseal time to what
it was at seal time and (optionally) won't unseal if they don't match.
(This is one of multiple sealing options.)
I don't think so; not necessary in my model, would require significant
re-architecting of Windows which won't happen, and inconsistent with
the claim that Palladium can load days after boot.
Must be true.  Some questions: how big is the TE space?  How many agents
can live there at once?  Do they swap in/out?  Does data go there, or
just code?
But ring-0 cannot make arbitrary restrictions on sup. mode.  Remember
they can't afford to re-architect either the entire CPU nor the entire
OS for this.  The simplest is that in ring-0 mode you disable certain
functions that could trap you into supervisor mode thereby losing control
of the CPU, and this ring-0 mode gains you access to the TE space.
I'm not much of artist but I would put the new stuff off to the side of
this in its own tower.  Ring-0 mode at the bottom, running the TOR which
is shown above it, which manages the user agents which would be on top.
The SCP is further off to the side, perhaps managed by the TOR.
I'm not sure what you mean by the OS observing system calls.
By definition, system calls go into the OS.  So I don't think that will
ever stop happening.  But it does mean that when a ring-0 trusted agent
makes a system call, we change to normal supervisor mode which makes
the trusted space invisible.
The point we are dancing around is this.  How does it protect the
data, along the whole path from the remote machine, through where it is
processed locally, until it is sealed on the local disk?  It seems that
it must be in the clear for a while on the local machine.  Where is that -
in regular memory, or TE space?
It's not that big a deal to be unable to read TE *code*.  From what
Peter says, that is typically not encrypted on the disk.  So the code
is no secret.  What we must be unable to read are the data being handled
by this code: the registers, the contents of memory that are sensitive.
And by the registers I include the PC, since that would leak information
about the data.  We can't single-step it, we can't put breakpoints into
it, we can't change it while it is running.
I am curious about this from the technical perspective.  I think this is
one of the most interesting developments in many years on the security
But frankly I don't think it will do them much good to tell you and most
other cypherpunks about it, because whatever they say, you and others will
twist it and lie if necessary a la Lucky to turn it into some horrible
perversion.  Even if the design were totally benign, that doesn't mean
Microsoft/Intel couldn't change it someday, right?  They could put a
machine gun into every PC aimed at the user, and a camera over his head.
That's the level of reasoning in Lucky's Defcon presentation, except
that he says that they've already done it.  I applaud Peter's patience
but pity him for his naive belief that he is engaging in a good faith
exchange where he will get a fair hearing.
I think this is pretty likely, but with the data encrypted by the time
the supervisor mode sees it.
If the data is in the clear, it would undermine the security guarantees!
Look at my online poker game - if the dealer can tap into the data going
out, he will learn what everyone else's hands are.  Look at the anonymous
network - an eavesdropper can learn where all the data is and how it
is flowing.  The data must not be made available in the clear anywhere
the user can get at it, to provide the proper security.
Well, he *does* know at least the address the data is going to.  There's
no way to hide that (short of anonymous message forwarding).
There are some applications which will still work if all the users can
*see* all of the data, but just not modify it.  Maybe my digital cash
example would fall into that category.  You can see how much you're
spending, but you can't manipulate your wallet.  But there are many
others, such as those above, where being able to hide even information
disclosure from network participants adds tremendous power.
You remember the Eternity network, how one concept had files being
shared across multiple nodes such that no one knew which files were on
his own computer.  That was crucially important for non-repudiation and
censorship-resistance.  This was done with cryptography, but the point
is the same: hiding information from network participants can greatly
increase security.  With TCPA/Palladium you can get some of the same
security properties with much simpler ode (with admittedly lower levels
of security until hardware improves).
Skipping down...
I am assuming that you are attesting to the remote system, and you only
can control your local one.  You want to get something from the remote,
and it will only give it if you are running "clean" on real hardware.
So you can't virtualize and still attest, since ultimately you don't
have a TPM endorsement key (or Palladium equivalent) with a nice TPME
endorsement certificate issued on it.
Of course if you have control of the server machine, you can ignore
attestation.  But that just says that the operator of the remote machine
can choose for himself which apps to run.  He can run an app that checks
remote client integrity or he can run one which doesn't care.
I'm not sure I follow this, but it sounds like you are talking about
manipulating the server machine doing the checks, while in most cases
you can only manipulate the client machine making the request.

@_date: 2002-08-08 16:55:45
@_author: AARG!Anonymous 
@_subject: Challenge to TCPA/Palladium detractors 
Matt Crawford replied:
It's likely that only a limited number of compiler configurations would
be in common use, and signatures on the executables produced by each of
those could be provided.  Then all the app writer has to do is to tell
people, get compiler version so-and-so and compile with that, and your
object will match the hash my app looks for.

@_date: 2002-08-09 10:00:18
@_author: AARG!Anonymous 
@_subject: dangers of TCPA/palladium 
Kragen Sitaker has a .sig that puts the concern concisely:
: Perilous to all of us are the devices of an art deeper than we possess
: ourselves.
:        -- Gandalf the White [J.R.R. Tolkien, "The Two Towers", Bk 3, Ch. XI]
It's a good question, although in practice the answer is that, first,
we don't have much choice given the technology we have, and second,
that it lets you do incredibly useful things.
But it is good to keep in mind that you can't avoid trusting others.
After all, every time you drive a car, or take the train across a bridge,
or cross a busy street for that matter, you are trusting your life
to others.

@_date: 2002-08-09 10:05:15
@_author: AARG!Anonymous 
@_subject: Thanks, Lucky, for helping to kill gnutella 
An article on Salon this morning (also being discussed on slashdot),
discusses how the file-trading network Gnutella is being threatened by
misbehaving clients.  In response, the developers are looking at limiting
the network to only authorized clients:
They intend to do this using digital signatures, and there is precedent
for this in past situations where there have been problems:
Not discussed in the article is the technical question of how this can
possibly work.  If you issue a digital certificate on some Gnutella
client, what stops a different client, an unauthorized client, from
pretending to be the legitimate one?  This is especially acute if the
authorized client is open source, as then anyone can see the cert,
see exactly what the client does with it, and merely copy that behavior.
If only there were a technology in which clients could verify and yes,
even trust, each other remotely.  Some way in which a digital certificate
on a program could actually be verified, perhaps by some kind of remote,
trusted hardware device.  This way you could know that a remote system was
actually running a well-behaved client before admitting it to the net.
This would protect Gnutella from not only the kind of opportunistic
misbehavior seen today, but the future floods, attacks and DOSing which
will be launched in earnest once the content companies get serious about
taking this network down.
If only...  Luckily the cypherpunks are doing all they can to make sure
that no such technology ever exists.  They will protect us from being able
to extend trust across the network.  They will make sure that any open
network like Gnutella must forever face the challenge of rogue clients.
They will make sure that open source systems are especially vulnerable
to rogues, helping to drive these projects into closed source form.
Be sure and send a note to the Gnutella people reminding them of all
you're doing for them, okay, Lucky?

@_date: 2002-08-09 16:10:08
@_author: AARG!Anonymous 
@_subject: No subject 
Adam Back writes a very thorough analysis of possible consequences of the
amazing power of the TCPA/Palladium model.  He is clearly beginning to
"get it" as far as what this is capable of.  There is far more to this
technology than simple DRM applications.  In fact Adam has a great idea
for how this could finally enable selling idle CPU cycles while protecting
crucial and sensitive business data.  By itself this could be a "killer
app" for TCPA/Palladium.  And once more people start thinking about how to
exploit the potential, there will be no end to the possible applications.
Of course his analysis is spoiled by an underlying paranoia.  So let me
ask just one question.  How exactly is subversion of the TPM a greater
threat than subversion of your PC hardware today?  How do you know that
Intel or AMD don't already have back doors in their processors that
the NSA and other parties can exploit?  Or that Microsoft doesn't have
similar backdoors in its OS?  And similarly for all the other software
and hardware components that make up a PC today?
In other words, is this really a new threat?  Or are you unfairly blaming
TCPA for a problem which has always existed and always will exist?

@_date: 2002-08-09 17:15:19
@_author: AARG!Anonymous 
@_subject: TCPA/Palladium -- likely future implications 
I want to follow up on Adam's message because, to be honest, I missed
his point before.  I thought he was bringing up the old claim that these
systems would "give the TCPA root" on your computer.
Instead, Adam is making a new point, which is a good one, but to
understand it you need a true picture of TCPA rather than the false one
which so many cypherpunks have been promoting.  Earlier Adam offered a
proposed definition of TCPA/Palladium's function and purpose:
IMO this is total bullshit, political rhetoric that is content-free
compared to the one I offered:
: Allow computers separated on the internet to cooperate and share data
: and computations such that no one can get access to the data outside
: the limitations and rules imposed by the applications.
It seems to me that my definition is far more useful and appropriate in
really understanding what TCPA/Palladium are all about.  Adam, what do
you think?
If we stick to my definition, you will come to understand that the purpose
of TCPA is to allow application writers to create closed spheres of trust,
where the application sets the rules for how the data is handled.  It's
not just DRM, it's Napster and banking and a myriad other applications,
each of which can control its own sensitive data such that no one can
break the rules.
At least, that's the theory.  But Adam points out a weak spot.  Ultimately
applications trust each other because they know that the remote systems
can't be virtualized.  The apps are running on real hardware which has
real protections.  But applications know this because the hardware has
a built-in key which carries a certificate from the manufacturer, who
is called the TPME in TCPA.  As the applications all join hands across
the net, each one shows his cert (in effect) and all know that they are
running on legitimate hardware.
So the weak spot is that anyone who has the TPME key can run a virtualized
TCPA, and no one will be the wiser.  With the TPME key they can create
their own certificate that shows that they have legitimate hardware,
when they actually don't.  Ultimately this lets them run a rogue client
that totally cheats, disobeys all the restrictions, shows the user all
of the data which is supposed to be secret, and no one can tell.
Furthermore, if people did somehow become suspicious about one particular
machine, with access to the TPME key the eavesdroppers can just create
a new virtual TPM and start the fraud all over again.
It's analogous to how someone with Verisign's key could masquerade as
any secure web site they wanted.  But it's worse because TCPA is almost
infinitely more powerful than PKI, so there is going to be much more
temptation to use it and to rely on it.
Of course, this will be inherently somewhat self-limiting as people learn
more about it, and realize that the security provided by TCPA/Palladium,
no matter how good the hardware becomes, will always be limited to
the political factors that guard control of the TPME keys.  (I say
keys because likely more than one company will manufacture TPM's.
Also in TCPA there are two other certifiers: one who certifies the
motherboard and computer design, and the other who certifies that the
board was constructed according to the certified design.  The NSA would
probably have to get all 3 keys, but this wouldn't be that much harder
than getting just one.  And if there are multiple manufacturers then
only 1 key from each of the 3 categories is needed.)
To protect against this, Adam offers various solutions.  One is to do
crypto inside the TCPA boundary.  But that's pointless, because if the
crypto worked, you probably wouldn't need TCPA.  Realistically most of the
TCPA applications can't be cryptographically protected.  "Computing with
encrypted instances" is a fantasy.  That's why we don't have all those
secure applications already.
Another is to use a web of trust to replace or add to the TPME certs.
Here's a hint.  Webs of trust don't work.  Either they require strong
connections, in which case they are too sparse, or they allow weak
connections, in which case they are meaningless and anyone can get in.
I have a couple of suggestions.  One early application for TCPA is in
closed corporate networks.  In that case the company usually buys all
the computers and prepares them before giving them to the employees.
At that time, the company could read out the TPM public key and sign
it with the corporate key.  Then they could use that cert rather than
the TPME cert.  This would protect the company's sensitive data against
eavesdroppers who manage to virtualize their hardware.
For the larger public network, the first thing I would suggest is that
the TPME key ought to be in hardware, so it can't be given out freely.
Of course the NSA could still come in and get their virtual-TPM keys
signed one at a time.  So the next step is that the device holding the
TPME key must be managed in a high security environment.  This may be
difficult, given the need to sign potentially thousands of TPM keys a
day, but I think it has to be done.  I want to see watchdogs from the
EFF and a lot of other groups sitting there 24 hours a day watching over
the device.  Remember how Clipper was going to use a vault, split keys
and all this elaborate precautions?  We need at least that much security.
Think about it: this one innocuous little box holding the TPME key could
ultimately be the root of trust for the entire world.  IMO we should
spare no expense in guarding it and making sure it is used properly.
With enough different interest groups keeping watch, we should be able
to keep it from being used for anything other than its defined purpose.

@_date: 2002-08-09 19:30:09
@_author: AARG!Anonymous 
@_subject: Challenge to TCPA/Palladium detractors 
Re the debate over whether compilers reliably produce identical object
(executable) files:
The measurement and hashing in TCPA/Palladium will probably not be done
on the file itself, but on the executable content that is loaded into
memory.  For Palladium it is just the part of the program called the
"trusted agent".  So file headers with dates, compiler version numbers,
etc., will not be part of the data which is hashed.
The only thing that would really break the hash would be changes to the
compiler code generator that cause it to create different executable
output for the same input.  This might happen between versions, but
probably most widely used compilers are relatively stable in that
respect these days.  Specifying the compiler version and build flags
should provide good reliability for having the executable content hash
the same way for everyone.

@_date: 2002-08-09 20:25:40
@_author: AARG!Anonymous 
@_subject: Thanks, Lucky, for helping to kill gnutella 
Several people have objected to my point about the anti-TCPA efforts of
Lucky and others causing harm to P2P applications like Gnutella.
Bran Cohen agrees:
I will just point out that it was not my idea, but rather that Salon
said that the Gnutella developers were considering moving to authorized
clients.  According to Eric, those developers are "fundamentally stupid."
According to Bram, the Gnutella developers don't understand their
own protocol, and they are supporting an idea which will not help.
Apparently their belief that clients like Qtrax are hurting the system
is totally wrong, and keeping such clients off the system won't help.
I can't help believing the Gnutella developers know more about their
own system than Bram and Eric do.  If they disagree, their argument is
not with me, but with the Gnutella people.  Please take it there.
Ant chimes in:
Pete Chown echoes:
As far as Freenet and MojoNation, we all know that the latter shut down,
probably in part because the attempted traffic-control mechanisms made
the whole network so unwieldy that it never worked.  At least in part
this was also due to malicious clients, according to the analysis at
  And Freenet has been
rendered inoperative in recent months by floods.  No one knows whether
they are fundamental protocol failings, or the result of selfish client
strategies, or calculated attacks by the RIAA and company.  Both of these
are object lessons in the difficulties of successful P2P networking in
the face of arbitrary client attacks.
Some people took issue with the personal nature of my criticism:
Right, as if my normal style has been so effective.  Not one person has
given me the least support in my efforts to explain the truth about TCPA
and Palladium.
Anyway, maybe I was too personal in singling out Lucky.  He is far from
the only person who has opposed TCPA.
But Lucky, in his slides at  claims that TCPA's
designers had as one of their objectives "To meet the operational needs
of law enforcement and intelligence services" (slide 2); and to give
privileged access to user's computers to "TCPA members only" (slide 3);
that TCPA has an OS downloading a "serial number revocation list" (SNRL)
which he has provided no evidence for whatsoever (slide 14); that it
loads an "initial list of undesirable applications" which is apparently
another of his fabrications (slide 15); that TCPA applications on startup
load both a serial number revocation list but also a document revocation
list, again a completely unsubstantiated claim (slide 19); that apps then
further verify that spyware is running, another fabrication (slide 20).
He then implies that the DMCA applies to reverse engineering when
it has an explicit exemption for that (slide 23); that the maximum
possible sentence of 5 years is always applied (slide 24); that TCPA is
intended to: defeat the GPL, enable information invalidation, facilitate
intelligence collection, meet law enforcement needs, and more (slide 27);
that only signed code will boot in TCPA, contrary to the facts (slide 28).
He provides more made-up details about the mythical DRL (slide 31);
more imaginary details about document IDs, information monitoring and
invalidation to support law enforcement and intelligence needs, none of
which has anything to do with TCPA (slide 32-33).  As apparent support for
these he provides an out-of-context quote[1] from a Palladium manager,
who if you read the whole article was describing their determination to
keep the system open (slide 34).
He repeats the unfounded charge that the Hollings bill would mandate TCPA,
when there's nothing in the bill that says such a thing (slide 35);
and he exaggerates the penalties in that bill by quoting the maximum
limits as if they are the default (slide 36).
Lucky can provide all this misinformation, all under the pretence,
mind you, that this *is* TCPA.  He was educating the audience, mostly
people who were completely unfamiliar with the system other than some
vague rumors.  And this is what he presents, a tissue of lies and
fabrications and unfounded sensationalism.
Don't forget, TCPA and Palladium were designed by real people.  In making
these charges, Lucky is not just talking about a standard, he is talking
about its authors.  He is saying that those people were attempting to
serve intelligence needs, to make sure that people had to run spyware,
to close down the system so it could keep "undesirable" applications off.
He is accusing the designers of far worse than anything I have said
about him.  He is basically saying that they are striving to bring about
a technological police state.
And yet, no one (other than me, of course) dared to criticize Lucky for
these claims.  He can say whatever he wants, be as outrageous as he wants,
and no one says a thing.  I don't know whether everyone agrees with him,
or is simply unwilling to risk criticism by departing from the groupthink
which is so universal around here.
I asked Eric Murray, who knows something about TCPA, what he thought
of some of the more ridiculous claims in Ross Anderson's FAQ (like the
SNRL), and he didn't respond.  I believe it is because he is unwilling
to publicly take a position in opposition to such a famous and respected
But anyway, maybe I was too personal in criticizing Lucky.  Tell you what.
I'll apologize to Lucky as soon as he apologizes to the designers of
TCPA for the fabrications in his slide show.  Deal?

@_date: 2002-08-10 11:40:14
@_author: AARG!Anonymous 
@_subject: responding to claims about TCPA 
John Gilmore replied:
Maybe, but he could reply just based on public information.  Despite this
he was unable or unwilling to challenge Ross Anderson.
I don't agree with this distinction.  If I use a smart card chip that
has a private key on it that won't come off, is that protecting me from
third parties, or vice versa?  If I run a TCPA-enhanced Gnutella that
keeps the RIAA from participating and easily finding out who is running
supernodes (see  for
the latest crackdown), I benefit, even though the system technically is
protecting the data from me.
I wrote earlier that if people were honest, trusted computing would not
be necessary, because they would keep their promises.  Trusted computing
allows people to prove to remote users that they will behave honestly.
How does that fit into your dichotomy?  Society has evolved a myriad
mechanisms to allow people to give strong evidence that they will keep
their word; without them, trade and commerce would be impossible.  By your
logic, these protect third parties from you, and hence should be rejected.
You would discard the economic foundation for our entire world.
David Grawrock of Intel has an interesting slide presentation on
TCPA at His slide 3 makes a good point: "All 5 members had very different ideas
of what should and should not be added."  It's possible that some of
the differences in perspective and direction on TCPA are due to the
several participants wanting to move in different ways.  Some may have
been strictly focused on DRM; others may have had a more expansive
vision of how trust can benefit all kinds of distributed applications.
So it's not clear that you can speak of the "real goal" of TCPA, when
there are all these different groups with different ideas.
Nonsense.  The web is ubiquitous, but is not a monopoly.
That same language is in the Credible Interoperability document presently
on the web site at
So I don't think there is necessarily any kind of a cover-up here.
Yes, DRM can clearly benefit from TCPA/Palladium.  And you might be
right that they are downplaying that now.  But the reason could be
that people have focused too much on it as the only purpose for TCPA,
just as you have done here.  So they are trying to play up the other
possibilities so as to get some balance in the discussion.
You are reading an awful lot into this one word "transaction".  That
doesn't necessarily mean buying digital content.  In the abstract sense
"transaction" is sometimes used to refer to any exchange of information in
a protocol.  Even if we do stick to its commercial meaning, it can mean
a B2B exchange or any of a wide range of other e-commerce activities.
It's not specific to DRM by any means.
I agree that the documentation is a problem, but IMO it probably reflects
lack of resources rather than obfuscation.  I believe that TCPA has many
more applications than you and other critics are giving it credit for,
and that a good, clear explanation of what it could do would actually
gain it support.  Do a blog search at daypop.com to see what people are
really thinking about TCPA.  They read Ross Anderson's TCPA FAQ and take
it for gospel.  They believe TCPA has serial number revocations and all
these other features that are not described in any documents I have seen.
A good clear TCPA description could only improve its reputation, which
certainly can't go any lower than it is.
I agree in principle, but I am appalled that you believe that Lucky in
particular is heading in the right direction.  Adam on the other hand
has at least begun to study TCPA and was asking good questions about
Palladium before Peter Biddle flew the coop.  Will this document say
that TCPA is designed to support intelligence agency access to computers?
to kill free software?  and other such claims from Lucky's presentation?
If so, you will only hurt your cause.  On the other hand, if you do
come up with factual and unbiased information showing both good and bad
aspects of TCPA, as I think Adam has come close to doing a few times,
then it could be a helpful document.
Conclusions should be based on technology.  TCPA can be rightly
criticized for weak protections of privacy, for ultimately depending on
the security of a few central keys and of possibly-weak hardware, and on
other technical grounds.  But you should not criticize it for supporting
DRM, or for making reverse engineering more difficult, because people
are under no obligation to give their creative works away for free,
or to make it easy for other people to copy their software.  Leave your
values at home and just present the facts.
No one has made any such allegation, although presumably it happens to
be true.  The point in contention is whether TCPA has DRLs!  Lucky has
claimed this, and Ross claimed the related serial number revocation
list, SNRL.  Both of them have linked this technology to TCPA/Palladium.
Yet as Ross admitted in
SNRL's do not need TCPA!
In fact, you are perfectly correct that Microsoft architectures would
make it easy at any time to implement DRL's or SNRL's.  They could do
that tomorrow!  They don't need TCPA.  So why blame TCPA for this feature?
TCPA is a technology.  You can't take every bad thing Microsoft ever
will do and say that TCPA is at fault.
I don't even see that TCPA would particularly help with a SNRL, except
insofar as TCPA can generally strengthen security in all respects.
But remote attestation and sealing, the core TCPA technologies, don't
have anything to do with SNRLs.
The association of TCPA with SNRLs is a perfect example of the bias and
sensationalism which has surrounded the critical appraisals of TCPA.
I fully support John's call for a fair and accurate evaluation of this
technology by security professionals.  But IMO people like Ross Anderson
and Lucky Green have disqualified themselves by virtue of their wild and
inaccurate public claims.  Anyone who says that TCPA has SNRLs is making
a political statement, not a technical one.  For a credible evaluation,
you need people who have no track record of bias with regard to the

@_date: 2002-08-10 15:15:07
@_author: AARG!Anonymous 
@_subject: Seth on TCPA at Defcon/Usenix 
Seth Schoen of the EFF has a good blog entry about Palladium and TCPA
at   He attended Lucky's
presentation at DEF CON and also sat on the TCPA/Palladium panel at
the USENIX Security Symposium.
Seth has a very balanced perspective on these issues compared to most
people in the community.  It makes me proud to be an EFF supporter
(in fact I happen to be wearing my EFF T-shirt right now).
His description of how the Document Revocation List could work is
interesting as well.  Basically you would have to connect to a server
every time you wanted to read a document, in order to download a key
to unlock it.  Then if "someone" decided that the document needed
to un-exist, they would arrange for the server no longer to download
that key, and the document would effectively be deleted, everywhere.
I think this clearly would not be a feature that most people would accept
as an enforced property of their word processor.  You'd be unable to
read things unless you were online, for one thing.  And any document you
were relying on might be yanked away from you with no warning.  Such a
system would be so crippled that if Microsoft really did this for Word,
sales of "vi" would go through the roof.
It reminds me of an even better way for a word processor company to make
money: just scramble all your documents, then demand ONE MILLION DOLLARS
for the keys to decrypt them.  The money must be sent to a numbered
Swiss account, and the software checks with a server to find out when
the money has arrived.  Some of the proposals for what companies will
do with Palladium seem about as plausible as this one.
Seth draws an analogy with Acrobat, where the paying customers are
actually the publishers, the reader being given away for free.  So Adobe
does have incentives to put in a lot of DRM features that let authors
control publication and distribution.
But he doesn't follow his reasoning to its logical conclusion when dealing
with Microsoft Word.  That program is sold to end users - people who
create their own documents for the use of themselves and their associates.
The paying customers of Microsoft Word are exactly the ones who would
be screwed over royally by Seth's scheme.  So if we "follow the money"
as Seth in effect recommends, it becomes even more obvious that Microsoft
would never force Word users to be burdened with a DRL feature.
And furthermore, Seth's scheme doesn't rely on TCPA/Palladium.  At the
risk of aiding the fearmongers, I will explain that TCPA technology
actually allows for a much easier implementation, just as it does in so
many other areas.  There is no need for the server to download a key;
it only has to download an updated DRL, and the Word client software
could be trusted to delete anything that was revoked.  But the point
is, Seth's scheme would work just as well today, without TCPA existing.
As I quoted Ross Anderson saying earlier with regard to "serial number
revocation lists", these features don't need TCPA technology.
So while I have some quibbles with Seth's analysis, on the whole it is
the most balanced that I have seen from someone who has no connection
with the designers (other than my own writing, of course).  A personal
gripe is that he referred to Lucky's "critics", plural, when I feel
all alone out here.  I guess I'll have to start using the royal "we".
But he redeemed himself by taking mild exception to Lucky's slide show,
which is a lot farther than anyone else has been willing to go in public.

@_date: 2002-08-12 10:55:19
@_author: AARG!Anonymous 
@_subject: Palladium: technical limits and implications 
I don't think this works.  According to Peter Biddle, the TOR can be
launched even days after the OS boots.  It does not underly the ordinary
user mode apps and the supervisor mode system call handlers and device
        +---------------+------------+   trusted-agent | user mode  |      space      | app space  |      (code      +------------+   compartment)  | supervisor |                 | mode / OS  |  +---+   +---------------+------------+
+---+   +---------------+
This is more how I would see it.  The SCP is more like a peripheral
device, a crypto co-processor, that is managed by the TOR.  Earlier you
quoted Seth's blog:
as justification for putting the nub (TOR) under the OS.  But I think in
this context "more privilege" could just refer to the fact that it is in
the secure memory, which is only accessed by this ring--1 or ring-0 or
whatever you want to call it.  It doesn't follow that the nub has anything
to do with the OS proper.  If the OS can run fine without it, as I think
you agreed, then why would the entire architecture have to reorient itself
once the TOR is launched? In other words, isn't my version simpler, as it adjoins the column at
the left to the pre-existing column at the right, when the TOR launches,
days after boot?  Doesn't it require less instantaneous, on-the-fly,
reconfiguration of the entire structure of the Windows OS at the moment
of TOR launch?  And what, if anything, does my version fail to accomplish
that we know that Palladium can do?
I had thought the hardware might also produce the metrics for trusted
agents, but you could be right that it is the TOR which does so.
That would be consistent with the "incremental extension of trust"
philosophy which many of these systems seem to follow.
No, that doesn't make sense.  Why would the TOR need to compute a metric
of the OS?  Peter has said that Palladium does not give information about
other apps running on your machine:
: Note that in Pd no one but the user can find out the totality of what SW is
: running except for the nub (aka TOR, or trusted operating root) and any
: required trusted services. So a service could say "I will only communicate
: with this app" and it will know that the app is what it says it is and
: hasn't been perverted. The service cannot say "I won't communicate with this
: app if this other app is running" because it has no way of knowing for sure
: if the other app isn't running.
Nothing Peter or anyone else has said indicates that this is a property of
Palladium, as far as I can remember.
No, I think it is there to prevent debuggers and supervisor-mode drivers
from manipulating secure code.  TCPA is more of a whole-machine spec
dealing with booting an OS, so it doesn't have to deal with the question
of running secure code next to insecure code.

@_date: 2002-08-12 11:15:17
@_author: AARG!Anonymous 
@_subject: responding to claims about TCPA 
I believe you did, because if you look at what I actually wrote, I did not
say that "bringing up the topic of DRLs is an indication of bias":
My core claim is the last sentence.  It's one thing to say, as you
are, that TCPA could make applications implement SNRLs more securely.
I believe that is true, and if this statement is presented in the context
of "dangers of TCPA" or something similar, it would be appropriate.
But even then, for a fair analysis, it should make clear that SNRLs can
be done without TCPA, and it should go into some detail about just how
much more effective a SNRL system would be with TCPA.  (I will write more
about this in responding to Joseph Ashwood.)
And to be truly unbiased, it should also talk about good uses of TCPA.
If you look at Ross Anderson's TCPA FAQ at
 he writes (question 4):
: When you boot up your PC, Fritz takes charge. He checks that the boot
: ROM is as expected, executes it, measures the state of the machine;
: then checks the first part of the operating system, loads and executes
: it, checks the state of the machine; and so on. The trust boundary, of
: hardware and software considered to be known and verified, is steadily
: expanded. A table is maintained of the hardware (audio card, video card
: etc) and the software (O/S, drivers, etc); Fritz checks that the hardware
: components are on the TCPA approved list, that the software components
: have been signed, and that none of them has a serial number that has
: been revoked.
He is not saying that TCPA could make SNRLs more effective.  He says
that "Fritz checks... that none of [the software components] has a
serial number that has been revoked."  He is flatly stating that the
TPM chip checks a serial number revocation list.  That is both biased
and factually untrue.
Ross's whole FAQ is incredibly biased against TCPA.  I don't see how
anyone can fail to see that.  If it were titled "FAQ about Dangers of
TCPA" at least people would be warned that they were getting a one-sided
presentation.  But it is positively shameful for a respected security
researcher like Ross Anderson to pretend that this document is giving
an unbiased and fair description.
I would be grateful if someone who disagrees with me, who thinks that
Ross's FAQ is fair and even-handed, would speak up.  It amazes me that
people can see things so differently.
And Lucky's slide presentation,  is if anything
even worse.  I already wrote about this in detail so I won't belabor
the point.  Again, I would be very curious to hear from someone who
thinks that his presentation was unbiased.

@_date: 2002-08-12 15:50:48
@_author: AARG!Anonymous 
@_subject: Seth on TCPA at Defcon/Usenix 
In discussing how TCPA would help enforce a document revocation list
(DRL) Joseph Ashwood contrasted the situation with and without TCPA
style hardware, below.  I just want to point out that his analysis of
the hardware vs software situation says nothing about DRL's specifically;
in fact it doesn't even mention them.
His analysis actually applies to a wide range of security features,
such as the examples given earlier: secure games, improved P2P,
distributed computing as Adam Back suggested, DRM of course, etc..
TCPA is a potentially very powerful security enhancement, so it does
make sense that it can strengthen all of these things, and DRLs as well.
But I don't see that it is fair to therefore link TCPA specifically with
DRLs, when there are any number of other security capabilities that are
also strengthened by TCPA.
It's not always as easy as you make it sound here.  Adam Back wrote
Saturday about the interesting history of the giFT project, which
reverse-engineered the Kazaa file-sharing protocol.  That was a terrific
effort that required considerable cryptographic know-how as well as
supreme software reverse engineering skills.  But then Kazaa changed the
protocol, and giFT never managed to become compatible with the new one.
I'm not sure whether it was lack of interest or just too difficult,
but in any case the project failed (as far as creating an open Kazaa
compatible client).
It is clear that software hacking is far from "almost trivial" and you
can't assume that every software-security feature can and will be broken.
Furthermore, even when there is a break, it won't be available to
everyone.  Ordinary people aren't clued in to the hacker community
and don't download all the latest patches and hacks to disable
security features in their software.  Likewise for business customers.
In practice, if Microsoft wanted to implement a global, facist DRL,
while some people might be able to patch around it, probably 95%+ of
ordinary users would be stuck with it.
Therefore a DRL in software would be far from useless, and if there
truly was a strong commercial need for such a solution then chances are
it would be there today.
I might mention BTW that for email there is such a product,
disappearingink.com, which works along the lines Seth suggested, I
believe.  It encrypts email with a centralized key, and when that email
needs to be deleted, the key is destroyed.  This allows corporations to
implement a "document retention policy" (which is of course a euphemism
for a document destruction policy) to help reduce their vulnerability to
lawsuits and fishing expeditions.  I don't recall anyone getting up in
arms over the disappearingink.com technology or claiming that it was a
threat, in the same way that DRLs and SNRLs are being presented in the
context of Palladium.
First, as far as this last point, you acknowledge that if they can't
tell where it came from, your hacked hardware can be an ongoing source of
un-DRL'd documents.  But watermarking technology so far has been largely
a huge failure, so it is likely that someone clueful enough to hack his
TPM could also strip away any identifying markings.
Second, given that you do hack the hardware, you may not actually need
to do that much in terms of protocol hacking.  If you can watch the data
going to and from the TPM you can extract keys directly, and that may
be enough to let you decrypt the "sealed" data.  (The TPM does only
public key operations; the symmetric crypto is all done by the app.
I don't know if Palladium will work that way or not.)
Third, if a document is "liberated" via this kind of hack, it can
then be distributed everywhere, outside the "secure trust perimeter"
enforced by TCPA/Palladium.  We are still in a "break once read anywhere"
situation with documents, and any attempt to make one disappear is not
going to be very successful, even with TCPA in existence.
In short, while TCPA could increase the effectiveness of global DRLs,
they wouldn't be *that* much more effective.  Most users will neither
hack their software nor their hardware, so the hardware doesn't make
any difference for them.  Hackers will be able to liberate documents
completely from DRL controls, whether they use hardware or software
to do it.  The only difference is that there will be fewer hackers,
if hardware is used, because it is more difficult.  Depending on the
rate at which important documents go on DRLs, that may not make any
difference at all.
I agree that providing an option to store documents in restricted form
could be a desirable feature for businesses.  And having the ability
to delete documents on a company-wide basis, a la disappearingink.com,
could make sense as well.  I don't know that there is a huge market for
this capability, or I suspect we'd see it already.  But it does make
sense as an auxiliary part of a business document product.
But to me this points to a localized DRL, part of a document-management
library that is used solely for documents within the company.  The company
would want to control the administration of its documents.  I don't
see this leading to the kind of centralized, global system which I
think opponents of TCPA are attempting to invoke when they talk about it
allowing DRLs, and which is the kind of thing I was talking about above.

@_date: 2002-08-13 10:10:08
@_author: AARG!Anonymous 
@_subject: Challenge to David Wagner on TCPA 
This makes a lot of sense, especially for "closed" systems like business
LANs and WANs where there is a reasonable centralized authority who can
validate the security of the SCP keys.  I suggested some time back that
since most large businesses receive and configure their computers in
the IT department before making them available to employees, that would
be a time that they could issue private certs on the embedded SCP keys.
The employees' computers could then be configured to use these private
certs for their business computing.
However the larger vision of trusted computing leverages the global
internet and turns it into what is potentially a giant distributed
computer.  For this to work, for total strangers on the net to have
trust in the integrity of applications on each others' machines, will
require some kind of centralized trust infrastructure.  It may possibly
be multi-rooted but you will probably not be able to get away from
this requirement.
The main problem, it seems to me, is that validating the integrity of
the SCP keys cannot be done remotely.  You really need physical access
to the SCP to be able to know what key is inside it.  And even that
is not enough, if it is possible that the private key may also exist
outside, perhaps because the SCP was initialized by loading an externally
generated public/private key pair.  You not only need physical access,
you have to be there when the SCP is initialized.
In practice it seems that only the SCP manufacturer, or at best the OEM
who (re) initializes the SCP before installing it on the motherboard,
will be in a position to issue certificates.  No other central authorities
will have physical access to the chips on a near-universal scale at the
time of their creation and installation, which is necessary to allow
them to issue meaningful certs.  At least with the PGP "web of trust"
people could in principle validate their keys over the phone, and even
then most PGP users never got anyone to sign their keys.  An effective
web of trust seems much more difficult to achieve with Palladium, except
possibly in small groups that already trust each other anyway.
If we do end up with only a few trusted root keys, most internet-scale
trusted computing software is going to have those roots built in.
Those keys will be extremely valuable, potentially even more so than
Verisign's root keys, because trusted computing is actually a far more
powerful technology than the trivial things done today with PKI.  I hope
the Palladium designers give serious thought to the issue of how those
trusted root keys can be protected appropriately.  It's not going to be
enough to say "it's not our problem".  For trusted computing to reach
its potential, security has to be engineered into the system from the
beginning - and that security must start at the root!

@_date: 2002-08-14 20:45:25
@_author: AARG!Anonymous 
@_subject: Overcoming the potential downside of TCPA 
Actually, this is not true for the endoresement key, PUBEK/PRIVEK, which
is the "main" TPM key, the one which gets certified by the "TPM Entity".
That key is generated only once on a TPM, before ownership, and must
exist before anyone can take ownership.  For reference, see section 9.2,
"The first call to TPM_CreateEndorsementKeyPair generates the endorsement
key pair. After a successful completion of TPM_CreateEndorsementKeyPair
all subsequent calls return TCPA_FAIL."  Also section 9.2.1 shows that
no ownership proof is necessary for this step, which is because there is
no owner at that time.  Then look at section 5.11.1, on taking ownership:
"user must encrypt the values using the PUBEK."  So the PUBEK must exist
before anyone can take ownership.
I don't quite follow what you are proposing here, but by the time you
purchase a board with a TPM chip on it, it will have already generated
its PUBEK and had it certified.  So you should not be able to transfer
a credential of this type from one board to another one.
Actually I don't see a function that will let the owner wipe the PUBEK.
He can wipe the rest of the TPM but that field appears to be set once,
retained forever.
For example, section 8.10: "Clear is the process of returning the TPM to
factory defaults."  But a couple of paragraphs later: "All TPM volatile
and non-volatile data is set to default value except the endorsement
key pair."
So I don't think your fraud will work.  Users will not wipe their
endorsement keys, accidentally or otherwise.  If a chip is badly enough
damaged that the PUBEK is lost, you will need a hardware replacement,
as I read the spec.
Keep in mind that I only started learning this stuff a few weeks ago,
so I am not an expert, but this is how it looks to me.

@_date: 2002-08-15 15:26:20
@_author: AARG!Anonymous 
@_subject: TCPA not virtualizable during ownership change 
Basically I agree with Adam's analysis.  At this point I think he
understands the spec equally as well as I do.  He has a good point
about the Privacy CA key being another security weakness that could
break the whole system.  It would be good to consider how exactly that
problem could be eliminated using more sophisticated crypto.  Keep in
mind that there is a need to be able to revoke Endorsement Certificates
if it is somehow discovered that a TPM has been cracked or is bogus.
I'm not sure that would be possible with straight Chaum blinding or
Brands credentials.  I would perhaps look at Group Signature schemes;
there is one with efficient revocation being presented at Crypto 02.
These involve a TTP but he can't forge credentials, just link identity
keys to endorsement keys (in TCPA terms).  Any system which allows for
revocation must have such linkability, right?
As for Joe Ashwood's analysis, I think he is getting confused between the
endorsement key, endorsement certificate, and endorsement credentials.
The first is the key pair created on the TPM.  The terms PUBEK and PRIVEK
are used to refer to the public and private parts of the endorsement
key.  The endorsement certificate is an X.509 certificate issued on the
endorsement key by the manufacturer.  The manufacturer is also called
the TPM Entity or TPME.  The endorsement credential is the same as the
endorsement certificate, but considered as an abstract data structure
rather than as a specific embodiment.
The PRIVEK never leaves the chip.  The PUBEK does, but it is considered
sensitive because it is a de facto unique identifier for the system,
like the Intel processor serial number which caused such controversy
a few years ago.  The endorsement certificate holds the PUBEK value
(in the SubjectPublicKeyInfo field) and so is equally a de facto unique
identifier, hence it is also not too widely shown.

@_date: 2002-08-16 15:56:09
@_author: AARG!Anonymous 
@_subject: Cryptographic privacy protection in TCPA 
Here are some more thoughts on how cryptography could be used to
enhance user privacy in a system like TCPA.  Even if the TCPA group
is not receptive to these proposals, it would be useful to have an
understanding of the security issues.  And the same issues arise in
many other kinds of systems which use certificates with some degree
of anonymity, so the discussion is relevant even beyond TCPA.
The basic requirement is that users have a certificate on a long-term key
which proves they are part of the system, but they don't want to show that
cert or that key for most of their interactions, due to privacy concerns.
They want to have their identity protected, while still being able to
prove that they do have the appropriate cert.  In the case of TCPA the
key is locked into the TPM chip, the "endorsement key"; and the cert
is called the "endorsement certificate", expected to be issued by the
chip manufacturer.  Let us call the originating cert issuer the CA in
this document, and the long-term cert the "permanent certificate".
A secondary requirement is for some kind of revocation in the case
of misuse.  For TCPA this would mean cracking the TPM and extracting
its key.  I can see two situations where this might lead to revocation.
The first is a "global" crack, where the extracted TPM key is published
on the net, so that everyone can falsely claim to be part of the TCPA
system.  That's a pretty obvious case where the key must be revoked for
the system to have any integrity at all.  The second case is a "local"
crack, where a user has extracted his TPM key but keeps it secret, using
it to cheat the TCPA protocols.  This would be much harder to detect,
and perhaps equally significantly, much harder to prove.  Nevertheless,
some way of responding to this situation is a desirable security feature.
The TCPA solution is to use one or more Privacy CAs.  You supply your
permanent cert and a new short-term "identity" key; the Privacy CA
validates the cert and then signs your key, giving you a new cert on the
identity key.  For routine use on the net, you show your identity cert
and use your identity key; your permanent key and cert are never shown
except to the Privacy CA.
This means that the Privacy CA has the power to revoke your anonymity;
and worse, he (or more precisely, his key) has the power to create bogus
identities.  On the plus side, the Privacy CA can check a revocation list
and not issue a new identity cert of the permanent key has been revoked.
And if someone has done a local crack and the evidence is strong enough,
the Privacy CA can revoke his anonymity and allow his permanent key to
be revoked.
Let us now consider some cryptographic alternatives.  The first is to
use Chaum blinding for the Privacy CA interaction.  As before, the user
supplies his permanent cert to prove that he is a legitimate part of
the system, but instead of providing an identity key to be certified,
he supplies it in blinded form.  The Privacy CA signs this blinded key,
the user strips the blinding, and he is left with a cert from the Privacy
CA on his identity key.  He uses this as in the previous example, showing
his privacy cert and using his privacy key.
In this system, the Privacy CA no longer has the power to revoke your
anonymity, because he only saw a blinded version of your identity key.
However, the Privacy CA retains the power to create bogus identities,
so the security risk is still there.  If there has been a global crack,
and a permanent key has been revoked, the Privacy CA can check the
revocation list and prevent that user from acquiring new identities,
so revocation works for global cracks.  However, for local cracks,
where there is suspicious behavior, there is no way to track down the
permanent key associated with the cheater.  All his interactions are
done with an identity key which is unlinkable.  So there is no way to
respond to local cracks and revoke the keys.
Actually, in this system the Privacy CA is not really protecting
anyone's privacy, because it doesn't see any identities.  There is no
need for multiple Privacy CAs and it would make more sense to merge
the Privacy CA and the original CA that issues the permanent certs.
That way there would be only one agency with the power to forge keys,
which would improve accountability and auditability.
One problem with revocation in both of these systems, especially the one
with Chaum blinding, is that existing identity certs (from before the
fraud was detected) may still be usable.  It is probably necessary to
have identity certs be valid for only a limited time so that users with
revoked keys are not able to continue to use their old identity certs.
Brands credentials provide a more flexible and powerful approach than
Chaum blinding which can potentially provide improvements.  The basic
setup is the same: users would go to a Privacy CA and show their
permanent cert, getting a new cert on an identity key which they would
use on the net.  The difference is that Brands provides for "restrictive
blinding".  This allows the Privacy CA to issue a cert on a key which
would be unlinkable to the permanent key under normal circumstances,
but perhaps linkability could be established in some cases.
It's not entirely clear how this technology could best be exploited to
solve the problems.  One possibility, for example, would be to encode
information about the permanent key in the restrictive blinding.
This would allow users to use their identity keys freely; but upon
request they could prove things about their associated permanent keys.
They could, for example, reveal the permanent key value associated with
their identity key, and do so unforgeably.  Or they could prove that their
permanent key is not on a given list of revoked keys.  Similar logical
operations are possible including partial revelation of the permanent
key information.
However it does not appear possible to solve the case of a local crack
using this technology.  In that case it is unlikely that they would
respond favorably to a request to reveal the permanent key associated with
their identity, so that it could be revoked.  Brands' technology would
allow them to do so in a convincing manner, but they would not cooperate.
In the end it's not clear how much Brands certificates really add over
the basic Chaum blinding in this application.  With the specific usage
described above, they have the same basic security properties as in
the case of Chaum blinding, except potentially for being able to prove
that an identity cert is not associated with a revoked permanent key.
Perhaps some other approach using his technology would be more successful.
One other cryptographic method that might be relevant is the group
signature.  This allows someone to sign with a key where he does not
reveal his signing key, but he proves that it is part of some group.
In the relevant variants, the group is defined as the set of keys
which has been certified by a "group membership key".  This approach
can therefore dispense with the Privacy CA entirely, and with blinding.
Instead, the permanent key itself is used for signing on the net, but
via a group signature which does not reveal the key value.  Instead,
the group signature protocol proves that the key exists and that it has
been certified by the CA.
The main problem with the group signature approach is handling revocation.
In the case of a global crack, where someone has published his permanent
key, at a minimum it is necessary to create a revocation list for those
keys.  This means that the group signature protocol must be extended
to not only prove that a key exists and has been certified, but also
that the key is not on the list of revoked keys - and to do this without
revealing the key itself.  That's a pretty complicated requirement which
is pushing the state of the art.  There is a paper being presented at
Crypto 02 which claims to offer the first group signature scheme with
efficient revocation.
Group signatures also offer an optional mechanism which can deal
with local cracks.  The original group signature concept included the
concept of a "revocation manager" who could link signatures to keys -
that is, there is one trusted party who can tell which key issued a
given signature.  In most of the modern variants, this is accomplished
by creating, as part of the group signature, an encrypted blob which
holds the user's permanent key, where that blob can be encrypted to any
specified key.  The only one who can tell who made the signature is the
key holder that the blob is encrypted to.
If this mechanism is used, we can bring back the Privacy CA, who
now functions as the party who can link signatures to permanent keys.
When someone uses a group signature to participate in a TCPA network, he
would optionally specify a Privacy CA who could reveal his permanent key.
This would allow for a multiplicity of Privacy CAs with different policies
about when and how they would reveal idenities, similar to the original
(non-cryptographic) TCPA concept.  Then it would be up to the recipients
of the signature to judge whether they trusted that Privacy CA to unmask
rogues upon sufficient evidence.
The main advantage of this scheme over the non-cryptographic TCPA method
is, first, that the Privacy CA is optional - users don't have to reveal
their identity to anyone if they don't want; and second, that the Privacy
CA no longer has the power to forge identities and disrupt the system.
This strengthens the overall security of the system.
Summing up, none of the alternatives presented here is ideal.  The current
scheme is among the worst, as it provides the weakest privacy protection
and allows the Privacy CAs to break the security of the entire system.
The Chaum and Brands blinding methods strengthen privacy at the cost of
reducing the ability to respond to local cracks, where the user extracts
his TPM key but keeps it to himself.  Group signatures provide good
privacy protection and can optionally respond to local cracks, but they
are cutting edge cryptography and are generally less efficient than the
other methods.

@_date: 2002-08-17 11:45:28
@_author: AARG!Anonymous 
@_subject: Cryptographic privacy protection in TCPA 
Dr. Mike wrote, patiently, persistently and truthfully:
Fine, but let me put this into perspective.  First, although the
discussion is in terms of a centralized issuer, the same issues arise if
there are multiple issuers, even in a web-of-trust situation.  So don't
get fixated on the fact that my analysis assumed a single issuer -
that was just for simplicity in what was already a very long message.
The abstract problem to be solved is this: given that there is some
property which is being asserted via cryptographic certificates
(credentials), we want to be able to show possession of that property
in an anonymous way.  In TCPA the property is "being a valid TPM".
Another example would be a credit rating agency who can give out a "good
credit risk" credential.  You want to be able to show it anonymously in
some cases.  Yet another case would be a state drivers license agency
which gives out an "over age 21" credential, again where you want to be
able to show it anonymously.
This is actually one of the oldest problems which proponents of
cryptographic anonymity attempted to address, going back to David Chaum's
seminal work.  TCPA could represent the first wide-scale example of
cryptographic credentials being shown anonymously.  That in itself ought
to be of interest to cypherpunks.  Unfortunately TCPA is not going for
full cryptographic protection of anonymity, but relying on Trusted Third
Parties in the form of Privacy CAs.  My analysis suggests that although
there are a number of solutions in the cryptographic literature, none of
them are ideal in this case.  Unless we can come up with a really strong
solution that satisfies all the security properties, it is going to be
hard to make a case that the use of TTPs is a mistake.
A certificate is a standardized and unforgeable statement that some
person or key has a particular property, that's all.  The kind of system
you are talking about, of personal knowledge and trust, can't really be
generalized to an international economy.
Whoever makes a statement about a property should have the power to
revoke it.  I am astounded that you think this is a radical notion.
If one or a few entities become widely trusted to make and revoke
statements that people care about, it is because they have earned that
trust.  If the NY Times says something is true, people tend to believe it.
If Intel says that such-and-such a key is in a valid TPM, people may
choose to believe this based on Intel's reputation.  If Intel later
determines that the key has been published on the net and so can no
longer be presumed to be a TPM key, it revokes its statement.
This does not mean that Intel would destroy any person's ability to use
their computer on a whim.  First, having the TPM cert revoked would not
destroy your ability to use your computer; at worst you could no longer
persuade other people of your trustworthiness.  And second, Intel would
not make these kind of decision on a whim, any more than the NY Times
would publish libelous articles on a whim; doing so would risk destroying
the company's reputation, one of its most valuable assets.
I can't really respond to the remainder of the message.  It doesn't seem
to have anything to do with the real issues.  Hopefully my introduction
above will have put the problem into perspective.  I suggest you educate
yourself on cryptographic technologies for anonymity.  You might start
with David Chaum's early CACM article,

@_date: 2002-08-21 23:45:29
@_author: AARG!Anonymous 
@_subject: New Palladium FAQ available 
Microsoft has apparently just made available a new FAQ on its
controversial Palladium technology at
Hopefully Microsoft will continue to release information about Palladium.
That should help to bring some of the more outrageous rumors under

@_date: 2002-08-30 20:56:03
@_author: AARG!Anonymous 
@_subject: Quantum computers inch closer? 
The problem is that you can't forcibly collapse the state vector into your
wished-for eigenstate, the one where the plaintext recognizer returns a 1.
Instead, it will collapse into a random state, associated with a random
key, and it is overwhelmingly likely that this key is one for which the
recognizer returns 0.

@_date: 2002-07-05 14:45:21
@_author: AARG!Anonymous 
@_subject: Ross's TCPA paper 
Wouldn't it be more accurate to say that a "trusted" OS will not peek
at system resources that it is not supposed to?  After all, since the
OS loads the application, it has full power to molest that application
in any way.  Any embedded keys or certs in the app could be changed by
the OS.  There is no way for an application to protect itself against
the OS.
And there is no need; a trusted OS by definition does not interfere with
the application's use of confidential data.  It does not allow other
applications to get access to that data.  And it provides no back doors
for "root" or the system owner or device drivers to get access to the
application data, either.
At  you provide more
information about your meeting with Microsoft.  It's an interesting
writeup, but the part about the system somehow protecting the app from the
OS can't be right.  Apps don't have that kind of structural integrity.
A chip in the system cannot protect them from an OS virtualizing that
chip.  What the chip does do is to let *remote* applications verify that
the OS is running in trusted mode.  But local apps can never achieve
that degree of certainty, they are at the mercy of the OS which can
twiddle their bits at will and make them "believe" anything it wants.
Of course a "trusted" OS would never behave in such an uncouth manner.
Absolutely.  The fantasies which have been floating here of filters
preventing people from typing virus-triggering command lines are utterly
absurd.  What are people trying to prove by raising such nonsensical
propositions?  Palladium needs no such capability.
Right, and you can boot untrusted OS's as well.  Recently there was
discussion here of HP making a trusted form of Linux that would work with
the TCPA hardware.  So you will have options in both the closed source and
open source worlds to boot trusted OS's, or you can boot untrusted ones,
like old versions of Windows.  The user will have more choice, not less.
Yes, your web page goes into somewhat more detail about how this would
work.  This way a program can run under a secure OS and store sensitive
data on the disk, such that booting into another OS will then make it
impossible to decrypt that data.
Some concerns have been raised here about upgrades.  Did Microsoft
discuss how that was planned to work, migrating from one version of a
secure OS to another?  Presumably they have different hashes, but it
is necessary for the new one to be able to unseal data sealed by the
old one.
One obvious solution would be for the new OS to present a cert to the chip
which basically said that its OS hash should be treated as an "alias"
of the older OS's hash.  So the chip would unseal using the old OS hash
even when the new OS was running, based on the fact that this cert was
signed by the TCPA trusted root key.
This seems to put more power than we would like into a single trusted
key, though.  It would be interesting to hear what Microsoft has in mind
along these lines.
If you've read the TCPA specs you're way ahead of most of the commentators
here.  You have undoubtedly noted how little connection there is between
the flights of fancy and speculation which have appeared recently and
the actual functionality of the TCPA system.

@_date: 2002-09-16 10:51:23
@_author: AARG!Anonymous 
@_subject: Cryptogram: Palladium Only for DRM 
One likely use of Pd for banking software would be to use the "secure
vault" to lock up account number and password information.  This would
ensure that no other software than the banking client could access this
data, so that if you got a virus it would not be able to empty your
banking account.  And if the virus infected the banking client software
itself, that would change its hash which would keep it from being able
to access the data.
Also, Palladium's attestation feature can be used to let the remote bank
server check that the local client is clean and uninfected.  This will
catch the case where a virus infects the client before it initially
creates the "vault".
Contrary to Niels Ferguson's comments, these kinds of applications
are far from silly.  As we move into an era where more individuals use
electronic banking systems, we face the risk that viruses can inflict
serious financial costs on their victims.  The next Nimda could empty
your bank account and transfer its entire contents irreversibly to an
overseas server.  Given this threat, the defenses above seem not only
desirable, but absolutely necessary to protect people from massive theft
and fraud.  Ordinary cryptography and CPU process architectures are not
enough to provide this type of security.

@_date: 2002-09-16 16:04:19
@_author: AARG!Anonymous 
@_subject: Cryptogram: Palladium Only for DRM 
The Palladium white paper [1] provides more information.  They have some
special memory which is supposed to be isolated from the rest of the
computer.  "Trusted code runs in memory that is physically isolated,
protected, and inaccessible to the rest of the system, making it
inherently impervious to viruses, spy-ware, or other software attacks."
Also, "Trusted code cannot be observed or modified when running in the
trusted execution space."
Probably what happens is that software is hashed as it is loaded into this
secure memory region.  This hash assures that the software is undamaged
at load time, and then the hardware keeps it safe from modification
after that.
Nothing done purely in software will be as effective as what can be done
when you have secure hardware as the foundation.  I discuss this in more
detail below.
Microsoft has never denied that Palladium can be used for DRM.  From the
white paper [1], "While DRM and 'Palladium' are both supportive of
Trustworthy Computing, neither is absolutely required for the other to
work. DRM can be deployed on non-'Palladium' machines, and 'Palladium'
can provide users with benefits independent of DRM. They are separate
technologies. That said, the current software-based DRM technologies
can be rendered stronger when deployed on 'Palladium'-based computers."
All they have ever said is that Palladium is not solely for DRM.  It seems
clear that Palladium has other uses.  Contrast this with the comment by
Niels, that the secure chip is "only there to support DRM".  Again and
again critics make this very strong claim, that Palladium's design is
ONLY for DRM.  Is it any wonder that Microsoft downplays the importance
of DRM, if only to try to get some balance against this extreme position?
You won't get as much security if you do it entirely in software.  As one
obvious point of attack, what if the security-critical portions of the
OS are altered by a virus, either in memory or on disk?  Palladium can
protect against that in two ways: by using its shielded memory which is
inaccessible to unauthorized programs; and by using its security chip
to make sure that the critical parts of the OS are the same as before
when they are loaded into this special area.
Your cryptography won't protect against an attack where a virus is
initiating the transaction, pretending to be the user.  That kind of
attack is out of scope for cryptographic protection, but it is one of
the things that Palladium is intended to address.
Only if the security kernel itself is intact, as well as its copy of the
root key that it trusts to issue these signatures.  Palladium gives you
a hardware root of trust for this problem of whether your software has
been hacked.
To recap, Palladium basically provides just three functions:
 - The secure memory allows software to be loaded and its hash taken by
   the security chip, and then it keeps the software safe.
 - The security chip can optionally report the hash to a remote party,
   signed with its key.
 - The security chip can encrypt and decrypt data, embedding the hash
   so that the decryption can only be done if the hash is unchanged.
A smart guy like you ought to be able to figure out uses for this beyond
DRM.  While walking the dog a few weeks ago I came up with several ways
to use the system [2] that rely on letting the user provably give up some
control of his machine.  Even Adam Back, who opposes this technology,
was willing to describe additional positive uses [3], including selling
CPU cycles such that the purchaser gains greater assurance of privacy
and integrity.
Adam had the intellectual honesty and integrity to say something favorable
about this technology even though he continued to oppose it on balance.
It's sad to see how few people have the courage to take this stance.
Ask yourself, if you did think of a good use for Palladium, would you
speak up about it in order to promote a balanced and fully informed
discussion?  Or would you remain silent in order to further your personal
political aims?
In any case, it is clear that even to the degree that Palladium is
designed to allow software to run in an environment where the user can
credibly promise not to molest it, there are many other applications
beyond DRM.
That's a great idea.  I don't know why nobody thought of that before.
Right, so all we need is good design!  Damn, all these years struggling
to write secure software, and it's been staring us in the face all along.
The idea is that Palladium is a small amount of code that manages access
to the shielded memory and the security coprocessor.  If you can get this
little bit of code right, then you can get the securitiy promised by
That's good to know.  Who exactly eats the charges if the money is no
longer present in the foreign bank account by the time you convince
everyone that the transfer was fraudulent?  And how are you going to
prove that, anyway?  Have you heard Ross Anderson's stories about people
trying to get unauthorized ATM withdrawals reversed?
[1] [2]  at wasabisystems.com/msg02497.html
[3]  at wasabisystems.com/msg02526.html

@_date: 2002-09-16 16:47:27
@_author: AARG!Anonymous 
@_subject: Cryptogram: Palladium Only for DRM 
Pd prevents users from auditing the program.  While it is on the disk,
the program is an ordinary file and not encrypted [1].  This will allow
it to be disassembled and inspected.
In addition, I have argued that trusted computing in general will work
very well with open source software.  It may even be possible to allow
the user to build the executable himself using a standard compilation
environment.  (Of course, in actuality few end users are prepared to run
compilers for themselves, but as long as at least some people can do it,
this can provide assurance that the executable matches the source.)
Running an open-source program on a trusted computing platform provides
the best of both worlds.  The user is protected against misbehavior
on the part of the executable, because he knows exactly what it can do.
And the software is protected against misbehavior on the part of the user,
by virtue of the hardware protection.  In this way, the interests of all
parties are balanced.
[1] A message from Microsoft's Peter Biddle on 5 Aug 2002; unfortunately
the cryptography archive is missing this day's messages.  "The memory
isn't encrypted, nor are the apps nor the TOR when they are on the
hard drive. Encrypting the apps wouldn't make them more secure, so they
aren't encrypted."  See also
 at wasabisystems.com/msg02554.html,
Lucky Green's description of Microsoft's lack of plans to use Pd for
copy protection.
